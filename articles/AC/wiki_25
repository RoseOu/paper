<doc id="26783" url="https://en.wikipedia.org/wiki?curid=26783" title="Statute">
Statute

A statute is a formal written enactment of a legislative authority that governs the legal entities of a city, state, or country by way of consent. Typically, statutes command or prohibit something, or declare policy. Statutes are rules made by legislative bodies; they are distinguished from case law or precedent, which is decided by courts, and regulations issued by government agencies.

In virtually all countries, newly enacted statutes are published in a Government gazette which is then distributed so that everyone can look up the statutory law.

A universal problem encountered by lawmakers throughout human history is how to organize published statutes. Such publications have a habit of starting small but growing rapidly over time, as new statutes are enacted in response to the exigencies of the moment. Eventually, persons trying to find the law are forced to sort through an enormous number of statutes enacted at various points in time to determine which portions are still in effect.
The solution adopted in many countries is to organize existing statutory law in topical arrangements (or "codified") within publications called codes, then ensure that new statutes are consistently drafted so that they add, amend, repeal or move various code sections. In turn, in theory, the code will thenceforth reflect the current cumulative state of the statutory law in that jurisdiction. In many nations statutory law is distinguished from and subordinate to constitutional law.

The term statute is also used to refer to an International treaty that establishes an institution, such as the Statute of the European Central Bank, a protocol to the international courts as well, such as the Statute of the International Court of Justice and the Rome Statute of the International Criminal Court. Statute is also another word for law. The term was adapted from England in about the 18th century.

In the autonomous communities of Spain, an autonomy statute is a legal document similar to the constitution of a federated state, save that it is enacted by the national legislature, rather than the autonomous community it governs. The autonomy statutes in Spain have the rank of "ley organica" (organic law), a category of special legislation reserved only for the main institutions and issues and mentioned in the constitution (the highest ranking legal instrument in Spain). "Leyes organicas" rank between the constitution and ordinary laws. The name was chosen, among others, to avoid confusion with the term "constitution" (i.e. the Spanish constitution of 1978).

In biblical terminology, statute (Hebrew "choq") refers to a law given without any reason or justification. The classic example is the statute regarding the Red Heifer.(Numbers 19:2)

The opposite of a chok is a "mishpat", a law given for a specified reason, e.g. the Sabbath laws, which were given because "God created the world in six days, but on the seventh day He rested" (Genesis 2:2-3).

"That which upholds, supports or maintains the regulatory order of the universe" meaning the "Law" or "Natural Law". This is a concept of central importance in Indian philosophy and religion.



</doc>
<doc id="26784" url="https://en.wikipedia.org/wiki?curid=26784" title="Statutory law">
Statutory law

Statutory law or statute law is written law passed by a body of legislature. This is as opposed to oral or customary law; or regulatory law promulgated by the executive or common law of the judiciary. 
Statutes may originate with national, state legislatures or local municipalities.

The term codified law refers to statutes that have been organized ("codified") by subject matter; in this narrower sense, some but not all statutes are considered "codified." The entire body of codified statute is referred to as a "code," such as the United States Code, the Ohio Revised Code or the Code of Canon Law. The substantive provisions of the Act could be codified (arranged by subject matter) in one or more titles of the United States Code while the provisions of the law that have not reached their "effective date" (remaining uncodified) would be available by reference to the United States Statutes at Large. Another meaning of "codified law" is a statute that takes the common law in a certain area of the law and puts it in statute or code form.

Another example of statutes that are not typically codified is a "private law" that may originate as a private bill, a law affecting only one person or a small group of persons. An example was divorce in Canada prior to the passage of the Divorce Act of 1968. If unavailable by administrative or judicial means, it was possible to obtain a legislative divorce by application to the Senate of Canada, which reviewed and investigated petitions for divorce, which would then be voted upon by the Senate and subsequently made into law. 

In the United Kingdom Parliament, private bills were used in the nineteenth century to create corporations, grant monopolies and give individuals attention to be more fully considered by the parliament. The government may also seek to have a bill introduced "unofficially" by a backbencher so as not to create a public scandal; such bills may also be introduced by the loyal opposition — members of the opposition party or parties. Sometimes a private member's bill may also have private bill aspects, in such case the proposed legislation is called a hybrid bill.




</doc>
<doc id="26785" url="https://en.wikipedia.org/wiki?curid=26785" title="Sanction">
Sanction

A sanction may be either a permission or a restriction, depending upon context, as the word is an auto-antonym.

Examples of sanctions include:





</doc>
<doc id="26786" url="https://en.wikipedia.org/wiki?curid=26786" title="Sarajevo">
Sarajevo

Sarajevo ( ; , ; "see ") is the capital and largest city of Bosnia and Herzegovina, with a population of 275,524 in its administrative limits. The Sarajevo metropolitan area, including Sarajevo Canton, East Sarajevo and nearby municipalities, is home to 555,210 inhabitants. Nestled within the greater Sarajevo valley of Bosnia, it is surrounded by the Dinaric Alps and situated along the Miljacka River in the heart of the Balkans.

Sarajevo is the political, financial, social and cultural center of Bosnia and Herzegovina and a prominent center of culture in the Balkans, with region-wide influence in entertainment, media, fashion and the arts.

Due to its long history of religious and cultural diversity, Sarajevo is sometimes called the "Jerusalem of Europe" or "Jerusalem of the Balkans". It is one of only a few major European cities to have a mosque, Catholic church, Orthodox church and synagogue within the same neighborhood. A regional center in education, the city is home to the Balkans’ first institution of tertiary education in the form of an Islamic madrasa, today part of the University of Sarajevo.

Although settlement in the area stretches back to prehistoric times, the modern city arose as an Ottoman stronghold in the 15th century. Sarajevo has attracted international attention several times throughout its history. In 1885, Sarajevo was the first city in Europe and the second city in the world to have a full-time electric tram network running through the city, following San Francisco. In 1914, it was the site of the assassination of Archduke Franz Ferdinand of Austria by local Young Bosnia activist Gavrilo Princip that sparked World War I, which also ended Austro-Hungarian rule in Bosnia and resulted in the creation of the Kingdom of Yugoslavia. Later, after World War II, the establishment of the Socialist Republic of Bosnia and Herzegovina within the Second Yugoslavia led to a massive expansion of Sarajevo, then the constituent republic's capital, which culminated with the hosting of the 1984 Winter Olympics marking a prosperous era for the city. However, after the start of the Yugoslav Wars, for 1,425 days, from April 1992 to February 1996, the city suffered the longest siege of a capital city in the history of modern warfare, during the Bosnian War and the breakup of Yugoslavia.

Sarajevo has been undergoing post-war reconstruction, and is the fastest growing city in Bosnia and Herzegovina. The travel guide series "Lonely Planet" has named Sarajevo as the 43rd best city in the world, and in December 2009 listed Sarajevo as one of the top ten cities to visit in 2010. In 2011, Sarajevo was nominated to be the 2014 European Capital of Culture and in 2019, it hosted the European Youth Olympic Festival. In October 2019, Sarajevo was designated as a UNESCO Creative City for placing culture at the center of its development strategies, and is one of the world's eighteen Cities of Film.

The earliest known name for the large central Bosnian region of today's Sarajevo is Vrhbosna.

The name "Sarajevo" derives from the Turkish noun "saray", meaning "palace" or "mansion" (from the Persian "sarāy", "house, palace"). The letter "j" in the Bosnian language is equivalent soundwise to the English letter "y" as in "boy" and "yet". The "evo" portion may come from the term "saray ovası" first recorded in 1455, meaning "the plains around the palace" or simply "palace plains".
However, in his Dictionary of Turkish loanwords, Abdulah Škaljić maintains that the ""evo"" ending is more likely to have come from the widespread Slavic suffix ""evo"" used to indicate place names, than from the Turkish ending ""ov"a", as proposed by some. The first mention of name Sarajevo was in 1507 letter written by Feriz Beg. The official name during the 400-year Ottoman period was "Saraybosna" (Palace of Bosnia), and it is still known by that name in modern Turkish.

Sarajevo has had many nicknames. The earliest is "Šeher", which is the term Isa-Beg Ishaković used to describe the town he was going to build. It is a Turkish word meaning an advanced city of key importance ("şehir") which in turn comes from "shahr" (city). As Sarajevo developed, numerous nicknames came from comparisons to other cities in the Islamic world, i.e. "Damascus of the North". The most popular of these was "European Jerusalem".

Sarajevo is near the geometric center of the triangular-shaped Bosnia-Herzegovina and within the historical region of Bosnia proper. It is situated above sea level and lies in the Sarajevo valley, in the middle of the Dinaric Alps. The valley itself once formed a vast expanse of greenery, but gave way to urban expansion and development in the post-World War II era. The city is surrounded by heavily forested hills and five major mountains. The highest of the surrounding peaks is Treskavica at , then Bjelašnica mountain at , Jahorina at , Trebević at , with Igman being the shortest. The last four are also known as the Olympic Mountains of Sarajevo (see also 1984 Winter Olympics). The city itself has its fair share of hilly terrain, as evidenced by the many steeply inclined streets and residences seemingly perched on the hillsides.

The Miljacka river is one of the city's chief geographic features. It flows through the city from east through the center of Sarajevo to west part of city where eventually meets up with the Bosna river. Miljacka river is "The Sarajevo River", with its source ("Vrelo Miljacke") south of the town of Pale at the foothills of Mount Jahorina, several kilometers to the east of Sarajevo center. The Bosna's source, Vrelo Bosne near Ilidža (west Sarajevo), is another notable natural landmark and a popular destination for Sarajevans and other tourists. Several smaller rivers and streams such as Koševski Potok also run through the city and its vicinity.

Sarajevo is close to the center of the triangular shape of Bosnia and Herzegovina in southeastern Europe. Sarajevo city proper consists of four municipalities (or "in Bosnian and Croatian: općina, in Serbian: opština"): Centar (Center), Novi Grad (New City), Novo Sarajevo (New Sarajevo), and Stari Grad (Old City), while Metropolitan area of Sarajevo (Greater Sarajevo area) includes these and the neighbouring municipalities of Ilidža, Hadžići, Vogošća and Ilijaš.

The Metropolitan area was reduced in the 1990s after the war and the Dayton-imposed administrative division of the country, with several municipalities partitioned along the border of the newly recognised Federation of Bosnia and Herzegovina (FBiH) and Republica Srpska (RS), creating several new municipalities which together form the city of Istočno Sarajevo in the Republica Srpska: Istočna Ilidza, Istočno Novo Sarajevo, Istočni Stari Grad, Lukavica, Pale (RS-section), and Trnovo (RS-section), along with the municipality of Sokolac (which was not traditionally part of the Sarajevo area and was not partitioned)

The city has an urban area of . Veliki Park (Great park) is the largest green area in the center of Sarajevo. It's nestled between Titova, Koševo, Džidžikovac, Tina Ujevića and Trampina Streets and in the lower part there is a monument dedicated to the Children of Sarajevo.

Sarajevo has either a humid continental climate (Köppen climate classification: Dfb), or an oceanic climate (Köppen climate classification: Cfb), depending on if either the 0°C or the -3°C isotherms are used. Sarajevo's climate exhibits four seasons and uniformly spread precipitation, typical of both Cfb and Dfb climates. The proximity of the Adriatic Sea moderates Sarajevo's climate somewhat, although the mountains to the south of the city greatly reduce this maritime influence. The average yearly temperature is , with January ( on average) being the coldest month of the year and July ( on average) the warmest.

The highest recorded temperature was on 19 August 1946, and on 23 August 2008 (41.0) while the lowest recorded temperature was on 25 January 1942. On average, Sarajevo has 7 days where the temperature exceeds and 4 days where the temperature drops below per year. The city typically experiences mildly cloudy skies, with an average yearly cloud cover of 45%.

The cloudiest month is December (75% average cloud cover) while the clearest is August (37%). Moderate precipitation occurs fairly consistently throughout the year, with an average 75 days of rainfall. Suitable climatic conditions have allowed winter sports to flourish in the region, as exemplified by the Winter Olympics in 1984 that were celebrated in Sarajevo. Average winds are and the city has 1,769 hours of sunshine.

Air pollution is a major issue in Sarajevo. According to the 2016 World Health Organization's Ambient Air Pollution Database, the annual average PM2.5 concentration in 2010 was estimated to be 30 μg/m based on PM10 measurement, which is 3 times higher than recommended by WHO Air Quality Guidelines for annual average PM2.5. There are no recent direct long-term PM2.5 measurements available in Sarajevo and only estimates can be made from PM10, which is the less health-relevant than PM2.5. Real-time air quality data in the form of PM10, ozone, NO, CO and SO by the Federal Hydrometeorological Institute.

One of the earliest findings of settlement in the Sarajevo area is that of the Neolithic Butmir culture. The discoveries at Butmir were made on the grounds of the modern-day Sarajevo suburb Ilidža in 1893 by Austro-Hungarian authorities during the construction of an agricultural school. The area's richness in flint was attractive to Neolithic humans, and the settlement flourished. The settlement developed unique ceramics and pottery designs, which characterize the Butmir people as a unique culture, as described at the International Congress of Archaeologists and Anthropologists meeting in Sarajevo in 1894.

The next prominent culture in Sarajevo were the Illyrians. The ancient people, who considered most of the West Balkans as their homeland, had several key settlements in the region, mostly around the river Miljacka and the Sarajevo valley. The Illyrians in the Sarajevo region belonged to the "Daesitiates", the last Illyrian people in Bosnia and Herzegovina to resist Roman occupation. Their defeat by the Roman emperor Tiberius in 9 AD marks the start of Roman rule in the region. The Romans never built up the region of modern-day Bosnia, but the Roman colony of Aquae Sulphurae was near the top of present-day Ilidža, and was the most important settlement of the time. After the Romans, the Goths settled the area, followed by the Slavs in the 7th century.

During the Middle Ages Sarajevo was part of the Bosnian province of Vrhbosna near the traditional center of the Kingdom of Bosnia. Though a city named "Vrhbosna" existed, the exact settlement in Sarajevo at this time is debated. Various documents note a place called "Tornik" in the region, most likely in the area of Marijin Dvor neighborhood. By all indications, Tornik was a very small marketplace surrounded by a proportionally small village, and was not considered very important by Ragusan merchants.

Other scholars say that "Vrhbosna" was a major town in the wider area of modern-day Sarajevo. Papal documents say that in 1238, a cathedral dedicated to Saint Paul was built in the area. Disciples of the notable saints Cyril and Methodius stopped in the region, founding a church near Vrelo Bosna. Whether or not the town was somewhere in the area of modern-day Sarajevo, the documents attest to its and the region's importance. There was also a citadel Hodidjed north-east to Old City, dating from around 1263 until it was occupied by the Ottoman Empire in 1429.

Sarajevo was founded by the Ottoman Empire in the 1450s upon its conquest of the region, with 1461 used as the city's founding date. The first Ottoman governor of Bosnia, Isa-Beg Ishaković, transformed the cluster of villages into a city and state capital by building a number of key structures, including a mosque, a closed marketplace, a public bath, a hostel, and of course the governor's castle ("Saray") which gave the city its present name. The mosque was named "Careva Džamija" (the Tsar's Mosque) in honor of the Sultan Mehmed II. With the improvements Sarajevo quickly grew into the largest city in the region. By the 15th Century the settlement was established as a city, named "Bosna-Saraj", around the citadel in 1461.

Following the expulsion of Jews from Spain at the end of the 15th century, and the invitation from the Ottoman Empire to resettle their population, Sephardic Jews arrived in Sarajevo, which over time would become a leading center of Sephardic culture and the Ladino language. Though relatively small in size, a Jewish quarter would develop over several blocks in Baščaršija.

Many local Christians converted to Islam at this time. To accommodate the new pilgrims on the road to Mecca, in 1541 Gazi Husrev-Bey’s quartermaster Vekil-Harrach built a Pilgrim's mosque for which it is still known to this day Hadžijska mosque.

Under leaders such as the second governor Gazi Husrev-beg, Sarajevo grew at a rapid rate. Husrev-beg greatly shaped the physical city, as most of what is now the Old Town was built during his reign. Sarajevo became known for its large marketplace and numerous mosques, which by the middle of the 16th century numbered more than 100. At the peak of the empire, Sarajevo was the biggest and most important Ottoman city in the Balkans after Istanbul. By 1660, the population of Sarajevo was estimated to be over 80,000. By contrast, Belgrade in 1683 had 100.000, and Zagreb as late as 1851 had 14,000 people. As political conditions changed, Sarajevo became the site of warfare.

In 1697, during the Great Turkish War, a raid was led by Prince Eugene of Savoy of the Habsburg Monarchy against the Ottoman Empire, which conquered Sarajevo and left it plague-infected and burned to the ground. After his men had looted thoroughly, they set the city on fire and destroyed nearly all of it in one day. Only a handful of neighborhoods, some mosques, and an Orthodox church, were left standing. Numerous other fires weakened the city, which was later rebuilt but never fully recovered from the destruction. By 1807, it had only some 60,000 residents.

In the 1830s, several battles of the Bosnian uprising had taken place around the city. These had been led by Husein Gradaščević. Today, a major city street is named "Zmaj od Bosne" (Dragon of Bosnia) in his honor. The rebellion failed and for several more decades the Ottoman state remained in control of Bosnia.

The Ottoman Empire made Sarajevo an important administrative centre by 1850. Baščaršija became the central commercial district and cultural center of the city in the 15th century when Isa-Beg Isaković founded the town. The toponym Baščaršija derives from the Turkish language.

Austria-Hungary's occupation of Bosnia and Herzegovina came in 1878 as part of the Treaty of Berlin, and complete annexation followed in 1908, angering the Serbs. Sarajevo was industrialized by Austria-Hungary, who used the city as a testing area for new inventions such as tramways, which were established in 1885 before they were later installed in Vienna. Architects and engineers wanting to help rebuild Sarajevo as a modern European capital rushed to the city. A fire that burned down a large part of the central city area ("čaršija") left more room for redevelopment. As a result, the city has a unique blend of the remaining Ottoman city market and contemporary western architecture. Sarajevo also has some examples of Secession- and Pseudo-Moorish styles that date from this period.

The Austro-Hungarian period was one of great development for the city, as the Western power brought its new acquisition up to the standards of the Victorian age. Various factories and other buildings were built at this time, and a large number of institutions were both Westernized and modernized. For the first time in history, Sarajevo's population began writing in Latin script.
For the first time in centuries, the city significantly expanded outside its traditional borders. Much of the city's contemporary central municipality (Centar) was constructed during this period.

Architecture in Sarajevo quickly developed into a wide range of styles and buildings. The Cathedral of Sacred Heart, for example, was constructed using elements of neo-gothic and Romanesque architecture. The National Museum, Sarajevo brewery, and City Hall were also constructed during this period. Additionally, Austrian officials made Sarajevo the first city in this part of Europe to have a tramway.

Although the Bosnia Vilayet "de jure" remained part of the Ottoman Empire, it was "de facto" governed as an integral part of Austria-Hungary with the Ottomans having no say in its day-to-day governance. This lasted until 1908 when the territory was formally annexed and turned into a condominium, jointly controlled by both Austrian Cisleithania and Hungarian Transleithania.

In the event that triggered World War I, Archduke Franz Ferdinand of Austria was assassinated, along with his wife Sophie, Duchess of Hohenberg in Sarajevo on 28 June 1914 by Gavrilo Princip, an ethnic Serb and self-declared Yugoslav, and member of Young Bosnia. This was followed by the Anti-Serb riots in Sarajevo, which resulted in two deaths and destruction of property.

In the ensuing war, however, most of the Balkan offensives occurred near Belgrade, and Sarajevo largely escaped damage and destruction. Following the war, Bosnia was annexed into the Kingdom of Yugoslavia, and Sarajevo became the capital of the Drina Province.

After World War I and pressure from the Royal Serbian Army, alongside rebelling Slavic nations in Austria-Hungary, Sarajevo became part of the Kingdom of Yugoslavia. Though it held some political significance as the center of first the Bosnian region and then the Drinska Banovina, the city was no longer a national capital and saw a decline in global influence.

During World War II the Kingdom of Yugoslavia's army was overrun by German and Italian forces. Following a German bombing campaign, Sarajevo was captured on 15 April 1941 by the 16th Motorized infantry Division. The Axis powers created the Independent State of Croatia and included Sarajevo in its territory.

Immediately following the occupation, the main Sephardi Jewish synagogue, Il Kal Grande, was looted, burned, and destroyed by the Nazis. Within a matter of months, the centuries-old Sephardi and Ashkenazi Jewish communities of Sarajevo, comprising the vast majority of Bosnian Jewry, would be rounded up in the Old Synagogue (Stari hram) and deported to their deaths in Croatian concentration camps. Roughly 85% of Bosnia's Jewish population would perish at the hands of the Nazis and the Ustaše during the Holocaust. The Sarajevo Haggadah was the most important artifact which survived this period, smuggled out of Sarajevo and saved from the Nazis and Ustaše by the chief librarian of the National Museum, Derviš Korkut.

On 12 October 1941, a group of 108 notable Bosniak citizens of Sarajevo signed the Resolution of Sarajevo Muslims by which they condemned the persecution of Serbs organized by the Ustaše, made a distinction between the Bosniaks who participated in such persecutions and the rest of the Bosniak population, presented information about the persecutions of Bosniaks by Serbs, and requested security for all citizens of the country, regardless of their identity. By mid-summer 1942, around 20,000 Serbs found refuge in Sarajevo from Ustaše terror.

The city was bombed by the Allies from 1943 to 1944. The Yugoslav Partisan movement was represented in the city. In period February–May 1945 Maks Luburić set up Ustaše headquarters in a building known as Villa Luburić and used it as torture and execution place whose 323 victims were identified after the war. Resistance was led by Vladimir "Walter" Perić, who died while leading the liberation of the city on 6 April 1945.

After the war, Sarajevo was the capital of the Socialist Republic of Bosnia and Herzegovina within the Socialist Federal Republic of Yugoslavia. The Republic Government invested heavily in Sarajevo, building many new residential blocks in Novi Grad Municipality and Novo Sarajevo Municipality, while simultaneously developing the city's industry and transforming Sarajevo into a modern city. Sarajevo grew rapidly as it became an important regional industrial center in Yugoslavia. Between the end of the war and the end of Yugoslavia, the city grew from a population of 115,000 to more than 600,000 people. The Vraca Memorial Park, a monument for victims of World War II, was dedicated on 25 November, the "Day of Statehood of Bosnia and Herzegovina" when the ZAVNOBIH held their first meeting in 1943.

A crowning moment of Sarajevo's time in Socialist Yugoslavia was the 1984 Winter Olympics. Sarajevo beat out Sapporo, Japan; and Falun/Göteborg, Sweden to host the Olympic games. The games were followed by a tourism boom, making the 1980s one of the city's most prosperous decades.

The Bosnian War for independence resulted in large-scale destruction and dramatic population shifts during the Siege of Sarajevo between 1992 and 1996. Thousands of Sarajevans lost their lives under the constant bombardment and sniper shooting at civilians by the Serb forces during the siege, the longest siege of a capital city in the history of modern warfare. Bosnian Serb forces of the Republika Srpska and the Yugoslav People's Army besieged Sarajevo from 5 April 1992 to 29 February 1996.

When Bosnia and Herzegovina declared independence from Yugoslavia and achieved United Nations recognition, Serbian leaders declared a new Serbian national state Republika Srpska (RS) which was carved out from the territory of Bosnia and Herzegovina. The Army of Republika Srpska encircled Sarajevo with a siege force of 18,000 stationed in the surrounding hills, from which they assaulted the city with artillery, mortars, tanks, anti-aircraft guns, heavy machine-guns, multiple rocket launchers, rocket-launched aircraft bombs, and sniper rifles. From 2 May 1992, the Serbs blockaded the city. The Bosnian government defence forces inside the besieged city were poorly equipped and unable to break the siege.

During the siege, 11,541 people lost their lives, including over 1,500 children. An additional 56,000 people were wounded, including nearly 15,000 children. The 1991 census indicates that before the siege the city and its surrounding areas had a population of 525,980.

When the siege ended, the concrete scars caused by mortar shell explosions left marks that were filled with red resin. After the red resin was placed, it left floral patterns which led to them being dubbed Sarajevo Roses.

Various modern buildings now occupy Sarajevo's skyline, most significantly the Bosmal City Center, BBI Centar, Sarajevo City Center and the Avaz Twist Tower, which at the time of its building was the tallest skyscraper in former Yugoslavia.

Recent years have seen population growth as well as increases in tourism. In 2014 the city saw anti-government protests and riots and record rainfall that caused historic flooding.

Sarajevo is the capital of the country of Bosnia and Herzegovina and its sub-entity, the Federation of Bosnia and Herzegovina, as well as of the Sarajevo Canton. It is also the "de jure" capital of another entity, Republika Srpska. Each of these levels of government has its parliament or council, as well as judicial courts, in the city. All national institutions and foreign embassies are in Sarajevo.

Sarajevo is home to the Council of Ministers of Bosnia and Herzegovina, Parliamentary Assembly of Bosnia and Herzegovina, Presidency of Bosnia and Herzegovina, the Constitutional Court of Bosnia and Herzegovina and the operational command of the Armed Forces of Bosnia and Herzegovina.

Bosnia and Herzegovina's Parliament office in Sarajevo was damaged heavily in the Bosnian War. Due to damage the staff and documents were moved to a nearby ground level office to resume the work. In late 2006, reconstruction work started on the Parliament and was finished in 2007. The cost of reconstruction is supported 80% by the Greek Government through the Hellenic Program of Balkans Reconstruction (ESOAV) and 20% by Bosnia-Herzegovina.

The city comprises four municipalities Centar, Novi Grad, Novo Sarajevo, and Stari Grad. Each operate their own municipal government, united they form one city government with its own constitution. The executive branch () consists of a mayor, with two deputies and a cabinet. The legislative branch consists of the City Council, or "Gradsko Vijeće". The council has 28 members, including a council speaker, two deputies, and a secretary. Councilors are elected by the municipality in numbers roughly proportional to their population. The city government also has a judicial branch based on the post-transitional judicial system as outlined by the High Representative's "High Judicial and Prosecutorial Councils".

Sarajevo's Municipalities are further split into "local communities" (Bosnian, "Mjesne zajednice"). Local communities have a small role in city government and are intended as a way for ordinary citizens to get involved in city government. They are based on key neighborhoods in the city.

Sarajevo's large manufacturing, administrative, and tourism sectors make it the strongest economic region of Bosnia and Herzegovina. Indeed, Sarajevo Canton generates almost 25% of the country's GDP. After years of war, Sarajevo's economy saw reconstruction and rehabilitation programs. The Central Bank of Bosnia and Herzegovina opened in Sarajevo in 1997 and the Sarajevo Stock Exchange began trading in 2002.

While Sarajevo had a large industrial base during its communist period, only a few pre-existing businesses have successfully adapted to the market economy. Sarajevo industries now include tobacco products, furniture, hosiery, automobiles, and communication equipment. Companies based in Sarajevo include BH Telecom, Bosnalijek, Energopetrol, Sarajevo Tobacco Factory, and Sarajevska Pivara (Sarajevo Brewery).

In 2002 the total export for the greater Sarajevo region was worth about 259,569,000KM. Most of Sarajevo's exports (28.2%) head to Germany, with Great Britain following behind at 16.8% and Serbia and Montenegro third with 12.8%. The largest amount of imported goods come from Germany, at 15.8%. With a worth of total import at about 1,322,585,000KM, the total import is almost 5.1 times the total export.

In 1981 Sarajevo's GDP per capita was 133% of the Yugoslav average. Gross pay in Sarajevo in February 2015 was or , while net salary was or .

Sarajevo has a wide tourist industry and a fast expanding service sector thanks to the strong annual growth in tourist arrivals. Sarajevo also benefits from being both a summer and winter destination with continuity in its tourism throughout the year. The travel guide series, "Lonely Planet" named Sarajevo as the 43rd best city in the world, and in December 2009 listed Sarajevo as one of the top ten cities to visit in 2010.

In 2019 573.227 tourists visited Sarajevo, giving 1.060.988 overnight stays, which is 18,8% more than in 2017.

Sports-related tourism uses the legacy facilities of the 1984 Winter Olympics, especially the skiing facilities on the nearby mountains of Bjelašnica, Igman, Jahorina, Trebević, and Treskavica. Sarajevo's 600 years of history, influenced by both Western and Eastern empires, makes it a tourist attraction with splendid variations.
Sarajevo has hosted travellers for centuries, because it was an important trading center during the Ottoman and Austria-Hungarian empires and because was a natural stop for many routes between East and West. Examples of popular destinations in Sarajevo include the Vrelo Bosne park, the Sarajevo cathedral, and the Gazi Husrev-beg's Mosque. Tourism in Sarajevo is chiefly focused on historical, religious, cultural sites and winter sports.

Sarajevo is host to many parks throughout the city and on the outskirts of city. A popular activity among Sarajevo citizens is street chess, usually played at Trg oslobođenja Alija Izetbegović. Veliki Park is the largest green area in the center of Sarajevo. It's nestled between Titova, Koševo, Džidžikovac, Tina Ujevića and Trampina Streets and in the lower part there is a monument dedicated to the Children of Sarajevo. Hastahana is a popular place to relax in the Austro-Hungarian neighborhood of Marijin Dvor. Goat's Bridge, locally known as "Kozija Ćuprija", in the Miljacka Canyon is also a popular park destination along the Dariva walkway and river Miljacka. On December 24 of 2012, a park hosting two brass sculptures resembling two mourning mothers was dedicated as the Friendship Park, commemorating over 45 years of friendship between Sarajevo and Baku.

Sarajevo is also famous for its city lookouts; including an observation deck on Avaz Twist Tower, Park Prinčeva restaurant, Vidikovac lookout (Mt. Trebević), Zmajevac lookout and Yellow/White fortresses lookouts (in Vratnik) as well as numerous other rooftops throughout the city (i.e. Alta Shopping Center, BBI Center, Hotel Hecco Deluxe). A symbol of Sarajevo is the Trebevic cable car which was reconstructed in 2018, also it is one of the most popular tourist attractions in the city taking visitors from the city center to mount Trebevic. 

The last official Yugoslav census took place 1991 and recorded 527,049 people living in the city of Sarajevo (ten municipalities). In the settlement of Sarajevo proper, there were 416,497 inhabitants. The war displaced hundreds of thousands of people, a large majority of whom have not returned.

The first census since Bosnia and Herzegovina became an independent country was not taken until 2013 and as a result, for many years Sarajevo's population was not known clearly and statistics were based on estimates contributed by the United Nations Statistics Division and the Federal Office of Statistics of the Federation of Bosnia and Herzegovina, among other national and international non-profit organizations. , the population of the city's four municipalities was estimated to be 411,161, whereas the Sarajevo Canton population was estimated at 578,757. With an area of , Sarajevo has a population density of about . According to these estimates, the Novo Sarajevo municipality is the most densely populated part of Sarajevo with about , while the least densely populated is the Stari Grad, with .

In June 2016, the final results of the 2013 census were published. According to the census, the population of the Sarajevo Canton was 413,593, with 55,181 residents in Centar Sarajevo, 118,553 in Novi Grad, 64,814 in Novo Sarajevo and 36,976 in Stari Grad.

The war changed the ethnic and religious profile of the city. It had long been a multicultural city, and often went by the nickname of "Europe's Jerusalem". At the time of the 1991 census, 49.2 per cent of the city's population of 527,049 were Bosniaks, 29.8 percent Serbs, 10.7 percent Yugoslavs, 6.6 percent Croats and 3.6 percent other ethnicities (Jews, Romas, etc.). By 2002, 79.6 per cent of the canton's population of 401,118 were Bosniak, 11.2 percent Serb, a significantly smaller number of Serbs after the war as a result of the people being forced out, 6.7 percent Croat and 2.5 percent others (Jews, Romas, Arabs, etc.).

According to academic Fran Markowitz there is a number of "administrative apparatuses and public pressures that push people who might prefer to identify as flexible, multiply constituted hybrids or with one of the now unnamed minority groups into one of the three Bosniac-Croat-Serb constituent nations". These include respondents being encouraged by census interviewers to identity as belonging to one of the three constituent peoples. Her analysis of marriage registration data shows, for instance, that 67 per cent of people marrying in 2003 identified as Bosniak or Muslim, which is significantly lower than the 79.6 per cent census figure from 2002 (unlike the census, where people respond to an interviewer, applicants to the marriage registry fill in the form themselves).

Sarajevo's location in a valley between mountains makes it a compact city. Narrow city streets and a lack of parking areas restrict automobile traffic but allow better pedestrian and cyclist mobility. The two main roads are Titova Ulica (Street of Marshal Tito) and the east-west Zmaj od Bosne (Dragon of Bosnia) highway (E761).
Located roughly at the center of the country, Sarajevo is Bosnia's main intersection. The city is connected to all the other major cities by highway or national road like Zenica, Banja Luka, Tuzla, Mostar, Goražde and Foča.

Tourists from Central Europe and elsewhere visiting Dalmatia driving via Budapest through Sarajevo also contribute to the traffic congestion in and around Sarajevo.
The trans-European highway, Corridor 5C, runs through Sarajevo connecting it to Budapest in the north, and Ploče at the Adriatic sea in the south. The highway is built by the government and should cost 3.5 billion Euro. Up until March 2012, the Federation of Bosnia and Herzegovina invested around 600 million Euro in the A1. In 2014 the sections Sarajevo-Zenica and Sarajevo-Tarcin were completed including the Sarajevo Beltway ring road.

Sarajevo's electric tramways, in operation since 1885, are the oldest form of public transportation in the city.
Sarajevo had the first full-time (dawn to dusk) tram line in Europe, and the second in the world. Opened on New Year's Day in 1885, it was the testing line for the tram in Vienna and the Austro-Hungarian Empire, and operated by horses. Originally built to , the present system in 1960 was upgraded to . The trams played a pivotal role in the growth of the city in the 20th century.

There are seven tramway lines supplemented by five trolleybus lines and numerous bus routes. The main railroad station in Sarajevo is in the north-central area of the city. From there, the tracks head west before branching off in different directions, including to industrial zones in the city. Sarajevo is undergoing a major infrastructure renewal; many highways and streets are being repaved, the tram system is undergoing modernization, and new bridges and roads are under construction.

To solve traffic congestion in the city, Sarajevo-based architect Muzafer Osmanagić has proposed a study called "Eco Energy 2010–2015", idealizing a subway system underneath the bed of the river Miljacka. The first line of Metro Sarajevo would connect Baščaršija with Otoka. This line would cost some 150 million KM and be financed by the European Bank for Reconstruction and Development.

Trebević Cable Car, Sarajevo's key landmark during 1984 Winter Olympic Games, was rebuilt by JKP GRAS Sarajevo and Sarajevo Canton as one of the new transportation systems in 2017 and it reopened on 6 April 2018 at 11:00 AM. The cable car runs from Sarajevo at Bistrik station to the slopes of Trebević at Vidikovac station.

Sarajevo International Airport , also called Butmir, is just a few kilometers southwest of the city and was voted Best European Airport With Under 1,000,000 Passengers at the 15th Annual ACI-Europe in Munich in 2005.

First regular flights to Sarajevo using an airfield in the suburb of Butmir begin in 1930 when the domestic airliner Aeroput opened a regular route linking Belgrade to Podgorica through Sarajevo. Later, Aeroput opened routed which linked Sarajevo with Split, Rijeka and Dubrovnik, and in 1938 first international flights were introduced when Aeroput extended the route Dubrovnik – Sarajevo – Zagreb to Vienna, Brno and Prague. The airfield in Butmir remained in use all the way until 1969. The need for a new airport in Sarajevo, with an asphalt-concrete runway, was acknowledged in the mid-1960s when JAT, Yugoslav national carrier at that time, began acquiring jet planes. The construction of the airport began in 1966 at its present location, not far from the old one.

Sarajevo Airport opened on 2 June 1969 for domestic traffic. In 1970 Frankfurt became the first international destination served. Most of the time the airport was a 'feeder' airport where passengers embarked for flights to Zagreb and Belgrade on their way to international destinations. Over time the traffic volume steadily grew from 70,000 to 600,000 passengers a year. Later, during the Bosnian war, the airport was used for UN flights and humanitarian relief. Since the Dayton Accord in 1996, the airport retook its role as main air gate to Bosnia and Herzegovina.

In 2017, 957,971 passengers traveled through the airport, which was 61,4% of the total airport traffic in Bosnia-Herzegovina.

Plans for extension of the passenger terminal, together with upgrading and expanding the taxiway and apron, are planned to start in fall 2012. The existing terminal will be expanded by approximately . The upgraded airport will also be directly linked to the commercial retail center Sarajevo Airport Center, making it easier for tourists and travellers to spend their time before flight boarding shopping and enjoying the many amenities that will be offered.
Between 2015 and 2018 the airport will be upgraded for more than 25 million euros.

Sarajevo has daily international connections which twice a day connect the city with Zagreb and Ploče. There are also connections between Sarajevo and all major cities within Bosnia and Herzegovina. Once, the East Bosnian railway connected Sarajevo to Belgrade.
The Sarajevo railway station is among the biggest in Europe.

Sarajevo is twinned with:

Sarajevo is befriended with:

As the largest city of Bosnia and Herzegovina, Sarajevo is the main center of the country's media. Most of the communications and media infrastructure was destroyed during the war but reconstruction monitored by the Office of the High Representative has helped to modernize the industry as a whole. For example, internet was first made available to the city in 1995.

"Oslobođenje" (Liberation), founded in 1943, is Sarajevo's longest running continuously circulating newspaper and the only one to survive the war. However, this long running and trusted newspaper has fallen behind "Dnevni Avaz" (Daily Voice), founded in 1995, and "Jutarnje Novine" (Morning News) in circulation in Sarajevo. Other local periodicals include the Croatian newspaper Hrvatska riječ and the Bosnian magazine Start, as well as weekly newspapers "Slobodna Bosna" ("Free Bosnia") and "BH Dani" ("BH Days"). "Novi Plamen", a monthly magazine, is the most left-wing publication.

The Radiotelevision of Bosnia-Herzegovina is Sarajevo's public television station and was created in 1945 under the umbrella of the Yugoslav Radio Television. It had its first television program aired in 1961, while continuous programming started in 1969. It is one of three main TV stations in Bosnia and Herzegovina. Other stations based in the city include NRTV "Studio 99", NTV Hayat, TV 1, Open Broadcast Network, TV Kantona Sarajevo and Televizija Alfa.

The headquarters of Al Jazeera Balkans are also in Sarajevo, with a broadcasting studio at the top of the BBI Center. The news channel covers Bosnia and Herzegovina, Serbia, Croatia and Montenegro and the surrounding Balkan states.

Many small independent radio stations exist, including established stations such as Radio M, Radio Stari Grad (Radio Old Town), Studentski eFM Radio, Radio 202, Radio BIR, and RSG. Radio Free Europe, as well as several American and Western European stations are available.
Higher Education
Higher education has a long and rich tradition in Sarajevo. The first institution that can be classified as a tertiary educational institution was a school of Sufi philosophy established by Gazi Husrev-beg in 1537; numerous other religious schools have been established over time. In 1887, under the Austro-Hungarian Empire, a Sharia Law School began a five-year program. In the 1940s the University of Sarajevo became the city's first secular higher education institute, effectively building upon the foundations established by the Saraybosna Hanıka in 1537. In the 1950s, post-bachelor graduate degrees became available. Severely damaged during the war, it was recently rebuilt in partnership with more than 40 other universities.

There are also several universities in Sarajevo, including:

Primary and Secondary Education

, in Sarajevo there are 46 elementary schools (Grades 1–9) and 33 high schools (Grades 10–13), including three schools for children with special needs,

There are also several international schools in Sarajevo, catering to the expatriate community; some of which are Sarajevo International School and the French International School of Sarajevo, established in 1998.

Sarajevo has been home to many different religions for centuries, giving the city a range of diverse cultures. In the time of Ottoman occupation of Bosnia, Muslims, Serbian Orthodox, Roman Catholics, and Sephardi Jews all shared the city while maintaining distinctive identities. They were joined during the brief occupation by Austria-Hungary by a smaller number of Germans, Hungarians, Slovaks, Czechs and Ashkenazi Jews. By 1909, about 50% of the city's inhabitants were Muslim, 25% were Catholic, 15% were Orthodox, and 10% were Jewish.

Historically, Sarajevo has been home to several prominent Bosnian poets, scholars, philosophers and writers. To list only a very few; Nobel Prize-winner Vladimir Prelog is from the city, as are the writer Zlatko Topčić and the poet Abdulah Sidran. Nobel Prize-winner Ivo Andrić attended high school in Sarajevo for two years. Academy Award-winning director Danis Tanović live in the city.

The Sarajevo National Theatre is the oldest professional theater in Bosnia and Herzegovina, having been established in 1921.

The city is rich in museums, including the Museum of Sarajevo, the Ars Aevi Museum of Contemporary Art, Historical Museum of Bosnia and Herzegovina, The Museum of Literature and Theatre Arts of Bosnia and Herzegovina, and the National Museum of Bosnia and Herzegovina (established in 1888) home to the Sarajevo Haggadah, an illuminated manuscript and the oldest Sephardic Jewish document in the world issued in Barcelona around 1350, containing the traditional Jewish Haggadah, is on permanent display at the museum. It is the only remaining illustrated Sephardic Haggadah in the world. The National Museum also hosts year-round exhibitions pertaining to local, regional and international culture and history, and exhibits over 5,000 artefacts from Bosnia's history.

The Alija Izetbegović Museum was opened on 19 October 2007 and is in the old town fort, more specifically in the Vratnik Kapija towers Ploča and Širokac. The museum is a commemoration to the influence and body of work of Alija Izetbegović, the first president of the Republic of Bosnia and Herzegovina.

The city also hosts the Sarajevo National Theater, established in 1921, and the Sarajevo Youth Theatre. Some other cultural institutions include the Center for Sarajevo Culture, Sarajevo City Library, Art Gallery of Bosnia and Herzegovina, and the Bosniak Institute, a privately owned library and art collection focusing on Bosniak history.

Demolitions associated with the war, as well as reconstruction, destroyed several institutions and cultural or religious symbols including the Gazi Husrev-beg library, the national library, the Sarajevo Oriental Institute, and a museum dedicated to the 1984 Winter Olympics. Consequently, the different levels of government established strong cultural protection laws and institutions. Bodies charged with cultural preservation in Sarajevo include the Institute for the Protection of the Cultural, Historical and Natural Heritage of Bosnia and Herzegovina (and their Sarajevo Canton counterpart), and the Bosnia and Herzegovina Commission to Preserve National Monuments.

Sarajevo is and has historically been one of the most important musical enclaves in the region. The Sarajevo school of pop rock developed in the city between 1961 and 1991. This type of music began with bands like Indexi, Pro Arte, and singer-songwriter Kemal Monteno. It continued into the 1980s, with bands such as Plavi Orkestar, Crvena Jabuka, and Divlje Jagode, by most accounts, pioneering the regional rock and roll movement. Sarajevo was also the home and birthplace of arguably the most popular and influential Yugoslav rock band of all time, Bijelo Dugme, somewhat of a Bosnian parallel to the Rolling Stones, in both popularity and influence. 

Sarajevo was also the home of a very notable post-punk urban subculture known as the New Primitives, which began during the early 1980s with the Baglama Band which was banned shortly after first LP and was brought into the mainstream through bands such as Zabranjeno Pušenje and Elvis J. Kurtović & His Meteors, as well as the Top Lista Nadrealista radio, and later television show. Other notable bands considered to be part of this subculture are Bombaj Štampa. Besides and separately from the "New Primitives", Sarajevo is the hometown to one of the most significant ex-Yugoslavian alternative industrial-noise bands, SCH (1983–current).

Perhaps more importantly, Sarajevo in the late 19th and throughout the 20th century was home to a burgeoning and large center of Sevdalinka record-making and contributed greatly to bringing this historical genre of music to the mainstream, which had for many centuries been a staple of Bosnian culture. Songwriters and musicians such as Himzo Polovina, Safet Isović, Zaim Imamović, Zehra Deović, Halid Bešlić, Hanka Paldum, Nada Mamula, Meho Puzić and many more composed and wrote some of their most important pieces in the city.

Sarajevo also greatly influenced the pop scene of Yugoslavia with musicians like Zdravko Čolić, Kemal Monteno, Dino Merlin, Seid Memić Vajta, Hari Mata Hari, Mladen Vojičić "Tifa", Željko Bebek, and many more.

Many newer Sarajevo-based bands have also found a name and established themselves in Sarajevo, such as Regina who also had two albums out in Yugoslavia and Letu Štuke, who actually formed their band in Yugoslavia with the famous Bosnian-American writer Aleksandar Hemon and got their real breakthrough later in the 2000s. Sarajevo is now home to an important and eclectic mix of new bands and independent musicians, which continue to thrive with the ever-increasing number of festivals, creative showcases and concerts around the country. The city is also home to the region's largest jazz festival, the Sarajevo Jazz Festival (see "Festival" section below this).

American heavy metal band Savatage, released a song entitled "Christmas Eve (Sarajevo 12/24)" on their 1995 album Dead Winter Dead, which was about a cello player playing a forgotten Christmas carol in war-torn Sarajevo. The song was later re-released by the same band under the name Trans-Siberian Orchestra on their 1996 debut album Christmas Eve and Other Stories, which the song gave them instant success.

Sarajevo is internationally renowned for its eclectic and diverse selection of over 50 annual festivals. The Sarajevo Film Festival was established in 1995 during the Bosnian War and has become the premier and largest film festival in South-East Europe. It has been hosted at the National Theater, with screenings at the Open-air theater Metalac and the Bosnian Cultural Center, all in downtown Sarajevo. The MESS International Festival is an experimental theatre festival and the oldest living theatre festival in the Balkans. The annual Sarajevo Youth Film Festival showcases feature, animated and short films from around the world and is the premier student film festival in the Balkans. The Sarajevo Winter Festival, Sarajevo Jazz Festival and Sarajevo International Music Festival are well-known, as is the Baščaršija Nights festival, a month-long showcase of local culture, music, and dance.

The first incarnation of the Sarajevo Film Festival was hosted in still-warring Sarajevo in 1995, and has now progressed into being the biggest and most significant festival in south-eastern Europe. A talent campus is also held during the duration of the festival, with lecturers speaking on behalf of world cinematography and holding workshops for film students from across South-Eastern Europe.

The Sarajevo Jazz Festival is the region's largest and most diverse of its kind. The festival takes place at the Bosnian Cultural Center (aka "Main Stage"), just down the street from the SFF, at the Sarajevo Youth Stage Theater (aka "Strange Fruits Stage"), at the Dom Vojske Federacije (aka "Solo Stage"), and at the CDA (aka "Groove Stage").

The city hosted the 1984 Winter Olympics. Yugoslavia won one medal, a silver in men's giant slalom awarded to Jure Franko. Many of the Olympic facilities survived the war or were reconstructed, including Olympic Hall Zetra and Asim Ferhatović Stadion. In an attempt to bring back some of Sarajevo's Olympic glory, the original Olympic luge and bobsled tracks are being repaired, due to the efforts of both the Olympic Committee of Bosnia and Herzegovina and local sports enthusiasts. After co-hosting the Southeast Europe Friendship games, Sarajevo was awarded the 2009 Special Olympic winter games, but cancelled these plans. The ice arena for the 1984 Olympics, Zetra Stadium, was used during the war as a temporary hospital and, later, for housing NATO troops of the IFOR.

In 2011 Sarajevo was the host city of the 51st World Military Skiing Championship with over 350 participants from 23 different nations. This was the first international event of such standing since the 1984 Olympics.
Football (soccer) is popular in Sarajevo; the city hosts "FK Sarajevo" and "FK Željezničar", which both compete in European and international cups and tournaments and have a very large trophy cabinet in the former Yugoslavia as well as independent Bosnia and Herzegovina. Other notable soccer clubs are "FK Olimpik", "SAŠK" and "Slavija".

One of only two stadiums in Bosnia and Herzegovina that has the UEFA category 3 is the Stadion Grbavica, the home stadium of FK Željezničar.

Another popular sport is basketball; the basketball club KK Bosna Sarajevo won the European Championship in 1979 as well as many Yugoslav and Bosnian national championships making it one of the greatest basketball clubs in the former Yugoslavia. The chess club, "Bosna" Sarajevo, has been a championship team since the 1980s and is the third ranked chess club in Europe, having won four consecutive European championships in the nineties. RK Bosna also competes in the European Champions League and is considered one of the most well organised handball clubs in South-Eastern Europe with a very large fan base and excellent national, as well as international results.
Sarajevo often holds international events and competitions in sports such as tennis and kickboxing.

The popularity of tennis has been picking up in recent years. Since 2003, BH Telecom Indoors is an annual tennis tournament in Sarajevo.

Since 2007, the Sarajevo Marathon is being organized in late September. Giro di Sarajevo is also run in the city with over 2,200 cyclists taking part in 2015.

In February 2019, Sarajevo and East Sarajevo hosted the European Youth Olympic Winter Festival (EYOWF).




</doc>
<doc id="26787" url="https://en.wikipedia.org/wiki?curid=26787" title="Science fiction">
Science fiction

Science fiction (sometimes called sci-fi or simply SF) is a genre of speculative fiction that typically deals with imaginative and futuristic concepts such as advanced science and technology, space exploration, time travel, parallel universes, and extraterrestrial life. It has been called the "literature of ideas", and often explores the potential consequences of scientific, social, and technological innovations.

Science fiction, whose roots go back to ancient times, is related to fantasy, horror, and superhero fiction, and contains many subgenres. However its exact definition has long been disputed among authors, critics, and scholars.

Science fiction literature, film, television, and other media have become popular and influential over much of the world. Besides providing entertainment, it can also criticize present-day society, and is often said to inspire a "sense of wonder".

"Science fiction" is difficult to define precisely, as it includes a wide range of concepts and themes. James Blish wrote: "Wells used the term originally to cover what we would today call "hard" science fiction, in which a conscientious attempt to be faithful to already known facts (as of the date of writing) was the substrate on which the story was to be built, and if the story was also to contain a miracle, it ought at least not to contain a whole arsenal of them."

According to Isaac Asimov, "Science fiction can be defined as that branch of literature which deals with the reaction of human beings to changes in science and technology." Robert A. Heinlein wrote that "A handy short definition of almost all science fiction might read: realistic speculation about possible future events, based solidly on adequate knowledge of the real world, past and present, and on a thorough understanding of the nature and significance of the scientific method."

Tom Shippey compared George Orwell's "Coming Up for Air" (1939) with Frederick Pohl and C. M. Kornbluth's "The Space Merchants" (1952), and concluded that the basic building block and distinguishing feature of a science fiction novel is the presence of the "novum", a term Darko Suvin adapted from Ernst Bloch and defined as "a discrete piece of information recognizable as not-true, but also as not-unlike-true, not-flatly- (and in the current state of knowledge) impossible."

Lester del Rey wrote, "Even the devoted aficionado or fan—has a hard time trying to explain what science fiction is," and the lack of a "full satisfactory definition" is because "there are no easily delineated limits to science fiction." Author and editor Damon Knight summed up the difficulty, saying "science fiction is what we point to when we say it." Mark C. Glassey described science fiction as U.S. Supreme Court Justice Potter Stewart described pornography: "I know it when I see it."

Science fiction had its beginnings in ancient times, when the line between myth and fact was blurred. Written in the 2nd century CE by the satirist Lucian, "A True Story" contains many themes and tropes characteristic of modern science fiction, including travel to other worlds, extraterrestrial lifeforms, interplanetary warfare, and artificial life. Some consider it the first science-fiction novel. Some of the stories from "The Arabian Nights", along with the 10th-century "The Tale of the Bamboo Cutter" and Ibn al-Nafis's 13th-century "Theologus Autodidactus," also contain elements of science fiction.

Products of the Scientific Revolution and the Age of Enlightenment, Johannes Kepler's "Somnium" (1634), Francis Bacon's "New Atlantis" (1627), Cyrano de Bergerac's "Comical History of the States and Empires of the Moon" (1657) and "The States and Empires of the Sun" (1662), Margaret Cavendish's "The Blazing World" (1666), Jonathan Swift's "Gulliver's Travels" (1726), Ludvig Holberg's "Nicolai Klimii Iter Subterraneum" (1741) and Voltaire's "Micromégas" (1752) are regarded as some of the first true science-fantasy works. Isaac Asimov and Carl Sagan considered "Somnium" the first science-fiction story; it depicts a journey to the Moon and how the Earth's motion is seen from there.

Following the 18th-century development of the novel as a literary form, Mary Shelley's "Frankenstein" (1818) and "The Last Man" (1826) helped define the form of the science-fiction novel. Brian Aldiss has argued that "Frankenstein" was the first work of science fiction. Edgar Allan Poe wrote several stories considered to be science fiction, including "The Unparalleled Adventure of One Hans Pfaall" (1835) which featured a trip to the Moon. Jules Verne was noted for his attention to detail and scientific accuracy, especially in "Twenty Thousand Leagues Under the Sea" (1870). In 1887, the novel "El anacronópete" by Spanish author Enrique Gaspar y Rimbau introduced the first time machine.

Many critics consider H. G. Wells one of science fiction's most important authors, or even "the Shakespeare of science fiction." His notable science-fiction works include "The Time Machine" (1895), "The Island of Doctor Moreau" (1896), "The Invisible Man" (1897), and "The War of the Worlds" (1898). His science fiction imagined alien invasion, biological engineering, invisibility, and time travel. In his non-fiction futurologist works he predicted the advent of airplanes, military tanks, nuclear weapons, satellite television, space travel, and something resembling the World Wide Web.

Edgar Rice Burroughs' "A Princess of Mars", published in 1912, was the first of his three-decade-long planetary romance series of Barsoom novels which were set on Mars and featured John Carter as the hero.

In 1926, Hugo Gernsback published the first American science-fiction magazine, "Amazing Stories". In its first issue he wrote:

In 1928, E. E. "Doc" Smith's first published work, "The Skylark of Space," written in collaboration with Lee Hawkins Garby, appeared in "Amazing Stories". It is often called the first great space opera. The same year, Philip Francis Nowlan's original Buck Rogers story, "Armageddon 2419", also appeared in "Amazing Stories". This was followed by a Buck Rogers comic strip, the first serious science-fiction comic.

In 1937, John W. Campbell became editor of "Astounding Science Fiction", an event which is sometimes considered the beginning of the Golden Age of Science Fiction, which is characterized by stories celebrating scientific achievement and progress. In 1942, Isaac Asimov started his Foundation series, which chronicles the rise and fall of galactic empires and introduced psychohistory. The "Golden Age" is often said to have ended in 1946, but sometimes the late 1940s and the 1950s are included.

Theodore Sturgeon's "More Than Human" (1953) explored possible future human evolution. In 1957, "" by the Russian writer and paleontologist Ivan Yefremov presented a view of a future interstellar communist civilization and is considered one of the most important Soviet science fiction novels. In 1959, Robert A. Heinlein's "Starship Troopers" marked a departure from his earlier juvenile stories and novels. It is one of the first and most influential examples of military science fiction, and introduced the concept of powered armor exoskeletons. The German space opera series "Perry Rhodan", written by various authors, started in 1961 with an account of the first Moon landing and has since expanded in space to multiple universes, and in time by billions of years. It has become the most popular science fiction book series of all time.

In the 1960s and 1970s, New Wave science fiction was known for its embrace of a high degree of experimentation, both in form and in content, and a highbrow and self-consciously "literary" or "artistic" sensibility. In 1961, "Solaris" by Stanisław Lem was published in Poland. The novel dealt with the theme of human limitations as its characters attempted to study a seemingly intelligent ocean on a newly discovered planet. 1965's "Dune" by Frank Herbert featured a much more complex and detailed imagined future society than had previous science fiction.

In 1968, Philip K. Dick's "Do Androids Dream of Electric Sheep?," was published. It is the literary source of the "Blade Runner" movie franchise. 1969's "The Left Hand of Darkness" by Ursula K. Le Guin was set on a planet in which the inhabitants have no fixed gender. It is one of the most influential examples of social science fiction, feminist science fiction, and anthropological science fiction.

In 1976, C. J. Cherryh published "Gate of Ivrel" and "Brothers of Earth", which began her Alliance-Union universe future history series. In 1979, "Science Fiction World" began publication in the People's Republic of China. It dominates the Chinese science fiction magazine market, at one time claiming a circulation of 300,000 copies per issue and an estimated 3-5 readers per copy (giving it a total estimated readership of at least 1 million), making it the world's most popular science fiction periodical.

In 1984, William Gibson's first novel, "Neuromancer," helped popularize cyberpunk and the word "cyberspace," a term he originally coined in his 1982 short story "Burning Chrome". In 1986, "Shards of Honor" by Lois McMaster Bujold began her Vorkosigan Saga. 1992's "Snow Crash" by Neal Stephenson predicted immense social upheaval due to the information revolution. In 2007, Liu Cixin's novel, "The Three-Body Problem", was published in China. It was translated into English by Ken Liu and published by Tor Books in 2014, and won the 2015 Hugo Award for Best Novel, making Liu the first Asian writer to win the award.

Emerging themes in late 20th and early 21st century science fiction include environmental issues, the implications of the Internet and the expanding information universe, questions about biotechnology, nanotechnology, and post-scarcity societies. Recent trends and subgenres include steampunk, biopunk, and mundane science fiction.

The first, or at least one of the first, recorded science fiction film is 1902's "A Trip to the Moon", directed by French filmmaker Georges Méliès. It was profoundly influential on later filmmakers, bringing a different kind of creativity and fantasy to the cinematic medium. In addition, Méliès's innovative editing and special effects techniques were widely imitated and became important elements of the medium.

1927's "Metropolis", directed by Fritz Lang, is the first feature-length science fiction film. Though not well-received in its time, it is now considered a great and influential film. In 1954, "Godzilla", directed by Ishirō Honda, began the kaiju subgenre of science fiction film, which feature large creatures of any form, usually attacking a major city or engaging other monsters in battle.
1968's "", directed by Stanley Kubrick and based on the work of Arthur C. Clarke, rose above the mostly B-movie offerings up to that time both in scope and quality, and greatly influenced later science fiction films. That same year, "Planet of the Apes" (the original), directed by Franklin J. Schaffner and based on the 1963 French novel "La Planète des Singes" by Pierre Boulle, was released to popular and critical acclaim, due in large part to its vivid depiction of a post-apocalyptic world in which intelligent apes dominate humans.

In 1977, George Lucas began the "Star Wars" film series with the film now identified as ""Star Wars: Episode IV – A New Hope."" The series, often called a space opera, went on to become a worldwide popular culture phenomenon, and the second-highest-grossing film series of all time.

Since the 1980s, science fiction films, along with fantasy, horror, and superhero films, have dominated Hollywood's big-budget productions. Science fiction films often "cross-over" with other genres, including animation "(WALL-E" - 2008, "Big Hero 6" - 2014), gangster ("Sky Racket" - 1937), Western ("Serenity" - 2005), comedy ("Spaceballs" -1987, "Galaxy Quest" - 1999), war ("Enemy Mine" - 1985), action ("Edge of Tomorrow" - 2014, "The Matrix" - 1999), adventure ("Jupiter Ascending" - 2015, "Interstellar" - 2014), sports ("Rollerball" - 1975), mystery ("Minority Report" - 2002), thriller ("Ex Machina" - 2014), horror ("Alien" - 1979), film noir ("Blade Runner" - 1982), superhero ("Marvel Cinematic Universe" - 2008-), drama ("Arrival" - 2016, "A.I.: Artificial Intelligence" -2001), and romantic comedy ("Eternal Sunshine of the Spotless Mind" - 2004).

Science fiction and television have consistently been in a close relationship. Television or television-like technologies frequently appeared in science fiction long before television itself became widely available in the late 1940s and early 1950s.

The first known science fiction television program was a thirty-five-minute adapted excerpt of the play "RUR", written by the Czech playwright Karel Čapek, broadcast live from the BBC's Alexandra Palace studios on 11 February 1938. The first popular science fiction program on American television was the children's adventure serial "Captain Video and His Video Rangers", which ran from June 1949 to April 1955.

"The Twilight Zone" (the original series), produced and narrated by Rod Serling, who also wrote or co-wrote most of the episodes, ran from 1959 to 1964. It featured fantasy, suspense, and horror as well as science fiction, with each episode being a complete story. Critics have ranked it as one of the best TV programs of any genre.

The animated series "The Jetsons", while intended as comedy and only running for one season (1962–1963), predicted many inventions now in common use: flat-screen televisions, newspapers on a computer-like screen, computer viruses, video chat, tanning beds, home treadmills, and more. In 1963, the time travel-themed "Doctor Who" premiered on BBC Television. The original series ran until 1989 and was revived in 2005. It has been extremely popular worldwide and has greatly influenced later TV science fiction. Other programs in the 1960s included "The Outer Limits" (1963-1965), "Lost in Space" (1965-1968), and "The Prisoner" (1967).

"" (the original series), created by Gene Roddenberry, premiered in 1966 on NBC Television and ran for three seasons. It combined elements of space opera and Space Western. Only mildly successful at first, the series gained popularity through syndication and extraordinary fan interest. It became a very popular and influential franchise with many films and television shows, novels, and other works and products. "" (1987-1994) led to four additional "Star Trek" shows ("" (1993-1999), "" (1995-2001)"," "" (2001-2005), and "" (2017–present))--with more in some form of development.

The miniseries "V" premiered in 1983 on NBC. It depicted an attempted takeover of Earth by reptilian aliens. "Red Dwarf", a comic science fiction series aired on BBC Two between 1988 and 1999, and on Dave since 2009. "The X-Files", which featured UFOs and conspiracy theories, was created by Chris Carter and broadcast by Fox Broadcasting Company from 1993 to 2002, and again from 2016-2018. "Stargate", a film about ancient astronauts and interstellar teleportation, was released in 1994. "Stargate SG-1" premiered in 1997 and ran for 10 seasons (1997-2007). Spin-off series included "Stargate Infinity" (2002-2003), "Stargate Atlantis" (2004-2009), and "Stargate Universe" (2009-2011). Other 1990s series included "Quantum Leap" (1989-1993) and "Babylon 5" (1994-1999).

SyFy, launched in 1992 as The Sci-Fi Channel, specializes in science fiction, supernatural horror, and fantasy.

Science fiction's great rise in popularity during the first half of the 20th century was closely tied to the popular respect paid to science at that time, as well as the rapid pace of technological innovation and new inventions. Science fiction has often predicted scientific and technological progress. Some works predict that new inventions and progress will tend to improve life and society, for instance the stories of Arthur C. Clarke and "Star Trek". Others, such as H.G. Wells's "The Time Machine" and Aldous Huxley's "Brave New World", warn about possible negative consequences.

In 2001 the National Science Foundation conducted a survey on "Public Attitudes and Public Understanding: Science Fiction and Pseudoscience." It found that people who read or prefer science fiction may think about or relate to science differently than other people. They also tend to support the space program and the idea of contacting extraterrestrial civilizations. Carl Sagan wrote: "Many scientists deeply involved in the exploration of the solar system (myself among them) were first turned in that direction by science fiction."

Brian Aldiss described science fiction as "cultural wallpaper." Evidence for this widespread influence can be found in trends for writers to employ science fiction as a tool for advocacy and generating cultural insights, as well as for educators when teaching across a range of academic disciplines not limited to the natural sciences. Scholar and science fiction critic George Edgar Slusser said that science fiction "is the one real international literary form we have today, and as such has branched out to visual media, interactive media and on to whatever new media the world will invent in the 21st century. Crossover issues between the sciences and the humanities are crucial for the century to come."

Science fiction has sometimes been used as a means of social protest. George Orwell's "Nineteen Eighty-Four" (1949) is an important work of dystopian science fiction. It is often invoked in protests against governments and leaders who are seen as totalitarian. James Cameron's 2009 film "Avatar" was intended as a protest against imperialism, and specifically the European colonization of the Americas. Its images have been used by, among others, Palestinians in their protest against Israel.

Robots, artificial humans, human clones, intelligent computers, and their possible conflicts with human society have all been major themes of science fiction since, at least, the publication of Shelly's "Frankenstein". Some critics have seen this as reflecting authors’ concerns over the social alienation seen in modern society.

Feminist science fiction poses questions about social issues such as how society constructs gender roles, the role reproduction plays in defining gender, and the inequitable political or personal power of one gender over others. Some works have illustrated these themes using utopias to explore a society in which gender differences or gender power imbalances do not exist, or dystopias to explore worlds in which gender inequalities are intensified, thus asserting a need for feminist work to continue.

Climate fiction, or "cli-fi," deals with issues concerning climate change and global warming. University courses on literature and environmental issues may include climate change fiction in their syllabi, and it is often discussed by other media outside of science fiction fandom.

Libertarian science fiction focuses on the politics and social order implied by right libertarian philosophies with an emphasis on individualism and private property, and in some cases anti-statism.

Science fiction comedy often satirizes and criticizes present-day society, and sometimes makes fun of the conventions and clichés of more serious science fiction.

Science fiction is often said to inspire a "sense of wonder." Science fiction editor and critic David Hartwell wrote: "Science fiction’s appeal lies in combination of the rational, the believable, with the miraculous. It is an appeal to the sense of wonder." Carl Sagan said: "One of the great benefits of science fiction is that it can convey bits and pieces, hints and phrases, of knowledge unknown or inaccessible to the reader . . . works you ponder over as the water is running out of the bathtub or as you walk through the woods in an early winter snowfall."

In 1967, Isaac Asimov commented on the changes then occurring in the science fiction community: "And because today’s real life so resembles day-before-yesterday’s fantasy, the old-time fans are restless. Deep within, whether they admit it or not, is a feeling of disappointment and even outrage that the outer world has invaded their private domain. They feel the loss of a 'sense of wonder' because what was once truly confined to 'wonder' has now become prosaic and mundane."

The study of science fiction, or science fiction studies, is the critical assessment, interpretation, and discussion of science fiction literature, film, TV shows, new media, fandom, and fan fiction. Science fiction scholars study science fiction to better understand it and its relationship to science, technology, politics, other genres, and culture-at-large. Science fiction studies began around the turn of the 20th century, but it was not until later that science fiction studies solidified as a discipline with the publication of the academic journals "Extrapolation" (1959), "" (1972), and "Science Fiction Studies" (1973), and the establishment of the oldest organizations devoted to the study of science fiction in 1970, the Science Fiction Research Association and the Science Fiction Foundation. The field has grown considerably since the 1970s with the establishment of more journals, organizations, and conferences, as well as science fiction degree-granting programs such as those offered by the University of Liverpool and the University of Kansas.

Science fiction has historically been sub-divided between hard science fiction and soft science fiction–with the division centering on the feasibility of the science central to the story. However, this distinction has come under increasing scrutiny in the 21st century. Some authors, such as Tade Thompson and Jeff VanderMeer, have pointed out that stories that focus explicitly on physics, astronomy, mathematics, and engineering tend to be considered "hard" science fiction, while stories that focus on botany, mycology, zoology, and the social sciences tend to be categorized as "soft," regardless of the relative rigor of the science.

Max Gladstone defined "hard" science fiction as stories "where the math works," but pointed out that this ends up with stories that often seem "weirdly dated," as scientific paradigms shift over time. Michael Swanwick dismissed the traditional definition of "hard" SF altogether, instead saying that it was defined by characters striving to solve problems "in the right way–with determination, a touch of stoicism, and the consciousness that the universe is not on his or her side."

Ursula K. Le Guin also criticized the more traditional view on the difference between "hard" and "soft" SF: "The 'hard' science fiction writers dismiss everything except, well, physics, astronomy, and maybe chemistry. Biology, sociology, anthropology—that's not science to them, that's soft stuff. They're not that interested in what human beings do, really. But I am. I draw on the social sciences a great deal."

Respected authors of main-stream literature have written science fiction. Mary Shelley wrote a number of science fiction novels including "Frankenstein; or, The Modern Prometheus" (1818), and is considered a major writer of the Romantic Age. Aldous Huxley's "Brave New World" (1932) is often listed as one of England's most important novels, both for its criticism of modern culture and its prediction of future trends including reproductive technology and social engineering. Kurt Vonnegut was a highly respected American author whose works contain science fiction premises or themes. Other science fiction authors whose works are widely considered to be "serious" literature include Ray Bradbury (including, especially, "Fahrenheit 451" (1953) and "The Martian Chronicles" (1951)), Arthur C. Clarke (especially for "Childhood's End"), and Paul Myron Anthony Linebarger, writing under the name Cordwainer Smith.

David Barnett has pointed out that there are books such as "The Road" (2006) by Cormac McCarthy, "Cloud Atlas" (2004) by David Mitchell, "The Gone-Away World" (2008) by Nick Harkaway, "The Stone Gods" (2007) by Jeanette Winterson, and "Oryx and Crake" (2003) by Margaret Atwood, which use recognizable science fiction tropes, but whose authors and publishers do not market them as science fiction. Doris Lessing, who was later awarded the Nobel Prize in literature, wrote a series of five SF novels, "Canopus in Argos: Archives" (1979-1983), which depict the efforts of more advanced species and civilizations to influence those less advanced, including humans on Earth.

In her much reprinted essay "Science Fiction and Mrs Brown," Le Guin asked: "Can a science fiction writer write a novel?"; and answered: "I believe that all novels, . . . deal with character, and that it is to express character–not to preach doctrines, sing songs, or celebrate the glories of the British Empire, that the form of the novel, so clumsy, verbose, and undramatic, so rich, elastic, and alive, has been evolved. . . . The great novelists have brought us to see whatever they wish us to see through some character. Otherwise they would not be novelists, but poets, historians, or pamphleteers." Orson Scott Card, best known for his 1985 science fiction novel "Ender's Game", has postulated that in science fiction the message and intellectual significance of the work is contained within the story itself and, therefore, does not need stylistic gimmicks or literary games.

Jonathan Lethem, in a 1998 essay in the "Village Voice" entitled "Close Encounters: The Squandered Promise of Science Fiction," suggested that the point in 1973 when Thomas Pynchon's "Gravity's Rainbow" was nominated for the Nebula Award and was passed over in favor of Clarke's "Rendezvous with Rama," stands as "a hidden tombstone marking the death of the hope that SF was about to merge with the mainstream." In the same year science fiction author and physicist Gregory Benford wrote: "SF is perhaps the defining genre of the twentieth century, although its conquering armies are still camped outside the Rome of the literary citadels."

Science fiction is being written, and has been written, by diverse authors from around the world. According to 2013 statistics by the science fiction publisher Tor Books, men outnumber women by 78% to 22% among submissions to the publisher. A controversy about voting slates in the 2015 Hugo Awards highlighted tensions in the science fiction community between a trend of increasingly diverse works and authors being honored by awards, and reaction by groups of authors and fans who preferred what they considered more "traditional" science fiction.

Among the most respected and well-known awards for science fiction are the Hugo Award for literature, presented by the World Science Fiction Society at Worldcon, and voted on by fans; the Nebula Award for literature, presented by the Science Fiction and Fantasy Writers of America, and voted on by the community of authors; the John W. Campbell Memorial Award for Best Science Fiction Novel, presented by a jury of writers; and the Theodore Sturgeon Memorial Award for short fiction, presented by a jury. One notable award for science fiction films and TV programs is the Saturn Award, which is presented annually by The Academy of Science Fiction, Fantasy, and Horror Films.

There are other national awards, like Canada's Prix Aurora Awards, regional awards, like the Endeavour Award presented at Orycon for works from the U.S. Pacific Northwest, and special interest or subgenre awards such as the Chesley Award for art, presented by the Association of Science Fiction & Fantasy Artists, or the World Fantasy Award for fantasy. Magazines may organize reader polls, notably the Locus Award.

Conventions (in fandom, often shortened as "cons," such as "comic-con") are held in cities around the world, catering to a local, regional, national, or international membership. General-interest conventions cover all aspects of science fiction, while others focus on a particular interest like media fandom, filking, and so on. Most science fiction conventions are organized by volunteers in non-profit groups, though most media-oriented events are organized by commercial promoters. The convention's activities are called "the program", which may include panel discussions, readings, autograph sessions, costume masquerades, and other events. Additional activities occur throughout the convention that are not part of the program. These commonly include a dealer's room, art show, and hospitality lounge (or "con suites").

Conventions may host award ceremonies. For instance, Worldcon presents the Hugo Awards each year. SF societies, referred to as "clubs" except in formal contexts, form a year-round base of activities for science fiction fans. They may be associated with an ongoing science fiction convention, or have regular club meetings, or both. Long-established groups like the New England Science Fiction Association and the Los Angeles Science Fantasy Society have clubhouses for meetings and storage of convention supplies and research materials. The Science Fiction and Fantasy Writers of America (SFWA) was founded by Damon Knight in 1965 as a non-profit organization to serve the community of professional science fiction authors.

Science fiction fandom is the "community of the literature of ideas[,] . . . the culture in which new ideas emerge and grow before being released into society at large." Members of this community ("fans"), as discussed above, are often in contact with each other at conventions or clubs, through print or online fanzines, or on the Internet using websites, mailing lists, and other resources. SF fandom emerged from the letters column in "Amazing Stories" magazine: soon fans began writing letters to each other, and then grouping their comments together in informal publications that became known as fanzines. Once they were in regular contact, fans wanted to meet each other, and they organized local clubs. In the 1930s, the first science fiction conventions gathered fans from a wider area.

The earliest organized online fandom was the SF Lovers Community, originally a mailing list in the late 1970s with a text archive file that was updated regularly. In the 1980s, Usenet groups greatly expanded the circle of fans online. In the 1990s, the development of the World-Wide Web exploded the community of online fandom by orders of magnitude, with thousands and then millions of websites devoted to science fiction and related genres for all media. Most such sites are relatively small, ephemeral, and/or narrowly focused, though sites like SF Site and SFcrowsnest offer a broad range of references and reviews.

The first science fiction fanzine, "The Comet", was published in 1930 by the Science Correspondence Club in Chicago, Illionois. Fanzine printing methods have changed over the decades, from the hectograph, the mimeograph, and the ditto machine, to modern photocopying. Distribution volumes rarely justify the cost of commercial printing. Contemporary fanzines are largely printed on computer printers or at local copy shops, or they may only be sent as email (termed "Ezines") or otherwise made available online (termed "webzines"). One of the best known fanzines today is "Ansible", edited by David Langford, winner of numerous Hugo awards. Other notable fanzines to win one or more Hugo awards include "File 770", "Mimosa", and "Plokta". Artists working for fanzines have frequently risen to prominence in the field, including Brad W. Foster, Teddy Harvia, and Joe Mayhew; the Hugos include a category for Best Fan Artists.

Forrest J Ackerman is credited with first using the term "sci-fi" (analogous to the then-trendy "hi-fi") in 1954. As science fiction entered popular culture, writers and fans active in the field came to associate the term with low-budget, low-tech "B-movies," and with low-quality pulp science fiction. By the 1970s, critics within the field, such as Damon Knight and Terry Carr, were using "sci fi" to distinguish hack-work from serious science fiction. Peter Nicholls writes that "SF" (or "sf") is "the preferred abbreviation within the community of sf writers and readers." Robert Heinlein found even "science fiction" insufficient for certain types of works in this genre, and suggested the term speculative fiction to be used instead for those that are more "serious" or "thoughtful."

Science fiction elements can include, among others:



</doc>
<doc id="26788" url="https://en.wikipedia.org/wiki?curid=26788" title="Spirotrich">
Spirotrich

The spirotrichs are a large and diverse group of ciliate protozoa. They typically have prominent oral cilia in the form of a series of polykinetids, called the adoral zone of membranelles, beginning anterior to the oral cavity and running down to the left side of the mouth. There may also be one or two paroral membranes on its right side. The body cilia are fused to form polykinetids called cirri in some, and are sparse to absent in others.

Forms with cirri are common throughout soil, freshwater, and marine environments. Individuals tend to be flattened, with cirri confined to the ventral surface. These are variously used for crawling over objects, acting as feet, swimming, or assisting in food capture. They are generally divided into hypotrichs and stichotrichs, but were originally all considered hypotrichs.

Forms with sparse or absent body cilia tend to be smaller and are mostly marine, but a few are common in freshwater. Again, they are generally divided into oligotrichs and choreotrichs, but were originally all considered oligotrichs. The latter group includes the tintinnids, which produce loricae or shells and are the predominant fossil ciliates.

As first defined by Bütschli in 1889 the spirotrichs were one of two orders, together with the now-abandoned holotrichs, and included all ciliates with prominent oral cilia: heterotrichs, hypotrichs, oligotrichs, and peritrichs, although the last were soon separated. The heterotrichs have an adoral zone of membranelles, but molecular and ultrastructure studies have shown they are a separate group that diverged from most other ciliates early on. A few of the smaller groups included with them may be genuine spirotrichs, however, such as the Protocruziida.

The remaining spirotrichs form a monophyletic group, but their relationships are uncertain. For the most part the oligotrichs and choreotrichs appear to form closely related, natural groups. However "Halteria" and its close relatives, originally considered oligotrichs, form a separate group and may even be modified stichotrichs. Studies also suggest the hypotrichs are paraphyletic to the stichotrichs, and possibly to the oligotrichs and choreotrichs as well. This stands in contrast to the earlier belief that they were the most advanced of all protozoa.



</doc>
<doc id="26789" url="https://en.wikipedia.org/wiki?curid=26789" title="Sexual selection">
Sexual selection

Sexual selection is a mode of natural selection in which members of one biological sex choose mates of the other sex to mate with (intersexual selection), and compete with members of the same sex for access to members of the opposite sex (intrasexual selection). These two forms of selection mean that some individuals have better reproductive success than others within a population, either because they are more attractive or prefer more attractive partners to produce offspring. For instance, in the breeding season, sexual selection in frogs occurs with the males first gathering at the water's edge and making their mating calls: croaking. The females then arrive and choose the males with the deepest croaks and best territories. In general, males benefit from frequent mating and monopolizing access to a group of fertile females. Females can have a limited number of offspring and maximize the return on the energy they invest in reproduction.

The concept was first articulated by Charles Darwin and Alfred Russel Wallace who described it as driving species adaptations and that many organisms had evolved features whose function was deleterious to their individual survival, and then developed by Ronald Fisher in the early 20th century. Sexual selection can lead males to extreme efforts to demonstrate their fitness to be chosen by females, producing sexual dimorphism in secondary sexual characteristics, such as the ornate plumage of birds such as birds of paradise and peafowl, or the antlers of deer, or the manes of lions, caused by a positive feedback mechanism known as a Fisherian runaway, where the passing-on of the desire for a trait in one sex is as important as having the trait in the other sex in producing the runaway effect. Although the sexy son hypothesis indicates that females would prefer male offspring, Fisher's principle explains why the sex ratio is 1:1 almost without exception. Sexual selection is also found in plants and fungi.

The maintenance of sexual reproduction in a highly competitive world is one of the major puzzles in biology given that asexual reproduction can reproduce much more quickly as 50% of offspring are not males, unable to produce offspring themselves. Many non-exclusive hypotheses have been proposed, including the positive impact of an additional form of selection, sexual selection, on the probability of persistence of a species.

Sexual selection was first proposed by Charles Darwin in "The Origin of Species" (1859) and developed in "The Descent of Man and Selection in Relation to Sex" (1871), as he felt that natural selection alone was unable to account for certain types of non-survival adaptations. He once wrote to a colleague that "The sight of a feather in a peacock's tail, whenever I gaze at it, makes me sick!" His work divided sexual selection into male-male competition and female choice.

These views were to some extent opposed by Alfred Russel Wallace, mostly after Darwin's death. He accepted that sexual selection could occur, but argued that it was a relatively weak form of selection. He argued that male-male competitions were forms of natural selection, but that the "drab" peahen's coloration is itself adaptive as camouflage. In his opinion, ascribing mate choice to females was attributing the ability to judge standards of beauty to animals (such as beetles) far too cognitively undeveloped to be capable of aesthetic feeling.

Ronald Fisher, the English statistician and evolutionary biologist developed a number of ideas about sexual selection in his 1930 book "The Genetical Theory of Natural Selection" including the sexy son hypothesis and Fisher's principle. The Fisherian runaway describes how sexual selection accelerates the preference for a specific ornament, causing the preferred trait and female preference for it to increase together in a positive feedback runaway cycle. In a remark that was not widely understood for another 50 years he said:

This causes a dramatic increase in both the male's conspicuous feature and in female preference for it, resulting in marked sexual dimorphism, until practical physical constraints halt further exaggeration. A positive feedback loop is created, producing extravagant physical structures in the non-limiting sex. A classic example of female choice and potential runaway selection is the long-tailed widowbird. While males have long tails that are selected for by female choice, female tastes in tail length are still more extreme with females being attracted to tails longer than those that naturally occur. Fisher understood that female preference for long tails may be passed on genetically, in conjunction with genes for the long tail itself. Long-tailed widowbird offspring of both sexes inherit both sets of genes, with females expressing their genetic preference for long tails, and males showing off the coveted long tail itself.

Richard Dawkins presents a non-mathematical explanation of the runaway sexual selection process in his book "The Blind Watchmaker". Females that prefer long tailed males tend to have mothers that chose long-tailed fathers. As a result, they carry both sets of genes in their bodies. That is, genes for long tails and for preferring long tails become linked. The taste for long tails and tail length itself may therefore become correlated, tending to increase together. The more tails lengthen, the more long tails are desired. Any slight initial imbalance between taste and tails may set off an explosion in tail lengths. Fisher wrote that:

The female widowbird chooses to mate with the most attractive long-tailed male so that her progeny, if male, will themselves be attractive to females of the next generation—thereby fathering many offspring that carry the female's genes. Since the rate of change in preference is proportional to the average taste amongst females, and as females desire to secure the services of the most sexually attractive males, an additive effect is created that, if unchecked, can yield exponential increases in a given taste and in the corresponding desired sexual attribute.

Since Fisher's initial conceptual model of the 'runaway' process, Russell Lande and Peter O'Donald have provided detailed mathematical proofs that define the circumstances under which runaway sexual selection can take place.

The reproductive success of an organism is measured by the number of offspring left behind, and their quality or probable fitness.

Sexual preference creates a tendency towards assortative mating or homogamy. The general conditions of sexual discrimination appear to be (1) the acceptance of one mate precludes the effective acceptance of alternative mates, and (2) the rejection of an offer is followed by other offers, either certainly or at such high chance that the risk of non-occurrence is smaller than the chance advantage to be gained by selecting a mate. The conditions determining which sex becomes the more limited resource in intersexual selection have been hypothesized with Bateman's principle, which states that the sex which invests the most in producing offspring becomes a limiting resource for which the other sex competes, illustrated by the greater nutritional investment of an egg in a zygote, and the limited capacity of females to reproduce; for example, in humans, a woman can only give birth every ten months, whereas a male can become a father numerous times in the same period. More recently, researchers have doubted whether Bateman was correct. Hubbell and Johnson suggested that variance in reproductive success can be influenced by the time and allocations of mating. In 2005, Gowaty and Hubbell suggested that mating tendencies depend on the choice of strategy; in some cases, males can be more selective than females, whereas Bateman suggested that his paradigm would be "almost universal" among sexually reproducing species. Critics proposed that females might be more subject to sexual selection than males, but not in all circumstances.

Darwin's ideas on sexual selection were met with scepticism by his contemporaries and not considered of great importance until in the 1930s biologists decided to include sexual selection as a mode of natural selection. Only in the 21st century have they become more important in biology; the theory is now seen as generally applicable and analogous to natural selection.

A ten-year study, experimentally varying sexual selection on flour beetles with other factors held constant, showed that sexual selection protected even an inbred population against extinction.

The handicap principle of Amotz Zahavi, Russell Lande and W. D. Hamilton, holds that the fact that the male is able to survive until and through the age of reproduction with such a seemingly maladaptive trait is taken by the female to be a testament to his overall fitness. Such handicaps might prove he is either free of or resistant to disease, or that he possesses more speed or a greater physical strength that is used to combat the troubles brought on by the exaggerated trait. Zahavi's work spurred a re-examination of the field and several new theories. In 1984, Hamilton and Marlene Zuk introduced the "Bright Male" hypothesis, suggesting that male elaborations might serve as a marker of health, by exaggerating the effects of disease and deficiency. In 1990, Michael Ryan and A.S. Rand, working with the Túngara frog, proposed the hypothesis of "Sensory Exploitation", where exaggerated male traits may provide a sensory stimulation that females find hard to resist. Subsequently, the theories of the "Gravity Hypothesis" by Jordi Moya-Larano et al. (2002), invoking a simple biomechanical model to account for the adaptive value for smaller male spiders of speed in climbing vertical surfaces, and "Chase Away" by Brett Holland and William R. Rice have been added. In the late 1970s, Janzen and Mary Willson, noting that male flowers are often larger than female flowers, expanded the field of sexual selection into plants.

In the past few years, the field has exploded to include other areas of study, not all of which fit Darwin's definition of sexual selection. These include cuckoldry, nuptial gifts, sperm competition, infanticide (especially in primates), physical beauty, mating by subterfuge, species isolation mechanisms, male parental care, ambiparental care, mate location, polygamy, and homosexual rape in certain male animals.

Focusing on the effect of sexual conflict, as hypothesized by William Rice, Locke Rowe and Göran Arnvist, Thierry Lodé argues that divergence of interest constitutes a key for evolutionary process. Sexual conflict leads to an antagonistic co-evolution in which one sex tends to control the other, resulting in a tug of war. Besides, "the sexual propaganda theory" only argued that mates were opportunistically led, on the basis of various factors determining the choice such as phenotypic characteristics, apparent vigour of individuals, strength of mate signals, trophic resources, territoriality, etc., but and could explain the maintenance of genetic diversity within populations.

Several workers have brought attention to the fact that elaborated characters that ought to be costly in one way or another for their bearers (e.g., the tail of the swordfish "Xiphophorus montezumae") do not always appear to have a cost in terms of energetics, performance or even survival. One possible explanation for the apparent lack of costs is that "compensatory traits" have evolved in concert with the sexually selected traits.<ref name="Oufiero 2015">


</doc>
<doc id="26790" url="https://en.wikipedia.org/wiki?curid=26790" title="Stanisław Lem">
Stanisław Lem

Stanisław Herman Lem (; 12 or 13 September 1921 – 27 March 2006) was a Polish writer of science fiction, philosophy, and satire, and a trained physician. Lem's books have been translated into 41 languages and have sold over 45 million copies. From the 1950s to 2000s, he published many books, both science fiction and philosophical/futurological. He is best known as the author of the 1961 novel "Solaris", which has been made into a feature film three times. In 1976, Theodore Sturgeon wrote that Lem was the most widely read science fiction writer in the world. The total print of Lem's books is over 30 million copies.

Lem's works explore philosophical themes through speculation on technology, the nature of intelligence, the impossibility of communication with and understanding of alien intelligence, despair about human limitations, and humanity's place in the universe. They are sometimes presented as fiction, but others are in the form of essays or philosophical books.

Translating his works is difficult due to passages with elaborate word formation, idiomatic wordplay, alien or robotic poetry, and puns.

Lem was born in 1921 in Lwów, interwar Poland (now Lviv, Ukraine) to a family of Jewish origin. According to his own account, he was actually born on the 13th of September, but the date was changed to the 12th on his birth certificate because of superstition. He was the son of Sabina Woller (1892–1979) and Samuel Lem (1879–1954), a wealthy laryngologist and former physician in the Austro-Hungarian Army, and first cousin to Polish poet Marian Hemar (Lem's father's sister's son). In later years Lem sometimes claimed to have been raised Roman Catholic, but he went to Jewish religious lessons during his school years. He later became an atheist "for moral reasons ... the world appears to me to be put together in such a painful way that I prefer to believe that it was not created ... intentionally". In later years he would call himself both an agnostic and an atheist.

After the Soviet invasion and occupation of Eastern Poland, he was not allowed to study at Lwow Polytechnic as he wished because of his "bourgeois origin", and only due to his father's connections was accepted to study medicine at Lwów University in 1940. During the subsequent Nazi occupation (1941–1944), Lem's family, which had Jewish roots, avoided imprisonment in a ghetto, surviving with false papers. He would later recall:
During that time, Lem earned a living as a car mechanic and welder, and occasionally stole munitions from storehouses (to which he had access as an employee of a German company) to pass them on to the Polish resistance.

In 1945, the Polish Eastern Borderlands were annexed into Soviet Ukraine, and the family, along with many other Poles, was resettled to Kraków, where Lem, at his father's insistence, took up medical studies at the Jagiellonian University. He did not take his final examinations on purpose, to avoid the career of military doctor, which he suspected could have become lifelong. After receiving "absolutorium" (Polish term for the evidence of completion of the studies without diploma), he did an obligatory monthly work at a hospital, at a maternity ward, where he assisted at a number of childbirths and a caesarean section. Lem said that the sight of blood was one of the reasons he decided to drop medicine.

Lem made his literary debut in 1946 with a number of works of different genres, including poetry as well as a science fiction novel, "The Man from Mars" ("Człowiek z Marsa"), serialized in "" ("New World of Adventures"). Between 1948 and 1950 Lem was working as a scientific research assistant at the Jagiellonian University, and published a number of short stories, poems, reviews and similar works, particularly at "Tygodnik Powszechny". In 1951, he published his first book, "The Astronauts" ("Astronauci"). In 1953 he met and married (civil marriage) Barbara Leśniak, a medical student. 
Their church marriage ceremony was performed in February, 1954. In 1954, he published a short story anthology, "Sesame and Other Stories" (""). The following year, 1955, saw the publication of another science fiction novel, "The Magellanic Cloud" ("Obłok Magellana").

During the era of Stalinism, which had begun in Poland in the late 1940s, all published works had to be directly approved by the communist regime. Thus "Astronauci" was not, in fact, the first novel Lem finished, just the first that made it past the censors. Going by the date of the finished manuscript, Lem's first book was a partly autobiographical novella "Hospital of the Transfiguration" ("Szpital Przemienienia"), finished in 1948. It would be published seven years later, in 1955, as a trilogy under the title "Czas nieutracony" ("Time Not Lost"). The experience of trying to push "Czas nieutracony" through the censors was one of the major reasons Lem decided to focus on the less-censored genre of science fiction. Nonetheless, most of Lem's works published in the 1950s also contain—forced upon him by the censors and editors—various references to socialist realism as well as the "glorious future of communism". Lem later criticized several of his early pieces as compromised by the ideological pressure.

Lem became truly productive after 1956, when the de-Stalinization period in the Soviet Union led to the "Polish October", when Poland experienced an increase in freedom of speech. Between 1956 and 1968, Lem authored seventeen books. His writing over the next three decades or so was split between science fiction (primarily prose) and essays about science and culture.

In 1957, he published his first non-fiction, philosophical book, "Dialogues" (), as well as a science fiction anthology, "The Star Diaries" ("Dzienniki gwiazdowe"), collecting short stories about one of his most popular characters, Ijon Tichy. 1959 saw the publication of three books: "Eden", "Śledztwo" and the short story anthology "Inwazja z Aldebarana". 1961 saw two more books, the first regarded as being among his top works: "Pamiętnik znaleziony w wannie", "Solaris", as well as "Powrót z gwiazd". This was followed by a collection of his essays and non-fiction prose, "Wejście na orbitę" (1962), and a short story anthology "Noc księżycowa" (1963). In 1964, Lem published a large work on the border of philosophy and sociology of science and futurology, "Summa Technologiae", as well as a novel, "The Invincible" ("Niezwyciężony").

1965 saw the publication of "The Cyberiad" ("Cyberiada") and of a short story anthology, "The Hunt" (). 1966 is the year of "Wysoki Zamek", followed in 1968 by "Głos Pana" and "Tales of Pirx the Pilot" ("Opowieści o pilocie Pirxie"). "Wysoki Zamek" was another of Lem's autobiographical works, and touched upon a theme that usually was not favored by the censors: Lem's youth in the pre-war, then-Polish, Lviv. 1967 and 1970 saw two more non-fiction treatises, "Filozofia przypadku" and "Fantastyka i futurologia". Ijon Tichy returned in 1971's "The Futurological Congress" "Kongres futurologiczny"; in the same year Lem released a genre-mixing experiment, "Doskonała próżnia", a collection of reviews of non-existent books. In 1973 a similar work, "Wielkość urojona", was published. In 1976, Lem published two novels: "Maska" and "Katar". In 1980, he published another set of reviews of non-existent works, "Prowokacja". The following year sees another Tichy novel, "Wizja lokalna", and "Golem XIV". Later in that decade, Lem published "Pokój na Ziemi" (1984) and "Fiasko" (1986), his final science fiction novel.

In the late 1970s and early 1980s, Lem cautiously supported the Polish dissident movement, and started publishing essays in Paris-based "Kultura". In 1982, with martial law in Poland declared, Lem moved to West Berlin, where he became a fellow of the Institute for Advanced Study, Berlin ("Wissenschaftskolleg zu Berlin"). After that, he settled in Vienna. He returned to Poland in 1988.

From the late 1980s onwards, he tended to concentrate on philosophical texts and essays, published in a number of Polish magazines ("Tygodnik Powszechny", "Odra", "Przegląd", and others). They were later collected in a number of anthologies.

In early 1980s literary critic and historian Stanisław Bereś conducted a lengthy interview with Lem, which got published in book format in 1987 as "Rozmowy ze Stanisławem Lemem" ("Conversations with Stanisław Lem)". That edition was subject to censorship. A revised, complete edition was published in 2002 as "Tako rzecze… Lem" ("Thus spoke... Lem").

In the early 1990s, Lem met with the literary scholar and critic Peter Swirski for a series of extensive interviews, published together with other critical materials and translations as "A Stanislaw Lem Reader" (1997); in the book, Lem speaks about a range of issues rarely touched on before in any interview. Moreover, the book includes Swirski's translation of Lem's retrospective essay "Thirty Years Later", devoted to Lem's nonfictional treatise "Summa Technologiae". During later interviews in 2005, Lem expressed his disappointment with the genre of science fiction, and his general pessimism regarding technical progress. He viewed the human body as unsuitable for space travel, held that information technology drowns people in a glut of low-quality information, and considered truly intelligent robots as both undesirable and impossible to construct. Subsequently, Peter Swirski has published a series of in-depth studies of Lem as a writer, philosopher, and futurologist; notable among them are the recent "From Literature to Biterature: Lem, Turing, Darwin" (2013), "Stanislaw Lem: Selected Letters to Michael Kandel" (2014), "Lemography" (2014), and "Stanislaw Lem: Philosopher of the Future" (2015).

Lem was a polyglot: he knew Polish, Latin (from medical school), German, French, English, Russian and Ukrainian.

Lem was married to Barbara Lem née Krymska until his death. She died on 27 April 2016. Their only son, Tomasz, was born in 1968. He studied physics and mathematics at the University of Vienna, and graduated with a degree in physics from Princeton University. Tomasz wrote a memoir about his father, "Awantury na tle powszechnego ciążenia" ("Tantrums on the Background of the Universal Gravitation"), which contain numerous personal details about Stanisław Lem. The annotation of the book says Tomasz works as a translator and has a daughter, Anna.

Stanisław Lem died from heart disease in Kraków on 27 March 2006 at the age of 84.

Lem was awarded an honorary membership in the Science Fiction Writers of America (SFWA) in 1973. SFWA Honorary membership is given to people who do not meet the publishing criteria for joining the regular membership, but who would be welcomed as members had their work appeared in the qualifying English-language publications. Lem never had a high opinion of American science fiction, describing it as ill-thought-out, poorly written, and interested more in making money than in ideas or new literary forms. After his eventual American publication, when he became eligible for regular membership, his honorary membership was rescinded. This formal action was interpreted by some of the SFWA members as a rebuke for his stance, and it seems that Lem interpreted it as such. Lem was invited to stay on with the organization with a regular membership, but declined. After many members (including Ursula K. Le Guin, who quit her membership and then refused the Nebula Award for Best Novelette for "The Diary of the Rose") protested against Lem's treatment by the SFWA, a member offered to pay his dues. Lem never accepted the offer.

Lem singled out only one American science fiction writer for praise, Philip K. Dick, in a 1984 English-language anthology of his critical essays, "". Lem had initially held a low opinion of Philip K. Dick (as he did for the bulk of American science fiction) and would later claim that this was due to a limited familiarity with Dick's work.

Dick, who had mental health problems, maintained that Stanisław Lem was probably a false name used by a composite committee operating on orders of the Communist party to gain control over public opinion, and wrote a letter to the FBI to that effect. Lem was also responsible for the Polish translation of Dick's work "Ubik" in 1972, and when Dick felt monetarily short-changed by the publisher, he held Lem personally responsible (see "").

Lem is one of the most highly acclaimed science fiction writers, hailed by critics as equal to such classic authors as H. G. Wells and Olaf Stapledon. In 1976, Theodore Sturgeon wrote that Lem was the most widely read science fiction writer in the world.

In Poland, in the 1960s and 1970s, Lem remained under the radar of mainstream critics, who dismissed him as a "mass market", low-brow, youth-oriented writer; such dismissal might have given him a form of invisibility from censorship.

His works were widely translated abroad, appearing in over 40 languages, though the bulk of them were in Eastern Bloc countries (Poland, Germany, Hungary, former Czechoslovakia and the former Soviet Union). Franz Rottensteiner, Lem's former agent abroad, had this to say about Lem's reception on international markets:
His best-known novels include "Solaris" (1961), "His Master's Voice" ("Głos pana", 1968), and the late "Fiasco" ("Fiasko", 1987). "Solaris" was made into a film in 1968 by Russian director Boris Nirenburg, a film in 1972 by Russian director Andrei Tarkovsky—which won a Special Jury Prize at the Cannes Film Festival in 1972—and an American film in 2002 by Steven Soderbergh.

"Solaris" is not the only work of Lem's to be filmed. Over ten film and television adaptations of his work exist, such as adaptations of "The Astronauts" ("First Spaceship on Venus", 1960) and "The Magellan Nebula" ("Ikarie XB-1", 1963). Lem himself was, however, critical of most of the screen adaptations, with the sole exception of "Przekładaniec" in 1968 by Andrzej Wajda. More recently, in 2013, the Israeli–Polish co-production "The Congress" was released, inspired by Lem's novel "The Futurological Congress".

Lem's works have been used in education, for example as teaching texts for philosophy students.

In 1981, the philosophers Douglas R. Hofstadter and Daniel C. Dennett included three extracts from Lem's fiction in their annotated anthology "The Mind's I", accompanied by Hofstadter's comment, which says in part that Lem's "literary and intuitive approach ... does a better job of convincing readers of his views than any hard-nosed scientific article ... might do".

Other influences exerted by Lem's works include Will Wright's popular city-planning game "SimCity", which was partly inspired by Lem's short story "The Seventh Sally".

A major character in the film "Planet 51", an alien Lem, was named by screenwriter Joe Stillman after Stanisław Lem. Since the film was intended to be a parody of American pulp science fiction shot in Eastern Europe, Stillman thought that it would be hilarious to hint at the writer whose works have nothing to do with little green men.

Stanisław Lem works were influenced by such masters of Polish literature as Cyprian Norwid and Stanisław Ignacy Witkiewicz. His prose show a mastery of numerous genres and themes.

One of Lem's major recurring themes, beginning from his very first novel, "The Man from Mars", was the impossibility of communication between profoundly alien beings, which may have no common ground with human intelligence, and humans. The best known example is the living planetary ocean in Lem's novel "Solaris". Other examples include swarms of mechanical insects (in "The Invincible"), and strangely ordered societies of more human-like beings in "Fiasco" and "Eden", describing the failure of the first contact. In "His Master's Voice", Lem describes the failure of humanity's intelligence to decipher and truly comprehend an apparent message from space.

Two overlapping arcs of short stories, "Fables for Robots" ("Bajki Robotów"), translated in the collection "Mortal Engines"), and "The Cyberiad" ("Cyberiada") provide a commentary on humanity in the form of a series of grotesque, humorous, fairytale-like short stories about a mechanical universe inhabited by robots (who have occasional contact with biological "slimies" and human "palefaces"). 

"Śledztwo" and "Katar" are crime novels (the latter without a murderer); "Pamiętnik..." is a psychological drama inspired by Kafka. "Doskonała próżnia" and "Wielkość urojona" are collections of reviews of non-existent books and introductions to them. Similarly, "Prowokacja" purports to review a Holocaust-themed work.

Lem's criticism of most science fiction surfaced in literary and philosophical essays "Science Fiction and Futurology" and interviews. In the 1990s, Lem forswore science fiction and returned to futurological prognostications, most notably those expressed in "Blink of an Eye" (""). He became increasingly critical of modern technology in his later life, criticizing inventions such as the Internet.

"Dialogi" and "Summa Technologiae" (1964) are Lem's two most famous philosophical texts. The "Summa" is notable for being a unique analysis of prospective social, cybernetic, and biological advances; in this work, Lem discusses philosophical implications of technologies that were completely in the realm of science fiction at the time, but are gaining importance today—for instance, virtual reality and nanotechnology.








</doc>
<doc id="26791" url="https://en.wikipedia.org/wiki?curid=26791" title="Satire">
Satire

In fiction and less frequently in non-fiction, satire is a genre of literature and performing arts, in which vices, follies, abuses and shortcomings are held up to ridicule, ideally with the intent of shaming individuals, corporations, government, or society itself into improvement. Although satire is usually meant to be humorous, its greater purpose is often constructive social criticism, using wit to draw attention to both particular and wider issues in society.

A feature of satire is strong irony or sarcasm—"in satire, irony is militant"—but parody, burlesque, exaggeration, juxtaposition, comparison, analogy, and double entendre are all frequently used in satirical speech and writing. This "militant" irony or sarcasm often professes to approve of (or at least accept as natural) the very things the satirist wishes to question.

Satire is nowadays found in many artistic forms of expression, including internet memes, literature, plays, commentary, television shows, and media such as lyrics.

The word satire comes from the Latin word "satur" and the subsequent phrase "lanx satura." "Satur" meant "full" but the juxtaposition with "lanx" shifted the meaning to "miscellany or medley": the expression "lanx satura" literally means "a full dish of various kinds of fruits".

The word "satura" as used by Quintilian, however, was used to denote only Roman verse satire, a strict genre that imposed hexameter form, a narrower genre than what would be later intended as "satire". Quintilian famously said that "satura," that is a satire in hexameter verses, was a literary genre of wholly Roman origin ("satura tota nostra est"). He was aware of and commented on Greek satire, but at the time did not label it as such, although today the origin of satire is considered to be Aristophanes' Old Comedy. The first critic to use the term "satire" in the modern broader sense was Apuleius.

To Quintilian, the satire was a strict literary form, but the term soon escaped from the original narrow definition. Robert Elliott writes:
The word "satire" derives from "satura", and its origin was not influenced by the Greek mythological figure of the "satyr". In the 17th century, philologist Isaac Casaubon was the first to dispute the etymology of satire from satyr, contrary to the belief up to that time.

Laughter is not an essential component of satire; in fact there are types of satire that are not meant to be "funny" at all. Conversely, not all humour, even on such topics as politics, religion or art is necessarily "satirical", even when it uses the satirical tools of irony, parody, and burlesque.

Even light-hearted satire has a serious "after-taste": the organizers of the Ig Nobel Prize describe this as "first make people laugh, and then make them think".

Satire and irony in some cases have been regarded as the most effective source to understand a society, the oldest form of social study. They provide the keenest insights into a group's collective psyche, reveal its deepest values and tastes, and the society's structures of power. Some authors have regarded satire as superior to non-comic and non-artistic disciplines like history or anthropology. In a prominent example from ancient Greece, philosopher Plato, when asked by a friend for a book to understand Athenian society, referred him to the plays of Aristophanes.

Historically, satire has satisfied the popular need to debunk and ridicule the leading figures in politics, economy, religion and other prominent realms of power. Satire confronts public discourse and the collective imaginary, playing as a public opinion counterweight to power (be it political, economic, religious, symbolic, or otherwise), by challenging leaders and authorities. For instance, it forces administrations to clarify, amend or establish their policies. Satire's job is to expose problems and contradictions, and it's not obligated to solve them. Karl Kraus set in the history of satire a prominent example of a satirist role as confronting public discourse.

For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. The satiric impulse, and its ritualized expressions, carry out the function of resolving social tension. Institutions like the ritual clowns, by giving expression to the antisocial tendencies, represent a safety valve which re-establishes equilibrium and health in the collective imaginary, which are jeopardized by the repressive aspects of society.

The state of political satire in a given society reflects the tolerance or intolerance that characterizes it, and the state of civil liberties and human rights. Under totalitarian regimes any criticism of a political system, and especially satire, is suppressed. A typical example is the Soviet Union where the dissidents, such as Aleksandr Solzhenitsyn and Andrei Sakharov were under strong pressure from the government. While satire of everyday life in the USSR was allowed, the most prominent satirist being Arkady Raikin, political satire existed in the form of anecdotes that made fun of Soviet political leaders, especially Brezhnev, famous for his narrow-mindedness and love for awards and decorations.

Satire is a diverse genre which is complex to classify and define, with a wide range of satiric "modes".

Satirical literature can commonly be categorized as either Horatian, Juvenalian, or Menippean.

Horatian satire, named for the Roman satirist Horace (65–8 BCE), playfully criticizes some social vice through gentle, mild, and light-hearted humour. Horace (Quintus Horatius Flaccus) wrote Satires to gently ridicule the dominant opinions and "philosophical beliefs of ancient Rome and Greece" (Rankin). Rather than writing in harsh or accusing tones, he addressed issues with humor and clever mockery. Horatian satire follows this same pattern of "gently [ridiculing] the absurdities and follies of human beings" (Drury).

It directs wit, exaggeration, and self-deprecating humour toward what it identifies as folly, rather than evil. Horatian satire's sympathetic tone is common in modern society.

A Horatian satirist's goal is to heal the situation with smiles, rather than by anger. Horatian satire is a gentle reminder to take life less seriously and evokes a wry smile. A Horatian satirist makes fun of general human folly rather than engaging in specific or personal attacks. Shamekia Thomas suggests, "In a work using Horatian satire, readers often laugh at the characters in the story who are the subject of mockery as well as themselves and society for behaving in those ways." Alexander Pope has been established as an author whose satire "heals with morals what it hurts with wit" (Green). Alexander Pope—and Horatian satire—attempt to teach.

Examples of Horatian satire:


Juvenalian satire, named for the writings of the Roman satirist Juvenal (late first century – early second century AD), is more contemptuous and abrasive than the Horatian. Juvenal disagreed with the opinions of the public figures and institutions of the Republic and actively attacked them through his literature. "He utilized the satirical tools of exaggeration and parody to make his targets appear monstrous and incompetent" (Podzemny). Juvenal's satire follows this same pattern of abrasively ridiculing societal structures. Juvenal also, unlike Horace, attacked public officials and governmental organizations through his satires, regarding their opinions as not just wrong, but evil.

Following in this tradition, Juvenalian satire addresses perceived social evil through scorn, outrage, and savage ridicule. This form is often pessimistic, characterized by the use of irony, sarcasm, moral indignation and personal invective, with less emphasis on humor. Strongly polarized political satire can often be classified as Juvenalian.

A Juvenal satirist's goal is generally to provoke some sort of political or societal change because he sees his opponent or object as evil or harmful. A Juvenal satirist mocks "societal structure, power, and civilization" (Thomas) by exaggerating the words or position of his opponent in order to jeopardize their opponent's reputation and/or power. Jonathan Swift has been established as an author who "borrowed heavily from Juvenal's techniques in [his critique] of contemporary English society" (Podzemny).

Examples of Juvenalian satire:

See Menippean satire.

In the history of theatre there has always been a conflict between engagement and disengagement on politics and relevant issue, between satire and grotesque on one side, and jest with teasing on the other. Max Eastman defined the spectrum of satire in terms of "degrees of biting", as ranging from satire proper at the hot-end, and "kidding" at the violet-end; Eastman adopted the term kidding to denote what is just satirical in form, but is not really firing at the target. Nobel laureate satirical playwright Dario Fo pointed out the difference between satire and teasing ("sfottò"). Teasing is the reactionary side of the comic; it limits itself to a shallow parody of physical appearance. The side-effect of teasing is that it humanizes and draws sympathy for the powerful individual towards which it is directed. Satire instead uses the comic to go against power and its oppressions, has a subversive character, and a moral dimension which draws judgement against its targets. Fo formulated an operational criterion to tell real satire from "sfottò", saying that real satire arouses an outraged and violent reaction, and that the more they try to stop you, the better is the job you are doing. Fo contends that, historically, people in positions of power have welcomed and encouraged good-humoured buffoonery, while modern day people in positions of power have tried to censor, ostracize and repress satire.

Teasing ("sfottò") is an ancient form of simple buffoonery, a form of comedy without satire's subversive edge. Teasing includes light and affectionate parody, good-humoured mockery, simple one-dimensional poking fun, and benign spoofs. Teasing typically consists of an impersonation of someone monkeying around with his exterior attributes, tics, physical blemishes, voice and mannerisms, quirks, way of dressing and walking, and/or the phrases he typically repeats. By contrast, teasing never touches on the core issue, never makes a serious criticism judging the target with irony; it never harms the target's conduct, ideology and position of power; it never undermines the perception of his morality and cultural dimension. "Sfottò" directed towards a powerful individual makes him appear more human and draws sympathy towards him. Hermann Göring propagated jests and jokes against himself, with the aim of humanizing his image.

Types of satire can also be classified according to the topics it deals with. From the earliest times, at least since the plays of Aristophanes, the primary topics of literary satire have been politics, religion and sex. This is partly because these are the most pressing problems that affect anybody living in a society, and partly because these topics are usually taboo. Among these, politics in the broader sense is considered the pre-eminent topic of satire. Satire which targets the clergy is a type of political satire, while religious satire is that which targets religious beliefs. Satire on sex may overlap with blue comedy, off-color humor and dick jokes.

Scatology has a long literary association with satire, as it is a classical mode of the grotesque, the grotesque body and the satiric grotesque. Shit plays a fundamental role in satire because it symbolizes death, the turd being "the ultimate dead object". The satirical comparison of individuals or institutions with human excrement, exposes their "inherent inertness, corruption and dead-likeness". The ritual clowns of clown societies, like among the Pueblo Indians, have ceremonies with filth-eating. In other cultures, sin-eating is an apotropaic rite in which the sin-eater (also called filth-eater), by ingesting the food provided, takes "upon himself the sins of the departed". Satire about death overlaps with black humor and gallows humor.

Another classification by topics is the distinction between political satire, religious satire and satire of manners. Political satire is sometimes called topical satire, satire of manners is sometimes called satire of everyday life, and religious satire is sometimes called philosophical satire. Comedy of manners, sometimes also called satire of manners, criticizes mode of life of common people; political satire aims at behavior, manners of politicians, and vices of political systems. Historically, comedy of manners, which first appeared in British theater in 1620, has uncritically accepted the social code of the upper classes. Comedy in general accepts the rules of the social game, while satire subverts them.

Another analysis of satire is the spectrum of his possible tones: wit, ridicule, irony, sarcasm, cynicism, the sardonic and invective.

Satire is found not only in written literary forms. In preliterate cultures it manifests itself in ritual and folk forms, as well as in trickster tales and oral poetry.

It appears also in graphic arts, music, sculpture, dance, cartoon strips, and graffiti. Examples are Dada sculptures, Pop Art works, music of Gilbert and Sullivan and Erik Satie, punk and rock music. In modern media culture, stand-up comedy is an enclave in which satire can be introduced into mass media, challenging mainstream discourse. Comedy roasts, mock festivals, and stand-up comedians in nightclubs and concerts are the modern forms of ancient satiric rituals.

One of the earliest examples of what we might call satire, The Satire of the Trades, is in Egyptian writing from the beginning of the 2nd millennium BC. The text's apparent readers are students, tired of studying. It argues that their lot as scribes is not only useful, but far superior to that of the ordinary man. Scholars such as Helck think that the context was meant to be serious.

The Papyrus Anastasi I (late 2nd millennium BC) contains a satirical letter which first praises the virtues of its recipient, but then mocks the reader's meagre knowledge and achievements.

The Greeks had no word for what later would be called "satire", although the terms cynicism and parody were used. Modern critics call the Greek playwright Aristophanes one of the best known early satirists: his plays are known for their critical political and societal commentary, particularly for the political satire by which he criticized the powerful Cleon (as in "The Knights"). He is also notable for the persecution he underwent. Aristophanes' plays turned upon images of filth and disease. His bawdy style was adopted by Greek dramatist-comedian Menander. His early play "Drunkenness" contains an attack on the politician Callimedon.

The oldest form of satire still in use is the Menippean satire by Menippus of Gadara. His own writings are lost. Examples from his admirers and imitators mix seriousness and mockery in dialogues and present parodies before a background of diatribe. As in the case of Aristophanes plays, menippean satire turned upon images of filth and disease.

The first Roman to discuss satire critically was Quintilian, who invented the term to describe the writings of Gaius Lucilius. The two most prominent and influential ancient Roman satirists are Horace and Juvenal, who wrote during the early days of the Roman Empire. Other important satirists in ancient Latin are Gaius Lucilius and Persius. "Satire" in their work is much wider than in the modern sense of the word, including fantastic and highly coloured humorous writing with little or no real mocking intent. When Horace criticized Augustus, he used veiled ironic terms. In contrast, Pliny reports that the 6th-century-BC poet Hipponax wrote "satirae" that were so cruel that the offended hanged themselves.

In the 2nd century AD, Lucian wrote "True History", a book satirizing the clearly unrealistic travelogues/adventures written by Ctesias, Iambulus, and Homer. He states that he was surprised they expected people to believe their lies, and stating that he, like they, has no actual knowledge or experience, but shall now tell lies as if he did. He goes on to describe a far more obviously extreme and unrealistic tale, involving interplanetary exploration, war among alien life forms, and life inside a 200 mile long whale back in the terrestrial ocean, all intended to make obvious the fallacies of books like "Indica" and "The Odyssey".

Medieval Arabic poetry included the satiric genre "hija". Satire was introduced into Arabic prose literature by the Afro-Arab author Al-Jahiz in the 9th century. While dealing with serious topics in what are now known as anthropology, sociology and psychology, he introduced a satirical approach, "based on the premise that, however serious the subject under review, it could be made more interesting and thus achieve greater effect, if only one leavened the lump of solemnity by the insertion of a few amusing anecdotes or by the throwing out of some witty or paradoxical observations. He was well aware that, in treating of new themes in his prose works, he would have to employ a vocabulary of a nature more familiar in "hija", satirical poetry." For example, in one of his zoological works, he satirized the preference for longer human penis size, writing: "If the length of the penis were a sign of honor, then the mule would belong to the (honorable tribe of) Quraysh". Another satirical story based on this preference was an "Arabian Nights" tale called "Ali with the Large Member".

In the 10th century, the writer Tha'alibi recorded satirical poetry written by the Arabic poets As-Salami and Abu Dulaf, with As-Salami praising Abu Dulaf's wide breadth of knowledge and then mocking his ability in all these subjects, and with Abu Dulaf responding back and satirizing As-Salami in return. An example of Arabic political satire included another 10th-century poet Jarir satirizing Farazdaq as "a transgressor of the Sharia" and later Arabic poets in turn using the term "Farazdaq-like" as a form of political satire.

The terms "comedy" and "satire" became synonymous after Aristotle's "Poetics" was translated into Arabic in the medieval Islamic world, where it was elaborated upon by Islamic philosophers and writers, such as Abu Bischr, his pupil Al-Farabi, Avicenna, and Averroes. Due to cultural differences, they disassociated comedy from Greek dramatic representation and instead identified it with Arabic poetic themes and forms, such as "hija" (satirical poetry). They viewed comedy as simply the "art of reprehension", and made no reference to light and cheerful events, or troubled beginnings and happy endings, associated with classical Greek comedy. After the Latin translations of the 12th century, the term "comedy" thus gained a new semantic meaning in Medieval literature.

Ubayd Zakani introduced satire in Persian literature during the 14th century. His work is noted for its satire and obscene verses, often political or bawdy, and often cited in debates involving homosexual practices. He wrote the "Resaleh-ye Delgosha", as well as "Akhlaq al-Ashraf" ("Ethics of the Aristocracy") and the famous humorous fable "Masnavi Mush-O-Gorbeh" (Mouse and Cat), which was a political satire. His non-satirical serious classical verses have also been regarded as very well written, in league with the other great works of Persian literature. Between 1905 and 1911, Bibi Khatoon Astarabadi and other Iranian writers wrote notable satires.

In the Early Middle Ages, examples of satire were the songs by Goliards or vagants now best known as an anthology called Carmina Burana and made famous as texts of a composition by the 20th-century composer Carl Orff. Satirical poetry is believed to have been popular, although little has survived. With the advent of the High Middle Ages and the birth of modern vernacular literature in the 12th century, it began to be used again, most notably by Chaucer. The disrespectful manner was considered "unchristian" and ignored, except for the moral satire, which mocked misbehaviour in Christian terms. Examples are "Livre des Manières" by (~1178), and some of Chaucer's "Canterbury Tales". Sometimes epic poetry (epos) was mocked, and even feudal society, but there was hardly a general interest in the genre.

Direct social commentary via satire returned with a vengeance in the 16th century, when farcical texts such as the works of François Rabelais tackled more serious issues (and incurred the wrath of the crown as a result).

Two major satirists of Europe in the Renaissance were Giovanni Boccaccio and François Rabelais. Other examples of Renaissance satire include "Till Eulenspiegel", "Reynard the Fox", Sebastian Brant's "Narrenschiff" (1494), Erasmus's "Moriae Encomium" (1509), Thomas More's "Utopia" (1516), and "Carajicomedia" (1519).

The Elizabethan (i.e. 16th-century English) writers thought of satire as related to the notoriously rude, coarse and sharp satyr play. Elizabethan "satire" (typically in pamphlet form) therefore contains more straightforward abuse than subtle irony. The French Huguenot Isaac Casaubon pointed out in 1605 that satire in the Roman fashion was something altogether more civilised. Casaubon discovered and published Quintilian's writing and presented the original meaning of the term (satira, not satyr), and the sense of wittiness (reflecting the "dishfull of fruits") became more important again. Seventeenth-century English satire once again aimed at the "amendment of vices" (Dryden).

In the 1590s a new wave of verse satire broke with the publication of Hall's "Virgidemiarum", six books of verse satires targeting everything from literary fads to corrupt noblemen. Although Donne had already circulated satires in manuscript, Hall's was the first real attempt in English at verse satire on the Juvenalian model. The success of his work combined with a national mood of disillusion in the last years of Elizabeth's reign triggered an avalanche of satire—much of it less conscious of classical models than Hall's — until the fashion was brought to an abrupt stop by censorship.

Satire ("Kataksh" or "Vyang") has played a prominent role in Indian and Hindi literature, and is counted as one of the "ras" of literature in ancient books. With the commencement of printing of books in local language in the nineteenth century and especially after India's freedom, this grew. Many of the works of Tulsi Das, Kabir, Munshi Premchand, village ministrels, Hari katha singers, poets, Dalit singers and current day stand up Indian comedians incorporate satire, usually ridiculing authoritarians, fundamentalists and incompetent people in power. In India, it has usually been used as a means of expression and an outlet for common people to express their anger against authoritarian entities. A popular custom in Northern India of "Bura na mano Holi hai" continues, in which comedians on the stage roast local people of importance (who are usually brought in as special guests).

The Age of Enlightenment, an intellectual movement in the 17th and 18th centuries advocating rationality, produced a great revival of satire in Britain. This was fuelled by the rise of partisan politics, with the formalisation of the Tory and Whig parties—and also, in 1714, by the formation of the Scriblerus Club, which included Alexander Pope, Jonathan Swift, John Gay, John Arbuthnot, Robert Harley, Thomas Parnell, and Henry St John, 1st Viscount Bolingbroke. This club included several of the notable satirists of early-18th-century Britain. They focused their attention on Martinus Scriblerus, "an invented learned fool... whose work they attributed all that was tedious, narrow-minded, and pedantic in contemporary scholarship". In their hands astute and biting satire of institutions and individuals became a popular weapon. The turn to the 18th century was characterized by a switch from Horatian, soft, pseudo-satire, to biting "juvenal" satire.

Jonathan Swift was one of the greatest of Anglo-Irish satirists, and one of the first to practise modern journalistic satire. For instance, In his "A Modest Proposal" Swift suggests that Irish peasants be encouraged to sell their own children as food for the rich, as a solution to the "problem" of poverty. His purpose is of course to attack indifference to the plight of the desperately poor. In his book "Gulliver's Travels" he writes about the flaws in human society in general and English society in particular. John Dryden wrote an influential essay entitled "A Discourse Concerning the Original and Progress of Satire" that helped fix the definition of satire in the literary world. His satirical "Mac Flecknoe" was written in response to a rivalry with Thomas Shadwell and eventually inspired Alexander Pope to write his satirical "The Rape of the Lock". Other satirical works by Pope include the "Epistle to Dr Arbuthnot".

Alexander Pope (b. May 21, 1688) was a satirist known for his Horatian satirist style and translation of the "Iliad". Famous throughout and after the long 18th century, Pope died in 1744. Pope, in his "The Rape of the Lock", is delicately chiding society in a sly but polished voice by holding up a mirror to the follies and vanities of the upper class. Pope does not actively attack the self-important pomp of the British aristocracy, but rather presents it in such a way that gives the reader a new perspective from which to easily view the actions in the story as foolish and ridiculous. A mockery of the upper class, more delicate and lyrical than brutal, Pope nonetheless is able to effectively illuminate the moral degradation of society to the public. "The Rape of the Lock" assimilates the masterful qualities of a heroic epic, such as the "Iliad", which Pope was translating at the time of writing "The Rape of the Lock". However, Pope applied these qualities satirically to a seemingly petty egotistical elitist quarrel to prove his point wryly.

Daniel Defoe pursued a more journalistic type of satire, being famous for his "The True-Born Englishman" which mocks xenophobic patriotism, and "The Shortest-Way with the Dissenters"—advocating religious toleration by means of an ironical exaggeration of the highly intolerant attitudes of his time.

The pictorial satire of William Hogarth is a precursor to the development of political cartoons in 18th-century England. The medium developed under the direction of its greatest exponent, James Gillray from London. With his satirical works calling the king (George III), prime ministers and generals (especially Napoleon) to account, Gillray's wit and keen sense of the ridiculous made him the pre-eminent cartoonist of the era.

Ebenezer Cooke (1665–1732), author of "The Sot-Weed Factor" (1708), was among the first American colonialists to write literary satire. Benjamin Franklin (1706–1790) and others followed, using satire to shape an emerging nation's culture through its sense of the ridiculous.

Several satiric papers competed for the public's attention in the Victorian era (1837–1901) and Edwardian period, such as "Punch" (1841) and "Fun" (1861).

Perhaps the most enduring examples of Victorian satire, however, are to be found in the Savoy Operas of Gilbert and Sullivan. In fact, in "The Yeomen of the Guard", a jester is given lines that paint a very neat picture of the method and purpose of the satirist, and might almost be taken as a statement of Gilbert's own intent:

Novelists such as Charles Dickens (1812-1870) often used passages of satiric writing in their treatment of social issues.

Continuing the tradition of Swiftian journalistic satire, Sidney Godolphin Osborne (1808-1889) was the most prominent writer of scathing "Letters to the Editor" of the London Times. Famous in his day, he is now all but forgotten. His maternal grandfather William Eden, 1st Baron Auckland was considered to be a possible candidate for the authorship of the Junius letters. If this were true, we can read Osborne as following in his grandfather's satiric "Letters to the Editor" path. Osborne's satire was so bitter and biting that at one point he received a public censure from Parliament's then Home Secretary Sir James Graham. Osborne wrote mostly in the Juvenalian mode over a wide range of topics mostly centered on British government's and landlords' mistreatment of poor farm workers and field laborers. He bitterly opposed the New Poor Laws and was passionate on the subject of Great Britain's botched response to the Irish Famine and its mistreatment of soldiers during the Crimean War.

Later in the nineteenth century, in the United States, Mark Twain (1835–1910) grew to become American's greatest satirist: his novel "Huckleberry Finn" (1884) is set in the antebellum South, where the moral values Twain wishes to promote are completely turned on their heads. His hero, Huck, is a rather simple but goodhearted lad who is ashamed of the "sinful temptation" that leads him to help a runaway slave. In fact his conscience, warped by the distorted moral world he has grown up in, often bothers him most when he is at his best. He is prepared to do good, believing it to be wrong.

Twain's younger contemporary Ambrose Bierce (1842–1913) gained notoriety as a cynic, pessimist and black humorist with his dark, bitterly ironic stories, many set during the American Civil War, which satirized the limitations of human perception and reason. Bierce's most famous work of satire is probably "The Devil's Dictionary" (1906), in which the definitions mock cant, hypocrisy and received wisdom.

Karl Kraus is considered the first major European satirist since Jonathan Swift. In 20th-century literature, satire was used by English authors such as Aldous Huxley (1930s) and George Orwell (1940s), which under the inspiration of Zamyatin's Russian 1921 novel "We", made serious and even frightening commentaries on the dangers of the sweeping social changes taking place throughout Europe. Anatoly Lunacharsky wrote ‘Satire attains its greatest significance when a newly evolving class creates an ideology considerably more advanced than that of the ruling class, but has not yet developed to the point where it can conquer it. Herein lies its truly great ability to triumph, its scorn for its adversary and its hidden fear of it. Herein lies its venom, its amazing energy of hate, and quite frequently, its grief, like a black frame around glittering images. Herein lie its contradictions, and its power.’ Many social critics of this same time in the United States, such as Dorothy Parker and H. L. Mencken, used satire as their main weapon, and Mencken in particular is noted for having said that "one horse-laugh is worth ten thousand syllogisms" in the persuasion of the public to accept a criticism. Novelist Sinclair Lewis was known for his satirical stories such as "Main Street" (1920), "Babbitt" (1922), "Elmer Gantry" (1927; dedicated by Lewis to H. L. Menchen), and "It Can't Happen Here" (1935), and his books often explored and satirized contemporary American values. The film "The Great Dictator" (1940) by Charlie Chaplin is itself a parody of Adolf Hitler; Chaplin later declared that he would have not made the film if he had known about the concentration camps.
In the United States 1950s, satire was introduced into American stand-up comedy most prominently by Lenny Bruce and Mort Sahl. As they challenged the taboos and conventional wisdom of the time, were ostracized by the mass media establishment as "sick comedians". In the same period, Paul Krassner's magazine "The Realist" began publication, to become immensely popular during the 1960s and early 1970s among people in the counterculture; it had articles and cartoons that were savage, biting satires of politicians such as Lyndon Johnson and Richard Nixon, the Vietnam War, the Cold War and the War on Drugs. This baton was also carried by the original National Lampoon magazine, edited by Doug Kenney and Henry Beard and featuring blistering satire written by Michael O'Donoghue, P.J. O'Rourke, and Tony Hendra, among others. Prominent satiric stand-up comedian George Carlin acknowledged the influence "The Realist" had in his 1970s conversion to a satiric comedian.

A more humorous brand of satire enjoyed a renaissance in the UK in the early 1960s with the satire boom, led by such luminaries as Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore, whose stage show "Beyond the Fringe" was a hit not only in Britain, but also in the United States. Other significant influences in 1960s British satire include David Frost, Eleanor Bron and the television program "That Was The Week That Was".

Joseph Heller's most famous work, "Catch-22" (1961), satirizes bureaucracy and the military, and is frequently cited as one of the greatest literary works of the twentieth century. Departing from traditional Hollywood farce and screwball, director and comedian Jerry Lewis used satire in his self-directed films "The Bellboy" (1960), "The Errand Boy" (1961) and "The Patsy" (1964) to comment on celebrity and the star-making machinery of Hollywood.
The film "Dr. Strangelove" (1964) starring Peter Sellers was a popular satire on the Cold War.

Contemporary popular usage of the term "satire" is often very imprecise. While satire often uses caricature and parody, by no means all uses of these or other humorous devices are satiric. Refer to the careful definition of satire that heads this article.
Satire is used on many UK television programmes, particularly popular panel shows and quiz shows such as "Mock the Week" (2005–ongoing) and "Have I Got News for You" (1990–ongoing). It is found on radio quiz shows such as "The News Quiz" (1977–ongoing) and "The Now Show" (1998–ongoing). One of the most watched UK television shows of the 1980s and early 1990s, the puppet show "Spitting Image" was a satire of the royal family, politics, entertainment, sport and British culture of the era. Court Flunkey from "Spitting Image" is a caricature of James Gillray, intended as a homage to the father of political cartooning. Created by DMA Design in 1997, satire features prominently in the British video game series "Grand Theft Auto".

Trey Parker and Matt Stone's "South Park" (1997–ongoing) relies almost exclusively on satire to address issues in American culture, with episodes addressing racism, anti-Semitism, militant atheism, homophobia, sexism, environmentalism, corporate culture, political correctness and anti-Catholicism, among many other issues.

Australian Chris Lilley produces comedy art in the style of mockumentaries ("", "Summer Heights High", "Angry Boys") and his work is often described as complex social satire.
Stephen Colbert's television program, "The Colbert Report" (2005–14), is instructive in the methods of contemporary American satire. Colbert's character is an opinionated and self-righteous commentator who, in his TV interviews, interrupts people, points and wags his finger at them, and "unwittingly" uses a number of logical fallacies. In doing so, he demonstrates the principle of modern American political satire: the ridicule of the actions of politicians and other public figures by taking all their statements and purported beliefs to their furthest (supposedly) logical conclusion, thus revealing their perceived hypocrisy or absurdity.

The American sketch comedy television show "Saturday Night Live" is also known for its satirical impressions and parodies of prominent persons and politicians, among some of the most notable, their parodies of U.S. political figures Hillary Clinton and of Sarah Palin.

Other political satire includes various political causes in the past, including the relatively successful Polish Beer-Lovers' Party and the joke political candidates Molly the Dog and Brian Miner.

In the United Kingdom, a popular modern satirist was the late Sir Terry Pratchett, author of the internationally best-selling "Discworld" book series. One of the most well-known and controversial British satirists is Chris Morris, co-writer and director of "Four Lions".

In Canada, satire has become an important part of the comedy scene. Stephen Leacock was one of the best known early Canadian satirists, and in the early 20th century, he achieved fame by targeting the attitudes of small town life. In more recent years, Canada has had several prominent satirical television series and radio shows. Some, including "CODCO", "The Royal Canadian Air Farce", "This Is That", and "This Hour Has 22 Minutes" deal directly with current news stories and political figures, while others, like "History Bites" present contemporary social satire in the context of events and figures in history. The Canadian organization "Canada News Network" provides commentary on contemporary news events that are primarily Canadian in nature. Canadian songwriter Nancy White uses music as the vehicle for her satire, and her comic folk songs are regularly played on CBC Radio.

Cartoonists often use satire as well as straight humour. Al Capp's satirical comic strip "Li'l Abner" was censored in September 1947. The controversy, as reported in "Time", centred on Capp's portrayal of the US Senate. Said Edward Leech of Scripps-Howard, "We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables." Walt Kelly's "Pogo" was likewise censored in 1952 over his overt satire of Senator Joe McCarthy, caricatured in his comic strip as "Simple J. Malarky". Garry Trudeau, whose comic strip "Doonesbury" focuses on satire of the political system, and provides a trademark cynical view on national events. Trudeau exemplifies humour mixed with criticism. For example, the character Mark Slackmeyer lamented that because he was not legally married to his partner, he was deprived of the "exquisite agony" of experiencing a nasty and painful divorce like heterosexuals. This, of course, satirized the claim that gay unions would denigrate the sanctity of heterosexual marriage.
Like some literary predecessors, many recent television satires contain strong elements of parody and caricature; for instance, the popular animated series "The Simpsons" and "South Park" both parody modern family and social life by taking their assumptions to the extreme; both have led to the creation of similar series. As well as the purely humorous effect of this sort of thing, they often strongly criticise various phenomena in politics, economic life, religion and many other aspects of society, and thus qualify as satirical. Due to their animated nature, these shows can easily use images of public figures and generally have greater freedom to do so than conventional shows using live actors.

News satire is also a very popular form of contemporary satire, appearing in as wide an array of formats as the news media itself: print (e.g. "The Onion", "Canada News Network", "Private Eye"), "Not Your Homepage," radio (e.g. "On the Hour"), television (e.g. "The Day Today", "The Daily Show", "Brass Eye") and the web (e.g. Mindry.in, The Fruit Dish, Scunt News, Faking News, El Koshary Today, The Giant Napkin, Unconfirmed Sources and The "Onion"s website). Other satires are on the list of satirists and satires. Another internet-driven form of satire is to lampoon bad internet performers. An example of this is the Internet meme character Miranda Sings.

In an interview with "Wikinews", Sean Mills, President of "The Onion", said angry letters about their news parody always carried the same message. "It's whatever affects that person", said Mills. "So it's like, 'I love it when you make a joke about murder or rape, but if you talk about cancer, well my brother has cancer and that's not funny to me.' Or someone else can say, 'Cancer's "hilarious", but don't talk about rape because my cousin got raped.' Those are rather extreme examples, but if it affects somebody personally, they tend to be more sensitive about it."

Zhou Libo, a comedian from Shanghai, is the most popular satirist in China. His humour has interested middle-class people and has sold-out shows ever since his rise to fame.

Paweł Kuczyński is a polish artist that has been awarded many prices for his works on political satyre. His topics range from the absurdity of war to the absorbing power of todays' technology.

Literary satire is usually written out of earlier satiric works, reprising previous conventions, commonplaces, stance, situations and tones of voice. Exaggeration is one of the most common satirical techniques. Contrarily diminution is also a satirical technique.

For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. In Germany and Italy satire is protected by the constitution.

Since satire belongs to the realm of art and artistic expression, it benefits from broader lawfulness limits than mere freedom of information of journalistic kind. In some countries a specific "right to satire" is recognized and its limits go beyond the "right to report" of journalism and even the "right to criticize". Satire benefits not only of the protection to freedom of speech, but also to that to culture, and that to scientific and artistic production.

In September 2017 The Juice Media received an e-mail from the Australian National Symbols Officer requesting that the use of a satirical logo, called the "Coat of Harms" based on the Australian Coat of Arms, no longer be used as they had received complaints from the members of the public. Coincidentally 5 days later a Bill was proposed to Australian parliament to amend the Criminal Code Act 1995. If successfully passed those found to be in breach of the new amendment can face 2–5 years imprisonment.

As of June 2018, the Criminal Code Amendment (Impersonating a Commonwealth Body) Bill 2017 was before the Australian Senate with the third reading moved May 10, 2018.

Descriptions of satire's biting effect on its target include 'venomous', 'cutting', 'stinging', vitriol. Because satire often combines anger and humor, as well as the fact that it addresses and calls into question many controversial issues, it can be profoundly disturbing.

Because it is essentially ironic or sarcastic, satire is often misunderstood. A typical misunderstanding is to confuse the satirist with his persona.

Common uncomprehending responses to satire include revulsion (accusations of poor taste, or that "it's just not funny" for instance) and the idea that the satirist actually does support the ideas, policies, or people he is attacking. For instance, at the time of its publication, many people misunderstood Swift's purpose in "A Modest Proposal", assuming it to be a serious recommendation of economically motivated cannibalism.

Some critics of Mark Twain see "Huckleberry Finn" as racist and offensive, missing the point that its author clearly intended it to be satire (racism being in fact only one of a number of Mark Twain's known concerns attacked in "Huckleberry Finn"). This same misconception was suffered by the main character of the 1960s British television comedy satire "Till Death Us Do Part". The character of Alf Garnett (played by Warren Mitchell) was created to poke fun at the kind of narrow-minded, racist, little Englander that Garnett represented. Instead, his character became a sort of anti-hero to people who actually agreed with his views. (The same situation occurred with Archie Bunker in American TV show "All in the Family", a character derived directly from Garnett.)

The Australian satirical television comedy show "The Chaser's War on Everything" has suffered repeated attacks based on various perceived interpretations of the "target" of its attacks. The "Make a Realistic Wish Foundation" sketch (June 2009), which attacked in classical satiric fashion the heartlessness of people who are reluctant to donate to charities, was widely interpreted as an attack on the Make a Wish Foundation, or even the terminally ill children helped by that organisation. Prime Minister of the time Kevin Rudd stated that The Chaser team "should hang their heads in shame". He went on to say that "I didn't see that but it's been described to me. ...But having a go at kids with a terminal illness is really beyond the pale, absolutely beyond the pale." Television station management suspended the show for two weeks and reduced the third season to eight episodes.

The romantic prejudice against satire is the belief spread by the romantic movement that satire is something unworthy of serious attention; this prejudice has held considerable influence to this day. Such prejudice extends to humour and everything that arouses laughter, which are often underestimated as frivolous and unworthy of serious study. For instance, humor is generally neglected as a topic of anthropological research and teaching.

Because satire criticises in an ironic, essentially indirect way, it frequently escapes censorship in a way more direct criticism might not. Periodically, however, it runs into serious opposition, and people in power who perceive themselves as attacked attempt to censor it or prosecute its practitioners. In a classic example, Aristophanes was persecuted by the demagogue Cleon.

In 1599, the Archbishop of Canterbury John Whitgift and the Bishop of London Richard Bancroft, whose offices had the function of licensing books for publication in England, issued a decree banning verse satire. The decree, now known as the Bishops' Ban of 1599, ordered the burning of certain volumes of satire by John Marston, Thomas Middleton, Joseph Hall, and others; it also required histories and plays to be specially approved by a member of the Queen's Privy Council, and it prohibited the future printing of satire in verse.

The motives for the ban are obscure, particularly since some of the books banned had been licensed by the same authorities less than a year earlier. Various scholars have argued that the target was obscenity, libel, or sedition. It seems likely that lingering anxiety about the Martin Marprelate controversy, in which the bishops themselves had employed satirists, played a role; both Thomas Nashe and Gabriel Harvey, two of the key figures in that controversy, suffered a complete ban on all their works. In the event, though, the ban was little enforced, even by the licensing authority itself.

In 2005, the Jyllands-Posten Muhammad cartoons controversy caused global protests by offended Muslims and violent attacks with many fatalities in the Near East. It was not the first case of Muslim protests against criticism in the form of satire, but the Western world was surprised by the hostility of the reaction: Any country's flag in which a newspaper chose to publish the parodies was being burnt in a Near East country, then embassies were attacked, killing 139 people in mainly four countries; politicians throughout Europe agreed that satire was an aspect of the freedom of speech, and therefore to be a protected means of dialogue. Iran threatened to start an International Holocaust Cartoon Competition, which was immediately responded to by Jews with an Israeli Anti-Semitic Cartoons Contest.

In 2006 British comedian Sacha Baron Cohen released "Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan", a "mockumentary" that satirized everyone, from high society to frat boys. The film was criticized by many. Although Baron Cohen is Jewish, some complained that it was antisemitic, and the government of Kazakhstan boycotted the film. The film itself had been a reaction to a longer quarrel between the government and the comedian.

In 2008, popular South African cartoonist and satirist Jonathan Shapiro (who is published under the pen name Zapiro) came under fire for depicting then-president of the ANC Jacob Zuma in the act of undressing in preparation for the implied rape of 'Lady Justice' which is held down by Zuma loyalists. The cartoon was drawn in response to Zuma's efforts to duck corruption charges, and the controversy was heightened by the fact that Zuma was himself acquitted of rape in May 2006. In February 2009, the South African Broadcasting Corporation, viewed by some opposition parties as the mouthpiece of the governing ANC, shelved a satirical TV show created by Shapiro, and in May 2009 the broadcaster pulled a documentary about political satire (featuring Shapiro among others) for the second time, hours before scheduled broadcast. Apartheid South Africa also had a long history of censorship.

On December 29, 2009, Samsung sued Mike Breen, and the "Korea Times" for $1 million, claiming criminal defamation over a satirical column published on Christmas Day, 2009.

On April 29, 2015, the UK Independence Party (UKIP) requested Kent Police investigate the BBC, claiming that comments made about Party leader Nigel Farage by a panelist on the comedy show "Have I Got News For You" might hinder his chances of success in the general election (which would take place a week later), and claimed the BBC breached the Representation of the People Act. Kent Police rebuffed the request to open an investigation, and the BBC released a statement, "Britain has a proud tradition of satire, and everyone knows that the contributors on "Have I Got News for You" regularly make jokes at the expense of politicians of all parties."

Satire is occasionally prophetic: the jokes precede actual events. Among the eminent examples are:
In June 2019, Punocracy, "Nigeria"'s foremost satire platform organised a nationwide writing competition for youth in the country with the objective to make satire a widely accepted and understood tool of socio-political commentary. Some of the entries addressed issues like "gender violence", "political corruption", religious hypocrisy, "internet fraud", educational decay and so on. The group also declared November 9 as World Satire Day with the idea of "trying to fight against the ills in the society not by ammunition but by humour, sarcasm etcetera". 






</doc>
<doc id="26792" url="https://en.wikipedia.org/wiki?curid=26792" title="Samuel Butler (poet)">
Samuel Butler (poet)

Samuel Butler (baptized 14 February 1613 – 25 September 1680) was a poet and satirist. He is remembered now chiefly for a long satirical poem titled "Hudibras".

Samuel Butler was born in Strensham, Worcestershire, and was the son of a farmer and churchwarden, also named Samuel. His date of birth is unknown, but there is documentary evidence for the date of his baptism of 14 February. The date of Butler's baptism is given as 8 February by Treadway Russell Nash in his 1793 edition of "Hudibras". Nash had already mentioned Butler in his "Collections for a History of Worcestershire" (1781), and perhaps because the latter date seemed to be a revised account, it has been repeated by many writers and editors. However, The parish register of Strensham records under the year 1612: "Item was christened Samuell Butler the sonne of Samuell Butler the xiiijth of February anno ut supra". Lady Day, 25 March, was New Year's Day in England at the time, so the year of his baptism was 1613 according to the change of the start of the year with the Calendar Act of 1750 (see Old Style and New Style dates). Nash also claims in his 1793 edition of "Hudibras" that Butler's father entered his son's baptism into the register, an error that was also repeated in later publications; however, the entry was clearly written by a different hand.

Butler was brought up in the household of Sir William Russell of Strensham and became his clerk. "When just a Boy he would make observations and reflections on every Thing one sayd or did, and censure it to be either well or ill. He was never at the University for the reason alleged." He was educated at the King's School, Worcester, under Henry Bright whose teaching is recorded favourably by Thomas Fuller, a contemporary writer, in his "Worthies of England". In early youth he was a servant to the Countess of Kent. Through Lady Kent he met her steward, the jurist John Selden who influenced his later writings. He also tried his hand at painting but was reportedly not very good at it; one of his editors reporting that "his pictures served to stop windows and save the tax" (on window glass). Conversely, John Aubrey who knew Butler quite well enough to be one of his pallbearers, wrote that "He was thinking once to have made painting his Profession. His love to and skill in painting made a great friendship between him and Mr. Samuel Cowper (The Prince of Limners of this Age." He studied law but did not practice. 

After the Restoration he became secretary, or steward, to Richard Vaughan, 2nd Earl of Carbery, Lord President of Wales, which entailed living at least a year in Ludlow, Shropshire, until January 1662 while he was paying craftsmen working on repairing the castle there. In late 1662 the first part of "Hudibras", which he began writing when lodging at Holborn, London, in 1658 and continued to work on while in Ludlow, was published, and the other two in 1664 and 1678 respectively. One early purchaser of the first two parts was Samuel Pepys. While the diarist acknowledged that the book was the "greatest fashion" he could not see why it was found to be so witty.

Despite the popularity of "Hudibras", Butler was not offered a place at Court. "Satyrical Witts disoblige whom they converse with;and consequently make to themselves many Enemies and few Friends;and this was his manner and case." However, Butler is thought to have been in the employment of the Duke of Buckingham in the summer of 1670, and accompanied him on a diplomatic mission to France. Butler also received financial support in the form of a grant from King Charles II.

During the latter part of his life, Butler lived in a house in the now partially demolished Rose Street, to the west of Covent Garden.

Butler died of consumption on 25 September 1680, and was buried on 27 September in the Church-yard of St. Paul's, Covent Garden;in the north part next to the church at the east end. "His feet touch the wall. His grave 2 yards distant from the Pillaster of the Dore (by his desire) 6 feet deep"at the expense of a Mr. Longueville, although he was not in debt when he died. Aubrey in "Brief Lives" describes his grave as "being in the north part next to the church at the east end ... 2 yards distant from the pillaster of the dore". Also, a monument to him was placed in Westminster Abbey in 1732 by a printer, John Barber, and the Lord Mayor of London. There is also a memorial plaque to him in the small village church of Strensham, Worcestershire, near the town of Upton upon Severn, his birthplace.

"Hudibras" is directed against religious sectarianism. The poem was very popular in its time, and several of its phrases have passed into the dictionary. It was sufficiently popular to spawn imitators. "Hudibras" takes some of its characterization from "Don Quixote" but, unlike that work, it has many more references to personalities and events of the day. Butler was also influenced by satirists such as John Skelton and Paul Scarron's "Virgile travesti"; a satire on classical literature, particularly Virgil.

"Hudibras" was reprinted many times in the centuries following Butler's death. Two of the more noteworthy editions are those edited by Zachery Grey (1744) and Treadway Russell Nash (1793). The standard edition of the work was edited by John Wilders (1967).

Most of his other writings never saw print until they were collected and published by Robert Thyer in 1759. Butler wrote many short biographies, epigrams and verses the earliest surviving from 1644. Of his verses, the best known is "The Elephant on the Moon", about a mouse trapped in a telescope, a satire on Sir Paul Neale of the Royal Society. Butler's taste for the mock heroic is shown by another early poem "Cynarctomachy", or Battle between Bear and Dogs, which is both a homage to and a parody of a Greek poem ascribed to Homer, "Batrachomyomachia". 
He wrote the poem "Upon Philip Nye's Thanksgiving Beard" about the Puritan Philip Nye and later also mentioned him in "Hudibras".

His supposed lack of money later in life is strange as he had numerous unpublished works which could have offered him income including a set of Theophrastan character sketches which were not printed until 1759. Many other works are dubiously attributed to him.


Attribution:



</doc>
<doc id="26794" url="https://en.wikipedia.org/wiki?curid=26794" title="List of science fiction and fantasy artists">
List of science fiction and fantasy artists

This is a list of science fiction and fantasy artists, notable and well-known 20th- and 21st-century artists who have created book covers or interior illustrations for books, or who have had their own books or comic books of fantastic art with science fiction or fantasy themes published. Artists known exclusively for their work in comic books are not included. Many of the artists are known for their work in both the fantasy and sf fields. Artists who have won the Hugo Award, the World Fantasy Award, or the Chesley Award are noted, as are inductees into the Science Fiction Hall of Fame.

























 


</doc>
<doc id="26795" url="https://en.wikipedia.org/wiki?curid=26795" title="Saxophone">
Saxophone

The saxophone (referred to colloquially as the sax) is a woodwind instrument usually made of brass and played with a single-reed mouthpiece. Although most saxophones are made from brass, they are categorized as woodwind instruments, because sound is produced by an oscillating reed, traditionally made out of woody cane, rather than lips vibrating in a mouthpiece cup as with the brass instrument family. As with the other woodwind instruments, the pitch of the note being played is controlled by covering holes in the body tube to control the resonant frequency of the air column by changing the effective length of the tube.
The player covers holes by pressing mechanical keys, triggering a system of pads, pivots, and linkages.

The saxophone is used in classical music (such as concert bands, chamber music, solo repertoire, and, occasionally, orchestras), military bands, marching bands, jazz (such as big bands and jazz combos), and contemporary music. The saxophone is also used as a solo and melody instrument or as a member of a horn section in some styles of rock and roll and popular music. Saxophone players are called "saxophonists".

Since the first saxophone was invented by the Belgian instrument maker Adolphe Sax in the early 1840s, saxophones have been produced in a variety of series distinguished by transpositions within instrument sets and tuning standard. Sax patented the saxophone on 28 June 1846, in two groups of seven instruments each. Each series consisted of instruments ranked by pitch, in alternating transposition. The series pitched in B and E soon became dominant and most saxophones encountered today are from this series. Instruments from the series pitched in C and F never gained a foothold and constituted only a small percentage of instruments made by Sax. "High Pitch" (also marked "H" or "HP") saxophones tuned sharper than the (concert) A = 440 Hz standard were produced into the early twentieth century for sonic qualities suited for outdoor uses, but are not playable to modern tuning and are considered obsolete. "Low Pitch" (also marked "L" or "LP") saxophones are equivalent in tuning to modern instruments. C soprano and C melody saxophones were produced for the casual market as parlor instruments during the early twentieth century. Saxophones in F were introduced during the late 1920s but never gained acceptance. The modern saxophone family consists entirely of instruments in the B – E series, historical and experimental instruments notwithstanding. The saxophones with widest use and availability are the soprano, alto, tenor, and baritone saxophones.

In the "keyed" (below overtone-produced "altissimo") ranges of the various saxophones, the pitch is controlled by "keys" with shallow "cups" in which are fastened leather "pads" that seal "toneholes", controlling the resonant length, and thereby frequency, of the air column within the "bore". Small holes called "vents", located between the toneholes and the mouthpiece, are opened by an "octave key" to raise the pitch by eliminating the fundamental frequency, leaving the first harmonic as the frequency defining the pitch. Most modern saxophones are keyed to produce a low B (relative to the instrument's transposition) with all keys closed; modern baritone saxophones commonly play a low A and altos keyed to low A have been produced in the past. The highest keyed note has traditionally been F two and a half octaves above low B, while the keyed range is extended to F on most recent performance-class instruments. A high G key is most common on modern soprano saxophones. Notes above F are considered part of the altissimo register of any saxophone, and can be produced using advanced embouchure techniques and fingering combinations. Keywork facilitating altissimo playing is a feature of modern saxophones. Modern saxophone players have extended the range to over four octaves on tenor and alto. Music for most saxophones is usually notated using treble clef.

Because all saxophones use the same key arrangement and fingering to produce a given notated pitch, it is not difficult for a competent player to switch among the various sizes when the music has been suitably transposed, and many do so. Since the baritone and alto are pitched in E, players can read concert pitch music notated in the bass clef by reading it as if it were treble clef and adding three sharps to the key signature. This process, referred to as "clef substitution", makes it possible for the Eb instruments to play from parts written for baritone horn, bassoon, euphonium, string bass, trombone, or tuba. This can be useful if a band or orchestra lacks one of those instruments.

The "straight" soprano and sopranino saxophones consist of a straight conical tube with a flared "bell" at the end opposite the mouthpiece. Alto and larger saxophones include a detachable curved "neck" above the highest tone hole, directing the mouthpiece to the player's mouth and, with rare exceptions, a U-shaped "bow" that directs the bore upward and a curve in the throat of the bell directing it forward. The set of curves near the bell has become a distinctive feature of the saxophone family, to the extent that soprano and even sopranino saxes are sometimes made in the curved style. The baritone, bass, and contrabass saxophones accommodate the length of the bore with extra bows and right-angle bends between the main body and the mouthpiece.

The left hand operates keys from the upper part of the body tube while the right hand operates keys from the lower part. The right thumb sits under a "thumb hook" and left thumb is placed on a "thumb rest" to stabilize and balance the saxophone, while the weight of most saxophones is shared by the right thumb and a neckstrap attached to a "strap ring" on the rear of the body of the instrument. With the smaller instruments, relatively more of the weight is supported by the thumb. The left thumb operates the octave key.

Keys consist of the cups, levers, and pivots that control the position of the pads over the toneholes. At rest, some keys are open and some are closed, held in position by springs that are overridden by finger or hand ("palm" keys) pressure. The keys are activated by pressure on "key touches", either directly on the pad cup or connected to it with levers, either directly or with joints called "linkages." The levers between the key cups and the pivots are called "key arms".

The fingering for the saxophone is a combination of that of the oboe with the Boehm system and is similar to the flute or upper register of the clarinet. The "stack" keys are operated by the first, second, and third fingers on each hand with slightly concave button-style key touches ("key buttons") operating with the same motion as the pad cups that they control. The stack keys are linked to higher stack keys with "regulation bar" and "bridge arm" linkages. Key buttons are advantageous for operating keys with direct downward finger pressure but provide disadvantages operating keys with other finger and hand motions, hence, their use on keys operated with such motions has diminished with the evolution of saxophone designs.

Palm keys and the "front F" key operated by the left hand, and the "high F", "high F" and "high G" keys operated by the right hand, control the upper end of the keyed range and are used to vent altissimo notes. "Chromatic" keys operated by the right hand provide alternate fingerings for F, B, and C within the stack range. The fourth fingers of the right and left hands open keys to raise pitch by a semitone as well as close keys towards the lower range of the instrument, with the lowest pitch "bell keys" operated by the left hand. The keys operated by the fourth fingers are referred to as "table" keys. Instruments that play to low A have a left thumb key for that note.

On saxophones produced since the early 1920s the G key operated from the left hand table is closed by closing keys on the lower stack regardless of pressure on the G actuating mechanism ("F-linked", or "stack-linked", G mechanism). That feature vastly increases the speed and playability of certain intervals to the point that saxophones with "direct G" action, in which the key stays open when the lower stack keys are depressed, are considered obsolete. Modern left hand tables also "articulate" the G key with the low C, B, and B keys to open it when any of those keys are depressed and the right hand stack keys are not. That also provides significant advantages for playing certain intervals near the lower range of the instrument. Some players willingly forego the benefits of the articulated G to play vintage instruments, but a front F key and a stack-linked G key are regarded as critical features by serious players.

From the earliest days of the saxophone the body and key cups have been made from sheet brass stock, owing to its workability in forming complex shapes. Mechanical keywork is assembled from components either hand-tooled or machined from other forms of brass stock. King introduced saxophones with necks and bells of sterling silver during the 1930s and continued that "silversonic" scheme into the early 1960s. Yanagisawa revived the scheme during the 1980s and later introduced entire instruments of sterling silver. 
Keilwerth and P. Mauriat have used nickel silver, a copper-nickel-zinc alloy more commonly used for flutes, for the bodies of some saxophone models. 
For visual and tonal effect, higher copper variants of brass are sometimes substituted for the more common "yellow brass" and "cartridge brass." Yanagisawa made its 902 and 992 series saxophones with the high copper alloy phosphor bronze to achieve a darker, more "vintage" tone than the brass 901 and 991 models. Other saxophones made of high copper alloys are sold under various brands.

Other materials are used for some mechanical parts and keywork. Since 1920, most saxophones have replaceable key buttons operating the stack keys, usually made from either plastic or mother of pearl. Some saxophones are made with abalone, stone, or wood key buttons. On some premium models, the key button material is used to form the convex key touches for other keys. The rods and screw pins that the keywork's hinges pivot on, and the needle and leaf springs that hold keys in their rest position, are usually made of blued or stainless steel. Mechanical buffers of felt, cork, leather, and various synthetic materials are used to reduce friction, to minimize mechanical noise from movement of keys, and to optimize the action of the keywork for positive pad sealing, intonation, speed, and "feel." Nickel silver is sometimes used for hinges for its advantages of mechanical durability, although the most common material for such applications has remained brass. Saxophones with high copper bodies still have brass keywork owing to its more suitable mechanical properties relative to those alloys.

Before final assembly, manufacturers usually apply a finish to the surface of the horn. The most common finish is a thin coating of clear or colored acrylic lacquer. The lacquer serves to protect the brass from oxidation and maintains its shiny appearance. Silver or gold plating are offered as premium options on some models. Some silver plated saxophones are also lacquered. Plating saxophones with gold is an expensive process because an underplating of silver is required for the gold to adhere to. Nickel plating has been used on the bodies of early budget model saxophones and is commonly used on keywork when a more durable finish than lacquer is desired, mostly with student model saxophones. Chemical surface treatment of the base metal has come into use as an alternative to the lacquer and plating finishes in recent years. Some saxophonists, retailers, and repair technicians argue that the type of lacquer or plating (or absence of lacquer) may be a factor affecting the instrument's tone quality.

The saxophone uses a single-reed mouthpiece similar to that of the clarinet. Each size of saxophone (alto, tenor, etc.) uses a different size of reed and mouthpiece.

Most saxophonists use reeds made from "Arundo donax" cane, but since middle of the twentieth century some have also been made of fiberglass and other composite materials. Saxophone reeds are proportioned slightly differently from clarinet reeds, being wider for the same length. Reeds are commercially available in a vast array of brands, styles, and strengths. Saxophonists experiment with reeds of different strength (hardnesses) and material to find which strength and cut suits their mouthpiece, embouchure, physiology, and playing style.

Mouthpiece design has a profound impact on tone. Different mouthpiece design characteristics and features tend to be favored for different styles. Early mouthpieces were designed to produce a "warm" and "round" sound for classical playing. Among classical mouthpieces, those with a concave ("excavated") "chamber" are more true to Adolphe Sax's original design; these provide a softer or less piercing tone favored by the Raschèr school of classical playing. Saxophonists who follow the French school of classical playing, influenced by Marcel Mule, generally use mouthpieces with smaller chambers for a somewhat "brighter" sound with relatively more upper harmonics. The use of the saxophone in dance orchestras and jazz ensembles from the 1920s onward placed emphasis on "dynamic range" and projection, leading to innovation in mouthpiece chamber shapes and "tip" designs, as well as metal construction. At the opposite extreme from the classical mouthpieces are those with a small chamber and a low clearance above the reed between the tip and the chamber, called high "baffle". These produce a bright sound with maximum projection, suitable for having a sound stand out among amplified instruments and are commonly used in modern pop and smooth jazz.

Mouthpieces come in a wide variety of materials, including vulcanized rubber (sometimes called hard rubber or ebonite), plastic, and metals such as bronze or surgical steel. Less common materials that have been used include wood, glass, crystal, porcelain, and bone. Recently, Delrin has been added to the stock of mouthpiece materials.

The effect of mouthpiece materials on tone of the saxophone has been the subject of much debate. According to Larry Teal, the mouthpiece material has little, if any, effect on the sound, and the physical dimensions give a mouthpiece its tone color. There are examples of "dark" sounding metal pieces and "bright" sounding hard rubber pieces. The lower rigidity of hard rubber relative to metal restricts some design characteristics affecting tone and response more than with metal. The extra bulk required near the tip with hard rubber affects mouth position and airflow characteristics. Recently, increased mass of the mouthpiece over the "shank", which fits over the neck cork, has become a design feature to enhance the integrity of the harmonic series by stabilizing the mouthpiece/neck connection. Shank weights (large rings of brass over the shank) are used with some Delrin mouthpieces to increase "resonance and projection." Other "hybrid" designs with a hard rubber body and a substantial metal shank have a similar mass distribution, although its contribution to sound characteristics is not highlighted in product descriptions.

The saxophone was designed around 1840 by Adolphe Sax, a Belgian instrument maker, flautist, and clarinetist. Born in Dinant and originally based in Brussels, he moved to Paris in 1842 to establish his musical instrument business. Before working on the saxophone, he made several improvements to the bass clarinet by improving its keywork and acoustics and extending its lower range. Sax was also a maker of the ophicleide, a large conical brass instrument in the bass register with keys similar to a woodwind instrument. His experience with these two instruments allowed him to develop the skills and technologies needed to make the first saxophones.

As an outgrowth of his work improving the bass clarinet, Sax began developing an instrument with the projection of a brass instrument and the agility of a woodwind. He wanted it to overblow at the octave, unlike the clarinet, which rises in pitch by a twelfth when overblown. An instrument that overblows at the octave has identical fingering for both registers.

Sax created an instrument with a single-reed mouthpiece and conical brass body. Having constructed saxophones in several sizes in the early 1840s, Sax applied for, and received, a 15-year patent for the instrument on 28 June 1846. The patent encompassed 14 versions of the fundamental design, split into two categories of seven instruments each, and ranging from sopranino to contrabass. A limited number of instruments in the series pitched in F and C were produced by Sax, but the series pitched in E and B quickly became the standard. All the instruments were given an initial written range from the B below the treble staff to the E one half-step below the third ledger line above staff, giving each saxophone a range of two and a half octaves. Sax's patent expired in 1866. Thereafter, numerous other instrument manufacturers implemented their own improvements to the design and keywork.

Sax's original keywork, which was based on the "Triebert system 3" oboe for the left hand and the "Boehm" clarinet for the right, was simplistic and made certain legato passages and wide intervals extremely difficult to finger; that system would later evolve with extra keys, linkage mechanisms, and alternate fingerings to make some intervals less difficult.

Early in the development of the saxophone the upper keyed range was extended to E, then F above the staff; 1880s era sheet music for saxophone was written for the range of low B to F. In 1887 the Buffet-Crampon company obtained a patent for extending the bell and adding an extra key to extend the range downwards by one semitone to B. This extension is currently standard in most modern designs, with the notable exception of baritone saxophones further extended and keyed to low A. The upper range to F would remain the standard for nearly a century until the altissimo F key became common on modern saxophones.

In the 1840s and 1850s, Sax's invention gained use in small classical ensembles (both all-saxophone and mixed), as a solo instrument, and in French and British military bands. Saxophone method books were published and saxophone instruction was offered at conservatories in France, Switzerland, Belgium, Spain, and Italy. By 1856 the French "Garde Republicaine" band included eight saxophones, making it the large ensemble that featured the instrument most prominently. The saxophone was used experimentally in orchestral scores, but never came into widespread use as an orchestral instrument. In 1853-54 the orchestra of Louis Antoine Jullien featured a soprano saxophone on a concert tour of the United States.

After an early period of interest and support from classical music communities in Europe, their interest in the instrument waned in the late nineteenth century. Saxophone teaching at the Paris Conservatory was suspended from 1870 to 1900 and classical saxophone repertoire stagnated during that period. But it was during this same period that the saxophone began to be promoted in the United States, largely through the efforts of Patrick Gilmore, leader of the "22nd Regiment band", and Edward A. Lefebre, a Dutch emigre and saxophonist with family business associations with Sax. Lefebre settled in New York in early 1872 after he arrived as a clarinetist with a British opera company. Gilmore organized the World Peace Jubilee and International Music Festival taking place in Boston that summer. The Garde Republicaine band performed and Lefebre was a clarinetist with the Great Festival Orchestra for that event. In the fall of 1873 Gilmore was reorganizing the 22nd Regiment band under the influence of the Garde Republicaine band and recruited Lefebre, who had established a reputation in New York as a saxophonist over the previous year. Gilmore's band soon featured a soprano-alto-tenor-baritone saxophone section, which also performed as a quartet. The Gilmore-Lefebre association lasted until Gilmore's death in 1892, during which time Lefebre also performed in smaller ensembles of various sizes and instrumentation, and worked with composers to increase light classical and popular repertoire for saxophone.

Lefebre's later promotional efforts were extremely significant in broadening adoption of the saxophone. Starting towards the end of the 1880s he consulted with the brass instrument manufacturer C.G. Conn to develop and start production of improved saxophones to replace the costly, scantly available, and mechanically unreliable European instruments in the American market. The early 1890s saw regular production of saxophones commence at Conn and its offshoot Buescher Manufacturing Company, which dramatically increased availability of saxophones in the US. Lefebre worked with the music publisher Carl Fischer to distribute his transcriptions, arrangements, and original works for saxophone, and worked with the Conn Conservatory to further saxophone pedagogy in the US. Lefebre's associations with Conn and Fischer lasted into the first decade of the twentieth century and Fischer continued to publish new arrangements of Lefebre's works posthumously.

While the saxophone remained marginal and regarded mainly as a novelty instrument in the classical music world, many new musical niches were established for it during the early decades of the twentieth century. Its early use in Vaudeville and ragtime bands around the turn of the century laid the groundwork for its use in dance orchestras and eventually jazz. As the market for saxophones grew in the US, the manufacturing industry grew; the Martin Band Instrument Company started producing saxophones between 1905 and 1912, and the Cleveland Band Instrument Company started producing saxophones under contract to the H. N. White Company in 1916. The saxophone was promoted for the casual market with introduction of the C-soprano and C-melody (between alto and tenor) saxophones to play in key with pianos from the same sheet music. Production of such instruments stopped during the Great Depression. During the 1920s the saxophone came into use as a jazz instrument, fostered by the influences of the Fletcher Henderson Orchestra and the Duke Ellington Orchestra. Starting in the late 1920s and early 1930s, the modern era of classical saxophone was launched largely through the efforts of Marcel Mule and Sigurd Raschèr, and classical repertoire for the instrument expanded rapidly.

The use of the saxophone for more dynamic and more technically demanding styles of playing added incentive for improvements in keywork and acoustic design. Early saxophones had two separate octave keys operated by the left thumb to control the two octave vents required on alto and larger saxophones. A substantial advance in keywork around the turn of the century was the development of mechanisms by which the left thumb operates the two octave vents with a single octave key. Ergonomic design of keywork evolved rapidly during the 1920s and 1930s. The front F mechanism supporting alternate fingerings for high E and F, and stack-linked G key action, became standard during the 1920s, followed by improvements to the left hand table key mechanisms controlling the G and bell keys. New bore designs during the 1920s and 1930s resulted from the quest for improved intonation, dynamic response, and tonal qualities. The 1920s were also the era of design experiments such as the Buescher straight altos and tenors, the King "Saxello" soprano, the C.G. Conn "mezzo-soprano" saxophone keyed in F, and the "Conn-O-Sax" saxophone – English horn hybrid.

The modern layout of the saxophone emerged during the 1930s and 1940s, first with right-side bell keys introduced by C. G. Conn on baritones, then by King on altos and tenors. The mechanics of the left hand table were revolutionized by Selmer with their Balanced Action instruments in 1936, capitalizing on the right-side bell key layout. In 1948 Selmer introduced their Super Action saxophones with offset left and right hand stack keys. Between 30 and 40 years after Selmer devised their final layout it had been adopted for virtually every saxophone being produced, from student to professional models.

The high F key was also first introduced as an option on the Balanced Action model, although it took several decades for it to gain acceptance because of perceived deleterious effects on intonation in its early implementations.

The saxophone first gained popularity in military bands. Although the instrument was initially ignored in Germany, French and Belgian military bands were quick to include the instrument in their ensembles. Most French and Belgian military bands incorporate at least a quartet of saxophones, comprising an E baritone, B tenor, E alto and B soprano. These four instruments have proved the most popular of all of Sax's creations, with the E contrabass and B bass usually considered impractically large and the E sopranino insufficiently powerful. British military bands tend to include at minimum two saxophonists, on the alto and tenor.

The saxophone was introduced into the concert band, which usually calls for an E alto saxophone, a B tenor saxophone, and an E baritone saxophone. A concert band may include two altos, one tenor, and one baritone. A B soprano saxophone is also used, in which case it is played by the first alto saxophonist. A bass saxophone in B is used in some concert band music (especially music by Percy Grainger).

Saxophones are used in chamber music, such as saxophone quartets and other chamber combinations of instruments. The classical saxophone quartet consists of a B soprano saxophone, E alto saxophone, B tenor saxophone, and E baritone saxophone (SATB). On occasion, the soprano is replaced with a second alto sax (AATB); a few professional saxophone quartets have featured non-standard instrumentation, such as James Fei's Alto Quartet (four altos).

There is a repertoire of classical compositions and arrangements for the SATB instrumentation dating back to the nineteenth century, particularly by French composers who knew Sax. However, the largest body of chamber works for saxophone are from the modern era of classical saxophone initiated by Marcel Mule in 1928. Sigurd Raschèr followed as a soloist in orchestral works, starting in 1931, and also figured prominently in development of modern classical saxophone repertoire. The Mule quartet is often considered the prototype for quartets due the level of virtuosity demonstrated by its members and its central role in the development of modern quartet repertoire. However, organized quartets existed before Mule's ensemble, the prime example being the quartet headed by Edward A. Lefebre (1834–1911), which was a subset of Patrick Gilmore's 22nd Regiment band between 1873 and 1893.

In the 20th and 21st centuries, the saxophone found increased popularity in symphony orchestras. The instrument has also been used in genres such as opera and choral music. Many musical theatre scores include parts for saxophone, sometimes doubling another woodwind or brass instrument.

Coincident with the more widespread availability of saxophones in the US around the turn of the century was the rise of ragtime music. The bands featuring the syncopated latin- and African-American rhythmic influences of ragtime were an exciting new feature of the American cultural landscape and provided the groundwork for new styles of dancing. The rise of dance bands into the 1920s followed from the popularity of ragtime. Two of the best known ragtime-playing brass bands with saxophones were those led by W. C. Handy and James R. Europe. The saxophone was also used in Vaudeville entertainment during the same period. Ragtime, Vaudeville, and dance bands introduced much of the American public to the saxophone. Europe's 369th Infantry Regiment Band popularized ragtime in France during its 1918 tour. Rudy Wiedoeft became the best known individual saxophone stylist and virtuoso during this period leading into the "saxophone craze" of the 1920s. Following it, the saxophone became featured in music as diverse as the "sweet" music of Paul Whiteman and Guy Lombardo, jazz, swing, and large stage show bands.

The rise of the saxophone as a jazz instrument followed its widespread adoption in dance bands during the early 1920s. The Fletcher Henderson Orchestra, formed in 1923, featured arrangements to back up improvisation, bringing the first elements of jazz to the large dance band format. Following the innovations of the Fletcher Henderson Orchestra, the Duke Ellington Orchestra and Jean Goldkette's Victor Recording Orchestra featured jazz solos with saxophones and other instruments. The association of dance bands with jazz would reach its peak with the swing music of the 1930s. The large show band format, influenced by the 1930s swing bands, would be used as backing for popular vocalists and stage shows in the post World War II era, and provided a foundation for big band jazz. Show bands with saxophone sections became a staple of television talk shows (such as the Tonight Show that featured bands led by Doc Severinsen and Branford Marsalis) and Las Vegas stage shows. The swing era fostered the later saxophone styles that permeated bebop and rhythm and blues in the early postwar era.

Coleman Hawkins established the tenor saxophone as a jazz solo instrument during his stint with Fletcher Henderson from 1923 to 1934. Hawkins' arpeggiated, rich-toned, vibrato-laden style was the main influence on swing era tenor players before Lester Young, and his influence continued with other big-toned tenor players into the era of modern jazz. Among the tenor players directly influenced by him were Chu Berry, Charlie Barnet, Tex Beneke, Ben Webster, Vido Musso, Herschel Evans, Buddy Tate, and Don Byas. Hawkins' band mate Benny Carter and Duke Ellington's alto saxophonist Johnny Hodges became influential on swing era alto styles, while Harry Carney brought the baritone saxophone to prominence with the Duke Ellington Orchestra. The New Orleans player Sidney Bechet gained recognition for playing the soprano saxophone during the 1920s, but the instrument did not come into wide use until the modern era of jazz.

As Chicago style jazz evolved from New Orleans jazz in the 1920s, one of its defining features was the addition of saxophones to the ensemble. The small Chicago ensembles offered more improvisational freedom than did the New Orleans or large band formats, fostering the innovations of saxophonists Jimmy Dorsey (alto), Frankie Trumbauer (c-melody), Bud Freeman (tenor) and Stump Evans (baritone). Dorsey and Trumbauer became important influences on tenor saxophonist Lester Young.

Lester Young's approach on tenor saxophone differed from Hawkins', emphasizing more melodic "linear" playing that wove in and out of the chordal structure and longer phrases that differed from those suggested by the tune. He used vibrato less, fitting it to the passage he was playing. His tone was smoother and darker than that of his 1930s contemporaries. Young's playing was a major influence on the modern jazz saxophonists Al Cohn, Stan Getz, Zoot Sims, Dexter Gordon, Wardell Gray, Lee Konitz, Warne Marsh, Charlie Parker, and Art Pepper.

The influence of Lester Young with the Count Basie Orchestra in the late 1930s and the popularity of Hawkins' 1939 recording of "Body and Soul" marked the saxophone as an influence on jazz equal to the trumpet, which had been the defining instrument of jazz since its beginnings in New Orleans. But the greatest influence of the saxophone on jazz was to occur a few years later when alto saxophonist Charlie Parker became an icon of the bebop revolution that influenced generations of jazz musicians. The small-group format of bebop and post-bebop jazz ensembles gained ascendancy in the 1940s as musicians used the harmonic and melodic freedom pioneered by Parker, Dizzy Gillespie, Thelonious Monk, and Bud Powell in extended jazz solos.

During the 1950s, prominent alto players included Sonny Stitt, Cannonball Adderley, Jackie McLean, Lou Donaldson, Sonny Criss and Paul Desmond, while prominent tenor players included Lester Young, Coleman Hawkins, Dexter Gordon, John Coltrane, Sonny Rollins, Stan Getz, Zoot Sims, Lucky Thompson, Eddie "Lockjaw" Davis, and Paul Gonsalves. Serge Chaloff, Gerry Mulligan, Pepper Adams and Leo Parker brought the baritone saxophone to prominence as a solo instrument. Steve Lacy renewed attention to the soprano saxophone in the context of modern jazz and John Coltrane boosted the instrument's popularity during the 1960s. Smooth jazz musician Kenny G also uses the soprano sax as his principal instrument.

Saxophonists such as John Coltrane, Ornette Coleman, Sam Rivers, and Pharoah Sanders defined the forefront of creative exploration with the avant-garde movement of the 1960s. The new realms offered with Modal, harmolodic, and free jazz were explored with every device that saxophonists could conceive of. Sheets of sound, tonal exploration, upper harmonics, and multiphonics were hallmarks of the creative possibilities that saxophones offered. One lasting influence of the avant-garde movement is the exploration of non-Western ethnic sounds on the saxophone, for example, the African-influenced sounds used by Sanders and the Indian-influenced sounds used by Coltrane. The devices of the avant-garde movement have continued to be influential in music that challenges the boundaries between avant-garde and other categories of jazz, such as that of alto saxophonists Steve Coleman and Greg Osby.

Some ensembles such as the World Saxophone Quartet use the soprano-alto-tenor-baritone (SATB) format of the classical saxophone quartet for jazz. In the 1990s, World Saxophone Quartet founder Hamiet Bluiett formed the quartet Baritone Nation (four baritones).

The "jump swing" bands of the 1940s gave rise to rhythm and blues, featuring horn sections and exuberant, strong-toned, heavily rhythmic styles of saxophone playing with a melodic sense based on blues tonalities. Illinois Jacquet, Sam Butera, Arnett Cobb, and Jimmy Forrest were major influences on R&B tenor styles and Louis Jordan, Eddie "Cleanhead" Vinson, Earl Bostic, and Bull Moose Jackson were major influences on alto. The R&B saxophone players influenced later genres including rock and roll, ska, soul, and funk. Horn section work continued with Johnny Otis and Ray Charles featuring horn sections and the Memphis Horns, the Phenix Horns, and Tower of Power achieving distinction for their section playing. Horn sections were added to the Chicago and West Coast blues bands of Lowell Fulson, T-Bone Walker, B.B. King, and Guitar Slim. Rock and soul fusion bands such as Chicago, The Electric Flag, and Blood, Sweat, and Tears featured horn sections; others employed more rock & roll style players the likes of Clarence Clemons and Bobby Keys. Junior Walker, King Curtis and Maceo Parker became influential soul and funk saxophone stylists, preceding the more technical jazz-fusion and post-bop sounds of Michael Brecker and Bob Mintzer.

A number of experimental saxophones and saxophone-related instruments have appeared since Sax's original work, most with no lasting impact. During the early 1920s Reiffel & Husted of Chicago produced a slide soprano saxophone.
During the 1920s some straight alto and tenor saxophones were produced by Buescher, which proved cumbersome to handle and difficult to transport. Buescher custom produced one straight baritone saxophone as novelty instrument for a vaudeville performer.
C.G. Conn introduced two new variants in 1928–1929, the "Conn-O-Sax" and the mezzo-soprano saxophone keyed in F. The Conn-O-Sax is a straight-conical bore instrument in F (one step above the E alto) with a slightly curved neck and spherical bell. This instrument, which combines a saxophone bore and keys with a bell shaped similar to that of a heckelphone, was intended to imitate the timbre of the English horn and was produced only in 1929 and 1930. The instrument has a key range from low A to high G. Fewer than 100 Conn-O-Saxes are in existence and they are highly sought by collectors. The Conn mezzo-soprano experienced a similarly short production run as the economics of the Great Depression curtailed the market for what were regarded as novelty instruments. Most were expended by Conn as objects of repair training exercises.

The most successful of the unusual 1920s designs was the King "Saxello", essentially a straight B soprano, but with a slightly curved neck and tipped bell, made by the H. N. White Company. Such instruments now command prices up to $4,000 USD. Its lasting influence is shown in the number of companies, including Keilwerth, Rampone & Cazzani ("altello" model), L.A. Sax and Sax Dakota USA, marketing straight-bore, tipped-bell soprano saxophones as saxellos (or "saxello sopranos").

Interest in two 1920s variants was revived by jazz musician Rahsaan Roland Kirk, who called his straight Buescher alto a "stritch" and his Saxello a "manzello.". The Buescher straight alto was a production instrument while the manzello was in fact a Saxello with a custom-made large bell and modified keywork. More recently, the mezzo-soprano, or a modern variant of it, came into use by jazz musicians Anthony Braxton, James Carter, Vinny Golia, and Joe Lovano.

Some of the 1920s experimental designs, in addition to the Saxello, provide the basis for similar instruments produced during the modern era. Straight altos and tenors have been revived by Keilwerth, L.A. Sax and Sax Dakota USA. A mezzo-soprano in the key of G has been produced by Danish woodwind technician Peter Jessen, most notably played by Joe Lovano. This instrument is more in the timbral quality of Bb soprano saxophone.

The "contralto" saxophone, similar in size to the orchestral c-soprano, was developed in the late 20th century by California instrument maker Jim Schmidt. This instrument has a larger bore and a new fingering system, and does not resemble the orchestral instrument except for its key and register.
Benedikt Eppelsheim, of Munich, Germany has introduced recent innovations at the upper and lower ends of the saxophone range. The soprillo sax is a piccolo-sized straight instrument with the upper speaker hole built into the mouthpiece. The instrument, which extends Sax's original family, is pitched a full octave higher than the B soprano sax. 
The tubax, developed in 1999 by Eppelsheim, plays the same range and with the same fingering as the E contrabass saxophone; its bore, however, is narrower than that of a contrabass saxophone, resulting in a more compact instrument with a "reedier" tone (akin to the double-reed contrabass sarrusophone). It can be played with the smaller (and more commonly available) baritone saxophone mouthpiece and reeds. Eppelsheim has also produced subcontrabass tubaxes in C and B, the latter being the lowest saxophone ever made.

Among the 2000s developments is the aulochrome, a double soprano saxophone invented by Belgian instrument maker François Louis in 2001.

Since the 1950s, saxophones with non-metallic bodies have occasionally been in production. Such instruments have failed to gain acceptance over a number of issues including durability, repairability, and deficiencies in key action and tone. The best known of these efforts is the 1950s Grafton acrylic alto saxophone used briefly by Charlie Parker and Ornette Coleman. It had a production run of over 10 years as a budget model saxophone. The polycarbonate Vibratosax is in production as a low cost alternative to metal saxophones. Wooden Sawat saxophones are made in Thailand on a small scale. Opinions vary on the significance of body materials to sound.

The fingering scheme of the saxophone, which has had only minor changes since the instrument's original invention, has presented inherent acoustic problems related to closed keys below the first open tonehole that affect response of, and slightly muffle, some notes. There is also a lack of tactile consistency between key centers, requiring extra effort from the player to adjust modes of muscle memory when moving between key centers. Two efforts to remedy the acoustic problems and awkward aspects of the original fingering system are noteworthy.

The Leblanc Rationale and System saxophones have key mechanics designed to remedy the acoustic problems associated with closed keys below the first open tonehole. They also enable players to make half-step shifts of scales by depressing one key while keeping the rest of the fingering consistent with that of the fingering a half step away. Some Leblanc System features were built into the Vito Model 35 saxophones of the 1950s and 1960s. Despite the advantages of that system, acceptance was impaired by the expense and mechanical reliability issues related to the complexity of certain key mechanisms.

The chromatic or linear fingering, saxophone is a project of instrument designer and builder Jim Schmidt, developing a horn maximizing tactile and logical consistency between every interval regardless of the key, and avoiding the acoustic problems associated closed keys below the first open tone hole. Several working prototypes have been built and presented at trade shows. Production of this original and expensive saxophone is on an individual order basis.

Inexpensive keyless folk versions of the saxophone made of bamboo (recalling a chalumeau) were developed in the 20th century by instrument makers in Hawaii, Jamaica, Thailand, Indonesia, Ethiopia, and Argentina. The Hawaiian instrument, called a xaphoon, was invented during the 1970s and is also marketed as a "bamboo sax", although its cylindrical bore more closely resembles that of a clarinet, and its lack of any keywork makes it more akin to a recorder. Jamaica's best known exponent of a similar type of homemade bamboo "saxophone" was the mento musician and instrument maker 'Sugar Belly' (William Walker). In the Minahasa region of the Indonesian island of Sulawesi, there exist entire bands made up of bamboo "saxophones" and "brass" instruments of various sizes. These instruments are imitations of European instruments, made using local materials. Similar instruments are produced in Thailand.

In Argentina, Ángel Sampedro del Río and Mariana García have produced bamboo saxophones of various sizes since 1985, the larger of which have bamboo keys to allow for the playing of lower notes.

Many synthesizer wind controllers are played and fingered like a saxophone.





</doc>
<doc id="26797" url="https://en.wikipedia.org/wiki?curid=26797" title="Sackbut">
Sackbut

A sackbut is a type of trombone from the Renaissance and Baroque eras, characterised by a telescopic slide that is used to vary the length of the tube to change pitch. Unlike the earlier slide trumpet from which it evolved, the sackbut possesses a U-shaped slide, with two parallel sliding tubes, which allows for playing scales in a lower range.

Records of the term "trombone" predates the term "sackbut" by two decades, and evidence for the German term "Posaune" is even older. "Sackbut", originally a French term, was used in England until the instrument fell into disuse in the eighteenth century; when it returned, the Italian term "trombone" became dominant. In modern English, an older trombone or its replica is called a sackbut.

An older instrument generally differs from modern trombones by its smaller, more cylindrically-proportioned bore, and its less-flared bell. The bell section was more resonant (since it did not contain the tuning slide and was loosely stayed rather than firmly braced to itself). These traits produce a "covered, blended sound which was a timbre particularly effective for working with voices... zincks and crumhorns", as in an alta capella.

The revived instrument had changed in specific ways. In the mid-18th century, the bell flare increased, crooks fell out of use, and flat, removable stays were replaced by tubular braces. The new shape produced a stronger sound, suitable to open-air performance in the marching bands where trombones became popular again in the 19th century. Before the early 19th century, most trombones adjusted tuning with a crook on the joint between the bell and slide or, more rarely, between the mouthpiece and the slide, rather than the modern tuning slide on the bell curve, whose cylindrical sections prevent the instrument from flaring smoothly through this section. Older trombones also generally don't have water keys, stockings, a leadpipe, or a slide lock, but as these parts are not critical to sound, replicas may include them. Bore size remained variable, as it still is today.

The first reference to a slide instrument was probably "trompette des ménestrels", first found in Burgundy in the 1420s and later in other regions of Europe. The name distinguished the instrument from the "trompettes de guerre" (war trumpets), which were of fixed length.

The next word to appear in the 15th century that implied a slide was the "sackbut" group of words. There are two theories for the sources: it is either derived from the Middle French "sacquer" (to pull) and "bouter" (to push) or from the Spanish "sacar" (to draw or pull) and "bucha" (a tube or pipe). The term survives in numerous English spelling variations including sacbut, sackbutte, sagbut, shagbolt, sacabushe, shakbusse and shakbusshe.

Closely related to "sackbut" was the name used in France: "sacqueboute" and in Spain, where it was "sacabuche". These terms were used in England and France until the 18th century.

In Scotland in 1538 the slide instrument is referred to as "draucht trumpet" (drawn trumpet) as opposed to a "weir trumpet" (war trumpet), which had a fixed length.

In Germany, the original word was "Posaune", appearing about 1450 and is still used today. This (as well as "bason") derives from "busine," which is Latinate and meant straight trumpet.

In Italy it was (and remains) "trombone", which derived from trumpet in the Latin "tromba" or "drompten", used in the Low Countries. The first records of it being used are around 1440, but it is not clear whether this was just a nickname for a trumpet player. In 1487 a writer links the words "trompone" and "sacqueboute" and mentions the instrument as playing the contratenor part in a danceband.

The trombone developed from the trumpet. Up until 1375 trumpets were simply a long straight tube with a bell flare.

There are various uses of "sackbut"-like words in the Bible, which has led to a faulty translation from the Latin bible that suggested the trombones date back as far as 600 BC, but there is no evidence of slides at this time.

From 1375 the iconography sees trumpets being made with bends, and some in 'S' shapes. Around 1400 we see the "loop"-shaped trumpet appear in paintings and at some point in the 15th century, a single slide was added. This slide trumpet was known as a "trompette des ménestrels" in the alta capella bands.

The earliest clear evidence of a double slide instrument is in a fresco painting by Filippino Lippi in Rome, "The Assumption of the Virgin", dating from 1488–93.

From the 15th to the 19th centuries, the instrument designs changed very little overall, apart from a slight widening of the bell in classical era. Since the 19th century, trombone bore sizes and bells have increased significantly.

It was one of the most important instruments in Baroque polychoral works, along with the cornett and organ.

Sackbuts come in several sizes. According to Michael Praetorius, these were:

The pitch of the trombones has (notionally) moved up a semi-tone since the 17th century, and this is explained in the section on pitch.

Because the tenor instrument is described as "Gemeine" (common or ordinary), this is probably the most widely used trombone.

The basses, due to their longer slides, have a hinged handle on the slide stay, which is used to reach the long positions.

The giant Octav-Posaun / double bass trombone / contra-bass trombone in the style of those made in 16th/17th centuries is represented by only a few existing instruments. There is an original instrument made by Georg Nicolaus Öller built in Stockholm in 1639 and housed in the Scenkonstmuseet. In addition, Ewald Meinl has made a modern copy of this instrument, and it is currently owned and played by Wim Becu.

The bore size of renaissance/baroque trombones is approximately and the bell rarely more than in diameter. This compares with modern tenor trombones, which commonly have bores to and bells to .

Modern reproductions of sackbuts sacrifice some authenticity to harness manufacturing techniques and inventions that make them more comfortable for modern players, while retaining much of the original character of the old instruments.

Some original instruments could be disassembled into the constituent straight tubes, bowed tubes, bell flare, and stays, with ferrules at the joints. Mersenne has a diagram. (Little imagination is needed to see how it could be reassembled—with an extra tube—into something approaching a natural trumpet.) There is a debate as to whether they used tight fittings, wax or another joining substance. Modern sackbut reproductions are usually soldered together. Some modern sackbut reproductions use glue as a compromise to give a loose fitting for high resonance without risk of falling apart.

Tuning slides came in during the very late 18th century. Early trombonists adjusted pitch with the slide, and by adding variously shaped and sized crooks. Modern reproductions often have a bell bow tuning slide or telescopic slide between the slide and bell sections. Crooks are still used, as are variously sized bell bow sections for larger changes.

The stays on period sackbuts are flat. While the bell stay remained flat, from about 1660 the slide stays became tubular. On many modern reproductions round slide stays are much more comfortable to play and easier to make.

A loose connection between the bell stay and the bell is thought key to a resonant bell, and thus a better sackbut sound. Original instruments have a hinge joint. Modern copies with a tuning slide in the bell can need more support for operation of the slide, so either an extra stay by the tuning slide is provided or a joint without play in only one axis is employed.

The original way to make the slide tubes was to roll a flat piece of metal around a solid cylinder mandrel, and the joining edges soldered together. Modern manufacturers now draw the tubes. They also tend to have stockings, which were only invented around 1850. In addition, modern made slides are usually made of nickel silver with chrome plating, giving a smoother finish and quieter action than simply the brass that would have originally been used.

The water key was added in the 19th century, but modern reproductions often have them.

Until some time in the 18th century, the trombone was in A and the pitch of that A was about a half-step higher than it is today—460–480 Hz. There was a transition around the 18th century when trombones started to be thought of in B at around 440 Hz. This change did not require a change in the instrument, merely a new set of slide positions for each note. But it does mean that the baroque and renaissance repertoire was intended to be played at the higher pitch. There are many examples of evidence for this:



The tenor trombones that survive are pitched closest to B at A=440 Hz, which is the same as A at A=466 Hz. So what we now think of as a tenor trombone with B in first position, pitched at A=440 was actually thought of as a trombone in A (in first position), pitched at A=466. Surviving basses in D at A=466 (E at 440)—for example: Ehe, 1612 (Leipzig) and Hainlein, c.1630 (Nuremberg) confirm Praetorius' description. It is also worth noting that Rognoni's "Suzanne ung jour" setting descends repeatedly to BB, which is a tone lower than the lowest note playable on a bass in F; on a bass in D, it falls in (modern) fifth position.

Many groups now perform at A=466 Hz for the sake of greater historical accuracy.

The sackbut was described as suitable for playing with the 'loud' ensembles in the outdoors, as well as the 'soft' ensembles inside.

The alta capella bands are seen in drawings as entertaining outside with ensembles including shawms, trumpets and trombones. When pushed, sackbuts can easily make a loud and brassy sound.

The sackbut also responds very well to rather soft playing—more so than a modern trombone. The sound is characterized by a more delicate, vocal timbre. The flat rims and shallow cups of the older mouthpieces are instrumental in providing the player with a much wider palette of articulations and tonal colours. This flexibility lends itself to a vocal style of playing and facilitates very characterful phrasing.

Mersenne wrote in 1636, "It should be blown by a skillful musician so that it may not imitate the sounds of the trumpet, but rather assimilate itself to the sweetness of the human voice, lest it should emit a warlike rather than a peaceful sound."

The Lorenzo da Lucca was said to have had "in his playing a certain grace and lightness with a manner so pleasing".

Musicians of the 16th and 17th centuries benefited from a broader base of skills than the average performer today.

These traditions continued into the baroque with musicians expected to give expression to the written music by ornamenting with a mixture of one-note “graces” and whole passage “divisions” (also known as “diminutions”). The suggestions for producing effective ornaments without disrupting the line and harmony are discussed alongside countless examples in the 16th and early 17th century Italian division tutors. Graces such as the accento, portar della voce, tremolo, groppo, trillo, esclamationo and intonatio are all to be considered by performers of any music in this period.

“Cornetts and trombones...play divisions that are neither scrappy, nor so wild and involved that they spoil the underlying melody and the composer's design: but are introduced at such moments and with such vivacity and charm that they give the music the greatest beauty and spirit”
Bottrigari, Venice 1594

Along with the improvisation, many of these tutors discuss articulation. Francesco Rognoni in 1620 describes the tonguing as the most important part of producing “a good and beautiful effect in playing wind instruments, and principally the cornett” (which of course had a very similar role to the trombone). The treatises discuss the various strengths of consonants from “le” through “de” to “te”. But the focus of the text is for playing rapid notes “similar to the gorgia of the human voice” with “soft and smooth” double tonguing (“lingua riversa”) using “le re le re”. This is opposed to using “te che te che,” which is described as “harsh, barbarous and displeasing”. The natural ‘pairing’ of notes these articulations provide is similar to the instructions for string players who are instructed to slur (“lireggiar”) pairs of eighth notes with one bow stroke per quarter beat.

Another integral part of the early music sound-world is the musical temperament. Music in the middle-ages favours intervals of the fourth and fifth, which is why Pythagorean tuning was used. The interval of a third was used as a clash until the Renaissance, when it became consonant in compositions, which went hand-in-hand with the widespread use of meantone temperament. During the 17th century, Well temperament began to become more and more popular as the range of keys increased. Temperament affects the colour of a composition, and therefore modern performances, typically employing equal temperament, may not be true representations of the composers' intentions.
These old tunings are the result of the natural harmonic series of a brass instrument such as the sackbut. 
As the bell is smaller than a modern trombone, the harmonic series is closer to a perfect harmonic series, which is the basis for just tuning. Without adjusting the slide, the first to second harmonic is a perfect octave, second to third harmonic is a fifth slightly wider than equal temperament and fourth to fifth harmonic is a major third slightly narrower than in equal temperament. These adjusted intervals make chords ring and are the basis of meantone. In fact, Daniel Speer says “Once you have found a good C (third position), this is also the place you will find your F.” Playing a sounding C and F in exactly the same position on a modern orchestra sounds out of tune, but it tunes perfectly well on in a sackbut choir if everyone plays natural harmonics.

Plenty of musical understanding can be gathered from reading the original music print. Publishers such as SPES and Arnaldo Forni Edition provide facsimile copies of plenty of music for trombone from this era. To read these it one needs to become familiar with the old clefs, time signatures, ligatures and notational conventions of the era.

The sound of sackbuts (and trombones) has long been thought especially solemn and noble, had an association with death and the afterlife. The instrument was a symbol of divine presence, the voice of the angels and instrument of judgment. This symbolism can be seen, for instance, in "L'Orfeo", "Alceste", "The Magic Flute", the "Death March" from "Saul", and funeral aequales.

This association was probably encouraged by the lack of distinction made between natural horns, slide trumpets, and trombones in this Renaissance; they were used and often named interchangeably. Martin Luther's 1534 translation of the Bible into German renders the Greek "shophar" and "salpigx" to "Posaune". "Posaune" at the time could refer to a natural horn or other brass instrument, but it later came to mean exclusively "trombone" (similarly, English translations generally have "trumpet", and only occasionally "horn" or "shofar"). This gives the later reader of the Luther Bible texts such as: “…we shall all be changed, in a moment, in the twinkling of an eye, at the last trombone; for the trombone shall sound and the dead shall be raised incorruptible” (1 Corinthians 15:52).

The sackbut replaced the slide trumpet in the 15th century alta capella wind bands that were common in towns throughout Europe playing courtly dance music. See Waits.

Another key use of the trombone was in ceremonies, in conjunction with the trumpet. In many towns in Germany and Northern Italy, 'piffari' bands were employed by local governments throughout the 16th century to give regular concerts in public squares and would lead processions for festivals. Piffari usually contained a mix of wind, brass and percussion instruments and sometimes viols.

Venice's doge had his own piffari company and they gave an hour-long concert in the Piazza each day, as well as sometimes performing for services in St. Mark's. Each of the six confraternities in Venice also had their own independent piffari groups too, which would all play at a lavish procession on the feast of Corpus Domini. These groups are in addition to the musicians employed by St. Mark's to play in the balconies with the choir (the piffari would play on the main level).

It also was used in church music both for instrumental service music and as a doubling instrument for choral music. The treble and high alto parts were most often played by cornetts or shawms, with the violin sometimes replacing the cornett in 17th century Italian music.

The first record of trombones being used in churches was in Innsbruck 1503. Seville Cathedral's records show employment of trombonists in 1526, followed by several other Spanish cathedrals during the 16th century, used not only for ceremonial music and processionals, but also for accompaniment of the liturgical texts as well, doubling voices.

The sacred use of trombones was brought to a fine art by the Andrea Gabrieli, Giovanni Gabrieli and their contemporaries c.1570-1620 Venice and there is also evidence of trombonists being employed in churches and cathedrals in Italy at times during the second half of the 16th century in Bologna, Rome, Padua, Mantua and Modena.

Since ensembles had flexible instrumentation at this time, there is relatively little music before Giovanni Gabrieli's publication "Symphoniae sacrae" (1597) that specifically mentions trombones. The only example currently known is the music by Francesco Corteccia for the Medici wedding 1539.

The 17th century brings two pieces of real solo trombone repertoire.

Giovanni Martino Cesare wrote "La Hieronyma," (Musikverlag Max Hieber, MH6012) the earliest known piece for accompanied solo trombone. It comes from Cesare's collection "Musicali Melodie per voci et instrumenti a una, due, tre, quattro, cinque, e sei" published in Munich 1621 of 28 pieces for a mixture of violins, cornetts, trombone, vocal soloists and organ continuo. The collection also contains "La Bavara" for four trombones.

The other solo trombone piece of the 17th century, "Sonata trombone & basso" (modern edition by H Weiner, Ensemble Publications), was written around 1665. This anonymous piece is also known as the 'St. Thomas Sonata' because it was kept in the library of the Saint Thomas Augustinian Monastery in Brno, Czech Republic.

Francesco Rognoni was another composer who specified the trombone in a set of divisions (variations) on the well-known song "Suzanne ung jour" (London Pro Musica, REP15). Rognoni was a master violin and gamba player whose treatise "Selva di Varie passaggi secondo l'uso moderno" (Milan 1620 and facsimile reprint by Arnaldo Forni Editore 2001) details improvisation of diminutions and Suzanne is given as one example. Although most diminutions are written for organ, string instruments or cornett, Suzanne is "per violone over Trombone alla bastarda". With virtuosic semiquaver passages across the range of the instrument, it reflects Praetorius' comments about the large range of the tenor and bass trombones, and good players of the Quartposaune (bass trombone in F) could play fast runs and leaps like a viola bastarda or cornetto. The term "bastarda" describes a technique that made variations on all the different voices of a part song, rather than just the melody or the bass: "considered illegitimate because it was not polyphonic".

In the 17th century, a considerable repertoire of chamber music using sackbut with various combinations of violins, cornetts and dulcians, often with continuo, appeared. Composers included Dario Castello, Giovanni Battista Fontana, Giovanni Paolo Cima, Andrea Cima, Johann Heinrich Schmelzer and Matthias Weckmann.
Antonio Bertali wrote several trio sonatas for 2 violins, trombone and bass continuo in the mid-17th century. One such "Sonata a 3" is freely available in facsimile form from the Düben Collection website hosted by Uppsala universitet. A "Sonata a3 in C" is published by Musica Rara and attributed to Biber, although the authorship is unclear and it is more likely to have been written by Bertali.

Dario Castello, a wind player at St. Mark's Venice in the early 17th century had two books of "Sonate Concertate" published in 1621 and 1629. The sonatas of 1-4 parts with bass continuo often specify trombones, as well as cornett, violin and bassoon. The numerous reprints during the 17th century affirm his popularity then, as perhaps now.

Giuseppe Scarani joined St. Mark's Venice in 1629 as a singer and in the following year published "Sonate concertate", a volume of works for 2 or 3 (unspecified) instruments (and b.c.). The title has been suggested was chosen to try and capture some of Castello's success.

Tiburtio Massaino wrote a Canzona for eight trombones, published in Raverio's 1608 collection.

Johann Heinrich Schmelzer wrote several sonatas that included trombones—such as his "Sonata à 7" for two cornetts, two trumpets, three trombones, and basso continuo.

Daniel Speer published a four-part sonata in "Neu-gebachene Taffel-Schnitz" (1685). In 1687, Speer published the first written instruction in sackbut (and several other instruments) playing: "Grund-richtiger/kurtz/leicht und noethiger Unterricht der Musicalischen Kunst". The second edition in 1697 provides two three part sonatas for trombones.

An English work of note from this period is Matthew Locke's "Music for His Majestys Sagbutts and Cornetts", a suite for Charles II's coronation 1661.

Non-serious music, often based on dances for festive occasions, rarely had specified instrumentation. Often you find something like "per diversi musici". Indeed, the groups that would perform them would often be full of multi-instrumentalists.

Johann Pezel wrote for Stadtpfeifer with his "Hora decima musicorum" (1670), containing sonatas, as well as "Fünff-stimmigte blasende Music" (1685) with five-part intradas and dance pieces.

Well known pieces from Germany includes Samuel Scheidt's "Ludi Musici" (1621) and Johann Hermann Schein's "Banchetto musicale" (1617).

The first English piece scored for trombone is John Adson's "Courtly Masquing Ayres" (1611). Another light collection suitable for including trombones is Anthony Holborne's "Pavans, Galliards, Allmains, and other short Aeirs both Grave and Light in Five Parts for Viols, Violins or Other Musicall Winde Instruments" (1599).

Trombonists were in the regular ensemble at St. Mark's Venice from its formation in 1568 until they left the payroll in 1732. The first two ensemble directors—"maestro di concerti"—Girolamo Dalla Casa (1568–1601) and Giovanni Bassano (1601–1617)—were cornett players and the nucleus of the group was two cornetts and two trombones, although for the larger ceremonies many extra players were hired. During a mass attended by the Doge, evidence suggests they would have played a canzona in the Gradual after the Epistle and the Agnus Dei, a sonata in the Offertory as well as reinforcing vocal parts or substituting for absent singers.

This ensemble was used extensively by Giovanni Gabrieli in pieces substantially for brass, voices and organ in Venice up until his death in 1612. He was greatly influential in Venetian composers in other churches and confraternities, and his early baroque and cori spezzati style is seen in contemporaries like Giovanni Picchi and Giovanni Battista Grillo.

It is suggested that Monteverdi wrote his "Vespro della Beata Vergine" (1610) as a pitch for employment at St. Mark's as successor to Giovanni Gabrieli. In addition to the Magnificat, two movements specify trombones: the opening "Deus in adiutorium" is for six voices, two violins, two cornetts, three trombones, five viole da braccio and basso continuo; Sonata sopra "Sancta Maria, ora pro nobis" is for soprano, two violins, two cornetts, three trombones (one of which can be a viola da braccio) and basso continuo. Monteverdi also leaves the option to use trombones as part of the "sex instrumentis" of the "Dixit Dominus" and in the instrumental "Ritornello a 5" between verses of "Ave maris stella".

From around 1617, when the "maestro de' concerti" at St. Marks changed to violinist Francesco Bonfante and correspondingly the ensemble changed from basically a brass ensemble to being more evenly mixed with brass, wind and string instruments.

Monteverdi arrived at St. Mark's in 1613 and it is unsurprising that he includes trombones and strings for several more sacred works during his time here, published in his "Selva morale e spirituale" 1641. Of the c.40 items in this collection, six specify three or four trombones (or viola da braccio, ad lib): SV268 Beatus vir I, SV263 Dixit Dominus I, SV263 Dixit Dominus II, SV261 Et iterum venturus est, SV258 Gloria in excelsis Deo, SV281 Magnificat I. Each is for 3-8 voices with 3 violins (apart from SV261), the trombones/violas and basso continuo. Monteverdi also specified trombones in two more sacred works: SV198 Laetatus sum (i) (1650) for 6 voices, 2 violins, 2 trombones and bassoon and SV272 Laudate Dominum omnes gentes I (1641) for 5 voices ‘concertato’, 4 voice chorus ad lib, 4 viola da braccio or trombones and basso continuo.

A prolific composer for trombones in Germany in the 17th century was Heinrich Schütz. His "Fili me, Absalon" (SWV 269) and "Attendite, popule meus" (SWV 270), are both scored for bass voice, four trombones (of which two are optionally violins) and basso continuo, are well known. They are part of his first "Symphoniae Sacrae" collection dating from 1629 and commentators have noted that the style reflects his studies in Venice with Giovanni Gabrieli 1609-1612. Other pieces that specify trombones (according to Grove) are (grouped by the collection they were published in): Concert mit 11 Stimmen (1618): SWV 21, in "Psalmen Davids" (Psalms of David) Op. 2 (1619): SWV 38, 40-46, Symphoniae sacrae I Op.6 (1629): SWV 259, 269-271, 274, Symphoniae sacrae II Op.10 (1647): SWV 344, Symphoniae sacrae III Op. 12 (1650): SWV 398a, Historia (1664): SWV 435, 448, 449, 453, 461, 452, 466-470, 473, 474-476, Schwanengesang Psalm 119 (1671): SWV 500, although many others are suitable for trombones too.

Johann Hermann Schein specified trombones in some of his sacred vocal works in the "Opella nova, ander Theil, geistlicher Concerten" collection (Leipzig, 1626). For example, "Uns ist ein Kind geboren" is scored for violino, traversa, alto trombone, tenor voice, fagotto and basso continuo. "Mach dich auf, werde licht, Zion" uses Canto 1: violino, cornetto, flauto picciolo e voce, Canto 2: voce e traversa, Alto: Trombone e Voce, Tenore: Voce e Trombone, Basso: Fagotto Trombone e Voce and Basso Continuo, during which solos for each of the trombonists are specified. Of particular interest is "Maria, gegrüsset seist du, Holdselige," which uses soprano and tenor voices, alto trombone, 2 tenor trombones and on the bass line "trombone grosso," which goes down to pedal A, and a couple of diatonic scale passages from bottom C.

German composer Johann Rudolf Ahle wrote some notable sacred pieces for voices and trombones. "Höre, Gott" uses five favoriti singers, two ripieno choirs (which double other parts at intense moments) and seven trombones, with basso continuo. And his most famous "Neu-gepflanzte Thüringische Lust-Garten.." (1657–65) contains several sacred works with 3 or 4 trombones, including "Magnificat a 8" for SATB soloists, cornett, 3 trombones and continuo and "Herr nun lässestu deinen Diener a 5" for bass, 4 trombones and continuo.

Dieterich Buxtehude specifies trombones in a few sacred concertos using style derived from polychoral Venetian works and one secular piece. For example, "Gott fähret auf mit Jauchzen" (BuxWV33 from CW v, 44) is scored for SSB voices, 2 violins, 2 violas, trombones, 2 cornetts, 2 trumpets, bassoon and basso continuo.

There are a few vocal works involving trombones in works by Andreas Hammerschmidt. These include "Lob- und Danck Lied aus dem 84. Psalm" for 9 voices, 5 trumpets, 3 trombones, 5 violas and basso continuo (Freiberg, 1652). There is also "Hochzeitsgesang für Daniel Sartorius: Es ist nicht gut, dass der Mensch allein sei" for 5 voices, 2 violins, 2 trombones, bassoon and basso continuo.

Johann Schelle has numerous sacred vocal works that use trombones. For instance "Vom Himmel kam der Engel Schar" is scored for soprano, tenor, SSATB choir, 2 violins, 2 violas, 2 cornetts, 3 trombones, 2 trumpets, timpani, basso continuo, and "Lobe den Herrn, meine Seele" is for two choirs of SSATB and similar instruments to the previous work.

The lesser known Austrian composer Christoph Strauss, Kapellmeister to the Habsburg Emperor Mathias 1616-1620, wrote two important collections for trombones, cornetts and voices. His motets published in Nova ac diversimoda sacrarum cantionum composition, seu motettae (Vienna, 1613) are in a similar tradition to Gabrieli's music. Of the sixteen motets in the collection, all are titled "concerto" apart from the "sonata" "Expectans Expectavi Dominum" for 6 trombones, cantus voice and tenor voice. In 1631 he published a number of masses, which were much more baroque, with basso continuo, rhetorical word painting and obligato usage of instruments.

Later in the 17th century, Heinrich Ignaz Franz Biber composed sacred works for voices and orchestra featuring trombones. His "Requiem" mass (1692) uses an orchestra of strings, 3 trombones and basso continuo. A similar ensemble accompanies 8 vocal lines in his "Lux perpetua" (c1673), and three more similar works in the 1690s.

Monteverdi ushers sackbuts into the first great opera, 'L'Orfeo' 1607. The orchestra at the first performance, as shown in the first publication, the list of "stromenti" at the front of the score specifies four trombones, but at one point in Act 3, however, the score calls for five trombones.

There is relatively little repertoire for the trombone in the late baroque.

Johann Sebastian Bach uses trombones in fourteen of his church cantatas—BWV 2, 3, 4, 21, 23, 25, 28, 38, 64, 68, 96, 101, 121, 135—as well as motet BWV 118. He uses the trombone sound to reflect the (by now) archaic sounds of the Renaissance trombones doubling voices (with cornett playing the soprano line), yet he also uses them independently, which John Eliot Gardiner says prepares the way for their use in Beethoven's Symphony No. 5. The cantatas were either composed in Leipzig during 1723-1725, or (for BWV 4, 21 & 23) the trombone parts were added to the existing cantata during the same period. The cornett and trombone parts would have been played by the Stadtpfeifer.

In England, George Frideric Handel includes trombones in three of his oratorios: "Saul" (1738), "Israel in Egypt" (1738) and "Samson" (1741). There are no other documented groups or performances with trombone players in England at this time, and it has been suggested that the premiers took place with a visiting group from Germany, as was the custom in Paris at this time.

Vienna's Imperial court used trombones in church music:

Johann Joseph Fux was Hofkapellmeister in Vienna from 1715 until 1741. Many of his masses use the choir strengthened by strings, cornetts and trombones, often with independent moments for the instrumentalists and sometimes. "Missa SS Trinitatis" uses two choirs, which again points to the traditions going back to Gabrieli. His highly successful Requiem is for five vocal parts, two cornetts, two trombones, strings and continuo. He also uses the trombone in smaller motets and antiphons, such as his setting of "Alma Redemptoris mater" for soprano, alto trombone, strings and continuo. Some of his chamber music involves trombones, as do many of his operas, used as an obbligato instrument.

Also in the Vienna court was Antonio Caldara, vice-kapellmeister 1717–1736. Among his output are two Holy Week settings as Da Capo arias: "Deh sciogliete, o mesti lumi" for soprano, unison violins, bassoon, two trombones and organ and "Dio, qual sia" for soprano, trombone, bassoon and basso continuo.

Again this period suffers from a lack of trombone players. Most of these works derive from Vienna and Salzburg.

Joseph Haydn uses trombones in "Il rotorno di Tobia", "Die sieben letzten Worte", "The Creation", "Die Jahreszeiten", "Der Sturm", "Orfeo ed Euridice" and secular cantata choruses.

Wolfgang Amadeus Mozart uses trombones in connection with death or the supernatural. This includes the Requiem (K626, 1791), Great Mass in C minor (K423, 1783), "Coronation Mass (C major)" (K317, 1779), several other masses, "Vesperae Solennes de Confessore" (K339, 1780), "Vesperae de Dominica", his arrangement of Handel's "Messiah" plus two of his three great operas: "Don Giovanni" (K527, 1787) and "Die Zauberflöte" (K620, 1791). Mozart's first use of the trombone was an obligato line in the oratorio "Die Schuldigkeit des ersten Gebots" (K35, 1767)

Christoph Willibald Gluck includes trombones in five of his operas: "Iphigénie en Aulide" (1774), Orfeo ed Euridice (1774), "Alceste" (1776), "Iphigénie en Tauride" (1779) and "Echo et Narcisse" (1779), as well as ballet "Don Juan" (1761).

Some chamber music in this period includes trombone in an obligato role with voice, and also as a concerto instrument with string orchestra. Composers include the likes of Leopold Mozart, Georg Christoph Wagenseil, Johann Albrechtsberger, Michael Haydn and Johann Ernst Eberlin.

For works for trombone post-1800, please see trombone.

Many groups specializing in period music make frequent and prominent use of the sackbut.

External links:



Plenty of recordings of the authentic sackbut are now available from the groups such as Concerto Palatino, HMSC, Gabrieli Consort and the Toulouse Sacqueboutiers. For a closer examination of the instrument, here are some recommended recordings where the sackbut is heavily featured in a "solo" capacity.

The earliest instruments:
Other notable sackbuts:
For more information, see Herbert (2006).







</doc>
<doc id="26798" url="https://en.wikipedia.org/wiki?curid=26798" title="Saxhorn">
Saxhorn

The saxhorn is a family of valved brass instruments that have conical bores and deep cup-shaped mouthpieces. The saxhorn family was developed by Adolphe Sax, who is also known for creating the saxophone family. The sound of the saxhorn has a characteristic mellow tone quality and blends well with other brass.

The saxhorns form a family of seven brass instruments (although at one point ten different sizes seem to have existed). Designed for band use, they are pitched alternately in E and B, like the saxophone group. 

Modern saxhorns still manufactured and in use:

Historically, much confusion exists as to the nomenclature of the various instruments in different languages.

The following table lists the members of the saxhorn family as described in the orchestration texts of Hector Berlioz and Cecil Forsyth, the J. Howard Foote catalog of 1893, and modern names. The modern instrument names continue to exhibit inconsistency, denoted by a "/" between the two names in use. In the table "Pitch" means the concert pitch of notational Middle C on each instrument (2nd partial, no valves depressed) in scientific pitch notation.

This list is not exhaustive of historic nomenclature for the saxhorns, for which there may exist no comprehensive and authoritative source.

The saxhorn is based on the same three-valve system as most other valved brass instruments. Each member of the family is named after the root note produced by the second partial with no valves actuated. Each member nominally possesses or possessed the typical three-valve brass range from the note one tritone below that root note (second partial, all valves actuated) to the note produced by eighth partial with no valves actuated, i.e., the note two octaves above the root note.

All the modern members of the family are transposing instruments written in the treble clef with the root note produced by the second partial with no valves actuated being written as middle C, though the baritone horn often plays bass clef parts, especially in concert band music and when playing parts written for the trombone.

Developed during the mid-to-late 1830s, the saxhorn family was patented in Paris in 1845 by Adolphe Sax. During the 19th century, the debate as to whether the saxhorn family was truly new, or rather a development of previously existing instruments, was the subject of prolonged lawsuits.

Throughout the mid-1850s, Sax continued to experiment with the instrument's valve pattern.

The Trojan March ("Marche Troyenne") of the Berlioz opera Les Troyens (185658) features an on-stage band which includes a family of saxhorns.

Saxhorns were popularized by the distinguished Distin Quintet, who toured Europe during the mid-19th century. This family of musicians, publishers and instrument manufacturers had a significant impact on the growth of the brass band movement in Britain during the mid- to late-19th century.

The saxhorn was the most common brass instrument in American Civil War bands. The over-the-shoulder variety of the instrument was used, as the backward-pointing bell of the instrument allowed troops marching behind the band to hear the music.

Contemporary works featuring this instrument are Désiré Dondeyne's "Tubissimo" for bass tuba or saxhorn and piano (1983) and Olivier Messiaen's "Et exspecto resurrectionem mortuorum" (1964).






</doc>
<doc id="26799" url="https://en.wikipedia.org/wiki?curid=26799" title="Scanner">
Scanner

Scanner may refer to:







Barbara Sher uses the word "scanner" for someone who scans the surface of things, as opposed to "divers" or experts.
Other words for scanner includes polymath, renaissance soul, multitalent, generalist and multipotentialite (as in Multipotentiality).



</doc>
<doc id="26800" url="https://en.wikipedia.org/wiki?curid=26800" title="Sonic Team">
Sonic Team

The initial team, formed in 1990, was composed of staff from Sega's Consumer Development division, including programmer Yuji Naka, artist Naoto Ohshima, and level designer Hirokazu Yasuhara. The team took the name Sonic Team in 1991 with the release of their first game, "Sonic the Hedgehog," for the Sega Genesis. The game was a major success and contributed to millions of Genesis sales. The next "Sonic" games were developed by Naka and Yasuhara in America at Sega Technical Institute, while Ohshima worked on "Sonic CD" in Japan. Naka returned to Japan in late 1994 to become the head of CS3, later renamed R&D No. 8. During this time, the division took on the Sonic Team brand but developed games that do not feature Sonic, such as "Nights into Dreams" (1996) and "Burning Rangers" (1998).

Following the release of "Sonic Adventure" in 1998, some Sonic Team staff moved to the United States to form Sonic Team USA and develop "Sonic Adventure 2" (2001). With Sega's divestiture of its studios into separate companies, R&D No. 8 became SONICTEAM Ltd. in 2000, with Naka as CEO and Sonic Team USA as its subsidiary. Sega's financial troubles led to several major structural changes in the early 2000s; the United Game Artists studio was absorbed by Sonic Team in 2003, and Sonic Team USA became Sega Studios USA in 2004. After Sammy Corporation purchased Sega in 2004, Sonic Team was reincorporated to become Sega's GE1 research and development department, later renamed CS2.

Naka departed Sonic Team during the development of "Sonic the Hedgehog" (2006), and Sega Studios USA was merged back into Sonic Team in 2008. The following decade was marked by "Sonic" titles of varying reception, with department head Takashi Iizuka acknowledging that Sonic Team had prioritized shipping over quality.

In 1983, programmer Yuji Naka was hired into Sega's Consumer Development division. His first project was "Girl's Garden", which he and Hiroshi Kawaguchi created as part of their training process. For his next game, "Phantasy Star" (1987) for the Master System, Naka created pseudo-3D animation effects. He met artist Naoto Ohshima while working on the game.

During the late 1980s and early 1990s, a rivalry formed between Sega and Nintendo due to the release of their 16-bit video game consoles: the Sega Genesis and the Super Nintendo Entertainment System. Sega needed a mascot character that would be as synonymous with their brand as Mario was with Nintendo. Sega wanted a killer app and character that could appeal to an older demographic than preteens, demonstrate the capabilities of the Genesis, and ensure commercial success in North America.

Some sources indicate Sega of Japan held an internal competition to submit characters designs for a mascot, while designer Hirokazu Yasuhara said the direction was given only to himself, Ohshima, and Naka. Ohshima designed a blue hedgehog named Sonic, who was inserted into a prototype game created by Naka. The Sonic design was refined to be less aggressive and appeal to a wider audience before the division began development on their platform game "Sonic the Hedgehog". According to Ohshima, Sega was looking for a game that would sell well in the United States as well as in Japan. Ohshima and Naka already had the game and character ready, with Ohshima having worked with Sega's toy and stationery department on design ideas. Ohshima claims that the progress they had already made encouraged the company to select their proposal, as theirs was the only team to have put in a high amount of time and effort. This left him confident their proposal would be selected.

The "Sonic the Hedgehog" project began with just Naka and Ohshima, but grew to involve two programmers, two sound engineers, and three designers. Yasuhara joined to supervise Naka and Ohshima and develop levels, and became the lead designer. He satisfied Naka's request for a simple, one-button design by having Sonic do damage by jumping. "Sonic the Hedgehog" was released in 1991 and proved a major success, contributing to millions of sales of the Genesis. The development team took the name Sonic Team for the game's release. Naka has referred to Sonic Team as only a "team name" at this point.

Shortly after the release of "Sonic the Hedgehog", Naka, Yasuhara, and a number of other Japanese developers relocated to California to join Sega Technical Institute (STI), a development division established by Mark Cerny intended as an elite studio combining the design philosophies of American and Japanese developers. While Naka and Yasuhara developed "Sonic the Hedgehog 2" with STI, Ohshima worked on "Sonic CD," a sequel for the Sega CD add-on. Though Naka was not directly involved in the "Sonic CD" development, he exchanged design ideas with Ohshima.

Following the release of "Sonic & Knuckles" in 1994, Yasuhara quit, citing differences with Naka. Naka returned to Japan, having been offered a role as a producer. He was placed in charge of Sega's Consumer Development Department 3, also known as CS3. Naka was reunited with Ohshima and brought with him Takashi Iizuka, who had also worked with Naka's team at STI. In the mid-1990s, Sonic Team started work on new intellectual property, leading to the creation of "Nights into Dreams" (1996) and "Burning Rangers" (1998) for the Sega Saturn. Naka stated that the release of "Nights" is when Sonic Team was truly formed as a brand.

The Saturn did not achieve the commercial success of the Genesis, so Sega focused its efforts on a new console, the Dreamcast, which debuted in Japan in 1998. The Dreamcast was seen as opportunity for Sonic Team to revisit the "Sonic" series, which had stalled in recent years. Sonic Team was originally creating a fully 3D "Sonic" game for the Saturn, but development moved to the Dreamcast to align with Sega's plans. Takashi Iizuka led the project; Iizuka had long wanted to create a "Sonic" role-playing video game and felt the Dreamcast was powerful enough to achieve his vision. The game became "Sonic Adventure", launched in 1998, which became the bestselling Dreamcast game.

Around this time, CS3 was renamed to Sega Research and Development Department 8 (R&D #8). While sometimes referred to as AM8 or "Sega-AM8", based on the R&D structure being titled the Sega Amusement Machine Research and Development (AM) teams, Sonic Team focused solely on home console games. Until 2000, media referred to Sonic Team's designation as both R&D #8 and AM8.

In 1999, shortly after the release of "Sonic Adventure", twelve Sonic Team members relocated to San Francisco to establish Sonic Team USA, while others remained in Japan. Shortly afterward, a number of key employees—including Ohshima—left Sega to form a new studio, Artoon. Sonic Team achieved success in the arcade game market in 1999 with the launch of rhythm game "Samba de Amigo", released the following year for the Dreamcast. The studio also began developing online games; in 1999, they released "ChuChu Rocket!", a puzzle game that made use of the Dreamcast's online capabilities. In 2000, Sonic Team launched the role-playing game "Phantasy Star Online" to critical and commercial success.

Sega began to restructure its studios as part of the dissolution of Sega Enterprises and spun off its software divisions into subsidiary companies. When the departments took new names, Naka felt it important to preserve the Sonic Team brand name, and the division's new legal name as a company was SONICTEAM, Ltd. Naka was installed as the CEO, and Sonic Team USA became a subsidiary of the new company.

Despite a number of well-received games, Sega discontinued the Dreamcast in 2001 and exited the hardware business. Sega transitioned into a third-party developer and began developing games for multiple platforms. From 2000, Sonic Team in Japan began to release fewer games, with a few releases such as the puzzle game "Puyo Pop" and the action game "Billy Hatcher and the Giant Egg". The company changes and lack of a Sega console affected Sonic Team; according to Naka, in a 2006 interview, "Our approach was always to create strategic title concepts, which included the hardware. We do somewhat miss the idea of being able to address these constant challenges."

Early in 2003, Sega president Hideki Sato and COO Tetsu Kamaya announced they were stepping down from their roles, with Sato being replaced by Hisao Oguchi, the head of Hitmaker. As part of Oguchi's restructuring plan, he announced his intention to consolidate Sega's studios into "four or five core operations." Sonic Team was financially solvent and absorbed United Game Artists, another Sega subsidiary led by Tetsuya Mizuguchi and known for the music games "Space Channel 5" (1999) and "Rez" (2001).

In 2004, Japanese company Sammy acquired a controlling interest in Sega and formed Sega Sammy Corporation. Prior to the merger, Sega began the process of re-integrating its subsidiaries into the main company. Sonic Team USA became Sega Studios USA, while SONICTEAM Ltd. became Sega's Global Entertainment 1 research and development division (GE1). The team is still referred to as Sonic Team. Naka announced his departure on 8 May 2006 and formed a new studio, Prope, to focus on creating original games. In a 2012 interview, Naka stated that a reason that he left the company was that he would have been required to continue making "Sonic" games, and he no longer wished to do that. He left Sonic Team during the development of the 2006 game "Sonic the Hedgehog" (2006), released as part of the 15-year anniversary of the "Sonic" franchise. Noted for its bugs and design flaws, "Sonic the Hedgehog" was panned, as was "Sonic Unleashed" (2008). Both games were released for the PlayStation 3 and Xbox 360; Sonic Team also developed a series of "Sonic" games for the Wii and Nintendo DS, such as 2007's "Sonic and the Secret Rings".

By 2010, Sonic Team became CS Research and Development No. 2 (CS2), Sega Studios USA was reintegrated into the Japanese team, and Iizuka was installed as the head of the department. After a series of poorly received "Sonic" releases, Sonic Team refocused on speed and more traditional side-scrolling in "" and "", "Sonic Generations", and "Sonic Colors", which all received better reviews. In 2015, Iizuka recognized in an interview with "Polygon" that Sonic Team had prioritized shipping games over quality, and had not had enough involvement in later third-party "Sonic" games, such as "". He hoped the Sonic Team logo would stand as a "mark of quality"; he planned to release quality games and expand the "Sonic" brand, while retaining the modern Sonic design. In another interview, Iizuka stated that Sonic Team was not involved in the "Sonic Boom" and that they were developed by "Sega of America from back in the day". Sonic Team's first "Sonic" game exclusive to smartphones, "Sonic Runners", was released in 2015. An endless runner, it was designed to have more replay value than other games in the genre. "Sonic Runners" received mixed reviews and was unprofitable, resulting in its discontinuation a year later.

In 2017, Sonic Team developed and released "Sonic Forces", and oversaw the development of "Sonic Mania" by Christian Whitehead. "Forces" was aimed at a broad audience of young and adult players, while "Mania" was focused on fans of the original Genesis games. "Mania" became the best reviewed "Sonic" game in fifteen years following nearly two decades of mixed reviews for the franchise. At SXSW in March 2019, Iizuka confirmed Sonic Team was also working on a new "Sonic" title.

Sega Studios USA, formerly Sonic Team USA, was a division of Sega and of Sonic Team while Sonic Team was a subsidiary company. It was founded when twelve Sonic Team members, including Takashi Iizuka, relocated to San Francisco, California, in 1999, and were set as a subsidiary of SONICTEAM Ltd. by 2000. The team worked on game development, translation, and market studies in the United States, until they returned to Japan and merged back into Sonic Team in 2008.

Sonic Team USA translated "Sonic Adventure" and tested "ChuChu Rocket!" in America before beginning work on "Sonic Adventure 2". They took inspiration from their location in San Francisco, as well as Yosemite National Park and other areas of the United States. "Sonic Adventure 2" was released on 20 June 2001, and was ported to the GameCube. The next Sonic Team USA project was "Sonic Heroes" (2003), the first "Sonic" game developed for multiple platforms. Sonic Team USA took a different approach with "Heroes" from the "Sonic Adventure" games, focusing on gameplay more similar to the Genesis games to which even casual gamers could adapt.

After SONICTEAM Ltd. merged back into Sega in 2004, Sonic Team USA was renamed Sega Studios USA. The division's next project was "Shadow the Hedgehog", released in 2005, a spin-off starring Shadow. Unlike previous games, "Shadow the Hedgehog" was targeted at older players and featured different gameplay styles, including the use of guns and different endings to the game. "Shadow the Hedgehog" was critically panned for its mature themes and level design, but was a commercial success, selling at least 1.59 million units.

The final Sega Studios USA game was "", the sequel to "Nights into Dreams" and the first "Nights" game since the cancellation of "Air Nights" in 2000. Iizuka felt it was important to retain the original game's concepts while developing new mechanics, and released it on the Wii, a more family-oriented console. "Journey of Dreams" was also designed to have a more European feel, in contrast to the "Sonic" games, which were more American. The sound and CGI were completed by Sonic Team in Japan, while Sega Studios USA handled the rest of the development for the 2007 release.

Sega Studios USA oversaw the development of "Sonic Rivals" (2006) and "Sonic Rivals 2" (2007) by Backbone Entertainment. In 2008, Sega Studios USA merged with Sonic Team, making Iizuka the head of Sonic Team and a vice president of product development for Sega. In 2016, Iizuka relocated to Los Angeles to oversee development with the goal of making Sega's studios in Los Angeles "a centralized hub for the global brand".

Sonic Team has developed a number of video games, with many of them becoming bestsellers. The studio is best known for its "Sonic the Hedgehog" series of platform games, which account for the majority of Sonic Team's work; the 1991 release of "Sonic the Hedgehog" is considered one of the most important moments in video game history, as it propelled Genesis sales and displaced Nintendo as the leading video game company. Sonic Team have also developed a wide variety of other games, including action games such as "Nights into Dreams", "Burning Rangers", and "Billy Hatcher and the Giant Egg," the online puzzle game "ChuChu Rocket!", the online role-playing game "Phantasy Star Online," and the music game "Samba de Amigo". "Phantasy Star Online" is credited for introducing online RPGs to consoles and was the first online RPG for many players. According to Sean Smith of "Retro Gamer", few companies could claim to have released as many AAA games over such a long period, especially between 1991 and 2000. Some Sonic Team games, such as the original "Sonic" games for the Genesis and "Nights", are considered some of the best video games ever made. Iizuka has said Sonic Team would be open to developing a third "Nights" game or a sequel to "Knuckles' Chaotix" (1995), if Sega were to commission them.

Sega and Sonic Team have been criticized for their handling of "Sonic the Hedgehog" after the beginning of the 3D era of video games. Edwin Evans-Thirlwell of "Eurogamer" described the 3D Sonic games as "20-odd years of slowly accumulating bullshit", and that unlike Sonic's main competitor, Nintendo's "Mario" series, "Sonic" in 3D never had a "transcendental hit". Zolani Stewart of "Kotaku" argued that Sonic's portrayal starting with "Sonic Adventure" with the addition of voice acting and a greater focus on plot and character narrative changed Sonic into "a flat, lifeless husk of a character, who spits out slogans and generally has only one personality mode, the radical attitude dude, the sad recycled image of vague '90s cultural concept." Sega of America marketing director Al Nilsen and "Sonic Mania" developer Christian Whitehead said they felt the number of additional characters added to the series was problematic, with Whitehead describing the characters as "padding". In 2015, Sega CEO Haruki Satomi acknowledged that Sega had "partially betrayed" the trust of the longtime fans of their games and hoped to focus on quality over quantity.


</doc>
<doc id="26805" url="https://en.wikipedia.org/wiki?curid=26805" title="Sex">
Sex

Organisms of many species are specialized into male and female varieties, each known as a sex. Sexual reproduction involves the combining and mixing of genetic traits: specialized cells known as gametes combine to form offspring that inherit traits from each parent. The gametes produced by an organism define its sex: males produce small gametes (e.g. spermatozoa, or sperm, in animals) while females produce large gametes (ova, or egg cells). Individual organisms which produce both male and female gametes are termed hermaphroditic. Gametes can be identical in form and function (known as isogamy), but, in many cases, an asymmetry has evolved such that two different types of gametes (heterogametes) exist (known as anisogamy).

Physical differences are often associated with the different sexes of an organism; these sexual dimorphisms can reflect the different reproductive pressures the sexes experience. For instance, mate choice and sexual selection can accelerate the evolution of physical differences between the sexes.

Among humans and other mammals, males typically carry an X and a Y chromosome (XY), whereas females typically carry two X chromosomes (XX), which are a part of the XY sex-determination system. Humans may also be intersex. Other animals have various sex-determination systems, such as the ZW system in birds, the X0 system in insects, and various environmental systems, for example in reptiles and crustaceans. Fungi may also have more complex allelic mating systems, with sexes not accurately described as male, female, or hermaphroditic.
One of the basic properties of life is reproduction, the capacity to generate new individuals, and sex is an aspect of this process. Life has evolved from simple stages to more complex ones, and so have the reproduction mechanisms. Initially the reproduction was a replicating process that consists in producing new individuals that contain the same genetic information as the original or parent individual. This mode of reproduction is called "asexual", and it is still used by many species, particularly unicellular, but it is also very common in multicellular organisms, including many of those with sexual reproduction. In sexual reproduction, the genetic material of the offspring comes from two different individuals. As sexual reproduction developed by way of a long process of evolution, intermediates exist. Bacteria, for instance, reproduce asexually, but undergo a process by which a part of the genetic material of an individual donor is transferred to another recipient.

Disregarding intermediates, the basic distinction between asexual and sexual reproduction is the way in which the genetic material is processed. Typically, prior to an asexual division, a cell duplicates its genetic information content, and then divides. This process of cell division is called mitosis. In sexual reproduction, there are special kinds of cells that divide without prior duplication of its genetic material, in a process named meiosis. The resulting cells are called gametes, and contain only half the genetic material of the parent cells. These gametes are the cells that are prepared for the sexual reproduction of the organism. Sex comprises the arrangements that enable sexual reproduction, and has evolved alongside the reproduction system, starting with similar gametes (isogamy) and progressing to systems that have different gamete types, such as those involving a large female gamete (ovum) and a small male gamete (sperm).

In complex organisms, the sex organs are the parts that are involved in the production and exchange of gametes in sexual reproduction. Many species, both plants and animals, have sexual specialization, and their populations are divided into male and female individuals. Conversely, there are also species in which there is no sexual specialization, and the same individuals both contain masculine and feminine reproductive organs, and they are called hermaphrodites. This is very frequent in plants.

Sexual reproduction first probably evolved about a billion years ago within ancestral single-celled eukaryotes. The reason for the evolution of sex, and the reason(s) it has survived to the present, are still matters of debate. Some of the many plausible theories include: that sex creates variation among offspring, sex helps in the spread of advantageous traits, that sex helps in the removal of disadvantageous traits, and that sex facilitates repair of germ-line DNA.

Sexual reproduction is a process specific to eukaryotes, organisms whose cells contain a nucleus and mitochondria. In addition to animals, plants, and fungi, other eukaryotes (e.g. the malaria parasite) also engage in sexual reproduction. Some bacteria use conjugation to transfer genetic material between cells; while not the same as sexual reproduction, this also results in the mixture of genetic traits.

The defining characteristic of sexual reproduction in eukaryotes is the difference between the gametes and the binary nature of fertilization. Multiplicity of gamete types within a species would still be considered a form of sexual reproduction. However, no third gamete type is known in multicellular plants or animals.

While the evolution of sex dates to the prokaryote or early eukaryote stage, the origin of chromosomal sex determination may have been fairly early in eukaryotes (see evolution of anisogamy). The ZW sex-determination system is shared by birds, some fish and some crustaceans. XY sex determination is used by most mammals, but also some insects, and plants ("Silene latifolia"). The X0 sex-determination is found in most arachnids, insects such as silverfish (Apterygota), dragonflies (Paleoptera) and grasshoppers (Exopterygota), and some nematodes, crustaceans, and gastropods.

No genes are shared between the avian ZW and mammal XY chromosomes, and from a comparison between chicken and human, the Z chromosome appeared similar to the autosomal chromosome 9 in human, rather than X or Y, suggesting that the ZW and XY sex-determination systems do not share an origin, but that the sex chromosomes are derived from autosomal chromosomes of the common ancestor of birds and mammals.
A paper from 2004 compared the chicken Z chromosome with platypus X chromosomes and suggested that the two systems are related.

Sexual reproduction in eukaryotes is a process whereby organisms produce offspring that combine genetic traits from both parents. Chromosomes are passed on from one generation to the next in this process. Each cell in the offspring has half the chromosomes of the mother and half of the father.
Genetic traits are contained within the deoxyribonucleic acid (DNA) of chromosomes—by combining one of each type of chromosomes from each parent, an organism is formed containing a doubled set of chromosomes. This double-chromosome stage is called "diploid", while the single-chromosome stage is "haploid". Diploid organisms can, in turn, form haploid cells (gametes) that randomly contain one of each of the chromosome pairs, via meiosis. Meiosis also involves a stage of chromosomal crossover, in which regions of DNA are exchanged between matched types of chromosomes, to form a new pair of mixed chromosomes. Crossing over and fertilization (the recombining of single sets of chromosomes to make a new diploid) result in the new organism containing a different set of genetic traits from either parent.

In many organisms, the haploid stage has been reduced to just gametes specialized to recombine and form a new diploid organism. In plants the diploid organism produces haploid spores that undergo cell division to produce multicellular haploid organisms known as gametophytes that produce haploid gametes at maturity. In either case, gametes may be externally similar, particularly in size (isogamy), or may have evolved an asymmetry such that the gametes are different in size and other aspects (anisogamy).
By convention, the larger gamete (called an ovum, or egg cell) is considered female, while the smaller gamete (called a spermatozoon, or sperm cell) is considered male. An individual that produces exclusively large gametes is female, and one that produces exclusively small gametes is male. An individual that produces both types of gametes is a hermaphrodite; in some cases hermaphrodites are able to self-fertilize and produce offspring on their own, without a second organism.

Most sexually reproducing animals spend their lives as diploid, with the haploid stage reduced to single-cell gametes. The gametes of animals have male and female forms—spermatozoa and egg cells. These gametes combine to form embryos which develop into a new organism.

The male gamete, a spermatozoon (produced in vertebrates within the testes), is a small cell containing a single long flagellum which propels it.
Spermatozoa are extremely reduced cells, lacking many cellular components that would be necessary for embryonic development. They are specialized for motility, seeking out an egg cell and fusing with it in a process called fertilization.

Female gametes are egg cells (produced in vertebrates within the ovaries), large immobile cells that contain the nutrients and cellular components necessary for a developing embryo.
Egg cells are often associated with other cells which support the development of the embryo, forming an egg. In mammals, the fertilized embryo instead develops within the female, receiving nutrition directly from its mother.

Animals are usually mobile and seek out a partner of the opposite sex for mating. Animals which live in the water can mate using external fertilization, where the eggs and sperm are released into and combine within the surrounding water. Most animals that live outside of water, however, use internal fertilization, transferring sperm directly into the female to prevent the gametes from drying up.

In most birds, both excretion and reproduction is done through a single posterior opening, called the cloaca—male and female birds touch cloaca to transfer sperm, a process called "cloacal kissing". In many other terrestrial animals, males use specialized sex organs to assist the transport of sperm—these male sex organs are called intromittent organs. In humans and other mammals this male organ is the penis, which enters the female reproductive tract (called the vagina) to achieve insemination—a process called sexual intercourse. The penis contains a tube through which semen (a fluid containing sperm) travels. In female mammals the vagina connects with the uterus, an organ which directly supports the development of a fertilized embryo within (a process called gestation).

Because of their motility, animal sexual behavior can involve coercive sex. Traumatic insemination, for example, is used by some insect species to inseminate females through a wound in the abdominal cavity—a process detrimental to the female's health.

Like animals, plants have specialized male and female gametes. Within seed plants, male gametes are produced by extremely reduced multicellular gametophytes known as pollen. The female gametes of seed plants are contained within ovules; once fertilized by male gametes produced by pollen these form seeds which, like eggs, contain the nutrients necessary for the development of the embryonic plant.

Many plants have flowers and these are the sexual organs of those plants. Flowers are usually hermaphroditic, producing both male and female gametes. The female parts, in the center of a flower, are the pistils, each unit consisting of a carpel, a style and a stigma. One or more of these reproductive units may be merged to form a single compound pistil. Within the carpels are ovules which develop into seeds after fertilization. The male parts of the flower are the stamens: these consist of long filaments arranged between the pistil and the petals that produce pollen in anthers at their tips. When a pollen grain lands upon the stigma on top of a carpel's style, it germinates to produce a pollen tube that grows down through the tissues of the style into the carpel, where it delivers male gamete nuclei to fertilize an ovule that eventually develops into a seed.

In pines and other conifers the sex organs are conifer cones and have male and female forms. The more familiar female cones are typically more durable, containing ovules within them. Male cones are smaller and produce pollen which is transported by wind to land in female cones. As with flowers, seeds form within the female cone after pollination.

Because plants are immobile, they depend upon passive methods for transporting pollen grains to other plants. Many plants, including conifers and grasses, produce lightweight pollen which is carried by wind to neighboring plants. Other plants have heavier, sticky pollen that is specialized for transportation by animals. The plants attract these insects or larger animals such as humming birds and bats with nectar-containing flowers. These animals transport the pollen as they move to other flowers, which also contain female reproductive organs, resulting in pollination.

Most fungi reproduce sexually, having both a haploid and diploid stage in their life cycles. These fungi are typically isogamous, lacking male and female specialization: haploid fungi grow into contact with each other and then fuse their cells. In some of these cases, the fusion is asymmetric, and the cell which donates only a nucleus (and not accompanying cellular material) could arguably be considered "male". Fungi may also have more complex allelic mating systems, with other sexes not accurately described as male, female, or hermaphroditic.

Some fungi, including baker's yeast, have mating types that create a duality similar to male and female roles. Yeast with the same mating type will not fuse with each other to form diploid cells, only with yeast carrying the other mating type.

Many species of higher fungi produce mushrooms as part of their sexual reproduction. Within the mushroom diploid cells are formed, later dividing into haploid spores. The height of the mushroom aids the dispersal of these sexually produced offspring.

The most basic sexual system is one in which all organisms are hermaphrodites, producing both male and female gametes. This is true of some animals (e.g. snails) and the majority of flowering plants. In many cases, however, specialization of sex has evolved such that some organisms produce only male or only female gametes. The biological cause for an organism developing into one sex or the other is called "sex determination". The cause may be genetic or non-genetic. Within animals and other organisms that have genetic sex determination systems, the determining factor may be the presence of a sex chromosome or other genetic differences. In plants also, such as the liverwort "Marchantia polymorpha" and the flowering plant genus "Silene" that have sexual dimorphism (dioicy or dioicy, respectively), sex may be determined by sex chromosomes. Non-genetic systems may use environmental cues, such as the temperature during early development in crocodiles, to determine the sex of the offspring.

In the majority of species with sex specialization, organisms are either male (producing only male gametes) or female (producing only female gametes). Exceptions are common—for example, the roundworm "C. elegans" has an hermaphrodite and a male sex (a system called androdioecy).

Sometimes an organism's development is intermediate between male and female, a condition called intersex. Sometimes intersex individuals are called "hermaphrodite"; but, unlike biological hermaphrodites, intersex individuals are unusual cases and are not typically fertile in both male and female aspects.

In genetic sex-determination systems, an organism's sex is determined by the genome it inherits. Genetic sex-determination usually depends on asymmetrically inherited sex chromosomes which carry genetic features that influence development; sex may be determined either by the presence of a sex chromosome or by how many the organism has. Genetic sex-determination, because it is determined by chromosome assortment, usually results in a 1:1 ratio of male and female offspring.

Humans and other mammals have an XY sex-determination system: the Y chromosome carries factors responsible for triggering male development. The "default sex," in the absence of a Y chromosome, is female-like. Thus, XX mammals are female and XY are male. In humans, biological sex is determined by five factors present at birth: the presence or absence of a Y chromosome (which alone determines the individual's "genetic sex"), the type of gonads, the sex hormones, the internal reproductive anatomy (such as the uterus in females), and the external genitalia.

XY sex determination is found in other organisms, including the common fruit fly and some plants. In some cases, including in the fruit fly, it is the number of X chromosomes that determines sex rather than the presence of a Y chromosome (see below).

In birds, which have a ZW sex-determination system, the opposite is true: the W chromosome carries factors responsible for female development, and default development is male. In this case ZZ individuals are male and ZW are female. The majority of butterflies and moths also have a ZW sex-determination system. In both XY and ZW sex determination systems, the sex chromosome carrying the critical factors is often significantly smaller, carrying little more than the genes necessary for triggering the development of a given sex.

Many insects use a sex determination system based on the number of sex chromosomes. This is called X0 sex-determination—the 0 indicates the absence of the sex chromosome. All other chromosomes in these organisms are diploid, but organisms may inherit one or two X chromosomes. In field crickets, for example, insects with a single X chromosome develop as male, while those with two develop as female. In the nematode "C. elegans" most worms are self-fertilizing XX hermaphrodites, but occasionally abnormalities in chromosome inheritance regularly give rise to individuals with only one X chromosome—these X0 individuals are fertile males (and half their offspring are male).

Other insects, including honey bees and ants, use a haplodiploid sex-determination system. In this case, diploid individuals are generally female, and haploid individuals (which develop from unfertilized eggs) are male. This sex-determination system results in highly biased sex ratios, as the sex of offspring is determined by fertilization rather than the assortment of chromosomes during meiosis.

For many species, sex is not determined by inherited traits, but instead by environmental factors experienced during development or later in life. Many reptiles have temperature-dependent sex determination: the temperature embryos experience during their development determines the sex of the organism. In some turtles, for example, males are produced at lower incubation temperatures than females; this difference in critical temperatures can be as little as 1–2 °C.

Many fish change sex over the course of their lifespan, a phenomenon called sequential hermaphroditism. In clownfish, smaller fish are male, and the dominant and largest fish in a group becomes female. In many wrasses the opposite is true—most fish are initially female and become male when they reach a certain size. Sequential hermaphrodites may produce both types of gametes over the course of their lifetime, but at any given point they are either female or male.

In some ferns the default sex is hermaphrodite, but ferns which grow in soil that has previously supported hermaphrodites are influenced by residual hormones to instead develop as male.

Many animals and some plants have differences between the male and female sexes in size and appearance, a phenomenon called sexual dimorphism. Sex differences in humans include, generally, a larger size and more body hair in men; women have breasts, wider hips, and a higher body fat percentage. In other species, the differences may be more extreme, such as differences in coloration or bodyweight.

Sexual dimorphisms in animals are often associated with sexual selection—the competition between individuals of one sex to mate with the opposite sex. Antlers in male deer, for example, are used in combat between males to win reproductive access to female deer. In many cases the male of a species is larger than the female. Mammal species with extreme sexual size dimorphism tend to have highly polygynous mating systems—presumably due to selection for success in competition with other males—such as the elephant seals. Other examples demonstrate that it is the preference of females that drive sexual dimorphism, such as in the case of the stalk-eyed fly.

Other animals, including most insects and many fish, have larger females. This may be associated with the cost of producing egg cells, which requires more nutrition than producing sperm—larger females are able to produce more eggs. For example, female southern black widow spiders are typically twice as long as the males. Occasionally this dimorphism is extreme, with males reduced to living as parasites dependent on the female, such as in the anglerfish. Some plant species also exhibit dimorphism in which the females are significantly larger than the males, such as in the moss "Dicranum" and the liverwort "Sphaerocarpos". There is some evidence that, in these genera, the dimorphism may be tied to a sex chromosome, or to chemical signalling from females.

In birds, males often have a more colourful appearance and may have features (like the long tail of male peacocks) that would seem to put the organism at a disadvantage (e.g. bright colors would seem to make a bird more visible to predators). One proposed explanation for this is the handicap principle. This hypothesis says that, by demonstrating he can survive with such handicaps, the male is advertising his genetic fitness to females—traits that will benefit daughters as well, who will not be encumbered with such handicaps.




</doc>
<doc id="26808" url="https://en.wikipedia.org/wiki?curid=26808" title="Star">
Star

A star is an astronomical object consisting of a luminous spheroid of plasma held together by its own gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye from Earth during the night, appearing as a multitude of fixed luminous points in the sky due to their immense distance from Earth. Historically, the most prominent stars were grouped into constellations and asterisms, the brightest of which gained proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable Universe contains an estimated stars, but most are invisible to the naked eye from Earth, including all stars outside our galaxy, the Milky Way.

For at least a portion of its life, a star shines due to thermonuclear fusion of hydrogen into helium in its core, releasing energy that traverses the star's interior and then radiates into outer space. Almost all naturally occurring elements heavier than helium are created by stellar nucleosynthesis during the star's lifetime, and for some stars by supernova nucleosynthesis when it explodes. Near the end of its life, a star can also contain degenerate matter. Astronomers can determine the mass, age, metallicity (chemical composition), and many other properties of a star by observing its motion through space, its luminosity, and spectrum respectively. The total mass of a star is the main factor that determines its evolution and eventual fate. Other characteristics of a star, including diameter and temperature, change over its life, while the star's environment affects its rotation and movement. A plot of the temperature of many stars against their luminosities produces a plot known as a Hertzsprung–Russell diagram (H–R diagram). Plotting a particular star on that diagram allows the age and evolutionary state of that star to be determined.

A star's life begins with the gravitational collapse of a gaseous nebula of material composed primarily of hydrogen, along with helium and trace amounts of heavier elements. When the stellar core is sufficiently dense, hydrogen becomes steadily converted into helium through nuclear fusion, releasing energy in the process. The remainder of the star's interior carries energy away from the core through a combination of radiative and convective heat transfer processes. The star's internal pressure prevents it from collapsing further under its own gravity. A star with mass greater than 0.4 times the Sun's will expand to become a red giant when the hydrogen fuel in its core is exhausted. In some cases, it will fuse heavier elements at the core or in shells around the core. As the star expands it throws a part of its mass, enriched with those heavier elements, into the interstellar environment, to be recycled later as new stars. Meanwhile, the core becomes a stellar remnant: a white dwarf, a neutron star, or, if it is sufficiently massive, a black hole.

Binary and multi-star systems consist of two or more stars that are gravitationally bound and generally move around each other in stable orbits. When two such stars have a relatively close orbit, their gravitational interaction can have a significant impact on their evolution. Stars can form part of a much larger gravitationally bound structure, such as a star cluster or a galaxy.

Historically, stars have been important to civilizations throughout the world. They have been part of religious practices and used for celestial navigation and orientation. Many ancient astronomers believed that stars were permanently affixed to a heavenly sphere and that they were immutable. By convention, astronomers grouped stars into constellations and used them to track the motions of the planets and the inferred position of the Sun. The motion of the Sun against the background stars (and the horizon) was used to create calendars, which could be used to regulate agricultural practices. The Gregorian calendar, currently used nearly everywhere in the world, is a solar calendar based on the angle of the Earth's rotational axis relative to its local star, the Sun.

The oldest accurately dated star chart was the result of ancient Egyptian astronomy in 1534 BC. The earliest known star catalogues were compiled by the ancient Babylonian astronomers of Mesopotamia in the late 2nd millennium BC, during the Kassite Period (c. 1531–1155 BC).

The first star catalogue in Greek astronomy was created by Aristillus in approximately 300 BC, with the help of Timocharis. The star catalog of Hipparchus (2nd century BC) included 1020 stars, and was used to assemble Ptolemy's star catalogue. Hipparchus is known for the discovery of the first recorded "nova" (new star). Many of the constellations and star names in use today derive from Greek astronomy.

In spite of the apparent immutability of the heavens, Chinese astronomers were aware that new stars could appear. In 185 AD, they were the first to observe and write about a supernova, now known as the SN 185. The brightest stellar event in recorded history was the SN 1006 supernova, which was observed in 1006 and written about by the Egyptian astronomer Ali ibn Ridwan and several Chinese astronomers. The SN 1054 supernova, which gave birth to the Crab Nebula, was also observed by Chinese and Islamic astronomers.

Medieval Islamic astronomers gave Arabic names to many stars that are still used today and they invented numerous astronomical instruments that could compute the positions of the stars. They built the first large observatory research institutes, mainly for the purpose of producing "Zij" star catalogues. Among these, the "Book of Fixed Stars" (964) was written by the Persian astronomer Abd al-Rahman al-Sufi, who observed a number of stars, star clusters (including the Omicron Velorum and Brocchi's Clusters) and galaxies (including the Andromeda Galaxy). According to A. Zahoor, in the 11th century, the Persian polymath scholar Abu Rayhan Biruni described the Milky Way galaxy as a multitude of fragments having the properties of nebulous stars, and also gave the latitudes of various stars during a lunar eclipse in 1019.

According to Josep Puig, the Andalusian astronomer Ibn Bajjah proposed that the Milky Way was made up of many stars that almost touched one another and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars on 500 AH (1106/1107 AD) as evidence. 
Early European astronomers such as Tycho Brahe identified new stars in the night sky (later termed "novae"), suggesting that the heavens were not immutable. In 1584, Giordano Bruno suggested that the stars were like the Sun, and may have other planets, possibly even Earth-like, in orbit around them, an idea that had been suggested earlier by the ancient Greek philosophers, Democritus and Epicurus, and by medieval Islamic cosmologists such as Fakhr al-Din al-Razi. By the following century, the idea of the stars being the same as the Sun was reaching a consensus among astronomers. To explain why these stars exerted no net gravitational pull on the Solar System, Isaac Newton suggested that the stars were equally distributed in every direction, an idea prompted by the theologian Richard Bentley.

The Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion of a pair of nearby "fixed" stars, demonstrating that they had changed positions since the time of the ancient Greek astronomers Ptolemy and Hipparchus.

William Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he established a series of gauges in 600 directions and counted the stars observed along each line of sight. From this he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern hemisphere and found a corresponding increase in the same direction. In addition to his other accomplishments, William Herschel is also noted for his discovery that some stars do not merely lie along the same line of sight, but are also physical companions that form binary star systems.

The science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption lines—the dark lines in stellar spectra caused by the atmosphere's absorption of specific frequencies. In 1865, Secchi began classifying stars into spectral types. However, the modern version of the stellar classification scheme was developed by Annie J. Cannon during the 1900s.
The first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens. Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899 when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as Friedrich Georg Wilhelm von Struve and S. W. Burnham, allowing the masses of stars to be determined from computation of orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827.
The twentieth century saw increasingly rapid advances in the scientific study of stars. The photograph became a valuable astronomical tool. Karl Schwarzschild discovered that the color of a star and, hence, its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed precise measurements of magnitude at multiple wavelength intervals. In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope at Mount Wilson Observatory.

Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.
With the exception of supernovae, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for our
galaxy). But some stars have been observed in the M100 galaxy of the Virgo Cluster, about 100 million light years from the Earth.
In the Local Supercluster it is possible to see star clusters, and current telescopes could in principle observe faint individual stars in the Local Group (see Cepheids). However, outside the Local Supercluster of galaxies, neither individual stars nor clusters of stars have been observed. The only exception is a faint image of a large star cluster containing hundreds of thousands of stars located at a distance of one billion light years—ten times further than the most distant star cluster previously observed.

In February 2018, astronomers reported, for the first time, a signal of the reionization epoch, an indirect detection of light from the earliest stars formed—about 180 million years after the Big Bang.

In April, 2018, astronomers reported the detection of the most distant "ordinary" (i.e., main sequence) star, named Icarus (formally, MACS J1149 Lensed Star 1), at 9 billion light-years away from Earth.

In May 2018, astronomers reported the detection of the most distant oxygen ever detected in the Universe—and the most distant galaxy ever observed by Atacama Large Millimeter Array or the Very Large Telescope—with the team inferring that the signal was emitted 13.3 billion years ago (or 500 million years after the Big Bang). They found that the observed brightness of the galaxy is well-explained by a model where the onset of star formation corresponds to only 250 million years after the Universe began, corresponding to a redshift of about 15.

The concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the more prominent individual stars were also given names, particularly with Arabic or Latin designations.

As well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some "stars", known as planets (Greek πλανήτης (planētēs), meaning "wanderer"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were also Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.)

Circa 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star's right ascension was invented and added to John Flamsteed's star catalogue in his book ""Historia coelestis Britannica"" (the 1712 edition), whereby this numbering system came to be called "Flamsteed designation" or "Flamsteed numbering".

The only internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs and standardizes proper names for stars. A number of private companies sell names of stars, which the British Library calls an unregulated commercial enterprise. The IAU has disassociated itself from this commercial practice, and these names are neither recognized by the IAU, professional astronomers, nor the amateur astronomy community. One such star-naming company is the International Star Registry, which, during the 1980s, was accused of deceptive practice for making it appear that the assigned name was official. This now-discontinued ISR practice was informally labeled a scam and a fraud, and the New York City Department of Consumer and Worker Protection issued a violation against ISR for engaging in a deceptive trade practice.

Although stellar parameters can be expressed in SI units or CGS units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of "nominal" solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:

The solar mass M was not explicitly defined by the IAU due to the large relative uncertainty (10) of the Newtonian gravitational constant G. However, since the product of the Newtonian gravitational constant and solar mass
together (GM) has been determined to much greater precision, the IAU defined the "nominal" solar mass parameter to be:

However, one can combine the nominal solar mass parameter with the most recent (2014) CODATA estimate of the Newtonian gravitational constant G to derive the solar mass to be approximately 1.9885 × 10 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.

Large lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.

Stars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as "molecular clouds"—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars.
Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions. Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation.

All stars spend the majority of their existence as "main sequence stars", fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass:

The formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force.

As the cloud collapses, individual conglomerations of dense dust and gas form "Bok globules". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 to 15 million years.
Early stars of less than 2 are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects.
These jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed.

Early in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track.

Most stars are observed to be members of binary star systems, and the properties of those binaries are the result of the conditions in which they formed. A gas cloud must lose its angular momentum in order to collapse and form a star. The fragmentation of the cloud into multiple stars distributes some of that angular momentum. The primordial binaries transfer some angular momentum by gravitational interactions during close encounters with other stars in young stellar clusters. These interactions tend to split apart more widely separated (soft) binaries while causing hard binaries to become more tightly bound. This produces the separation of binaries into their two observed populations distributions.

Stars spend about 90% of their existence fusing hydrogen into helium in high-temperature and high-pressure reactions near the core. Such stars are said to be on the main sequence, and are called dwarf stars. Starting at zero-age main sequence, the proportion of helium in a star's core will steadily increase, the rate of nuclear fusion at the core will slowly increase, as will the star's temperature and luminosity.
The Sun, for example, is estimated to have increased in luminosity by about 40% since it reached the main sequence 4.6 billion (4.6 × 10) years ago.

Every star generates a stellar wind of particles that causes a continual outflow of gas into space. For most stars, the mass lost is negligible. The Sun loses 10 every year, or about 0.01% of its total mass over its entire lifespan. However, very massive stars can lose 10 to 10 each year, significantly affecting their evolution. Stars that begin with more than 50 can lose over half their total mass while on the main sequence.

The time a star spends on the main sequence depends primarily on the amount of fuel it has and the rate at which it fuses it. The Sun is expected to live 10 billion (10) years. Massive stars consume their fuel very rapidly and are short-lived. Low mass stars consume their fuel very slowly. Stars less massive than 0.25 , called red dwarfs, are able to fuse nearly all of their mass while stars of about 1 can only fuse about 10% of their mass. The combination of their slow fuel-consumption and relatively large usable fuel supply allows low mass stars to last about one trillion (10) years; the most extreme of 0.08 ) will last for about 12 trillion years. Red dwarfs become hotter and more luminous as they accumulate helium. When they eventually run out of hydrogen, they contract into a white dwarf and decline in temperature. However, since the lifespan of such stars is greater than the current age of the universe (13.8 billion years), no stars under about 0.85 are expected to have moved off the main sequence.

Besides mass, the elements heavier than helium can play a significant role in the evolution of stars. Astronomers label all elements heavier than helium "metals", and call the chemical concentration of these elements in a star, its metallicity. A star's metallicity can influence the time the star takes to burn its fuel, and controls the formation of its magnetic fields, which affects the strength of its stellar wind. Older, population II stars have substantially less metallicity than the younger, population I stars due to the composition of the molecular clouds from which they formed. Over time, such clouds become increasingly enriched in heavier elements as older stars die and shed portions of their atmospheres.

As stars of at least 0.4 exhaust their supply of hydrogen at their core, they start to fuse hydrogen in a shell outside the helium core. Their outer layers expand and cool greatly as they form a red giant. In about 5 billion years, when the Sun enters the helium burning phase, it will expand to a maximum radius of roughly , 250 times its present size, and lose 30% of its current mass.

As the hydrogen shell burning produces more helium, the core increases in mass and temperature. In a red giant of up to 2.25 , the mass of the helium core becomes degenerate prior to helium fusion. Finally, when the temperature increases sufficiently, helium fusion begins explosively in what is called a helium flash, and the star rapidly shrinks in radius, increases its surface temperature, and moves to the horizontal branch of the HR diagram. For more massive stars, helium core fusion starts before the core becomes degenerate, and the star spends some time in the red clump, slowly burning helium, before the outer convective envelope collapses and the star then moves to the horizontal branch.

After the star has fused the helium of its core, the carbon product fuses producing a hot core with an outer shell of fusing helium. The star then follows an evolutionary path called the asymptotic giant branch (AGB) that parallels the other described red giant phase, but with a higher luminosity. The more massive AGB stars may undergo a brief period of carbon fusion before the core becomes degenerate.

During their helium-burning phase, a star of more than 9 solar masses expands to form first a blue and then a red supergiant. Particularly massive stars may evolve to a Wolf-Rayet star, characterised by spectra dominated by emission lines of elements heavier than hydrogen, which have reached the surface due to strong convection and intense mass loss.

When helium is exhausted at the core of a massive star, the core contracts and the temperature and pressure rises enough to fuse carbon (see Carbon-burning process). This process continues, with the successive stages being fueled by neon (see neon-burning process), oxygen (see oxygen-burning process), and silicon (see silicon-burning process). Near the end of the star's life, fusion continues along a series of onion-layer shells within a massive star. Each shell fuses a different element, with the outermost shell fusing hydrogen; the next shell fusing helium, and so forth.

The final stage occurs when a massive star begins producing iron. Since iron nuclei are more tightly bound than any heavier nuclei, any fusion beyond iron does not produce a net release of energy. To a very limited degree such a process proceeds, but it consumes energy. Likewise, since they are more tightly bound than all lighter nuclei, such energy cannot be released by fission.

As a star's core shrinks, the intensity of radiation from that surface increases, creating such radiation pressure on the outer shell of gas that it will push those layers away, forming a planetary nebula. If what remains after the outer atmosphere has been shed is less than roughly 1.4 , it shrinks to a relatively tiny object about the size of Earth, known as a white dwarf. White dwarfs lack the mass for further gravitational compression to take place. The electron-degenerate matter inside a white dwarf is no longer a plasma, even though stars are generally referred to as being spheres of plasma. Eventually, white dwarfs fade into black dwarfs over a very long period of time.

In massive stars, fusion continues until the iron core has grown so large (more than 1.4 ) that it can no longer support its own mass. This core will suddenly collapse as its electrons are driven into its protons, forming neutrons, neutrinos, and gamma rays in a burst of electron capture and inverse beta decay. The shockwave formed by this sudden collapse causes the rest of the star to explode in a supernova. Supernovae become so bright that they may briefly outshine the star's entire home galaxy. When they occur within the Milky Way, supernovae have historically been observed by naked-eye observers as "new stars" where none seemingly existed before.

A supernova explosion blows away the star's outer layers, leaving a remnant such as the Crab Nebula. The core is compressed into a neutron star, which sometimes manifests itself as a pulsar or X-ray burster. In the case of the largest stars, the remnant is a black hole greater than 4 . In a neutron star the matter is in a state known as neutron-degenerate matter, with a more exotic form of degenerate matter, QCD matter, possibly present in the core. Within a black hole, the matter is in a state that is not currently understood.

The blown-off outer layers of dying stars include heavy elements, which may be recycled during the formation of new stars. These heavy elements allow the formation of rocky planets. The outflow from supernovae and the stellar wind of large stars play an important part in shaping the interstellar medium.

The post–main-sequence evolution of binary stars may be significantly different from the evolution of single stars of the same mass. If stars in a binary system are sufficiently close, when one of the stars expands to become a red giant it may overflow its Roche lobe, the region around a star where material is gravitationally bound to that star, leading to transfer of material to the other. When the Roche lobe is violated, a variety of phenomena can result, including contact binaries, common-envelope binaries, cataclysmic variables, and type Ia supernovae.

Stars are not spread uniformly across the universe, but are normally grouped into galaxies along with interstellar gas and dust. A typical galaxy contains hundreds of billions of stars, and there are more than 2 trillion (10) galaxies. Overall, there are as many as an estimated stars (more stars than all the grains of sand on planet Earth). While it is often believed that stars only exist within galaxies, intergalactic stars have been discovered.

A multi-star system consists of two or more gravitationally bound stars that orbit each other. The simplest and most common multi-star system is a binary star, but systems of three or more stars are also found. For reasons of orbital stability, such multi-star systems are often organized into hierarchical sets of binary stars. Larger groups called star clusters also exist. These range from loose stellar associations with only a few stars, up to enormous globular clusters with hundreds of thousands of stars. Such systems orbit their host galaxy.

It has been a long-held assumption that the majority of stars occur in gravitationally bound, multiple-star systems. This is particularly true for very massive O and B class stars, where 80% of the stars are believed to be part of multiple-star systems. The proportion of single star systems increases with decreasing star mass, so that only 25% of red dwarfs are known to have stellar companions. As 85% of all stars are red dwarfs, most stars in the Milky Way are likely single from birth.
The nearest star to the Earth, apart from the Sun, is Proxima Centauri, which is 39.9 trillion kilometres, or 4.2 light-years. Travelling at the orbital speed of the Space Shuttle (8 kilometres per second—almost 30,000 kilometres per hour), it would take about 150,000 years to arrive. This is typical of stellar separations in galactic discs. Stars can be much closer to each other in the centres of galaxies and in globular clusters, or much farther apart in galactic halos.

Due to the relatively vast distances between stars outside the galactic nucleus, collisions between stars are thought to be rare. In denser regions such as the core of globular clusters or the galactic center, collisions can be more common. Such collisions can produce what are known as blue stragglers. These abnormal stars have a higher surface temperature than the other main sequence stars with the same luminosity of the cluster to which it belongs.

Almost everything about a star is determined by its initial mass, including such characteristics as luminosity, size, evolution, lifespan, and its eventual fate.

Most stars are between 1 billion and 10 billion years old. Some stars may even be close to 13.8 billion years old—the observed age of the universe. The oldest star yet discovered, HD 140283, nicknamed Methuselah star, is an estimated 14.46 ± 0.8 billion years old. (Due to the uncertainty in the value, this age for the star does not conflict with the age of the Universe, determined by the Planck satellite as 13.799 ± 0.021).

The more massive the star, the shorter its lifespan, primarily because massive stars have greater pressure on their cores, causing them to burn hydrogen more rapidly. The most massive stars last an average of a few million years, while stars of minimum mass (red dwarfs) burn their fuel very slowly and can last tens to hundreds of billions of years.

When stars form in the present Milky Way galaxy they are composed of about 71% hydrogen and 27% helium, as measured by mass, with a small fraction of heavier elements. Typically the portion of heavy elements is measured in terms of the iron content of the stellar atmosphere, as iron is a common element and its absorption lines are relatively easy to measure. The portion of heavier elements may be an indicator of the likelihood that the star has a planetary system.

The star with the lowest iron content ever measured is the dwarf HE1327-2326, with only 1/200,000th the iron content of the Sun. By contrast, the super-metal-rich star μ Leonis has nearly double the abundance of iron as the Sun, while the planet-bearing star 14 Herculis has nearly triple the iron. There also exist chemically peculiar stars that show unusual abundances of certain elements in their spectrum; especially chromium and rare earth elements. Stars with cooler outer atmospheres, including the Sun, can form various diatomic and polyatomic molecules.

Due to their great distance from the Earth, all stars except the Sun appear to the unaided eye as shining points in the night sky that twinkle because of the effect of the Earth's atmosphere. The Sun is also a star, but it is close enough to the Earth to appear as a disk instead, and to provide daylight. Other than the Sun, the star with the largest apparent size is R Doradus, with an angular diameter of only 0.057 arcseconds.

The disks of most stars are much too small in angular size to be observed with current ground-based optical telescopes, and so interferometer telescopes are required to produce images of these objects. Another technique for measuring the angular size of stars is through occultation. By precisely measuring the drop in brightness of a star as it is occulted by the Moon (or the rise in brightness when it reappears), the star's angular diameter can be computed.

Stars range in size from neutron stars, which vary anywhere from 20 to in diameter, to supergiants like Betelgeuse in the Orion constellation, which has a diameter about 1,000 times that of our sun. Betelgeuse, however, has a much lower density than the Sun.

The motion of a star relative to the Sun can provide useful information about the origin and age of a star, as well as the structure and evolution of the surrounding galaxy. The components of motion of a star consist of the radial velocity toward or away from the Sun, and the traverse angular movement, which is called its proper motion.

Radial velocity is measured by the doppler shift of the star's spectral lines, and is given in units of km/s. The proper motion of a star, its parallax, is determined by precise astrometric measurements in units of milli-arc seconds (mas) per year. With knowledge of the star's parallax and its distance, the proper motion velocity can be calculated. Together with the radial velocity, the total velocity can be calculated. Stars with high rates of proper motion are likely to be relatively close to the Sun, making them good candidates for parallax measurements.

When both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has allowed astronomers to trace their origin to common points in giant molecular clouds, and are referred to as stellar associations.

The magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, wherein the movement of electrical charges induce magnetic fields, as does a mechanical dynamo. Those magnetic fields have a great range that extend throughout and beyond the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic field flux lines that rise from a star's surface into the star's outer atmosphere, its corona. The coronal loops can be seen due to the plasma they conduct along their length. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity.

Young, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During
the Maunder Minimum, for example, the Sun underwent a
70-year period with almost no sunspot activity.

One of the most massive stars known is Eta Carinae, which,
with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. Studies of the most massive open clusters suggests as an upper limit for stars in the current era of the universe. This
represents an empirical value for the theoretical limit on the mass of forming stars due to increasing radiation pressure on the accreting gas cloud. Several stars in the R136 cluster in the Large Magellanic Cloud have been measured with larger masses, but
it has been determined that they could have been created through the collision and merger of massive stars in close binary systems, sidestepping the 150 limit on massive star formation.
The first stars to form after the Big Bang may have been larger, up to 300 , due
to the complete absence of elements heavier than lithium in their composition. This generation of supermassive population III stars is likely to have existed in the very early universe (i.e., they are observed to have a high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at .

With a mass only 80 times that of Jupiter (), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For
stars with metallicity similar to the Sun, the theoretical minimum mass the star can have and still undergo fusion at the core, is estimated to be about 75 . When the metallicity is very low, however, the minimum star size seems to be about 8.3% of the solar mass, or about 87 . Smaller bodies called brown dwarfs, occupy a poorly defined grey area between stars and gas giants.

The combination of the radius and the mass of a star determines its surface gravity. Giant stars have a much lower surface gravity than do main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a broadening of the absorption lines.

The rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking their starspots. Young stars can have a rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial velocity of about 225 km/s or greater, causing its equator to bulge outward and giving it an equatorial diameter that is more than 50% greater than between the poles. This rate of rotation is just below the critical velocity of 300 km/s at which speed the star would break apart. By contrast, the Sun rotates once every 25–35 days depending on latitude, with an equatorial velocity of 1.93 km/s. A main sequence star's magnetic field and the stellar wind serve to slow its rotation by a significant amount as it evolves on the main sequence.

Degenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation.

The surface temperature of a main sequence star is determined by the rate of energy production of its core and by its radius, and is often estimated from the star's color index. The temperature is normally given in terms of an effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. Note that the effective temperature is only a representative of the surface, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins.

The stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below).

Massive main sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they also have a high luminosity due to their large exterior surface area.

The energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which
streams from the outer layers as electrically charged protons and alpha and beta particles. Although almost massless, there also exists a steady stream of neutrinos emanating from the star's core.

The production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.

The color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere. Besides
visible light, stars also emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.

Using the stellar spectrum, astronomers can also determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can also estimate the age of the star.

The luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time. It has units of power. The luminosity of a star is determined by its radius and surface temperature. Many stars do not radiate uniformly across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux (power per unit area) at its poles than along its equator.

Patches of the star's surface with a lower temperature and luminosity than average are known as starspots. Small, "dwarf" stars such as our Sun generally have essentially featureless disks with only small starspots. "Giant" stars have much larger, more obvious starspots, and
they also exhibit strong stellar limb darkening. That is, the brightness decreases towards the edge of the stellar disk. Red
dwarf flare stars such as UV Ceti may also possess prominent starspot features.

The apparent brightness of a star is expressed in terms of its apparent magnitude. It is a function of the star's luminosity, its distance from Earth, the extinction effect of interstellar dust and gas, and the altering of the star's light as it passes through Earth's atmosphere. Intrinsic or absolute magnitude is directly related to a star's luminosity, and is what the apparent magnitude a star would be if the distance between the Earth and the star were 10 parsecs (32.6 light-years).

Both the apparent and absolute magnitude scales are logarithmic units: one whole number difference in magnitude is equal to a brightness variation of about 2.5 times (the 5th root of 100 or approximately 2.512). This means that a first magnitude star (+1.00) is about 2.5 times brighter than a second magnitude (+2.00) star, and about 100 times brighter than a sixth magnitude star (+6.00). The faintest stars visible to the naked eye under good seeing conditions are about magnitude +6.

On both apparent and absolute magnitude scales, the smaller the magnitude number, the brighter the star; the larger the magnitude number, the fainter the star. The brightest stars, on either scale, have negative magnitude numbers. The variation in brightness (Δ"L") between two stars is calculated by subtracting the magnitude number of the brighter star ("m") from the magnitude number of the fainter star ("m"), then using the difference as an exponent for the base number 2.512; that is to say:

Relative to both luminosity and distance from Earth, a star's absolute magnitude ("M") and apparent magnitude ("m") are not equivalent; for example, the bright star Sirius has an apparent magnitude of −1.44, but it has an absolute magnitude of +1.41.

The Sun has an apparent magnitude of −26.7, but its absolute magnitude is only +4.83. Sirius, the brightest star in the night sky as seen from Earth, is approximately 23 times more luminous than the Sun, while Canopus, the second brightest star in the night sky with an absolute magnitude of −5.53, is approximately 14,000 times more luminous than the Sun. Despite Canopus being vastly more luminous than Sirius, however, Sirius appears brighter than Canopus. This is because Sirius is merely 8.6 light-years from the Earth, while Canopus is much farther away at a distance of 310 light-years.

As of 2006, the star with the highest known absolute magnitude is LBV 1806-20, with a magnitude of −14.2. This star is at least 5,000,000 times more luminous than the Sun. The least luminous stars that are currently known are located in the NGC 6397 cluster. The faintest red dwarfs in the cluster were magnitude 26, while a 28th magnitude white dwarf was also discovered. These faint stars are so dim that their light is as bright as a birthday candle on the Moon when viewed from the Earth.

The current stellar classification system originated in the early 20th century, when stars were classified from "A" to "Q" based on the strength of the hydrogen line. It was thought that the hydrogen line strength was a simple linear function of temperature. Instead, it was more complicated: it strengthened with increasing temperature, peaked near 9000 K, and then declined at greater temperatures. The classifications were since reordered by temperature, on which the modern scheme is based.

Stars are given a single-letter classification according to their spectra, ranging from type "O", which are very hot, to "M", which are so cool that molecules may form in their atmospheres. The main classifications in order of decreasing surface temperature are: "O, B, A, F, G, K", and "M". A variety of rare spectral types are given special classifications. The most common of these are types "L" and "T", which classify the coldest low-mass stars and brown dwarfs. Each letter has 10 sub-divisions, numbered from 0 to 9, in order of decreasing temperature. However, this system breaks down at extreme high temperatures as classes "O0" and "O1" may not exist.

In addition, stars may be classified by the luminosity effects found in their spectral lines, which correspond to their spatial size and is determined by their surface gravity. These range from "0" (hypergiants) through "III" (giants) to "V" (main sequence dwarfs); some authors add "VII" (white dwarfs). Main sequence stars fall along a narrow, diagonal band when graphed according to their absolute magnitude and spectral type. The Sun is a main sequence "G2V" yellow dwarf of intermediate temperature and ordinary size.

Additional nomenclature, in the form of lower-case letters added to the end of the spectral type to indicate peculiar features of the spectrum. For example, an ""e"" can indicate the presence of emission lines; ""m"" represents unusually strong levels of metals, and ""var"" can mean variations in the spectral type.

White dwarf stars have their own class that begins with the letter "D". This is further sub-divided into the classes "DA", "DB", "DC", "DO", "DZ", and "DQ", depending on the types of prominent lines found in the spectrum. This is followed by a numerical value that indicates the temperature.

Variable stars have periodic or random changes in luminosity because of intrinsic or extrinsic properties. Of the intrinsically variable stars, the primary types can be subdivided into three principal groups.

During their stellar evolution, some stars pass through phases where they can become pulsating variables. Pulsating variable stars vary in radius and luminosity over time, expanding and contracting with periods ranging from minutes to years, depending on the size of the star. This category includes Cepheid and Cepheid-like stars, and long-period variables such as Mira.

Eruptive variables are stars that experience sudden increases in luminosity because of flares or mass ejection events. This group includes protostars, Wolf-Rayet stars, and flare stars, as well as giant and supergiant stars.

Cataclysmic or explosive variable stars are those that undergo a dramatic change in their properties. This group includes novae and supernovae. A binary star system that includes a nearby white dwarf can produce certain types of these spectacular stellar explosions, including the nova and a Type 1a supernova. The explosion is created when the white dwarf accretes hydrogen from the companion star, building up mass until the hydrogen undergoes fusion. Some novae are also recurrent, having periodic outbursts of moderate amplitude.

Stars can also vary in luminosity because of extrinsic factors, such as eclipsing binaries, as well as rotating stars that produce extreme starspots. A notable example of an eclipsing binary is Algol, which regularly varies in magnitude from 2.1 to 3.4 over a period of 2.87 days.

The interior of a stable star is in a state of hydrostatic equilibrium: the forces on any small volume almost exactly counterbalance each other. The balanced forces are inward gravitational force and an outward force due to the pressure gradient within the star. The pressure gradient is established by the temperature gradient of the plasma; the outer part of the star is cooler than the core. The temperature at the core of a main sequence or giant star is at least on the order of 10 K. The resulting temperature and pressure at the hydrogen-burning core of a main sequence star are sufficient for nuclear fusion to occur and for sufficient energy to be produced to prevent further collapse of the star.

As atomic nuclei are fused in the core, they emit energy in the form of gamma rays. These photons interact with the surrounding plasma, adding to the thermal energy at the core. Stars on the main sequence convert hydrogen into helium, creating a slowly but steadily increasing proportion of helium in the core. Eventually the helium content becomes predominant, and energy production ceases at the core. Instead, for stars of more than 0.4 , fusion occurs in a slowly expanding shell around the degenerate helium core.

In addition to hydrostatic equilibrium, the interior of a stable star will also maintain an energy balance of thermal equilibrium. There is a radial temperature gradient throughout the interior that results in a flux of energy flowing toward the exterior. The outgoing flux of energy leaving any layer within the star will exactly match the incoming flux from below.

The radiation zone is the region of the stellar interior where the flux of energy outward is dependent on radiative heat transfer, since convective heat transfer is inefficient in that zone. In this region the plasma will not be perturbed, and any mass motions will die out. If this is not the case, however, then the plasma becomes unstable and convection will occur, forming a convection zone. This can occur, for example, in regions where very high energy fluxes occur, such as near the core or in areas with high opacity (making radiatative heat transfer inefficient) as in the outer envelope.

The occurrence of convection in the outer envelope of a main sequence star depends on the star's mass. Stars with several times the mass of the Sun have a convection zone deep within the interior and a radiative zone in the outer layers. Smaller stars such as the Sun are just the opposite, with the convective zone located in the outer layers. Red dwarf stars with less than 0.4 are convective throughout, which prevents the accumulation of a helium core. For most stars the convective zones will also vary over time as the star ages and the constitution of the interior is modified.
The photosphere is that portion of a star that is visible to an observer. This is the layer at which the plasma of the star becomes transparent to photons of light. From here, the energy generated at the core becomes free to propagate into space. It is within the photosphere that sun spots, regions of lower than average temperature, appear.

Above the level of the photosphere is the stellar atmosphere. In a main sequence star such as the Sun, the lowest level of the atmosphere, just above the photosphere, is the thin chromosphere region, where spicules appear and stellar flares begin. Above this is the transition region, where the temperature rapidly increases within a distance of only . Beyond this is the corona, a volume of super-heated plasma that can extend outward to several million kilometres. The existence of a corona appears to be dependent on a convective zone in the outer layers of the star. Despite its high temperature, and the corona emits very little light, due to its low gas density. The corona region of the Sun is normally only visible during a solar eclipse.

From the corona, a stellar wind of plasma particles expands outward from the star, until it interacts with the interstellar medium. For the Sun, the influence of its solar wind extends throughout a bubble-shaped region called the heliosphere.

A variety of nuclear fusion reactions take place in the cores of stars, that depend upon their mass and composition. When nuclei fuse, the mass of the fused product is less than the mass of the original parts. This lost mass is converted to electromagnetic energy, according to the mass–energy equivalence relationship "E" = "mc".

The hydrogen fusion process is temperature-sensitive, so a moderate increase in the core temperature will result in a significant increase in the fusion rate. As a result, the core temperature of main sequence stars only varies from 4 million kelvin for a small M-class star to 40 million kelvin for a massive O-class star.

In the Sun, with a 10-million-kelvin core, hydrogen fuses to form helium in the proton–proton chain reaction:

These reactions result in the overall reaction:

where e is a positron, γ is a gamma ray photon, ν is a neutrino, and H and He are isotopes of hydrogen and helium, respectively. The energy released by this reaction is in millions of electron volts, which is actually only a tiny amount of energy. However enormous numbers of these reactions occur constantly, producing all the energy necessary to sustain the star's radiation output. In comparison, the combustion of two hydrogen gas molecules with one oxygen gas molecule releases only 5.7 eV.
In more massive stars, helium is produced in a cycle of reactions catalyzed by carbon called the carbon-nitrogen-oxygen cycle.

In evolved stars with cores at 100 million kelvin and masses between 0.5 and 10 , helium can be transformed into carbon in the triple-alpha process that uses the intermediate element beryllium:

For an overall reaction of:

In massive stars, heavier elements can also be burned in a contracting core through the neon-burning process and oxygen-burning process. The final stage in the stellar nucleosynthesis process is the silicon-burning process that results in the production of the stable isotope iron-56. Any further fusion would be an endothermic process that consumes energy, and so further energy can only be produced through gravitational collapse.

The example below shows the amount of time required for a star of 20 to consume all of its nuclear fuel. As an O-class main sequence star, it would be 8 times the solar radius and 62,000 times the Sun's luminosity.




</doc>
<doc id="26809" url="https://en.wikipedia.org/wiki?curid=26809" title="StarCraft (video game)">
StarCraft (video game)

StarCraft is a 1998 military science fiction real-time strategy game developed and published by Blizzard Entertainment for Microsoft Windows. The game spawned the StarCraft franchise, and became the first game of the video game series. A Classic Mac OS version was released in 1999, and a Nintendo 64 adaptation, co-developed with Mass Media, was released in 2000.

Blizzard started work on the game shortly after "", another real-time strategy game, was released in 1995. The first incarnation debuted at the 1996 Electronic Entertainment Expo, where it was unfavorably compared to "Warcraft II". As a result, the project was entirely overhauled before being showcased to the public in early 1997, at which time it received a far more positive response. The game's multiplayer is particularly popular in South Korea, as of 2006, where players and teams participate in , earn sponsorships, and compete in televised tournaments.

Set in a fictitious future timeline during the 25th century CE in a distant part of the Milky Way galaxy known as the "Koprulu Sector", the game revolves around three intelligent species fighting for dominance: the Terrans are humans exiled from Earth who are now skilled at adapting to any situation; the Zerg are a race of insectoid aliens in pursuit of genetic perfection and obsessed with assimilating other races; the Protoss are a humanoid species with advanced technology and psionic abilities who are attempting to preserve their civilization and strict philosophy about their way of life from the Zerg.

Many journalists of the video game industry have praised "StarCraft" as one of the most important, and one of the greatest video games of all time. The game is also said to have raised the bar for developing real-time strategy (RTS) games. With more than 11 million copies sold worldwide by February 2009, "StarCraft" became one of the best-selling games for the personal computer. It has been praised for pioneering the use of unique "factions" in RTS gameplay, and for having a compelling story.

"StarCraft" has had its storyline adapted and expanded through a series of novels published between 2000 and 2016, the expansion pack "", and two officially authorized add-ons, and "Retribution". A sequel, "", was released in July 2010, which generated two expansion packs and a campaign pack between 2013 and 2016, while of the original and its expansion pack was released in August 2017. The original game, along with the expansion, was released for free in April 2017.

Blizzard Entertainment's use of three distinct races in "StarCraft" is widely credited with revolutionizing the real-time strategy genre. All units are unique to their respective races, and while rough comparisons can be drawn between certain types of units in the technology tree, every unit performs differently and requires different tactics for a player to succeed.

The psionic and technologically adept Protoss have access to powerful units and machinery and advanced technologies such as energy shields and localized warp capabilities, powered by their psionic traits. However, their forces have lengthy and expensive manufacturing processes, encouraging players to follow a strategy of the quality of their units over the quantity. The insectoid Zerg possess entirely organic units and structures, which can be produced quickly and at a far cheaper cost to resources, but are accordingly weaker, relying on sheer numbers and speed to overwhelm enemies. The humanoid Terrans provide a middle ground between the other two races, providing units that are versatile and flexible. They have access to a range of more ballistic military technologies and machinery, such as tanks and nuclear weapons.

Although each race is unique in its composition, no race has an innate advantage over the other. Each species is balanced out so that while they have different strengths, powers, and abilities, their overall strength is the same. The balance stays complete via infrequent patches (game updates) provided by Blizzard.

"StarCraft" features artificial intelligence that scales in difficulty, although the player cannot change the difficulty level in the single-player campaigns. Each campaign starts with enemy factions running easy AI modes, scaling through the course of the campaign to the hardest AI modes. In the level editor provided with the game, a designer has access to four levels of AI difficulties: "easy", "medium", "hard", and "insane", each setting differing in the units and technologies allowed to an AI faction and the extent of the AI's tactical and strategic planning. The single-player campaign consists of thirty missions, split into ten for each race.

Each race relies on two resources to sustain their game economies and to build their forces: minerals and vespene gas. Minerals are needed for all units and structures, and they are obtained by using a worker unit to harvest the resource directly from mineral nodes scattered around the battlefield. Players require vespene gas to construct advanced units and buildings, and they acquire it by constructing a gas extraction building on top of a geyser and using worker units to extract the gas from it. In addition, players need to regulate the supplies for their forces to ensure that they can construct the number of units they need. Although the nature of the supply differs between the races—Terrans use physical supplies held in depots, Protoss use psionic energy channeled from their homeworld via pylons, and Zerg are regulated by the number of controlling overlord units present—the supply mechanic essentially works in exactly the same way for each race (just with differing impacts on gameplay), allowing players to create new units when there are sufficient resources to sustain them.

Protoss and Zerg building construction is limited to specific locations: Protoss buildings need to be linked to a power grid, while almost every Zerg structure must be placed on a carpet of biomass, called "creep", that is produced by certain structures. Terran buildings are far less limited, with certain primary base structures possessing the ability to take off and fly slowly to a new location. Terran buildings, however, require the worker unit to continue construction on the building until it is completed. Also, once a Terran building has taken a certain amount of damage, it will catch fire and can eventually burn to the ground without further enemy action if repairs are not performed by a worker unit. The Protoss, by contrast, only require a worker unit to begin the process of transporting a building to the theater of operations via warp, and their buildings' shields (but not their structure) are regenerative. The Zerg worker unit physically transforms into the structure created, which is capable of slowly healing itself.

Multiplayer on "StarCraft" is powered through Blizzard Entertainment's Battle.net Internet service. Through this, a maximum of eight players can compete in a variety of game modes, including simply destroying all other players (which may be competitive, as in Ladder play, or non-ranked, as in melee play), to king of the hill and capture the flag objective-based games. In addition, the game incorporates a variety of specialized scenarios for different types of game, such as simulating a football game, using the Terran hoverbike unit to conduct a bike race, or hosting a Zerg hunting competition. "StarCraft" is also one of the few games that include a spawn installation, which allows for limited multiplayer. It must be installed from a disc, and requires a product key to work just as the full version does. However, one product key can support up to eight spawned installations with access to Battle.net. Limitations of a spawned installation include the inability to play single-player missions, create multiplayer games, or use the campaign editor. Newer releases of the game available through Battle.net or discs that include the Windows Vista label don't support the spawn installation.

"StarCraft" takes place in a science fiction universe created by Chris Metzen and James Phinney for Blizzard Entertainment. According to the story presented in the game's manual, the overpopulation of Earth in the early 24th century has caused the international governing body, known as United Powers League (which was later succeeded by United Earth Directorate), to exile certain members of the human race, such as criminals, the cybernetically enhanced, and genetic mutants, to colonize the far reaches of the galaxy. An attempt to colonize a nearby solar system goes wrong, resulting in humanity's arrival in the Koprulu Sector. In the distant Koprulu Sector of the galaxy, the exiles form several governments, but quickly fall into conflict with each other. One government, the Confederacy of Man, eventually emerges as the strongest faction, but its oppressive nature and brutal methods of suppressing dissidents stir up major rebel opposition in the form of a terrorist group called the Sons of Korhal. Just prior to the beginning of the game, in December 2499, an alien race possessing advanced technology and psionic power, the Protoss, makes first contact with humanity by destroying a Confederate colony world without any prior warning. Soon after this, the Terrans discover that a second alien race, the insectoid Zerg, has been stealthily infesting the surface of several of the Terran colonies, and that the Protoss are destroying the planets to prevent the Zerg from spreading. With the Confederacy threatened by two alien races and internal rebellion, it begins to crumble.

The player assumes the role of three nameless characters over the course of the game. In the first act, the player acts as the Confederate magistrate of an outlying colony world of Mar Sara, threatened by both the Zerg and the Protoss, and is forced through events to join the rebel Sons of Korhal under its leader Arcturus Mengsk. Mengsk's campaign is accompanied by Jim Raynor, a morally conscious law enforcement officer from Mar Sara, and Sarah Kerrigan, a psychic assassin and Mengsk's second-in-command. The second episode of the game sees the player as a cerebrate, a commander within the Zerg Swarm. The player is ruled over by the Zerg Overmind — the manifestation of the collective consciousness of the Swarm and the game's primary antagonist — and is given advice from other cerebrates of higher rank and status while accomplishing the objectives of the Swarm. In the final part of "StarCraft", the player is a newly appointed Executor within the Protoss military reporting to Aldaris, a representative of the Protoss government. Aldaris is at odds with Tassadar — the former occupant of the player's position — over his association with Zeratul, a member of a heretical group known as dark templar.

The story of "StarCraft" is presented through its instruction manual, the briefings to each mission, and conversations within the missions themselves, along with the use of cinematic cutscenes at key points. The game itself is split into three episodes, one for the player to command each race. In the first segment of the game, the player and Jim Raynor are attempting to control the colony of Mar Sara in the wake of the Zerg attacks on other Terran worlds. After the Confederacy arrests Raynor for destroying Confederate property, despite the fact that it had been infested by the Zerg, the player joins Arcturus Mengsk and the Sons of Korhal. Raynor, who is freed by Mengsk's troops, also joins and frequently accompanies the player on missions. Mengsk then begins to use Confederate technology captured on Mar Sara to lure the Zerg to Confederate installations and further his own goals. After forcing Confederate general Edmund Duke to join him, Mengsk sacrifices his own second-in-command, Sarah Kerrigan, to ensure the destruction of the Confederacy by luring the Zerg to the Confederate capital Tarsonis. Raynor is outraged by Mengsk's true aims of obtaining power at any cost and deserts, taking with him a small army of the former colonial militia of Mar Sara. Mengsk reorganizes what remains of the Terran population into the Terran Dominion, crowning himself as emperor.

The second campaign reveals that Kerrigan was not killed by the Zerg, but rather is captured and infested in an effort to incorporate her psionic traits into the Zerg gene pool. She emerges with far more psionic powers and physical strength, her DNA completely altered. Meanwhile, the Protoss commander Tassadar discovers that the Zerg's cerebrates cannot be killed by conventional means, but that they can be harmed by the powers wielded by the heretical dark templar. Tassadar allies himself with the dark templar prelate Zeratul, who assassinates Zasz, one of the Zerg's cerebrates in their hive clusters on Char. The cerebrate's death results in its forces running amok through the Zerg hives, but briefly links the minds of Zeratul and the Zerg Overmind, allowing the Overmind to finally learn the location of the Protoss homeworld Aiur, which the Overmind has been seeking for millennia. The main Zerg swarm promptly invades Aiur, while Kerrigan is dispatched to deal with Tassadar, and despite facing heavy Protoss resistance, the Overmind is able to embed itself into the crust of the planet.

The final episode of the game sees Aldaris and the Protoss government branding Tassadar a traitor and a heretic for conspiring with the dark templar. The player (later hinted to be Artanis) initially serves Aldaris in defending Aiur from the Zerg invasion, but while on a mission to arrest Tassadar, the player joins him instead. A Protoss civil war erupts, pitting Tassadar, Zeratul, and their allies against the Protoss establishment. The dark templar prove their worth when they use their energies to slay two more of the Zerg cerebrates on Aiur, and the Conclave reconciles with them. Aided by Raynor's forces—who sided with Tassadar back on Char—the Protoss break through the Overmind's weakened defenses and destroy the Overmind's outer shell, but take heavy casualties in the process. Tassadar channels his own psionic energies in combination with those of the dark templar through the hull of his command ship and crashes it into the Overmind, sacrificing himself in order to destroy it.

Blizzard Entertainment began development on "StarCraft" in 1995, shortly after the release of highly successful "". Using the "Tides of Darkness" game engine as a base, "StarCraft" made its debut at E3 1996. The version of the game displayed, assembled by the team's lead programmer Bob Fitch, received a rather weak response from the convention and was criticized by many for being ""Warcraft" in space." As a consequence the entire project was overhauled, bringing the focus onto creating three distinct species. Bill Roper, one of the game's producers, stated this would be a major departure from the "Warcraft" approach, comparing its two equal sides to those of chess and stating that "StarCraft" would allow players to "develop very unique strategies based on which species is being played, and will require players to think of different strategies to combat the other two species." The hand-drawn graphics seen in the E3 version were also replaced with rendered graphics. In early 1997, the new version of "StarCraft" was unveiled, receiving a far more positive response.
However, the game was still marred by technical difficulties, so Bob Fitch completely redesigned the "Warcraft II" engine within two months to ensure that many of the features desired by the designers, such as the abilities for units to burrow and cloak, could be implemented. Later improvements to the game included pre-rendered sprites and backgrounds, constructed using 3D Studio Max. An isometric in-game view was also adopted, in contrast to "Warcraft II"s 3/4s birdseye perspective. In addition, the game utilized high quality music, composed by Blizzard's resident composers, and professional voice actors were hired.

Despite the progress, "StarCraft" was slow to emerge. The continual delays inspired a group of "StarCraft" fans on the official forums who labeled themselves "Operation: Can't Wait Any Longer" to write a series of fictional stories in which the members of Operation CWAL attempted to retrieve the beta version of "StarCraft" from Blizzard's headquarters in Irvine, California. To pay homage to their presence on the forums and enthusiasm for the game, Blizzard Entertainment later incorporated the group's name into "StarCraft" as a cheat code to speed up the production of units and gave the group thanks in the game's credits. The game was released for Windows on March 31, 1998, with the Classic Mac OS version following a year later in 1999. Development on a Nintendo 64 version, "StarCraft 64", began in 1999, converted from PC by Mass Media Interactive Entertainment—a subsidiary of THQ—and published by Nintendo. "StarCraft 64" was released on June 13, 2000 in the USA and Europe. It was also released in Australia on May 25, 2001.
The musical score to "StarCraft" was composed by Blizzard Entertainment's composers. Glenn Stafford composed the Terran and Protoss in-game themes, while Derek Duke, who was a contracted composer at the time, wrote all the in-game music for the Zerg. The cinematic scores were composed by Stafford and Hayes. Hayes also collaborated with Stafford on one of the Protoss in-game tracks. Tracy W. Bush provided additional support in composing. The musical score of the game was received well by reviewers, who have described it as "appropriately melodic and dark" and "impressive", with one reviewer noting that some of the music owed much of its inspiration to Jerry Goldsmith's score for the film "Alien". The first official game soundtrack, "StarCraft: Game Music Vol. 1", was released in 2000, comprising tracks from both "StarCraft" and "", as well as a sizable portion of remix tracks and music inspired by "StarCraft", created by several South Korean disc jockeys. The soundtrack was distributed by Net Vision Entertainment. In September 2008, Blizzard Entertainment announced that a second soundtrack, "StarCraft Original Soundtrack", had been released on iTunes. This soundtrack consisted entirely of the original music from "StarCraft" and "Brood War", both from in-game themes to music used in the cinematic cut scenes.

Before the release of "StarCraft", Blizzard Entertainment released a free-to-download game demo entitled "Loomings", comprising three missions and a tutorial. The prequel was made available for the full game in October 1999 as a custom map campaign, adding two extra missions and hosting it on Battle.net. In addition, the full release of "StarCraft" included a secondary campaign entitled "Enslavers". Consisting of five missions played as both the Terrans and the Protoss, "Enslavers" is set in the second campaign in "StarCraft" and follows the story of a Terran smuggler who manages to take control of a Zerg cerebrate and is pursued by both the Protoss and Terran Dominion. "Enslavers" acts as an exemplar single-player campaign for the game's level editor, highlighting how to use the features of the program.
"StarCraft"s first expansion, Insurrection, was released for Windows on July 31, 1998. The expansion was developed by Aztech New Media and authorized by Blizzard Entertainment. Its story focused on a separate Confederate colony alluded to in the manual to "StarCraft", following a group of Terran colonists and a Protoss fleet in their fight against the Zerg and a rising local insurgency. "Insurrection" was not received well, being criticized by reviewers for lacking the quality of the original game. "Insurrection" was followed within a few months by a second expansion, Retribution. Developed by Stardock, published by WizardWorks and authorized by Blizzard Entertainment, "Retribution" follows all three races attempting to seize control of a powerful crystal on a Terran Dominion colony. The expansion was not received with critical support, instead being regarded as average but at least challenging. After the release of "Retribution", Blizzard Entertainment announced a new official expansion pack that would continue on the story of "StarCraft". "" was consequently created, developed jointly by Blizzard Entertainment and Saffire Corporation. "Brood War" continues the story of "StarCraft" from days after its conclusion, and was released for both Windows and Mac to critical praise on December 18, 1998 in the US and in March 1999 in Europe.

Before Insurrection, an unauthorized expansion pack, called Stellar Forces, was published by Micro Star but was recalled weeks later when Blizzard won the court case against it. It consisted of 22 single player maps and 32 multi-player maps which are considered to be rather plain.

In 2000, "StarCraft 64" was released In North America for the Nintendo 64, co-developed by Blizzard Entertainment and Mass Media Inc. and published by Nintendo. The game featured all of the missions from both "StarCraft" and the expansion "Brood War", as well as some exclusive missions, such as two different tutorials and a new secret mission, "Resurrection IV". Blizzard Entertainment had previously considered a PlayStation port of the game, but it was decided that the game would instead be released on the Nintendo 64. "Resurrection IV" is set after the conclusion of "Brood War", and follows Jim Raynor embarking on a mission to rescue the "Brood War" character Alexei Stukov, a vice admiral from Earth who has been captured by the Zerg. The "Brood War" missions required the use of a Nintendo 64 memory Expansion Pak to run. In addition, "StarCraft 64" features a split screen cooperative mode, also requiring the expansion pak, allowing two players to control one force in-game. "StarCraft 64" lacked the online multiplayer capabilities and speech in mission briefings. In addition, cut scenes were shortened.

A remastered edition of the game, "", released August 14, 2017, preserves the gameplay of the original while adding support for ultra-high-definition graphics, Blizzard's modern online features, and re-recorded audio (soundtrack and sound effects).

On June 8, 2019, as part of the grand finals of the third season of the KSL, Blizzard announced a graphics overhaul pack for the game by Carbot Animations, the producers of multiple Blizzard-related parody animations, including their first and longest-running one, the "StarCrafts" series. As a graphical overhaul, its effect applies to all game modes and menus in "StarCraft: Remastered". It was released on July 10, 2019 as "StarCraft: Cartooned" alongside an announcer pack featuring South Korean YouTuber and children's television host Hyejin "Hey Jini" Kang.

"StarCraft" was released internationally on March 31, 1998 and became the best-selling PC game for that year, selling over 1.5 million copies worldwide. In the United States, it was best-selling computer game of 1998, with 746,365 units sold. It was the country's 14th best-selling release of the period between 1993 and 1999, selling 916,000 copies. By April 1999, South Korean players had purchased almost 300,000 units of the game. "StarCraft"s worldwide sales reached 4 million units by July 2001; South Korea accounted for 50% of these copies. By May 2007, "StarCraft" had sold over 9.5 million copies across the globe, with 4.5 million of these being sold in South Korea. Since the initial release of "StarCraft", Blizzard Entertainment reported that its Battle.net online multiplayer service grew by 800 percent.

Generally, "StarCraft" was received positively by critics, with many contemporary reviewers noting that while the game may not have deviated significantly from the status quo of most real-time strategy games, it was one of the best to have applied the formula. In addition, "StarCraft"s pioneering use of three distinct, unique, and balanced races over two equal sides was praised by critics, with GameSpot commenting that this helped the game to "avoid the problem that has plagued every other game in the genre". Many critics also praised the strength of the story accompanying the game, with some reviewers being impressed by how well the story was folded into the gameplay. The game's voice acting in particular was praised; GameSpot later hailed the voice work in the game as one of the ten best in the industry at the time. Equally, the multiplayer aspects of the game were positively received. "StarCraft" has received multiple awards, including being named as one of the best games of all time by GameSpot, IGN, and "Game Informer". According to Blizzard Entertainment, "StarCraft" has won 37 awards and has received a star on the floor of the Metreon as part of the Walk of Game in San Francisco in early 2006.

"Next Generation" reviewed the PC version of the game, rating it five stars out of five, and stated that "The quality of the play balancing and the elegance of the design mean that "StarCraft" sets a new high watermark for all real-time strategy games."

Although at the time "StarCraft"s graphics and audio were praised by critics, later reviews have noted that the graphics do not compare to more modern games. The capacity for the game's artificial intelligence to navigate units to waypoints also faced some heavy criticism, with "PC Zone" stating that the inability for developers to make an effective pathfinding system was "the single most infuriating element of the real-time strategy genre". In addition, several reviewers expressed concern over some familiarities between the unit structures of each race, as well as over the potential imbalance of players using rushing tactics early in multiplayer games. Blizzard Entertainment has strived to balance rush tactics in later updates. The Nintendo 64 version of the game was not received as positively by reviewers, and was criticized for poor graphics in comparison to the PC version. However, critics did praise the game and Mass Media for using effective controls on the gamepad and maintaining the high quality audio.

"Starcraft" won the Origins Award for Best Strategy Computer Game of 1998.

In 1998, "PC Gamer" declared it the 5th-best computer game ever released, and the editors called it "a strategy game that continues to evolve and surprise many months after its release, and that currently represents the state of the genre's art".

GameSpot described "StarCraft" as "The defining game of its genre. It is the standard by which all real-time strategy games are judged." IGN stated that "StarCraft" "is hands down one of the best, if not the best, real-time strategy games ever created." "StarCraft" is frequently included in the industry's best games rankings, for example it ranked 37 in "Edge"s top 100 games of all time. "StarCraft" has even been taken into space, as Daniel Barry took a copy of the game with him on the Space Shuttle mission STS-96 in 1999. "StarCraft"s popularity resulted in "Guinness World Records" awarding the game four world records, including "Best Selling PC Strategy Game," "Largest Income in Professional Gaming," and "Largest Audience for a Game Competition" when 120,000 fans turned out to watch the final of the SKY proleague season 2005 in Busan, South Korea. Researchers have shown that the audience for watching "StarCraft" games is diverse and that "StarCraft" uses instances of information asymmetry to make the game more entertaining for spectators. In addition, "StarCraft" has been the subject of an academic course; the University of California, Berkeley offered a student-run introductory course on theory and strategy in spring 2009.

After its release, "StarCraft" rapidly grew in popularity in South Korea, eventually making its way to become the country's national e-sport after establishing a successful pro-gaming scene. Professional gamers in South Korea are media celebrities, and "StarCraft" games are broadcast over three television channels dedicated to the professional gaming scene. Professional gamers in South Korea have gained television contracts, sponsorships, and tournament prizes, allowing one of the most famous players, Lim "BoxeR" Yo-hwan, to gain a fan club of over half a million people. One player, Lee Yun-yeol, reported earnings in 2005 of .

"StarCraft" was part of the United States Air Force's Air and Space Basic Course, used to teach newly active officers about crisis planning under stress and joint service teamwork. Other efforts to make more 'realistic' current-day battle software led to distractions when simulated hardware didn't align with the real hardware active duty officers knew about. The science fiction setting allowed students to focus on the battle tactics.

The annual Conference on Artificial Intelligence and Interactive Digital Entertainment hosts a competition for AIs playing the game. As of 2015, humans still win.

In 2014, an unofficial version for the Pandora handheld and the ARM architecture became available by static recompilation and reverse engineering of the original x86 version.

The storyline of "StarCraft" has been adapted into several novels. The first novel, "Uprising", which was written by Blizzard employee Micky Neilson and published in December 2000, acts as a prequel to the events of "StarCraft". Other novels—"Liberty's Crusade" by Jeff Grubb and Aaron Rosenberg's "Queen of Blades"—retell the story of the game from different perspectives. At BlizzCon 2007, "StarCraft" creator Chris Metzen stated that he hoped to novelize the entirety of "StarCraft" and its expansion "Brood War" into a definitive text-based story. Later novels, such as Gabriel Mesta's "Shadow of the Xel'Naga" and Christie Golden's "The Dark Templar Saga", further expand the storyline, creating the setting for "".

A number of action figures and collectable statues based upon the characters and units in "StarCraft" have been produced by ToyCom. A number of model kits, made by Academy Hobby Model Kits, were also produced, displaying 1/30 scale versions of the marine and the hydralisk. In addition, Blizzard Entertainment teamed up with Fantasy Flight Games to create a board game with detailed sculptures of game characters. Blizzard Entertainment also licensed Wizards of the Coast to produce an Alternity based game entitled "StarCraft Adventures".




</doc>
<doc id="26810" url="https://en.wikipedia.org/wiki?curid=26810" title="Skepticism">
Skepticism

Skepticism (American English) or scepticism (British English, Australian English, and Canadian English) is generally a questioning attitude or doubt towards one or more items of putative knowledge or belief or dogma. It is often directed at domains, such as the supernatural, morality (moral skepticism), theism (skepticism about the existence of God), or knowledge (skepticism about the possibility of knowledge, or of certainty). Formally, skepticism as a topic occurs in the context of philosophy, particularly epistemology, although it can be applied to any topic such as politics, religion, and pseudoscience.

Philosophical skepticism comes in various forms. Radical forms of skepticism deny that knowledge or rational belief is possible and urge us to suspend judgment on many or all controversial matters. More moderate forms of skepticism claim only that nothing can be known with certainty, or that we can know little or nothing about the big questions in life, such as whether God exists or whether there is an afterlife. Religious skepticism is "doubt concerning basic religious principles (such as immortality, providence, and revelation)". Scientific skepticism concerns testing beliefs for reliability, by subjecting them to systematic investigation using the scientific method, to discover empirical evidence for them.

In ordinary usage, skepticism (US) or scepticism (UK) (Greek: 'σκέπτομαι' "skeptomai", to search, to think about or look for; see also spelling differences) can refer to:


In philosophy, skepticism can refer to:

As a philosophical school or movement, skepticism arose both in ancient Greece and India. In India the Ajñana school of philosophy espoused skepticism. It was a major early rival of Buddhism and Jainism, and a major influence on Buddhism. Two of the foremost disciples of the Buddha, Sariputta and Moggallāna, were initially the students of the Ajñana philosopher Sanjaya Belatthiputta, and a strong element of skepticism is found in Early Buddhism, most particularly in the Aṭṭhakavagga sutra. Since skepticism is a philosophical attitude and a style of philosophising rather than a position, the Ajñanins may have influenced other skeptical thinkers of India such as Nagarjuna, Jayarāśi Bhaṭṭa, and Shriharsha.

In Greece philosophers as early as Xenophanes (c. 570 – c. 475 BC) expressed skeptical views, as did Democritus 
and a number of Sophists. Gorgias, for example, reputedly argued that nothing exists, that even if there were something we could not know it, and that even if we could know it we could not communicate it. The Heraclitean philosopher Cratylus refused to discuss anything and would merely wriggle his finger, claiming that communication is impossible since meanings are constantly changing. Socrates also had skeptical tendencies, claiming that he knew nothing, or at least nothing worthwhile.

There were two major schools of skepticism in the ancient Greek and Roman world. The first was Pyrrhonism, was founded by Pyrrho of Elis (c. 360–270 BCE). The second was Academic Skepticism, so-called because its two leading defenders, Arcesilaus (c. 315–240 BCE) who initiated the philosophy, and Carneades (c. 217–128 BCE), the philosophy's most famous proponent, were heads of Plato's Academy. Pyrrhonism's aims are psychological. It urges suspension of judgment ("epoche") to achieve mental tranquility ("ataraxia"). The Academic Skeptics denied that knowledge is possible. The Academic Skeptics claimed that some beliefs are more reasonable or probable than others, whereas Pyrrhonian skeptics argue that equally compelling arguments can be given for or against any disputed view. Nearly all the writings of the ancient skeptics are now lost. Most of what we know about ancient skepticism is from Sextus Empiricus, a Pyrrhonian skeptic who lived in the second or third century CE. His works contain a lucid summary of stock skeptical arguments.

Ancient skepticism faded out during the late Roman Empire, particularly after Augustine (354–430 CE) attacked the skeptics in his work "Against the Academics" (386 CE). There was little knowledge of, or interest in, ancient skepticism in Christian Europe during the Middle Ages. Interest revived during the Renaissance and Reformation, particularly after the complete writings of Sextus Empiricus were translated into Latin in 1569. A number of Catholic writers, including Francisco Sanches (c. 1550–1623), Michel de Montaigne (1533–1592), Pierre Gassendi (1592–1655), and Marin Mersenne (1588–1648) deployed ancient skeptical arguments to defend moderate forms of skepticism and to argue that faith, rather than reason, must be the primary guide to truth. Similar arguments were offered later (perhaps ironically) by the Protestant thinker Pierre Bayle in his influential Historical and Critical Dictionary (1697–1702).

The growing popularity of skeptical views created an intellectual crisis in seventeenth-century Europe. One major response was offered by the French philosopher and mathematician René Descartes (1596–1650). In his classic work, "Meditations of First Philosophy" (1641), Descartes sought to refute skepticism, but only after he had formulated the case for skepticism as powerfully as possible. Descartes argued that no matter what radical skeptical possibilities we imagine there are certain truths (e.g., that thinking is occurring, or that I exist) that are absolutely certain. Thus, the ancient skeptics were wrong to claim that knowledge is impossible. Descartes also attempted to refute skeptical doubts about the reliability of our senses, our memory, and other cognitive faculties. To do this, Descartes tried to prove that God exists and that God would not allow us to be systematically deceived about the nature of reality. Many contemporary philosophers question whether this second stage of Descartes’ critique of skepticism is successful.

In the eighteenth century a powerful new case for skepticism was offered by the Scottish philosopher David Hume (1711–1776). Hume was an empiricist, claiming that all genuine ideas can be traced back to original impressions of sensation or introspective consciousness. Hume argued forcefully that on empiricist grounds there are no sound reasons for belief in God, an enduring self or soul, an external world, causal necessity, objective morality, or inductive reasoning. In fact, he argued that "Philosophy would render us entirely Pyrrhonian, were not Nature too strong for it." As Hume saw it, the real basis of human belief is not reason, but custom or habit. We are hard-wired by nature to trust, say, our memories or inductive reasoning, and no skeptical arguments, however powerful, can dislodge those beliefs. In this way, Hume embraced what he called a "mitigated" skepticism, while rejecting an "excessive" Pyrrhonian skepticism that he saw as both impractical and psychologically impossible.

Hume's skepticism provoked a number of important responses. Hume's Scottish contemporary, Thomas Reid (1710–1796), challenged Hume's strict empiricism and argued that it is rational to accept "common-sense" beliefs such as the basic reliability of our senses, our reason, our memories, and inductive reasoning, even though none of these things can be proved. In Reid's view, such common-sense beliefs are foundational and require no proof in order to be rationally justified. Not long after Hume's death, the great German philosopher Immanuel Kant (1724–1804) argued that human moral awareness makes no sense unless we reject Hume's skeptical conclusions about the existence of God, the soul, free will, and an afterlife. According to Kant, while Hume was right to claim that we cannot strictly "know" any of these things, our moral experience entitles us to believe in them.

Today, skepticism continues to be a topic of lively debate among philosophers.

Religious skepticism generally refers to doubting given religious beliefs or claims. Historically, religious skepticism can be traced back to Xenophanes, who doubted many religious claims of his time. Modern religious skepticism typically emphasizes scientific and historical methods or evidence, with Michael Shermer writing that skepticism is a process for discovering the truth rather than general non-acceptance. For example, a religious skeptic might believe that Jesus existed while questioning claims that he was the messiah or performed miracles (see historicity of Jesus). Religious skepticism is not the same as atheism or agnosticism, though these often do involve skeptical attitudes toward religion and philosophical theology (for example, towards divine omnipotence). Religious people are generally skeptical about claims of other religions, at least when the two denominations conflict concerning some stated belief. Additionally, they may also be skeptical of the claims made by atheists. The historian Will Durant writes that Plato was "as skeptical of atheism as of any other dogma".

A scientific or empirical skeptic is one who questions beliefs on the basis of scientific understanding and empirical evidence.

Scientific skepticism may discard beliefs pertaining to "purported phenomena" not subject to reliable observation and thus not systematic or testable empirically. Most scientists, being scientific skeptics, test the reliability of certain kinds of claims by subjecting them to a systematic investigation using some type of the scientific method. As a result, a number of claims are considered as "pseudoscience", if they are found to improperly apply or ignore the fundamental aspects of the scientific method.

Professional skepticism is an important concept in auditing. It requires an auditor to have a "questioning mind", to make a critical assessment of evidence, and to consider the sufficiency of the evidence.







</doc>
<doc id="26818" url="https://en.wikipedia.org/wiki?curid=26818" title="Stagflation">
Stagflation

In economics, stagflation, or recession-inflation, is a situation in which the inflation rate is high, the economic growth rate slows, and unemployment remains steadily high. It presents a dilemma for economic policy, since actions intended to lower inflation may exacerbate unemployment.

The term, a portmanteau of "stagnation" and "inflation", is generally attributed to Iain Macleod, a British Conservative Party politician who became Chancellor of the Exchequer in 1970. Macleod used the word in a 1965 speech to Parliament during a period of simultaneously high inflation and unemployment in the United Kingdom.
Warning the House of Commons of the gravity of the situation, he said: Macleod used the term again on 7 July 1970, and the media began also to use it, for example in "The Economist" on 15 August 1970, and "Newsweek" on 19 March 1973.

John Maynard Keynes did not use the term, but some of his work refers to the conditions that most would recognise as stagflation. In the version of Keynesian macroeconomic theory that was dominant between the end of World War II and the late 1970s, inflation and recession were regarded as mutually exclusive, the relationship between the two being described by the Phillips curve. Stagflation is very costly and difficult to eradicate once it starts, both in social terms and in budget deficits.

The term "stagflation", a portmanteau of "stagnation" and "inflation", was first coined during a period of inflation and unemployment in the United Kingdom. The United Kingdom experienced an outbreak of inflation in the 1960s and 1970s.
inflation rose in the 1960s and 1970s, UK policy makers failed to recognize the primary role of monetary policy in controlling inflation. Instead, they attempted to use non-monetary policies and devices to respond to the economic crisis. Policy makers also made "inaccurate estimates of the degree of excess demand in the economy, [which] contributed significantly to the outbreak of inflation in the United Kingdom in the 1960s and 1970s.

Stagflation was not limited to the United Kingdom, however. Economists have shown that stagflation was prevalent among seven major market economies from 1973 to 1982. After inflation rates began to fall in 1982, economists' focus shifted from the causes of stagflation to the "determinants of productivity growth and the effects of real wages on the demand for labor".

Economists offer two principal explanations for why stagflation occurs. First, stagflation can result when the economy faces a supply shock, such as a rapid increase in the price of oil. An unfavorable situation like that tends to raise prices at the same time as it slows economic growth by making production more costly and less profitable.

Second, the government can cause stagflation if it creates policies that harm industry while growing the money supply too quickly. These two things would probably have to occur simultaneously because policies that slow economic growth do not usually cause inflation, and policies that cause inflation do not usually slow economic growth.

Both explanations are offered in analyses of the global stagflation of the 1970s. It began with a huge rise in oil prices, but then continued as central banks used excessively stimulative monetary policy to counteract the resulting recession, causing a price/wage spiral.

Up to the 1960s, many Keynesian economists ignored the possibility of stagflation, because historical experience suggested that high unemployment was typically associated with low inflation, and vice versa (this relationship is called the Phillips curve). The idea was that high demand for goods drives up prices, and also encourages firms to hire more; and likewise high employment raises demand. However, in the 1970s and 1980s, when stagflation occurred, it became obvious that the relationship between inflation and employment levels was not necessarily stable: that is, the Phillips relationship could shift. Macroeconomists became more skeptical of Keynesian theories, and Keynesians themselves reconsidered their ideas in search of an explanation for stagflation.

The explanation for the shift of the Phillips curve was initially provided by the monetarist economist Milton Friedman, and also by Edmund Phelps. Both argued that when workers and firms begin to expect more inflation, the Phillips curve shifts up (meaning that more inflation occurs at any given level of unemployment). In particular, they suggested that if inflation lasted for several years, workers and firms would start to take it into account during wage negotiations, causing workers' wages and firms' costs to rise more quickly, thus further increasing inflation. While this idea was a severe criticism of early Keynesian theories, it was gradually accepted by most Keynesians, and has been incorporated into New Keynesian economic models.

Neo-Keynesian theory distinguished two distinct kinds of inflation: demand-pull (caused by shifts of the aggregate demand curve) and cost-push (caused by shifts of the aggregate supply curve). Stagflation, in this view, is caused by cost-push inflation. Cost-push inflation occurs when some force or condition increases the costs of production. This could be caused by government policies (such as taxes) or from purely external factors such as a shortage of natural resources or an act of war.

Contemporary Keynesian analyses argue that stagflation can be understood by distinguishing factors that affect aggregate demand from those that affect aggregate supply. While monetary and fiscal policy can be used to stabilise the economy in the face of aggregate demand fluctuations, they are not very useful in confronting aggregate supply fluctuations. In particular, an adverse shock to aggregate supply, such as an increase in oil prices, can give rise to stagflation.

Supply theories are based on the neo-Keynesian cost-push model and attribute stagflation to significant disruptions to the supply side of the supply-demand market equation, such as when there is a sudden real or relative scarcity of key commodities, natural resources, or natural capital needed to produce goods and services. Other factors may also cause supply problems, for example, social and political conditions such as policy changes, acts of war, extremely restrictive government control of production. In this view, stagflation is thought to occur when there is an adverse supply shock (for example, a sudden increase in the price of oil or a new tax) that causes a subsequent jump in the "cost" of goods and services (often at the wholesale level). In technical terms, this results in contraction or negative shift in an economy's aggregate supply curve.

In the resource scarcity scenario (Zinam 1982), stagflation results when economic growth is inhibited by a restricted supply of raw materials. That is, when the actual or relative supply of basic materials (fossil fuels (energy), minerals, agricultural land in production, timber, etc.) decreases and/or cannot be increased fast enough in response to rising or continuing demand. The resource shortage may be a real physical shortage, or a relative scarcity due to factors such as taxes or bad monetary policy influencing the "cost" or availability of raw materials. This is consistent with the cost-push inflation factors in neo-Keynesian theory (above). The way this plays out is that after supply shock occurs, the economy first tries to maintain momentum. That is, consumers and businesses begin paying higher prices to maintain their level of demand. The central bank may exacerbate this by increasing the money supply, by lowering interest rates for example, in an effort to combat a recession. The increased money supply props up the demand for goods and services, though demand would normally drop during a recession.

In the Keynesian model, higher prices prompt increases in the supply of goods and services. However, during a supply shock (i.e., scarcity, "bottleneck" in resources, etc.), supplies do not respond as they normally would to these price pressures. So, inflation jumps and output drops, producing stagflation.

Following Richard Nixon's imposition of wage and price controls on 15 August 1971, an initial wave of cost-push shocks in commodities were blamed for causing spiraling prices. The second major shock was the 1973 oil crisis, when the Organization of Petroleum Exporting Countries (OPEC) constrained the worldwide supply of oil. Both events, combined with the overall energy shortage that characterized the 1970s, resulted in actual or relative scarcity of raw materials. The price controls resulted in shortages at the point of purchase, causing, for example, queues of consumers at fuelling stations and increased production costs for industry.

Through the mid-1970s, it was alleged that none of the major macroeconomic models (Keynesian, New Classical, and monetarist) were able to explain stagflation.

Later, an explanation was provided based on the effects of adverse supply shocks on both inflation and output. According to Blanchard (2009), these adverse events were one of two components of stagflation; the other was "ideas"—which Robert Lucas, Thomas Sargent, and Robert Barro were cited as expressing as "wildly incorrect" and "fundamentally flawed" predictions (of Keynesian economics) which, they said, left stagflation to be explained by "contemporary students of the business cycle". In this discussion, Blanchard hypothesizes that the recent oil price increases could trigger another period of stagflation, although this has not yet happened (pg. 152).

A purely neoclassical view of the macroeconomy rejects the idea that monetary policy can have real effects. Neoclassical macroeconomists argue that real economic quantities, like real output, employment, and unemployment, are determined by real factors only. Nominal factors like changes in the money supply only affect nominal variables like inflation. The neoclassical idea that nominal factors cannot have real effects is often called "monetary neutrality" or also the "classical dichotomy".

Since the neoclassical viewpoint says that real phenomena like unemployment are essentially unrelated to nominal phenomena like inflation, a neoclassical economist would offer two separate explanations for 'stagnation' and 'inflation'. Neoclassical explanations of stagnation (low growth and high unemployment) include inefficient government regulations or high benefits for the unemployed that give people less incentive to look for jobs. Another neoclassical explanation of stagnation is given by real business cycle theory, in which any decrease in labour productivity makes it efficient to work less. The main neoclassical explanation of inflation is very simple: it happens when the monetary authorities increase the money supply too much.

In the neoclassical viewpoint, the real factors that determine output and unemployment affect the aggregate supply curve only. The nominal factors that determine inflation affect the aggregate demand curve only. When some adverse changes in real factors are shifting the aggregate supply curve left at the same time that unwise monetary policies are shifting the aggregate demand curve right, the result is stagflation.

Thus the main explanation for stagflation under a classical view of the economy is simply policy errors that affect both inflation and the labour market. Ironically, a very clear argument in favour of the classical explanation of stagflation was provided by Keynes himself. In 1919, John Maynard Keynes described the inflation and economic stagnation gripping Europe in his book The Economic Consequences of the Peace. Keynes wrote:

Keynes explicitly pointed out the relationship between governments printing money and inflation.

Keynes also pointed out how government price controls discourage production.

Keynes detailed the relationship between German government deficits and inflation.

While most economists believe that changes in money supply can have some real effects in the short run, neoclassical and neo-Keynesian economists tend to agree that there are no long-run effects from changing the money supply. Therefore, even economists who consider themselves neo-Keynesians usually believe that in the long run, money is neutral. In other words, while neoclassical and neo-Keynesian models are often seen as competing points of view, they can also be seen as two descriptions appropriate for different time horizons. Many mainstream textbooks today treat the neo-Keynesian model as a more appropriate description of the economy in the short run, when prices are 'sticky', and treat the neoclassical model as a more appropriate description of the economy in the long run, when prices have sufficient time to adjust fully.

Therefore, while mainstream economists today might often attribute short periods of stagflation (not more than a few years) to adverse changes in supply, they would not accept this as an explanation of very prolonged stagflation. More prolonged stagflation would be explained as the effect of inappropriate government policies: excessive regulation of product markets and labor markets leading to long-run stagnation, and excessive growth of the money supply leading to long-run inflation.

Political economists Jonathan Nitzan and Shimshon Bichler have proposed an explanation of stagflation as part of a theory they call differential accumulation, which says firms seek to beat the average profit and capitalisation rather than maximise. According to this theory, periods of mergers and acquisitions oscillate with periods of stagflation. When mergers and acquisitions are no longer politically feasible (governments clamp down with anti-monopoly rules), stagflation is used as an alternative to have higher relative profit than the competition. With increasing mergers and acquisitions, the power to implement stagflation increases.

Stagflation appears as a societal crisis, such as during the period of the oil crisis in the 70s and in 2007 to 2010. Inflation in stagflation, however, does not affect all firms equally. Dominant firms are able to increase their own prices at a faster rate than competitors. While in the aggregate no one appears to profit, differentially dominant firms improve their positions with higher relative profits and higher relative capitalisation. Stagflation is not due to any actual supply shock, but because of the societal crisis that hints at a supply crisis. It is mostly a 20th and 21st century phenomenon that has been mainly used by the "weapondollar-petrodollar coalition" creating or using Middle East crises for the benefit of pecuniary interests.

Demand-pull stagflation theory explores the idea that stagflation can result exclusively from monetary shocks without any concurrent supply shocks or negative shifts in economic output potential. Demand-pull theory describes a scenario where stagflation can occur following a period of monetary policy implementations that cause inflation. This theory was first proposed in 1999 by Eduardo Loyo of Harvard University's John F. Kennedy School of Government.

Supply-side economics emerged as a response to US stagflation in the 1970s. It largely attributed inflation to the ending of the Bretton Woods system in 1971 and the lack of a specific price reference in the subsequent monetary policies (Keynesian and Monetarism). Supply-side economists asserted that the contraction component of stagflation resulted from an inflation-induced rise in real tax rates (see bracket creep)

Adherents to the Austrian School maintain that creation of new money ex nihilo benefits the creators and early recipients of the new money relative to late recipients. Money creation is not wealth creation; it merely allows early money recipients to outbid late recipients for resources, goods, and services.
Since the actual producers of wealth are typically late recipients, increases in the money supply weakens wealth formation and undermines the rate of economic growth. Says Austrian economist Frank Shostak:

"The increase in the money supply rate of growth coupled with the slowdown in the rate of growth of goods produced is what the increase in the rate of price inflation is all about. (Note that a price is the amount of money paid for a unit of a good.) What we have here is a faster increase in price inflation and a decline in the rate of growth in the production of goods. But this is exactly what stagflation is all about, i.e., an increase in price inflation and a fall in real economic growth. Popular opinion is that stagflation is totally made up. It seems therefore that the phenomenon of stagflation is the normal outcome of loose monetary policy. This is in agreement with [Phelps and Friedman (PF)]. Contrary to PF, however, we maintain that stagflation is not caused by the fact that in the short run people are fooled by the central bank. Stagflation is the natural result of monetary pumping which weakens the pace of economic growth and at the same time raises the rate of increase of the prices of goods and services."

In 1984, journalist and activist Jane Jacobs proposed the failure of major macroeconomic theories to explain stagflation was due to their focus on the nation as the salient unit of economic analysis, rather than the city. She proposed that the key to avoiding stagflation was for a nation to focus on the development of "import-replacing cities" that would experience economic ups and downs at different times, providing overall national stability and avoiding widespread stagflation. According to Jacobs, import-replacing cities are those with developed economies that balance their own production with domestic imports—so they can respond with flexibility as economic supply and demand cycles change. While lauding her originality, clarity, and consistency, urban planning scholars have criticized Jacobs for not comparing her own ideas to those of major theorists (e.g., Adam Smith, Karl Marx) with the same depth and breadth they developed, as well as a lack of scholarly documentation. Despite these issues, Jacobs' work is notable for having widespread public readership and influence on decision-makers.

Stagflation undermined support for the Keynesian consensus.

Federal Reserve chairman Paul Volcker very sharply increased interest rates from 1979–1983 in what was called a "disinflationary scenario". After U.S. prime interest rates had soared into the double-digits, inflation did come down; these interest rates were the highest long-term prime interest rates that had ever existed in modern capital markets. Volcker is often credited with having stopped at least the inflationary side of stagflation, although the American economy also dipped into recession. Starting in approximately 1983, growth began a recovery. Both fiscal stimulus and money supply growth were policy at this time. A five- to six-year jump in unemployment during the Volcker disinflation suggests Volcker may have trusted unemployment to self-correct and return to its natural rate within a reasonable period.


</doc>
<doc id="26819" url="https://en.wikipedia.org/wiki?curid=26819" title="Soundness">
Soundness

In mathematical logic, a logical system has the soundness property if and only if every formula that can be proved in the system is logically valid with respect to the semantics of the system.

The converse of soundness is known as completeness. In most cases, this comes down to its rules having the property of "preserving truth".

A system with syntactic entailment formula_1 and semantic entailment formula_2 is sound if for any sequence formula_3 of sentences in its language, if formula_4, then formula_5. In other words, a system is sound when all of its theorems are tautologies.

Soundness is among the most fundamental properties of mathematical logic. The soundness property provides the initial reason for counting a logical system as desirable. The completeness property means that every validity (truth) is provable. Together they imply that all and only validities are provable.

Most proofs of soundness are trivial. For example, in an axiomatic system, proof of soundness amounts to verifying the validity of the axioms and that the rules of inference preserve validity (or the weaker property, truth). If the system allows Hilbert-style deduction, it requires only verifying the validity of the axioms and one rule of inference, namely modus ponens. (and sometimes substitution)

Soundness properties come in two main varieties: weak and strong soundness, of which the former is a restricted form of the latter.

Soundness of a deductive system is the property that any sentence that is provable in that deductive system is also true on all interpretations or structures of the semantic theory for the language upon which that theory is based. In symbols, where "S" is the deductive system, "L" the language together with its semantic theory, and "P" a sentence of "L": if ⊢ "P", then also ⊨ "P".

Strong soundness of a deductive system is the property that any sentence "P" of the language upon which the deductive system is based that is derivable from a set Γ of sentences of that language is also a logical consequence of that set, in the sense that any model that makes all members of Γ true will also make "P" true. In symbols where Γ is a set of sentences of "L": if Γ ⊢ "P", then also Γ ⊨ "P". Notice that in the statement of strong soundness, when Γ is empty, we have the statement of weak soundness.

If "T" is a theory whose objects of discourse can be interpreted as natural numbers, we say "T" is "arithmetically sound" if all theorems of "T" are actually true about the standard mathematical integers. For further information, see ω-consistent theory.

The converse of the soundness property is the semantic completeness property. A deductive system with a semantic theory is strongly complete if every sentence "P" that is a semantic consequence of a set of sentences Γ can be derived in the deduction system from that set. In symbols: whenever , then also . Completeness of first-order logic was first explicitly established by Gödel, though some of the main results were contained in earlier work of Skolem.

Informally, a soundness theorem for a deductive system expresses that all provable sentences are true. Completeness states that all true sentences are provable.

Gödel's first incompleteness theorem shows that for languages sufficient for doing a certain amount of arithmetic, there can be no consistent and effective deductive system that is complete with respect to the intended interpretation of the symbolism of that language. Thus, not all sound deductive systems are complete in this special sense of completeness, in which the class of models (up to isomorphism) is restricted to the intended one. The original completeness proof applies to "all" classical models, not some special proper subclass of intended ones.





</doc>
<doc id="26820" url="https://en.wikipedia.org/wiki?curid=26820" title="Syllabary">
Syllabary

A syllabary is a set of written symbols that represent the syllables or (more frequently) moras which make up words. 
A symbol in a syllabary, called a syllabogram, typically represents an (optional) consonant sound (simple onset) followed by a vowel sound (nucleus)—that is, a CV or V syllable—but other phonographic mappings such as CVC, CV- tone, and C (normally nasals at the end of syllables) are also found in syllabaries.

A writing system using a syllabary is "complete" when it covers all syllables in the corresponding spoken language without requiring complex orthographic / graphemic rules, like implicit codas ( ⇒ /CVC/) silent vowels ( ⇒ /CVC/) or echo vowels ( ⇒ /CVC/). This loosely corresponds to "shallow" orthographies in alphabetic writing systems.

"True" syllabograms are those that encompass all parts of a syllable, i.e. initial onset, medial nucleus and final coda, but since onset and coda are optional in at least some languages, there are "middle" (nucleus), "start" (onset-nucleus), "end" (nucleus-coda) and "full" (onset-nucleus-coda) true syllabograms. Most syllabaries only feature one or two kinds of syllabograms and form other syllables by graphemic rules.

Syllabograms, hence syllabaries, are "pure", "analytic" or "arbitrary" if they do not share graphic similarities that correspond to phonic similarities, e.g. the symbol for "ka" does not resemble in any predictable way the symbol for "ki", nor the symbol for "a".
Otherwise they are "synthetic", if they vary by onset, rime, nucleus "or" coda, or "systematic", if they vary by all of them.
Some scholars, e.g. Daniels, reserve the general term for analytic syllabaries and invent other terms (abugida, abjad) as necessary. Some system provides katakana language conversion.

Languages that use syllabic writing include Japanese, Cherokee, Vai, the Yi languages of eastern Asia, the English-based creole language Ndyuka, Shaozhou Tuhua, and the ancient language Mycenaean Greek (Linear B). In addition, the undecoded Cretan Linear A is also believed by some to be a syllabic script, though this is not proven.

Chinese characters, the cuneiform script used for Sumerian, Akkadian and other languages, and the former Maya script are largely syllabic in nature, although based on logograms. They are therefore sometimes referred to as "logosyllabic".

The contemporary Japanese language uses two syllabaries together called kana (in addition to the non-syllabic systems kanji and romaji), namely hiragana and katakana, which were developed around 700. Because Japanese uses mainly CV (consonant + vowel) syllables, a syllabary is well suited to write the language. As in many syllabaries, vowel sequences and final consonants are written with separate glyphs, so that both "atta" and "kaita" are written with three kana: あった ("a-t-ta") and かいた ("ka-i-ta"). It is therefore sometimes called a "moraic" writing system.

Languages that use syllabaries today tend to have simple phonotactics, with a predominance of monomoraic (CV) syllables. For example, the modern Yi script is used to write languages that have no diphthongs or syllable codas; unusually among syllabaries, there is a separate glyph for every consonant-vowel-tone combination (CVT) in the language (apart from one tone which is indicated with a diacritic).

Few syllabaries have glyphs for syllables that are not monomoraic, and those that once did have simplified over time to eliminate that complexity. 
For example, the Vai syllabary originally had separate glyphs for syllables ending in a coda "(doŋ)," a long vowel "(soo)," or a diphthong "(bai)," though not enough glyphs to distinguish all CV combinations (some distinctions were ignored). The modern script has been expanded to cover all moras, but at the same time reduced to exclude all other syllables. Bimoraic syllables are now written with two letters, as in Japanese: diphthongs are written with the help of V or "h"V glyphs, and the nasal coda is written with the glyph for "ŋ", which can form a syllable of its own in Vai.

In Linear B, which was used to transcribe Mycenaean Greek, a language with complex syllables, complex consonant onsets were either written with two glyphs or simplified to one, while codas were generally ignored, e.g. "ko-no-so" for "Knōsos", "pe-ma" for "sperma."

The Cherokee syllabary generally uses dummy vowels for coda consonants, but also has a segmental grapheme for /s/, which can be used both as a coda and in an initial /sC/ consonant cluster.

The languages of India and Southeast Asia, as well as the Ethiopian Semitic languages, have a type of alphabet called an "abugida" or "alphasyllabary". In these scripts, unlike in pure syllabaries, syllables starting with the same consonant are generally expressed with graphemes based in a regular way on a common graphical element. Usually each character representing a syllable consists of several elements which designate the individual sounds of that syllable.

In the 19th century these systems were called "syllabics", a term which has survived in the name of Canadian Aboriginal syllabics (also an abugida).

In a true syllabary there may be graphic similarity between characters that share a common consonant or vowel sound, but it is not systematic or at all regular. For example, the characters for 'ke', 'ka', and 'ko' in Japanese hiragana have no similarity to indicate their common "k" sound (these being: け, か and こ). Compare this with Devanagari, an abugida, where the characters for 'ke', 'ka' and 'ko' are के, का and को respectively, with क indicating their common "k" sound.

English, along with many other Indo-European languages like German and Russian, allows for complex syllable structures, making it cumbersome to write English words with a syllabary. A "pure" syllabary based on English would require a separate glyph for every possible syllable. Thus one would need separate symbols for "bag", "beg", "big", "bog", "bug", "bad", "bed", "bid", "bod", "bud", "bead", "bide", "bode", etc. Since English has well over 10,000 different possibilities for individual syllables, a syllabary would be poorly suited to represent the English language. However, such pure systems are rare. A workaround to this problem, common to several syllabaries around the world (including English loanwords in Japanese), is to write an echo vowel, as if the syllable coda were a second syllable: "ba-gu" for "bag", etc. Another common approach is to simply ignore the coda, so that "bag" would be written "ba". This obviously would not work well for English, but was done in Mycenaean Greek when the root word was two or three syllables long and the syllable coda was a weak consonant such as "n" or "s" (example: χρυσός "chrysos" written as "ku-ru-so").



</doc>
<doc id="26822" url="https://en.wikipedia.org/wiki?curid=26822" title="Steve Reich">
Steve Reich

Stephen Michael Reich ( born October 3, 1936) is an American composer known for his contribution to the development of minimal music in the mid to late 1960s.

Reich's work is marked by its use of repetitive figures, slow harmonic rhythm, and canons. His innovations include using tape loops to create phasing patterns, as on the early compositions "It's Gonna Rain" (1965) and "Come Out" (1966), and the use of simple, audible processes, as on "Pendulum Music" (1968) and "Four Organs" (1970). The 1978 recording "Music for 18 Musicians" would help entrench minimalism as a movement. Reich's work took on a darker character in the 1980s with the introduction of historical themes as well as themes from his Jewish heritage, notably "Different Trains" (1988).

Reich's style of composition has influenced many contemporary composers and groups, especially in the US. Writing in "The Guardian", music critic Andrew Clements suggested that Reich is one of "a handful of living composers who can legitimately claim to have altered the direction of musical history". 

Reich was born in New York City to the Broadway lyricist June Sillman and Leonard Reich. When he was one year old, his parents divorced, and Reich divided his time between New York and California. He is the half-brother of writer Jonathan Carroll. He was given piano lessons as a child and describes growing up with the "middle-class favorites", having no exposure to music written before 1750 or after 1900. At the age of 14 he began to study music in earnest, after hearing music from the Baroque period and earlier, as well as music of the 20th century. Reich studied drums with Roland Kohloff in order to play jazz. While attending Cornell University, he minored in music and graduated in 1957 with a B.A. in Philosophy. Reich's B.A. thesis was on Ludwig Wittgenstein; later he would set texts by that philosopher to music in "Proverb" (1995) and "You Are (variations)" (2006).

For a year following graduation, Reich studied composition privately with Hall Overton before he enrolled at Juilliard to work with William Bergsma and Vincent Persichetti (1958–1961). Subsequently, he attended Mills College in Oakland, California, where he studied with Luciano Berio and Darius Milhaud (1961–1963) and earned a master's degree in composition. At Mills, Reich composed "Melodica" for melodica and tape, which appeared in 1986 on the three-LP release "Music from Mills".

Reich worked with the San Francisco Tape Music Center along with Pauline Oliveros, Ramon Sender, Morton Subotnick, Phil Lesh and Terry Riley. He was involved with the premiere of Riley's "In C" and suggested the use of the eighth note pulse, which is now standard in performance of the piece.

Reich's early forays into composition involved experimentation with twelve-tone composition, but he found the rhythmic aspects of the number twelve more interesting than the pitch aspects. Reich also composed film soundtracks for "Plastic Haircut" (1963), "Oh Dem Watermelons" (1965), and "Thick Pucker" (1965), three films by Robert Nelson. The soundtrack of "Plastic Haircut", composed in 1963, was a short tape collage, possibly Reich's first. The "Watermelons" soundtrack used two 19th-century minstrel tunes as its basis, and used repeated phrasing together in a large five-part canon. The music for "Thick Pucker" arose from street recordings Reich made walking around San Francisco with Nelson, who filmed in black and white 16mm. This film no longer survives. A fourth film from 1965, about 25 minutes long and tentatively entitled "Thick Pucker II", was assembled by Nelson from outtakes of that shoot and more of the raw audio Reich had recorded. Nelson was not happy with the resulting film and never showed it.

Reich was influenced by fellow minimalist Terry Riley, whose work "In C" combines simple musical patterns, offset in time, to create a slowly shifting, cohesive whole. Reich adopted this approach to compose his first major work, "It's Gonna Rain". Composed in 1965, the piece used a fragment of a sermon about the end of the world given by a black Pentecostal street-preacher known as Brother Walter. Reich built on his early tape work, transferring the last three words of the fragment, "it's gonna rain!", to multiple tape loops which gradually move out of phase with one another.

The 13-minute "Come Out" (1966) uses similarly manipulated recordings of a single spoken line given by Daniel Hamm, one of the falsely accused Harlem Six, who was severely injured by police. The survivor, who had been beaten, punctured a bruise on his own body to convince police about his beating. The spoken line includes the phrase "to let the bruise’s blood come out to show them." Reich rerecorded the fragment "come out to show them" on two channels, which are initially played in unison. They quickly slip out of sync; gradually the discrepancy widens and becomes a reverberation. The two voices then split into four, looped continuously, then eight, and continues splitting until the actual words are unintelligible, leaving the listener with only the speech's rhythmic and tonal patterns. 

"Melodica" (1966) takes the phase looping idea of his previous works and applies it to instrumental music. Steve Reich took a simple melody, which he played on a melodica, then recorded it. He then sets the melody to two separate channels, and slowly moves them out of phase, creating an intricate interlocking melody. This piece is very similar to "Come Out" in rhythmic structure, and are an example of how one rhythmic process can be realized in different sounds to create two different pieces of music. Reich was inspired to compose this piece from a dream he had on May 22, 1966, and put the piece together in one day. "Melodica" was the last piece Reich composed solely for tape, and he considers it his transition from tape music to instrumental music.

Reich's first attempt at translating this phasing technique from recorded tape to live performance was the 1967 "Piano Phase", for two pianos. In "Piano Phase" the performers repeat a rapid twelve-note melodic figure, initially in unison. As one player keeps tempo with robotic precision, the other speeds up very slightly until the two parts line up again, but one sixteenth note apart. The second player then resumes the previous tempo. This cycle of speeding up and then locking in continues throughout the piece; the cycle comes full circle three times, the second and third cycles using shorter versions of the initial figure. "Violin Phase", also written in 1967, is built on these same lines. "Piano Phase" and "Violin Phase" both premiered in a series of concerts given in New York art galleries.

A similar, lesser known example of this so-called process music is "Pendulum Music" (1968), which consists of the sound of several microphones swinging over the loudspeakers to which they are attached, producing feedback as they do so. "Pendulum Music" has never been recorded by Reich himself, but was introduced to rock audiences by Sonic Youth in the late 1990s.

Reich also tried to create the phasing effect in a piece "that would need no instrument beyond the human body". He found that the idea of phasing was inappropriate for the simple ways he was experimenting to make sound. Instead, he composed "Clapping Music" (1972), in which the players do not phase in and out with each other, but instead one performer keeps one line of a 12-eighth-note-long (12-quaver-long) phrase and the other performer shifts by one eighth note beat every 12 bars, until both performers are back in unison 144 bars later.

The 1967 prototype piece "Slow Motion Sound" was not performed although Chris Hughes performed it 27 years later as "Slow Motion Blackbird" on his Reich-influenced 1994 album "Shift". It introduced the idea of slowing down a recorded sound until many times its original length without changing pitch or timbre, which Reich applied to "Four Organs" (1970), which deals specifically with augmentation. The piece has maracas playing a fast eighth note pulse, while the four organs stress certain eighth notes using an 11th chord. This work therefore dealt with repetition and subtle rhythmic change. It is unique in the context of Reich's other pieces in being linear as opposed to cyclic like his earlier works – the superficially similar "Phase Patterns", also for four organs but without maracas, is (as the name suggests) a phase piece similar to others composed during the period. "Four Organs" was performed as part of a Boston Symphony Orchestra program, and was Reich's first composition to be performed in a large traditional setting.

In 1970, Reich embarked on a five-week trip to study music in Ghana, during which he learned from the master drummer Gideon Alorwoyie. Reich also studied Balinese gamelan in Seattle in 1973 and 1974. From his African experience, as well as A. M. Jones's "Studies in African Music" about the music of the Ewe people, Reich drew inspiration for his 90-minute piece "Drumming", which he composed shortly after his return. Composed for a nine-piece percussion ensemble with female voices and piccolo, "Drumming" marked the beginning of a new stage in his career, for around this time he formed his ensemble, Steve Reich and Musicians, and increasingly concentrated on composition and performance with them. Steve Reich and Musicians, which was to be the sole ensemble to interpret his works for many years, still remains active with many of its original members.

After "Drumming", Reich moved on from the "phase shifting" technique that he had pioneered, and began writing more elaborate pieces. He investigated other musical processes such as augmentation (the temporal lengthening of phrases and melodic fragments). It was during this period that he wrote works such as "Music for Mallet Instruments, Voices and Organ" (1973) and "Six Pianos" (1973).

In 1974, Reich began writing "Music for 18 Musicians". This piece involved many new ideas, although it also hearkened back to earlier pieces. It is based on a cycle of eleven chords introduced at the beginning (called "Pulses"), followed by a small section of music based on each chord ("Sections I-XI"), and finally a return to the original cycle ("Pulses"). This was Reich's first attempt at writing for larger ensembles. The increased number of performers resulted in more scope for psychoacoustic effects, which fascinated Reich, and he noted that he would like to "explore this idea further". Reich remarked that this one work contained more harmonic movement in the first five minutes than any other work he had written. Steve Reich and Musicians made the premier recording of this work on ECM Records.

Reich explored these ideas further in his frequently recorded pieces "Music for a Large Ensemble" (1978) and "Octet" (1979). In these two works, Reich experimented with "the human breath as the measure of musical duration ... the chords played by the trumpets are written to take one comfortable breath to perform". Human voices are part of the musical palette in "Music for a Large Ensemble" but the wordless vocal parts simply form part of the texture (as they do in "Drumming"). With "Octet" and his first orchestral piece "Variations for Winds, Strings and Keyboards" (also 1979), Reich's music showed the influence of Biblical cantillation, which he had studied in Israel since the summer of 1977. After this, the human voice singing a text would play an increasingly important role in Reich's music.

In 1974 Reich published the book "Writings About Music", containing essays on his philosophy, aesthetics, and musical projects written between 1963 and 1974. An updated and much more extensive collection, "Writings On Music (1965–2000)", was published in 2002.

Reich's work took on a darker character in the 1980s with the introduction of historical themes as well as themes from his Jewish heritage. "Tehillim" (1981), Hebrew for "psalms", is the first of Reich's works to draw explicitly on his Jewish background. The work is in four parts, and is scored for an ensemble of four women's voices (one high soprano, two lyric sopranos and one alto), piccolo, flute, oboe, English horn, two clarinets, six percussion (playing small tuned tambourines without jingles, clapping, maracas, marimba, vibraphone and crotales), two electronic organs, two violins, viola, cello and double bass, with amplified voices, strings, and winds. A setting of texts from Psalms 19:2–5 (19:1–4 in Christian translations), 34:13–15 (34:12–14), 18:26–27 (18:25–26), and 150:4–6, "Tehillim" is a departure from Reich's other work in its formal structure; the setting of texts several lines long rather than the fragments used in previous works makes melody a substantive element. Use of formal counterpoint and functional harmony also contrasts with the loosely structured minimalist works written previously.
"Different Trains" (1988), for string quartet and tape, uses recorded speech, as in his earlier works, but this time as a melodic rather than a rhythmic element. In "Different Trains", Reich compares and contrasts his childhood memories of his train journeys between New York and California in 1939–1941 with the very different trains being used to transport contemporaneous European children to their deaths under Nazi rule. The Kronos Quartet recording of "Different Trains" was awarded the Grammy Award for Best Classical Contemporary Composition in 1990. The composition was described by Richard Taruskin as "the only adequate musical response—one of the few adequate artistic responses in any medium—to the Holocaust", and he credited the piece with earning Reich a place among the great composers of the 20th century.

In 1993, Reich collaborated with his wife, the video artist Beryl Korot, on an opera, "The Cave", which explores the roots of Judaism, Christianity and Islam through the words of Israelis, Palestinians, and Americans, echoed musically by the ensemble. The work, for percussion, voices, and strings, is a musical documentary, named for the Cave of Machpelah in Hebron, where a mosque now stands and Abraham is said to have been buried.

Reich and Korot collaborated on the opera "Three Tales", which concerns the "Hindenburg" disaster, the testing of nuclear weapons on Bikini Atoll, and other more modern concerns, specifically Dolly the sheep, cloning, and the technological singularity.

Reich used sampling techniques for pieces like "Three Tales" and "City Life" from 1994. Reich returned to composing purely instrumental works for the concert hall, starting with "Triple Quartet" in 1998 written for the Kronos Quartet that can either be performed by string quartet and tape, three string quartets or 36-piece string orchestra. According to Reich, the piece is influenced by Bartók's and Alfred Schnittke's string quartets, and Michael Gordon's "Yo Shakespeare".

The instrumental series for the concert hall continued with "Dance Patterns" (2002), "Cello Counterpoint" (2003), and sequence of works centered around Variations: "You Are (Variations)" (2004), a work which looks back to the vocal writing of works like "Tehillim" or "The Desert Music", "Variations for Vibes, Pianos, and Strings" in 2005, for the London Sinfonietta and "Daniel Variations" (2006).

in 2002 Reich was invited by Walter Fink to the annual Komponistenporträt of the Rheingau Musik Festival, as the 12th composer featured.

In an interview with "The Guardian", Reich stated that he continued to follow this direction with his piece "Double Sextet" (2007), which was commissioned by eighth blackbird, an American ensemble consisting of the instrumental quintet (flute, clarinet, violin or viola, cello and piano) of Schoenberg's piece "Pierrot Lunaire" (1912) plus percussion. Reich states that he was thinking about Stravinsky's "Agon" (1957) as a model for the instrumental writing.

December 2010 Nonesuch Records and Indaba Music held a community remix contest in which over 250 submissions were received, and Steve Reich and Christian Carey judged the finals. Reich spoke in a related BBC interview that once he composed a piece he would not alter it again himself; "When it's done, it's done," he said. On the other hand, he acknowledged that remixes have an old tradition e.g. famous religious music pieces where melodies were further developed into new songs.

Reich premiered a piece, "WTC 9/11", written for String Quartet and Tape (a similar instrumentation to that of "Different Trains") in March 2011. It was performed by the Kronos Quartet, at Duke University, North Carolina, US.

On March 5, 2013 the London Sinfonietta, conducted by Brad Lubman, at the Royal Festival Hall in London gave the world premiere of "Radio Rewrite" for ensemble with 11 players, inspired by the music of Radiohead. The programme also included "Double Sextet" for ensemble with 12 players, "Clapping Music", for two people and four hands featuring Reich himself alongside percussionist Colin Currie, "Electric Counterpoint", with electric guitar by Mats Bergstrom accompanied by a layered soundtrack, as well as two of Reich's small ensemble pieces, one for acoustic instruments, the other for electric instruments and tape.

"Music for Ensemble and Orchestra" was premiered on November 4, 2018 by the Los Angeles Philharmonic under Susanna Mälkki at Walt Disney Concert Hall, marking Reich's return to writing for orchestra after an interval of more than thirty years.

In 2005, Reich was awarded the Edward MacDowell Medal.

Reich was awarded with the Praemium Imperiale Award in Music in October 2006.

On January 25, 2007, Reich was named 2007 recipient of the Polar Music Prize with jazz saxophonist Sonny Rollins.

On April 20, 2009, Reich was awarded the 2009 Pulitzer Prize for Music, recognizing "Double Sextet", first performed in Richmond March 26, 2008. The citation called it "a major work that displays an ability to channel an initial burst of energy into a large-scale musical event, built with masterful control and consistently intriguing to the ear".

In May 2011 Steve Reich received an honorary doctorate from the New England Conservatory of Music.

In 2012, Steve Reich received the Gold Medal in Music by the American Academy of Arts and Letters.

In 2013 Reich received the US$400,000 BBVA Foundation Frontiers of Knowledge Award in contemporary music for bringing a new conception of music, based on the use of realist elements from the realm of daily life and others drawn from the traditional music of Africa and Asia.

In September 2014, Reich was awarded the "Leone d'Oro" (Golden Lion for Lifetime Achievement in Music) from the Venice Biennale.

In March 2016, Reich was awarded an Honorary Doctorate by the Royal College of Music in London.

The American composer and critic Kyle Gann has said that Reich "may ... be considered, by general acclamation, America's greatest living composer". Reich's style of composition has influenced many other composers and musical groups, including John Adams, the progressive rock band King Crimson, the new-age guitarist Michael Hedges, the art-pop and electronic musician Brian Eno, the experimental art/music group The Residents, the composers associated with the Bang on a Can festival (including David Lang, Michael Gordon, and Julia Wolfe), and numerous indie rock musicians including songwriter Sufjan Stevens and instrumental ensembles Tortoise, The Mercury Program (themselves influenced by Tortoise), and Godspeed You! Black Emperor (who titled an unreleased song "Steve Reich").

John Adams commented, "He didn't reinvent the wheel so much as he showed us a new way to ride." He has also influenced visual artists such as Bruce Nauman, and many notable choreographers have made dances to his music, Eliot Feld, Jiří Kylián, Douglas Lee and Jerome Robbins among others; he has expressed particular admiration of Anne Teresa De Keersmaeker's work set to his pieces.

In featuring a sample of Reich's "Electric Counterpoint" (1987) the British ambient techno act the Orb exposed a new generation of listeners to the composer's music with its 1990 production "Little Fluffy Clouds". In 1999 the album "Reich Remixed" featured "re-mixes" of a number of Reich's works by various electronic dance-music producers, such as DJ Spooky, Kurtis Mantronik, Ken Ishii, and Coldcut among others.

Reich's "Cello Counterpoint" (2003) was the inspiration for a series of commissions for solo cello with pre-recorded cellos made by Ashley Bathgate in 2017 including new works by Emily Cooley and Alex Weiser.

Reich often cites Pérotin, J. S. Bach, Debussy, Bartók, and Stravinsky as composers whom he admires and who greatly influenced him when he was young. Jazz is a major part of the formation of Reich's musical style, and two of the earliest influences on his work were vocalists Ella Fitzgerald and Alfred Deller, whose emphasis on the artistic capabilities of the voice alone with little vibrato or other alteration was an inspiration to his earliest works. John Coltrane's style, which Reich has described as "playing a lot of notes to very few harmonies", also had an impact; of particular interest was the album "Africa/Brass", which "was basically a half-an-hour in F." Reich's influence from jazz includes its roots, also, from the West African music he studied in his readings and visit to Ghana. Other important influences are Kenny Clarke and Miles Davis, and visual artist friends such as Sol LeWitt and Richard Serra. Reich has also stated that he admires the music of the band Radiohead, which led to his composition "Radio Rewrite".












</doc>
<doc id="26823" url="https://en.wikipedia.org/wiki?curid=26823" title="Simon &amp; Garfunkel">
Simon &amp; Garfunkel

Simon & Garfunkel were an American folk-rock duo consisting of singer-songwriter Paul Simon and singer Art Garfunkel. One of the best-selling music groups of the 1960s, their biggest hits—including "The Sound of Silence" (1965), "Mrs. Robinson" (1968), "The Boxer" (1969), and "Bridge over Troubled Water" (1970)—reached number one on singles charts worldwide.

Simon and Garfunkel met in elementary school in Queens, New York, in 1953, where they learned to harmonize together and began writing material. By 1957, under the name Tom & Jerry, the teenagers had their first minor success with "Hey Schoolgirl", a song imitating their idols the Everly Brothers. In 1963, aware of a growing public interest in folk music, they regrouped and were signed to Columbia Records as Simon & Garfunkel. Their debut, "Wednesday Morning, 3 A.M.," sold poorly, and they once again disbanded; Simon returned to a solo career, this time in England. In June 1965, a new version of "The Sound of Silence" overdubbed with electric guitar and drums became a major U.S. AM radio hit, reaching number one on the "Billboard" Hot 100. The duo reunited to release a second studio album, "Sounds of Silence," and tour colleges nationwide. On their third release, "Parsley, Sage, Rosemary and Thyme" (1966), the duo assumed more creative control. Their music was featured in the 1967 film "The Graduate", giving them further exposure. Their next album "Bookends" (1968) topped the "Billboard" 200 chart and included the number-one single "Mrs. Robinson" from the film.

The duo's often rocky relationship led to artistic disagreements and their breakup in 1970. Their final studio album, "Bridge over Troubled Water", was released that year and became their most successful, becoming one of the world's best-selling albums. After their breakup, Simon released a number of acclaimed albums, including 1986's "Graceland". Garfunkel released solo hits such as "All I Know" and briefly pursued an acting career, with leading roles in two Mike Nichols films, "Catch-22" and "Carnal Knowledge", and in Nicolas Roeg's 1980 "Bad Timing." The duo have reunited several times, most famously in 1981 for "The Concert in Central Park", which attracted more than 500,000 people, one of the largest concert attendances in history.

Simon & Garfunkel won 10 Grammy Awards and were inducted into the Rock and Roll Hall of Fame in 1990. "Bridge over Troubled Water" is ranked at number 51 on Rolling Stone's 500 Greatest Albums of All Time. Richie Unterberger described them as "the most successful folk-rock duo of the 1960s" and one of the most popular artists from the decade. They are among the best-selling music artists, having sold more than 100 million records.

Paul Simon and Art Garfunkel grew up in the 1940s and 1950s in their predominantly Jewish neighborhood of Kew Gardens Hills in Queens, New York, three blocks away from one another. They attended the same schools: Public School 164 in Kew Gardens Hills, Parsons Junior High School, and Forest Hills High School. They were both fascinated by music; both listened to the radio and were taken with rock and roll as it emerged, particularly the Everly Brothers. Simon first noticed Garfunkel when Garfunkel was singing in a fourth grade talent show, which Simon thought was a good way to attract girls; he hoped for a friendship, which started in 1953, when they appeared in a sixth grade adaptation of "Alice in Wonderland". They formed a streetcorner doo-wop group called the Peptones with three friends and learned to harmonize. They began performing as a duo at school dances.

Simon and Garfunkel moved to Forest Hills High School, where in 1956 they wrote their first song, "The Girl for Me"; Simon's father sent a handwritten copy to the Library of Congress to register a copyright. While trying to remember the lyrics to the Everly Brothers song "Hey Doll Baby", they wrote "Hey Schoolgirl", which they recorded for $25 at Sanders Recording Studio in Manhattan. While recording they were overheard by promoter Sid Prosen, who signed them to his independent label Big Records after speaking to their parents. They were 15.

Under Big Records, Simon and Garfunkel assumed the name Tom & Jerry; Garfunkel named himself Tom Graph, a reference to his interest in mathematics, and Simon Jerry Landis, after the surname of a girl he had dated. Their first single, "Hey Schoolgirl", was released with the B-side "Dancin' Wild" in 1957. Prosen, using the payola system, bribed DJ Alan Freed $200 to play the single on his radio show, where it became a nightly staple. "Hey Schoolgirl" attracted regular rotation on nationwide AM pop stations, leading it to sell over 100,000 copies and to land on "Billboard" charts at number 49. Prosen promoted the group heavily, getting them a headlining spot on Dick Clark's "American Bandstand" alongside Jerry Lee Lewis. Simon and Garfunkel shared approximately $4,000 from the song – earning two percent each from royalties, the rest staying with Prosen. They released three more singles on Big Records: "Our Song", "That's My Story", and "Don't Say Goodbye", none of them successful.

After graduating from Forest Hills High School in 1958, the pair continued their education should a music career not unfold. Simon studied English at Queens College, City University of New York, and Garfunkel studied architecture before switching to art history at Columbia College, Columbia University. While still with Big Records as a duo, Simon released a solo single, "True or False", under the name "True Taylor". This upset Garfunkel, who regarded it as a betrayal; the emotional tension from the incident occasionally surfaced throughout their relationship.

Simon and Garfunkel continued recording as solo artists: Garfunkel composed and recorded "Private World" for Octavia Records, and—under the name Artie Garr—"Beat Love" for Warwick; Simon recorded with the Mystics and Tico & The Triumphs, and wrote and recorded under the names Jerry Landis and Paul Kane. Simon also wrote and performed demos for other artists, working for a while with Carole King and Gerry Goffin.

After graduating in 1963, Simon joined Garfunkel, who was still at Columbia, to perform again as a duo, this time with a shared interest in folk music. Simon enrolled part-time in Brooklyn Law School. By late 1963, billing themselves as Kane & Garr, they performed at Gerde's Folk City, a Greenwich club that hosted Monday night open mic performances. They performed three new songs—"Sparrow", "He Was My Brother", and "The Sound of Silence"—and attracted the attention of Columbia producer Tom Wilson, an African-American jazz musician who would also become the key architect of Bob Dylan's transition from folk to rock. As a "star producer" for the label, he wanted to record "He Was My Brother" with a new British act, the Pilgrims. Simon convinced Wilson to let him and Garfunkel audition in the studio, where they performed "The Sound of Silence". At Wilson's urging, Columbia signed them.

Simon & Garfunkel's debut studio album, "Wednesday Morning, 3 A.M.", was recorded over three sessions in March 1964 and released in October. It contains five compositions by Simon, three traditional folk songs, and four folk-influenced singer-songwriter songs. Simon was adamant that they would no longer use stage names. Columbia set up a promotional showcase at Folk City on March 31, 1964, the duo's first public concert as Simon & Garfunkel. The showcase, as well as other scheduled performances, did not go well.

"Wednesday Morning, 3 A.M." sold only 3,000 copies on release. Its poor sales caused Simon to move to England, where he had previously visited and played some gigs. He toured small folk clubs, befriending British folk artists such as Bert Jansch, Martin Carthy, Al Stewart, and Sandy Denny. He met Kathy Chitty, who became the object of his affection and is the Kathy in "Kathy's Song" and "America".

A small music publishing company, Lorna Music, licensed "Carlos Dominguez", a single Simon had recorded two years prior as Paul Kane, for a cover by Val Doonican that sold well. Simon visited Lorna to thank them, and the meeting resulted in a publishing and recording contract. He signed to the Oriole label and released "He Was My Brother" as a single. Simon invited Garfunkel to stay for the summer of 1964.

Near the end of the season, Garfunkel returned to Columbia for class, and Simon surprised his friends by saying that he would return to the States as well. He resumed his studies at Brooklyn Law School for one semester, partially at his parents' insistence. He returned to England in January 1965, now certain that music was his calling. In the meantime, his landlady, Judith Piepe, had compiled a tape from his work at Lorna and sent it to the BBC in hopes they would play it. The demos aired on the "Five to Ten" morning show, and were instantly successful. Oriole had folded into CBS by that point, and hoped to record a new Simon album. Simon recorded his first solo album, "The Paul Simon Songbook," in June 1965 and featured future Simon & Garfunkel staples including "I Am a Rock" and "April Come She Will". CBS flew Wilson over to produce the record, and he stayed at Simon's flat. The album was released in August; although sales were poor, Simon felt content with his future in England. Garfunkel graduated in 1965, returning to Columbia University to do a master's degree in mathematics.

Meanwhile, in the United States, Dick Summer, a late-night DJ at WBZ in Boston played "The Sound of Silence", where it was popular with a college audience. It was picked up the next day along the East Coast of the United States, down to Cocoa Beach, Florida. When Wilson heard about this new wave of interest, he took inspiration from the success of the folk-rock hybrid that he and Dylan had created with "Like a Rolling Stone", and crafted a rock remix of the song using studio musicians. The remix was issued in September 1965, where it reached the "Billboard" Hot 100. Wilson did not inform the duo of his plan, and Simon was "horrified" when he first heard it.

By January 1966, "The Sound of Silence" had topped the Hot 100, selling over one million copies. Simon reunited with Garfunkel in New York, leaving Chitty and his friends in England behind. CBS demanded a new album, to be called "Sounds of Silence", to ride the wave of the hit. Recorded in three weeks, and consisting of rerecorded songs from "The Paul Simon Songbook" plus four new tracks, "Sounds of Silence" was rush-released in mid-January 1966, peaking at number 21 "Billboard" Top LPs chart. A week later, "Homeward Bound" was released as a single, entering the USA top ten, followed by "I Am a Rock" peaking at number three. The duo supported the recordings with a nationwide tour of America, while CBS continued their promotion by re-releasing "Wednesday Morning, 3 A.M.", which charted at number 30. Despite the success, the duo received critical derision, as many considered them a manufactured imitation of folk music.

As they considered "The Sounds of Silence" a "rush job" to capitalize on their sudden success, Simon & Garfunkel spent more time crafting the follow-up. It was the first time Simon insisted on total control in aspects of recording. Work began in 1966 and took nine months. Garfunkel considered the recording of "Scarborough Fair" to be the point at which they stepped into the role of producer, as they were constantly beside engineer Roy Halee mixing. "Parsley, Sage, Rosemary and Thyme" was issued in October 1966, following the release of several singles and sold-out college campus shows. The duo resumed their college circuit tour eleven days later, crafting an image that was described as "alienated", "weird", and "poetic". Manager Mort Lewis also was responsible for this public perception, as he withheld them from television appearances unless they were allowed to play an uninterrupted set or choose the setlist. Simon, then 26, felt he had "made it" into an upper echelon of rock and roll while retaining artistic integrity; according to his biographer Marc Eliot, this made him "spiritually closer to Bob Dylan than to, say, Bobby Darin". The duo chose William Morris as their booking agency after a recommendation from Wally Amos, a mutual friend of Wilson.

During the sessions for "Parsley", Simon and Garfunkel recorded "A Hazy Shade of Winter"; it was released as a single, peaking at number 13 on the national charts. Similarly, they recorded "At the Zoo" for single release in early 1967; it charted at number 16. Simon began work for their next album around this time, telling "High Fidelity" that "I'm not interested in singles anymore". He developed writer's block, which led to no new album on the horizon for 1967. Artists at the time were expected to release two or three albums each year, and the lack of productivity worried Columbia executives. Amid concerns for Simon's idleness, Columbia Records chairman Clive Davis arranged for up-and-coming producer John Simon to kick-start the recording. Simon was distrustful of label executives; on one occasion, he and Garfunkel recorded a meeting with Davis, who was giving a "fatherly talk" on speeding up production, to laugh at it later. The rare television appearances at this time saw the duo performing on network broadcasts as "The Ed Sullivan Show", "The Mike Douglas Show", and "The Andy Williams Show" in 1966, and twice on "The Smothers Brothers Comedy Hour" in 1967.

Meanwhile, director Mike Nichols, then filming "The Graduate", had become fascinated with Simon & Garfunkel's records, listening to them extensively before and after filming. He met Davis to ask for permission to license Simon & Garfunkel music for his film. Davis viewed it as a perfect fit and envisioned a bestselling soundtrack album. Simon was not as receptive and was cautious of "selling out". However, after meeting Nichols and being impressed by his wit and the script, he agreed to write new songs for the film. Leonard Hirshan, a powerful agent at William Morris, negotiated a deal that paid Simon $25,000 to submit three songs to Nichols and producer Lawrence Turman. When Nichols was not impressed by Simon's songs "Punky's Dilemma" and "Overs", Simon and Garfunkel offered another, incomplete song, which became "Mrs. Robinson"; Nichols loved it.

Simon & Garfunkel's fourth studio album, "Bookends", was recorded in fits and starts from late 1966 to early 1968. Although the album had long been planned, work did not begin in earnest until late 1967. The duo were signed under an older contract that specified the label pay for sessions, and Simon & Garfunkel took advantage of this, hiring viola and brass players and percussionists. The record's brevity reflects its concise and perfectionist production; the team spent over 50 hours recording "Punky's Dilemma", for example, and rerecorded vocal parts, sometimes note by note, until they were satisfied. Garfunkel's songs and voice took a lead role on some of the songs, and the harmonies for which the duo was known gradually disappeared. For Simon, "Bookends" represented the end of the collaboration and became an early indicator of his intentions to go solo.

Prior to release, the band helped put together and performed at the Monterey Pop Festival, which signaled the beginning of the Summer of Love on the West Coast. "Fakin' It" was issued as a single that summer and found only modest success on AM radio; the duo were much more focused on the rising FM format, which played album tracks and treated their music with respect. In January 1968, the duo appeared on a Kraft Music Hall special, "Three for Tonight", performing ten songs, largely taken from their previous album. "Bookends" was released by Columbia Records in April 1968, 24 hours before the assassination of civil rights movement activist Martin Luther King Jr., which spurred nationwide outrage and riots. The album debuted on the "Billboard" Top LPs in the issue dated April 27, 1968, climbing to number one and staying at that position for seven non-consecutive weeks; it remained on the chart as a whole for 66 weeks. "Bookends" received such heavy orders weeks in advance of its release that Columbia was able to apply for award certification before copies left the warehouse, a fact it touted in magazine ads. The album became the duo's bestselling to date, helped by the attention for the "Graduate" soundtrack ten weeks earlier, creating an initial combined sales figure of over five million units.

Davis had predicted this, and suggested raising the list price of "Bookends" by one dollar to $5.79, above the then standard retail price, to compensate for a large poster included in vinyl copies. Simon scoffed and viewed it as charging a premium on "what was sure to be that year's best-selling Columbia album". According to biographer Marc Eliot, Davis was "offended by what he perceived as their lack of gratitude for what he believed was his role in turning them into superstars". Rather than implement Davis' plan, Simon & Garfunkel signed a contract extension with Columbia that guaranteed them a higher royalty rate. At the 1969 Grammy Awards, the lead single "Mrs. Robinson" became the first rock and roll song to receive Record of the Year, and also won Best Contemporary Pop Performance by a Duo or Group.

"Bookends", alongside the "Graduate" soundtrack, made Simon & Garfunkel the biggest rock duo in the world. Simon was approached by producers to write music for films or license songs; he turned down Franco Zeffirelli, who was preparing to film "Brother Sun, Sister Moon", and John Schlesinger, who was preparing to film "Midnight Cowboy". In addition to Hollywood proposals, Simon declined a request by producers from the Broadway show "Jimmy Shine" (starring Simon's friend Dustin Hoffman, also the lead in "Midnight Cowboy"). He collaborated briefly with Leonard Bernstein on a sacred mass before withdrawing from the project due to "finding it perhaps too far afield from his comfort zone".

Garfunkel began acting, and played Captain Nately in the Nichols film "Catch-22" based on the novel of the same name. Simon was to play the character of Dunbar, but screenwriter Buck Henry felt the film was already crowded with characters and wrote Simon's part out. Filming began in January 1969 and lasted about eight months, longer than expected. The production endangered the duo's relationship; Simon had completed no new songs, and the duo planned to collaborate after filming ended. Following the end of filming in October, the first performance of what was planned to be their last tour took place in Ames, Iowa. The US leg of the tour ended in the sold-out Carnegie Hall on November 27. Meanwhile, the duo, working with director Charles Grodin, produced an hourlong CBS special, "Songs of America", a mixture of scenes featuring notable political events and leaders concerning the US, such as the Vietnam War, Martin Luther King, John F. Kennedy's funeral procession, Cesar Chavez and the Poor People's March. It was broadcast only once, due to tension at the network regarding its content.

"Bridge over Troubled Water", Simon & Garfunkel's final studio album, was released in January 1970 and charted in over 11 countries, topping the charts in 10, including the "Billboard" Top LP's chart in the US and the UK Albums Chart. It was the best-selling album in 1970, 1971 and 1972 and was at that time the best-selling album of all time. It was also CBS Records' best-selling album before the release of Michael Jackson's "Thriller" in 1982. The album topped the "Billboard" charts for 10 weeks and stayed in the charts for 85 weeks. In the United Kingdom, the album topped the charts for 35 weeks, and spent 285 weeks in the top 100, from 1970 to 1975. It has since sold over 25 million copies worldwide. "Bridge over Troubled Water", the lead single, reached number one in five countries and became the duo's biggest seller. The song has been covered by over 50 artists, including Elvis Presley, Johnny Cash, Aretha Franklin, Willie Nelson, Roy Orbison, and Josh Groban. "Cecilia", the follow-up, reached number four in the US, and "El Condor Pasa" hit number 18. A brief British tour followed the album release, and the duo's last concert as Simon & Garfunkel took place at Forest Hills Stadium. In 1971, the album won six awards at the 13th Annual Grammy Awards, including Album of the Year.

The recording of "Bridge over Troubled Water" was difficult and Simon and Garfunkel's relationship had deteriorated. "At that point, I just wanted out," Simon later said. Garfunkel hoped for a two-year break and did not intend to pursue a film career; likewise, Simon did not intend to begin a solo career. At the urging of Simon's wife, Peggy Harper, he called Davis to confirm the duo's breakup. For the next several years, the duo would only speak "two or three" times a year.

In the 1970s, the duo reunited several times. Their first reunion was a benefit concert for presidential candidate George McGovern at New York's Madison Square Garden in June 1972. In 1975, they reconciled when they visited a recording session with John Lennon and Harry Nilsson. For the rest of the year, they attempted to make the reunion work, but their collaboration only yielded one song, "My Little Town", that was featured on Simon's "Still Crazy After All These Years" and Garfunkel's "Breakaway". It peaked at number nine on the Hot 100. In 1975, Garfunkel joined Simon for a medley of three songs on "Saturday Night Live," guest-hosted by Simon. In 1977, Garfunkel joined Simon for a brief performance of their old songs on "The Paul Simon Special", and later that year they recorded a cover of Sam Cooke's "(What a) Wonderful World" with James Taylor. Old tensions appeared to dissipate upon Garfunkel's return to New York in 1978, when the duo began interacting more often. On May 1, 1978, Simon joined Garfunkel for a concert held at Carnegie Hall to benefit the hearing disabled.
By 1980, the duo's respective solo careers were not doing well. To help alleviate New York's economic decline, concert promoter Ron Delsener suggested a free concert in Central Park. Delsener contacted Simon with the idea of a Simon & Garfunkel reunion, and once Garfunkel had agreed, plans were made. The concert, held on September 19, 1981 attracted more than 500,000 people, at that time the largest ever concert attendance. Warner Bros. Records released a live album of the show, "The Concert in Central Park", which went double platinum in the US. A 90-minute recording of the concert was sold to Home Box Office (HBO) for over $1 million. The concert created a renewed interest in Simon & Garfunkel's work. They had several "heart-to-heart talks", attempting to put their disagreements behind them. The duo planned a world tour to begin in May 1982, but their relationship grew contentious: for the majority of the tour, they did not speak to one another.

Warner Bros. pushed for the duo to extend the tour and release an all new Simon & Garfunkel studio album. After recording several vocal tracks for a possible new studio album, Simon decided to make it his own solo album. Garfunkel refused to learn the songs in the studio and would not give up his longstanding cannabis and cigarette habits despite Simon's requests. A spokesperson said: "Paul simply felt the material he wrote is so close to his own life that it had to be his own record. Art was hoping to be on the album, but I'm sure there will be other projects that they will work on together. They are still friends." The material was released on Simon's 1983 album "Hearts and Bones". Another rift opened between the duo when the lengthy recording of Simon's 1986 album "Graceland" prevented Garfunkel from working with "Graceland" engineer Roy Halee on a Christmas album.

In 1990, Simon and Garfunkel were inducted into the Rock and Roll Hall of Fame. Garfunkel thanked his partner, calling him "the person who most enriched my life by putting those songs through me," to which Simon responded, "Arthur and I agree about almost nothing. But it's true, I have enriched his life quite a bit." After performing three songs, the duo left without speaking. In August 1991, Simon staged his own concert in Central Park, released as a live album, "Paul Simon's Concert in the Park," a few months later. He declined an offer from Garfunkel to perform with him at the park.
By 1993, their relationship had thawed, and Simon invited Garfunkel on an international tour. Following a 21-date sold-out run at the Paramount Theater in New York and an appearance at that year's Bridge School Benefit in California, they toured the Far East. They became acrimonious again for the rest of the decade. Simon thanked Garfunkel at his 2001 induction into the Rock and Roll Hall of Fame as a solo artist: "I regret the ending of our friendship. I hope that some day before we die we will make peace with each other," adding after a pause, "No rush."

In 2003, Simon and Garfunkel received a Lifetime Achievement Award at the 45th Annual Grammy Awards, for which the promoters convinced them to open with a performance of "The Sound of Silence". The performance was satisfying for both, and they planned a full-scale reunion tour. The Old Friends tour began in October 2003 and played to sold-out audiences across the United States for 40 dates until mid-December, earning an estimated $123 million. A second U.S. leg commenced in June, 2004, consisting of twenty cities. Following a twelve-city run in Europe in 2004, they ended their nine-month tour with a free concert along Via dei Fori Imperiali, in front of the Colosseum in Rome, on 31 July 2004. It attracted 600,000 fans, more than their Concert in Central Park. In 2005, Simon and Garfunkel performed three songs for a Hurricane Katrina benefit concert in Madison Square Garden, including a performance with singer Aaron Neville.
In February 2009, Simon and Garfunkel reunited for three songs during Simon's two-night engagement at New York's Beacon Theatre. This led to a reunion tour of Asia and Australia in June and July, 2009. On October 29, 2009, they performed five songs at the 25th Anniversary Rock and Roll Hall of Fame Concert at Madison Square Garden. Their headlining set at the 2010 New Orleans Jazz and Heritage Festival was difficult for Garfunkel, who had vocal problems. "I was terrible, and crazy nervous. I leaned on Paul Simon and the affection of the crowd," he told "Rolling Stone" several years later. Garfunkel was diagnosed with vocal cord paresis, and the remaining tour dates were postponed indefinitely. His manager, John Scher, informed Simon's camp that Garfunkel would be ready within a year, which did not happen, damaging relations between the two. Simon continued to publicly wish Garfunkel better health and praised his "angelic" voice. Garfunkel regained his vocal strength over the course of the next four years, performing shows in a Harlem theater and to underground audiences.

In 2014, Garfunkel told "Rolling Stone" that he believed he and Simon would tour again in future, but said: "I know that audiences all over the world like Simon and Garfunkel. I'm with them. But I don't think Paul Simon's with them." Asked about a reunion in 2016, Simon stated: "Quite honestly, we don't get along. So it's not like it's fun. If it was fun, I'd say, OK, sometimes we'll go out and sing old songs in harmony. That's cool. But when it's not fun, you know, and you're going to be in a tense situation, well, then I have a lot of musical areas that I like to play in. So that'll never happen again. That's that." In February 2018, Simon announced his retirement from touring.

Over the course of their career, Simon & Garfunkel's music gradually moved from a basic folk rock sound to incorporate more experimental elements for the time, including Latin and gospel music. Their music, according to "Rolling Stone", struck a chord among lonely, alienated young adults near the end of the decade.

Simon & Garfunkel received criticism at the height of their success. In 1968, "Rolling Stone" critic Arthur Schmidt described their music as "questionable ... it exudes a sense of process, and it is slick, and nothing too much happens." "New York Times" critic Robert Shelton said that the duo had "a kind of Mickey Mouse, timid, contrived" approach. According to Richie Unterberger of AllMusic, their clean sound and muted lyricism "cost them some hipness points during the psychedelic era ... the pair inhabited the more polished end of the folk-rock spectrum and was sometimes criticized for a certain collegiate sterility." He noted that some critics regard Simon's later solo work as superior to Simon & Garfunkel.

According to "Pitchfork", though Simon & Garfunkel were highly regarded folk act "distinguished by their intuitive harmonies and Paul Simon's articulate songwriting", they were more conservative than the folk music revivalists of Greenwich Village. By the late 1960s, they had become the "folk establishment ... primarily unthreatening and accessible, which forty years later makes them an ideal gateway act to the weirder, harsher, more complex folkies of the 60s counterculture". However, their later albums explored more ambitious production techniques and incorporated elements of gospel, rock, R&B, and classical, revealing a "voracious musical vocabulary".

The Grammy Awards are held annually by the National Academy of Recording Arts and Sciences. Simon & Garfunkel have won 9 total competitive awards, 4 Hall of Fame awards, and a Lifetime Achievement Award.







</doc>
<doc id="26824" url="https://en.wikipedia.org/wiki?curid=26824" title="State Street Corporation">
State Street Corporation

State Street Corporation is an American financial services and bank holding company headquartered at One Lincoln Street in Boston with operations worldwide. It is the second United States bank on the list of oldest banks in continuous operation; its predecessor, Union Bank, was founded in 1792. State Street is ranked 15th on the list of largest banks in the United States by assets. It is one of the largest asset management companies in the world with US$2.511 trillion under management and US$31.62 trillion under custody and administration. It is the second largest custodian bank in the world.

The company is ranked 247th on the Fortune 500 as of 2019. The company is on the list of the banks that are too big to fail published by the Financial Stability Board.

The company is named after State Street in Boston, which was known as the "Great Street to the Sea" in the 18th century as Boston became a flourishing maritime capital. The company's logo includes a clipper to reflect the maritime industry in Boston during this time.

State Street Bank and Trust Company, also known as Global Services, is the investment servicing division of State Street. It provides asset owners and managers with custodian bank services (safekeeping, corporate actions), fund accounting (pricing and valuation), and administration (financial reporting, tax, compliance, and legal) services. Global Services handles assets from many classes, including stocks, derivatives, exchange-traded funds, fixed income assets, private equity, and real estate. State Street administers 40% of the assets under administration in the US mutual fund market. Global Services also provides outsourcing for operations activities and handles US$10.2 trillion of middle-office assets.

State Street Global Advisors dates back to 1978. It provides asset management, investment management, research, and advisory services to corporations, mutual funds, insurance companies, and other institutional investors. Global Advisors develops both passive management and active management strategies using both quantitative and fundamental approaches.

In 1993, the company created the SPDR S&P 500 Trust ETF, the first exchange-traded fund (ETF), and is now one of the largest ETF providers worldwide.

Trading on SPDR began January 29, 1993.

Global Markets is State Street's securities business. It offers research, trading, and securities lending services for foreign exchange, stocks, fixed income, and derivatives. To avoid a conflict of interest, the company does not run proprietary trading books. Global Markets maintains trading desks in Boston, London, Sydney, Toronto, and Tokyo.

The company traces its roots to Union Bank, which received a charter in 1792 from Massachusetts Governor John Hancock. It was the third bank to be chartered in Boston and its office was at the corner of State and Exchange Streets. In 1865, Union Bank received a national charter and became the National Union Bank of Boston. The bank later built a headquarters at Washington and State streets.

State Street Deposit & Trust Co opened in 1891. It became the custodian of the first U.S. mutual fund in 1924, the Massachusetts Investors Trust (now MFS Investment Management).

State Street and National Union merged in 1925. The merged bank took the State Street name, but National Union was the nominal survivor, and it operated under National Union's charter, thus giving the current entity its rank among the oldest banks in the United States.

The company merged with Second National Bank in 1955 and with the Rockland-Atlas National Bank in 1961.

In 1966, the company completed construction of the State Street Bank Building, a new headquarters building, the first high-rise office tower in downtown Boston.

In 1972, the company opened its first international office in Munich.

In 1973, as a 50/50 joint venture with DST Systems, the company formed Boston Financial Data Services, a provider of shareholder record-keeping, intermediary and investor services, and regulatory compliance. More than 100 top staff from IBM were hired by State Street as it set about implementing IBM mainframe computer systems.

In 1975, William Edgerly became president and chief executive officer of the bank and shifted the company's strategy from commercial banking to investments and securities processing.

During the 1980s and 1990s, the company opened offices in Montreal, Toronto, Dublin, London, Paris, Dubai, Sydney, Wellington, Hong Kong, and Tokyo.

By 1992, most of State Street's revenue came from fees for holding securities, settling trades, keeping records, and performing accounting. In 1994, the company formed State Street Global Advisors, a global asset management business.

In 1995, State Street acquired Investors Fiduciary Trust of Kansas City for $162 million from DST Systems and Kemper Financial Services. In 1996, Bank of New York acquired the unit investment trust servicing business of Investors Fiduciary Trust Co., Kansas City, Mo.

In 1999, State Street sold its retail and commercial banking businesses to Citizens Financial Group.

In 1990, State Street Bank Luxembourg was founded, and is the largest player in the country's fund industry by assets.

In 2003, the company acquired the securities services division of Deutsche Bank for $1.5 billion. The company also sold its corporate trust business to U.S. Bancorp for $725 million. Also in 2003, State Street sold its private asset management business to U.S. Trust. 

In July 2007, the company acquired Investors Bank & Trust for $4.5 billion.

In October 2008, the United States Department of the Treasury invested $2 billion in the company as part of the Troubled Asset Relief Program and in July 2009, the company became the first major financial firm to repay the Treasury.

In 2010, the company acquired Mourant International Finance Administration. It also acquired the securities services group of Intesa Sanpaolo for $1.87 billion. In December 2010, the company announced that it would be retrenching 5% of its workforce and effectively reducing the hourly wages of remaining employees by 10% via increased standard work hours.

In November 2011, the company was named as amongst the world's 29 systemic banks.

In 2012, the company acquired Goldman Sachs Administration Services, a hedge fund administrator, for $550 million.

In November 2014, the company sold SSARIS Advisors, its hedge fund unit, to senior management.

In 2016, State Street launched a program called Beacon, focused on cutting costs and improving reporting technology. Their main focus was to shrink their US workforce in order to bolster profits in excess of $2.5 billion (2018 figures). Also in 2016, the company acquired the asset management business of General Electric. 

In 2017, the company announced that Jay Hooley, the chief executive officer of the company, would retire.

In 2018, State Street completed its acquisition of Charles River Development, a Burlington, Massachusetts provider of investment management software. The deal closed October 1, 2018, at a cost of approximately $2.6 billion that will be financed by the suspension of share repurchases and the issuing of common and preferred equity. News of the acquisition led to a drop in State Street shares of nearly 10% with share prices remaining flat since the purchase.

In January 2019, State Street announced that it planned to lay off 1,500 employees, increasing the number to 2,300 in July. The company is shifting their workforce from the United States to countries like China, India and Poland and is operating under a hiring freeze. Offsetting this is increased overseas hiring, resulting in a 3K+ net gain of employment.

In 2009, California alleged on behalf of its pension funds CalPERS and CalSTRS that State Street had committed fraud on currency trades handled by the custodian bank. In October 2011, two executives from State Street Global Markets left the company following charges over the pricing of a fixed income transaction. In April 2016, they were charged by the United States Department of Justice.

On February 28, 2012, State Street Global Advisors entered into a consent order with the Massachusetts Securities Division. The Division was investigating the firm's role as the investment manager of a $1.65 billion (USD) hybrid collateralized debt obligation. The investigation resulted in a fine of $5 million (USD) for the non-disclosure of certain initial investors taking a short position on portions of the CDO.

During the May 2012 annual shareholders meeting, chairman and chief executive Jay Hooley was shouted down on numerous occasions by protesters in relation to the outsourcing and other grievances.

On January 18, 2017, State Street agreed to pay $64.6 million to resolve U.S. investigations into what prosecutors said was a scheme to defraud six clients through secret commissions on billions of dollars of trades.

In 2018 former employee of State Street, Edward Pennings was sentenced to six months in prison for his role in the scheme.  Employee Ross McLellan was also sentenced in the United States to 18 months in prison.

In March 2017, State Street Global Advisors commissioned a statue called "Fearless Girl" by Kristen Visbal and placed it temporarily in the Financial District, Manhattan, in front of the Wall Street icon "Charging Bull". The statue is an advertisement for an index fund which comprises gender diverse companies that have a higher percentage of women among their senior leadership. While some have seen it as an encouragement of women in business, some women criticized the statue as "corporate feminism" that violated their own feminist principles. In October 2017, the company paid $5 million to settle a lawsuit charging that it had paid certain female and African-American executives less than their male and European-American peers.




</doc>
<doc id="26825" url="https://en.wikipedia.org/wiki?curid=26825" title="Spanish language">
Spanish language

Spanish (; ), or Castilian (, ), is a Romance language that originated in the Iberian Peninsula and today has over 483 million native speakers, mainly in Spain and the Americas. It is a global language, the world's second-most spoken native language, after Mandarin Chinese, and the world's fourth-most spoken language, after Mandarin Chinese, English and Hindi.

Spanish is a part of the Ibero-Romance group of languages, which evolved from several dialects of Vulgar Latin in Iberia after the collapse of the Western Roman Empire in the 5th century. The oldest Latin texts with traces of Spanish come from mid-northern Iberia in the 9th century, and the first systematic written use of the language happened in Toledo, a prominent city of the Kingdom of Castile, in the 13th century. Beginning in 1492, the Spanish language was taken to the viceroyalties of the Spanish Empire, most notably to the Americas, as well as territories in Africa, Oceania and the Philippines.

A 1949 study by Italian-American linguist Mario Pei, analyzing the degree of difference from a language's parent (Latin, in the case of Romance languages) by comparing phonology, inflection, syntax, vocabulary, and intonation, indicated the following percentages (the higher the percentage, the greater the distance from Latin): In the case of Spanish, it is one of the closest Romance languages to Latin (20% distance), only behind Sardinian (8% distance) and Italian (12% distance). Around 75% of modern Spanish vocabulary is derived from Latin, including Latin borrowings from Ancient Greek.
Spanish vocabulary has been in contact with Arabic from an early date, having developed during the Al-Andalus era in the Iberian Peninsula. With around 8% of its vocabulary being Arabic in origin, this language makes up the second greatest vocabulary source after Latin itself. It has also been influenced by Basque, Iberian, Celtiberian, Visigothic, and by neighboring Ibero-Romance languages. Additionally, it has absorbed vocabulary from other languages, particularly other Romance languages—French, Italian, Andalusi Romance, Portuguese, Galician, Catalan, Occitan, and Sardinian—as well as from Quechua, Nahuatl, and other indigenous languages of the Americas.

Spanish is one of the six official languages of the United Nations. It is also used as an official language by the European Union, the Organization of American States, the Union of South American Nations, the Community of Latin American and Caribbean States, the African Union and many other international organizations.

Despite its large number of speakers, the Spanish language does not feature prominently in scientific writing, though it is better represented in the humanities. 75% of scientific production in Spanish is divided into three thematic areas: social sciences, medical sciences and arts/humanities. Spanish is the third most used language on the internet after English and Chinese. 
It is estimated that there are more than 437 million people who speak Spanish as a native language, which qualifies it as second on the lists of languages by number of native speakers. Instituto Cervantes claims that there are an estimated 477 million Spanish speakers with native competence and 572 million Spanish speakers as a first or second language—including speakers with limited competence—and more than 21 million students of Spanish as a foreign language.

Spanish is the official or national language in Spain, Equatorial Guinea, and 19 countries in the Americas. Speakers in the Americas total some 418 million. It is also an optional language in the Philippines as it was a Spanish colony from 1569 to 1899. In the European Union, Spanish is the mother tongue of 8% of the population, with an additional 7% speaking it as a second language. Spanish is the most popular second language learned in the United States. In 2011 it was estimated by the American Community Survey that of the 55 million Hispanic United States residents who are five years of age and over, 38 million speak Spanish at home.

According to a 2011 paper by U.S. Census Bureau Demographers Jennifer Ortman and Hyon B. Shin, the number of Spanish speakers is projected to rise through 2020 to anywhere between 39 million and 43 million, depending on the assumptions one makes about immigration. Most of these Spanish speakers will be Hispanic, with Ortman and Shin projecting between 37.5 million and 41 million Hispanic Spanish speakers by 2020.

In Spain and in some other parts of the Spanish-speaking world, Spanish is called not only (Spanish) but also (Castilian), the language from the kingdom of Castile, contrasting it with other languages spoken in Spain such as Galician, Basque, Asturian, Catalan, Aragonese and Occitan.

The Spanish Constitution of 1978 uses the term to define the official language of the whole Spanish State in contrast to (lit. "the other Spanish languages"). Article III reads as follows:
The Spanish Royal Academy, on the other hand, currently uses the term in its publications, but from 1713 to 1923 called the language .

The (a language guide published by the Spanish Royal Academy) states that, although the Spanish Royal Academy prefers to use the term in its publications when referring to the Spanish language, both terms— and —are regarded as synonymous and equally valid.

The term "castellano" (Castillian), comes from the Latin word "castellanus", which means "from Castilla", the medieval kingdom located in the central part of the Iberian Peninsula, where this language originated.

Different etymologies have been suggested for the term "español" (Spanish). According to the Royal Spanish Academy, "español" (Spanish) derives from the Provençal word "espaignol" and that, in turn, derives from the Medieval Latin word "Hispaniolus", which means "from —or pertaining to— Hispania". The Latin form comes from the Latin name of the province of that included the current territory of the Iberian Peninsula. In late Latin, the // was silent and // evolved into a brief /e/ resulting in the word .

There are other hypotheses apart from the one suggested by the Royal Spanish Academy. Some philologists argue that "español" comes from Occitan "Espaignon". On the other hand, Spanish philologist Menéndez Pidal suggested that the classic "hispanus" or "hispanicus" took the suffix "-one" from Vulgar Latin, as it happened with other words such as bretón (Breton) or sajón (Saxon). The term "hispanione" evolved into the Old Spanish "españón", which eventually, became "español".

The Spanish language evolved from Vulgar Latin, which was brought to the Iberian Peninsula by the Romans during the Second Punic War, beginning in 210 BC. Previously, several pre-Roman languages (also called Paleohispanic languages)—some related to Latin via Indo-European, and some that are not related at all—were spoken in the Iberian Peninsula. These languages included Basque (still spoken today), Iberian, Celtiberian and Gallaecian.

The first documents to show traces of what is today regarded as the precursor of modern Spanish are from the 9th century. Throughout the Middle Ages and into the modern era, the most important influences on the Spanish lexicon came from neighboring Romance languages—Mozarabic (Andalusi Romance), Navarro-Aragonese, Leonese, Catalan, Portuguese, Galician, Occitan, and later, French and Italian. Spanish also borrowed a considerable number of words from Arabic, as well as a minor influence from the Germanic Gothic language through the migration of tribes and a period of Visigoth rule in Iberia. In addition, many more words were borrowed from Latin through the influence of written language and the liturgical language of the Church. The loanwords were taken from both Classical Latin and Renaissance Latin, the form of Latin in use at that time.

According to the theories of Ramón Menéndez Pidal, local sociolects of Vulgar Latin evolved into Spanish, in the north of Iberia, in an area centered in the city of Burgos, and this dialect was later brought to the city of Toledo, where the written standard of Spanish was first developed, in the 13th century. In this formative stage, Spanish developed a strongly differing variant from its close cousin, Leonese, and, according to some authors, was distinguished by a heavy Basque influence (see Iberian Romance languages). This distinctive dialect spread to southern Spain with the advance of the , and meanwhile gathered a sizable lexical influence from the Arabic of Al-Andalus, much of it indirectly, through the Romance Mozarabic dialects (some 4,000 Arabic-derived words, make up around 8% of the language today). The written standard for this new language was developed in the cities of Toledo, in the 13th to 16th centuries, and Madrid, from the 1570s.

The development of the Spanish sound system from that of Vulgar Latin exhibits most of the changes that are typical of Western Romance languages, including lenition of intervocalic consonants (thus Latin > Spanish ). The diphthongization of Latin stressed short and —which occurred in open syllables in French and Italian, but not at all in Catalan or Portuguese—is found in both open and closed syllables in Spanish, as shown in the following table:
Spanish is marked by the palatalization of the Latin double consonants and (thus Latin

The consonant written or in Latin and pronounced in Classical Latin had probably "fortified" to a bilabial fricative in Vulgar Latin. In early Spanish (but not in Catalan or Portuguese) it merged with the consonant written "b" (a bilabial with plosive and fricative allophones). In modern Spanish, there is no difference between the pronunciation of orthographic and , with some exceptions in Caribbean Spanish.

Peculiar to Spanish (as well as to the neighboring Gascon dialect of Occitan, and attributed to a Basque substratum) was the mutation of Latin initial into whenever it was followed by a vowel that did not diphthongize. The , still preserved in spelling, is now silent in most varieties of the language, although in some Andalusian and Caribbean dialects it is still aspirated in some words. Because of borrowings from Latin and from neighboring Romance languages, there are many -/-doublets in modern Spanish: and (both Spanish for "Ferdinand"), and (both Spanish for "smith"), and (both Spanish for "iron"), and and (both Spanish for "deep", but means "bottom" while means "deep"); (Spanish for "to make") is cognate to the root word of (Spanish for "to satisfy"), and ("made") is similarly cognate to the root word of (Spanish for "satisfied").

Compare the examples in the following table:

Some consonant clusters of Latin also produced characteristically different results in these languages, as shown in the examples in the following table:

In the 15th and 16th centuries, Spanish underwent a dramatic change in the pronunciation of its sibilant consonants, known in Spanish as the , which resulted in the distinctive velar pronunciation of the letter and—in a large part of Spain—the characteristic interdental ("th-sound") for the letter (and for before or ). See History of Spanish (Modern development of the Old Spanish sibilants) for details.

The , written in Salamanca in 1492 by Elio Antonio de Nebrija, was the first grammar written for a modern European language. According to a popular anecdote, when Nebrija presented it to Queen Isabella I, she asked him what was the use of such a work, and he answered that language is the instrument of empire. In his introduction to the grammar, dated 18 August 1492, Nebrija wrote that "... language was always the companion of empire."

From the sixteenth century onwards, the language was taken to the Spanish-discovered America and the Spanish East Indies via Spanish colonization of America. Miguel de Cervantes Saavedra, author of "Don Quixote", is such a well-known reference in the world that Spanish is often called ("the language of Cervantes").

In the twentieth century, Spanish was introduced to Equatorial Guinea and the Western Sahara, and to areas of the United States that had not been part of the Spanish Empire, such as Spanish Harlem in New York City. For details on borrowed words and other external influences upon Spanish, see Influences on the Spanish language.

Most of the grammatical and typological features of Spanish are shared with the other Romance languages. Spanish is a fusional language. The noun and adjective systems exhibit two genders and two numbers, in addition articles and some pronouns and determiners have a neuter gender in singular. There are about fifty conjugated forms per verb, with 3 tenses: past, present, future; 2 aspects for past: perfective, imperfective; 4 moods: indicative, subjunctive, conditional, imperative; 3 persons: first, second, third; 2 numbers: singular, plural; 3 verboid forms: infinitive, gerund, and past participle. Verbs express T-V distinction by using different persons for formal and informal addresses. (For a detailed overview of verbs, see Spanish verbs and Spanish irregular verbs.)

Spanish syntax is considered right-branching, meaning that subordinate or modifying constituents tend to be placed after their head words. The language uses prepositions (rather than postpositions or inflection of nouns for case), and usually—though not always—places adjectives after nouns, as do most other Romance languages.

The language is classified as a subject–verb–object language; however, as in most Romance languages, constituent order is highly variable and governed mainly by topicalization and focus rather than by syntax. It is a "pro-drop", or "null-subject" language—that is, it allows the deletion of subject pronouns when they are pragmatically unnecessary. Spanish is described as a "verb-framed" language, meaning that the "direction" of motion is expressed in the verb while the "mode" of locomotion is expressed adverbially (e.g. "subir corriendo" or "salir volando"; the respective English equivalents of these examples—'to run up' and 'to fly out'—show that English is, by contrast, "satellite-framed", with mode of locomotion expressed in the verb and direction in an adverbial modifier).

Subject/verb inversion is not required in questions, and thus the recognition of declarative or interrogative may depend entirely on intonation.

The Spanish phonemic system is originally descended from that of Vulgar Latin. Its development exhibits some traits in common with the neighboring dialects—especially Leonese and Aragonese—as well as other traits unique to Castilian. Castilian is unique among its neighbors in the aspiration and eventual loss of the Latin initial sound (e.g. Cast. vs. Leon. and Arag. ). The Latin initial consonant sequences , , and in Spanish typically become (originally pronounced ), while in Aragonese they are preserved, and in Leonese they present a variety of outcomes, including , , and . Where Latin had before a vowel (e.g. ) or the ending , (e.g. ), Old Spanish produced , that in Modern Spanish became the velar fricative (, , where neighboring languages have the palatal lateral (e.g. Portuguese , ; Catalan , ).

The Spanish phonemic inventory consists of five vowel phonemes (, , , , ) and 17 to 19 consonant phonemes (the exact number depending on the dialect). The main allophonic variation among vowels is the reduction of the high vowels and to glides— and respectively—when unstressed and adjacent to another vowel. Some instances of the mid vowels and , determined lexically, alternate with the diphthongs and respectively when stressed, in a process that is better described as morphophonemic rather than phonological, as it is not predictable from phonology alone.

The Spanish consonant system is characterized by (1) three nasal phonemes, and one or two (depending on the dialect) lateral phoneme(s), which in syllable-final position lose their contrast and are subject to assimilation to a following consonant; (2) three voiceless stops and the affricate ; (3) three or four (depending on the dialect) voiceless fricatives; (4) a set of voiced obstruents—, , , and sometimes —which alternate between approximant and plosive allophones depending on the environment; and (5) a phonemic distinction between the "tapped" and "trilled" "r"-sounds (single and double in orthography).

In the following table of consonant phonemes, is marked with an asterisk (*) to indicate that it is preserved only in some dialects. In most dialects it has been merged with in the merger called . Similarly, is also marked with an asterisk to indicate that most dialects do not distinguish it from (see ), although this is not a true merger but an outcome of different evolution of sibilants in Southern Spain.

The phoneme is in parentheses () to indicate that it appears only in loanwords. Each of the voiced obstruent phonemes , , , and appears to the right of a "pair" of voiceless phonemes, to indicate that, while the "voiceless" phonemes maintain a phonemic contrast between plosive (or affricate) and fricative, the "voiced" ones alternate allophonically (i.e. without phonemic contrast) between plosive and approximant pronunciations.

Spanish is classified by its rhythm as a syllable-timed language: each syllable has approximately the same duration regardless of stress.

Spanish intonation varies significantly according to dialect but generally conforms to a pattern of falling tone for declarative sentences and wh-questions (who, what, why, etc.) and rising tone for yes/no questions. There are no syntactic markers to distinguish between questions and statements and thus, the recognition of declarative or interrogative depends entirely on intonation.

Stress most often occurs on any of the last three syllables of a word, with some rare exceptions at the fourth-last or earlier syllables. The "tendencies" of stress assignment are as follows:

In addition to the many exceptions to these tendencies, there are numerous minimal pairs that contrast solely on stress such as ('sheet') and ('savannah'); ('boundary'), ('[that] he/she limits') and ('I limited'); ('liquid'), ('I sell off') and ('he/she sold off').

The orthographic system unambiguously reflects where the stress occurs: in the absence of an accent mark, the stress falls on the last syllable unless the last letter is , , or a vowel, in which cases the stress falls on the next-to-last (penultimate) syllable. Exceptions to those rules are indicated by an acute accent mark over the vowel of the stressed syllable. (See Spanish orthography.)

Spanish is the primary language of 20 countries worldwide. It is estimated that the combined total number of Spanish speakers is between 470 and 500 million, making it the second most widely spoken language in terms of native speakers.

Spanish is the third most spoken language by total number of speakers (after Mandarin and English). Internet usage statistics for 2007 also show Spanish as the third most commonly used language on the Internet, after English and Mandarin.

In Europe, Spanish is an official language of Spain, the country after which it is named and from which it originated. It is widely spoken in Gibraltar, and also commonly spoken in Andorra, although Catalan is the official language there.

Spanish is also spoken by small communities in other European countries, such as the United Kingdom, France, Italy, and Germany. Spanish is an official language of the European Union. In Switzerland, which had a massive influx of Spanish migrants in the 20th century, Spanish is the native language of 2.2% of the population.

Most Spanish speakers are in Hispanic America; of all countries with a majority of Spanish speakers, only Spain and Equatorial Guinea are outside the Americas. Nationally, Spanish is the official language—either "de facto" or "de jure"—of Argentina, Bolivia (co-official with Quechua, Aymara, Guarani, and 34 other languages), Chile, Colombia, Costa Rica, Cuba, Dominican Republic, Ecuador, El Salvador, Guatemala, Honduras, Mexico (co-official with 63 indigenous languages), Nicaragua, Panama, Paraguay (co-official with Guaraní), Peru (co-official with Quechua, Aymara, and "the other indigenous languages"), Puerto Rico (co-official with English), Uruguay, and Venezuela.
Spanish has no official recognition in the former British colony of Belize; however, per the 2000 census, it is spoken by 43% of the population. Mainly, it is spoken by the descendants of Hispanics who have been in the region since the seventeenth century; however, English is the official language.

Due to their proximity to Spanish-speaking countries, Trinidad and Tobago and Brazil have implemented Spanish language teaching into their education systems. The Trinidad government launched the "Spanish as a First Foreign Language" (SAFFL) initiative in March 2005. In 2005, the National Congress of Brazil approved a bill, signed into law by the President, making it mandatory for schools to offer Spanish as an alternative foreign language course in both public and private secondary schools in Brazil. In September 2016 this law was revoked by Michel Temer after impeachment of Dilma Rousseff. In many border towns and villages along Paraguay and Uruguay, a mixed language known as Portuñol is spoken.

According to 2006 census data, 44.3 million people of the U.S. population were Hispanic or Hispanic American by origin; 38.3 million people, 13 percent of the population over five years old speak Spanish at home. The Spanish language has a long history of presence in the United States due to early Spanish and, later, Mexican administration over territories now forming the southwestern states, also Louisiana ruled by Spain from 1762 to 1802, as well as Florida, which was Spanish territory until 1821.

Spanish is by far the most common second language in the US, with over 50 million total speakers if non-native or second-language speakers are included. While English is the de facto national language of the country, Spanish is often used in public services and notices at the federal and state levels. Spanish is also used in administration in the state of New Mexico. The language also has a strong influence in major metropolitan areas such as those of Los Angeles, Miami, San Antonio, New York, San Francisco, Dallas, and Phoenix; as well as more recently, Chicago, Las Vegas, Boston, Denver, Houston, Indianapolis, Philadelphia, Cleveland, Salt Lake City, Atlanta, Nashville, Orlando, Tampa, Raleigh and Baltimore-Washington, D.C. due to 20th- and 21st-century immigration.

In Africa, Spanish is official (along with Portuguese and French) in Equatorial Guinea, as well as an official language of the African Union. In Equatorial Guinea, Spanish is the predominant language when native and non-native speakers (around 500,000 people) are counted, while Fang is the most spoken language by number of native speakers.

Spanish is also spoken in the integral territories of Spain in North Africa, which include the Spanish cities of Ceuta and Melilla, the Plazas de soberanía, and the Canary Islands archipelago (population 2,000,000), located some off the northwest coast of mainland Africa. In northern Morocco, a former Spanish protectorate that is also geographically close to Spain, approximately 20,000 people speak Spanish as a second language, while Arabic is the "de jure" official language. A small number of Moroccan Jews also speak the Sephardic Spanish dialect Haketia (related to the Ladino dialect spoken in Israel). Spanish is spoken by some small communities in Angola because of the Cuban influence from the Cold War and in South Sudan among South Sudanese natives that relocated to Cuba during the Sudanese wars and returned in time for their country's independence.

In Western Sahara, formerly Spanish Sahara, Spanish was officially spoken during the late nineteenth and twentieth centuries. Today, Spanish in this disputed territory is maintained by populations of Sahrawi nomads numbering about 500,000 people, and is de facto official alongside Arabic in the Sahrawi Arab Democratic Republic, although this entity receives limited international recognition.

Spanish was an official language of the Philippines from the beginning of Spanish administration in 1565 to a constitutional change in 1973. During Spanish colonization (1565–1898), it was the language of government, trade and education, and spoken as a first language by Spaniards and educated Filipinos. In the mid-nineteenth century, the colonial government set up a free public education system with Spanish as the medium of instruction. This increased use of Spanish throughout the islands led to the formation of a class of Spanish-speaking intellectuals called the "Ilustrados". By the time of Philippine independence in 1898, around 70% of the population had knowledge of Spanish, with 10% speaking it as their first and only language and about 60% of the population spoke it as their second or third language.

Despite American administration after the defeat of Spain in the Spanish–American War in 1898, the usage of Spanish continued in Philippine literature and press during the early years of American administration. Gradually, however, the American government began increasingly promoting the use of English, and it characterized Spanish as a negative influence of the past. Eventually, by the 1920s, English became the primary language of administration and education. But despite a significant decrease in influence and speakers, Spanish remained an official language of the Philippines when it became independent in 1946, alongside English and Filipino, a standardized version of Tagalog.
Spanish was removed from official status in 1973 under the administration of Ferdinand Marcos, but regained its status as an official language two months later under Presidential Decree No. 155, dated 15 March 1973. It remained an official language until 1987, with the ratification of the present constitution, in which it was re-designated as a voluntary and optional auxiliary language. In 2010, President Gloria Macapagal-Arroyo encouraged the reintroduction of Spanish-language teaching in the Philippine education system. But by 2012, the number of secondary schools at which the language was either a compulsory subject or an elective had become very limited. Today, despite government promotions of Spanish, less than 0.5% of the population report being able to speak the language proficiently. Aside from standard Spanish, a Spanish-based creole language—Chavacano—developed in the southern Philippines. The number of Chavacano-speakers was estimated at 1.2 million in 1996. However, it is not mutually intelligible with Spanish. Speakers of the Zamboangueño variety of Chavacano were numbered about 360,000 in the 2000 census. The local languages of the Philippines also retain some Spanish influence, with many words being derived from Mexican Spanish, owing to the administration of the islands by Spain through New Spain until 1821, and then directly from Madrid until 1898.

Spanish is also the official language and the most spoken on Easter Island which is geographically part of Polynesia in Oceania and politically part of Chile. Easter Island's traditional language is Rapa Nui, an Eastern Polynesian language.

Spanish loan words are present in the local languages of Guam, Northern Mariana Islands, Palau, Marshall Islands and Micronesia, all of which formerly comprised the Spanish East Indies.

The following table shows the number of Spanish speakers in some 79 countries.
There are important variations (phonological, grammatical, and lexical) in the spoken Spanish of the various regions of Spain and throughout the Spanish-speaking areas of the Americas.

The variety with the most speakers is Mexican Spanish. It is spoken by more than twenty percent of the world's Spanish speakers (more than 112 million of the total of more than 500 million, according to the table above). One of its main features is the reduction or loss of unstressed vowels, mainly when they are in contact with the sound /s/.

In Spain, northern dialects are popularly thought of as closer to the standard, although positive attitudes toward southern dialects have increased significantly in the last 50 years. Even so, the speech of Madrid, which has typically southern features such as yeísmo and s-aspiration, is the standard variety for use on radio and television. However, the variety used in the media is that of Madrid's educated classes, where southern traits are less evident, in contrast with the variety spoken by working-class Madrid, where those traits are pervasive. The educated variety of Madrid is indicated by many as the one that has most influenced the written standard for Spanish.

The four main phonological divisions are based respectively on (1) the phoneme ("theta"), (2) the debuccalization of syllable-final , (3) the sound of the spelled , (4) and the phoneme ("turned "y""),

The main morphological variations between dialects of Spanish involve differing uses of pronouns, especially those of the second person and, to a lesser extent, the object pronouns of the third person.

Virtually all dialects of Spanish make the distinction between a formal and a familiar register in the second-person singular and thus have two different pronouns meaning "you": in the formal and either or in the familiar (and each of these three pronouns has its associated verb forms), with the choice of or varying from one dialect to another. The use of (and/or its verb forms) is called . In a few dialects, all three pronouns are used, with , , and denoting respectively formality, familiarity, and intimacy.

In , is the subject form (, "you say") and the form for the object of a preposition (, "I am going with you"), while the direct and indirect object forms, and the possessives, are the same as those associated with : ("You know your friends respect you").

The verb forms of "general voseo" are the same as those used with except in the present tense (indicative and imperative) verbs. The forms for generally can be derived from those of (the traditional second-person familiar "plural") by deleting the glide , or , where it appears in the ending: > ; > , () > (), () > () .
In Chilean on the other hand, almost all verb forms are distinct from their standard -forms.
The use of the pronoun with the verb forms of () is called "pronominal ". Conversely, the use of the verb forms of with the pronoun ( or ) is called "verbal ". 
In Chile, for example, "verbal voseo" is much more common than the actual use of the pronoun "vos", which is usually reserved for highly informal situations.

And in Central American , one can see even further distinction.
Although is not used in Spain, it occurs in many Spanish-speaking regions of the Americas as the primary spoken form of the second-person singular familiar pronoun, with wide differences in social consideration. Generally, it can be said that there are zones of exclusive use of (the use of ) in the following areas: almost all of Mexico, the West Indies, Panama, most of Colombia, Peru, Venezuela and coastal Ecuador.

Areas of generalized include Argentina, Nicaragua, eastern Bolivia, El Salvador, Guatemala, Honduras, Costa Rica, Paraguay, Uruguay and the Colombian departments of Antioquia, Caldas, Risaralda, Quindio and Valle del Cauca.

 functions as formal and informal second person plural in over 90% of the Spanish-speaking world, including all of Hispanic America, the Canary Islands, and some regions of Andalusia. In Seville, Huelva, Cadiz, and other parts of western Andalusia, the familiar form is constructed as , using the traditional second-person plural form of the verb. Most of Spain maintains the formal/familiar distinction with and respectively.

 is the usual second-person singular pronoun in a formal context, but it is used jointly with the third-person singular voice of the verb. It is used to convey respect toward someone who is a generation older or is of higher authority ("you, sir"/"you, ma'am"). It is also used in a "familiar" context by many speakers in Colombia and Costa Rica and in parts of Ecuador and Panama, to the exclusion of or . This usage is sometimes called in Spanish.

In Central America, especially in Honduras, is often used as a formal pronoun to convey respect between the members of a romantic couple. is also used that way between parents and children in the Andean regions of Ecuador, Colombia and Venezuela.

Most speakers use (and the prefers) the pronouns and for "direct" objects (masculine and feminine respectively, regardless of animacy, meaning "him", "her", or "it"), and for "indirect" objects (regardless of gender or animacy, meaning "to him", "to her", or "to it"). The usage is sometimes called "etymological", as these direct and indirect object pronouns are a continuation, respectively, of the accusative and dative pronouns of Latin, the ancestor language of Spanish.

Deviations from this norm (more common in Spain than in the Americas) are called "", "", or "", according to which respective pronoun, , , or , has expanded beyond the etymological usage ( as a direct object, or or as an indirect object).

Some words can be significantly different in different Hispanophone countries. Most Spanish speakers can recognize other Spanish forms even in places where they are not commonly used, but Spaniards generally do not recognize specifically American usages. For example, Spanish , and (respectively, 'butter', 'avocado', 'apricot') correspond to (word used for lard in Peninsular Spanish), , and , respectively, in Argentina, Chile (except ), Paraguay, Peru (except and ), and Uruguay.

Spanish is closely related to the other West Iberian Romance languages, including Asturian, Aragonese, Galician, Ladino, Leonese, Mirandese and Portuguese.

It is generally acknowledged that Portuguese and Spanish speakers can communicate in written form, with varying degrees of mutual intelligibility.
Mutual intelligibility of the "written" Spanish and Portuguese languages is remarkably high, and the difficulties of the spoken forms are based more on phonology than on grammatical and lexical dissimilarities. "Ethnologue" gives estimates of the lexical similarity between related languages in terms of precise percentages. For Spanish and Portuguese, that figure is 89%. Italian, on the other hand its phonology similar to Spanish, but has a lower lexical similarity of 82%. Mutual intelligibility between Spanish and French or between Spanish and Romanian is lower still, given lexical similarity ratings of 75% and 71% respectively. And comprehension of Spanish by French speakers who have not studied the language is much lower, at an estimated 45%. In general, thanks to the common features of the writing systems of the Romance languages, interlingual comprehension of the written word is greater than that of oral communication.

The following table compares the forms of some common words in several Romance languages:

Judaeo-Spanish, also known as Ladino, is a variety of Spanish which preserves many features of medieval Spanish and Portuguese and is spoken by descendants of the Sephardi Jews who were expelled from Spain in the 15th century. Conversely, in Portugal the vast majority of the Portuguese Jews converted and became 'New Christians'. Therefore, its relationship to Spanish is comparable with that of the Yiddish language to German. Ladino speakers today are almost exclusively Sephardi Jews, with family roots in Turkey, Greece, or the Balkans, and living mostly in Israel, Turkey, and the United States, with a few communities in Hispanic America. Judaeo-Spanish lacks the Native American vocabulary which was acquired by standard Spanish during the Spanish colonial period, and it retains many archaic features which have since been lost in standard Spanish. It contains, however, other vocabulary which is not found in standard Spanish, including vocabulary from Hebrew, French, Greek and Turkish, and other languages spoken where the Sephardim settled.

Judaeo-Spanish is in serious danger of extinction because many native speakers today are elderly as well as elderly "olim" (immigrants to Israel) who have not transmitted the language to their children or grandchildren. However, it is experiencing a minor revival among Sephardi communities, especially in music. In the case of the Latin American communities, the danger of extinction is also due to the risk of assimilation by modern Castilian.

A related dialect is Haketia, the Judaeo-Spanish of northern Morocco. This too tended to assimilate with modern Spanish, during the Spanish occupation of the region.

Spanish is written in the Latin script, with the addition of the character (, representing the phoneme , a letter distinct from , although typographically composed of an with a tilde). Formerly the digraphs (, representing the phoneme ) and (, representing the phoneme ), were also considered single letters. However, the digraph (, 'strong r', , 'double r', or simply ), which also represents a distinct phoneme , was not similarly regarded as a single letter. Since 1994 and have been treated as letter pairs for collation purposes, though they remained a part of the alphabet until 2010. Words with are now alphabetically sorted between those with and , instead of following as they used to. The situation is similar for .

Thus, the Spanish alphabet has the following 27 letters:

Since 2010, none of the digraphs () is considered a letter by the Spanish Royal Academy.

The letters and are used only in words and names coming from foreign languages (, etc.).

With the exclusion of a very small number of regional terms such as (see Toponymy of Mexico), pronunciation can be entirely determined from spelling. Under the orthographic conventions, a typical Spanish word is stressed on the syllable before the last if it ends with a vowel (not including ) or with a vowel followed by or an ; it is stressed on the last syllable otherwise. Exceptions to this rule are indicated by placing an acute accent on the stressed vowel.

The acute accent is used, in addition, to distinguish between certain homophones, especially when one of them is a stressed word and the other one is a clitic: compare ('the', masculine singular definite article) with ('he' or 'it'), or ('you', object pronoun) with ('tea'), (preposition 'of') versus ('give' [formal imperative/third-person present subjunctive]), and (reflexive pronoun) versus ('I know' or imperative 'be').

The interrogative pronouns (, , , , etc.) also receive accents in direct or indirect questions, and some demonstratives (, , , etc.) can be accented when used as pronouns. Accent marks used to be omitted on capital letters (a widespread practice in the days of typewriters and the early days of computers when only lowercase vowels were available with accents), although the advises against this and the orthographic conventions taught at schools enforce the use of the accent.

When is written between and a front vowel or , it indicates a "hard g" pronunciation. A diaeresis indicates that it is not silent as it normally would be (e.g., , 'stork', is pronounced ; if it were written *, it would be pronounced *).

Interrogative and exclamatory clauses are introduced with inverted question and exclamation marks ( and , respectively).

The (Royal Spanish Academy), founded in 1713, together with the 21 other national ones (see Association of Spanish Language Academies), exercises a standardizing influence through its publication of dictionaries and widely respected grammar and style guides.
Because of influence and for other sociohistorical reasons, a standardized form of the language (Standard Spanish) is widely acknowledged for use in literature, academic contexts and the media.

The Association of Spanish Language Academies (, or ) is the entity which regulates the Spanish language. It was created in Mexico in 1951 and represents the union of all the separate academies in the Spanish-speaking world. It comprises the academies of 23 countries, ordered by date of Academy foundation: Spain (1713), Colombia (1871), Ecuador (1874), Mexico (1875), El Salvador (1876), Venezuela (1883), Chile (1885), Peru (1887), Guatemala (1887), Costa Rica (1923), Philippines (1924), Panama (1926), Cuba (1926),
Paraguay (1927), Dominican Republic (1927), Bolivia (1927), Nicaragua (1928), Argentina (1931), Uruguay (1943), Honduras (1949), Puerto Rico (1955), United States (1973) and Equatorial Guinea (2016).
The (Cervantes Institute) is a worldwide nonprofit organization created by the Spanish government in 1991. This organization has branched out in over 20 different countries, with 75 centers devoted to the Spanish and Hispanic American cultures and Spanish language. The ultimate goals of the Institute are to promote universally the education, the study, and the use of Spanish as a second language, to support methods and activities that help the process of Spanish-language education, and to contribute to the advancement of the Spanish and Hispanic American cultures in non-Spanish-speaking countries. The Institute's 2015 report "El español, una lengua viva" (Spanish, a living language) estimated that there were 559 million Spanish speakers worldwide. Its latest annual report "El español en el mundo 2018" (Spanish in the world 2018) counts 577 million Spanish speakers worldwide. Among the sources cited in the report is the U.S. Census Bureau, which estimates that the U.S. will have 138 million Spanish speakers by 2050, making it the biggest Spanish-speaking nation on earth, with Spanish the mother tongue of almost a third of its citizens.

Spanish is one of the official languages of the United Nations, the European Union, the World Trade Organization, the Organization of American States, the Organization of Ibero-American States, the African Union, the Union of South American Nations, the Antarctic Treaty Secretariat, the Latin Union, the Caricom, the North American Free Trade Agreement, and numerous other international organizations.












</doc>
<doc id="26826" url="https://en.wikipedia.org/wiki?curid=26826" title="Sodium">
Sodium

Sodium is a chemical element with the symbol Na (from Latin "natrium") and atomic number 11. It is a soft, silvery-white, highly reactive metal. Sodium is an alkali metal, being in group 1 of the periodic table, because it has a single electron in its outer shell, which it readily donates, creating a positively charged ion—the Na cation. Its only stable isotope is Na. The free metal does not occur in nature, and must be prepared from compounds. Sodium is the sixth most abundant element in the Earth's crust and exists in numerous minerals such as feldspars, sodalite, and rock salt (NaCl). Many salts of sodium are highly water-soluble: sodium ions have been leached by the action of water from the Earth's minerals over eons, and thus sodium and chlorine are the most common dissolved elements by weight in the oceans.

Sodium was first isolated by Humphry Davy in 1807 by the electrolysis of sodium hydroxide. Among many other useful sodium compounds, sodium hydroxide (lye) is used in soap manufacture, and sodium chloride (edible salt) is a de-icing agent and a nutrient for animals including humans.

Sodium is an essential element for all animals and some plants. Sodium ions are the major cation in the extracellular fluid (ECF) and as such are the major contributor to the ECF osmotic pressure and ECF compartment volume. Loss of water from the ECF compartment increases the sodium concentration, a condition called hypernatremia. Isotonic loss of water and sodium from the ECF compartment decreases the size of that compartment in a condition called ECF hypovolemia.

By means of the sodium-potassium pump, living human cells pump three sodium ions out of the cell in exchange for two potassium ions pumped in; comparing ion concentrations across the cell membrane, inside to outside, potassium measures about 40:1, and sodium, about 1:10. In nerve cells, the electrical charge across the cell membrane enables transmission of the nerve impulse—an action potential—when the charge is dissipated; sodium plays a key role in that activity.

Sodium at standard temperature and pressure is a soft silvery metal that combines with oxygen in the air and forms grayish white sodium oxide unless immersed in oil or inert gas, which are the conditions it is usually stored in. Sodium metal can be easily cut with a knife and is a good conductor of electricity and heat because it has only one electron in its valence shell, resulting in weak metallic bonding and free electrons, which carry energy. Due to having low atomic mass and large atomic radius, sodium is third-least dense of all elemental metals and is one of only three metals that can float on water, the other two being lithium and potassium. The melting (98 °C) and boiling (883 °C) points of sodium are lower than those of lithium but higher than those of the heavier alkali metals potassium, rubidium, and caesium, following periodic trends down the group. These properties change dramatically at elevated pressures: at 1.5 Mbar, the color changes from silvery metallic to black; at 1.9 Mbar the material becomes transparent with a red color; and at 3 Mbar, sodium is a clear and transparent solid. All of these high-pressure allotropes are insulators and electrides.
In a flame test, sodium and its compounds glow yellow because the excited 3s electrons of sodium emit a photon when they fall from 3p to 3s; the wavelength of this photon corresponds to the D line at about 589.3 nm. Spin-orbit interactions involving the electron in the 3p orbital split the D line into two, at 589.0 and 589.6 nm; hyperfine structures involving both orbitals cause many more lines.

Twenty isotopes of sodium are known, but only Na is stable. Na is created in the carbon-burning process in stars by fusing two carbon atoms together; this requires temperatures above 600 megakelvins and a star of at least three solar masses. Two radioactive, cosmogenic isotopes are the byproduct of cosmic ray spallation: Na has a half-life of 2.6 years and Na, a half-life of 15 hours; all other isotopes have a half-life of less than one minute. Two nuclear isomers have been discovered, the longer-lived one being Na with a half-life of around 20.2 milliseconds. Acute neutron radiation, as from a nuclear criticality accident, converts some of the stable Na in human blood to Na; the neutron radiation dosage of a victim can be calculated by measuring the concentration of Na relative to Na.

Sodium atoms have 11 electrons, one more than the stable configuration of the noble gas neon. The first and second ionization energies are 495.8 kJ/mol and 4562 kJ/mol), respectively. As a result, sodium usually forms ionic compounds involving the Na cation.

Metallic sodium is generally less reactive than potassium and more reactive than lithium. Sodium metal is highly reducing, with the standard reduction potential for the Na/Na couple being −2.71 volts, though potassium and lithium have even more negative potentials.

Sodium compounds are of immense commercial importance, being particularly central to industries producing glass, paper, soap, and textiles. The most important sodium compounds are table salt (NaCl), soda ash (NaCO), baking soda (NaHCO), caustic soda (NaOH), sodium nitrate (NaNO), di- and tri-sodium phosphates, sodium thiosulfate (NaSO·5HO), and borax (NaBO·10HO). In compounds, sodium is usually ionically bonded to water and anions and is viewed as a hard Lewis acid.
Most soaps are sodium salts of fatty acids. Sodium soaps have a higher melting temperature (and seem "harder") than potassium soaps.

Like all the alkali metals, sodium reacts exothermically with water. The reaction produces caustic soda (sodium hydroxide) and flammable hydrogen gas. When burned in air, it forms primarily sodium peroxide with some sodium oxide.

Sodium tends to form water-soluble compounds, such as halides, sulfates, nitrates, carboxylates and carbonates. The main aqueous species are the aquo complexes [Na(HO)], where "n" = 4–8; with "n" = 6 indicated from X-ray diffraction data and computer simulations.

Direct precipitation of sodium salts from aqueous solutions is rare because sodium salts typically have a high affinity for water. An exception is sodium bismuthate (NaBiO). Because of the high solubility of its compounds, sodium salts are usually isolated as solids by evaporation or by precipitation with an organic antisolvent, such as ethanol; for example, only 0.35 g/L of sodium chloride will dissolve in ethanol. Crown ethers, like 15-crown-5, may be used as a phase-transfer catalyst.

Sodium content of samples is determined by atomic absorption spectrophotometry or by potentiometry using ion-selective electrodes.

Like the other alkali metals, sodium dissolves in ammonia and some amines to give deeply colored solutions; evaporation of these solutions leaves a shiny film of metallic sodium. The solutions contain the coordination complex (Na(NH)), with the positive charge counterbalanced by electrons as anions; cryptands permit the isolation of these complexes as crystalline solids. Sodium forms complexes with crown ethers, cryptands and other ligands. For example, 15-crown-5 has a high affinity for sodium because the cavity size of 15-crown-5 is 1.7–2.2 Å, which is enough to fit the sodium ion (1.9 Å). Cryptands, like crown ethers and other ionophores, also have a high affinity for the sodium ion; derivatives of the alkalide Na are obtainable by the addition of cryptands to solutions of sodium in ammonia via disproportionation.

Many organosodium compounds have been prepared. Because of the high polarity of the C-Na bonds, they behave like sources of carbanions (salts with organic anions). Some well-known derivatives include sodium cyclopentadienide (NaCH) and trityl sodium ((CH)CNa). Sodium naphthalenide, Na[CH•], a strong reducing agent, forms upon mixing Na and naphthalene in ethereal solutions.

Sodium forms alloys with many metals, such as potassium, calcium, lead, and the group 11 and 12 elements. Sodium and potassium form KNa and NaK. NaK is 40–90% potassium and it is liquid at ambient temperature. It is an excellent thermal and electrical conductor. Sodium-calcium alloys are by-products of the electrolytic production of sodium from a binary salt mixture of NaCl-CaCl and ternary mixture NaCl-CaCl-BaCl. Calcium is only partially miscible with sodium. In a liquid state, sodium is completely miscible with lead. There are several methods to make sodium-lead alloys. One is to melt them together and another is to deposit sodium electrolytically on molten lead cathodes. NaPb, NaPb, NaPb, NaPb, and NaPb are some of the known sodium-lead alloys. Sodium also forms alloys with gold (NaAu) and silver (NaAg). Group 12 metals (zinc, cadmium and mercury) are known to make alloys with sodium. NaZn and NaCd are alloys of zinc and cadmium. Sodium and mercury form NaHg, NaHg, NaHg, NaHg, and NaHg.

Because of its importance in human health, salt has long been an important commodity, as shown by the English word "salary", which derives from "salarium", the wafers of salt sometimes given to Roman soldiers along with their other wages. In medieval Europe, a compound of sodium with the Latin name of "sodanum" was used as a headache remedy. The name sodium is thought to originate from the Arabic "suda", meaning headache, as the headache-alleviating properties of sodium carbonate or soda were well known in early times. Although sodium, sometimes called "soda", had long been recognized in compounds, the metal itself was not isolated until 1807 by Sir Humphry Davy through the electrolysis of sodium hydroxide. In 1809, the German physicist and chemist Ludwig Wilhelm Gilbert proposed the names "Natronium" for Humphry Davy's "sodium" and "Kalium" for Davy's "potassium". The chemical abbreviation for sodium was first published in 1814 by Jöns Jakob Berzelius in his system of atomic symbols, and is an abbreviation of the element's New Latin name "natrium", which refers to the Egyptian "natron", a natural mineral salt mainly consisting of hydrated sodium carbonate. Natron historically had several important industrial and household uses, later eclipsed by other sodium compounds.

Sodium imparts an intense yellow color to flames. As early as 1860, Kirchhoff and Bunsen noted the high sensitivity of a sodium flame test, and stated in Annalen der Physik und Chemie:

In a corner of our 60 m room farthest away from the apparatus, we exploded 3 mg of sodium chlorate with milk sugar while observing the nonluminous flame before the slit. After a while, it glowed a bright yellow and showed a strong sodium line that disappeared only after 10 minutes. From the weight of the sodium salt and the volume of air in the room, we easily calculate that one part by weight of air could not contain more than 1/20 millionth weight of sodium.

The Earth's crust contains 2.27% sodium, making it the seventh most abundant element on Earth and the fifth most abundant metal, behind aluminium, iron, calcium, and magnesium and ahead of potassium. Sodium's estimated oceanic abundance is 1.08 milligrams per liter. Because of its high reactivity, it is never found as a pure element. It is found in many minerals, some very soluble, such as halite and natron, others much less soluble, such as amphibole and zeolite. The insolubility of certain sodium minerals such as cryolite and feldspar arises from their polymeric anions, which in the case of feldspar is a polysilicate.

Atomic sodium has a very strong spectral line in the yellow-orange part of the spectrum (the same line as is used in sodium vapour street lights). This appears as an absorption line in many types of stars, including the Sun. The line was first studied in 1814 by Joseph von Fraunhofer during his investigation of the lines in the solar spectrum, now known as the Fraunhofer lines. Fraunhofer named it the 'D line', although it is now known to actually be a group of closely spaced lines split by a fine and hyperfine structure.

The strength of the D line means it has been detected in many other astronomical environments. In stars, it is seen in any whose surfaces are cool enough for sodium to exist in atomic form (rather than ionised). This corresponds to stars of roughly F-type and cooler. Many other stars appear to have a sodium absorption line, but this is actually caused by gas in the foreground interstellar medium. The two can be distinguished via high-resolution spectroscopy, because interstellar lines are much narrower than those broadened by stellar rotation.

Sodium has also been detected in numerous Solar System environments, including Mercury's atmosphere, the exosphere of the Moon, and numerous other bodies. Some comets have a sodium tail, which was first detected in observations of Comet Hale-Bopp in 1997. Sodium has even been detected in the atmospheres of some extrasolar planets via transit spectroscopy.

Employed only in rather specialized applications, only about 100,000 tonnes of metallic sodium are produced annually. Metallic sodium was first produced commercially in the late 19th century by carbothermal reduction of sodium carbonate at 1100 °C, as the first step of the Deville process for the production of aluminium:

The high demand for aluminium created the need for the production of sodium. The introduction of the Hall–Héroult process for the production of aluminium by electrolysing a molten salt bath ended the need for large quantities of sodium. A related process based on the reduction of sodium hydroxide was developed in 1886.

Sodium is now produced commercially through the electrolysis of molten sodium chloride, based on a process patented in 1924. This is done in a Downs cell in which the NaCl is mixed with calcium chloride to lower the melting point below 700 °C. As calcium is less electropositive than sodium, no calcium will be deposited at the cathode. This method is less expensive than the previous Castner process (the electrolysis of sodium hydroxide).

The market for sodium is volatile due to the difficulty in its storage and shipping; it must be stored under a dry inert gas atmosphere or anhydrous mineral oil to prevent the formation of a surface layer of sodium oxide or sodium superoxide.

Though metallic sodium has some important uses, the major applications for sodium use compounds; millions of tons of sodium chloride, hydroxide, and carbonate are produced annually. Sodium chloride is extensively used for anti-icing and de-icing and as a preservative; examples of the uses of sodium bicarbonate include baking, as a raising agent, and sodablasting. Along with potassium, many important medicines have sodium added to improve their bioavailability; though potassium is the better ion in most cases, sodium is chosen for its lower price and atomic weight. Sodium hydride is used as a base for various reactions (such as the aldol reaction) in organic chemistry, and as a reducing agent in inorganic chemistry.

Metallic sodium is used mainly for the production of sodium borohydride, sodium azide, indigo, and triphenylphosphine. A once-common use was the making of tetraethyllead and titanium metal; because of the move away from TEL and new titanium production methods, the production of sodium declined after 1970. Sodium is also used as an alloying metal, an anti-scaling agent, and as a reducing agent for metals when other materials are ineffective. Note the free element is not used as a scaling agent, ions in the water are exchanged for sodium ions. Sodium plasma ("vapor") lamps are often used for street lighting in cities, shedding light that ranges from yellow-orange to peach as the pressure increases. By itself or with potassium, sodium is a desiccant; it gives an intense blue coloration with benzophenone when the desiccate is dry. In organic synthesis, sodium is used in various reactions such as the Birch reduction, and the sodium fusion test is conducted to qualitatively analyse compounds. Sodium reacts with alcohol and gives alkoxides, and when sodium is dissolved in ammonia solution, it can be used to reduce alkynes to trans-alkenes. Lasers emitting light at the sodium D line are used to create artificial laser guide stars that assist in the adaptive optics for land-based visible-light telescopes.

Liquid sodium is used as a heat transfer fluid in some types of nuclear reactors because it has the high thermal conductivity and low neutron absorption cross section required to achieve a high neutron flux in the reactor. The high boiling point of sodium allows the reactor to operate at ambient (normal) pressure, but the drawbacks include its opacity, which hinders visual maintenance, and its explosive properties. Radioactive sodium-24 may be produced by neutron bombardment during operation, posing a slight radiation hazard; the radioactivity stops within a few days after removal from the reactor. If a reactor needs to be shut down frequently, NaK is used; because NaK is a liquid at room temperature, the coolant does not solidify in the pipes. In this case, the pyrophoricity of potassium requires extra precautions to prevent and detect leaks. Another heat transfer application is poppet valves in high-performance internal combustion engines; the valve stems are partially filled with sodium and work as a heat pipe to cool the valves.

In humans, sodium is an essential mineral that regulates blood volume, blood pressure, osmotic equilibrium and pH. The minimum physiological requirement for sodium is estimated to range from about 120 milligrams per day in newborns to 500 milligrams per day over the age of 10.

Sodium chloride is the principal source of sodium in the diet, and is used as seasoning and preservative in such commodities as pickled preserves and jerky; for Americans, most sodium chloride comes from processed foods. Other sources of sodium are its natural occurrence in food and such food additives as monosodium glutamate (MSG), sodium nitrite, sodium saccharin, baking soda (sodium bicarbonate), and sodium benzoate.

The U.S. Institute of Medicine set its Tolerable Upper Intake Level for sodium at 2.3 grams per day, but the average person in the United States consumes 3.4 grams per day.

Studies have found that lowering sodium intake by 2 g per day tends to lower systolic blood pressure by about two to four mm Hg. It has been estimated that such a decrease in sodium intake would lead to between 9 and 17% fewer cases of hypertension.

Hypertension causes 7.6 million premature deaths worldwide each year. (Note that salt contains about 39.3% sodiumthe rest being chlorine and trace chemicals; thus, 2.3 g sodium is about 5.9 g, or 5.3 ml, of saltabout one US teaspoon.) The American Heart Association recommends no more than 1.5 g of sodium per day.

One study found that people with or without hypertension who excreted less than 3 grams of sodium per day in their urine (and therefore were taking in less than 3 g/d) had a "higher" risk of death, stroke, or heart attack than those excreting 4 to 5 grams per day. Levels of 7 g per day or more in people with hypertension were associated with higher mortality and cardiovascular events, but this was not found to be true for people without hypertension. The US FDA states that adults with hypertension and prehypertension should reduce daily intake to 1.5 g.

The renin–angiotensin system regulates the amount of fluid and sodium concentration in the body. Reduction of blood pressure and sodium concentration in the kidney result in the production of renin, which in turn produces aldosterone and angiotensin, which stimulates the reabsorption of sodium back into the bloodstream. When the concentration of sodium increases, the production of renin decreases, and the sodium concentration returns to normal. The sodium ion (Na) is an important electrolyte in neuron function, and in osmoregulation between cells and the extracellular fluid. This is accomplished in all animals by Na/K-ATPase, an active transporter pumping ions against the gradient, and sodium/potassium channels. Sodium is the most prevalent metallic ion in extracellular fluid.

Unusually low or high sodium levels in humans are recognized in medicine as hyponatremia and hypernatremia. These conditions may be caused by genetic factors, ageing, or prolonged vomiting or diarrhea.

In C4 plants, sodium is a micronutrient that aids metabolism, specifically in regeneration of phosphoenolpyruvate and synthesis of chlorophyll. In others, it substitutes for potassium in several roles, such as maintaining turgor pressure and aiding in the opening and closing of stomata. Excess sodium in the soil can limit the uptake of water by decreasing the water potential, which may result in plant wilting; excess concentrations in the cytoplasm can lead to enzyme inhibition, which in turn causes necrosis and chlorosis. In response, some plants have developed mechanisms to limit sodium uptake in the roots, to store it in cell vacuoles, and restrict salt transport from roots to leaves; excess sodium may also be stored in old plant tissue, limiting the damage to new growth. Halophytes have adapted to be able to flourish in sodium rich environments.

Sodium forms flammable hydrogen and caustic sodium hydroxide on contact with water; ingestion and contact with moisture on skin, eyes or mucous membranes can cause severe burns. Sodium spontaneously explodes in the presence of water due to the formation of hydrogen (highly explosive) and sodium hydroxide (which dissolves in the water, liberating more surface). However, sodium exposed to air and ignited or reaching autoignition (reported to occur when a molten pool of sodium reaches about 290 °C) displays a relatively mild fire. In the case of massive (non-molten) pieces of sodium, the reaction with oxygen eventually becomes slow due to formation of a protective layer. Fire extinguishers based on water accelerate sodium fires; those based on carbon dioxide and bromochlorodifluoromethane should not be used on sodium fire. Metal fires are Class D, but not all Class D extinguishers are workable with sodium. An effective extinguishing agent for sodium fires is Met-L-X. Other effective agents include Lith-X, which has graphite powder and an organophosphate flame retardant, and dry sand. Sodium fires are prevented in nuclear reactors by isolating sodium from oxygen by surrounding sodium pipes with inert gas. Pool-type sodium fires are prevented using diverse design measures called catch pan systems. They collect leaking sodium into a leak-recovery tank where it is isolated from oxygen.



</doc>
<doc id="26828" url="https://en.wikipedia.org/wiki?curid=26828" title="Suriname">
Suriname

Suriname (, , sometimes spelled Surinam), officially known as the Republic of Suriname ( ), is a country on the northeastern Atlantic coast of South America. It is bordered by the Atlantic Ocean to the north, French Guiana to the east, Guyana to the west and Brazil to the south. At just under , it is the smallest sovereign state in South America. Suriname has a population of approximately , most of whom live on the country's north coast, in and around the capital and largest city, Paramaribo.

Suriname was long inhabited by various indigenous people before being invaded and contested by European powers from the 16th century, eventually coming under Dutch rule in the late 17th century. As the chief sugar colony during the Dutch colonial period, it was primarily a plantation economy dependent on African slaves and, following the abolition of slavery in 1863, indentured servants from Asia. Suriname was ruled by the Dutch-chartered company Society of Suriname between 1683 and 1795.

In 1954, Suriname became one of the constituent countries of the Kingdom of the Netherlands. On 25 November 1975, the country of Suriname left the Kingdom of the Netherlands to become an independent state, nonetheless maintaining close economic, diplomatic, and cultural ties to its former colonizer. Suriname is considered to be a culturally Caribbean country, and is a member of the Caribbean Community (CARICOM). While Dutch is the official language of government, business, media, and education, Sranan Tongo, an English-based creole language, is a widely used "lingua franca". Suriname is the only sovereign nation outside Europe where Dutch is spoken by a majority of the population. The people of Suriname are among the most diverse in the world, spanning a multitude of ethnic, religious, and linguistic groups.

The name "Suriname" may derive from an indigenous people called "Surinen," who inhabited the area at the time of European contact. It may also be derived from a corruption of the name ""Surryham"" which was the name given to the Suriname River by Lord Willoughby in honour of the Earl of Surrey when an English colony was established under a grant from King Charles II. 

British settlers, who founded the first European colony at Marshall's Creek along the Suriname River, spelled the name as "Surinam".

When the territory was taken over by the Dutch, it became part of a group of colonies known as Dutch Guiana. The official spelling of the country's English name was changed from "Surinam" to "Suriname" in January 1978, but "Surinam" can still be found in English. A notable example is Suriname's national airline, Surinam Airways. The older English name is reflected in the English pronunciation, . In Dutch, the official language of Suriname, the pronunciation is , with the main stress on the third syllable and a schwa terminal vowel.

Indigenous settlement of Suriname dates back to 3,000 BC. The largest tribes were the Arawak, a nomadic coastal tribe that lived from hunting and fishing. They were the first inhabitants in the area. The Carib also settled in the area and conquered the Arawak by using their superior sailing ships. They settled in Galibi ("Kupali Yumï," meaning "tree of the forefathers") at the mouth of the Marowijne River. While the larger Arawak and Carib tribes lived along the coast and savanna, smaller groups of indigenous people lived in the inland rainforest, such as the Akurio, Trió, Warrau, and Wayana.

Beginning in the 16th century, French, Spanish and English explorers visited the area. A century later, Dutch and English settlers established plantation colonies along the many rivers in the fertile Guiana plains. The earliest documented colony in Guiana was an English settlement named Marshall's Creek along the Suriname River. After that there was another short-lived English colony called Willoughbyland that lasted from 1650 to 1674.

Disputes arose between the Dutch and the English for control of this territory. In 1667, during negotiations leading to the Treaty of Breda, the Dutch decided to keep the nascent plantation colony of Suriname they had gained from the English. The English were able to keep New Amsterdam, the main city of the former colony of New Netherland in North America on the mid-Atlantic coast. Already a cultural and economic hub in those days, they renamed it after the Duke of York: New York City.

In 1683, the Society of Suriname was founded by the city of Amsterdam, the Van Aerssen van Sommelsdijck family, and the Dutch West India Company. The society was chartered to manage and defend the colony. The planters of the colony relied heavily on African slaves to cultivate, harvest and process the commodity crops of coffee, cocoa, sugar cane and cotton plantations along the rivers. Planters' treatment of the slaves was notoriously bad—historian C. R. Boxer wrote that "man's inhumanity to man just about reached its limits in Surinam"—and many slaves escaped the plantations. In November 1795, the Society was nationalized by the Batavian Republic and from then on, the Batavian Republic and its legal successors (the Kingdom of Holland and the Kingdom of the Netherlands) governed the territory as a national colony, barring a period of British occupation between 1799 and 1802, and between 1804 and 1816.

With the help of the native South Americans living in the adjoining rain forests, these runaway slaves established a new and unique culture in the interior that was highly successful in its own right. They were known collectively in English as Maroons, in French as "Nèg'Marrons" (literally meaning "brown negroes", that is "pale-skinned negroes"), and in Dutch as "Marrons." The Maroons gradually developed several independent tribes through a process of ethnogenesis, as they were made up of slaves from different African ethnicities. These tribes include the Saramaka, Paramaka, Ndyuka or Aukan, Kwinti, Aluku or Boni, and Matawai.
The Maroons often raided plantations to recruit new members from the slaves and capture women, as well as to acquire weapons, food and supplies. They sometimes killed planters and their families in the raids; colonists built defenses, which were so important they were shown on 18th-century maps, but these were not sufficient.

The colonists also mounted armed campaigns against the Maroons, who generally escaped through the rain forest, which they knew much better than did the colonists. To end hostilities, in the 18th century the European colonial authorities signed several peace treaties with different tribes. They granted the Maroons sovereign status and trade rights in their inland territories, giving them autonomy.

From 1861 to 1863, with the American Civil War underway, and enslaved people escaping to Southern territory controlled by the Union, United States President Abraham Lincoln and his administration looked abroad for places to relocate people who were freed from enslavement and who wanted to leave the United States. It opened negotiations with the Dutch government regarding African-American emigration to and colonization of the Dutch colony of Suriname. Nothing came of the idea, and the idea was dropped after 1864.

The Netherlands abolished slavery in Suriname in 1863, under a gradual process that required enslaved people to work on plantations for 10 transition years for minimal pay, which was considered as partial compensation for their masters. After 1873, most freedmen largely abandoned the plantations where they had worked for several generations in favor of the capital city, Paramaribo.
As a plantation colony, Suriname had an economy dependent on labor-intensive commodity crops. To make up for a shortage of labor, the Dutch recruited and transported contract or indentured laborer from the Dutch East Indies (modern Indonesia) and India (the latter through an arrangement with the British, who then ruled the area). In addition, during the late 19th and early 20th centuries, small numbers of laborers, mostly men, were recruited from China and the Middle East.

Although Suriname's population remains relatively small, because of this complex colonization and exploitation, it is one of the most ethnically and culturally diverse countries in the world.

During World War II, on 23 November 1941, under an agreement with the Netherlands government-in-exile, the United States occupied Suriname to protect the bauxite mines to support the Allies' war effort. In 1942, the Dutch government-in-exile began to review the relations between the Netherlands and its colonies in terms of the post-war period.

In 1954, Suriname became one of the constituent countries of the Kingdom of the Netherlands, along with the Netherlands Antilles and the Netherlands. In this construction, the Netherlands retained control of its defense and foreign affairs. In 1974, the local government, led by the National Party of Suriname (NPS) (whose membership was largely Creole, meaning ethnically African or mixed African-European) started negotiations with the Dutch government leading towards full independence, which was granted on 25 November 1975. A large part of Suriname's economy for the first decade following independence was fueled by foreign aid provided by the Dutch government.

The first President of the country was Johan Ferrier, the former governor, with Henck Arron (the then leader of the NPS) as Prime Minister. In the years leading up to independence, nearly one-third of the population of Suriname emigrated to the Netherlands, amidst concern that the new country would fare worse under independence than it had as a constituent country of the Kingdom of the Netherlands. Surinamese politics did degenerate into ethnic polarisation and corruption soon after independence, with the NPS using Dutch aid money for partisan purposes. Its leaders were accused of fraud in the 1977 elections, in which Arron won a further term, and the discontent was such that a large chunk of the population fled to the Netherlands, joining the already significant Surinamese community there.

On 25 February 1980, a military coup overthrew Arron's government. It was initiated by a group of 16 sergeants, led by Dési Bouterse. Opponents of the military regime attempted counter-coups in April 1980, August 1980, 15 March 1981, and again on 12 March 1982. The first counter attempt was led by Fred Ormskerk, the second by Marxist-Leninists, the third by Wilfred Hawker, and the fourth by Surendre Rambocus.

Hawker escaped from prison during the fourth counter-coup attempt, but he was captured and summarily executed. Between 2 am and 5 am on 7 December 1982, the military, under the leadership of Dési Bouterse, rounded up 13 prominent citizens who had criticized the military dictatorship and held them at Fort Zeelandia in Paramaribo. The dictatorship had all these men executed over the next three days, along with Rambocus and Jiwansingh Sheombar (who was also involved in the fourth counter-coup attempt).

National elections were held in 1987. The National Assembly adopted a new constitution that allowed Bouterse to remain in charge of the army. Dissatisfied with the government, Bouterse summarily dismissed the ministers in 1990, by telephone. This event became popularly known as the "Telephone Coup". His power began to wane after the 1991 elections.

The brutal civil war between the Suriname army and Maroons loyal to rebel leader Ronnie Brunswijk, begun in 1986, continued and its effects further weakened Bouterse's position during the 1990s. Due to the civil war, more than 10,000 Surinamese, mostly Maroons, fled to French Guiana in the late 1980s.

In 1999, the Netherlands tried Bouterse "in absentia" on drug smuggling charges. He was convicted and sentenced to prison but remained in Suriname.

On 19 July 2010, the former dictator Dési Bouterse returned to power when he was elected as the new President of Suriname. Before his election in 2010, he, along with 24 others, had been charged with the murders of 15 prominent dissidents in the December murders. However, in 2012, two months before the verdict in the trial, the National Assembly extended its amnesty law and provided Bouterse and the others with amnesty of these charges. He was reelected on 14 July 2015. However, Bouterse was convicted by a Surinamese court on 29 November 2019 and given a 20-year sentence for his role in the 1982 killings.

The Republic of Suriname is a representative democratic republic, based on the Constitution of 1987. The legislative branch of government consists of a 51-member unicameral National Assembly, simultaneously and popularly elected for a five-year term.

In the elections held on Tuesday, 25 May 2010, the "Megacombinatie" won 23 of the National Assembly seats followed by "Nationale Front" with 20 seats. A much smaller number, important for coalition-building, went to the "A‑combinatie" and to the "Volksalliantie." The parties held negotiations to form coalitions. Elections were held on 25 May 2015, and the National Assembly again elected Desire Bouterse as President.

The President of Suriname is elected for a five-year term by a two-thirds majority of the National Assembly. If at least two-thirds of the National Assembly cannot agree to vote for one presidential candidate, a People's Assembly is formed from all National Assembly delegates and regional and municipal representatives who were elected by popular vote in the most recent national election. The president may be elected by a majority of the People's Assembly called for the special election.

As head of government, the president appoints a sixteen-minister cabinet. A vice president, is normally elected for a five-year term at the same time as the president, by a simple majority in the National Assembly or People's Assembly. There is no constitutional provision for removal or replacement of the president, except in the case of resignation.

The judiciary is headed by the High Court of Justice of Suriname (Supreme Court). This court supervises the magistrate courts. Members are appointed for life by the president in consultation with the National Assembly, the State Advisory Council, and the National Order of Private Attorneys. In April 2005, the regional Caribbean Court of Justice, based in Trinidad, was inaugurated. As the final court of appeal, it was intended to replace the London-based Privy Council.

President Dési Bouterse was convicted and sentenced in the Netherlands to 11 years of imprisonment for drug trafficking. He is the main suspect in the court case concerning the 'December murders,' the 1982 assassination of opponents of military rule in Fort Zeelandia, Paramaribo. These two cases still strain relations between the Netherlands and Suriname.

Due to Suriname's Dutch colonial history, Suriname had a long-standing special relationship with the Netherlands. The Dutch government has stated that it will only maintain limited contact with the president.

Bouterse was elected as president of Suriname in 2010. The Netherlands in July 2014 dropped Suriname as a member of its development program.

Since 1991, the United States has maintained positive relations with Suriname. The two countries work together through the Caribbean Basin Security Initiative (CBSI) and the U.S. President's Emergency Plan for AIDS Relief (PEPFAR). Suriname also receives military funding from the U.S. Department of Defense.

European Union relations and cooperation with Suriname are carried out both on a bilateral and a regional basis. There are ongoing EU-Community of Latin American and Caribbean States (CELAC) and EU-CARIFORUM dialogues. Suriname is party to the Cotonou Agreement, the partnership agreement among the members of the African, Caribbean and Pacific Group of States and the European Union.

On 17 February 2005, the leaders of Barbados and Suriname signed the "Agreement for the deepening of bilateral cooperation between the Government of Barbados and the Government of the Republic of Suriname." On 23–24 April 2009, both nations formed a Joint Commission in Paramaribo, Suriname, to improve relations and to expand into various areas of cooperation. They held a second meeting toward this goal on 3–4 March 2011, in Dover, Barbados. Their representatives reviewed issues of agriculture, trade, investment, as well as international transport.

In the late 2000s, Suriname intensified development cooperation with other developing countries. China's South-South cooperation with Suriname has included a number of large-scale infrastructure projects, including port rehabilitation and road construction. Brazil signed agreements to cooperate with Suriname in education, health, agriculture, and energy production.

The Armed Forces of Suriname have three branches: the Army, the Air Force, and the Navy. The President of the Republic, Dési Bouterse, is the Supreme Commander-in-Chief of the Armed Forces ("Opperbevelhebber van de Strijdkrachten"). The President is assisted by the Minister of Defence. Beneath the President and Minister of Defence is the Commander of the Armed Forces ("Bevelhebber van de Strijdkrachten"). The Military Branches and regional Military Commands report to the Commander.

After the creation of the Statute of the Kingdom of the Netherlands, the Royal Netherlands Army was entrusted with the defense of Suriname, while the defense of the Netherlands Antilles was the responsibility of the Royal Netherlands Navy. The army set up a separate "Troepenmacht in Suriname" (Forces in Suriname, TRIS). Upon independence in 1975, this force was turned into the "Surinaamse Krijgsmacht" (SKM):, Surinamese Armed Forces. On 25 February 1980, a group of 15 non-commissioned officers and one junior SKM officer, under the leadership of sergeant major Dési Bouterse, overthrew the Government. Subsequently, the SKM was rebranded as "Nationaal Leger" (NL), National Army.

In 1965 the Dutch and Americans used Suriname's Coronie site for multiple Nike Apache sounding rocket launches.

The country is divided into ten administrative districts, each headed by a district commissioner appointed by the president, who also has the power of dismissal. Suriname is further subdivided into 62 resorts (ressorten).

Suriname is the smallest independent country in South America. Situated on the Guiana Shield, it lies mostly between latitudes 1° and 6°N, and longitudes 54° and 58°W. The country can be divided into two main geographic regions. The northern, lowland coastal area (roughly above the line Albina-Paranam-Wageningen) has been cultivated, and most of the population lives here. The southern part consists of tropical rainforest and sparsely inhabited savanna along the border with Brazil, covering about 80% of Suriname's land surface.

The two main mountain ranges are the Bakhuys Mountains and the Van Asch Van Wijck Mountains. Julianatop is the highest mountain in the country at above sea level. Other mountains include Tafelberg at , Mount Kasikasima at , Goliathberg at and Voltzberg at .

Suriname's forest cover is 90.2%, the highest of any nation in the world.

Suriname is situated between French Guiana to the east and Guyana to the west. The southern border is shared with Brazil and the northern border is the Atlantic coast. The southernmost borders with French Guiana and Guyana are disputed by these countries along the Marowijne and Corantijn rivers, respectively, while a part of the disputed maritime boundary with Guyana was arbitrated by a tribunal convened under the rules set out in of the United Nations Convention on the Law of the Sea on 20 September 2007.

Lying 2 to 5 degrees north of the equator, Suriname has a very hot and wet tropical climate, and temperatures do not vary much throughout the year. Average relative humidity is between 80% and 90%. Its average temperature ranges from 29 to 34 degrees Celsius (84 to 93 degrees Fahrenheit). Due to the high humidity, actual temperatures are distorted and may therefore feel up to 6 degrees Celsius (11 degrees Fahrenheit) hotter than the recorded temperature. The year has two wet seasons, from April to August and from November to February. It also has two dry seasons, from August to November and February to April.

Located in the upper Coppename River watershed, the Central Suriname Nature Reserve has been designated a UNESCO World Heritage Site for its unspoiled forests and biodiversity. There are many national parks in the country including Galibi National Reserve along the coast; Brownsberg Nature Park and Eilerts de Haan Nature Park in central Suriname; and the Sipaliwani Nature Reserve on the Brazilian border. In all, 16% of the country's land area is national parks and lakes, according to the UNEP World Conservation Monitoring Centre.

Suriname's democracy gained some strength after the turbulent 1990s, and its economy became more diversified and less dependent on Dutch financial assistance. Bauxite (aluminium ore) mining continues to be a strong revenue source, and the discovery and exploitation of oil and gold has added substantially to Suriname's economic independence. Agriculture, especially rice and bananas, remains a strong component of the economy, and ecotourism is providing new economic opportunities. More than 80% of Suriname's land-mass consists of unspoiled rain forest; with the establishment of the Central Suriname Nature Reserve in 1998, Suriname signalled its commitment to conservation of this precious resource. The Central Suriname Nature Reserve became a World Heritage Site in 2000.
The economy of Suriname was dominated by the bauxite industry, which accounts for more than 15% of GDP and 70% of export earnings up to 2016. Other main export products include rice, bananas and shrimp. Suriname has recently started exploiting some of its sizeable oil and gold reserves. About a quarter of the people work in the agricultural sector. The Surinamese economy is very dependent on commerce, its main trade partners being the Netherlands, the United States, Canada, and Caribbean countries, mainly Trinidad and Tobago and the islands of the former Netherlands Antilles.

After assuming power in the fall of 1996, the Wijdenbosch government ended the structural adjustment program of the previous government, claiming it was unfair to the poorer elements of society. Tax revenues fell as old taxes lapsed and the government failed to implement new tax alternatives. By the end of 1997, the allocation of new Dutch development funds was frozen as Surinamese Government relations with the Netherlands deteriorated. Economic growth slowed in 1998, with decline in the mining, construction, and utility sectors. Rampant government expenditures, poor tax collection, a bloated civil service, and reduced foreign aid in 1999 contributed to the fiscal deficit, estimated at 11% of GDP. The government sought to cover this deficit through monetary expansion, which led to a dramatic increase in inflation. It takes longer on average to register a new business in Suriname than virtually any other country in the world (694 days or about 99 weeks).


According to the 2012 census, Suriname had a population of 541,638 inhabitants. The Surinamese populace is characterized by its high level of diversity, wherein no particular demographic group constitutes a majority. This is a legacy of centuries of Dutch rule, which entailed successive periods of forced, contracted, or voluntary migration by various nationalities and ethnic groups from around the world.

The largest ethnic group are the Afro-Surinamese which form about 37% of the population, and are usually divided into two groups: the Creoles and the Maroons. Surinamese Maroons, whose ancestors are mostly runaway slaves that fled to the interior, comprise 21.7% of the population; they are divided into five main groups: Ndyuka (Aucans), Kwinti, Matawai, Saramaccans and Paramaccans. Surinamese Creoles, mixed people descending from African slaves and mostly Dutch Europeans, form 15.7% of the population. East Indians, who form 27% of the population, are the second largest group. They are descendants of 19th-century contract workers from India, hailing mostly from the modern Indian states of Bihar, Jharkhand, and Eastern Uttar Pradesh along the Nepali border. Javanese make up 14% of the population, and like the East Indians, descend largely from workers contracted from the island of Java in the former Dutch East Indies (modern Indonesia). 13.4% of the population identifies as being of mixed ethnic heritage.

Other sizeable groups include the Chinese, originating from 19th-century contract workers and some recent migration, who number over 40,000 ; Lebanese, primarily Maronites; Jews of Sephardic and Ashkenazi origin, whose center of population was the community of Jodensavanne; and Brazilians, many of them laborers mining for gold.

A small but influential number of Europeans remain in the country, comprising about 1 percent of the population. They are descended mostly from Dutch 19th-century immigrant farmers, known as "Boeroes" (derived from "boer", the Dutch word for "farmer"), and to a lesser degree other European groups, such as Portuguese from Madeira. Many Boeroes left after independence in 1975.

Various indigenous peoples make up 3.7% of the population, with the main groups being the Akurio, Arawak, Kalina (Caribs), Tiriyó and Wayana. They live mainly in the districts of Paramaribo, Wanica, Para, Marowijne and Sipaliwini.

The vast majority of Suriname's inhabitants (about 90%) live in Paramaribo or on the coast.

The choice of becoming Surinamese or Dutch citizens in the years leading up to Suriname's independence in 1975 led to a mass migration to the Netherlands. This migration continued in the period immediately after independence and during military rule in the 1980s and for largely economic reasons extended throughout the 1990s. The Surinamese community in the Netherlands numbered 350,300 (including children and grandchildren of Suriname migrants born in The Netherlands); this is compared to approximately 566,000 Surinamese in Suriname itself.

According to the International Organization for Migration, around 272,600 people from Suriname lived in other countries in the late 2010s, in particular in the Netherlands (ca 192,000), the French Republic (ca 25,000, most of them in French Guiana), the United States (ca 15,000), Guyana (ca 5,000), Aruba (ca 1,500), and Canada (ca 1,000).

Suriname's religious makeup is heterogeneous and reflective of the country's multicultural character.

According to the 2012 census, 48.4% were Christians; 26.7% of Surinamese were Protestants (11.18% Pentecostal, 11.16% Moravian, and 4.4% of various other Protestant denominations) and 21.6% were Roman Catholics. Hindus formed the second-largest religious group in Suriname, comprising 22.3% of the population, the third largest proportion of any country in the Western Hemisphere after Guyana and Trinidad and Tobago, both of which also have large proportions of Indians. Almost all practitioners of Hinduism are found among the Indo-Surinamese population. Muslims constitute 13.9% of the population, the highest proportion of Muslims in the Americas; they are largely of Javanese or Indian descent. Other religious groups include Winti (1.8%), an Afro-American religion practiced mostly by those of Maroon ancestry; Javanism (0.8%), a syncretic faith found among some Javanese Surinamese; and various indigenous folk traditions that are often incorporated into one of the larger religions (usually Christianity). In the 2012 census, 7.5% of the population declared they had "no religion", while a further 3.2% left the question unanswered.

Dutch is the sole official language, and is the language of education, government, business, and the media. Over 60% of the population speaks Dutch as a mother tongue, and most of the rest speaks it as a second language. In 2004, Suriname became an associate member of the Dutch Language Union. It is the only Dutch-speaking country in South America as well as the only independent nation in the Americas where Dutch is spoken by a majority of the population, and one of the two non-Romance-speaking countries in South America, the other being English-speaking Guyana.

In Paramaribo, Dutch is the main home language in two-thirds of the households. The recognition of ""Surinaams-Nederlands"" (""Surinamese Dutch"") as a national dialect equal to ""Nederlands-Nederlands"" (""Dutch Dutch"") and ""Vlaams-Nederlands"" (""Flemish Dutch"") was expressed in 2009 by the publication of the "Woordenboek Surinaams Nederlands" ("Surinamese–Dutch Dictionary"). It is the most commonly spoken language in urban areas; only in the interior of Suriname (namely parts of Sipaliwini and Brokopondo) is Dutch seldom spoken.

Sranan Tongo, a local creole language originally spoken by the Creole population group, is the most widely used vernacular language in day-to-day life and business. It and Dutch are considered to be the two principal languages of Surinamese diglossia; both are further influenced by other spoken languages which are spoken primarily within ethnic communities. Sranan Tongo is often used interchangeably with Dutch depending on the formality of the setting, where Dutch is seen as a prestige dialect and Sranan Tongo the common vernacular.

Caribbean Hindustani or Sarnami, a dialect of Bhojpuri, is the fourth-most used language (after English), spoken by the descendants of South Asian contract workers from then British India. The Javanese language is used by the descendants of Javanese contract workers, and is common in Suriname. The Maroon languages, somewhat intelligible with Sranan, include Saramaka, Paramakan, Ndyuka (also called "Aukan"), Kwinti and Matawai. Amerindian languages, spoken by Amerindians, include Carib and Arawak. Hakka and Cantonese are spoken by the descendants of the Chinese contract workers. Mandarin is spoken by some few recent Chinese immigrants. English, Spanish, and Portuguese are also used as second languages.

The national capital, Paramaribo, is by far the dominant urban area, accounting for nearly half of Suriname's population and most of its urban residents; indeed, its population is greater than the next nine largest cities combined. Most municipalities are located within the capital's metropolitan area, or along the densely populated coastline.

Owing to the country's multicultural heritage, Suriname celebrates a variety of distinct ethnic and religious festivals.


There are several Hindu and Islamic national holidays like Diwali (deepavali), Phagwa and Eid ul-Fitr and Eid-ul-adha. These holidays do not have fixed dates on the Gregorian calendar, as they are based on the Hindu and Islamic calendars, respectively.

There are several holidays which are unique to Suriname. These include the Indian, Javanese and Chinese arrival days. They celebrate the arrival of the first ships with their respective immigrants.

New Year's Eve in Suriname is called "Oud jaar", "Owru Yari", or "old year". It is during this period that the Surinamese population goes to the city's commercial district to watch "demonstrational fireworks". The bigger stores invest in these firecrackers and display them out in the streets. Every year the length of them is compared, and high praises are given for the company that has imported the largest ribbon.

These celebrations start at 10 in the morning and finish the next day. The day is usually filled with laughter, dance, music, and drinking. When the night starts, the big street parties are already at full capacity. The most popular fiesta is the one that is held at café 't Vat in the main tourist district. The parties there stop between 10 and 11 at night, after which people go home to light their pagaras (red-firecracker-ribbons) at midnight.
After 12, the parties continue and the streets fill again until daybreak.

The major sports in Suriname are football, basketball, and volleyball. The Suriname Olympic Committee is the national governing body for sports in Suriname. The mayor mind sports are chess, draughts, bridge and troefcall.

Many Suriname-born football players and Dutch-born football players of Surinamese descent, like Gerald Vanenburg, Ruud Gullit, Frank Rijkaard, Edgar Davids, Clarence Seedorf, Patrick Kluivert, Aron Winter, Georginio Wijnaldum, Virgil van Dijk, Jimmy Floyd Hasselbaink and Ryan Babel have turned out to play for the Dutch national team. In 1999, Humphrey Mijnals, who played for both Suriname and the Netherlands, was elected Surinamese footballer of the century. Another famous player is André Kamperveen, who captained Suriname in the 1940s and was the first Surinamese to play professionally in the Netherlands.

The most famous international track & field athlete from Suriname is Letitia Vriesde, who won a silver medal at the 1995 World Championships behind Ana Quirot in the 800 metres, the first medal won by a South American female athlete in World Championship competition. In addition, she also won a bronze medal at the 2001 World Championships and won several medals in the 800 and 1500 metres at the Pan-American Games and Central American and Caribbean Games. Tommy Asinga also received acclaim for winning a bronze medal in the 800 metres at the 1991 Pan American Games.

Swimmer Anthony Nesty is the only Olympic medalist for Suriname. He won gold in the 100-meter butterfly at the 1988 Summer Olympics in Seoul and he won bronze in the same discipline at the 1992 Summer Olympics in Barcelona. Originally from Trinidad and Tobago, he now lives in Gainesville, Florida, and is the coach of the University of Florida, mainly coaching distance swimmers.

Cricket is popular in Suriname to some extent, influenced by its popularity in the Netherlands and in neighbouring Guyana. The Surinaamse Cricket Bond is an associate member of the International Cricket Council (ICC). Suriname and Argentina are the only ICC associates in South America, although Guyana is represented on the West Indies Cricket Board, a full member. The national cricket team was ranked 47th in the world and sixth in the ICC Americas region as of June 2014, and competes in the World Cricket League (WCL) and ICC Americas Championship. Iris Jharap, born in Paramaribo, played women's One Day International matches for the Dutch national side, the only Surinamese to do so.

In the sport of badminton the local heroes are Virgil Soeroredjo & Mitchel Wongsodikromo and also Crystal Leefmans. All winning medals for Suriname at the Carebaco Caribbean Championships, the Central American and Caribbean Games (CACSO Games) and also at the South American Games, better known as the ODESUR Games. Virgil Soeroredjo also participated for Suriname at the 2012 London Summer Olympics, only the second badminton player, after Oscar Brandon, for Suriname to achieve this. Current National Champion Sören Opti was the third Surinamese badminton player to participate at the Summer Olympics in 2016.

Multiple time K-1 kickboxing champions Ernesto Hoost and Remy Bonjasky were born in Suriname or are of Surinamese descent. Other kickboxing world-champions include Rayen Simson, Tyrone Spong, Regian Eersel, and MMA and kickboxing champions Melvin Manhoef and Jairzinho Rozenstruik.

Suriname, along with neighboring Guyana, is one of only two countries on the mainland South American continent that drive on the left, although many vehicles are left hand drive as well as right hand drive. One explanation for this practice is that at the time of its colonization of Suriname, the Netherlands itself used left-hand traffic, also introducing the practice in the Dutch East Indies, now Indonesia. Another is that Suriname was first colonized by the British, and for practical reasons, this was not changed when it came under Dutch administration. Although the Netherlands converted to driving to the right at the end of the 18th century, Suriname did not.

Airlines with departures from Suriname:

Airlines with arrivals in Suriname:

Other national companies with an air operator certification:

Education in Suriname is compulsory until the age of 12, and the nation had a net primary enrollment rate of 94% in 2004. Literacy is very common, particularly among men. The main university in the country is the Anton de Kom University of Suriname.

From elementary school to high school there are 13 grades. The elementary school has six grades, middle school four grades and high school three grades. Students take a test in the end of elementary school to determine whether they will go to the MULO (secondary modern school) or a middle school of lower standards like LBO. Students from the elementary school wear a green shirt with jeans, while middle school students wear a blue shirt with jeans.

Students going from the second grade of middle school to the third grade have to choose between the business or science courses. This will determine what their major subjects will be. In order to go on to study math and physics, the student must have a total of 12 points. If the student has fewer points, he/she will go into the business courses or fail the grade.

Due to the variety of habitats and temperatures, biodiversity in Suriname is considered high. In October 2013, 16 international scientists researching the ecosystems during a three-week expedition in Suriname's Upper Palumeu River Watershed catalogued 1,378 species and found 60—including six frogs, one snake, and 11 fish—that may be previously unknown species. According to the environmental non-profit Conservation International, which funded the expedition, Suriname's ample supply of fresh water is vital to the biodiversity and healthy ecosystems of the region.

Snakewood ("Brosimum guianense"), a shrub-like tree, is native to this tropical region of the Americas. Customs in Suriname report that snakewood is often illegally exported to French Guiana, thought to be for the crafts industry.

On 21 March 2013, Suriname's REDD+ Readiness Preparation Proposal (R-PP 2013) was approved by the member countries of the Participants Committee of the Forest Carbon Partnership Facility (FCPF).

As in other parts of Central and South America, indigenous communities have increased their activism to protect their lands and preserve habitat. In March 2015, the "Trio and Wayana communities presented a declaration of cooperation to the National Assembly of Suriname that announces an indigenous conservation corridor spanning 72,000 square kilometers (27,799 square miles) of southern Suriname. The declaration, led by these indigenous communities and with the support of Conservation International (CI) and World Wildlife Fund (WWF) Guianas, comprises almost half of the total area of Suriname." This area includes large forests and is considered "essential for the country's climate resilience, freshwater security, and green development strategy."

Traditionally, "De Ware Tijd" was the major newspaper of the country, but since the '90s "Times of Suriname, De West" and "Dagblad Suriname" have also been well-read newspapers; all publish primarily in Dutch.

Suriname has twenty-four radio stations, most of them also broadcast through the Internet. There are twelve television sources:
ABC (Ch. 4-1, 2), RBN (Ch. 5-1, 2), Rasonic TV (Ch. 7), STVS (Ch. 8–1, 2, 3, 4, 5, 6), Apintie (Ch. 10–1), ATV (Ch. 12–1, 2, 3, 4), Radika (Ch. 14), SCCN (Ch. 17–1, 2, 3), Pipel TV (Ch. 18–1, 2), Trishul (Ch. 20–1, 2, 3, 4), Garuda (Ch. 23–1, 2, 3), Sangeetmala (Ch. 26), Ch. 30, Ch. 31, Ch.32, Ch.38, SCTV (Ch. 45). Also listened to is mArt, a broadcaster from Amsterdam founded by people from Suriname. Kondreman is one of the popular cartoons in Suriname.

There are also two major news sites: Starnieuws and Suriname Herald.

In 2012, Suriname was ranked joint 22nd with Japan in the worldwide Press Freedom Index by the organization Reporters Without Borders. This was ahead of the US (47th), the UK (28th), and France (38th).

Most tourists visit Suriname for the biodiversity of the Amazonian rain forests in the south of the country, which are noted for their flora and fauna. The Central Suriname Nature Reserve is the biggest and one of the most popular reserves, along with the Brownsberg Nature Park which overlooks the Brokopondo Reservoir, one of the largest man-made lakes in the world. Tonka Island in the reservoir is home to a rustic eco-tourism project run by the Saramaccaner Maroons. Pangi wraps and bowls made of calabashes are the two main products manufactured for tourists. The Maroons have learned that colorful and ornate pangis are popular with tourists. Other popular decorative souvenirs are hand-carved purple-hardwood made into bowls, plates, canes, wooden boxes, and wall decors.

There are also many waterfalls throughout the country. Raleighvallen, or Raleigh Falls, is a nature reserve on the Coppename River, rich in bird life. Also are the Blanche Marie Falls on the Nickerie River and the Wonotobo Falls. Tafelberg Mountain in the centre of the country is surrounded by its own reserve – the Tafelberg Nature Reserve – around the source of the Saramacca River, as is the Voltzberg Nature Reserve further north on the Coppename River at Raleighvallen. In the interior are many Maroon and Amerindian villages, many of which have their own reserves that are generally open to visitors.

Suriname is one of the few countries in the world where at least one of each biome that the state possesses has been declared a wildlife reserve. Around 30% of the total land area of Suriname is protected by law as reserves.

Other attractions include plantations such as Laarwijk, which is situated along the Suriname River. This plantation can be reached only by boat via Domburg, in the north central Wanica District of Suriname.

Crime rates continue to rise in Paramaribo and armed robberies are not uncommon. According to the current U.S. Department of State Travel Advisory at the date of the 2018 report's publication, Suriname has been assessed as Level 1: exercise normal precautions.

The Jules Wijdenbosch Bridge is a bridge over the river Suriname between Paramaribo and Meerzorg in the Commewijne district. The bridge was built during the tenure of President Jules Albert Wijdenbosch (1996–2000) and was completed in 2000. The bridge is high, and long. It connects Paramaribo with Commewijne, a connection which previously could only be made by ferry. The purpose of the bridge was to facilitate and promote the development of the eastern part of Suriname. The bridge consists of two lanes (one lane each way) and is not accessible to pedestrians.

The construction of the Sts. Peter and Paul Cathedral started on 13 January 1883. Before it became a cathedral it was a theatre. The theatre was built in 1809 and burned down in 1820.

Suriname is one of the few countries in the world where a synagogue is located next to a mosque.
The two buildings are located next to each other in the centre of Paramaribo and have been known to share a parking facility during their respective religious rites, should they happen to coincide with one another.

A relatively new landmark is the Hindu Arya Dewaker temple in the Johan Adolf Pengelstraat in Wanica, Paramaribo, which was inaugurated in 2001. A special characteristic of the temple is that it does not have images of the Hindu divinities, as they are forbidden in the Arya Samaj, the Hindu movement to which the people who built the temple belong. Instead, the building is covered by many texts derived from the Vedas and other Hindu scriptures. The beautiful architecture makes the temple a tourist attraction.




</doc>
<doc id="26829" url="https://en.wikipedia.org/wiki?curid=26829" title="Category of sets">
Category of sets

In the mathematical field of category theory, the category of sets, denoted as Set, is the category whose objects are sets. The arrows or morphisms between sets "A" and "B" are the total functions from "A" to "B", and the composition of morphisms is the composition of functions.

Many other categories (such as the category of groups, with group homomorphisms as arrows) add structure to the objects of the category of sets and/or restrict the arrows to functions of a particular kind.

The axioms of a category are satisfied by Set because composition of functions is associative, and because every set "X" has an identity function "id : X → X" which serves as identity element for function composition.

The epimorphisms in Set are the surjective maps, the monomorphisms are the injective maps, and the isomorphisms are the bijective maps.

The empty set serves as the initial object in Set with empty functions as morphisms. Every singleton is a terminal object, with the functions mapping all elements of the source sets to the single target element as morphisms. There are thus no zero objects in Set. 

The category Set is complete and co-complete. The product in this category is given by the cartesian product of sets. The coproduct is given by the disjoint union: given sets "A" where "i" ranges over some index set "I", we construct the coproduct as the union of "A"×{"i"} (the cartesian product with "i" serves to ensure that all the components stay disjoint).

Set is the prototype of a concrete category; other categories are concrete if they are "built on" Set in some well-defined way.

Every two-element set serves as a subobject classifier in Set. The power object of a set "A" is given by its power set, and the exponential object of the sets "A" and "B" is given by the set of all functions from "A" to "B". Set is thus a topos (and in particular cartesian closed and exact in the sense of Barr).

Set is not abelian, additive nor preadditive.

Every non-empty set is an injective object in Set. Every set is a projective object in Set (assuming the axiom of choice).

The finitely presentable objects in Set are the finite sets. Since every set is a direct limit of its finite subsets, the category Set is a locally finitely presentable category.

If "C" is an arbitrary category, the contravariant functors from "C" to Set are often an important object of study. If "A" is an object of "C", then the functor from "C" to Set that sends "X" to Hom("X","A") (the set of morphisms in "C" from "X" to "A") is an example of such a functor. If "C" is a small category (i.e. the collection of its objects forms a set), then the contravariant functors from "C" to Set, together with natural transformations as morphisms, form a new category, a functor category known as the category of presheaves on "C".

In Zermelo–Fraenkel set theory the collection of all sets is not a set; this follows from the axiom of foundation. One refers to collections that are not sets as proper classes. One cannot handle proper classes as one handles sets; in particular, one cannot write that those proper classes belong to a collection (either a set or a proper class). This is a problem because it means that the category of sets cannot be formalized straightforwardly in this setting. Categories like Set whose collection of objects forms a proper class are known as large categories, to distinguish them from the small categories whose objects form a set. 

One way to resolve the problem is to work in a system that gives formal status to proper classes, such as NBG set theory. In this setting, categories formed from sets are said to be "small" and those (like Set) that are formed from proper classes are said to be "large".

Another solution is to assume the existence of Grothendieck universes. Roughly speaking, a Grothendieck universe is a set which is itself a model of ZF(C) (for instance if a set belongs to a universe, its elements and its powerset will belong to the universe). The existence of Grothendieck universes (other than the empty set and the set formula_1 of all hereditarily finite sets) is not implied by the usual ZF axioms; it is an additional, independent axiom, roughly equivalent to the existence of strongly inaccessible cardinals. Assuming this extra axiom, one can limit the objects of Set to the elements of a particular universe. (There is no "set of all sets" within the model, but one can still reason about the class "U" of all inner sets, i.e., elements of "U".)

In one variation of this scheme, the class of sets is the union of the entire tower of Grothendieck universes. (This is necessarily a proper class, but each Grothendieck universe is a set because it is an element of some larger Grothendieck universe.) However, one does not work directly with the "category of all sets". Instead, theorems are expressed in terms of the category Set whose objects are the elements of a sufficiently large Grothendieck universe "U", and are then shown not to depend on the particular choice of "U". As a foundation for category theory, this approach is well matched to a system like Tarski–Grothendieck set theory in which one cannot reason directly about proper classes; its principal disadvantage is that a theorem can be true of all Set but not of Set.

Various other solutions, and variations on the above, have been proposed.

The same issues arise with other concrete categories, such as the category of groups or the category of topological spaces.




</doc>
<doc id="26830" url="https://en.wikipedia.org/wiki?curid=26830" title="Slovakia">
Slovakia

Slovakia (; ), officially the Slovak Republic (, ), is a landlocked country in the eastern part of Central Europe. It is bordered by Poland to the north, Ukraine to the east, Hungary to the south, Austria to the southwest, and the Czech Republic to the northwest. Slovakia's territory spans about and is mostly mountainous. The population is over 5.6 million and consists mostly of Slovaks. The capital and largest city is Bratislava, and the second-largest city is Košice. The official language is Slovak.

The Slavs arrived in the territory of present-day Slovakia in the 5th and 6th centuries. In the 7th century they played a significant role in the creation of Samo's Empire and in the 9th century established the Principality of Nitra, which was later conquered by the Principality of Moravia to establish Great Moravia. In the 10th century, after the dissolution of Great Moravia, the territory was integrated into the Principality of Hungary, which would become the Kingdom of Hungary in 1000. In 1241 and 1242, much of the territory was destroyed by the Mongols during their invasion of Central and Eastern Europe. The area was recovered largely thanks to Béla IV of Hungary who also settled Germans who became an important ethnic group in the area, especially in what are today parts of central and eastern Slovakia. After World War I and the dissolution of the Austro-Hungarian Empire, the Czechoslovak National Council established Czechoslovakia (1918–1939). A separate (First) Slovak Republic (1939–1945) existed during World War II as a totalitarian, clero-fascist one-party client state of Nazi Germany. At the end of World War II, Czechoslovakia was re-established as an independent country. After a "coup" in 1948 Czechoslovakia became a totalitarian one-party socialist state under a communist administration, during which the country was part of the Soviet led Eastern Bloc. Attempts to liberalize communism in Czechoslovakia culminated in the Prague Spring, which was crushed by the Warsaw Pact invasion of Czechoslovakia in August 1968. In 1989, the Velvet Revolution ended the Communist rule in Czechoslovakia peacefully. Slovakia became an independent state on 1 January 1993 after the peaceful dissolution of Czechoslovakia, sometimes known as the Velvet Divorce.

Slovakia is a high-income advanced economy with a very high Human Development Index, a very high standard of living and performs favourably in measurements of civil liberties, press freedom, internet freedom, democratic governance and peacefulness. The country maintains a combination of a market economy with a comprehensive social security system. Citizens of Slovakia are provided with universal health care, free education and one of the longest paid parental leaves in the OECD. The country joined the European Union on 1 May 2004 and joined the Eurozone on 1 January 2009. Slovakia is also a member of the Schengen Area, NATO, the United Nations, the OECD, the WTO, CERN, the OSCE, the Council of Europe and the Visegrád Group. As part of Eurozone, Slovak legal tender is the euro, the world's 2nd-most-traded currency. Slovakia is the world's largest per-capita car producer with a total of 1,090,000 cars manufactured in the country in 2018 alone and the 6th largest car producer in the European Union, representing 43% of Slovakia's total industrial output.

The first written mention of name "Slovakia" is in 1586 (). It derives from the Czech word "Slováky"; previous German forms were "Windischen landen" and "Windenland" (the 15th century). The native name "Slovensko" (1791) derives from an older name of Slovaks "Sloven" what may indicate its origin before the 15th century. The original meaning was geographic (not political), since Slovakia was a part of the multiethnic Kingdom of Hungary and did not form a separate administrative unit in this period.

Radiocarbon dating puts the oldest surviving human artefacts from Slovakia—found near Nové Mesto nad Váhom—at 270,000 BCE, in the Early Paleolithic era. These ancient tools, made by the Clactonian technique, bear witness to the ancient habitation of Slovakia.

Other stone tools from the Middle Paleolithic era (200,000–80,000 BCE) come from the Prévôt (Prepoštská) cave in Bojnice and from other nearby sites. The most important discovery from that era is a Neanderthal cranium (c. 200,000 BCE), discovered near Gánovce, a village in northern Slovakia.

Archaeologists have found prehistoric human skeletons in the region, as well as numerous objects and vestiges of the Gravettian culture, principally in the river valleys of Nitra, Hron, Ipeľ, Váh and as far as the city of Žilina, and near the foot of the Vihorlat, Inovec, and Tribeč mountains, as well as in the Myjava Mountains. The most well-known finds include the oldest female statue made of mammoth bone (22,800 BCE), the famous Venus of Moravany. The statue was found in the 1940s in Moravany nad Váhom near Piešťany. Numerous necklaces made of shells from Cypraca thermophile gastropods of the Tertiary period have come from the sites of Zákovská, Podkovice, Hubina, and Radošina. These findings provide the most ancient evidence of commercial exchanges carried out between the Mediterranean and Central Europe.

During the Bronze Age, the geographical territory of modern-day Slovakia went through three stages of development, stretching from 2000 to 800 BCE. Major cultural, economic, and political development can be attributed to the significant growth in production of copper, especially in central Slovakia (for example in Špania Dolina) and northwest Slovakia. Copper became a stable source of prosperity for the local population.

After the disappearance of the Čakany and Velatice cultures, the Lusatian people expanded building of strong and complex fortifications, with the large permanent buildings and administrative centres. Excavations of Lusatian hill forts document the substantial development of trade and agriculture at that period. The richness and the diversity of tombs increased considerably. The inhabitants of the area manufactured arms, shields, jewellery, dishes, and statues.

The arrival of tribes from Thrace disrupted the people of the Kalenderberg culture, who lived in the hamlets located on the plain (Sereď) and in the hill forts like Molpír, near Smolenice, in the Little Carpathians. During Hallstatt times, monumental burial mounds were erected in western Slovakia, with princely equipment consisting of richly decorated vessels, ornaments and decorations. The burial rites consisted entirely of cremation. Common people were buried in flat urnfield cemeteries.

A special role was given to weaving and the production of textiles. The local power of the "Princes" of the Hallstatt period disappeared in Slovakia during the century before the middle of first millennium BC, after strife between the Scytho-Thracian people and locals, resulting in abandonment of the old hill-forts. Relatively depopulated areas soon caught the interest of emerging Celtic tribes, who advanced from the south towards the north, following the Slovak rivers, peacefully integrating into the remnants of the local population.

From around 500 BCE, the territory of modern-day Slovakia was settled by Celts, who built powerful "oppida" on the sites of modern-day Bratislava and Devín. Biatecs, silver coins with inscriptions in the Latin alphabet, represent the first known use of writing in Slovakia. At the northern regions, remnants of the local population of Lusatian origin, together with Celtic and later Dacian influence, gave rise to the unique Púchov culture, with advanced crafts and iron-working, many hill-forts and fortified settlements of central type with coinage of the "Velkobysterecky" type (no inscriptions, with a horse on one side and a head on the other). This culture is often connected with the Celtic tribe mentioned in Roman sources as Cotini.

From 2 AD, the expanding Roman Empire established and maintained a series of outposts around and just south of the Danube, the largest of which were known as Carnuntum (whose remains are on the main road halfway between Vienna and Bratislava) and Brigetio (present-day Szőny at the Slovak-Hungarian border). Such Roman border settlements were built on the present area of Rusovce, currently a suburb of Bratislava. The military fort was surrounded by a civilian vicus and several farms of the villa rustica type. The name of this settlement was Gerulata. The military fort had an auxiliary cavalry unit, approximately 300 horses strong, modelled after the Cananefates. The remains of Roman buildings have also survived in Devín Castle (present-day downtown Bratislava), the suburbs of Dúbravka and Stupava, and Bratislava Castle Hill.

Near the northernmost line of the Roman hinterlands, the Limes Romanus, there existed the winter camp of Laugaricio (modern-day Trenčín) where the Auxiliary of Legion II fought and prevailed in a decisive battle over the Germanic Quadi tribe in 179 CE during the Marcomannic Wars. The Kingdom of Vannius, a kingdom founded by the Germanic Suebian tribes of Quadi and Marcomanni, as well as several small Germanic and Celtic tribes, including the Osi and Cotini, existed in western and central Slovakia from 8–6 BCE to 179 CE.

In the 2nd and 3rd centuries AD, the Huns began to leave the Central Asian steppes. They crossed the Danube in 377 AD and occupied Pannonia, which they used for 75 years as their base for launching looting-raids into Western Europe. However, Attila's death in 453 brought about the disappearance of the Hun tribe. In 568, a Turko-Mongol tribal confederacy, the Avars, conducted its own invasion into the Middle Danube region. The Avars occupied the lowlands of the Pannonian Plain, and established an empire dominating the Carpathian Basin.

In 623, the Slavic population living in the western parts of Pannonia seceded from their empire after a revolution led by Samo, a Frankish merchant. After 626, the Avar power started a gradual decline but its reign lasted to 804.

The Slavic tribes settled in the territory of present-day Slovakia in the 5th century. Western Slovakia was the centre of Samo's empire in the 7th century. A Slavic state known as the Principality of Nitra arose in the 8th century and its ruler Pribina had the first known Christian church of the territory of present-day Slovakia consecrated by 828. Together with neighbouring Moravia, the principality formed the core of the Great Moravian Empire from 833. The high point of this Slavonic empire came with the arrival of Saints Cyril and Methodius in 863, during the reign of Duke Rastislav, and the territorial expansion under Duke Svätopluk I.

Great Moravia arose around 830 when Mojmír I unified the Slavic tribes settled north of the Danube and extended the Moravian supremacy over them. When Mojmír I endeavoured to secede from the supremacy of the king of East Francia in 846, King Louis the German deposed him and assisted Mojmír's nephew Rastislav (846–870) in acquiring the throne. The new monarch pursued an independent policy: after stopping a Frankish attack in 855, he also sought to weaken influence of Frankish priests preaching in his realm. Duke Rastislav asked the Byzantine Emperor Michael III to send teachers who would interpret Christianity in the Slavic vernacular.

Upon Rastislav's request, two brothers, Byzantine officials and missionaries Saints Cyril and Methodius came in 863. Cyril developed the first Slavic alphabet and translated the Gospel into the Old Church Slavonic language. Rastislav was also preoccupied with the security and administration of his state. Numerous fortified castles built throughout the country are dated to his reign and some of them (e.g., "Dowina", sometimes identified with Devín Castle) are also mentioned in connection with Rastislav by Frankish chronicles.
During Rastislav's reign, the Principality of Nitra was given to his nephew Svätopluk as an appanage. The rebellious prince allied himself with the Franks and overthrew his uncle in 870. Similarly to his predecessor, Svätopluk I (871–894) assumed the title of the king ("rex"). During his reign, the Great Moravian Empire reached its greatest territorial extent, when not only present-day Moravia and Slovakia but also present-day northern and central Hungary, Lower Austria, Bohemia, Silesia, Lusatia, southern Poland and northern Serbia belonged to the empire, but the exact borders of his domains are still disputed by modern authors. Svatopluk also withstood attacks of the Magyar tribes and the Bulgarian Empire, although sometimes it was he who hired the Magyars when waging war against East Francia.

In 880, Pope John VIII set up an independent ecclesiastical province in Great Moravia with Archbishop Methodius as its head. He also named the German cleric Wiching the Bishop of Nitra.
After the death of Prince Svatopluk in 894, his sons Mojmír II (894–906?) and Svatopluk II succeeded him as the Prince of Great Moravia and the Prince of Nitra respectively. However, they started to quarrel for domination of the whole empire. Weakened by an internal conflict as well as by constant warfare with Eastern Francia, Great Moravia lost most of its peripheral territories.

In the meantime, the semi-nomadic Magyar tribes, possibly having suffered defeat from the similarly nomadic Pechenegs, left their territories east of the Carpathian Mountains, invaded the Carpathian Basin and started to occupy the territory gradually around 896. Their armies' advance may have been promoted by continuous wars among the countries of the region whose rulers still hired them occasionally to intervene in their struggles.

It is not known what happened with both Mojmír II and Svatopluk II because they are not mentioned in written sources after 906. In three battles (4–5 July and 9 August 907) near Bratislava, the Magyars routed Bavarian armies. Some historians put this year as the date of the break-up of the Great Moravian Empire, due to the Hungarian conquest; other historians take the date a little bit earlier (to 902).

Great Moravia left behind a lasting legacy in Central and Eastern Europe. The Glagolitic script and its successor Cyrillic were disseminated to other Slavic countries, charting a new path in their sociocultural development. The administrative system of Great Moravia may have influenced the development of the administration of the Kingdom of Hungary.

Following the disintegration of the Great Moravian Empire at the turn of the 10th century, the Hungarians annexed the territory comprising modern Slovakia. After their defeat on the Lech River they abandoned their nomadic ways; they settled in the centre of the Carpathian valley, adopted Christianity and began to build a new state—the Hungarian kingdom.

From the 11th century, when the territory inhabited by the Slavic-speaking population of Danubian Basin was incorporated into the Kingdom of Hungary, until 1918, when the Austro-Hungarian empire collapsed, the territory of modern Slovakia was an integral part of the Hungarian state. The ethnic composition became more diverse with the arrival of the Carpathian Germans in the 13th century, and the Jews in the 14th century.

A significant decline in the population resulted from the invasion of the Mongols in 1241 and the subsequent famine. However, in medieval times the area of the present-day Slovakia was characterised by German and Jewish immigration, burgeoning towns, construction of numerous stone castles, and the cultivation of the arts. In 1465, King Matthias Corvinus founded the Hungarian Kingdom's third university, in Pressburg (Bratislava, Pozsony), but it was closed in 1490 after his death. Hussites also settled in the region after the Hussite Wars.

Owing to the Ottoman Empire's expansion into Hungarian territory, Bratislava was designated the new capital of Hungary in 1536, ahead of the old Hungarian capital of Buda falling in 1541. It became part of the Austrian Habsburg monarchy, marking the beginning of a new era. The territory comprising modern Slovakia, then known as Upper Hungary, became the place of settlement for nearly two-thirds of the Magyar nobility fleeing the Turks and far more linguistically and culturally Hungarian than it was before. Partly thanks to old Hussite families, and Slovaks studying under Martin Luther, the region then experienced a growth in Protestantism. For a short period in the 17th century, most Slovaks were Lutherans. They defied the Catholic Habsburgs and sought protection from neighboring Transylvania, a rival continuation of the Magyar state that practiced religious tolerance and normally had Ottoman backing. Upper Hungary, modern Slovakia, became the site of frequent wars between Catholics in the west territory and Protestants in the east, also against Turks, the frontier was on a constant state of military alert and heavily fortified by castles and citadels often manned by Catholic German and Slovak troops on the Habsburg side. By 1648, Slovakia was not spared the Counter-Reformation, which brought the majority of its population from Lutheranism back to Roman Catholicism. In 1655, the printing press at the Trnava university produced the Jesuit Benedikt Szöllősi's Cantus Catholici, a Catholic hymnal in the Slovak language that reaffirmed links to the earlier works of Cyril and Methodius.

The Ottoman wars, rivalry between Austria and Transylvania, and the frequent insurrections against the Habsburg Monarchy inflicted a great deal of devastation, especially in the rural areas. In the Austro-Turkish War (1663–1664) a Turkish army led by the Grand Vizier decimated Slovakia. Even so, Thököly's kuruc rebels from the Principality of Upper Hungary fought alongside the Turks against the Austrians and Poles at the Battle of Vienna of 1683 led by John III Sobieski. As the Turks withdrew from Hungary in the late 17th century, the importance of the territory comprising modern Slovakia decreased, although Pressburg retained its status as the capital of Hungary until 1848, when it was transferred back to Buda.

During the revolution of 1848–49, the Slovaks supported the Austrian Emperor, hoping for independence from the Hungarian part of the Dual Monarchy, but they failed to achieve their aim. Thereafter relations between the nationalities deteriorated (see Magyarization), culminating in the secession of Slovakia from Hungary after World War I.

In 1918, Slovakia and the regions of Bohemia, Moravia, Czech Silesia and Carpathian Ruthenia proclaimed a common state, Czechoslovakia. In 1919, during the chaos following the break-up of Austria-Hungary, Czechoslovakia was formed with numerous Germans and Hungarians within the newly set borders. The borders were set by the Treaty of Saint Germain and Treaty of Trianon. In the peace following the World War, Czechoslovakia emerged as a sovereign European state. It provided what were at the time rather extensive rights to its minorities and remained the only democracy in this part of Europe in the interwar period.

During the Interwar period, democratic Czechoslovakia was allied with France, and also with Romania and Yugoslavia (Little Entente); however, the Locarno Treaties of 1925 left East European security open. Both Czechs and Slovaks enjoyed a period of relative prosperity. There was progress in not only the development of the country's economy, but also culture and educational opportunities. The minority Germans came to accept their role in the new country and relations with Austria were good. Yet the Great Depression caused a sharp economic downturn, followed by political disruption and insecurity in Europe.

Thereafter Czechoslovakia came under continuous pressure from the revisionist governments of Germany and Hungary. Eventually this led to the Munich Agreement of September 1938, which allowed Nazi Germany to partially dismember the country by occupying what was called the Sudetenland, a region with a German-speaking majority and bordering Germany and Austria. The remainder of "rump" Czechoslovakia was renamed Czecho-Slovakia and included a greater degree of Slovak political autonomy. Southern and eastern Slovakia, however, was reclaimed by Hungary at the First Vienna Award of November 1938.

After the Munich Agreement and its Vienna Award, Nazi Germany threatened to annex part of Slovakia and allow the remaining regions to be partitioned by Hungary or Poland unless independence was declared. Thus, Slovakia seceded from Czecho-Slovakia in March 1939 and allied itself, as demanded by Germany, with Hitler's coalition. Secession had created the first Slovak state in history. The government of the First Slovak Republic, led by Jozef Tiso and Vojtech Tuka, was strongly influenced by Germany and gradually became a puppet regime in many respects.

Meanwhile, the Czechoslovak government-in-exile sought to reverse the Munich Agreement and the subsequent German occupation of Czechoslovakia, and to return the Republic to its 1937 boundaries. The government operated from London and it was ultimately considered, by those countries that recognised it, the legitimate government for Czechoslovakia throughout the Second World War.
As part of the Holocaust in Slovakia, 75,000 Jews out of 80,000 who remained on Slovak territory after Hungary had seized southern regions were deported and taken to German death camps. Thousands of Jews, Gypsies and other politically undesirable people remained in Slovak forced labor camps in Sereď, Vyhne, and Nováky. Tiso, through the granting of presidential exceptions, allowed between 1,000 and 4,000 people crucial to the war economy to avoid deportations.
Under Tiso's government and Hungarian occupation, the vast majority of Slovakia's pre-war Jewish population (between 75,000–105,000 individuals including those who perished from the occupied territory) were murdered. The Slovak state paid Germany 500 RM per every deported Jew for "retraining and accommodation" (a similar but smaller payment of 30 RM was paid by Croatia).

After it became clear that the Soviet Red Army was going to push the Nazis out of eastern and central Europe, an anti-Nazi resistance movement launched a fierce armed insurrection, known as the Slovak National Uprising, near the end of summer 1944. A bloody German occupation and a guerilla war followed. Germans and their local collaborators completely destroyed 93 villages and massacred thousands of civilians, often hundreds at a time. The territory of Slovakia was liberated by Soviet and Romanian forces by the end of April 1945.

After World War II, Czechoslovakia was reconstituted and Jozef Tiso was executed in 1947 for collaboration with the Nazis. More than 80,000 Hungarians and 32,000 Germans were forced to leave Slovakia, in a series of population transfers initiated by the Allies at the Potsdam Conference. Out of about 130,000 Carpathian Germans in Slovakia in 1938, by 1947 only some 20,000 remained.

As a result of the Yalta Conference, Czechoslovakia came under the influence and later under direct occupation of the Soviet Union and its Warsaw Pact, after a coup in 1948. Eight thousand two hundred and forty people went to forced labour camps in 1948–1953.

The country was invaded by the Warsaw Pact forces (People's Republic of Bulgaria, People's Republic of Hungary, People's Republic of Poland, and Soviet Union, with the exception of Socialist Republic of Romania and People's Socialist Republic of Albania) in 1968, ending a period of liberalisation under the leadership of Alexander Dubček. 137 Czechoslovakian civilians were killed and 500 seriously wounded during the occupation. In 1969 Czechoslovakia became a federation of the Czech Socialist Republic and the Slovak Socialist Republic. Czechoslovakia became a puppet state of the Soviet Union. Czechoslovak Socialist Republic was never part of the Soviet Union and remained independent to a degree.

Borders with the West were protected by the Iron Curtain. About 600 people, men, women, and children, were killed on the Czechoslovak border with Austria and West Germany between 1948 and 1989.

The end of Communist rule in Czechoslovakia in 1989, during the peaceful Velvet Revolution, was followed once again by the country's dissolution, this time into two successor states. The word "socialist" was dropped in the names of the two republics, with the Slovak Socialist Republic renamed as Slovak Republic. On 17 July 1992, Slovakia, led by Prime Minister Vladimír Mečiar, declared itself a sovereign state, meaning that its laws took precedence over those of the federal government. Throughout the autumn of 1992, Mečiar and Czech Prime Minister Václav Klaus negotiated the details for disbanding the federation. In November, the federal parliament voted to dissolve the country officially on 31 December 1992.

The Slovak Republic and the Czech Republic went their separate ways after 1 January 1993, an event sometimes called the Velvet Divorce. Slovakia has, nevertheless, remained a close partner with the Czech Republic. Both countries co-operate with Hungary and Poland in the Visegrád Group. Slovakia became a member of NATO on 29 March 2004 and of the European Union on 1 May 2004. On 1 January 2009, Slovakia adopted the Euro as its national currency.

Slovakia lies between latitudes 47° and 50° N, and longitudes 16° and 23° E. The Slovak landscape is noted primarily for its mountainous nature, with the Carpathian Mountains extending across most of the northern half of the country. Among these mountain ranges are the high peaks of the Fatra-Tatra Area (including Tatra Mountains, Greater Fatra and Lesser Fatra), Slovak Ore Mountains, Slovak Central Mountains or Beskids. The largest lowland is the fertile Danubian Lowland in the southwest, followed by the Eastern Slovak Lowland in the southeast. Forests cover 41% of Slovak land surface.

The Tatra Mountains, with 29 peaks higher than AMSL, are the highest mountain range in the Carpathian Mountains. The Tatras occupy an area of , of which the greater part lies in Slovakia. They are divided into several parts.

To the north, close to the Polish border, are the High Tatras which are a popular hiking and skiing destination and home to many scenic lakes and valleys as well as the highest point in Slovakia, the Gerlachovský štít at and the country's highly symbolic mountain Kriváň. To the west are the Western Tatras with their highest peak of Bystrá at and to the east are the Belianske Tatras, smallest by area.

Separated from the Tatras proper by the valley of the Váh river are the Low Tatras, with their highest peak of Ďumbier at .

The Tatra mountain range is represented as one of the three hills on the coat of arms of Slovakia.

There are 9 national parks in Slovakia, covering 6.5% of Slovak land surface.

Slovakia has hundreds of caves and caverns under its mountains, of which 30 are open to the public. Most of the caves have stalagmites rising from the ground and stalactites hanging from above. There are currently five Slovak caves under UNESCO's World Heritage Site status. They are Dobšinská Ice Cave, Domica, Gombasek Cave, Jasovská Cave and Ochtinská Aragonite Cave. Other caves open to the public include Belianska Cave, Demänovská Cave of Liberty, Demänovská Ice Cave or Bystrianska Cave.

Most of the rivers arise in the Slovak mountains. Some only pass through Slovakia, while others make a natural border with surrounding countries (more than ). For example, the Dunajec () to the north, the Danube () to the south or the Morava () to the West. The total length of the rivers on Slovak territory is .

The longest river in Slovakia is the Váh (), the shortest is the Čierna voda. Other important and large rivers are the Myjava, the Nitra (), the Orava, the Hron (), the Hornád (), the Slaná (), the Ipeľ (, forming the border with Hungary), the Bodrog, the Laborec, the Latorica and the Ondava.

The biggest volume of discharge in Slovak rivers is during spring, when the snow melts from the mountains. The only exception is the Danube, whose discharge is the greatest during summer when the snow melts in the Alps. The Danube is the largest river that flows through Slovakia.

The Slovak climate lies between the temperate and continental climate zones with relatively warm summers and cold, cloudy and humid winters. Temperature extremes are between although temperatures below are rare. The weather differs from the mountainous north to the plains in the south.

The warmest region is Bratislava and Southern Slovakia where the temperatures may reach in summer, occasionally to in Hurbanovo. During night, the temperatures drop to . The daily temperatures in winter average in the range of to . During night it may be freezing, but usually not below .

In Slovakia, there are four seasons, each season (spring, summer, autumn and winter) lasts three months. The dry continental air brings in the summer heat and winter frosts. In contrast, oceanic air brings rainfalls and reduces summer temperatures. In the lowlands and valleys there is often fog, especially in winter.

Spring starts with 21 March and is characterised by colder weather with average daily temperature of in the first weeks and about in May and in June. In Slovakia, the weather and climate in the spring is very unstable.

Summer starts on 22 June and is usually characterised by hot weather with daily temperatures exceeding . July is the warmest month with temperatures up to about , especially in regions of southern Slovakia—in the urban area of Komárno, Hurbanovo or Štúrovo. Showers or thunderstorms may occur because of the summer monsoon called Medardova kvapka (Medard drop—40 days of rain). Summer in Northern Slovakia is usually mild with temperatures around (less in the mountains).

Autumn in Slovakia starts on 23 September and is mostly characterised by wet weather and wind, although the first weeks can be very warm and sunny. The average temperature in September is around , in November to . Late September and early October is a dry and sunny time of year (so-called Indian Summer).

Winter starts on 21 December with temperatures around . In December and January it is usually snowing, these are the coldest months of the year. At lower altitudes, snow does not stay the whole winter, it changes into the thaw and frost. Winters are colder in the mountains, where the snow usually lasts until March or April and the night temperatures fall to and colder.

Slovakia signed the Rio Convention on Biological Diversity on 19 May 1993, and became a party to the convention on 25 August 1994. It has subsequently produced a National Biodiversity Strategy and Action Plan, which was received by the convention on 2 November 1998.

The biodiversity of Slovakia comprises animals (such as annellids, arthropods, molluscs, nematodes and vertebrates), fungi (Ascomycota, Basidiomycota, Chytridiomycota, Glomeromycota and Zygomycota), micro-organisms (including Mycetozoa), and plants. The geographical position of Slovakia determines the richness of the diversity of fauna and flora. More than 11,000 plant species have been described throughout its territory, nearly 29,000 animal species and over 1,000 species of protozoa. Endemic biodiversity is also common. 

Slovakia is located in the biome of temperate broadleaf and mixed forests. As the altitude changes, the vegetation associations and animal communities are forming height levels (oak, beech, spruce, scrub pine, alpine meadows and subsoil). Forests cover 44% of the territory of Slovakia. In terms of forest stands, 60% are broadleaf trees and 40% are coniferous trees. The occurrence of animal species is strongly connected to the appropriate types of plant associations and biotopes.

Over 4,000 species of fungi have been recorded from Slovakia. Of these, nearly 1,500 are lichen-forming species. Some of these fungi are undoubtedly endemic, but not enough is known to say how many. Of the lichen-forming species, about 40% have been classified as threatened in some way. About 7% are apparently extinct, 9% endangered, 17% vulnerable, and 7% rare. The conservation status of non-lichen-forming fungi in Slovakia is not well documented, but there is a red list for its larger fungi.

Slovakia is a parliamentary democratic republic with a multi-party system. The last parliamentary elections were held on 5 March 2016 and two rounds of presidential elections took place on 16 and 30 March 2019.

The Slovak head of state and the formal head of the executive is the president (currently Zuzana Čaputová, the first female president), though with very limited powers. The president is elected by direct, popular vote under the two-round system for a five-year term. Most executive power lies with the head of government, the prime minister (currently Peter Pellegrini), who is usually the leader of the winning party, but he or she needs to form a majority coalition in the parliament. The prime minister is appointed by the president. The remainder of the cabinet is appointed by the president on the recommendation of the prime minister.

Slovakia's highest legislative body is the 150-seat unicameral National Council of the Slovak Republic ("Národná rada Slovenskej republiky"). Delegates are elected for a four-year term on the basis of proportional representation.

Slovakia's highest judicial body is the Constitutional Court of Slovakia ("Ústavný súd"), which rules on constitutional issues. The 13 members of this court are appointed by the president from a slate of candidates nominated by parliament.

The Constitution of the Slovak Republic was ratified 1 September 1992, and became effective 1 January 1993. It was amended in September 1998 to allow direct election of the president and again in February 2001 due to EU admission requirements. The civil law system is based on Austro-Hungarian codes. The legal code was modified to comply with the obligations of Organization on Security and Cooperation in Europe (OSCE) and to expunge the Marxist–Leninist legal theory. Slovakia accepts the compulsory International Court of Justice jurisdiction with reservations.

The Ministry of Foreign and European Affairs () is responsible for maintaining the Slovak Republic's external relations and the management of its international diplomatic missions. The ministry's director is Miroslav Lajčák. The ministry oversees Slovakia's affairs with foreign entities, including bilateral relations with individual nations and its representation in international organizations.

Slovakia joined the European Union and NATO in 2004 and the Eurozone in 2009.

Slovakia is a member of the United Nations (since 1993) and participates in its specialized agencies. The country was, on 10 October 2005, elected to a two-year term on the UN Security Council from 2006 to 2007. It is also a member of the Schengen Area, the Council of Europe (CoE), the Organization for Security and Cooperation in Europe (OSCE), the World Trade Organization (WTO), the Organisation for Economic Co-operation and Development (OECD), the European Organization for Nuclear Research (CERN) and part of the Visegrád Four (V4: Slovakia, Hungary, the Czech Republic, and Poland).

In 2019, Slovak citizens had visa-free or visa-on-arrival access to 181 countries and territories, ranking the Slovak passport 9th in the world.
Slovakia maintains diplomatic relations with 134 countries, primarily through its Ministry of Foreign Affairs. As of December 2013, Slovakia maintained 90 missions abroad, including 64 embassies, seven missions to multilateral organisations, nine consulates-general, one consular office, one Slovak Economic and Cultural Office and eight Slovak Institutes. There are 44 embassies and 35 honorary consulates in Bratislava.

Slovakia and the United States retain strong diplomatic ties and cooperate in the military and law enforcement areas. The U.S. Department of Defense programs have contributed significantly to Slovak military reforms. Hundreds of thousands of Americans have their roots in Slovakia, and many retain strong cultural and familial ties to the Slovak Republic. President Woodrow Wilson and the United States played a major role in the establishment of the original Czechoslovak state on 28 October 1918.

The Armed Forces of the Slovak Republic number 14,000 uniformed personnel. Slovakia joined NATO in March 2004. The country has been an active participant in US- and NATO-led military actions. There is a joint Czech-Slovak peacekeeping force in Kosovo. From 2006 the army transformed into a fully professional organisation and compulsory military service was abolished.

Slovak Ground Forces are made up of two active mechanised infantry brigades. The Air and Air Defence Forces comprise one wing of fighters, one wing of utility helicopters, and one SAM brigade. Training and support forces comprise a National Support Element (Multifunctional Battalion, Transport Battalion, Repair Battalion), a garrison force of the capital city Bratislava, as well as a training battalion, and various logistics and communication and information bases. Miscellaneous forces under the direct command of the General Staff include the 5th Special Forces Regiment.

The US State Department in 2017 reported:

The government generally respected the human rights of its citizens; however, there were problems in some areas. The most significant human rights issues included incidents of interference with privacy; corruption; widespread discrimination against Roma minority; and security force violence against ethnic and racial minorities government actions and rhetoric did little to discourage. The government investigated reports of abuses by members of the security forces and other government institutions, although some observers questioned the thoroughness of these investigations. Some officials engaged in corrupt practices with impunity. Two former ministers were convicted of corruption during the year.

Human rights in Slovakia are guaranteed by the Constitution of Slovakia from the year 1992 and by multiple international laws signed in Slovakia between 1948 and 2006.

According to the European Roma Rights Centre (ERRC), Romani people in Slovakia "endure racism in the job market, housing and education fields and are often subjected to forced evictions, vigilante intimidation, disproportionate levels of police brutality and more subtle forms of discrimination."

Slovakia is divided into 8 "krajov" (singular—"kraj", usually translated as "region"), each of which is named after its principal city. Regions have enjoyed a certain degree of autonomy since 2002. Their self-governing bodies are referred to as Self-governing (or autonomous) Regions (sg. "samosprávny kraj", pl. "samosprávne kraje") or Upper-Tier Territorial Units (sg. "vyšší územný celok", pl. "vyššie územné celky", abbr. VÚC).

The "kraje" are subdivided into many "okresy" (sg. "okres", usually translated as counties). Slovakia currently has 79 counties.

The "okresy" are further divided into "obcí" (sg. "obec", usually translated as "municipality"). There are currently 2,890 municipalities.

In terms of economics and unemployment rate, the western regions are richer than eastern regions. Bratislava is the third-richest region of the European Union by GDP (PPP) per capita (after Hamburg and Luxembourg City); GDP at purchasing power parity is about three times higher than in other Slovak regions.

The Slovak economy is a developed, high-income economy, with the GDP per capita equalling 78% of the average of the European Union in 2018. The country has difficulties addressing regional imbalances in wealth and employment. GDP per capita ranges from 188% of EU average in Bratislava to 54% in Eastern Slovakia. Although regional income inequality is high, 90% of citizens own their homes.

The OECD in 2017 reported:

The Slovak Republic continues exhibiting robust economic performance, with strong growth backed by a sound financial sector, low public debt and high international competitiveness drawing on large inward investment.

In 2018, Slovakia was ranked by the International Monetary Fund as the 37th richest country in the world (out of 187 countries), with purchasing power parity per capita GDP of $35,130. The country used to be dubbed the "Tatra Tiger". Slovakia successfully transformed from a centrally planned economy to a market-driven economy. Major privatisations are completed, the banking sector is almost completely in private hands, and foreign investment has risen.
The Slovak economy is one of the fastest growing economies in Europe and 3rd-fastest in eurozone (2017). In 2007, 2008 and 2010 (with GDP growth of 10.5%, 6% and 4%, retrospectively). In 2016, more than 86% of Slovak exports went to European Union, and more than 50% of Slovak imports came from other European Union member states.

The ratio of government debt to GDP in Slovakia reached 49.4% by the end of 2018, far below the OECD average.

Unemployment, peaking at 19% at the end of 1999, decreased to 4,9% in 2019, lowest recorded rate in Slovak history.

Slovakia adopted the Euro currency on 1 January 2009 as the 16th member of the Eurozone. The euro in Slovakia was approved by the European commission on 7 May 2008. The Slovak koruna was revalued on 28 May 2008 to 30.126 for 1 euro, which was also the exchange rate for the euro.
The Slovak government encourages foreign investment, since it is one of the driving forces of the economy. Slovakia is an attractive country for foreign investors mainly because of its low wages, low tax rates, well educated labour force, favorable geographic location in the heart of Central Europe, strong political stability and good international relations reinforced by the country's accession to the European Union. Some regions, mostly at the east of Slovakia have failed to attract major investment, which has aggravated regional disparities in many economic and social areas. Foreign direct investment inflow grew more than 600% from 2000 and cumulatively reached an all-time high of $17.3 billion in 2006, or around $22,000 per capita by the end of 2008.

Slovakia ranks 45th out of 190 economies in terms of ease of doing business, according to the 2020 World Bank Doing Business Report.

Although Slovakia's GDP comes mainly from the tertiary (services) sector, the industrial sector also plays an important role within its economy. The main industry sectors are car manufacturing and electrical engineering. Since 2007, Slovakia has been the world's largest producer of cars per capita, with a total of 1,090,000 cars manufactured in the country in 2018 alone. 275,000 people are employed directly and indirectly
by the automotive industry. There are currently four automobile assembly plants: Volkswagen's in Bratislava (models: Volkswagen Up, Volkswagen Touareg, Audi Q7, Audi Q8, Porsche Cayenne, Lamborghini Urus), PSA Peugeot Citroën's in Trnava (models: Peugeot 208, Citroën C3 Picasso), Kia Motors' Žilina Plant (models: Kia Cee'd, Kia Sportage, Kia Venga) and Jaguar Land Rover's in Nitra (model: Land Rover Discovery). Hyundai Mobis in Žilina is the largest suppliers for the automotive industry in Slovakia.

From electrical engineering companies, Foxconn has a factory at Nitra for LCD TV manufacturing, Samsung at Galanta for computer monitors and television sets manufacturing. Slovnaft based in Bratislava with 4,000 employees, is an oil refinery with a processing capacity of 5.5 - 6 million tonnes of crude oil, annually. Steel producer U. S. Steel in Košice is the largest employer at the east of Slovakia with 12,000 employees.
ESET is an IT security company from Bratislava with more than 1,000 employees worldwide at present. Their branch offices are in the United States, Ireland, United Kingdom, Argentina, the Czech Republic, Singapore and Poland. In recent years, service and high-tech-oriented businesses have prospered in Bratislava. Many global companies, including IBM, Dell, Lenovo, AT&T, SAP, and Accenture, have built outsourcing and service centres here. Reasons for the influx of multi-national corporations include proximity to Western Europe, skilled labour force and the high density of universities and research facilities. Other large companies and employers with headquarters in Bratislava include Amazon, Slovak Telekom, Orange Slovensko, Slovenská sporiteľňa, Tatra banka, Doprastav, Hewlett-Packard Slovakia, Henkel Slovensko, Slovenský plynárenský priemysel, Microsoft Slovakia, Mondelez Slovakia, Whirlpool Slovakia and Zurich Insurance Group Slovakia.

Bratislava's geographical position in Central Europe has long made Bratislava a crossroads for international trade traffic. Various ancient trade routes, such as the Amber Road and the Danube waterway, have crossed territory of present-day Bratislava. Today, Bratislava is the road, railway, waterway and airway hub.

In 2012, Slovakia produced a total of 28,393 GWh of electricity while at the same time consumed 28 786 GWh. The slightly higher level of consumption than the capacity of production (- 393 GWh) meant the country was not self-sufficient in energy sourcing. Slovakia imported electricity mainly from the Czech Republic (9,961 GWh—73.6% of total import) and exported mainly to Hungary (10,231 GWh—78.2% of total export).

Nuclear energy accounts for 53.8% of total electricity production in Slovakia, followed by 18.1% of thermal power energy, 15.1% by hydro power energy, 2% by solar energy, 9.6% by other sources and the rest 1.4% is imported.

The two nuclear power-plants in Slovakia are in Jaslovské Bohunice and Mochovce, each of them containing two operating reactors. Prior to the accession of Slovakia to the EU in 2004, the government agreed to turn-off the V1 block of Jaslovské Bohunice power-plant, built in 1978. After deactivating the last of the two reactors of the V1 block in 2008, Slovakia stopped being self-dependent in energy production. Currently there is another block (V2) with two active reactors in Jaslovské Bohunice. It is scheduled for decommissioning in 2025. Two new reactors are under construction in Mochovce plant. The nuclear power production in Slovakia occasionally draws the attention of Austrian green-energy activists who organise protests and block the borders between the two countries.

There are four main highways D1 to D4 and eight express ways R1 to R8. Many of them are still under construction.

The D1 motorway connects Bratislava to Trnava, Nitra, Trenčín, Žilina and beyond, while the D2 motorway connects it to Prague, Brno and Budapest in the north-south direction. A large part of D4 motorway (an outer bypass), which should ease the pressure on Bratislava's highway system, is scheduled to open in 2020. The A6 motorway to Vienna connects Slovakia directly to the Austrian motorway system and was opened on 19 November 2007.

Slovakia has four international airports. Bratislava's M. R. Štefánik Airport is the main and largest international airport. It is located northeast of the city centre. It serves civil and governmental, scheduled and unscheduled domestic and international flights. The current runways support the landing of all common types of aircraft currently used. The airport has enjoyed rapidly growing passenger traffic in recent years; it served 279,028 passengers in 2000 and 2,292,712 in 2018. Košice International Airport is an airport serving Košice. It is the second largest international airport in Slovakia. The Poprad–Tatry Airport is the third busiest airport, the airport is located 5 km east—northeast of ski resort town Poprad. It is an airport with one of the highest elevations in Central Europe, at 718 m, which is 150 m higher than Innsbruck Airport in Austria. The Sliač Airport is the smallest international airport and currently operates only summer charter flights to popular sea resort destinations.

Railways of Slovak Republic provides railway transport services on national and international lines.

The Port of Bratislava is one of the two international river ports in Slovakia. The port connects Bratislava to international boat traffic, especially the interconnection from the North Sea to the Black Sea via the Rhine-Main-Danube Canal.
Additionally, tourist boats operate from Bratislava's passenger port, including routes to Devín, Vienna and elsewhere. The Port of Komárno is the second largest port in Slovakia with an area of over 20 hectares and is located approximately 100 km east of Bratislava. It lies at the confluence of two rivers - the Danube and Váh.

Slovakia features natural landscapes, mountains, caves, medieval castles and towns, folk architecture, spas and ski resorts. More than 5,4 million tourists visited Slovakia in 2017, and the most attractive destinations are the capital of Bratislava and the High Tatras. Most visitors come from the Czech Republic (about 26%), Poland (15%) and Germany (11%).

Slovakia contains many castles, most of which are in ruins. The best known castles include Bojnice Castle (often used as a filming location), Spiš Castle, (on the UNESCO list), Orava Castle, Bratislava Castle, and the ruins of Devín Castle. Čachtice Castle was once the home of the world's most prolific female serial killer, the 'Bloody Lady', Elizabeth Báthory.

Slovakia's position in Europe and the country's past (part of the Kingdom of Hungary, the Habsburg monarchy and Czechoslovakia) made many cities and towns similar to the cities in the Czech Republic (such as Prague), Austria (such as Salzburg) or Hungary (such as Budapest). A historical center with at least one square has been preserved in many towns. Large historical centers can be found in Bratislava, Trenčín, Košice, Banská Štiavnica, Levoča, and Trnava. Historical centers have been going through restoration in recent years.

Historical churches can be found in virtually every village and town in Slovakia. Most of them are built in the Baroque style, but there are also many examples of Romanesque and Gothic architecture, for example Banská Bystrica, Bardejov and Spišská Kapitula. The Basilica of St. James in Levoča with the tallest wood-carved altar in the world and the Church of the Holy Spirit in Žehra with medieval frescos are UNESCO World Heritage Sites. The St. Martin's Concathedral in Bratislava served as the coronation church for the Kingdom of Hungary. The oldest sacral buildings in Slovakia stem from the Great Moravian period in the 9th century.
Very precious structures are the complete wooden churches of northern and northern-eastern Slovakia. Most were built from the 15th century onwards by Catholics, Lutherans and members of eastern-rite churches.

Typical souvenirs from Slovakia are dolls dressed in folk costumes, ceramic objects, crystal glass, carved wooden figures, črpáks (wooden pitchers), fujaras (a folk instrument on the UNESCO list) and valaškas (a decorated folk hatchet) and above all products made from corn husks and wire, notably human figures. Souvenirs can be bought in the shops run by the state organisation ÚĽUV ("Ústredie ľudovej umeleckej výroby"—Centre of Folk Art Production). "Dielo" shop chain sells works of Slovak artists and craftsmen. These shops are mostly found in towns and cities.

Prices of imported products are generally the same as in the neighbouring countries, whereas prices of local products and services, especially food, are usually lower.

The Slovak Academy of Sciences has been the most important scientific and research institution in the country since 1953. Slovaks have made notable scientific and technical contributions during the history. Slovakia is currently in the negotiation process of becoming a member of the European Space Agency. Observer status was granted in 2010, when Slovakia signed the General Agreement on Cooperation in which information about ongoing education programmes was shared and Slovakia was invited to various negotiations of the ESA. In 2015, Slovakia signed the European Cooperating State Agreement based on which Slovakia committed to the finance entrance programme named PECS (Plan for the European Cooperating States) which serves as preparation for full membership. Slovak research and development organizations can apply for funding of projects regarding space technologies advancement. Full membership of Slovakia in the ESA is expected in 2020 after signing the ESA Convention. Slovakia will be obliged to set state budget inclusive ESA funding.

The population is over 5.4 million and consists mostly of Slovaks. The average population density is 110 inhabitants per km². According to the 2011 census, the majority of the inhabitants of Slovakia are Slovaks (80.7%). Hungarians are the largest ethnic minority (8.5%). Other ethnic groups include Roma (2%), Czechs (0.6%), Rusyns (0.6%) and others or unspecified (7.6%). Unofficial estimates on the Roma population are much higher, around 5.6%.

In 2018 the median age of the Slovak population was 41 years.

The largest waves of Slovak emigration occurred in the 19th and early 20th centuries. In the 1990 US census, 1.8 million people self-identified as having Slovak ancestry.

The official language is Slovak, a member of the Slavic language family. Hungarian is widely spoken in the southern regions, and Rusyn is used in some parts of the Northeast. Minority languages hold co-official status in the municipalities in which the size of the minority population meets the legal threshold of 15% in two consecutive censuses.

Slovakia is ranked among the top EU countries regarding the knowledge of foreign languages. In 2007, 68% of the population aged from 25 to 64 years claimed to speak two or more foreign languages, finishing 2nd highest in the European Union. The best known foreign language in Slovakia is Czech. Eurostat report also shows that 98.3% of Slovak students in the upper secondary education take on two foreign languages, ranking highly over the average 60.1% in the European Union.

The deaf community uses the Slovak Sign Language. Even though spoken Czech and Slovak are similar, the Slovak Sign language is not particularly close to Czech Sign Language.

The Slovak constitution guarantees freedom of religion. In 2011, 62.0% of Slovaks identified themselves as Roman Catholics, 8.9% as Protestants, 3.8% as Greek Catholics, 0.9% as Orthodox, 13.4% identified themselves as atheists or non-religious, and 10.6% did not answer the question about their belief. In 2004, about one third of the church members regularly attended church services. The Slovak Greek Catholic Church is an Eastern rite sui iuris Catholic Church. Before World War II, an estimated 90,000 Jews lived in Slovakia (1.6% of the population), but most were murdered during the Holocaust. After further reductions due to postwar emigration and assimilation, only about 2,300 Jews remain today (0.04% of the population).

There are 18 state-registered religions in Slovakia, of which 16 are Christian, one is Jewish, and one is Bahá'í. In 2016, a two-third majority of the Slovak parliament passed a new bill that will obstruct Islam and other religious organisations from becoming state-recognised religion by doubling the minimum followers threshold from 25,000 to 50,000; however, Slovak president Andrej Kiska vetoed the bill. In 2010, there were an estimated 5,000 Muslims in Slovakia representing less than 0.1% of the country's population. Slovakia is the last member state of the European Union without a mosque.

The Programme for International Student Assessment, coordinated by the OECD, currently ranks Slovak secondary education the 30th in the world (placing it just below the United States and just above Spain).
Education in Slovakia is compulsory from age 6 to 16. The education system consists of elementary school which is divided into two parts, the first grade (age 6–10) and the second grade (age 10–15) which is finished by taking nationwide testing called Monitor, from Slovak language and math. Parents may apply for social assistance for a child that is studying on an elementary school or a high-school. If approved, the state provides basic study necessities for the child. Schools provide books to all their students with usual exceptions of books for studying a foreign language and books which require taking notes in them, which are mostly present at the first grade of elementary school.

After finishing elementary school, students are obliged to take one year in high school.

After finishing high school, students can go to university and are highly encouraged to do so. Slovakia has a wide range of universities. The biggest university is Comenius University, established in 1919. Although it's not the first university ever established on Slovak territory, it's the oldest university that is still running. Most universities in Slovakia are public funded, where anyone can apply. Every citizen has a right to free education in public schools.

Slovakia has several privately funded universities, however public universities consistently score better in the ranking than their private counterparts. Universities have different criteria for accepting students. Anyone can apply to any number of universities.

Folk tradition has rooted strongly in Slovakia and is reflected in literature, music, dance and architecture. The prime example is a Slovak national anthem, ""Nad Tatrou sa blýska"", which is based on a melody from ""Kopala studienku"" folk song.

Manifestation of Slovak folklore culture is the ""Východná"" Folklore Festival. It is the oldest and largest nationwide festival with international participation, which takes place in Východná annually. Slovakia is usually represented by many groups but mainly by SĽUK ("Slovenský ľudový umelecký kolektív—Slovak folk art collective"). SĽUK is the largest Slovak folk art group, trying to preserve the folklore tradition.

An example of wooden folk architecture in Slovakia can be seen in the well preserved village of Vlkolínec which has been the UNESCO World Heritage Site since 1993. The Prešov Region preserves the world's most remarkable folk wooden churches. Most of them are protected by Slovak law as cultural heritage, but some of them are on the UNESCO list too, in Bodružal, Hervartov, Ladomirová and Ruská Bystrá.

The best known Slovak hero, found in many folk mythologies, is Juraj Jánošík (1688–1713) (the Slovak equivalent of Robin Hood). The legend says he was taking from the rich and giving to the poor. Jánošík's life was depicted in a list of literature works and many movies throughout the 20th century. One of the most popular is a film "Jánošík" directed by Martin Frič in 1935.

Visual art in Slovakia is represented through painting, drawing, printmaking, illustration, arts and crafts, sculpture, photography or conceptual art. The Slovak National Gallery founded in 1948, is the biggest network of galleries in Slovakia. Two displays in Bratislava are situated in Esterházy Palace ("Esterházyho palác") and the Water Barracks ("Vodné kasárne"), adjacent one to another. They are located on the Danube riverfront in the Old Town.

The Bratislava City Gallery, founded in 1961 is the second biggest Slovak gallery of its kind. It stores about 35,000 pieces of Slovak and international art and offers permanent displays in Pálffy Palace and Mirbach Palace, located in the Old Town. Danubiana Art Museum, one of the youngest art museums in Europe, is situated near Čunovo waterworks (part of Gabčíkovo Waterworks). Other major galleries include: Andy Warhol Museum of Modern Art (Warhol's parents were from Miková), East Slovak Gallery, Ernest Zmeták Art Gallery, Zvolen Castle.

For a list of notable Slovak writers and poets, see List of Slovak authors.

Christian topics include: poem Proglas as a foreword to the four Gospels, partial translations of the Bible into Old Church Slavonic, "Zakon sudnyj ljudem".

Medieval literature, in the period from the 11th to the 15th centuries, was written in Latin, Czech and Slovakised Czech. Lyric (prayers, songs and formulas) was still controlled by the Church, while epic was concentrated on legends. Authors from this period include Johannes de Thurocz, author of the Chronica Hungarorum and Maurus, both of them Hungarians. The worldly literature also emerged and chronicles were written in this period.

There were two leading persons who codified the Slovak language. The first was Anton Bernolák whose concept was based on the western Slovak dialect in 1787. It was the codification of the first ever literary language of Slovaks. The second was Ľudovít Štúr, whose formation of the Slovak language took principles from the central Slovak dialect in 1843.

Slovakia is also known for its polyhistors, of whom include Pavol Jozef Šafárik, Matej Bel, Ján Kollár, and its political revolutionaries and reformists, such Milan Rastislav Štefánik and Alexander Dubček.

Traditional Slovak cuisine is based mainly on pork, poultry (chicken is the most widely eaten, followed by duck, goose, and turkey), flour, potatoes, cabbage, and milk products. It is relatively closely related to Hungarian, Czech and Austrian cuisine. On the east it is also influenced by Ukrainian and Polish cuisine. In comparison with other European countries, "game meat" is more accessible in Slovakia due to vast resources of forest and because hunting is relatively popular. Boar, rabbit, and venison are generally available throughout the year. Lamb and goat are eaten but are not widely popular.

The traditional Slovak meals are bryndzové halušky, bryndzové pirohy and other meals with potato dough and bryndza. Bryndza is a salty cheese made of a sheep milk, characterised by a strong taste and aroma. Bryndzové halušky must be on the menu of every traditional Slovak restaurant.

A typical soup is a sauerkraut soup ("kapustnica"). A blood sausage called "krvavnica", made from any and all parts of a butchered pig is also a specific Slovak meal.

Wine is enjoyed throughout Slovakia. Slovak wine comes predominantly from the southern areas along the Danube and its tributaries; the northern half of the country is too cold and mountainous to grow grapevines. Traditionally, white wine was more popular than red or rosé (except in some regions), and sweet wine more popular than dry, but in recent years tastes seem to be changing. Beer (mainly of the pilsener style, though dark lagers are also consumed) is also popular.

Sporting activities are practised widely in Slovakia, many of them on a professional level. Ice hockey and football have traditionally been regarded as the most popular sports in Slovakia, though tennis, handball, basketball, volleyball, whitewater slalom, cycling and athletics are also popular.


One of the most popular team sports in Slovakia is ice hockey. Slovakia became a member of the IIHF on 2 February 1993 and since then has won 4 medals in Ice Hockey World Championships, consisting of 1 gold, 2 silver and 1 bronze. The most recent success was a silver medal at the 2012 IIHF World Championship in Helsinki. The Slovak national hockey team made five appearances in the Olympic games, finishing 4th in the 2010 Winter Olympics in Vancouver. The country has 8,280 registered players and is ranked 7th in the IIHF World Ranking at present. Prior to 2012, the Slovak team HC Slovan Bratislava participated in the Kontinental Hockey League, considered the strongest hockey league in Europe, and the second-best in the world.

Slovakia hosted the 2011 IIHF World Championship, where Finland won the gold medal and 2019 IIHF World Championship, where Finland also won the gold medal. Both competitions took place in Bratislava and Košice.


Association football is the most popular sport in Slovakia, with over 400,000 registered players. Since 1993, the Slovak national football team has qualified for the FIFA World Cup once, in 2010. They progressed to the last 16, where they were defeated by the Netherlands. The most notable result was the 3–2 victory over Italy. In 2016, the Slovak national football team qualified for the UEFA Euro 2016 tournament, under head coach Ján Kozák. This helped the team reach their best ever position of 14th in the FIFA World Rankings.

In club competitions, only three teams have qualified for the UEFA Champions League Group Stage, namely MFK Košice in 1997–98, FC Artmedia Bratislava in 2005–06 season, and MŠK Žilina in 2010–11. FC Artmedia Bratislava has been the most successful team, finishing 3rd at the Group Stage of the UEFA Cup, therefore qualifying for the knockout stage. They remain the only Slovak club that has won a match at the group stage.






</doc>
<doc id="26833" url="https://en.wikipedia.org/wiki?curid=26833" title="Scientific method">
Scientific method

The scientific method is an empirical method of acquiring knowledge that has characterized the development of science since at least the 17th century. It involves careful observation, applying rigorous skepticism about what is observed, given that cognitive assumptions can distort how one interprets the observation. It involves formulating hypotheses, via induction, based on such observations; experimental and measurement-based testing of deductions drawn from the hypotheses; and refinement (or elimination) of the hypotheses based on the experimental findings. These are "principles" of the scientific method, as distinguished from a definitive series of steps applicable to all scientific enterprises.

Though diverse models for the scientific method are available, there is in general a continuous process that includes observations about the natural world. People are naturally inquisitive, so they often come up with questions about things they see or hear, and they often develop ideas or hypotheses about why things are the way they are. The best hypotheses lead to predictions that can be tested in various ways. The most conclusive testing of hypotheses comes from reasoning based on carefully controlled experimental data. Depending on how well additional tests match the predictions, the original hypothesis may require refinement, alteration, expansion or even rejection. If a particular hypothesis becomes very well supported, a general theory may be developed.

Although procedures vary from one field of inquiry to another, they are frequently the same from one to another. The process of the scientific method involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments or empirical observations based on those predictions. A hypothesis is a conjecture, based on knowledge obtained while seeking answers to the question. The hypothesis might be very specific, or it might be broad. Scientists then test hypotheses by conducting experiments or studies. A scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment or observation that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.

The purpose of an experiment is to determine whether observations agree with or conflict with the predictions derived from a hypothesis. Experiments can take place anywhere from a garage to CERN's Large Hadron Collider. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always in the same order.

Important debates in the history of science concern rationalism, especially as advocated by René Descartes; inductivism and/or empiricism, as argued for by Francis Bacon, and rising to particular prominence with Isaac Newton and his followers; and hypothetico-deductivism, which came to the fore in the early 19th century.

The term "scientific method" emerged in the 19th century, when a significant institutional development of science was taking place and terminologies establishing clear boundaries between science and non-science, such as "scientist" and "pseudoscience", appeared. Throughout the 1830s and 1850s, by which time Baconianism was popular, naturalists like William Whewell, John Herschel, John Stuart Mill engaged in debates over "induction" and "facts" and were focused on how to generate knowledge. In the late 19th and early 20th centuries, a debate over realism vs. antirealism was conducted as powerful scientific theories extended beyond the realm of the observable.

The term "scientific method" came into popular use in the twentieth century, popping up in dictionaries and science textbooks, although there was little scientific consensus over its meaning. Although there was a growth through the middle of the twentieth century, by the 1960s and 1970s numerous influential philosophers of science such as Thomas Kuhn and Paul Feyerabend had questioned the universality of the "scientific method" and in doing so largely replaced the notion of science as a homogeneous and universal method with that of it being a heterogeneous and local practice. In particular, Paul Feyerabend, in the 1975 first edition of his book "Against Method", argued against there being any universal rules of science. Later examples include physicist Lee Smolin's 2013 essay "There Is No Scientific Method" and historian of science Daniel Thurs's 2015 book "Newton's Apple and Other Myths about Science", which concluded that the scientific method is a myth or, at best, an idealization. Philosophers Robert Nola and Howard Sankey, in their 2007 book "Theories of Scientific Method", said that debates over scientific method continue, and argued that Feyerabend, despite the title of "Against Method", accepted certain rules of method and attempted to justify those rules with a metamethodology.

The scientific method is the process by which science is carried out. As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time. This model can be seen to underlie the scientific revolution.

The ubiquitous element in the model of the scientific method is empiricism. This is in opposition to stringent forms of rationalism: the scientific method embodies that reason alone cannot solve a particular scientific problem. A strong formulation of the scientific method is not always aligned with a form of empiricism in which the empirical data is put forward in the form of experience or other abstracted forms of knowledge; in current scientific practice, however, the use of scientific modelling and reliance on abstract typologies and theories is normally accepted. The scientific method is of necessity also an expression of an opposition to claims that e.g. revelation, political or religious dogma, appeals to tradition, commonly held beliefs, common sense, or, importantly, currently held theories, are the only possible means of demonstrating truth.

Different early expressions of empiricism and the scientific method can be found throughout history, for instance with the ancient Stoics, Epicurus, Alhazen, Roger Bacon, and William of Ockham. From the 16th century onwards, experiments were advocated by Francis Bacon, and performed by Giambattista della Porta, Johannes Kepler, and Galileo Galilei. There was particular development aided by theoretical works by Francisco Sanches, John Locke, George Berkeley, and David Hume.

The hypothetico-deductive model formulated in the 20th century, is the ideal although it has undergone significant revision since first proposed (for a more formal discussion, see below). Staddon (2017) argues it is a mistake to try following rules which are best learned through careful study of examples of scientific investigation.

The overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, these actions are better considered as general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always done in the same order. As noted by scientist and philosopher William Whewell (1794–1866), "invention, sagacity, [and] genius" are required at every step.

The question can refer to the explanation of a specific observation, as in "Why is the sky blue?" but can also be open-ended, as in "How can I design a drug to cure this particular disease?" This stage frequently involves finding and evaluating evidence from previous experiments, personal scientific observations or assertions, as well as the work of other scientists. If the answer is already known, a different question that builds on the evidence can be posed. When applying the scientific method to research, determining a good question can be very difficult and it will affect the outcome of the investigation.

A hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain any given behavior. The hypothesis might be very specific; for example, Einstein's equivalence principle or Francis Crick's "DNA makes RNA makes protein", or it might be broad; for example, unknown species of life dwell in the unexplored depths of the oceans. A statistical hypothesis is a conjecture about a given statistical population. For example, the population might be "people with a particular disease." The conjecture might be that a new drug will cure the disease in some of those people. Terms commonly associated with statistical hypotheses are null hypothesis and alternative hypothesis. A null hypothesis is the conjecture that the statistical hypothesis is false; for example, that the new drug does nothing and that any cure is caused by chance. Researchers normally want to show that the null hypothesis is false. The alternative hypothesis is the desired outcome, that the drug does better than chance. A final point: a scientific hypothesis must be falsifiable, meaning that one can identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, it cannot be meaningfully tested.

This step involves determining the logical consequences of the hypothesis. One or more predictions are then selected for further testing. The more unlikely that a prediction would be correct simply by coincidence, then the more convincing it would be if the prediction were fulfilled; evidence is also stronger if the answer to the prediction is not already known, due to the effects of hindsight bias (see also postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).

This is an investigation of whether the real world behaves as predicted by the hypothesis. Scientists (and other people) test hypotheses by conducting experiments. The purpose of an experiment is to determine whether observations of the real world agree with or conflict with the predictions derived from a hypothesis. If they agree, confidence in the hypothesis increases; otherwise, it decreases. Agreement does not assure that the hypothesis is true; future experiments may reveal problems. Karl Popper advised scientists to try to falsify hypotheses, i.e., to search for and test those experiments that seem most doubtful. Large numbers of successful confirmations are not convincing if they arise from experiments that avoid risk. Experiments should be designed to minimize possible errors, especially through the use of appropriate scientific controls. For example, tests of medical treatments are commonly run as double-blind tests. Test personnel, who might unwittingly reveal to test subjects which samples are the desired test drugs and which are placebos, are kept ignorant of which are which. Such hints can bias the responses of the test subjects. Furthermore, failure of an experiment does not necessarily mean the hypothesis is false. Experiments always depend on several hypotheses, e.g., that the test equipment is working properly, and a failure may be a failure of one of the auxiliary hypotheses. (See the Duhem–Quine thesis.) Experiments can be conducted in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars (using one of the working rovers), and so on. Astronomers do experiments, searching for planets around distant stars. Finally, most individual experiments address highly specific topics for reasons of practicality. As a result, evidence about broader topics is usually accumulated gradually.

This involves determining what the results of the experiment show and deciding on the next actions to take. The predictions of the hypothesis are compared to those of the null hypothesis, to determine which is better able to explain the data. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test may be required. If the evidence has falsified the hypothesis, a new hypothesis is required; if the experiment supports the hypothesis but the evidence is not strong enough for high confidence, other predictions from the hypothesis must be tested. Once a hypothesis is strongly supported by evidence, a new question can be asked to provide further insight on the same topic. Evidence from other scientists and experience are frequently incorporated at any stage in the process. Depending on the complexity of the experiment, many iterations may be required to gather sufficient evidence to answer a question with confidence, or to build up many answers to highly specific questions in order to answer a single broader question.

The basic elements of the scientific method are illustrated by the following example from the discovery of the structure of DNA:

The discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article.

The scientific method also includes other components required even when all the iterations of the steps above have been completed:

If an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.

The process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer review does not certify correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.

Scientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others. Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain.

Scientific inquiry generally aims to obtain knowledge in the form of testable explanations that scientists can use to
predict the results of future experiments. This allows scientists to gain a better understanding of the topic under study, and later to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it will continue to explain a body of evidence better than its alternatives. The most successful explanations – those which explain and make accurate predictions in a wide range of circumstances – are often called scientific theories.

Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding typically result from a gradual process of development over time, sometimes across different domains of science. Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question proves more powerful than its alternatives at explaining the evidence. Often subsequent researchers re-formulate the explanations over time, or combined explanations to produce new explanations.

Tow sees the scientific method in terms of an evolutionary algorithm applied to science and technology.

Scientific knowledge is closely tied to empirical findings, and can remain subject to falsification if new experimental observations are incompatible with what is found. That is, no theory can ever be considered final, since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory can be argued to relate to how long it has persisted without major alteration to its core principles.

Theories can also become subsumed by other theories. For example, Newton's laws explained thousands of years of scientific observations of the planets . However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicted and explained other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected to each other, unified by principles of increasing explanatory power.

Since new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors. For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world; its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology.

Scientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy.

The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).

A historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touches the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together.

Another important human bias that plays a role is a preference for new, surprising statements (see appeal to novelty), which can result in a search for evidence that the new is true. Poorly attested beliefs can be believed and acted upon via a less rigorous heuristic.

Goldhaber and Nieto published in 2010 the observation that if theoretical structures with "many closely neighboring subjects are described by connecting theoretical concepts then the theoretical structure .. becomes increasingly hard to overturn". When a narrative is constructed its elements become easier to believe. For more on the narrative fallacy, see also : "Words and ideas are originally phonetic and mental equivalences of the experiences coinciding with them. ... Such proto-ideas are at first always too broad and insufficiently specialized. ... Once a structurally complete and closed system of opinions consisting of many details and relations has been formed, it offers enduring resistance to anything that contradicts it." Sometimes, these have their elements assumed "a priori", or contain some other logical or methodological flaw in the process that ultimately produced them. Donald M. MacKay has analyzed these elements in terms of limits to the accuracy of measurement and has related them to instrumental elements in a category of measurement.

There are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of natural sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.

The scientific method is an iterative, cyclical process through which information is continually revised. It is generally recognized to develop advances in knowledge through the following elements, in varying combinations or contributions:

Each element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do (see below) but apply mostly to experimental sciences (e.g., physics, chemistry, and biology). The elements above are often taught in the educational system as "the scientific method".

The scientific method is not a single recipe: it requires intelligence, imagination, and creativity. In this sense, it is not a mindless set of standards and procedures to follow,
but is rather an ongoing cycle, constantly developing more useful, accurate and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's "Principia". On the contrary, if the astronomically massive, the feather-light, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase confidence in Newton's work.

A linearized, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:


The iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.

While this schema outlines a typical hypothesis/testing method, a number of philosophers, historians, and sociologists of science, including Paul Feyerabend, claim that such descriptions of scientific method have little relation to the ways that science is actually practiced.

The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The "subjects" can also be called or the "unknowns".) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.

The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.

Measurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken.

Measurements demand the use of "operational definitions" of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact or "idealized" definition. For example, electric current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of "mass" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.

The scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.

New theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, "I do not define , space, place and motion, as being well known to all." Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood. In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them.

 The history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's "transforming principle"). But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle. .."2. DNA-hypotheses"

The characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Albert Einstein as a possible early test of his theory of General relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century.

A hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.

Normally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.

Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Albert Einstein once observed that "there is no logical bridge between phenomena and their theoretical principles." Charles Sanders Peirce, borrowing a page from Aristotle ("Prior Analytics", 2.25) described the incipient stages of inquiry, instigated by the "irritation of doubt" to venture a plausible guess, as "abductive reasoning". The history of science is filled with stories of scientists claiming a "flash of inspiration", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.

William Glen observes that

In general scientists tend to look for theories that are "elegant" or "beautiful". Scientists often use these terms to refer to a theory that is in accordance with the known facts, but is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses.

To minimize the confirmation bias which results from entertaining a single hypothesis, strong inference emphasizes the need for entertaining multiple alternative hypotheses.

 Linus Pauling proposed that DNA might be a triple helix. This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) "..3. DNA-predictions"

Any useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.

It is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.

If the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. For example, while a hypothesis on the existence of other intelligent species may be convincing with scientifically based speculation, there is no known experiment that can test this hypothesis. Therefore, science itself can have little to say about the possibility. In the future, a new technique may allow for an experimental test and the speculation would then become part of accepted science.

 James D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'. This prediction followed from the work of Cochran, Crick and Vand (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.

In their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, "It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material". " ..4. DNA-experiments"

Einstein's theory of General Relativity makes several specific predictions about the observable structure of space-time, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.

Once predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed, when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples (or observations) under differing conditions to see what varies or what remains the same. We vary the conditions for each measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is. Factor analysis is one technique for discovering the important factor in an effect.

Depending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment which tests the aerodynamical hypotheses used for constructing the plane.

Scientists assume an attitude of openness and accountability on the part of those conducting an experiment. Detailed record keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of Jābir ibn Hayyān (721–815 CE), al-Battani (853–929) and Alhazen (965–1039).

 Watson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from Kings College – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an X-shape and was able to confirm the structure was helical. This rekindled Watson and Crick's model building and led to the correct structure. "..1. DNA-characterizations"

The scientific method is iterative. At any stage it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.

Other scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility.

 After considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts, Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it. They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. .."DNA Example"

Science is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.

To protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including "Nature" and "Science", have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at a number of national archives in the U.S. or in the World Data Center.

The classical model of scientific inquiry derives from Aristotle, who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.

The hypothetico-deductive model or method is a proposed description of scientific method. Here, predictions from the hypothesis are central: if you assume the hypothesis to be true, what consequences follow?

If subsequent empirical investigation does not demonstrate that these consequences or predictions correspond to the observable world, the hypothesis can be concluded to be false.

In 1877, Charles Sanders Peirce (1839–1914) characterized inquiry in general not as the pursuit of truth "per se" but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless. He outlined four methods of settling opinion, ordered from least to most successful:

Peirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research, which in turn should not be trammeled by the other methods and practical ends; reason's "first rule" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry. The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth "per se" but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.

For Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators "would" reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points. In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to "Do the science." Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge. As inference, "logic is rooted in the social principle" since it depends on a standpoint that is, in a sense, unlimited.

Paying special attention to the generation of explanations, Peirce outlined the scientific method as a coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in "A Neglected Argument" except as otherwise noted):

Science applied to complex systems can involve elements such as transdisciplinarity, systems theory and scientific modelling. The Santa Fe Institute studies such systems; Murray Gell-Mann interconnects these topics with message passing.

In general, the scientific method may be difficult to apply stringently to diverse, interconnected systems and large data sets. In particular, practices used within Big data, such as predictive analytics, may be considered to be at odds with the scientific method.

Frequently the scientific method is employed not only by a single person, but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment.

Scientific journals use a process of "peer review", in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three, and usually anonymous) fellow scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals, and can have the effect of keeping the literature free of obvious errors and to generally improve the quality of the material, especially in the journals who use the standard most rigorously. The peer review process can have limitations when considering research outside the conventional scientific paradigm: problems of "groupthink" can interfere with open and fair deliberation of some new research.

Sometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments in order to duplicate the results.

Researchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses and source code can be preserved in order to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery.

When additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research.

Since it is impossible for a scientist to record "everything" that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed in order to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'.

Philosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world. These assumptions from methodological naturalism form a basis on which science may be grounded. Logical Positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized.

Thomas Kuhn examined the history of science in his "The Structure of Scientific Revolutions", and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.

Norwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the "theory laden" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description. He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a "different" sun rise despite the same physiological phenomenon. Kuhn and Feyerabend acknowledge the pioneering significance of his work.

Kuhn (1961) said the scientist generally has a theory in mind before designing and undertaking experiments so as to make empirical observations, and that the "route from theory to measurement can almost never be traveled backward". This implies that the way in which theory is tested is dictated by the nature of the theory itself, which led Kuhn (1961, p. 166) to argue that "once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed".

Paul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book "Against Method" he argues that scientific progress is "not" the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'. Criticisms such as his led to the strong programme, a radical approach to the sociology of science.

The postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (note that this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.

In anthropology, following the anthropological fieldworks in an academic scientific laboratory by Latour and Woolgar, Karin Knorr Cetina has conducted a comparative anthropological study of two scientific fields (namely high energy physics and molecular biology) to conclude that the epistemic practices and reasonings within both scientific communities are different enough to introduce the concept of "epistemic cultures", in contradiction with the idea that a so-called "scientific method" is unique and a unifying concept.

Somewhere between 33% and 50% of all scientific discoveries are estimated to have been "stumbled upon", rather than sought out. This may explain why scientists so often express that they were lucky. Louis Pasteur is credited with the famous saying that "Luck favours the prepared mind", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected. This is what Nassim Nicholas Taleb calls "Anti-fragility"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.

Psychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they "think" is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.

Science is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines try to distinguish what is "known" from what is "unknown" at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be "falsifiable" (capable of disproof). In mathematics, a statement need not yet be proven; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.

Mathematical work and scientific work can inspire each other. For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proven using time as a mathematical concept in which objects can flow (see Ricci flow).

Nevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, "The Unreasonable Effectiveness of Mathematics in the Natural Sciences", is a very well known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.

George Pólya's work on problem solving, the construction of mathematical proofs, and heuristic show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps.

In Pólya's view, "understanding" involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; "analysis", which Pólya takes from Pappus, involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; "synthesis" is the strict Euclidean exposition of step-by-step details of the proof; "review" involves reconsidering and re-examining the result and the path taken to it.

Gauss, when asked how he came about his theorems, once replied "durch planmässiges Tattonieren" (through systematic palpable experimentation).

Imre Lakatos argued that mathematicians actually use contradiction, criticism and revision as principles for improving their work. In like manner to science, where truth is sought, but certainty is not found, in "Proofs and refutations" (1976), what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true, by rewriting them, as did Poincaré ("Proofs and Refutations", 1976).)

Lakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In "Proofs and Refutations", Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.

When the scientific method employs statistics as part of its arsenal, there are mathematical and practical issues that can have a deleterious effect on the reliability of the output of scientific methods. This is described in a popular 2005 scientific paper "Why Most Published Research Findings Are False" by John Ioannidis, which is considered foundational to the field of metascience. Much research in metascience seeks to identify poor use of statistics and improve its use.

The particular points raised are statistical ("The smaller the studies conducted in a scientific field, the less likely the research findings are to be true" and "The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.") and economical ("The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true" and "The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.") Hence: "Most research findings are false for most research designs and for most fields" and "As shown, the majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings." However: "Nevertheless, most new discoveries will continue to stem from hypothesis-generating research with low or very low pre-study odds," which means that *new* discoveries will come from research that, when that research started, had low or very low odds (a low or very low chance) of succeeding. Hence, if the scientific method is used to expand the frontiers of knowledge, research into areas that are outside the mainstream will yield most new discoveries.





</doc>
<doc id="26838" url="https://en.wikipedia.org/wiki?curid=26838" title="Shotgun">
Shotgun

A shotgun (also known as a scattergun, or historically as a fowling piece) is a firearm that is usually designed to be fired from the shoulder, which uses the energy of a fixed shell to fire a number of small spherical pellets called shot, or a solid projectile called a slug. Shotguns come in a wide variety of sizes, ranging from 5.5 mm (.22 inch) bore up to bore, and in a range of firearm operating mechanisms, including breech loading, single-barreled, double or combination gun, pump-action, bolt-, and lever-action, revolver, semi-automatic, and even fully automatic variants.

A shotgun was originally a smoothbore firearm, which means that the inside of the barrel is not rifled but later rifled shotgun barrels and slugs become available. Preceding smoothbore firearms, such as the musket, were widely used by armies in the 18th century. The direct ancestor to the shotgun, the blunderbuss, was also used in a similar variety of roles from self-defense to riot control. It was often used by cavalry troops because of its generally shorter length and ease of use, as well as by coachmen for its substantial power. In the 19th century, however, these weapons were largely replaced on the battlefield with breechloading rifled firearms, which were more accurate over longer ranges. The military value of shotguns was rediscovered in the First World War, when American forces used 12-gauge pump action shotguns in close-quarters trench fighting to great effect. Since then, it has been used in a variety of roles in civilian, law enforcement, and military applications.

The shot pellets from a shotgun spread upon leaving the barrel, and the power of the burning charge is divided among the pellets, which means that the energy of any one ball of shot is fairly low. In a hunting context, this makes shotguns useful primarily for hunting birds and other small game. However, in a military or law enforcement context, the large number of projectiles makes the shotgun useful as a close quarters combat weapon or a defensive weapon. Militants or insurgents may use shotguns in asymmetric engagements, as shotguns are commonly owned civilian weapons in many countries. Shotguns are also used for target shooting sports such as skeet, trap, and sporting clays. These involve shooting clay disks, known as clay pigeons, thrown in various ways.

Shotguns come in a wide variety of forms, from very small up to massive punt guns, and in nearly every type of firearm operating mechanism. The common characteristics that make a shotgun unique center on the requirements of firing shot. These features are the features typical of a shotgun shell, namely a relatively short, wide cartridge, with straight walls, and operating at a relatively low pressure.

Ammunition for shotguns is referred to in the US as shotgun shells, shotshells, or just shells (when it is not likely to be confused with artillery shells). The term cartridges is standard usage in the United Kingdom.

The shot is usually fired from a smoothbore barrel; another configuration is the rifled slug barrel, which fires more accurate solitary projectiles.

The typical use of a shotgun is against small and fast moving targets, often while in the air. The spreading of the shot allows the user to point the shotgun close to the target, rather than having to aim precisely as in the case of a single projectile. The disadvantages of shot are limited range and limited penetration of the shot, which is why shotguns are used at short ranges, and typically against smaller targets. Larger shot sizes, up to the extreme case of the single projectile slug load, result in increased penetration, but at the expense of fewer projectiles and lower probability of hitting the target.

Aside from the most common use against small, fast moving targets, the shotgun has several advantages when used against still targets. First, it has enormous stopping power at short range, more than nearly all handguns and many rifles. Though many believe the shotgun is a great firearm for inexperienced shooters, the truth is, at close range, the spread of shot is not very large at all, and competency in aiming is still required. A typical self-defense load of buckshot contains 8–27 large lead pellets, resulting in many wound tracks in the target. Also, unlike a fully jacketed rifle bullet, each pellet of shot is less likely to penetrate walls and hit bystanders. It is favored by law enforcement for its low penetration and high stopping power.

On the other hand, the hit potential of a defensive shotgun is often overstated. The typical defensive shot is taken at very close ranges, at which the shot charge expands no more than a few centimeters. This means the shotgun must still be aimed at the target with some care. Balancing this is the fact that shot spreads further upon entering the target, and the multiple wound channels of a defensive load are far more likely to produce a disabling wound than a rifle or handgun.

Some of the most common uses of shotguns are the sports of skeet shooting, trap shooting, and sporting clays. These involve shooting clay discs, also known as clay pigeons, thrown in by hand and by machine. Both skeet and trap competitions are featured at the Olympic Games.

The shotgun is popular for bird hunting (called "game-shooting" in the UK, where "hunting" refers to hunting mammals with a pack of hounds), it is also used for more general forms of hunting especially in semi-populated areas where the range of rifle bullets may pose a hazard. Use of a smooth bore shotgun with a rifled slug or, alternatively, a rifled barrel shotgun with a sabot slug, improves accuracy to or more. This is well within the range of the majority of kill shots by experienced hunters using shotguns.

However, given the relatively low muzzle velocity of slug ammunition, typically around 500 m/s (about 1600 feet per second), and the blunt, poorly streamlined shape of typical slugs (which cause them to lose velocity very rapidly, compared to rifle bullets), a hunter must pay close attention to the ballistics of the particular ammunition used to ensure an effective and humane kill shot.

At any reasonable range, shotgun slugs make effective lethal wounds due to their tremendous mass, reducing the length of time that an animal might suffer. For example, a typical 12 gauge shotgun slug is a blunt piece of metal that could be described as an 18 mm (.729 inch) caliber that weighs 28 grams (432 grains). For comparison, a common deer-hunting rifle round is a 7.62 mm (.308 inch) slug weighing 9.7 grams (150 grains), but the dynamics of the rifle cartridge allow for a different type of wound, and a much further reach.

Shotguns are often used with rifled barrels in locations where it is not lawful to hunt with a rifle. Typically, a sabot slug is used in these barrels for maximum accuracy and performance. Shotguns are often used to hunt whitetail deer in the thick brush and briers of the Southeastern and upper Midwestern United States, where, due to the dense cover, ranges tend to be close – 25m or less.

Sabot slugs are essentially very large hollow point bullets, and are streamlined for maximum spin and accuracy when shot through a rifled barrel. They have greater ranges than older Foster and Brenneke-type slugs.

People often use semiautomatic or pump action shotguns for hunting waterfowl to small game.

In the US and Canada, shotguns are widely used as a support weapon by police forces. One of the rationales for issuing shotguns is that, even without much training, an officer will probably be able to hit targets at close to intermediate range, due to the "spreading" effect of buckshot. This is largely a myth, as the spread of buckshot at 25 feet averages 8 inches, which is still very capable of missing a target. Some police forces are replacing shotguns in this role with carbine rifles such as AR-15s. Shotguns are also used in roadblock situations, where police are blocking a highway to search cars for suspects. In the US, law enforcement agencies often use riot shotguns, especially for crowd and riot control where they may be loaded with less-lethal rounds such as rubber bullets or bean bags. Shotguns are also often used as breaching devices to defeat locks.

Shotguns are common weapons in military use, particularly for special purposes. Shotguns are found aboard naval vessels for shipboard security, because the weapon is very effective at close range as a way of repelling enemy boarding parties. In a naval setting, stainless steel shotguns are often used, because regular steel is more prone to corrosion in the marine environment. Shotguns are also used by military police units. U.S. Marines have used shotguns since their inception at the squad level, often in the hands of NCOs, while the U.S. Army often issued them to a squad's point man. Shotguns were modified for and used in the trench warfare of WWI, in the jungle combat of WWII and the Vietnam. Shotguns were also used in the Iraq, being popular with soldiers in urban combat environments. Some U.S. units in Iraq used shotguns with special frangible breaching rounds to blow the locks off doors when making a surprise entry into a dwelling.

Shotguns are a popular means of home defense for many of the same reasons they are preferred for close-quarters tasks in law enforcement and the military.

Compared to handguns, shotguns are heavier, larger, and not as maneuverable in close quarters (which also presents a greater retention problem), but do have these advantages:

The wide range of forms the shotgun can take leads to some significant differences between what is technically a shotgun and what is legally considered a shotgun. A fairly broad attempt to define a shotgun is made in the United States Code (18 USC 921), which defines the shotgun as "a weapon designed or redesigned, made or remade, and intended to be fired from the shoulder, and designed or redesigned and made or remade to use the energy of the explosive in a fixed shotgun shell to fire through a smooth bore either a number of ball shot or a single projectile for each single pull of the trigger." It is even more broadly defined in English law: "a smooth bore gun not being an air gun" (s.1(3)(a) Firearms Act 1968).

A rifled slug, with finned rifling designed to enable the projectile to be safely fired through a choked barrel, is an example of a single projectile. Some shotguns have rifled barrels and are designed to be used with a "saboted" bullet, one which is typically encased in a two-piece plastic ring ("sabot") designed to peel away after it exits the barrel, leaving the bullet, now spinning after passing through the rifled barrel, to continue toward the target. These shotguns, although they have rifled barrels, still use a shotgun-style shell instead of a rifle cartridge and may in fact still fire regular multipellet shotgun shells, but the rifling in the barrel will affect the shot pattern. The use of a rifled barrel blurs the distinction between rifle and shotgun, and in fact the early rifled shotgun barrels went by the name "Paradox" for just that reason. Hunting laws may differentiate between smooth barreled and rifled barreled guns.

Combat shotgun is a shotgun designed for offensive purposes, typically for the military.

Riot shotgun has long been a synonym for a shotgun, especially a short-barrelled shotgun. During the 19th and early 20th century, these were used to disperse protesters, rioters and revolutionaries. The wide spray of the shot ensured a large group would be hit, but the light shot would ensure more wounds than fatalities. When the ground was paved, police officers would often ricochet the shot off the ground, slowing down the shot and spreading pattern even further. To this day specialized police and defensive shotguns are called riot shotguns. The introduction of rubber bullets and bean bag rounds ended the practice of using shot for the most part, but riot shotguns are still used to fire a variety of less-lethal rounds for riot control.

A sawed-off shotgun (or "sawn-off") refers to a shotgun whose barrel has been shortened, leaving it more maneuverable, easier to use at short range and more readily concealed. Many countries establish a legal minimum barrel length that precludes easy concealment (this length is in the U.S. and 24 inches in the UK). The sawed-off shotgun is sometimes known as a "lupara" (in Italian a generic reference to the word ""lupo"" ("wolf")) in Southern Italy and Sicily.

Coach guns are similar to sawn-off shotguns, except they are manufactured with a 46 cm (18") barrel and are legal for civilian ownership in some jurisdictions. Coach guns are also more commonly associated with the American Old West or Australian Colonial period, and often used for hunting in bush, scrub, or marshland where a longer barrel would be unwieldy or impractical.

Snake Charmer shotguns are commonly used by gardeners and farmers for pest control. They have short barrels and either a full-size stocks or pistol grips, depending on legislation in intended markets. The overall length of these weapons is frequently less than , with some measuring up at less than . These weapons are typically single-shot break-action .410 "gauge" (caliber), which may or may not hold extra shot-shells in the butt-stock. They typically have a cylinder bore and sometimes are available in modified choke as well. Snake Charmers are popular for "home defense" purposes and as "survival" weapons.

Other examples include a variety of .410 / rifle "survival" guns manufactured in over/under designs. In the combination gun arrangement, a rimfire or centrefire rifle barrel is located beneath the barrel of a .410 gauge shotgun. Generally, there is one manually cocked external hammer and an external selection lever to select which caliber of cartridge to fire. A notable example is the Springfield Armory M6 Scout, a .410 / .22 issued to United States Air Force personnel as a "survival" gun in the event of a forced landing or accident in a wilderness area. Variants have been used by Israeli, Canadian, and American armed forces. Shotgun-rifle combination guns with two, three, and occasionally even four barrels are available from a number of makers, primarily European. These provided flexibility, enabling the hunter to effectively shoot at flushing birds or more distant small mammals while only carrying one gun.

Most early firearms, such as the blunderbuss, arquebus, and musket had large diameter, smoothbore barrels, and could fire shot as well as solid balls. A firearm intended for use in wing shooting of birds was known as a fowling piece. The 1728 " Cyclopaedia" defines a "fowling piece" as:

For example, the Brown Bess musket, in service with the British army from 1722 to 1838, had a 19 mm (.75 inch) smoothbore barrel, roughly the same as a 10 gauge shotgun, and was long, just short of the above recommended 168 cm (5 feet). On the other hand, records from the Plymouth colony show a maximum length of 137 cm (4 feet) for fowling pieces, shorter than the typical musket.

Shot was also used in warfare; the buck and ball loading, combining a musket ball with three or six buckshot, was used throughout the history of the smoothbore musket. The first recorded use of the term "shotgun" was in 1776 in Kentucky. It was noted as part of the "frontier language of the West" by James Fenimore Cooper.

With the adoption of smaller bores and rifled barrels, the shotgun began to emerge as a separate entity. Shotguns have long been the preferred method for sport hunting of birds, and the largest shotguns, the punt guns, were used for commercial hunting. The double-barreled shotgun has changed little since the development of the boxlock action in 1875. Modern innovations such as interchangeable chokes and subgauge inserts make the double-barreled shotgun the shotgun of choice in skeet, trap shooting, and sporting clays, as well as with many hunters.

As wing shooting has been a prestige sport, specialty gunsmiths such as Krieghoff or Perazzi have produced fancy double-barrel guns for wealthy European and American hunters. These weapons can cost US$5,000 or more; some elaborately decorated presentation guns have sold for up to US$100,000.

During its long history, the shotgun has been favored by bird hunters, guards, and law enforcement officials. The shotgun has fallen in and out of favor with military forces several times in its long history. Shotguns and similar weapons are simpler than long-range rifles, and were developed earlier. The development of more accurate and deadlier long-range rifles minimized the usefulness of the shotgun on the open battlefields of European wars. But armies have "rediscovered" the shotgun for specialty uses many times.

During the 19th century, shotguns were mainly employed by cavalry units. Both sides of the American Civil War employed shotguns. U.S. cavalry used the shotgun extensively during the Indian Wars in the latter half of the 19th century. Mounted units favored the shotgun for its moving target effectiveness, and devastating close-range firepower. The shotgun was also favored by citizen militias and similar groups.

With the exception of cavalry units, the shotgun saw less and less use throughout the 19th century on the battlefield. As a defense weapon it remained popular with guards and lawmen, however, and the shotgun became one of many symbols of the American Old West. Lawman Cody Lyons killed two men with a shotgun; his friend Doc Holliday's only confirmed kill was with a shotgun. The weapon both these men used was the short-barreled version favored by private strongbox guards on stages and trains. These guards, called express messengers, became known as shotgun messengers, since they rode with the weapon (loaded with buckshot) for defense against bandits. Passenger carriages carrying a strongbox usually had at least one private guard armed with a shotgun riding in front of the coach, next to the driver. This practice has survived in American slang; the term "riding shotgun" is used for the passenger who sits in the front passenger seat. The shotgun was a popular weapon for personal protection in the American Old West, requiring less skill on the part of the user than a revolver.

The origins of the hammerless shotgun are European but otherwise obscure. The earliest breechloading shotguns originated in France and Belgium in the early 19th century (see also the history of the Pinfire) and a number of them such as those by Robert and Chateauvillard from the 1830s and 1840s did not use hammers. In fact during these decades a wide variety of ingenious weapons, including rifles, adopted what is now often known as a 'needle-fire' method of igniting the charge, where a firing pin or a longer sharper needle provided the necessary impact. The most widely used British hammerless needle-fire shotgun was the unusual hinged-chamber fixed-barrel breech-loader by Joseph Needham, produced from the 1850s. By the 1860s hammerless guns were increasingly used in Europe both in war and sport although hammer guns were still very much in the majority. The first significant encroachment on hammer guns was a hammerless patent which could be used with a conventional side-lock. This was British gunmaker T Murcott's 1871 action nicknamed the 'mousetrap' on account of its loud snap action. However, the most successful hammerless innovation of the 1870s was Anson and Deeley's boxlock patent of 1875. This simple but ingenious design only used four moving parts allowing the production of cheaper and reliable shotguns.

Daniel Myron LeFever is credited with the invention of the American hammerless shotgun. Working for Barber & LeFever in Syracuse, N.Y. he introduced his first hammerless shotgun in 1878. This gun was cocked with external cocking levers on the side of the breech. He went on to patent the first truly automatic hammerless shotgun in 1883. This gun automatically cocked itself when the breech was closed. He later developed the mechanism to automatically eject the shells when the breech was opened.

One of the men most responsible for the modern development of the shotgun was prolific gun designer John Browning. While working for Winchester Firearms, Browning revolutionized shotgun design. In 1887, Browning introduced the Model 1887 Lever Action Repeating Shotgun, which loaded a fresh cartridge from its internal magazine by the operation of the action lever. Before this time most shotguns were the 'break open' type.

This development was greatly overshadowed by two further innovations he introduced at the end of the 19th century. In 1893, Browning produced the Model 1893 Pump Action Shotgun, introducing the now familiar pump action to the market. And in 1900, he patented the Browning Auto-5, the world's first semi-automatic shotgun. The Browning Auto-5 remained in production until 1998.

The decline in military use of shotguns reversed in World War I. American forces under General Pershing employed 12-gauge pump action shotguns when they were deployed to the Western front in 1917. These shotguns were fitted with bayonets and a heat shield so the barrel could be gripped while the bayonet was deployed. Shotguns fitted in this fashion became known as "trench guns" by the United States Army. Those without such modifications were known as "riot guns". After World War I, the United States military began referring to all shotguns as "riot guns".

Due to the cramped conditions of trench warfare, the American shotguns were extremely effective. Germany even filed an official diplomatic protest against their use, alleging they violated the laws of warfare. The judge advocate general reviewed the protest, and it was rejected because the Germans protested use of lead shot (which would have been illegal) but military shot was plated. This is the only occasion the legality of the shotgun's use in warfare has been questioned.

During World War II, the shotgun was not heavily used in the war in Europe by official military forces. However, the shotgun was a favorite weapon of Allied-supported partisans, such as the French Resistance. By contrast, in the Pacific theater, thick jungles and heavily fortified positions made the shotgun a favorite weapon of the United States Marines. Marines tended to use pump shotguns, since the pump action was less likely to jam in the humid and dirty conditions of the Pacific campaign. Similarly, the United States Navy used pump shotguns to guard ships when in port in Chinese harbors (e.g., Shanghai). The United States Army Air Forces also used pump shotguns to guard bombers and other aircraft against saboteurs when parked on airbases across the Pacific and on the West Coast of the United States. Pump and semi-automatic shotguns were used in marksmanship training, particularly for bomber gunners. The most common pump shotguns used for these duties were the 12 gauge Winchester Model 97 and Model 12. The break-open action, single barrel shotgun was used by the British Home Guard and U.S. home security forces. Notably, industrial centers (such as the Gopher State Steel Works) were guarded by National Guard soldiers with Winchester Model 37 12 gauge shotguns.

Since the end of World War II, the shotgun has remained a specialty weapon for modern armies. It has been deployed for specialized tasks where its strengths were put to particularly good use. It was used to defend machine gun emplacements during the Korean War, American and French jungle patrols used shotguns during the Vietnam War, and shotguns saw extensive use as door breaching and close quarter weapons in the early stages of the Iraq War, and saw limited use in tank crews. Many modern navies make extensive use of shotguns by personnel engaged in boarding hostile ships, as any shots fired will almost certainly be over a short range. Nonetheless, shotguns are far less common in military use than rifles, carbines, submachineguns, or pistols.

On the other hand, the shotgun has become a standard in law enforcement use. A variety of specialty less-lethal or non-lethal ammunitions, such as tear gas shells, bean bags, flares, explosive sonic stun rounds, and rubber projectiles, all packaged into 12 gauge shotgun shells, are produced specifically for the law enforcement market. Recently, Taser International introduced a self-contained electronic weapon which is fired from a standard 12 gauge shotgun.

The shotgun remains a standard firearm for hunting throughout the world for all sorts of game from birds and small game to large game such as deer. The versatility of the shotgun as a hunting weapon has steadily increased as slug rounds and more advanced rifled barrels have given shotguns longer range and higher killing power. The shotgun has become a ubiquitous firearm in the hunting community.

Action is the term for the operating mechanism of a gun. There are many types of shotguns, typically categorized by the number of barrels or the way the gun is reloaded.

For most of the history of the shotgun, the break-action breech loading double was the most common type, typically divided into two subtypes: the traditional "side by side" shotgun features two barrels mounted one beside the other (as the name suggests), whereas the "over and under" shotgun has the two barrels mounted one on top of the other. Side by side shotguns were traditionally used for hunting and other sporting pursuits (early long barreled side-by side shotguns were known as "fowling pieces" for their use hunting ducks and other birds), whereas over and under shotguns are more commonly associated with recreational use (such as clay pigeon and skeet shooting). Both types of double-barrel shotgun are used for hunting and sporting use, with the individual configuration largely being a matter of personal preference.

Another, less commonly encountered type of break-action shotgun is the combination gun, which is an over and under design with one shotgun barrel and one rifle barrel (more often rifle on top, but rifle on bottom was not uncommon). There is also a class of break action guns called "drillings", which contain three barrels, usually two shotgun barrels of the same gauge and a rifle barrel, though the only common theme is that at least one barrel be a shotgun barrel. The most common arrangement was essentially a side-by-side shotgun with the rifle barrel below and centered. Usually a drilling containing more than one rifle barrel would have both rifle barrels in the same caliber, but examples do exist with different caliber barrels, usually a .22 long rifle and a centerfire cartridge. Although very rare, drillings with three and even four (a "vierling") shotgun barrels were made.

In pump-action shotguns, a sliding forearm handle (the "pump") works the action, extracting the spent shell and inserting a new one while cocking the hammer or striker as the pump is worked. A pump gun is typically fed from a tubular magazine underneath the barrel, which also serves as a guide for the pump. The rounds are fed in one by one through a port in the receiver, where they are lifted by a lever called the "elevator" and pushed forward into the chamber by the bolt. A pair of latches at the rear of the magazine hold the rounds in place and facilitate feeding of one shell at a time. If it is desired to load the gun fully, a round may be loaded through the ejection port directly into the chamber, or cycled from the magazine, which is then topped off with another round. Well-known examples include the Winchester Model 1897, Remington 870 and Mossberg 500/590.

Pump-action shotguns are common hunting, fowling and sporting shotguns. Hunting models generally have a barrel between 600 and 700 mm (24"-28"). Tube-fed models designed for hunting often come with a dowel rod or other stop that is inserted into the magazine and reduces the capacity of the gun to three shells (two in the magazine and one chambered) as is mandated by U.S. federal law when hunting migratory birds. They can also easily be used with an empty magazine as a single-shot weapon, by simply dropping the next round to be fired into the open ejection port after the spent round is ejected. For this reason, pump-actions are commonly used to teach novice shooters under supervision, as the trainer can load each round more quickly than with a break-action, while unlike a break-action the student can maintain his grip on the gun and concentrate on proper handling and firing of the weapon.

Pump action shotguns with shorter barrels and little or no barrel choke are highly popular for use in home defense, military and law enforcement, and are commonly known as riot guns. The minimum barrel length for shotguns in most of the U.S. is , and this barrel length (sometimes to increase magazine capacity and/or ensure the gun is legal regardless of measuring differences) is the primary choice for riot shotguns. The shorter barrel makes the weapon easier to maneuver around corners and in tight spaces, though slightly longer barrels are sometimes used outdoors for a tighter spread pattern or increased accuracy of slug projectiles. Home-defense and law enforcement shotguns are usually chambered for 12-gauge shells, providing maximum shot power and the use of a variety of projectiles such as buckshot, rubber, sandbag and slug shells, but 20-gauge (common in bird-hunting shotguns) or .410 (common in youth-size shotguns) are also available in defense-type shotgun models allowing easier use by novice shooters.

A riot shotgun has many advantages over a handgun or rifle. Compared to "defense-caliber" handguns (chambered for 9mm Parabellum, .38 Special, .357 Magnum, .40 S&W, .45 ACP and similar), a shotgun has far more power and damage potential (up to 10 times the muzzle energy of a .45 ACP cartridge), allowing a "one-shot stop" that is more difficult to achieve with typical handgun loads. Compared to a rifle, riot shotguns are easier to maneuver due to the shorter barrel, still provide better damage potential at indoor distances (generally 3–5 meters/yards), and reduce the risk of "overpenetration"; that is, the bullet or shot passing completely through the target and continuing beyond, which poses a risk to those behind the target through walls. The wide spread of the shot reduces the importance of shot placement compared to a single projectile, which increases the effectiveness of "point shooting" – rapidly aiming simply by pointing the weapon in the direction of the target. This allows easy, fast use by novices.

"See article: Mare's Leg"

Early attempts at repeating shotguns invariably centred around either bolt-or lever-action designs, drawing inspiration from contemporary repeating rifles, with the earliest successful repeating shotgun being the lever-action Winchester M1887, designed by John Browning at the behest of the Winchester Repeating Arms Company.

Lever shotguns, while less common, were popular in the late 19th century with the Winchester Model 1887 and Model 1901 being prime examples. Initially very popular, demand waned after the introduction of pump-action shotguns around the start of the 20th century, and production was eventually discontinued in 1920.

One major issue with lever-actions (and to a lesser extent pump-actions) was that early shotgun shells were often made of paper or similar fragile materials (modern hulls are plastic or metal). As a result, the loading of shells, or working of the action of the shotgun, could often result in cartridges getting crushed and becoming unusable, or even damaging the gun.

Lever shotguns have seen a return to the gun market in recent years, however, with Winchester producing the Model 9410 (chambering the .410 gauge shotgun shell and using the action of the Winchester Model 94 series lever-action rifle, hence the name), and a handful of other firearm manufacturers (primarily Norinco of China and ADI Ltd. of Australia) producing versions of the Winchester Model 1887/1901 designed for modern 12-gauge smokeless shotshells with more durable plastic casings. There has been a notable uptick in lever-action shotgun sales in Australia since 1997, when pump-actions were effectively outlawed.

Bolt-action shotguns, while uncommon, do exist. One of the best-known examples is a 12-gauge manufactured by Mossberg featuring a 3-round magazine, marketed in Australia just after changes to the gun laws in 1997 heavily restricted the ownership and use of pump-action and semi-automatic shotguns. They were not a huge success, as they were somewhat slow and awkward to operate, and the rate of fire was noticeably slower (on average) than a double-barrelled gun. The Rifle Factory Ishapore in India also manufactured a single-shot .410 bore shotgun based on the SMLE Mk III* rifle. The Russian Berdana shotgun was effectively a single-shot bolt-action rifle that became obsolete, and was subsequently modified to chamber 16-gauge shotgun shells for civilian sale. The U.S. military M26 is also a bolt-action weapon. Bolt-action shotguns have also been used in the "goose gun" application, intended to kill birds such as geese at greater range. Typically, goose guns have long barrels (up to 36 inches), and small bolt-fed magazines. Bolt-action shotguns are also used in conjunction with slug shells for the maximum possible accuracy from a shotgun.

In Australia, some straight-pull bolt-action shotguns, such as the Turkish-made Pardus BA12 and Dickinson T1000, the American C-More Competition M26, as well as the indigenous-designed SHS STP 12, have become increasingly popular alternatives to lever-action shotguns, largely due to the better ergonomics with less stress on the shooter's trigger hand and fingers when cycling the action.

Colt briefly manufactured several revolving shotguns that were met with mixed success. The Colt Model 1839 Shotgun was manufactured between 1839 and 1841. Later, the Colt Model 1855 Shotgun, based on the Model 1855 revolving rifle, was manufactured between 1860 and 1863. Because of their low production numbers and age they are among the rarest of all Colt firearms.

The Armsel Striker was a modern take on the revolving shotgun that held 10 rounds of 12 Gauge ammunition in its cylinder. It was copied by Cobray as the Streetsweeper.

Taurus manufactures a carbine variant of the Taurus Judge revolver along with its Australian partner company, Rossi known as the "Taurus/Rossi Circuit Judge". It comes in the original combination chambering of .410 bore and .45 Long Colt, as well as the .44 Remington Magnum chambering. The rifle has small blast shields attached to the cylinder to protect the shooter from hot gases escaping between the cylinder and barrel.

The MTs255 () is a shotgun fed by a 5-round internal revolving cylinder. It is produced by the TsKIB SOO, Central Design and Research Bureau of Sporting and Hunting Arms. They are available in 12, 20, 28 and 32 gauges, and .410 bore.

Gas, inertia, or recoil operated actions are other popular methods of increasing the rate of fire of a shotgun; these are generally referred to as autoloaders or semi-automatics. Instead of having the action manually operated by a pump or lever, the action automatically cycles each time the shotgun is fired, ejecting the spent shell and reloading a fresh one into the chamber. The first successful semi-automatic shotgun was John Browning's Auto-5, first produced by Fabrique Nationale beginning in 1902. Other well-known examples include the Remington 1100, Benelli M1, and Saiga-12.

Some, such as the Franchi SPAS-12 and Benelli M3, are capable of switching between semi-automatic and pump action. These are popular for two reasons; first, some jurisdictions forbid the use of semi-automatic actions for hunting, and second, lower-powered rounds, like "reduced-recoil" buckshot shells and many less-lethal cartridges, have insufficient power to reliably cycle a semi-automatic shotgun.

Fully automatic shotguns, such as Auto Assault-12 (AA-12) also exist, but they're still rare.

In addition to the commonly encountered shotgun actions already listed, there are also shotguns based on the Martini-Henry rifle design, originally designed by British arms maker W.W. Greener.

Some of the more interesting advances in shotgun technology include the versatile NeoStead 2000 and fully automatics such as the Pancor Jackhammer or Auto-Assault 12.

In 1925, Rodolfo Cosmi produced the first working hybrid prototype semi-automatic shotgun, which had an 8-round magazine located in the stock. While it reloaded automatically after each shot like a semi-automatic, it had a break-action to load the first shell. This design has only been repeated once, by Beretta with their UGB25 automatic shotgun. The user loads the first shell by breaking the gun in the manner of a break-action shotgun, then closes it and inserts the second shell into a clip on the gun's right side. The spent hulls are ejected downwards. The guns combine the advantages of the break action (they can be proven to be safe by breaking open, there are no flying hulls) with those of the semi-automatic (low recoil, low barrel axis position hence low muzzle flip).

The French firearm manufacturer Verney-Carron produces the Véloce shotgun, a lever-release blowback firearm like the similarly designed SpeedLine rifle. The Véloce is in essence an inertia-driven semi-automatic shotgun, but after blowback the bolt is trapped by a bolt stop and will not return to battery unless the bolt stop is manually released by depressing a thumb lever near the tang of the gunstock. This design makes the gun technically not really a self-loading weapon, and Verney-Carron described it as a "manual repeating shotgun".

The gauge number is determined by the weight, in fractions of a pound, of a solid sphere of lead with a diameter equal to the inside diameter of the barrel. So, a 10 gauge shotgun nominally should have an inside diameter equal to that of a sphere made from one-tenth of a pound of lead. Each gauge has a set caliber. By far the most common gauges are 12 (0.729 in, 18.5 mm diameter) and 20 (0.614 in, 15.6 mm), although 67 (.410 in diameter), 32, 28, 24, 16, and 10 (19.7 mm) gauge also exist.

Different gauges have different typical applications. Twelve gauge shotguns are common for hunting geese, large ducks, or other big larger gamebirds; professional skeet and trap shooting; military applications; and home-defense applications. Sixteen gauge shotguns were once common for hunters who wanted to use only a single shotgun for gamebirds normally pursued with twelve or twenty gauge shotguns, but have become rarer in recent years. Twenty gauge shotguns are often used for gamebirds such as doves, smaller ducks, and quail. Twenty-eight gauge shotguns are not common, but are classic quail-hunting guns. .410 shotguns are typically used for squirrel hunting or for sportsmen seeking the challenge of killing game with a smaller load.

Other, less common shotgun cartridges have their own unique uses. Ammunition manufacturer CCI produces 9 mm (.355 in.) and several other popular pistol calibers up to .45 ACP as well as .22 (5.5 mm) for firing from handguns. These are commonly called snake shot cartridges. Larger gauges, up to 4 bore, too powerful to shoulder, have been built, but were generally affixed to small boats and referred to as punt guns. These were used for commercial waterfowl hunting, to kill large numbers of birds resting on the water.
Handguns have also been produced that are capable of firing either .45 (Long) Colt or .410 shotgun shells from the same chamber; they are commonly known as "snake guns". Derringers such as the "Snake Slayer and Cowboy Defender" are popular among some outdoors-men in the South and Southwest regions of the United States. There are also some revolvers, such as the Taurus Judge and Smith & Wesson Governor, that are capable of shooting the .45LC/.410 rounds; but as with derringers they are not considered shotguns.

The .410 bore (10.4 mm) is unusual, being measured in inches, and would be approximately 67 "real" gauge, though its short hull versions are nominally called 36 gauge in Europe. It uses a relatively small charge of shot. It is used for hunting and for skeet. Because of its very light recoil (approx 10 N), it is often used as a beginner's gun. However, the small charge and typically tight choke make it more difficult to hit targets. It is also frequently used by expert shooters because of the difficulty, especially in expensive side by side and over/under models for hunting small bird game such as quail and doves. Inexpensive bolt-action .410 shotguns are a very common first hunting shotgun among young pre-teen hunters, as they are used mostly for hunting squirrels, while additionally teaching bolt-action manipulation skills that will transfer easily later to adult-sized hunting rifles. Most of these young hunters move up to a 20-gauge within a few years, and to 12 gauge shotguns and full-size hunting rifles by their late teens. Still, many who are particularly recoil-averse choose to stay with 20-gauge shotguns all their adult life, as it is a suitable gauge for many popular hunting uses.

A recent innovation is the back-boring of barrels, in which the barrels are bored out slightly larger than their actual gauge. This reduces the compression forces on the shot when it transitions from the chamber to the barrel. This leads to a slight reduction in perceived recoil, and an improvement in shot pattern due to reduced deformation of the shot.

Most shotguns are used to fire "a number of ball shot", in addition to slugs and sabots. The ball shot or pellets is for the most part made of lead but this has been partially replaced by bismuth, steel, tungsten-iron, tungsten-nickel-iron and even tungsten polymer loads. Non-toxic loads are required by Federal law for waterfowl hunting in the US, as the shot may be ingested by the waterfowl, which some authorities believe can lead to health problems due to the lead exposure. Shot is termed either birdshot or buckshot depending on the shot size. Informally, birdshot pellets have a diameter smaller than and buckshot are larger than that. Pellet size is indicated by a number; for bird shot this ranges from the smallest 12 (1.2 mm, 0.05 in) to 2 (3.8 mm, 0.15 in) and then BB (4.6 mm, 0.18 in).

For buckshot, the numbers usually start at 4 (6.1 mm, 0.24 in) and go down to 1, 0, 00 ("double aught"), 000, and finally 0000 (9.7 mm, .38 in). A different informal distinction is that "bird shot" pellets are small enough that they can be measured into the cartridge by weight, and simply poured in, whereas "buckshot" pellets are so large they must be stacked inside the cartridge in a fixed geometric arrangement in order to fit. The diameter in hundredths of an inch of bird shot sizes from #9 to #1 can be obtained by subtracting the shot size from 17. Thus, #4 bird shot is 17 – 4 = 13 = in diameter. Different terminology is used outside the United States. In England and Australia, for example, 00 buckshot cartridges are commonly referred to as "S.G." (small game) cartridges.

Shot, small and round and delivered without spin, is ballistically inefficient. As the shot leaves the barrel it begins to disperse in the air. The resulting cloud of pellets is known as the shot pattern, or shotgun shot spread. The ideal pattern would be a circle with an even distribution of shot throughout, with a density sufficient to ensure enough pellets will intersect the target to achieve the desired result, such as a kill when hunting or a break when shooting clay targets. In reality the pattern is closer to a Gaussian, or normal distribution, with a higher density in the center that tapers off at the edges. Patterns are usually measured by firing at a diameter circle on a large sheet of paper placed at varying distances. The hits inside the circle are counted, and compared to the total number of pellets, and the density of the pattern inside the circle is examined. An "ideal" pattern would put nearly 100% of the pellets in the circle and would have no voids—any region where a target silhouette will fit and not cover 3 or more holes is considered a potential problem.

A constriction in the end of the barrel known as the choke is used to tailor the pattern for different purposes. Chokes may either be formed as part of the barrel at the time of manufacture, by squeezing the end of the bore down over a mandrel, or by threading the barrel and screwing in an interchangeable choke tube. The choke typically consists of a conical section that smoothly tapers from the bore diameter down to the choke diameter, followed by a cylindrical section of the choke diameter. Briley Manufacturing, a maker of interchangeable shotgun chokes, uses a conical portion about 3 times the bore diameter in length, so the shot is gradually squeezed down with minimal deformation. The cylindrical section is shorter, usually . The use of interchangeable chokes has made it easy to tune the performance of a given combination of shotgun and shotshell to achieve the desired performance.

The choke should be tailored to the range and size of the targets. A skeet shooter shooting at close targets might use 127 micrometres (0.005 inches) of constriction to produce a diameter pattern at a distance of . A trap shooter shooting at distant targets might use 762 micrometres (0.030 inches) of constriction to produce a diameter pattern at . Special chokes for turkey hunting, which requires long range shots at the small head and neck of the bird, can go as high as 1500 micrometres (0.060 inches). The use of too much choke and a small pattern increases the difficulty of hitting the target, whereas the use of too little choke produces large patterns with insufficient pellet density to reliably break targets or kill game. "Cylinder barrels" have no constriction. See also: Slug barrel

Other specialized choke tubes exist as well. Some turkey hunting tubes have constrictions greater than "Super Full", or additional features like porting to reduce recoil, or "straight rifling" that is designed to stop any spin that the shot column might acquire when traveling down the barrel. These tubes are often extended tubes, meaning they project beyond the end of the bore, giving more room for things like a longer conical section. Shot spreaders or diffusion chokes work opposite of normal chokes—they are designed to spread the shot more than a cylinder bore, generating wider patterns for very short range use. A number of recent spreader chokes, such as the Briley "Diffusion" line, actually use rifling in the choke to spin the shot slightly, creating a wider spread. The Briley Diffusion uses a 1 in 36 cm twist, as does the FABARM Lion Paradox shotgun.

Oval chokes, which are designed to provide a shot pattern wider than it is tall, are sometimes found on combat shotguns, primarily those of the Vietnam War era. They were available for aftermarket addition in the 1970s from companies like A & W Engineering. Military versions of the Ithaca 37 with "duckbill" choke were used in limited numbers during the Vietnam War by US Navy Seals. It arguably increased effectiveness in close range engagements against multiple targets. Two major disadvantages plagued the system. One was erratic patterning. The second was that the shot would spread too quickly providing a limited effective zone.

Offset chokes, where the pattern is intentionally slightly off of center, are used to change the point of impact. For instance, an offset choke can be used to make a double barrelled shotgun with poorly aligned barrels hit the same spot with both barrels.

Shotguns generally have longer barrels than modern rifles. Unlike rifles, however, the long shotgun barrel is not for ballistic purposes; shotgun shells use small powder charges in large diameter bores, and this leads to very low muzzle pressures (see internal ballistics) and very little velocity change with increasing barrel length. According to Remington, modern powder in a shotgun burns completely in 25 (9.8425 in) to 36 (14.173 in) cm barrels.

Since shotguns are generally used for shooting at small, fast moving targets, it is important to "lead" the target by firing slightly ahead of the target, so that when the shot reaches the range of the target, the target will have moved into the pattern. On uphill shooting, this means to shoot "above" the target. Conversely, on downhill shooting, this means to shoot "below" the target, which is somewhat counterintuitive for many beginning hunters. Depending on the barrel length, the amount of "lead" employed will vary for different barrel lengths, and must be learned by experience.

Shotguns made for close ranges, where the angular speed of the targets is great (such as skeet or upland bird hunting), tend to have shorter barrels, around . Shotguns for longer range shooting, where angular speeds are small (trap shooting; quail, pheasant, and waterfowl hunting), tend to have longer barrels, 28 to . The longer barrels have more angular momentum, and will therefore swing more slowly but more steadily. The short, low angular momentum barrels swing faster, but are less steady. These lengths are for pump or semi-auto shotguns; break open guns have shorter overall lengths for the same barrel length, and so will use longer barrels. The break open design saves between in overall length, but in most cases pays for this by having two barrels, which adds weight at the muzzle. Barrels for shotguns have been getting longer as modern steels and production methods make the barrels stronger and lighter; a longer, lighter barrel gives the same inertia for less overall weight.

Shotguns for use against larger, slower targets generally have even shorter barrels. Small game shotguns, for hunting game like rabbits and squirrels, or shotguns for use with buckshot for deer, are often .

Shotguns intended for all-round hunting are a compromise, but a barrel pump-action 12-gauge shotgun with a modified choke can serve admirably for use as one gun intended for general all-round hunting of small-game such as quails, rabbits, pheasants, doves, and squirrels in semi-open wooded or farmland areas in many parts of the eastern US (Kentucky, Indiana, Tennessee) where dense brush is less of a hindrance and the ability to have more reach is important. For hunting in dense brush, shorter barrel lengths are often preferred when hunting the same types of game.

Shotguns are well suited for the use caliber conversion sleeves, allowing most single- and double-barrel shotguns to fire a wide range of ammunition. The X Caliber system consists of eight adapter sleeves that allow the 12 gauge models to fire: .380 ACP, 9mm Luger, .38 Special, .357 Magnum, .40 S&W, .44 Special, .44 Magnum, .45 ACP, .45 Long Colt, .410 gauge and 20 gauge ammunition. The X caliber 12 gauge adapter sleeves also come in .22 Long Rifle, .223 Remington, 7.62x39mm and .308 Winchester as well. They even make four adapter sleeves that allow the 20 gauge models to fire: 9mm Luger, .38 Special, .357 Magnum, .45 ACP, .45 Long Colt, and .410 gauge ammunition.

The extremely large caliber of shotgun shells has led to a wide variety of different ammunition.

Shotshells are the most commonly used round, filled with lead or lead substitute pellets.

Of this general class, the most common subset is birdshot, which uses a large number (from dozens to hundreds) of small pellets, meant to create a wide "kill spread" to hunt birds in flight. Shot shells are described by the size and number of the pellets within, and numbered in reverse order (the smaller the number, the bigger the pellet size, similar to bore gauge). Size nine (#9) shot is the smallest size normally used for hunting and is used on small upland game birds such as dove and quail. Larger sizes are used for hunting larger upland game birds and waterfowl.

Buckshot is similar to but larger than birdshot, and was originally designed for hunting larger game, such as deer (hence the name). While the advent of new, more accurate slug technologies is making buckshot less attractive for hunting, it is still the most common choice for police, military, and home defense uses. Like birdshot, buckshot is described by pellet size, with larger numbers indicating smaller shot. From the smallest to the largest, buckshot sizes are: #4, (called "number four"), #1, 0 ("one-aught"), 00 ("double-aught"), 000 ("triple-aught") and 0000 ("four-aught"). A typical round for defensive use would be a 12 gauge length 00 buck shell, which contains 9 pellets roughly 8.4 mm (.33 inch) in diameter, each comparable to a .38 Special bullet in damage potential. New "tactical" buckshot rounds, designed specifically for defensive use, use slightly fewer shot at lower velocity to reduce recoil and increase controllability of the shotgun. There are some shotgun rounds designed specifically for police use that shoot effectively from with a 20" diameter grouping of the balls.

Slug rounds are rounds that fire a single solid slug. They are used for hunting large game, and in certain military and law enforcement applications. Modern slugs are moderately accurate, especially when fired from special rifled slug barrels. They are often used in "shotgun-only" hunting zones near inhabited areas, where rifles are prohibited due to their greater range.

Sabots are a common type of slug round. While some slugs are exactly that—a 12-gauge metal projectile in a cartridge—a sabot is a smaller but more aerodynamic projectile surrounded by a "shoe" of some other material. This "sabot" jacket seals the barrel, increasing pressure and acceleration, while also inducing spin on the projectile in a rifled barrel. Once the projectile clears the barrel, the sabot material falls away, leaving an unmarked, aerodynamic bullet to continue toward the target. The advantages over a traditional slug are increased shot power, increased bullet velocity due to the lighter-mass bullet, and increased accuracy due to the velocity and the reduction in deformation of the slug itself. Disadvantages versus a traditional slug include lower muzzle momentum due to reduced mass, reduced damage due to smaller bullet diameter, and significantly higher per-unit cost.

The unique properties of the shotgun, such as large case capacity, large bore, and the lack of rifling, has led to the development of a large variety of specialty shells, ranging from novelties to high tech military rounds.

Brenneke and Foster type slugs have the same basic configuration as normal slugs, but have increased accuracy. The hollowed rear of the Foster slug improves accuracy by placing more mass in the front of the projectile, therefore inhibiting the "tumble" that normal slugs may generate. The Brenneke slug takes this concept a bit further, with the addition of a wad that stays connected to the projectile after discharge, increasing accuracy. Both slugs are commonly found with fins or rib, which are meant to allow the projectile to safely squeeze down during passage through chokes, but they do not increase stability in flight.

Flechette rounds contain aerodynamic darts, typically from 8 to 20 in number. The flechette provide greatly extended range due to their aerodynamic shape, and improved penetration of light armor. American troops during the Vietnam War packed their own flechette shotgun rounds, called "beehive rounds", after the similar artillery rounds. However, terminal performance was poor due to the very light weight of the flechettes, and their use was quickly dropped.

Grenade rounds use exploding projectiles to increase long range lethality. These are currently experimental, but the British FRAG-12, which comes in High Explosive (HE), High Explosive Armor-piercing (HEAP) and High Explosive Fragmenting Antipersonnel (HEFA) forms, is under consideration by military forces.

Flexible baton rounds, commonly called "bean bags", fire a fabric bag filled with birdshot or a similar loose, dense substance. The "punch" effect of the bag is useful for knocking down targets; the rounds are used by police to subdue violent suspects. The bean bag round is by far the most common less-lethal round used. Due to the large surface area of these rounds, they lose velocity rapidly, and must be used at fairly short ranges to be effective, though use at extremely short ranges, under , can result in broken bones or other serious or lethal injuries. The rounds can also fly in a frisbee-like fashion and cut the person or animal being fired at. For this reason, these types of rounds are referred to as less-lethal, as opposed to less-than-lethal.

Gas shells spray a cone of gas for several meters. These are primarily used by riot police. They normally contain pepper gas or tear gas. Other variations launch a gas-grenade-like projectile.

Rock salt shells are hand loaded with coarse rock salt crystals, replacing the standard lead or steel shot. Rock salt shells could be seen as the forerunners of modern less-lethal rounds. In the United States, rock salt shells were and are sometimes still used by rural civilians to defend their property. The brittle salt was unlikely to cause serious injury at long ranges, but would cause painful stinging injuries and served as a warning. British gamekeepers have used rock salt shells to deter poachers. Rather than get into a physical confrontation, they stalk the poachers, making themselves known by a loud shout of "Run!" just before firing, to avoid hitting the now-fleeing subject in the eyes.

Rubber slugs or rubber buckshot are similar in principle to the bean bag rounds. Composed of flexible rubber or plastic and fired at low velocities, these rounds are probably the most common choice for riot control.

Taser International announced in 2007 a new 12 gauge eXtended Range Electronic Projectile or XREP, which contains a small electroshock weapon unit in a carrier that can be fired from a standard 12 gauge shotgun. The XREP projectile is fin stabilized, and travels at an initial velocity of 100 m/s (300 ft/s). Barbs on the front attach the electroshock unit to the target, with a tassel deploying from the rear to widen the circuit. A twenty-second burst of electrical energy is delivered to the target. This product was expected to be released to market in 2008. They were used—despite still being subject to testing, in breach of the supplier's license—by Northumbria police in their standoff with Raoul Moat in 2010.

Breaching rounds, often called frangible, Disintegrator, or Hatton rounds, are designed to destroy door locking mechanisms without risking lives. They are constructed of a very brittle substance that transfers most of the energy to the primary target but then fragment into much smaller pieces or dust so as not to injure unseen targets such as hostages or non-combatants that may be standing behind a breached door.

Bird bombs are low-powered rounds that fire a firecracker that is fused to explode a short time after firing. They are designed to scare animals, such as birds that congregate on airport runways.

Screechers fire a pyrotechnic whistle that emits a loud whistling sound for the duration of its flight. These are also used to scare animals.

Blank shells contain only a small amount of powder and no actual load. When fired, the blanks provide the sound and flash of a real load, but with no projectile. These may be used for simulation of gunfire, scaring wildlife, or as power for a launching device such as the Mossberg #50298 marine line launcher.

Stinger is a type of shotgun shell which contains sixteen 00-buck balls made of Zytel, and is designed as a non-lethal ammunition ideally used in small spaces.

Bolo rounds are made of two or more slugs molded onto steel wire. When fired, the slugs separate, pulling the wire taut creating a flying blade, which could theoretically decapitate people and animals or amputate limbs. However, many active shotgun users consider this to be overstated, and view bolo shells as being less effective than conventional ammunition. Bolo shell rounds are banned in many locations (including the US states of Florida and Illinois) due to concerns about their potential lethality. The round is named in reference to bolas, which use two or more weighted balls on a rope to trap cattle or game.

Dragon's breath usually refers to a zirconium-based pyrotechnic shotgun round. When fired, a gout of flame erupts from the barrel of the gun (up to 20 ft). The visual effect it produces is impressive, similar to that of a short ranged flamethrower. However, it has few tactical uses, mainly distraction/disorientation.

Flare rounds are sometimes carried by hunters for safety and rescue purposes. They are available in low and high altitude versions. Some brands claim they can reach a height of up to .

Globally, shotguns are generally not as heavily regulated as rifles or handguns, likely because they lack the range of rifles and are not easily concealable as handguns are; thus, they are perceived as a lesser threat by legislative authorities. The one exception is a sawed-off shotgun, especially a lupara, as it is more easily concealed than a normal shotgun.

Within Australia, all shotguns manufactured after 1 January 1901 are considered firearms and are subject to registration and licensing. Most shotguns (including break-action, bolt-action and lever-action shotguns) are classed as "Category A" weapons and, as such, are comparatively easy to obtain a licence for, given a legally recognised "legitimate reason" (compare to the British requirement for "good reason" for a FAC), such as sport shooting or hunting. However, pump-action and semi-automatic shotguns are classed as "Category C" (magazine capacity no more than 5 rounds) or "Category D" (magazine capacity more than 5 rounds) weapons; a licence for this type of firearm is, practically speaking, unavailable to the average citizen due to the difficulty and red tape of acquiring one. For more information, see Gun politics in Australia.

Canada has three classifications of firearms: non-restricted, restricted, and prohibited. Shotguns are found in all three classes.

All non-restricted shotguns must have an overall length of at least . Semi-automatic shotguns must also have a barrel length of more than and have a capacity of 5 shells or less in the magazine to remain non-restricted. All other shotgun action types (pump/slide, break open, lever, bolt) do not have a magazine limit restriction or a minimum barrel length provided the overall length of the firearm remains more than and the barrel was produced by an approved manufacturer. Shotgun barrels may only be reduced in length to a minimum of . Non-restricted shotguns may be possessed with any Possession and Acquisition Licence (PAL) or Possession-Only License (POL) and may be transported throughout the country without special authorization and may be used for hunting certain species at certain times of the year.

Semi-automatic shotguns with a barrel length of less than are considered restricted and any shotgun that has been altered so its barrel length is less than or if its overall length is less than is considered prohibited. Restricted and prohibited shotguns may be possessed with a PAL or POL that has been endorsed for restricted or prohibited grandfathered firearms. These shotguns require special Authorization to Transport (ATT).

The Canadian Firearms Registry was a government-run registry of all legally owned firearms in Canada. The government provided amnesty from prosecution to shotgun and rifle owners if they fail to register non-restricted shotguns and rifles. The long gun portion of the registry was scrapped in 2011.

See online for an official Canadian list of non-restricted and restricted and prohibited firearms.

In the United Kingdom, a Shotgun Certificate (SGC) is required to possess a "Section 2" shotgun. These cost £50 and can only be denied if the chief of police in the area believes and can prove that the applicant poses a real danger to the public, or if the applicant has been convicted of a crime punishable by imprisonment for a term of three years or more or if the applicant cannot securely store a shotgun (gun clamps, wire locks and locking gun cabinets are considered secure). The round number restrictions apply only to the magazine, not the chamber, so it is legal to have a single-barreled semi-auto or pump-action shotgun that holds three rounds in total, or a shotgun with separate chambers (which would need to also be multi-barrelled). For a shotgun to qualify as a section 2 shotgun, it must meet the following criteria:

(a) has a barrel not less than in length and does not have any barrel with a bore more than in diameter;

(b) either has no magazine or has a non-detachable magazine not capable of holding more than two cartridges;

(c) is not a revolver gun.

Prior to a SGC being issued an interview is conducted with the local Firearms Officer, in the past this was a duty undertaken by the local police although more recently this function has been "contracted out" to civilian staff. The officer will check the location and suitability of the gun safe that is to be used for storage and conduct a general interview to establish the reasons behind the applicant requiring a SGC.

An SGC holder can own any number of shotguns meeting these requirements so long as he/she can store them securely. No certificate is required to own shotgun ammunition, but one is required to buy it. There is no restriction on the amount of shotgun ammunition that can be bought or owned. There are also no rules regarding the storage of ammunition.

However, shotgun ammunition which contains fewer than 6 projectiles requires a section 1 Firearms Certificate (FAC). Shotguns with a magazine capacity greater than 2 rounds are also considered to be section 1 firearms and, as such, require an FAC to own. An FAC costs £50 but is much more restrictive than an SGC. The applicant must nominate two referees who are known to the applicant to vouch for his or her character; a new 'variation' is required for each new caliber of gun to be owned; limits are set on how much ammunition a person can own at any one time; and an FAC can be denied if the applicant does not have sufficient 'good reason'. 'Good reason' generally means hunting, collecting, or target shooting – though other reasons may be acceptable. Personal defense is not an acceptable reason.

Any pump-action or semi-automatic smooth-bore gun (such as a shotgun) with a barrel length of less than 24 inches or total length of less than 40 inches is considered to be a section 5 firearm, that is, one that is subject to general prohibition, unless it is chambered for .22 caliber rimfire ammunition.

In the US, federal law prohibits shotguns from being capable of holding more than three shells including the round in the chamber when used for hunting migratory gamebirds such as doves, ducks, and geese. For other uses, a capacity of any number of shells is generally permitted. Most magazine-fed shotguns come with a removable magazine plug to limit capacity to 2, plus one in the chamber, for hunting migratory gamebirds. Certain states have restrictions on magazine capacity or design features under hunting or assault weapon laws.

Shotguns intended for defensive use have barrels as short as for private use (the minimum shotgun barrel length allowed by law in the United States without federal registration. Barrel lengths of less than as measured from the breechface to the muzzle when the weapon is in battery, or have an overall length of less than are classified as short barreled shotguns (SBS) under the 1934 National Firearms Act and are regulated. A similar short barreled weapon having a pistol grip may be classified as an AOW or "Any Other Weapon" or "Firearm," depending on barrel length. A shotgun is defined as a weapon (with a buttstock) designed to be fired from the shoulder. The classification varies depending on how the weapon was originally manufactured.

Shotguns used by military, police, and other government agencies are regulated under the National Firearms Act of 1934; however, they are exempt from transfer taxes. These weapons commonly have barrels as short as so that they are easier to handle in confined spaces. Non-prohibited private citizens may own short-barreled shotguns by passing extensive background checks (state and local laws may be more restrictive) as well as paying a $200 federal tax and being issued a stamp. Defensive shotguns sometimes have no buttstock or will have a folding stock to reduce overall length even more when required. AOWs transfer with a $5 tax stamp from the BATFE.





</doc>
<doc id="26840" url="https://en.wikipedia.org/wiki?curid=26840" title="Saskatchewan">
Saskatchewan

Saskatchewan () is a prairie and boreal province in western Canada, the only province without a natural border. It has an area of , nearly 10 percent of which () is fresh water, composed mostly of rivers, reservoirs, and the province's 100,000 lakes.

Saskatchewan is bordered on the west by Alberta, on the north by the Northwest Territories, on the east by Manitoba, to the northeast by Nunavut, and on the south by the U.S. states of Montana and North Dakota. As of Q3 2019, Saskatchewan's population was estimated at 1,178,657. Residents primarily live in the southern prairie half of the province, while the northern boreal half is mostly forested and sparsely populated. Of the total population, roughly half live in the province's largest city Saskatoon, or the provincial capital Regina. Other notable cities include Prince Albert, Moose Jaw, Yorkton, Swift Current, North Battleford, Melfort, and the border city Lloydminster (partially within Alberta).

Saskatchewan is a landlocked province with large distances to moderating bodies of waters. As a result, its climate is extremely continental, rendering severe winters throughout the province. Southern areas have very warm or hot summers. Midale and Yellow Grass (both near the U.S. border) are tied for the highest ever recorded temperatures in Canada, with observed at both locations on July 5, 1937. In winter, temperatures below are possible even in the south during extreme cold snaps.

Saskatchewan has been inhabited for thousands of years by various indigenous groups. Europeans first explored the area in 1690 and first settled in the area in 1774. It became a province in 1905, carved out from the vast North-West Territories, which had until then included most of the Canadian Prairies. In the early 20th century the province became known as a stronghold for Canadian social democracy; North America's first social-democratic government was elected in 1944. The province's economy is based on agriculture, mining, and energy.

The former Lieutenant Governor, Thomas Molloy, died in office on July 2, 2019. On July 17, 2019, the federal government announced the appointment of Russell Mirasty, former Assistant Commissioner with the Royal Canadian Mounted Police, as the new Lieutenant Governor. The current premier is Scott Moe.

In 1992, the federal and provincial governments signed a historic land claim agreement with First Nations in Saskatchewan. The First Nations received compensation and were permitted to buy land on the open market for the bands; they have acquired about , now reserve lands. Some First Nations have used their settlement to invest in urban areas, including Saskatoon.

Its name derived from the Saskatchewan River. The river was known as ("swift flowing river") in the Cree language.

As Saskatchewan's borders largely follow the geographic coordinates of longitude and latitude, the province is roughly a quadrilateral, or a shape with four sides. However, the 49th parallel boundary and the 60th northern border appear curved on globes and many maps. Additionally, the eastern boundary of the province is partially crooked rather than following a line of longitude, as correction lines were devised by surveyors prior to the homestead program (1880–1928).

Saskatchewan is part of the Western Provinces and is bounded on the west by Alberta, on the north by the Northwest Territories, on the north-east by Nunavut, on the east by Manitoba, and on the south by the U.S. states of Montana and North Dakota. Saskatchewan has the distinction of being the only Canadian province for which no borders correspond to physical geographic features (i.e. they are all parallels and meridians). Along with Alberta, Saskatchewan is one of only two land-locked provinces.

The overwhelming majority of Saskatchewan's population is located in the southern third of the province, south of the 53rd parallel.

Saskatchewan contains two major natural regions: the Boreal Forest in the north and the Prairies in the south. They are separated by an aspen parkland transition zone near the North Saskatchewan River on the western side of the province, and near to south of the Saskatchewan River on the eastern side. Northern Saskatchewan is mostly covered by forest except for the Lake Athabasca Sand Dunes, the largest active sand dunes in the world north of 58°, and adjacent to the southern shore of Lake Athabasca. Southern Saskatchewan contains another area with sand dunes known as the "Great Sand Hills" covering over . The Cypress Hills, located in the southwestern corner of Saskatchewan and Killdeer Badlands (Grasslands National Park), are areas of the province that were unglaciated during the last glaciation period, the Wisconsin glaciation.

The province's highest point, at , is located in the Cypress Hills less than 2 km from the provincial boundary with Alberta. The lowest point is the shore of Lake Athabasca, at . The province has 14 major drainage basins made up of various rivers and watersheds draining into the Arctic Ocean, Hudson Bay and the Gulf of Mexico.

Saskatchewan receives more hours of sunshine than any other Canadian province. The province lies far from any significant body of water. This fact, combined with its northerly latitude, gives it a warm summer, corresponding to its humid continental climate (Köppen type "Dfb") in the central and most of the eastern parts of the province, as well as the Cypress Hills; drying off to a semi-arid steppe climate (Köppen type "BSk") in the southwestern part of the province. Drought can affect agricultural areas during long periods with little or no precipitation at all. The northern parts of Saskatchewan – from about La Ronge northward – have a subarctic climate (Köppen "Dfc") with a shorter summer season. Summers can get very hot, sometimes above during the day, and with humidity decreasing from northeast to southwest. Warm southern winds blow from the plains and intermontane regions of the Western United States during much of July and August, very cool or hot but changeable air masses often occur during spring and in September. Winters are usually bitterly cold, with frequent Arctic air descending from the north. with high temperatures not breaking for weeks at a time. Warm chinook winds often blow from the west, bringing periods of mild weather. Annual precipitation averages 30 to 45 centimetres (12 to 18 inches) across the province, with the bulk of rain falling in June, July, and August.

Saskatchewan is one of the most tornado-active parts of Canada, averaging roughly 12 to 18 tornadoes per year, some violent. In 2012, 33 tornadoes were reported in the province. The Regina Cyclone took place in June 1912 when 28 people died in an F4 Fujita scale tornado. Severe and non-severe thunderstorm events occur in Saskatchewan, usually from early spring to late summer. Hail, strong winds and isolated tornadoes are a common occurrence.

The hottest temperature ever recorded anywhere in Canada happened in Saskatchewan. The temperature rose to in Midale and Yellow Grass. The coldest ever recorded in the province was in Prince Albert, which is north of Saskatoon.

The effects of climate change in Saskatchewan are now being observed in parts of the province. There is evidence of reduction of biomass in Saskatchewan's boreal forests (as with those of other Canadian prairie provinces) is linked by researchers to drought-related water stress, stemming from global warming, most likely caused by greenhouse gas emissions. While studies, as early as 1988 (Williams, et al., 1988) have shown climate change will affect agriculture, whether the effects can be mitigated through adaptations of cultivars, or crops, is less clear. Resiliency of ecosystems may decline with large changes in temperature. The provincial government has responded to the threat of climate change by introducing a plan to reduce carbon emissions, "The Saskatchewan Energy and Climate Change Plan," in June 2007.

Saskatchewan has been populated by various indigenous peoples of North America, including members of the Sarcee, Niitsitapi, Atsina, Cree, Saulteaux, Assiniboine (Nakoda), Lakota and Sioux. The first known European to enter Saskatchewan was Henry Kelsey in 1690, who travelled up the Saskatchewan River in hopes of trading fur with the region's indigenous peoples. The first permanent European settlement was a Hudson's Bay Company post at Cumberland House, founded in 1774 by Samuel Hearne. In 1762 the south of the province was part of the Spanish Louisiana until 1802.

In 1803 the Louisiana Purchase transferred from France to the United States part of what is now Alberta and Saskatchewan. In 1818 the U.S. ceded the area to Britain. Most of what is now Saskatchewan was part of Rupert's Land and controlled by the Hudson's Bay Company, which claimed rights to all watersheds flowing into Hudson Bay, including the Saskatchewan River, Churchill, Assiniboine, Souris, and Qu'Appelle River systems.

In the late 1850s and early 1860s, scientific expeditions led by John Palliser and Henry Youle Hind explored the prairie region of the province.

In 1870, Canada acquired the Hudson's Bay Company's territories and formed the North-West Territories to administer the vast territory between British Columbia and Manitoba. The Crown also entered into a series of numbered treaties with the indigenous peoples of the area, which serve as the basis of the relationship between First Nations, as they are called today, and the Crown. Since the late twentieth century, land losses and inequities as a result of those treaties have been subject to negotiation for settlement between the First Nations in Saskatchewan and the federal government, in collaboration with provincial governments.

In 1876, following their defeat of United States Army forces at the Battle of the Little Bighorn in Montana Territory in the United States, the Lakota Chief Sitting Bull led several thousand of his people to Wood Mountain. Survivors and descendants founded Wood Mountain Reserve in 1914.

The North-West Mounted Police set up several posts and forts across Saskatchewan, including Fort Walsh in the Cypress Hills, and Wood Mountain Post in south-central Saskatchewan near the United States border.

Many Métis people, who had not been signatories to a treaty, had moved to the Southbranch Settlement and Prince Albert district north of present-day Saskatoon following the Red River Rebellion in Manitoba in 1870. In the early 1880s, the Canadian government refused to hear the Métis' grievances, which stemmed from land-use issues. Finally, in 1885, the Métis, led by Louis Riel, staged the North-West Rebellion and declared a provisional government. They were defeated by a Canadian militia brought to the Canadian prairies by the new Canadian Pacific Railway. Riel, who surrendered and was convicted of treason in a packed Regina courtroom, was hanged on November 16, 1885. Since then, the government has recognized the Métis as an aboriginal people with status rights and provided them with various benefits.

The national policy set by the federal government, the Canadian Pacific Railway, the Hudson's Bay Company and associated land companies encouraged immigration. The "Dominion Lands Act" of 1872 permitted settlers to acquire one quarter of a square mile of land to homestead and offered an additional quarter upon establishing a homestead. In 1874, the North-West Mounted Police began providing police services. In 1876, the "North-West Territories Act" provided for appointment, by the Ottawa, of a Lieutenant Governor and a Council to assist him.

Highly optimistic advertising campaigns promoted the benefits of prairie living. Potential immigrants read leaflets information painted Canada as a veritable garden of Eden and downplayed the need for agricultural expertise. Ads in "The Nor'-West Farmer" by the Commissioner of Immigration implied that western land was blessed with water, wood, gold, silver, iron, copper, and cheap coal for fuel, all of which were readily at hand. Reality was far harsher, especially for the first arrivals who lived in sod houses. However eastern money poured in and by 1913, long term mortgage loans to Saskatchewan farmers had reached $65 million.

The dominant groups comprised British settlers from eastern Canada and Britain, who comprised about half of the population during the late 19th and early 20th centuries. They played the leading role in establishing the basic institutions of plains society, economy and government.

Gender roles were sharply defined. Men were primarily responsible for breaking the land; planting and harvesting; building the house; buying, operating and repairing machinery; and handling finances. At first, there were many single men on the prairie, or husbands whose wives were still back east, but they had a hard time. They realized the need for a wife. In 1901, there were 19,200 families, but this surged to 150,300 families only 15 years later. Wives played a central role in settlement of the prairie region. Their labor, skills, and ability to adapt to the harsh environment proved decisive in meeting the challenges. They prepared bannock, beans and bacon, mended clothes, raised children, cleaned, tended the garden, helped at harvest time and nursed everyone back to health. While prevailing patriarchal attitudes, legislation, and economic principles obscured women's contributions, the flexibility exhibited by farm women in performing productive and nonproductive labor was critical to the survival of family farms, and thus to the success of the wheat economy.

On September 1, 1905, Saskatchewan became a province, with inauguration day held September 4. Its political leaders at the time proclaimed its destiny was to become Canada's most powerful province. Saskatchewan embarked on an ambitious province-building program based on its Anglo-Canadian culture and wheat production for the export market. Population quintupled from 91,000 in 1901 to 492,000 to 1911, thanks to heavy immigration of farmers from the Ukraine, U.S., Germany and Scandinavia. Efforts were made to assimilate the newcomers to British Canadian culture and values.

In the 1905 provincial elections, Liberals won 16 of 25 seats in Saskatchewan. The Saskatchewan government bought out Bell Telephone Company in 1909, with the government owning the long-distance lines and left local service to small companies organized at the municipal level. Premier Walter Scott preferred government assistance to outright ownership because he thought enterprises worked better if citizens had a stake in running them; he set up the Saskatchewan Cooperative Elevator Company in 1911. Despite pressure from farm groups for direct government involvement in the grain handling business, the Scott government opted to loan money to a farmer-owned elevator company. Saskatchewan in 1909 provided bond guarantees to railway companies for the construction of branch lines, alleviating the concerns of farmers who had trouble getting their wheat to market by wagon. The Saskatchewan Grain Growers Association, was the dominant political force in the province until the 1920s; it had close ties with the governing Liberal party. In 1913, the Saskatchewan Stock Growers Association was established with three goals: to watch over legislation; to forward the interests of the stock growers in every honourable and legitimate way; and to suggest to parliament legislation to meet changing conditions and requirements.

Immigration peaked in 1910, and in spite of the initial difficulties of frontier life – distance from towns, sod homes, and backbreaking labour – new settlers established a European-Canadian style of prosperous agrarian society. The long-term prosperity of the province depended on the world price of grain, which headed steadily upward from the 1880s to 1920, then plunged down. Wheat output was increased by new strains, such as the "Marquis wheat" strain which matured 8 days sooner and yielded 7 more bushels per acre (0.72 m/ha) than the previous standard, "Red Fife". The national output of wheat soared from in 1896, to in 1901, reaching by 1921.

Urban reform movements in Regina were based on support from business and professional groups. City planning, reform of local government, and municipal ownership of utilities were more widely supported by these two groups, often through such organizations as the Board of Trade. Church-related and other altruistic organizations generally supported social welfare and housing reforms; these groups were generally less successful in getting their own reforms enacted.

The province responded to the First World War in 1914 with patriotic enthusiasm and enjoyed the resultant economic boom for farms and cities alike. Emotional and intellectual support for the war emerged from the politics of Canadian national identity, the rural myth, and social gospel progressivism The Church of England was especially supportive. However, there was strong hostility toward German-Canadian farmers. Recent Ukrainian immigrants were enemy aliens because of their citizenship in the Austro-Hungarian Empire. A small fraction were taken to internment camps. Most of the internees were unskilled unemployed labourers who were imprisoned "because they were destitute, not because they were disloyal."

The price of wheat tripled and acreage seeded doubled. The wartime spirit of sacrifice intensified social reform movements that had predated the war and now came to fruition. Saskatchewan gave women the right to vote in 1916 and at the end of 1916 passed a referendum to prohibit the sale of alcohol.

In the late 1920s, the Ku Klux Klan, imported from the United States and Ontario, gained brief popularity in nativist circles in Saskatchewan and Alberta. The Klan, briefly allied with the provincial Conservative party because of their mutual dislike for Premier James G. "Jimmy" Gardiner and his Liberals (who ferociously fought the Klan), enjoyed about two years of prominence. It declined and disappeared, subject to widespread political and media opposition, plus internal scandals involving the use of the organization's funds.

In 1970, the first annual Canadian Western Agribition was held in Regina. This farm-industry trade show, with its strong emphasis on livestock, is rated as one of the five top livestock shows in North America, along with those in Houston, Denver, Louisville and Toronto.

The province celebrated the 75th anniversary of its establishment in 1980, with Princess Margaret, Countess of Snowdon, presiding over the official ceremonies. In 2005, 25 years later, her sister, Queen Elizabeth II, attended the events held to mark Saskatchewan's centennial.

Since the late 20th century, First Nations have become more politically active in seeking justice for past inequities, especially related to the taking of indigenous lands by various governments. The federal and provincial governments have negotiated on numerous land claims, and developed a program of "Treaty Land Entitlement", enabling First Nations to buy land to be taken into reserves with money from settlements of claims.
"In 1992, the federal and provincial governments signed a historic land claim agreement with Saskatchewan First Nations. Under the Agreement, the First Nations received money to buy land on the open market. As a result, about 761,000 acres have been turned into reserve land and many First Nations continue to invest their settlement dollars in urban areas", including Saskatoon. The money from such settlements has enabled First Nations to invest in businesses and other economic infrastructure.

According to the Canada 2011 Census, the largest ethnic group in Saskatchewan is German (28.6%), followed by English (24.9%), Scottish (18.9%), Canadian (18.8%), Irish (15.5%), Ukrainian (13.5%), French (Fransaskois) (12.2%), First Nations (12.1%), Norwegian (6.9%), and Polish (5.8%).

The largest denominations by number of adherents according to the 2001 census were the Roman Catholic Church with 286,815 (30%); the United Church of Canada with 187,450 (20%); and the Evangelical Lutheran Church in Canada with 78,520 (8%). 148,535 (15.4%) responded "no religion".

Ten largest municipalities by population

This list does not include Lloydminster, which has a total population of 31,410 but straddles the Alberta–Saskatchewan border. As of 2016, 11,765 people lived on the Saskatchewan side, which would make it Saskatchewan's 8th largest municipality. All of the listed communities are considered cities by the province; municipalities in the province with a population of 5,000 or more can receive official city status.

Historically, Saskatchewan's economy was primarily associated with agriculture, with wheat being the precious symbol on the province's flag. Increasing diversification has resulted in agriculture, forestry, fishing, and hunting only making up 8.9% of the province's GDP in 2018. Saskatchewan grows a large portion of Canada's grain. In 2017, the production of canola surpassed the production of wheat, which is Saskatchewan's most familiar crop and the one most often associated with the province. Total net income from farming was $3.3 billion in 2017, which was $0.9 billion less than the income in 2016. Other grains such as flax, rye, oats, peas, lentils, canary seed, and barley are also produced in the province. Saskatchewan is the world's largest exporter of mustard seed. Beef cattle production by a Canadian province is only exceeded by Alberta. In the northern part of the province, forestry is also a significant industry. 

Mining is a major industry in the province, with Saskatchewan being the world's largest exporter of potash and uranium. Oil and natural gas production is also a very important part of Saskatchewan's economy, although the oil industry is larger. Among Canadian provinces, only Alberta exceeds Saskatchewan in overall oil production. Heavy crude is extracted in the Lloydminster-Kerrobert-Kindersley areas. Light crude is found in the Kindersley-Swift Current areas as well as the Weyburn-Estevan fields. Natural gas is found almost entirely in the western part of Saskatchewan, from the Primrose Lake area through Lloydminster, Unity, Kindersley, Leader, and around Maple Creek areas.

A list of the companies includes The Potash Corporation of Saskatchewan (defunct in December 2017), Federated Cooperatives Ltd. and IPSCO.

Major Saskatchewan-based Crown corporations are Saskatchewan Government Insurance (SGI), SaskTel, SaskEnergy (the province's main supplier of natural gas), and SaskPower. Bombardier runs the NATO Flying Training Centre at 15 Wing, near Moose Jaw. Bombardier was awarded a long-term contract in the late 1990s for $2.8 billion from the federal government for the purchase of military aircraft and the running of the training facility. SaskPower since 1929 has been the principal supplier of electricity in Saskatchewan, serving more than 451,000 customers and managing $4.5 billion in assets. SaskPower is a major employer in the province with almost 2,500 permanent full-time staff located in 71 communities.

The Tabulated Data covers each fiscal year (e.g. 2015–2016 covers April 1, 2015 – March 31, 2016).
All data is in $1,000s.

"Source: Government of Saskatchewan."

Publicly funded elementary and secondary schools in the province are administered by the Saskatchewan Ministry of Education. Public elementary and secondary schools either operate as secular or as a separate schools. Nearly all school divisions, except one operate as an English first language school board. The Division scolaire francophone No. 310 is the only school division that operates French first language schools. In addition to elementary and secondary schools, the province is also home to several post-secondary institutions.

The first education on the prairies took place within the family groups of the First Nation and early fur trading settlers. There were only a few missionary or trading post schools established in Rupert's Land – later known as the North West Territories. The first 76 North-West Territories school districts and the first Board of Education meeting formed in 1886. The pioneering boom formed ethnic bloc settlements. Communities were seeking education for their children similar to the schools of their home land. Log cabins, and dwellings were constructed for the assembly of the community, school, church, dances and meetings.

The prosperity of the Roaring Twenties and the success of farmers in proving up on their homesteads helped provide funding to standardize education. Textbooks, normal schools for educating teachers, formal school curricula and state of the art school house architectural plans provided continuity throughout the province. English as the school language helped to provide economic stability because one community could communicate with another and goods could be traded and sold in a common language. The number of one-room schoolhouse districts across Saskatchewan totalled approximately 5,000 at the height of this system of education in the late 1940s.

Following World War II, the transition from many one-room schoolhouses to fewer and larger consolidated modern technological town and city schools occurred as a means of ensuring technical education. School buses, highways, and family vehicles create ease and accessibility of a population shift to larger towns and cities. Combines and tractors mean the farmer could manage more than a quarter section of land, so there was a shift from family farms and subsistence crops to cash crops grown on many sections of land. School vouchers have been newly proposed as a means of allowing competition between rural schools and making the operation of co-operative schools practicable in rural areas.

Saskatchewan's Ministry of Health is responsible for policy direction, sets and monitors standards, and provides funding for regional health authorities and provincial health services. Saskatchewan's medical health system is widely and inaccurately characterized as "socialized medicine": medical practitioners in Saskatchewan, as in other Canadian provinces, are not civil servants but remit their accounts to the publicly funded Saskatchewan Medical Care Insurance Plan rather than to patients (i.e. a single-payer system).

Saskatchewan medical health system has faced criticism due to a lack of accessibility to the midwifery program. According to Leanne Smith, the director for maternal services in the Saskatoon Health Region declared half of the women who apply for the midwifery program are turned away. Ministry of Health data shows midwives saw 1,233 clients in the 2012–13 fiscal year (which runs April to March). But in that fourth quarter, 359 women were still on waiting lists for immediate or future care. The provincial Health Ministry received 47 letters about midwifery services in 2012, most of which asked for more midwives. As a continuing problem in the Saskatchewan health care system, more pressure has been placed to recruit more midwives for the province.

Saskatchewan has the same form of government as the other Canadian provinces with a lieutenant-governor (who is the representative of the Queen in Right of Saskatchewan), premier, and a unicameral legislature.

During the 20th century, Saskatchewan was one of Canada's more left-wing provinces, reflecting the slant of its many rural citizens which distrusted the distant capital government and which favored a strong local government to attend to their issues. In 1944 Tommy Douglas became premier of the first avowedly socialist regional government in North America. Most of his Members of the Legislative Assembly (MLAs) represented rural and small-town ridings. Under his Cooperative Commonwealth Federation government, Saskatchewan became the first province to have Medicare. In 1961, Douglas left provincial politics to become the first leader of the federal New Democratic Party. In the 21st century, Saskatchewan began to drift to the right-wing, generally attributed to the province's economy shifting toward oil and gas production. In the 2015 federal election, the Conservative Party of Canada won ten of the province's fourteen seats, followed by the New Democratic Party with three and the Liberal Party of Canada with one; in the 2019 election, the Conservatives won in all of Saskatchewan's 14 seats, sweeping their competition.

Provincial politics in Saskatchewan is dominated by the social-democratic Saskatchewan New Democratic Party and the centre-right Saskatchewan Party, with the latter holding the majority in the Legislative Assembly of Saskatchewan since 2007. The current Premier of Saskatchewan is Scott Moe, who took over the leadership of the Saskatchewan Party in 2018 following the resignation of Brad Wall. Numerous smaller political parties also run candidates in provincial elections, including the Green Party of Saskatchewan, Liberal Party of Saskatchewan, and the Progressive Conservative Party of Saskatchewan, but none is currently represented in the Legislative Assembly (federal Conservatives and Liberals generally favour the Saskatchewan Party in provincial elections).

No Prime Minister of Canada has been born in Saskatchewan, but two (William Lyon Mackenzie King and John Diefenbaker) represented the province in the House of Commons of Canada during their tenures as head of government.

Transportation in Saskatchewan includes an infrastructure system of roads, highways, freeways, airports, ferries, pipelines, trails, waterways and railway systems serving a population of approximately 1,003,299 (according to 2007 estimates) inhabitants year-round. It is funded primarily with local and federal government funds. The Saskatchewan Department of Highways and Transportation estimates 80% of traffic is carried on the 5,031-kilometre principal system of highways.

The Ministry of Highways and Infrastructure operates over of highways and divided highways. There are also municipal roads which comprise different surfaces. Asphalt concrete pavements comprise almost , granular pavement almost , non structural or thin membrane surface TMS are close to and finally gravel highways make up over through the province. In the northern sector, ice roads which can only be navigated in the winter months comprise another approximately of travel.

Saskatchewan has over 250,000 kilometres (150,000 mi) of roads and highways, the highest length of road surface of any Canadian province. The major highways in Saskatchewan are the Trans Canada expressway, Yellowhead Highway northern Trans Canada route, Louis Riel Trail, CanAm Highway, Red Coat Trail, Northern Woods and Water route, and Saskota travel route.

The first Canadian transcontinental railway was constructed by the Canadian Pacific Railway between 1881 and 1885. After the great east-west transcontinental railway was built, north-south connector branch lines were established.
The 1920s saw the largest rise in rail line track as the CPR and CNR fell into competition to provide rail service within ten kilometres. In the 1960s there were applications for abandonment of branch lines. Today the only two passenger rail services in the province are "The Canadian" and Winnipeg–Churchill train, both operated by Via Rail. "The Canadian" is a transcontinental service linking Toronto with Vancouver.

The main Saskatchewan waterways are the North Saskatchewan River or South Saskatchewan River routes. In total, there are 3,050 bridges maintained by the Department of Highways in Saskatchewan. There are currently twelve ferry services operating in the province, all under the jurisdiction of the Department of Highways.

The Saskatoon Airport (YXE) was initially established as part of the Royal Canadian Air Force training program during World War II. It was renamed the "John G. Diefenbaker Airport" in the official ceremony, June 23, 1993. "Roland J. Groome Airfield" is the official designation for the Regina International Airport (YQR) as of August 3, 2005; the airport was established in 1930. Under the British Commonwealth Air Training Plan (BCATP), twenty Service Flying Training Schools (RAF) were established at various Saskatchewan locations in World War II. 15 Wing Moose Jaw is home to the Canadian Forces formation aerobatics team, the "Snowbirds".

Airlines offering service to Saskatchewan are Air Canada, WestJet Airlines, United Airlines, Delta Air Lines, Transwest Air, Sunwing Airlines, Norcanair Airlines, La Ronge Aviation Services Ltd, La Loche Airways, Osprey Wings Ltd, Buffalo Narrows Airways Ltd, Île-à-la-Crosse Airways Ltd, Voyage Air, Pronto Airways, Venture Air Ltd, Pelican Narrows Air Service, Jackson Air Services Ltd, and Northern Dene Airways Ltd.

The Government of Canada has agreed to contribute $20 million for two new interchanges in Saskatoon. One of them being at the Sk Hwy 219 / Lorne Ave intersection with Circle Drive, the other at the Senator Sid Buckwold Bridge (Idylwyld Freeway) and Circle Drive. This is part of the Asia-Pacific Gateway and Corridor Initiative to improve access to the Canadian National Railway's intermodal freight terminal thereby increasing Asia-Pacific trade. Also, the Government of Canada will contribute $27 million to Regina to construct a Canadian Pacific Railway CPR intermodal facility and improve infrastructure transportation to the facility from both national highway networks, Sk Hwy 1, the TransCanada Highway and Sk Hwy 11, Louis Riel Trail. This also is part of the Asia-Pacific Gateway and Corridor Initiative to improve access to the CPR terminal and increase Asia-Pacific trade.

Saskatchewan is home to a number of museums. The Royal Saskatchewan Museum serves as the provincial museum of the province. Other museums include Diefenbaker House, Evolution of Education Museum, Museum of Antiquities, the RCMP Heritage Centre, Rotary Museum of Police and Corrections, Saskatchewan Science Centre, Saskatchewan Western Development Museum, and the T.rex Discovery Centre.

The province is home to several art galleries, including MacKenzie Art Gallery, and Remai Modern. The province is also home to several performing arts centres including the Conexus Arts Centre in Regina, and TCU Place in Saskatoon. PAVED Arts, a new media artist-run space, is also located in Saskatoon. The province is presently home to several concert orchestras, the Regina Symphony Orchestra, the Saskatoon Symphony Orchestra, and the Saskatoon Youth Orchestra. The Regina Symphony Orchestra is at the Conexus Arts Centre, while the Saskatoon perform at TCU Place.

The Saskatchewan Roughriders Canadian football team is the province's professional football franchise (playing in the Canadian Football League), and are extremely popular across Saskatchewan. The team's fans are also found to congregate on game days throughout Canada, and collectively they are known as "Rider Nation". The province's other major sport franchise is the Saskatchewan Rush of the National Lacrosse League. In their first year of competition, 2016, the Rush won both their Division Title and the League Championship.

Hockey is the most popular sport in the province. More than 490 NHL players have been born in Saskatchewan, the highest per capita output of any Canadian province, U.S. state, or European country. Notable NHL figures born in Saskatchewan include Keith Allen, Gordie Howe, Bryan Trottier, Bernie Federko, Clark Gillies, Fern Flaman, Bert Olmstead, Harry Watson, Elmer Lach, Max Bentley, Sid Abel, Doug Bentley, Eddie Shore, Clint Smith, Bryan Hextall, Johnny Bower, Emile Francis, Glenn Hall, Chuck Rayner, Brad McCrimmon, Patrick Marleau, Dave Manson, Theo Fleury, Terry Harper, Wade Redden, Brian Propp, Scott Hartnell, Ryan Getzlaf, and Chris Kunitz. Saskatchewan does not have an NHL or minor professional franchise, but five teams in the junior Western Hockey League are located in the province: the Moose Jaw Warriors, Prince Albert Raiders, Regina Pats, Saskatoon Blades and Swift Current Broncos.

In 2015, Budweiser honoured Saskatchewan for their abundance of hockey players by sculpting a 12-foot-tall hockey player monument in ice for Saskatchewan's capital city of Regina. The company then filmed this frozen monument for a national television commercial, thanking the province for creating so many goal scorers throughout hockey's history. Budweiser also gifted the “hockey player” province a trophy made of white birch—Saskatchewan's provincial tree—which bears the name of every pro player in history. Sitting atop the trophy was a golden Budweiser Red Light, synched to every current Saskatchewan player in the pros. This trophy can currently be seen at Victoria Bar in Regina.

Historically, Saskatchewan has been one of the strongest curling provinces. Teams from Saskatchewan have finished in the top three places at 38 briers and Saskatchewan has more women's championships than any other province with 11. Notable curlers from Saskatchewan include Sandra Schmirler, Ernie Richardson, and Vera Pezer. In a 2019 TSN poll, experts ranked Schmirler's Saskatchewan team, which won a gold medal at the 1998 Olympics, as the greatest women's team in Canada's history.

The flag of Saskatchewan was officially adopted on September 22, 1969. The flag features the provincial shield in the upper quarter nearest the staff, with the floral emblem, the Prairie Lily, in the fly. The upper green (in forest green) half of the flag represents the northern Saskatchewan forest lands, while the golden lower half of the flag symbolizes the southern wheat fields and prairies. A province-wide competition was held to design the flag, and drew over 4,000 entries. The winning design was by Anthony Drake, then living in Hodgeville.

In 2005, Saskatchewan Environment held a province-wide vote to recognize Saskatchewan's centennial year, receiving more than 10,000 online and mail-in votes from the public. The walleye was the overwhelming favourite of the six native fish species nominated for the designation, receiving more than half the votes cast. Other species in the running were the lake sturgeon, lake trout, lake whitefish, northern pike and yellow perch.

Saskatchewan's other symbols include the tartan, the license plate, and the provincial flower. Saskatchewan's official tartan was registered with the Court of Lord Lyon King of Arms in Scotland in 1961. It has seven colours: gold, brown, green, red, yellow, white and black. The provincial licence plates display the slogan "Land of Living Skies". The provincial flower of Saskatchewan is the Western Red Lily.

In 2005, Saskatchewan celebrated its centennial. To honour it, the Royal Canadian Mint issued a commemorative five-dollar coin depicting Canada's wheat fields as well as a circulation 25-cent coin of a similar design. Queen Elizabeth II and Prince Philip visited Regina, Saskatoon, and Lumsden, and the Saskatchewan-reared Joni Mitchell issued an album in Saskatchewan's honour.






</doc>
<doc id="26841" url="https://en.wikipedia.org/wiki?curid=26841" title="Summer solstice (disambiguation)">
Summer solstice (disambiguation)

Summer solstice is the astronomical phenomenon that occurs on the longest day of the year.

Summer solstice may also refer to:



</doc>
<doc id="26842" url="https://en.wikipedia.org/wiki?curid=26842" title="Salting">
Salting

Salting or Salted may refer to:





</doc>
<doc id="26847" url="https://en.wikipedia.org/wiki?curid=26847" title="Socialism">
Socialism

Socialism is a political, social, and economic philosophy encompassing a range of economic and social systems characterised by social ownership of the means of production and workers' self-management of enterprise as well as the political theories and movements associated with such systems. Social ownership can be public, collective or cooperative ownership, or citizen ownership of equity. There are many varieties of socialism and there is no single definition encapsulating all of them, with social ownership being the common element shared by its various forms.
Socialist systems are divided into non-market and market forms. Non-market socialism involves replacing factor markets and money with engineering and technical criteria based on calculation performed in-kind, thereby producing an economic mechanism that functions according to different economic laws from those of capitalism. Non-market socialism aims to circumvent the inefficiencies and crises traditionally associated with capital accumulation and the profit system. The socialist calculation debate, originated by the economic calculation problem, concerns the feasibility and methods of resource allocation for a planned socialist system. By contrast, market socialism retains the use of monetary prices, factor markets and in some cases the profit motive, with respect to the operation of socially owned enterprises and the allocation of capital goods between them. Profits generated by these firms would be controlled directly by the workforce of each firm, or accrue to society at large in the form of a social dividend. 

Socialist politics has been both internationalist and nationalist in orientation; organised through political parties and opposed to party politics; at times overlapping with trade unions and at other times independent and critical of them; and present in both industrialised and developing nations. Social democracy originated within the socialist movement, supporting economic and social interventions to promote social justice. While having socialism as a long-term goal, it has come to embrace a Keynesian mixed economy within a predominantly, developed capitalist market economy and liberal democratic polity that includes substantial state intervention in the form of income redistribution, regulation and a welfare state. Economic democracy proposes a sort of market socialism, with more democratic control of companies, currencies, investments and natural resources.
The socialist political movement includes a set of political philosophies that originated in the revolutionary movements of the mid-to-late 18th century and out of concern for the social problems that were associated with capitalism. By the late 19th century, after the work of Karl Marx and his collaborator Friedrich Engels, socialism had come to signify opposition to capitalism and advocacy for a post-capitalist system based on some form of social ownership of the means of production. By the 1920s, social democracy and communism had become the two dominant political tendencies within the international socialist movement, with socialism itself becoming "the most influential secular movement of the twentieth century". While the emergence of the Soviet Union as the world's first nominally socialist state led to socialism's widespread association with the Soviet economic model, some economists and intellectuals argued that in practice the model functioned as a form of state capitalism, or a non-planned administrative or command economy. Socialist parties and ideas remain a political force with varying degrees of power and influence on all continents, heading national governments in many countries around the world. Today, many socialists have also adopted the causes of other social movements such as environmentalism, feminism, and progressivism.

For Andrew Vincent, "[t]he word 'socialism' finds its root in the Latin "sociare", which means to combine or to share. The related, more technical term in Roman and then medieval law was "societas". This latter word could mean companionship and fellowship as well as the more legalistic idea of a consensual contract between freemen".

The term socialism was created by Henri de Saint-Simon, one of the founders of what would later be labelled utopian socialism. Simon coined the term as a contrast to the liberal doctrine of individualism which stressed that people act or should act as if they are in isolation from one another. The original utopian socialists condemned liberal individualism for failing to address social concerns during the industrial revolution, including poverty, social oppression and gross inequalities in wealth, viewing liberal individualism as degenerating society into supporting selfish egoism that harmed community life through promoting a society based on competition. They presented socialism as an alternative to liberal individualism based on the shared ownership of resources, although their proposals for socialism differed significantly. Saint-Simon proposed economic planning, scientific administration and the application of modern scientific advancements to the organisation of society. By contrast, Robert Owen proposed the organisation of production and ownership in cooperatives.

The term socialism is also attributed to Pierre Leroux and to Marie Roch Louis Reybaud in France; and to Robert Owen in Britain who became one of the fathers of the cooperative movement.

The modern definition and usage of socialism settled by the 1860s, becoming the predominant term among the group of words associationist, co-operative and mutualist which had previously been used as synonyms. The term communism also fell out of use during this period despite earlier distinctions between socialism and communism from the 1840s. An early distinction between socialism and communism was that the former aimed to only socialise production while the latter aimed to socialise both production and consumption (in the form of free access to final goods). However, Marxists employed the term socialism in place of communism by 1888 which had come to be considered an old-fashion synonym for socialism. It was not until 1917 after the Bolshevik Revolution that socialism came to refer to a distinct stage between capitalism and communism, introduced by Vladimir Lenin as a means to defend the Bolshevik seizure of power against traditional Marxist criticisms that Russia's productive forces were not sufficiently developed for socialist revolution.

A distinction between communist and socialist as descriptors of political ideologies arose in 1918 after the Russian Social-Democratic Labour Party renamed itself to the All-Russian Communist Party, where communist came to specifically mean socialists who supported the politics and theories of Leninism, Bolshevism and later Marxism–Leninism, although communist parties continued to describe themselves as socialists dedicated to socialism.

The words socialism and communism eventually accorded with the adherents' and opponents' cultural attitude towards religion. In Christian Europe, communism was believed to be the atheist way of life. In Protestant England, the word communism was too culturally and aurally close to the Roman Catholic communion rite, hence English atheists denoted themselves socialists. Friedrich Engels argued that in 1848, at the time when "The Communist Manifesto" was published, that "socialism was respectable on the continent, while communism was not". The Owenites in England and the Fourierists in France were considered respectable socialists while working-class movements that "proclaimed the necessity of total social change" denoted themselves communists. This latter branch of socialism produced the communist work of Étienne Cabet in France and Wilhelm Weitling in Germany. The British moral philosopher John Stuart Mill also came to advocate a form of economic socialism within a liberal context. In later editions of his "Principles of Political Economy" (1848), Mill would argue that "as far as economic theory was concerned, there is nothing in principle in economic theory that precludes an economic order based on socialist policies". While democrats looked to the Revolutions of 1848 as a democratic revolution which in the long run ensured liberty, equality and fraternity, Marxists denounced 1848 as a betrayal of working-class ideals by a bourgeoisie indifferent to the legitimate demands of the proletariat.

Socialist models and ideas espousing common or public ownership have existed since antiquity. The economy of the 3rd century BCE Mauryan Empire of India has been described as "a socialized monarchy" and "a sort of state socialism". Although controversially, it has been claimed that there were elements of socialist thought in the politics of classical Greek philosophers Plato and Aristotle. Mazdak the Younger (died c. 524 or 528 CE), a Persian communal proto-socialist, instituted communal possessions and advocated the public good. Abū Dharr al-Ghifārī, a Companion of Prophet Muhammad, is credited by many as a principal antecedent of Islamic socialism. The teachings of Jesus the messiah of the Christian religion are frequently highlighted as socialist in nature, though this is disputed. Acts 4:35 records that the early church in Jerusalem 'No one claimed that any of their possessions was their own'; however the pattern soon disappears from church history except within monasticism. Christian socialism was one of the founding threads of the UK Labour Party and is said to be a tradition going back 600 years to the uprising of Wat Tyler and John Ball. In the period right after the French Revolution, activists and theorists like François-Noël Babeuf, Étienne-Gabriel Morelly, Philippe Buonarroti and Auguste Blanqui influenced the early French labour and socialist movements. In Britain, Thomas Paine proposed a detailed plan to tax property owners to pay for the needs of the poor in "Agrarian Justice" while Charles Hall wrote "The Effects of Civilization on the People in European States", denouncing capitalism's effects on the poor of his time which influenced the utopian schemes of Thomas Spence.

The first self-conscious socialist movements developed in the 1820s and 1830s. The Owenites, Saint-Simonians and Fourierists provided a series of coherent analyses and interpretations of society. They also, especially in the case of the Owenites, overlapped with a number of other working-class movements like the Chartists in the United Kingdom. The Chartists gathered significant numbers around the People's Charter of 1838, which sought a number of democratic reforms focused on the extension of suffrage to all male adults. Leaders in the movement also called for a more equitable distribution of income and better living conditions for the working classes. The very first trade unions and consumers' cooperative societies also emerged in the hinterland of the Chartist movement as a way of bolstering the fight for these demands. A later important socialist thinker in France was Pierre-Joseph Proudhon, who proposed his philosophy of mutualism in which "everyone had an equal claim, either alone or as part of a small cooperative, to possess and use land and other resources as needed to make a living". There were also currents inspired by dissident Christianity of Christian socialism "often in Britain and then usually coming out of left liberal politics and a romantic anti-industrialism" which produced theorists such as Edward Bellamy, Frederick Denison Maurice and Charles Kingsley.

The first advocates of socialism favoured social levelling in order to create a meritocratic or technocratic society based on individual talent. Count Henri de Saint-Simon is regarded as the first individual to coin the term "socialism". Saint-Simon was fascinated by the enormous potential of science and technology and advocated a socialist society that would eliminate the disorderly aspects of capitalism and would be based on equal opportunities. He advocated the creation of a society in which each person was ranked according to his or her capacities and rewarded according to his or her work. The key focus of Saint-Simon's socialism was on administrative efficiency and industrialism and a belief that science was the key to progress. This was accompanied by a desire to implement a rationally organised economy based on planning and geared towards large-scale scientific and material progress, thus embodied a desire for a more directed or planned economy. Other early socialist thinkers, such as Thomas Hodgkin and Charles Hall, based their ideas on David Ricardo's economic theories. They reasoned that the equilibrium value of commodities approximated prices charged by the producer when those commodities were in elastic supply and that these producer prices corresponded to the embodied labour—the cost of the labour (essentially the wages paid) that was required to produce the commodities. The Ricardian socialists viewed profit, interest and rent as deductions from this exchange-value.

West European social critics, including Robert Owen, Charles Fourier, Pierre-Joseph Proudhon, Louis Blanc, Charles Hall, and Saint-Simon were the first modern socialists who criticised the excessive poverty and inequality of the Industrial Revolution. They advocated reform, with some such as Robert Owen advocating the transformation of society to small communities without private property. Robert Owen's contribution to modern socialism was his understanding that actions and characteristics of individuals were largely determined by the social environment they were raised in and exposed to. On the other hand, Charles Fourier advocated phalansteres which were communities that respected individual desires (including sexual preferences), affinities and creativity and saw that work has to be made enjoyable for people. The ideas of Owen and Fourier were tried in practice in numerous intentional communities around Europe and the American continent in the mid-19th century.

The Paris Commune was a government that briefly ruled Paris from 18 March (more formally, from 28 March) to 28 May 1871. The Commune was the result of an uprising in Paris after France was defeated in the Franco-Prussian War. The Commune elections held on 26 March elected a Commune council of 92 members, one member for each 20,000 residents. Despite internal differences, the council began to organise the public services essential for a city of two million residents. It also reached a consensus on certain policies that tended towards a progressive, secular and highly democratic social democracy.

Because the Commune was only able to meet on fewer than 60 days in all, only a few decrees were actually implemented. These included the separation of church and state; the remission of rents owed for the entire period of the siege (during which payment had been suspended); the abolition of night work in the hundreds of Paris bakeries; the granting of pensions to the unmarried companions and children of National Guards killed on active service; and the free return, by the city pawnshops, of all workmen's tools and household items valued up to 20 francs, pledged during the siege. The Commune was concerned that skilled workers had been forced to pawn their tools during the war; the postponement of commercial debt obligations and the abolition of interest on the debts; and the right of employees to take over and run an enterprise if it were deserted by its owner. The Commune nonetheless recognised the previous owner's right to compensation.

The International Workingmen's Association (IWA), often called the First International, was founded in London in 1864. The International Workingmen's Association united diverse revolutionary currents including French followers of Proudhon, Blanquists, Philadelphes, English trade unionists, socialists and social democrats. The IWA held a preliminary conference in 1865 and had its first congress at Geneva in 1866. Due to the wide variety of philosophies present in the First International, there was conflict from the start. The first objections to Marx came from the mutualists who opposed communism and statism. However, shortly after Mikhail Bakunin and his followers (called collectivists while in the International) joined in 1868, the First International became polarised into two camps headed by Marx and Bakunin respectively. The clearest differences between the groups emerged over their proposed strategies for achieving their visions of socialism. The First International became the first major international forum for the promulgation of socialist ideas.

The followers of Bakunin were called collectivist anarchists and sought to collectivise ownership of the means of production while retaining payment proportional to the amount and kind of labour of each individual. Like Proudhonists, they asserted the right of each individual to the product of his labour and to be remunerated for their particular contribution to production. By contrast, anarcho-communists sought collective ownership of both the means and the products of labour. Errico Malatesta put it: "[I]nstead of running the risk of making a confusion in trying to distinguish what you and I each do, let us all work and put everything in common. In this way each will give to society all that his strength permits until enough is produced for every one; and each will take all that he needs, limiting his needs only in those things of which there is not yet plenty for every one". Anarcho-communism as a coherent, modern economic-political philosophy was first formulated in the Italian section of the First International by Carlo Cafiero, Emilio Covelli, Errico Malatesta, Andrea Costa and other ex Mazzinian republicans. Out of respect for Mikhail Bakunin, they did not make their differences with collectivist anarchism explicit until after Bakunin's death.

Syndicalism emerged in France inspired in part by the ideas of Pierre-Joseph Proudhon and later by Fernand Pelloutier and Georges Sorel. It developed at the end of the 19th century out of the French trade-union movement ("syndicat" is the French word for trade union). It was a significant force in Italy and Spain in the early 20th century until it was crushed by the fascist regimes in those countries. In the United States, syndicalism appeared in the guise of the Industrial Workers of the World, or "Wobblies", founded in 1905. Syndicalism is an economic system where industries are organised into confederations (syndicates) and the economy is managed by negotiation between specialists and worker representatives of each field, comprising multiple non-competitive categorised units. Syndicalism is thus a form of communism and economic corporatism, but also refers to the political movement and tactics used to bring about this type of system. An influential anarchist movement based on syndicalist ideas is anarcho-syndicalism. The International Workers Association is an international anarcho-syndicalist federation of various labour unions from different countries.

The Fabian Society is a British socialist organisation which was established with the purpose of advancing the principles of socialism via gradualist and reformist means. The society laid many of the foundations of the Labour Party and subsequently affected the policies of states emerging from the decolonisation of the British Empire, most notably India and Singapore. Originally, the Fabian Society was committed to the establishment of a socialist economy, alongside a commitment to British imperialism as a progressive and modernising force. Today, the society functions primarily as a think tank and is one of fifteen socialist societies affiliated with the Labour Party. Similar societies exist in Australia (the Australian Fabian Society), in Canada (the Douglas-Coldwell Foundation and the now disbanded League for Social Reconstruction) and in New Zealand.

Guild socialism is a political movement advocating workers' control of industry through the medium of trade-related guilds "in an implied contractual relationship with the public". It originated in the United Kingdom and was at its most influential in the first quarter of the 20th century. Inspired by medieval guilds, theorists such as Samuel G. Hobson and G. D. H. Cole advocated the public ownership of industries and their organisation into guilds, each of which would be under the democratic control of its trade union. Guild socialists were less inclined than Fabians to invest power in a state. At some point, like the American Knights of Labor, guild socialism wanted to abolish the wage system.

As the ideas of Marx and Engels took on flesh, particularly in central Europe, socialists sought to unite in an international organisation. In 1889 (the centennial of the French Revolution of 1789), the Second International was founded, with 384 delegates from twenty countries representing about 300 labour and socialist organisations. It was termed the Socialist International and Engels was elected honorary president at the third congress in 1893. Anarchists were ejected and not allowed in, mainly due to pressure from Marxists. It has been argued that at some point the Second International turned "into a battleground over the issue of libertarian versus authoritarian socialism. Not only did they effectively present themselves as champions of minority rights; they also provoked the German Marxists into demonstrating a dictatorial intolerance which was a factor in preventing the British labour movement from following the Marxist direction indicated by such leaders as H. M. Hyndman".

Reformism arose as an alternative to revolution. Eduard Bernstein was a leading social democrat in Germany who proposed the concept of evolutionary socialism. Revolutionary socialists quickly targeted reformism: Rosa Luxemburg condemned Bernstein's "Evolutionary Socialism" in her 1900 essay "Social Reform or Revolution?" Revolutionary socialism encompasses multiple social and political movements that may define "revolution" differently from one another. The Social Democratic Party (SPD) in Germany became the largest and most powerful socialist party in Europe, despite working illegally until the anti-socialist laws were dropped in 1890. In the 1893 elections, it gained 1,787,000 votes, a quarter of the total votes cast, according to Engels. In 1895, the year of his death, Engels emphasised the "Communist Manifesto"'s emphasis on winning, as a first step, the "battle of democracy".

In Argentina the Socialist Party of Argentina was established in the 1890s led by, among others, Juan B. Justo and Nicolás Repetto, thus becoming the first mass party in the country and in Latin America. The party affiliated itself with the Second International. Between 1924 and 1940 it was a member of the Labour and Socialist International. In 1904, Australians elected Chris Watson as the first Australian Labor Party Prime Minister, becoming the first democratically elected social democrat. In 1909, the first Kibbutz was established in Palestine by Russian Jewish Immigrants. The Kibbutz Movement would then expand through the 20th century following a doctrine of Zionist socialism. The British Labour Party first won seats in the House of Commons in 1902. The International Socialist Commission (ISC, also known as Berne International) was formed in February 1919 at a meeting in Bern by parties that wanted to resurrect the Second International.

By 1917, the patriotism of World War I changed into political radicalism in most of Europe, the United States and Australia. Other socialist parties from around the world who were beginning to gain importance in their national politics in the early 20th century included the Italian Socialist Party, the French Section of the Workers' International, the Spanish Socialist Workers' Party, the Swedish Social Democratic Party, the Russian Social Democratic Labour Party, the Socialist Party of America in the United States, the Argentinian Socialist Party and the Chilean Partido Obrero Socialista.

In February 1917, revolution exploded in Russia. Workers, soldiers and peasants established soviets (councils), the monarchy fell and a provisional government convoked pending the election of a constituent assembly. In April of that year, Vladimir Lenin, leader of the Bolshevik faction of socialists in Russia and known for his profound and controversial expansions of Marxism, was allowed to cross Germany to return to his country from exile in Switzerland.

Lenin had published essays on his analysis of imperialism, the monopoly and globalisation phase of capitalism as predicted by Marx, as well as analyses on the social conditions of his contemporary time. He observed that as capitalism had further developed in Europe and America, the workers remained unable to gain class consciousness so long as they were too busy working and concerned with how to make ends meet. He therefore proposed that the social revolution would require the leadership of a vanguard party of class-conscious revolutionaries from the educated and politically active part of the population.

Upon arriving in Petrograd, Lenin declared that the revolution in Russia was not over but had only begun, and that the next step was for the workers' soviets to take full state authority. He issued a thesis outlining the Bolshevik's party programme, including rejection of any legitimacy in the provisional government and advocacy for state power to be given to the peasant and working class through the soviets. The Bolsheviks became the most influential force in the soviets and on 7 November the capitol of the provisional government was stormed by Bolshevik Red Guards in what afterwards known as the "Great October Socialist Revolution". The rule of the provisional government was ended and the Russian Socialist Federative Soviet Republic—the world's first constitutionally socialist state—was established. On 25 January 1918 at the Petrograd Soviet, Lenin declared "Long live the world socialist revolution!" and proposed an immediate armistice on all fronts and transferred the land of the landed proprietors, the crown and the monasteries to the peasant committees without compensation.

The day after assuming executive power on 25 January, Lenin wrote "Draft Regulations on Workers' Control", which granted workers control of businesses with more than five workers and office employees and access to all books, documents and stocks and whose decisions were to be "binding upon the owners of the enterprises". Governing through the elected soviets and in alliance with the peasant-based Left Socialist-Revolutionaries, the Bolshevik government began nationalising banks and industry; and disavowed the national debts of the deposed Romanov royal régime. It sued for peace, withdrawing from World War I and convoked a Constituent Assembly in which the peasant Socialist-Revolutionary Party (SR) won a majority.

The Constituent Assembly elected Socialist-Revolutionary leader Victor Chernov President of a Russian republic, but rejected the Bolshevik proposal that it endorse the Soviet decrees on land, peace and workers' control and acknowledge the power of the Soviets of Workers', Soldiers' and Peasants' Deputies. The next day, the Bolsheviks declared that the assembly was elected on outdated party lists and the All-Russian Central Executive Committee of the Soviets dissolved it. In March 1919, world communist parties formed Comintern (also known as the Third International) at a meeting in Moscow.

Parties which did not want to be a part of the resurrected Second International (ISC) or Comintern formed the International Working Union of Socialist Parties (IWUSP, also known as Vienna International/Vienna Union/Two-and-a-Half International) on 27 February 1921 at a conference in Vienna. The ISC and the IWUSP joined to form the Labour and Socialist International (LSI) in May 1923 at a meeting in Hamburg Left-wing groups which did not agree to the centralisation and abandonment of the soviets by the Bolshevik Party led left-wing uprisings against the Bolsheviks—such groups included Socialist Revolutionaries, Left Socialist Revolutionaries, Mensheviks and anarchists.

Within this left-wing discontent, the most large-scale events were the worker's Kronstadt rebellion and the anarchist led Revolutionary Insurrectionary Army of Ukraine uprising which controlled an area known as the Free Territory.

The Bolshevik Russian Revolution of January 1918 engendered communist parties worldwide and their concomitant revolutions of 1917–1923. Few communists doubted that the Russian success of socialism depended on successful, working-class socialist revolutions in developed capitalist countries. In 1919, Lenin and Trotsky organised the world's communist parties into a new international association of workers—the Communist International (Comintern), also called the Third International.

The Russian Revolution also influenced uprisings in other countries around this time. The German Revolution of 1918–1919 resulted in the replacing Germany's imperial government with a republic. The revolutionary period lasted from November 1918 until the formal establishment of the Weimar Republic in August 1919 and included an episode known as the Bavarian Soviet Republic and the Spartacist uprising. In Italy, the events known as the "Biennio Rosso" were characterised by mass strikes, worker manifestations and self-management experiments through land and factory occupations. In Turin and Milan, workers' councils were formed and many factory occupations took place led by anarcho-syndicalists organised around the Unione Sindacale Italiana.

By 1920, the Red Army under its commander Trotsky had largely defeated the royalist White Armies. In 1921, War Communism was ended and under the New Economic Policy (NEP) private ownership was allowed for small and medium peasant enterprises. While industry remained largely state-controlled, Lenin acknowledged that the NEP was a necessary capitalist measure for a country unripe for socialism. Profiteering returned in the form of "NEP men" and rich peasants (kulaks) gained power in the countryside. Nevertheless, the role of Trotsky in this episode has been questioned by other socialists, including ex Trotskyists. In the United States, Dwight Macdonald broke with Trotsky and left the Trotskyist Socialist Workers Party by raising the question of the Kronstadt rebellion, which Trotsky as leader of the Soviet Red Army and the other Bolsheviks had brutally repressed. He then moved towards democratic socialism. and anarchism.

A similar critique of Trotsky's role on the events around the Kronstadt rebellion was raised by the American anarchist Emma Goldman. In her essay "Trotsky Protests Too Much", she says: "I admit, the dictatorship under Stalin's rule has become monstrous. That does not, however, lessen the guilt of Leon Trotsky as one of the actors in the revolutionary drama of which Kronstadt was one of the bloodiest scenes".

In 1922, the fourth congress of the Communist International took up the policy of the United Front, urging communists to work with rank and file Social Democrats while remaining critical of their leaders, whom they criticised for betraying the working class by supporting the war efforts of their respective capitalist classes. For their part, the social democrats pointed to the dislocation caused by revolution and later the growing authoritarianism of the communist parties. When the Communist Party of Great Britain applied to affiliate to the Labour Party in 1920, it was turned down.

On seeing the Soviet State's growing coercive power in 1923, a dying Lenin said Russia had reverted to "a bourgeois tsarist machine... barely varnished with socialism". After Lenin's death in January 1924, the Communist Party of the Soviet Union—then increasingly under the control of Joseph Stalin—rejected the theory that socialism could not be built solely in the Soviet Union in favour of the concept of "Socialism in One Country". Despite the marginalised Left Opposition's demand for the restoration of Soviet democracy, Stalin developed a bureaucratic, authoritarian government that was condemned by democratic socialists, anarchists and Trotskyists for undermining the initial socialist ideals of the Bolshevik Russian Revolution.

In 1924, the Mongolian People's Republic was established and was ruled by the Mongolian People's Party. The Russian Revolution and the appearance of the Soviet State motivated a worldwide current of national communist parties which ended having varying levels of political and social influence. Among these there appeared the Communist Party of France, the Communist Party USA, the Italian Communist Party, the Chinese Communist Party, the Mexican Communist Party, the Brazilian Communist Party, the Chilean Communist Party and the Communist Party of Indonesia.

In Spain in 1936, the national anarcho-syndicalist trade union Confederación Nacional del Trabajo (CNT) initially refused to join a popular front electoral alliance and abstention by CNT supporters led to a right-wing election victory. In 1936, the CNT changed its policy and anarchist votes helped bring the popular front back to power. Months later, the former ruling class responded with an attempted coup, sparking the Spanish Civil War (1936–1939).

In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain where they collectivised the land. The events known as the Spanish Revolution was a workers' social revolution that began during the outbreak of the Spanish Civil War in 1936 and resulted in the widespread implementation of anarchist and more broadly libertarian socialist organisational principles throughout various portions of the country for two to three years, primarily Catalonia, Aragon, Andalusia and parts of Levante.

Much of Spain's economy was put under worker control and in anarchist strongholds like Catalonia the figure was as high as 75%, but lower in areas with heavy Communist Party of Spain influence, as the Soviet-allied party actively resisted attempts at collectivisation enactment. Factories were run through worker committees, agrarian areas became collectivised and run as libertarian communes. Anarchist historian Sam Dolgoff estimated that about eight million people participated directly or indirectly in the Spanish Revolution.

Leon Trotsky's Fourth International was established in France in 1938 when Trotskyists argued that the Comintern or Third International had become irretrievably "lost to Stalinism" and thus incapable of leading the international working class to political power. The rise of Nazism and the start of World War II led to the dissolution of the LSI in 1940. After the War, the Socialist International was formed in Frankfurt in July 1951 as a successor to the LSI.

After World War II, social democratic governments introduced social reform and wealth redistribution via state welfare and taxation. Social democratic parties dominated post-war politics in countries such as France, Italy, Czechoslovakia, Belgium and Norway. At one point, France claimed to be the world's most state-controlled capitalist country. The nationalised public utilities included Charbonnages de France (CDF), Electricité de France (EDF), Gaz de France (GDF), Air France, Banque de France and Régie Nationale des Usines Renault.

In 1945, the British Labour Party led by Clement Attlee was elected to office based on a radical socialist programme. The Labour government nationalised major public utilities such as mines, gas, coal, electricity, rail, iron, steel and the Bank of England. British Petroleum was officially nationalised in 1951. Anthony Crosland said that in 1956 25% of British industry was nationalised and that public employees, including those in nationalised industries, constituted a similar proportion of the country's total employed population. The Labour Governments of 1964–1970 and 1974–1979 intervened further. It re-nationalised steel (1967, British Steel) after the Conservatives had denationalised it and nationalised car production (1976, British Leyland). The National Health Service provided taxpayer-funded health care to everyone, free at the point of service. Working-class housing was provided in council housing estates and university education became available via a school grant system.

During most of the post-war era, Sweden was governed by the Swedish Social Democratic Party largely in cooperation with trade unions and industry. In Sweden, the Swedish Social Democratic Party held power from 1936 to 1976, 1982 to 1991, 1994 to 2006 and 2014 to present. Tage Erlander was the leader of the Swedish Social Democratic Party and led the government from 1946 to 1969, an uninterrupted tenure of twenty-three years, the longest parliamentary democracy. During Erlander's tenure, the welfare state was substantially expanded. Prominent Swedish Prime Minister Olof Palme identified as a democratic socialist and has been described as a "revolutionary reformist".

The Norwegian Labour Party was established in 1887 and was originally largely a federation of trade unions. The party did not proclaim a socialist agenda, but it had universal suffrage and dissolution of the union with Sweden as top priorities. In 1899, the Norwegian Confederation of Trade Unions was separated from the Labour party. Around the time of the Russian Revolution, the Labour Party moved markedly to the left and joined the Communist International in 1919. Although leaving the Communist International in 1923, the party still regarded itself as revolutionary. The party's left-wing broke out and established the Communist Party of Norway while the Labour Party gradually changed to a reformist line around 1930. In 1935, Johan Nygaardsvold established a coalition that lasted until 1945.

From 1946 to 1962, the Norwegian Labour Party held an absolute majority in the parliament led by Einar Gerhardsen, who was Prime Minister with seventeen years in office. Although the party abandoned large parts of its pre-war socialist ideas, under Gerhardsen the welfare states was expanded to ensure the universal provision of basic human rights and stabilising the economy. In the first post-war election, the Communist Party of Norway obtained a strong position with 12% of the votes, but it largely vanished during the Cold War. In the 1950s, popular socialism emerged as a vital current of the left in Nordic countries could be characterised as a democratic socialism in the same vein as it placed itself between communism and social democracy. In the early 1960s, the Socialist Left Party challenged the Labour Party from the left. In the 1970s, a more radical socialist party, the Worker's Communist Party (AKP), broke off from the Socialist Left Party and had notable influence in student associations and some trade unions. The AKP identified with Communist China and Albania, rather than Soviet Union.
In countries like Sweden, the Rehn–Meidner model allowed capitalists owning very productive and efficient firms to retain excess profits at the expense of the firms' workers, exacerbating inequality and causing workers in these firms to agitate for a share of the profits in the 1970s just as women working in the state sector began to assert pressure for better wages as well. Rudolf Meidner established a study committee that came up with a 1976 proposal that entailed transferring the excess profits into investment funds controlled by the workers in the efficient firms, with the intention that firms would create further employment and pay more workers higher wages rather than increasing the wealth of company owners and managers. Capitalists immediately distinguished this proposal as socialism and launched an unprecedented opposition—including calling off the class compromise established in the 1938 Saltsjöbaden Agreement.

Social democracy is a moderate, reformist and democratic socialist ideology. Nordic social democratic parties are one of the oldest social democratic parties and include the Swedish Social Demcoratic Party in Sweden, the Norwegian Labour Party in Norway, the Social Democratic Party and the Social Democratic Alliance in Iceland and the Social Democrats in Denmark. Countries or political systems that for a long time have been dominated by social democratic parties are sometimes labelled social democratic.

The Nordic model is a form of economic-political system common to the Nordic countries (Denmark, Finland, Iceland, Norway and Sweden). The Nordic model has three main ingredients, namely peaceful, institutionalised negotiation between employers and trade unions notably about nationwide collective wage agreements; active, predictable and measured macro economic policy; and universal welfare and free education. In Norway and Sweden, the welfare system is governmental whereas in Denmark, Finland and Iceland trade unions play a greater role in provision of welfare. The Nordic model is often labelled social democratic in contrast with the conservative continental model and the liberal Anglo-American model, even if major reforms in the Nordic countries are the results of consensus and compromises across the political spectrum. In Denmark, Norway and Sweden, key reforms were implemented under social democratic cabinets while in Finland and Iceland centre-right parties dominated during the buildup of the model. Since World War II, the Nordic countries have largely maintained a mixed-market economy. These countries have been characterised by labour force participation, gender equality, egalitarian and universal benefits, redistribution of wealth and expansionary fiscal policy.

The Soviet Union played a decisive role in the Allied victory in World War II. After the war, the Soviet Union became a recognised superpower. The Soviet era saw some of the most significant technological achievements of the 20th century, including the world's first spacecraft and the first astronaut. The Soviet economy was the modern world's first centrally planned economy. It was based on a system of state ownership of industry managed through Gosplan (the State Planning Commission), Gosbank (the State Bank) and the Gossnab (State Commission for Materials and Equipment Supply).

Economic planning was conducted through a series of Five-Year Plans. The emphasis was on fast development of heavy industry and the nation became one of the world's top manufacturers of a large number of basic and heavy industrial products, but it lagged in light industrial production and consumer durables. Modernisation brought about a general increase in the standard of living.

The Eastern Bloc was the group of former Communist states of Central and Eastern Europe, generally the Soviet Union and the countries of the Warsaw Pact which included the People's Republic of Poland, the German Democratic Republic, the People's Republic of Hungary, the People's Republic of Bulgaria, the Czechoslovak Socialist Republic, the Socialist Republic of Romania, the People's Socialist Republic of Albania and the Socialist Federal Republic of Yugoslavia. The Hungarian Revolution of 1956 was a spontaneous nationwide revolt against the government of the People's Republic of Hungary and its Soviet-imposed policies, lasting from 23 October until 10 November 1956. Soviet leader Nikita Khrushchev's denunciation of the excesses of Stalin's regime during the Twentieth Party Congress of the Communist Party of the Soviet Union on 1956 as well as the revolt in Hungary, produced ideological fractures and disagreements within the communist and socialist parties of Western Europe.

In the post-war years, socialism became increasingly influential throughout the so-called Third World. Embracing a new Third World socialism, countries in Africa, Asia and Latin America often nationalised industries held by foreign owners. The Chinese Kuomintang Party, the previous ruling party in Taiwan, was referred to as having a socialist ideology since Kuomintang's revolutionary ideology in the 1920s incorporated unique Chinese socialism as part of its ideology. The Soviet Union trained Kuomintang revolutionaries in the Moscow Sun Yat-sen University. Movie theatres in the Soviet Union showed newsreels and clips of Chiang at Moscow Sun Yat-sen University portraits of Chiang were hung on the walls and in the Soviet May Day parades that year Chiang's portrait was to be carried along with the portraits of Marx, Lenin, Stalin and other socialist leaders.

The Chinese Revolution was the second stage in the Chinese Civil War which ended in the establishment of the People's Republic of China led by the Chinese Communist Party. The term "Third World" was coined by French demographer Alfred Sauvy in 1952 on the model of the Third Estate, which according to the Abbé Sieyès represented everything, but was nothing "because at the end this ignored, exploited, scorned Third World like the Third Estate, wants to become something too".

The emergence of this new political entity in the frame of the Cold War was complex and painful. Several tentatives were made to organise newly independent states in order to oppose a common front towards both the United States' and the Soviet Union's influence on them, with the consequences of the Sino-Soviet split already at works. The Non-Aligned Movement constituted itself around the main figures of Prime Minister Jawaharlal Nehru of India, President Sukarno of Indonesia, leader Josip Broz Tito of Yugoslavia and Gamal Abdel Nasser of Egypt who successfully opposed the French and British imperial powers during the 1956 Suez crisis. After the 1954 Geneva Conference which ended the French war against Ho Chi Minh in Vietnam, the 1955 Bandung Conference gathered Nasser, Nehru, Tito, Sukarno and Zhou Enlai, Premier of the People's Republic of China.

As many African countries gained independence during the 1960s, some of them rejected capitalism in favour of a more afrocentric economic model. The main architects of African socialism were Julius Nyerere of Tanzania, Léopold Senghor of Senegal, Kwame Nkrumah of Ghana and Sékou Touré of Guinea.

The Cuban Revolution (1953–1959) was an armed revolt conducted by Fidel Castro's 26th of July Movement and its allies against the government of Cuban President Fulgencio Batista. The revolution began in July 1953 and finally ousted Batista on 1 January 1959, replacing his government with Castro's revolutionary state. Castro's government later reformed along communist lines, becoming the Communist Party of Cuba in October 1965.

In Indonesia, a right-wing military regime led by Suharto killed between 500,000 and one million people in 1965 and 1966, mainly to crush the growing influence of the Communist Party of Indonesia and other leftist sectors, with support from the United States government, which provided kill lists containing thousands of names of suspected high-ranking Communists.

The New Left was a term used mainly in the United Kingdom and United States in reference to activists, educators, agitators and others in the 1960s and 1970s who sought to implement a broad range of reforms on issues such as gay rights, abortion, gender roles and drugs in contrast to earlier leftist or Marxist movements that had taken a more vanguardist approach to social justice and focused mostly on labour unionisation and questions of social class. The New Left rejected involvement with the labour movement and Marxism's historical theory of class struggle.

In the United States, the New Left was associated with the Hippie movement and anti-war college campus protest movements as well as the black liberation movements such as the Black Panther Party. While initially formed in opposition to the "Old Left" Democratic Party, groups composing the New Left gradually became central players in the Democratic coalition.

The protests of 1968 represented a worldwide escalation of social conflicts, predominantly characterised by popular rebellions against military, capitalist and bureaucratic elites who responded with an escalation of political repression. These protests marked a turning point for the civil rights movement in the United States, which produced revolutionary movements like the Black Panther Party; the prominent civil rights leader Martin Luther King Jr. organised the "Poor People's Campaign" to address issues of economic justice, while personally showing sympathy with democratic socialism. In reaction to the Tet Offensive, protests also sparked a broad movement in opposition to the Vietnam War all over the United States and even into London, Paris, Berlin and Rome. In 1968 in Carrara, Italy, the International of Anarchist Federations was founded during an international anarchist conference held there by the three existing European federations of France, the Italian and the Iberian Anarchist Federation as well as the Bulgarian federation in French exile.

Mass socialist or communist movements grew not only in the United States, but also in most European countries. The most spectacular manifestation of this were the May 1968 protests in France in which students linked up with strikes of up to ten million workers and for a few days the movement seemed capable of overthrowing the government.

In many other capitalist countries, struggles against dictatorships, state repression and colonisation were also marked by protests in 1968, such as the beginning of the Troubles in Northern Ireland, the Tlatelolco massacre in Mexico City and the escalation of guerrilla warfare against the military dictatorship in Brazil. Countries governed by communist parties had protests against bureaucratic and military elites. In Eastern Europe there were widespread protests that escalated particularly in the Prague Spring in Czechoslovakia. In response, Soviet Union occupied Czechoslovakia, but the occupation was denounced by the Italian and French communist parties and the Communist Party of Finland. Few western European political leaders defended the occupation, among them the Portuguese communist secretary-general Álvaro Cunhal. along with the Luxembourg party and conservative factions of the Communist Party of Greece.

In the Chinese Cultural Revolution, a social-political youth movement mobilised against "bourgeois" elements which were seen to be infiltrating the government and society at large, aiming to restore capitalism. This movement motivated Maoism-inspired movements around the world in the context of the Sino-Soviet split.

In Latin America in the 1960s, a socialist tendency within the catholic church appeared which was called liberation theology which motivated even the Colombian priest Camilo Torres to enter the ELN guerrilla. In Chile, Salvador Allende, a physician and candidate for the Socialist Party of Chile, was elected president through democratic elections in 1970. In 1973, his government was ousted by the United States-backed military dictatorship of Augusto Pinochet, which lasted until the late 1980s. Pinochet's regime was a leader of Operation Condor, a U.S.-backed campaign of repression and state terrorism carried out by the intelligence services of the Southern Cone countries of Latin America to eliminate suspected Communist subversion. In Jamaica, the democratic socialist Michael Manley served as the fourth Prime Minister of Jamaica from 1972 to 1980 and from 1989 to 1992. According to opinion polls, he remains one of Jamaica's most popular Prime Ministers since independence. The Nicaraguan Revolution encompassed the rising opposition to the Somoza dictatorship in the 1960s and 1970s, the campaign led by the Sandinista National Liberation Front (FSLN) to violently oust the dictatorship in 1978–1979, the subsequent efforts of the FSLN to govern Nicaragua from 1979 until 1990 and the socialist measures which included wide-scale agrarian reform and educational programs. The People's Revolutionary Government was proclaimed on 13 March 1979 in Grenada which was overthrown by armed forces of the United States in 1983. The Salvadoran Civil War (1979–1992) was a conflict between the military-led government of El Salvador and the Farabundo Martí National Liberation Front (FMLN), a coalition or umbrella organisation of five socialist guerrilla groups. A coup on 15 October 1979 led to the killings of anti-coup protesters by the government as well as anti-disorder protesters by the guerrillas, and is widely seen as the tipping point towards the civil war.

In Italy, Autonomia Operaia was a leftist movement particularly active from 1976 to 1978. It took an important role in the autonomist movement in the 1970s, aside earlier organisations such as Potere Operaio (created after May 1968) and Lotta Continua. This experience prompted the contemporary socialist radical movement autonomism. In 1982, the newly elected French socialist government of François Mitterrand made nationalisations in a few key industries, including banks and insurance companies. Eurocommunism was a trend in the 1970s and 1980s in various Western European communist parties to develop a theory and practice of social transformation that was more relevant for a Western European country and less aligned to the influence or control of the Communist Party of the Soviet Union. Outside Western Europe, it is sometimes called neocommunism. Some communist parties with strong popular support, notably the Italian Communist Party (PCI) and the Communist Party of Spain (PCE) adopted Eurocommunism most enthusiastically and the Communist Party of Finland was dominated by Eurocommunists. The French Communist Party (PCF) and many smaller parties strongly opposed Eurocommunism and stayed aligned with the Communist Party of the Soviet Union until the end of the Soviet Union.

In the late 1970s and in the 1980s, the Socialist International (SI) had extensive contacts and discussion with the two powers of the Cold War, the United States and the Soviet Union, about East-West relations and arms control. Since then, the SI has admitted as member parties the Nicaraguan FSLN, the left-wing Puerto Rican Independence Party, as well as former communist parties such as the Democratic Party of the Left of Italy and the Front for the Liberation of Mozambique (FRELIMO). The SI aided social democratic parties in re-establishing themselves when dictatorship gave way to democracy in Portugal (1974) and Spain (1975). Until its 1976 Geneva Congress, the SI had few members outside Europe and no formal involvement with Latin America.
After Mao's death in 1976 and the arrest of the faction known as the Gang of Four, who were blamed for the excesses of the Cultural Revolution, Deng Xiaoping took power and led the People's Republic of China to significant economic reforms. The Communist Party of China loosened governmental control over citizens' personal lives and the communes were disbanded in favour of private land leases, thus China's transition from a planned economy to a mixed economy named as "socialism with Chinese characteristics" which maintained state ownership rights over land, state or cooperative ownership of much of the heavy industrial and manufacturing sectors and state influence in the banking and financial sectors. China adopted its current constitution on 4 December 1982. President Jiang Zemin and Premier Zhu Rongji led the nation in the 1990s. Under their administration, China's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%. At the Sixth National Congress of the Communist Party of Vietnam in December 1986, reformist politicians replaced the "old guard" government with new leadership. The reformers were led by 71-year-old Nguyen Van Linh, who became the party's new general secretary. Linh and the reformers implemented a series of free market reforms—known as "" ("Renovation")—which carefully managed the transition from a planned economy to a "socialist-oriented market economy". Mikhail Gorbachev wished to move the Soviet Union towards of Nordic-style social democracy, calling it "a socialist beacon for all mankind". Prior to its dissolution in 1991, the Soviet Union had the second largest economy in the world after the United States. With the collapse of the Soviet Union, the economic integration of the Soviet republics was dissolved and overall industrial activity declined substantially. A lasting legacy remains in the physical infrastructure created during decades of combined industrial production practices, and widespread environmental destruction. The transition to capitalism in the former Soviet Union and Eastern bloc, which was accompanied by Washington Consensus-inspired "shock therapy", resulted in a steep fall in the standard of living. The region experienced rising economic inequality and poverty a surge in excess mortality and a decline in life expectancy, which was accompanied by the entrenchment of a newly established business oligarchy in the former. The average post-communist country had returned to 1989 levels of per-capita GDP by 2005, although some are still far behind that. These developments led to increased nationalist sentiment and nostalgia for the Communist era.

Many social democratic parties, particularly after the Cold War, adopted neoliberal market policies including privatisation, deregulation and financialisation. They abandoned their pursuit of moderate socialism in favour of market liberalism. By the 1980s, with the rise of conservative neoliberal politicians such as Ronald Reagan in the United States, Margaret Thatcher in Britain, Brian Mulroney in Canada and Augusto Pinochet in Chile, the Western welfare state was attacked from within, but state support for the corporate sector was maintained. Monetarists and neoliberals attacked social welfare systems as impediments to private entrepreneurship. In the United Kingdom, Labour Party leader Neil Kinnock made a public attack against the entryist group Militant at the 1985 Labour Party conference. The Labour Party ruled that Militant was ineligible for affiliation with the Labour Party, and the party gradually expelled Militant supporters. The Kinnock leadership had refused to support the 1984–1985 miner's strike over pit closures, a decision that the party's left wing and the National Union of Mineworkers blamed for the strike's eventual defeat. In 1989 at Stockholm, the 18th Congress of the Socialist International adopted a new Declaration of Principles, saying: Democratic socialism is an international movement for freedom, social justice, and solidarity. Its goal is to achieve a peaceful world where these basic values can be enhanced and where each individual can live a meaningful life with the full development of his or her personality and talents, and with the guarantee of human and civil rights in a democratic framework of society.

In the 1990s, the British Labour Party under Tony Blair enacted policies based on the free market economy to deliver public services via the private finance initiative. Influential in these policies was the idea of a "Third Way" which called for a re-evaluation of welfare state policies. In 1995, the Labour Party re-defined its stance on socialism by re-wording Clause IV of its constitution, effectively rejecting socialism by removing all references to public, direct worker or municipal ownership of the means of production. The Labour Party stated: "The Labour Party is a democratic socialist party. It believes that, by the strength of our common endeavour we achieve more than we achieve alone, so as to create, for each of us, the means to realise our true potential, and, for all of us, a community in which power, wealth, and opportunity are in the hands of the many, not the few".

African socialism has been and continues to be a major ideology around the continent. Julius Nyerere was inspired by Fabian socialist ideals. He was a firm believer in rural Africans and their traditions and ujamaa, a system of collectivisation that according to Nyerere was present before European imperialism. Essentially he believed Africans were already socialists. Other African socialists include Jomo Kenyatta, Kenneth Kaunda, Nelson Mandela and Kwame Nkrumah. Fela Kuti was inspired by socialism and called for a democratic African republic. In South Africa the African National Congress (ANC) abandoned its partial socialist allegiances after taking power and followed a standard neoliberal route. From 2005 through to 2007, the country was wracked by many thousands of protests from poor communities. One of these gave rise to a mass movement of shack dwellers, Abahlali baseMjondolo that despite major police suppression continues to work for popular people's planning and against the creation of a market economy in land and housing.

In Asia, states with socialist economies—such as the People's Republic of China, North Korea, Laos and Vietnam—have largely moved away from centralised economic planning in the 21st century, placing a greater emphasis on markets. Forms include the Chinese socialist market economy and the Vietnamese socialist-oriented market economy. They use state-owned corporate management models as opposed to modelling socialist enterprise on traditional management styles employed by government agencies. In China living standards continued to improve rapidly despite the late-2000s recession, but centralised political control remained tight. Brian Reynolds Myers in his book "The Cleanest Race", later supported by other academics, dismisses the idea that "Juche" is North Korea's leading ideology, regarding its public exaltation as designed to deceive foreigners and that it exists to be praised and not actually read, pointing out that North Korea's constitution of 2009 omits all mention of communism.

Though the authority of the state remained unchallenged under "Đổi Mới", the government of Vietnam encourages private ownership of farms and factories, economic deregulation and foreign investment, while maintaining control over strategic industries. The Vietnamese economy subsequently achieved strong growth in agricultural and industrial production, construction, exports and foreign investment. However, these reforms have also caused a rise in income inequality and gender disparities.

Elsewhere in Asia, some elected socialist parties and communist parties remain prominent, particularly in India and Nepal. The Communist Party of Nepal in particular calls for multi-party democracy, social equality and economic prosperity. In Singapore, a majority of the GDP is still generated from the state sector comprising government-linked companies. In Japan, there has been a resurgent interest in the Japanese Communist Party among workers and youth. In Malaysia, the Socialist Party of Malaysia got its first Member of Parliament, Dr. Jeyakumar Devaraj, after the 2008 general election. In 2010, there were 270 kibbutzim in Israel. Their factories and farms account for 9% of Israel's industrial output, worth US$8 billion and 40% of its agricultural output, worth over $1.7 billion. Some Kibbutzim had also developed substantial high-tech and military industries. Also in 2010, Kibbutz Sasa, containing some 200 members, generated $850 million in annual revenue from its military-plastics industry.

The United Nations "World Happiness Report 2013" shows that the happiest nations are concentrated in Northern Europe, where the Nordic model is employed, with Denmark topping the list. This is at times attributed to the success of the Nordic model in the region that has been labelled social democratic in contrast with the conservative continental model and the liberal Anglo-American model. The Nordic countries ranked highest on the metrics of real GDP per capita, healthy life expectancy, having someone to count on, perceived freedom to make life choices, generosity and freedom from corruption.

The objectives of the Party of European Socialists, the European Parliament's socialist and social democratic bloc, are now "to pursue international aims in respect of the principles on which the European Union is based, namely principles of freedom, equality, solidarity, democracy, respect of Human Rights and Fundamental Freedoms, and respect for the Rule of Law". As a result, today the rallying cry of the French Revolution—"Liberté, égalité, fraternité"—is promoted as essential socialist values. To the left of the PES at the European level is the Party of the European Left (PEL), also commonly abbreviated "European Left"), which is a political party at the European level and an association of democratic socialist, socialist and communist political parties in the European Union and other European countries. It was formed in January 2004 for the purposes of running in the 2004 European Parliament elections. PEL was founded on 8–9 May 2004 in Rome. Elected MEPs from member parties of the European Left sit in the European United Left–Nordic Green Left (GUE/NGL) group in the European parliament.
The socialist Left Party in Germany grew in popularity due to dissatisfaction with the increasingly neoliberal policies of the SPD, becoming the fourth biggest party in parliament in the general election on 27 September 2009. Communist candidate Dimitris Christofias won a crucial presidential runoff in Cyprus, defeating his conservative rival with a majority of 53%. In Ireland, in the 2009 European election Joe Higgins of the Socialist Party took one of three seats in the capital Dublin European constituency.

In Denmark, the Socialist People's Party (SF) more than doubled its parliamentary representation to 23 seats from 11, making it the fourth largest party. In 2011, the Social Democrats, Socialist People's Party and the Danish Social Liberal Party formed government, after a slight victory over the main rival political coalition. They were led by Helle Thorning-Schmidt, and had the Red-Green Alliance as a supporting party.

In Norway, the Red-Green Coalition consists of the Labour Party (Ap), the Socialist Left Party (SV) and the Centre Party (Sp) and governed the country as a majority government from the 2005 general election until 2013.

In the Greek legislative election of January 2015, the Coalition of the Radical Left (SYRIZA) led by Alexis Tsipras won a legislative election for the first time while the Communist Party of Greece won 15 seats in parliament. SYRIZA has been characterised as an anti-establishment party, whose success has sent "shock-waves across the EU".

In the United Kingdom, the National Union of Rail, Maritime and Transport Workers put forward a slate of candidates in the 2009 European Parliament elections under the banner of No to EU – Yes to Democracy, a broad left-wing alter-globalisation coalition involving socialist groups such as the Socialist Party, aiming to offer an alternative to the "anti-foreigner" and pro-business policies of the UK Independence Party. In the following May 2010 United Kingdom general election, the Trade Unionist and Socialist Coalition, launched in January 2010 and backed by Bob Crow, the leader of the National Union of Rail, Maritime and Transport Workers union (RMT), other union leaders and the Socialist Party among other socialist groups, stood against Labour in 40 constituencies. The Trade Unionist and Socialist Coalition contested the 2011 local elections, having gained the endorsement of the RMT June 2010 conference, but gained no seats. Left Unity was also founded in 2013 after the film director Ken Loach appealed for a new party of the left to replace the Labour Party, which he claimed had failed to oppose austerity and had shifted towards neoliberalism. In 2015, following a defeat at the 2015 United Kingdom general election, self-described socialist Jeremy Corbyn took over from Ed Miliband as leader of the Labour Party.

In France, Olivier Besancenot, the Revolutionary Communist League (LCR) candidate in the 2007 presidential election, received 1,498,581 votes, 4.08%, double that of the communist candidate. The LCR abolished itself in 2009 to initiate a broad anti-capitalist party, the New Anticapitalist Party, whose stated aim is to "build a new socialist, democratic perspective for the twenty-first century".

On 25 May 2014, the Spanish left-wing party Podemos entered candidates for the 2014 European parliamentary elections, some of which were unemployed. In a surprise result, it polled 7.98% of the vote and thus was awarded five seats out of 54 while the older United Left was the third largest overall force obtaining 10.03% and 5 seats, 4 more than the previous elections.

The government of Portugal established on 26 November 2015 was a Socialist Party (PS) minority government led by prime minister António Costa, who succeeded in securing support for a Socialist minority government by the Left Bloc (B.E.), the Portuguese Communist Party (PCP) and the Ecologist Party "The Greens" (PEV).

All around Europe and in some places of Latin America there exists a social centre and squatting movement mainly inspired by autonomist and anarchist ideas.

According to a 2013 article in "The Guardian", "[c]ontrary to popular belief, Americans don't have an innate allergy to socialism. Milwaukee has had several socialist mayors (Frank Zeidler, Emil Seidel and Daniel Hoan), and there is currently an independent socialist in the US Senate, Bernie Sanders of Vermont". Sanders, once mayor of Vermont's largest city, Burlington, has described himself as a democratic socialist and has praised Scandinavian-style social democracy. In 2016, Sanders made a bid for the Democratic Party presidential candidate, thereby gaining considerable popular support, particularly among the younger generation, but lost the nomination to Hillary Clinton. As of 2019, the Democratic Socialists of America have two members in Congress, and various members in state legislatures and city councils. According to a 2018 Gallup poll, 37% of American adults have a positive view of socialism, including 57% of Democrat-leaning voters and 16% of Republican-leaning voters. A 2019 YouGov poll found that 7 out of 10 millennials would vote for a socialist presidential candidate, and 36% had a favorable view of communism. An earlier 2019 Harris Poll found that socialism is more popular with women than men, with 55% of women between the ages of 18 and 54 preferring to live in a socialist society while a majority of men surveyed in the poll chose capitalism over socialism.

Anti-capitalism, anarchism and the anti-globalisation movement rose to prominence through events such as protests against the World Trade Organization Ministerial Conference of 1999 in Seattle. Socialist-inspired groups played an important role in these movements, which nevertheless embraced much broader layers of the population and were championed by figures such as Noam Chomsky. In Canada, the Co-operative Commonwealth Federation (CCF), the precursor to the social democratic New Democratic Party (NDP), had significant success in provincial politics. In 1944, the Saskatchewan CCF formed the first socialist government in North America. At the federal level, the NDP was the Official Opposition, from 2011 through 2015.

In their "Johnson" linguistics column, "The Economist" opines that in the 21st century United States, the term "socialism", without clear definition, has become a pejorative used by conservatives to attack liberal and progressive policies, proposals, and public figures.

For the "Encyclopedia Britannica", "the attempt by Salvador Allende to unite Marxists and other reformers in a socialist reconstruction of Chile is most representative of the direction that Latin American socialists have taken since the late 20th century. [...] Several socialist (or socialist-leaning) leaders have followed Allende's example in winning election to office in Latin American countries". Venezuelan President Hugo Chávez, Nicaraguan President Daniel Ortega, Bolivian President Evo Morales and Ecuadorian president Rafael Correa refer to their political programmes as socialist and Chávez adopted the term "socialism of the 21st century". After winning re-election in December 2006, Chávez said: "Now more than ever, I am obliged to move Venezuela's path towards socialism". Chávez was also reelected in October 2012 for his third six-year term as President, but he died in March 2013 from cancer. After Chávez's death on 5 March 2013, Vice President from Chavez's party Nicolás Maduro assumed the powers and responsibilities of the President. A special election was held on 14 April of the same year to elect a new President, which Maduro won by a tight margin as the candidate of the United Socialist Party of Venezuela and he was formally inaugurated on 19 April. "Pink tide" is a term being used in contemporary 21st-century political analysis in the media and elsewhere to describe the perception that leftist ideology in general and left-wing politics in particular are increasingly influential in Latin America.
Foro de São Paulo is a conference of leftist political parties and other organisations from Latin America and the Caribbean. It was launched by the Workers' Party () of Brazil in 1990 in the city of São Paulo. The Forum of São Paulo was constituted in 1990 when the Brazilian Workers' Party approached other parties and social movements of Latin America and the Caribbean with the objective of debating the new international scenario after the fall of the Berlin Wall and the consequences of the implementation of what were taken as neoliberal policies adopted at the time by contemporary right-leaning governments in the region, the stated main objective of the conference being to argue for alternatives to neoliberalism. Among its member include current socialist and social-democratic parties currently in government in the region such as Bolivia's Movement for Socialism, Brazil's Workers Party, the Communist Party of Cuba, Ecuador's PAIS Alliance, the United Socialist Party of Venezuela, the Socialist Party of Chile, Uruguay's Broad Front, Nicaragua's Sandinista National Liberation Front and El Salvador's Farabundo Martí National Liberation Front.

Australia saw an increase in interest of socialism in the early 21st century, especially amongst youth. It is strongest in Victoria, where three socialist parties have merged into the Victorian Socialists, who aim to address problems in housing and public transportation.

In New Zealand, socialism emerged within the budding trade union movement during the late 19th century and early 20th century. In July 1916, several left-wing political organisations and trade unions merged to form the New Zealand Labour Party. While Labour traditionally had a socialist orientation, the party shifted towards a more social democratic orientation during the 1920s and 1930s. Following the 1935 general election, the First Labour Government pursued socialist policies such as nationalising industry, broadcasting, transportation, and implementing a Keynesian welfare state. However, the party did not seek to abolish capitalism, instead opting for a mixed economy. Labour's welfare state and mixed economy were not challenged until the 1980s. During the 1980s, the Fourth Labour Government implemented a raft of neoliberal economic reforms known as Rogernomics which saw New Zealand society and the economy shift towards a more free market model. Labour's abandonment of its traditional values fractured the party. Successive Labour governments have since pursued centre-left social and economic policies while maintaining a free market economy. The current Prime Minister of New Zealand Jacinda Ardern formerly served as President of the International Union of Socialist Youth. Ardern is a self-described social democrat who has criticized capitalism as a "blatant failure" due to high levels of homelessness and low wages. New Zealand still has a small socialist scene, mainly dominated by Trotskyist groups.

Melanesian socialism developed in the 1980s, inspired by African socialism. It aims to achieve full independence from Britain and France in Melanesian territories and creation of a Melanesian federal union. It is very popular with the New Caledonia independence movement.

The Progressive Alliance is a political international founded on 22 May 2013 by political parties, the majority of whom are current or former members of the Socialist International. The organisation states the aim of becoming the global network of "the progressive", democratic, social-democratic, socialist and labour movement".

Early socialist thought took influences from a diverse range of philosophies such as civic republicanism, Enlightenment rationalism, romanticism, forms of materialism, Christianity (both Catholic and Protestant), natural law and natural rights theory, utilitarianism and liberal political economy. Another philosophical basis for a lot of early socialism was the emergence of positivism during the European Enlightenment. Positivism held that both the natural and social worlds could be understood through scientific knowledge and be analysed using scientific methods. This core outlook influenced early social scientists and different types of socialists ranging from anarchists like Peter Kropotkin to technocrats like Saint Simon.
The fundamental objective of socialism is to attain an advanced level of material production and therefore greater productivity, efficiency and rationality as compared to capitalism and all previous systems, under the view that an expansion of human productive capability is the basis for the extension of freedom and equality in society. Many forms of socialist theory hold that human behaviour is largely shaped by the social environment. In particular, socialism holds that social mores, values, cultural traits and economic practices are social creations and not the result of an immutable natural law. The object of their critique is thus not human avarice or human consciousness, but the material conditions and man-made social systems (i.e. the economic structure of society) that gives rise to observed social problems and inefficiencies. Bertrand Russell, often considered to be the father of analytic philosophy, identified as a socialist. Russell opposed the class struggle aspects of Marxism, viewing socialism solely as an adjustment of economic relations to accommodate modern machine production to benefit all of humanity through the progressive reduction of necessary work time.

Socialists view creativity as an essential aspect of human nature and define freedom as a state of being where individuals are able to express their creativity unhindered by constraints of both material scarcity and coercive social institutions. The socialist concept of individuality is thus intertwined with the concept of individual creative expression. Karl Marx believed that expansion of the productive forces and technology was the basis for the expansion of human freedom and that socialism, being a system that is consistent with modern developments in technology, would enable the flourishing of "free individualities" through the progressive reduction of necessary labour time. The reduction of necessary labour time to a minimum would grant individuals the opportunity to pursue the development of their true individuality and creativity.

Socialists argue that the accumulation of capital generates waste through externalities that require costly corrective regulatory measures. They also point out that this process generates wasteful industries and practices that exist only to generate sufficient demand for products to be sold at a profit (such as high-pressure advertisement), thereby creating rather than satisfying economic demand.

Socialists argue that capitalism consists of irrational activity, such as the purchasing of commodities only to sell at a later time when their price appreciates, rather than for consumption, even if the commodity cannot be sold at a profit to individuals in need and therefore a crucial criticism often made by socialists is that "making money", or accumulation of capital, does not correspond to the satisfaction of demand (the production of use-values). The fundamental criterion for economic activity in capitalism is the accumulation of capital for reinvestment in production, but this spurs the development of new, non-productive industries that do not produce use-value and only exist to keep the accumulation process afloat (otherwise the system goes into crisis), such as the spread of the financial industry, contributing to the formation of economic bubbles.

Socialists view private property relations as limiting the potential of productive forces in the economy. According to socialists, private property becomes obsolete when it concentrates into centralised, socialised institutions based on private appropriation of revenue"—"but based on cooperative work and internal planning in allocation of inputs"—"until the role of the capitalist becomes redundant. With no need for capital accumulation and a class of owners, private property in the means of production is perceived as being an outdated form of economic organisation that should be replaced by a free association of individuals based on public or common ownership of these socialised assets. Private ownership imposes constraints on planning, leading to uncoordinated economic decisions that result in business fluctuations, unemployment and a tremendous waste of material resources during crisis of overproduction.

Excessive disparities in income distribution lead to social instability and require costly corrective measures in the form of redistributive taxation, which incurs heavy administrative costs while weakening the incentive to work, inviting dishonesty and increasing the likelihood of tax evasion while (the corrective measures) reduce the overall efficiency of the market economy. These corrective policies limit the incentive system of the market by providing things such as minimum wages, unemployment insurance, taxing profits and reducing the reserve army of labour, resulting in reduced incentives for capitalists to invest in more production. In essence, social welfare policies cripple capitalism and its incentive system and are thus unsustainable in the long-run. Marxists argue that the establishment of a socialist mode of production is the only way to overcome these deficiencies. Socialists and specifically Marxian socialists argue that the inherent conflict of interests between the working class and capital prevent optimal use of available human resources and leads to contradictory interest groups (labour and business) striving to influence the state to intervene in the economy in their favour at the expense of overall economic efficiency.

Early socialists (utopian socialists and Ricardian socialists) criticised capitalism for concentrating power and wealth within a small segment of society. In addition, they complained that capitalism does not use available technology and resources to their maximum potential in the interests of the public.

Karl Marx and Friedrich Engels argued that socialism would emerge from historical necessity as capitalism rendered itself obsolete and unsustainable from increasing internal contradictions emerging from the development of the productive forces and technology. It was these advances in the productive forces combined with the old social relations of production of capitalism that would generate contradictions, leading to working-class consciousness.

Marx and Engels held the view that the consciousness of those who earn a wage or salary (the working class in the broadest Marxist sense) would be moulded by their conditions of wage slavery, leading to a tendency to seek their freedom or emancipation by overthrowing ownership of the means of production by capitalists and consequently, overthrowing the state that upheld this economic order. For Marx and Engels, conditions determine consciousness and ending the role of the capitalist class leads eventually to a classless society in which the state would wither away. The Marxist conception of socialism is that of a specific historical phase that would displace capitalism and precede communism. The major characteristics of socialism (particularly as conceived by Marx and Engels after the Paris Commune of 1871) are that the proletariat would control the means of production through a workers' state erected by the workers in their interests. Economic activity would still be organised through the use of incentive systems and social classes would still exist, but to a lesser and diminishing extent than under capitalism.

For orthodox Marxists, socialism is the lower stage of communism based on the principle of "from each according to his ability, to each according to his contribution" while upper stage communism is based on the principle of "from each according to his ability, to each according to his need", the upper stage becoming possible only after the socialist stage further develops economic efficiency and the automation of production has led to a superabundance of goods and services. Marx argued that the material productive forces (in industry and commerce) brought into existence by capitalism predicated a cooperative society since production had become a mass social, collective activity of the working class to create commodities but with private ownership (the relations of production or property relations). This conflict between collective effort in large factories and private ownership would bring about a conscious desire in the working class to establish collective ownership commensurate with the collective efforts their daily experience.

Socialists have taken different perspectives on the state and the role it should play in revolutionary struggles, in constructing socialism and within an established socialist economy.

In the 19th century the philosophy of state socialism was first explicitly expounded by the German political philosopher Ferdinand Lassalle. In contrast to Karl Marx's perspective of the state, Lassalle rejected the concept of the state as a class-based power structure whose main function was to preserve existing class structures. Thus Lassalle also rejected the Marxist view that the state was destined to "wither away". Lassalle considered the state to be an entity independent of class allegiances and an instrument of justice that would therefore be essential for achieving socialism.

Preceding the Bolshevik-led revolution in Russia, many socialists including reformists, orthodox Marxist currents such as council communism, anarchists and libertarian socialists criticised the idea of using the state to conduct central planning and own the means of production as a way to establish socialism. Following the victory of Leninism in Russia, the idea of "state socialism" spread rapidly throughout the socialist movement and eventually state socialism came to be identified with the Soviet economic model.

Joseph Schumpeter rejected the association of socialism (and social ownership) with state ownership over the means of production because the state as it exists in its current form is a product of capitalist society and cannot be transplanted to a different institutional framework. Schumpeter argued that there would be different institutions within socialism than those that exist within modern capitalism, just as feudalism had its own distinct and unique institutional forms. The state, along with concepts like property and taxation, were concepts exclusive to commercial society (capitalism) and attempting to place them within the context of a future socialist society would amount to a distortion of these concepts by using them out of context.

Utopian socialism is a term used to define the first currents of modern socialist thought as exemplified by the work of Henri de Saint-Simon, Charles Fourier and Robert Owen, which inspired Karl Marx and other early socialists. However, visions of imaginary ideal societies, which competed with revolutionary social democratic movements, were viewed as not being grounded in the material conditions of society and as reactionary. Although it is technically possible for any set of ideas or any person living at any time in history to be a utopian socialist, the term is most often applied to those socialists who lived in the first quarter of the 19th century who were ascribed the label "utopian" by later socialists as a negative term in order to imply naivete and dismiss their ideas as fanciful or unrealistic.

Religious sects whose members live communally such as the Hutterites, for example, are not usually called "utopian socialists", although their way of living is a prime example. They have been categorised as religious socialists by some. Likewise, modern intentional communities based on socialist ideas could also be categorised as "utopian socialist".

For Marxists, the development of capitalism in Western Europe provided a material basis for the possibility of bringing about socialism because according to "The Communist Manifesto" "[w]hat the bourgeoisie produces above all is its own grave diggers", namely the working class, which must become conscious of the historical objectives set it by society.

Revolutionary socialists believe that a social revolution is necessary to effect structural changes to the socioeconomic structure of society. Among revolutionary socialists there are differences in strategy, theory and the definition of "revolution". Orthodox Marxists and left communists take an impossibilist stance, believing that revolution should be spontaneous as a result of contradictions in society due to technological changes in the productive forces. Lenin theorised that under capitalism the workers cannot achieve class consciousness beyond organising into unions and making demands of the capitalists. Therefore, Leninists advocate that it is historically necessary for a vanguard of class conscious revolutionaries to take a central role in coordinating the social revolution to overthrow the capitalist state and eventually the institution of the state altogether. "Revolution" is not necessarily defined by revolutionary socialists as violent insurrection, but as a complete dismantling and rapid transformation of all areas of class society led by the majority of the masses: the working class.

Reformism is generally associated with social democracy and gradualist democratic socialism. Reformism is the belief that socialists should stand in parliamentary elections within capitalist society and if elected use the machinery of government to pass political and social reforms for the purposes of ameliorating the instabilities and inequities of capitalism.

Socialist economics starts from the premise that "individuals do not live or work in isolation but live in cooperation with one another. Furthermore, everything that people produce is in some sense a social product, and everyone who contributes to the production of a good is entitled to a share in it. Society as a whole, therefore, should own or at least control property for the benefit of all its members".

The original conception of socialism was an economic system whereby production was organised in a way to directly produce goods and services for their utility (or use-value in classical and Marxian economics): the direct allocation of resources in terms of physical units as opposed to financial calculation and the economic laws of capitalism (see law of value), often entailing the end of capitalistic economic categories such as rent, interest, profit and money. In a fully developed socialist economy, production and balancing factor inputs with outputs becomes a technical process to be undertaken by engineers.

Market socialism refers to an array of different economic theories and systems that use the market mechanism to organise production and to allocate factor inputs among socially owned enterprises, with the economic surplus (profits) accruing to society in a social dividend as opposed to private capital owners. Variations of market socialism include libertarian proposals such as mutualism, based on classical economics, and neoclassical economic models such as the Lange Model. However, some economists such as Joseph Stiglitz, Mancur Olson and others not specifically advancing anti-socialists positions have shown that prevailing economic models upon which such democratic or market socialism models might be based have logical flaws or unworkable presuppositions.

The ownership of the means of production can be based on direct ownership by the users of the productive property through worker cooperative; or commonly owned by all of society with management and control delegated to those who operate/use the means of production; or public ownership by a state apparatus. Public ownership may refer to the creation of state-owned enterprises, nationalisation, municipalisation or autonomous collective institutions. Some socialists feel that in a socialist economy, at least the "" of the economy must be publicly owned. However, economic liberals and right libertarians view private ownership of the means of production and the market exchange as natural entities or moral rights which are central to their conceptions of freedom and liberty and view the economic dynamics of capitalism as immutable and absolute, therefore they perceive public ownership of the means of production, cooperatives and economic planning as infringements upon liberty.

Management and control over the activities of enterprises are based on self-management and self-governance, with equal power-relations in the workplace to maximise occupational autonomy. A socialist form of organisation would eliminate controlling hierarchies so that only a hierarchy based on technical knowledge in the workplace remains. Every member would have decision-making power in the firm and would be able to participate in establishing its overall policy objectives. The policies/goals would be carried out by the technical specialists that form the coordinating hierarchy of the firm, who would establish plans or directives for the work community to accomplish these goals.

The role and use of money in a hypothetical socialist economy is a contested issue. According to the Austrian school economist Ludwig von Mises, an economic system that does not use money, financial calculation and market pricing would be unable to effectively value capital goods and coordinate production and therefore these types of socialism are impossible because they lack the necessary information to perform economic calculation in the first place. Socialists including Karl Marx, Robert Owen, Pierre-Joseph Proudhon and John Stuart Mill advocated various forms of labour vouchers or labour credits, which like money would be used to acquire articles of consumption, but unlike money they are unable to become capital and would not be used to allocate resources within the production process. Bolshevik revolutionary Leon Trotsky argued that money could not be arbitrarily abolished following a socialist revolution. Money had to exhaust its "historic mission", meaning it would have to be used until its function became redundant, eventually being transformed into bookkeeping receipts for statisticians and only in the more distant future would money not be required for even that role.

A planned economy is a type of economy consisting of a mixture of public ownership of the means of production and the coordination of production and distribution through economic planning. There are two major types of planning: decentralised-planning and centralised-planning. Enrico Barone provided a comprehensive theoretical framework for a planned socialist economy. In his model, assuming perfect computation techniques, simultaneous equations relating inputs and outputs to ratios of equivalence would provide appropriate valuations in order to balance supply and demand.

The most prominent example of a planned economy was the economic system of the Soviet Union and as such the centralised-planned economic model is usually associated with the communist states of the 20th century, where it was combined with a single-party political system. In a centrally planned economy, decisions regarding the quantity of goods and services to be produced are planned in advance by a planning agency (see also the analysis of Soviet-type economic planning). The economic systems of the Soviet Union and the Eastern Bloc are further classified as "command economies", which are defined as systems where economic coordination is undertaken by commands, directives and production targets. Studies by economists of various political persuasions on the actual functioning of the Soviet economy indicate that it was not actually a planned economy. Instead of conscious planning, the Soviet economy was based on a process whereby the plan was modified by localised agents and the original plans went largely unfulfilled. Planning agencies, ministries and enterprises all adapted and bargained with each other during the formulation of the plan as opposed to following a plan passed down from a higher authority, leading some economists to suggest that planning did not actually take place within the Soviet economy and that a better description would be an "administered" or "managed" economy.

Although central planning was largely supported by Marxist–Leninists, some factions within the Soviet Union before the rise of Stalinism held positions contrary to central planning. Leon Trotsky rejected central planning in favour of decentralised planning. He argued that central planners, regardless of their intellectual capacity, would be unable to coordinate effectively all economic activity within an economy because they operated without the input and tacit knowledge embodied by the participation of the millions of people in the economy. As a result, central planners would be unable to respond to local economic conditions. State socialism is unfeasible in this view because information cannot be aggregated by a central body and effectively used to formulate a plan for an entire economy, because doing so would result in distorted or absent price signals.

A self-managed, decentralised economy is based on autonomous self-regulating economic units and a decentralised mechanism of resource allocation and decision-making. This model has found support in notable classical and neoclassical economists including Alfred Marshall, John Stuart Mill and Jaroslav Vanek. There are numerous variations of self-management, including labour-managed firms and worker-managed firms. The goals of self-management are to eliminate exploitation and reduce alienation. Guild socialism is a political movement advocating workers' control of industry through the medium of trade-related guilds "in an implied contractual relationship with the public". It originated in the United Kingdom and was at its most influential in the first quarter of the 20th century. It was strongly associated with G. D. H. Cole and influenced by the ideas of William Morris.

One such system is the cooperative economy, a largely free market economy in which workers manage the firms and democratically determine remuneration levels and labour divisions. Productive resources would be legally owned by the cooperative and rented to the workers, who would enjoy usufruct rights. Another form of decentralised planning is the use of cybernetics, or the use of computers to manage the allocation of economic inputs. The socialist-run government of Salvador Allende in Chile experimented with Project Cybersyn, a real-time information bridge between the government, state enterprises and consumers. Another, more recent variant is participatory economics, wherein the economy is planned by decentralised councils of workers and consumers. Workers would be remunerated solely according to effort and sacrifice, so that those engaged in dangerous, uncomfortable and strenuous work would receive the highest incomes and could thereby work less. A contemporary model for a self-managed, non-market socialism is Pat Devine's model of negotiated coordination. Negotiated coordination is based upon social ownership by those affected by the use of the assets involved, with decisions made by those at the most localised level of production.

Michel Bauwens identifies the emergence of the open software movement and peer-to-peer production as a new alternative mode of production to the capitalist economy and centrally planned economy that is based on collaborative self-management, common ownership of resources and the production of use-values through the free cooperation of producers who have access to distributed capital.

Anarcho-communism is a theory of anarchism which advocates the abolition of the state, private property and capitalism in favour of common ownership of the means of production. Anarcho-syndicalism was practised in Catalonia and other places in the Spanish Revolution during the Spanish Civil War. Sam Dolgoff estimated that about eight million people participated directly or at least indirectly in the Spanish Revolution.

The economy of the former Socialist Federal Republic of Yugoslavia established a system based on market-based allocation, social ownership of the means of production and self-management within firms. This system substituted Yugoslavia's Soviet-type central planning with a decentralised, self-managed system after reforms in 1953.

The Marxian economist Richard D. Wolff argues that "re-organising production so that workers become collectively self-directed at their work-sites" not only moves society beyond both capitalism and state socialism of the last century, but would also mark another milestone in human history, similar to earlier transitions out of slavery and feudalism. As an example, Wolff claims that Mondragon is "a stunningly successful alternative to the capitalist organisation of production".

State socialism can be used to classify any variety of socialist philosophies that advocates the ownership of the means of production by the state apparatus, either as a transitional stage between capitalism and socialism, or as an end-goal in itself. Typically it refers to a form of technocratic management, whereby technical specialists administer or manage economic enterprises on behalf of society (and the public interest) instead of workers' councils or workplace democracy.

A state-directed economy may refer to a type of mixed economy consisting of public ownership over large industries, as promoted by various Social democratic political parties during the 20th century. This ideology influenced the policies of the British Labour Party during Clement Attlee's administration. In the biography of the 1945 United Kingdom Labour Party Prime Minister Clement Attlee, Francis Beckett states: "[T]he government... wanted what would become known as a mixed economy".

Nationalisation in the United Kingdom was achieved through compulsory purchase of the industry (i.e. with compensation). British Aerospace was a combination of major aircraft companies British Aircraft Corporation, Hawker Siddeley and others. British Shipbuilders was a combination of the major shipbuilding companies including Cammell Laird, Govan Shipbuilders, Swan Hunter and Yarrow Shipbuilders, whereas the nationalisation of the coal mines in 1947 created a coal board charged with running the coal industry commercially so as to be able to meet the interest payable on the bonds which the former mine owners' shares had been converted into.

Market socialism consists of publicly owned or cooperatively owned enterprises operating in a market economy. It is a system that uses the market and monetary prices for the allocation and accounting of the means of production, thereby retaining the process of capital accumulation. The profit generated would be used to directly remunerate employees, collectively sustain the enterprise or finance public institutions. In state-oriented forms of market socialism, in which state enterprises attempt to maximise profit, the profits can be used to fund government programs and services through a social dividend, eliminating or greatly diminishing the need for various forms of taxation that exist in capitalist systems. Neoclassical economist Léon Walras believed that a socialist economy based on state ownership of land and natural resources would provide a means of public finance to make income taxes unnecessary. Yugoslavia implemented a market socialist economy based on cooperatives and worker self-management.
Mutualism is an economic theory and anarchist school of thought that advocates a society where each person might possess a means of production, either individually or collectively, with trade representing equivalent amounts of labour in the free market. Integral to the scheme was the establishment of a mutual-credit bank that would lend to producers at a minimal interest rate, just high enough to cover administration. Mutualism is based on a labour theory of value that holds that when labour or its product is sold, in exchange it ought to receive goods or services embodying "the amount of labour necessary to produce an article of exactly similar and equal utility".

The current economic system in China is formally referred to as a socialist market economy with Chinese characteristics. It combines a large state sector that comprises the commanding heights of the economy, which are guaranteed their public ownership status by law, with a private sector mainly engaged in commodity production and light industry responsible from anywhere between 33% to over 70% of GDP generated in 2005. Although there has been a rapid expansion of private-sector activity since the 1980s, privatisation of state assets was virtually halted and were partially reversed in 2005. The current Chinese economy consists of 150 corporatised state-owned enterprises that report directly to China's central government. By 2008, these state-owned corporations had become increasingly dynamic and generated large increases in revenue for the state, resulting in a state-sector led recovery during the 2009 financial crises while accounting for most of China's economic growth. However, the Chinese economic model is widely cited as a contemporary form of state capitalism, the major difference between Western capitalism and the Chinese model being the degree of state-ownership of shares in publicly listed corporations.

The Socialist Republic of Vietnam has adopted a similar model after the Doi Moi economic renovation, but slightly differs from the Chinese model in that the Vietnamese government retains firm control over the state sector and strategic industries, but allows for private-sector activity in commodity production.

The major socialist political movements are described below. Independent socialist theorists, utopian socialist authors and academic supporters of socialism may not be represented in these movements. Some political groups have called themselves socialist while holding views that some consider antithetical to socialism. The term "socialist" has also been used by some politicians on the political right as an epithet against certain individuals who do not consider themselves to be socialists and against policies that are not considered socialist by their proponents.

There are many variations of socialism and as such there is no single definition encapsulating all of socialism. However, there have been common elements identified by scholars. In his "Dictionary of Socialism" (1924), Angelo S. Rappoport analysed forty definitions of socialism to conclude that common elements of socialism include: general criticisms of the social effects of private ownership and control of capital—as being the cause of poverty, low wages, unemployment, economic and social inequality and a lack of economic security; a general view that the solution to these problems is a form of collective control over the means of production, distribution and exchange (the degree and means of control vary amongst socialist movements); an agreement that the outcome of this collective control should be a society based upon social justice, including social equality, economic protection of people and should provide a more satisfying life for most people. In "The Concepts of Socialism" (1975), Bhikhu Parekh identifies four core principles of socialism and particularly socialist society: sociality, social responsibility, cooperation and planning. In his study "Ideologies and Political Theory" (1996), Michael Freeden states that all socialists share five themes: the first is that socialism posits that society is more than a mere collection of individuals; second, that it considers human welfare a desirable objective; third, that it considers humans by nature to be active and productive; fourth, it holds the belief of human equality; and fifth, that history is progressive and will create positive change on the condition that humans work to achieve such change.

Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions, but that several authors have defined as more specific institutions based on non-hierarchical free associations. Anarchism holds the state to be undesirable, unnecessary or harmful. While anti-statism is central, some argue that anarchism entails opposing authority or hierarchical organisation in the conduct of human relations including, but not limited to, the state system. Mutualists advocate market socialism, collectivist anarchists workers cooperatives and salaries based on the amount of time contributed to production, anarcho-communists advocate a direct transition from capitalism to libertarian communism and a gift economy and anarcho-syndicalists worker's direct action and the general strike.

Modern democratic socialism is a broad political movement that seeks to promote the ideals of socialism within the context of a democratic system. Some democratic socialists support social democracy as a temporary measure to reform the current system while others reject reformism in favour of more revolutionary methods. Modern social democracy emphasises a program of gradual legislative modification of capitalism in order to make it more equitable and humane, while the theoretical end goal of building a socialist society is either completely forgotten or redefined in a pro-capitalist way. The two movements are widely similar both in terminology and in ideology, although there are a few key differences.

The major difference between social democracy and democratic socialism is the object of their politics: contemporary social democrats support a welfare state and unemployment insurance as a means to "humanise" capitalism, whereas democratic socialists seek to replace capitalism with a socialist economic system, arguing that any attempt to "humanise" capitalism through regulations and welfare policies would distort the market and create economic contradictions.

Democratic socialism generally refers to any political movement that seeks to establish an economy based on economic democracy by and for the working class. Democratic socialism is difficult to define and groups of scholars have radically different definitions for the term. Some definitions simply refer to all forms of socialism that follow an electoral, reformist or evolutionary path to socialism rather than a revolutionary one.

Blanquism refers to a conception of revolution generally attributed to Louis Auguste Blanqui which holds that socialist revolution should be carried out by a relatively small group of highly organised and secretive conspirators. Having seized power, the revolutionaries would then use the power of the state to introduce socialism. It is considered a particular sort of "putschism"—that is, the view that political revolution should take the form of a "putsch" or "coup d'état". Rosa Luxemburg and Eduard Bernstein have criticised Vladimir Lenin that his conception of revolution was elitist and essentially Blanquist. Marxism–Leninism is a political ideology combining Marxism (the scientific socialist concepts theorised by Karl Marx and Friedrich Engels) and Leninism (Lenin's theoretical expansions of Marxism which include anti-imperialism, democratic centralism and party-building principles). Marxism–Leninism was the official ideology of the Communist Party of the Soviet Union and of the Communist International (1919–1943) and later it became the main guiding ideology for Trotskyists, Maoists and Stalinists.

Libertarian socialism, sometimes called left-libertarianism, social anarchism and socialist libertarianism, is a group of anti-authoritarian political philosophies inside the socialist movement that rejects socialism as centralised state ownership and control of the economy including criticism of wage labour relationships within the workplace, as well as the state itself. It emphasises workers' self-management of the workplace and decentralised structures of political organisation, asserting that a society based on freedom and equality can be achieved through abolishing authoritarian institutions that control certain means of production and subordinate the majority to an owning class or political and economic elite. Libertarian socialists generally place their hopes in decentralised means of direct democracy and federal or confederal associations such as libertarian municipalism, citizens' assemblies, trade unions, and workers' councils.

Relatedly, anarcho-syndicalist Gaston Leval explained: "We therefore foresee a Society in which all activities will be coordinated, a structure that has, at the same time, sufficient flexibility to permit the greatest possible autonomy for social life, or for the life of each enterprise, and enough cohesiveness to prevent all disorder...In a well-organised society, all of these things must be systematically accomplished by means of parallel federations, vertically united at the highest levels, constituting one vast organism in which all economic functions will be performed in solidarity with all others and that will permanently preserve the necessary cohesion". All of this is generally done within a general call for libertarian and voluntary human relationships through the identification, criticism and practical dismantling of illegitimate authority in all aspects of human life. As such, libertarian socialism within the larger socialist movement seeks to distinguish itself both from Leninism/Bolshevism and from social democracy.

Past and present political philosophies and movements commonly described as libertarian socialist include anarchism (especially anarcho-communism, anarcho-syndicalism collectivist anarchism and mutualism) as well as autonomism, Communalism, participism, revolutionary syndicalism and libertarian Marxist philosophies such as council communism and Luxemburgism; as well as some versions of utopian socialism and individualist anarchism.

Christian socialism is a broad concept involving an intertwining of the Christian religion with the politics and economic theories of socialism.

Islamic socialism is a term coined by various Muslim leaders to describe a more spiritual form of socialism. Muslim socialists believe that the teachings of the Qur'an and Muhammad are compatible with principles of equality and public ownership drawing inspiration from the early Medina welfare state established by Muhammad. Muslim socialists are more conservative than their western contemporaries and find their roots in anti-imperialism, anti-colonialism and Arab nationalism. Islamic socialist leaders believe in democracy and deriving legitimacy from public mandate as opposed to religious texts.

Social democracy is a political ideology which "is derived from a socialist tradition of political thought. Many social democrats refer to themselves as socialists or democratic socialists, and some, for example Tony Blair, use or have used these terms interchangeably. Others have opined that there are clear differences between the three terms, and preferred to describe their own political beliefs by using the term 'social democracy' only". There are two main directions, either to establish democratic socialism, or to build a welfare state within the framework of the capitalist system. The first variant has officially its goal by establishing democratic socialism through reformist and gradualist methods. In the second variant, social democracy becomes a policy regime involving a welfare state, collective bargaining schemes, support for publicly financed public services and a capitalist-based economy like a mixed economy. It is often used in this manner to refer to the social models and economic policies prominent in Western and Northern Europe during the later half of the 20th century. It has been described by Jerry Mander as "hybrid" economics, an active collaboration of capitalist and socialist visions and while such systems are not perfect they tend to provide high standards of living. Numerous studies and surveys indicate that people tend to live happier lives in social democratic societies rather than neoliberal ones.
Social democrats supporting the first variant advocate for a peaceful, evolutionary transition of the economy to socialism through progressive social reform of capitalism. It asserts that the only acceptable constitutional form of government is representative democracy under the rule of law. It promotes extending democratic decision-making beyond political democracy to include economic democracy to guarantee employees and other economic stakeholders sufficient rights of co-determination. It supports a mixed economy that opposes the excesses of capitalism such as inequality, poverty and oppression of various groups, while rejecting both a totally free market or a fully planned economy. Common social democratic policies include advocacy of universal social rights to attain universally accessible public services such as education, health care, workers' compensation and other services, including child care and care for the elderly. Social democracy is connected with the trade union labour movement and supports collective bargaining rights for workers. Most social democratic parties are affiliated with the Socialist International.

Liberal socialism is a socialist political philosophy that includes liberal principles within it. Liberal socialism does not have the goal of abolishing capitalism with a socialist economy, instead it supports a mixed economy that includes both public and private property in capital goods. Although liberal socialism unequivocally favours a mixed market economy, it identifies legalistic and artificial monopolies to be the fault of capitalism and opposes an entirely unregulated economy. It considers both liberty and equality to be compatible and mutually dependent on each other. Principles that can be described as "liberal socialist" have been based upon or developed by the following philosophers: John Stuart Mill, Eduard Bernstein, John Dewey, Carlo Rosselli, Norberto Bobbio and Chantal Mouffe. Other important liberal socialist figures include Guido Calogero, Piero Gobetti, Leonard Trelawny Hobhouse, John Maynard Keynes and R. H. Tawney. Liberal socialism has been particularly prominent in British and Italian politics.

Socialist feminism is a branch of feminism that focuses upon both the public and private spheres of a woman's life and argues that liberation can only be achieved by working to end both the economic and cultural sources of women's oppression. Marxist feminism's foundation is laid by Friedrich Engels in his analysis of gender oppression in "The Origin of the Family, Private Property, and the State" (1884). August Bebel's "Woman under Socialism" (1879), the "single work dealing with sexuality most widely read by rank-and-file members of the Social Democratic Party of Germany (SPD)". In the late 19th and early 20th centuries, both Clara Zetkin and Eleanor Marx were against the demonisation of men and supported a proletariat revolution that would overcome as many male-female inequalities as possible. As their movement already had the most radical demands in women's equality, most Marxist leaders, including Clara Zetkin and Alexandra Kollontai, counterposed Marxism against liberal feminism rather than trying to combine them. Anarcha-feminism began with late 19th and early 20th century authors and theorists such as anarchist feminists Emma Goldman and Voltairine de Cleyre In the Spanish Civil War, an anarcha-feminist group, ("Free Women") linked to the , organised to defend both anarchist and feminist ideas. In 1972, the Chicago Women's Liberation Union published "Socialist Feminism: A Strategy for the Women's Movement", which is believed to be the first to use the term "socialist feminism" in publication.
Many socialists were early advocates for LGBT rights. For early socialist Charles Fourier, true freedom could only occur without suppressing passions, as the suppression of passions is not only destructive to the individual, but to society as a whole. Writing before the advent of the term "homosexuality", Fourier recognised that both men and women have a wide range of sexual needs and preferences which may change throughout their lives, including same-sex sexuality and "androgénité". He argued that all sexual expressions should be enjoyed as long as people are not abused and that "affirming one's difference" can actually enhance social integration. In Oscar Wilde's "The Soul of Man Under Socialism", he passionately advocates for an egalitarian society where wealth is shared by all, while warning of the dangers of social systems that crush individuality. Wilde's libertarian socialist politics were shared by other figures who actively campaigned for homosexual emancipation in the late 19th century such as Edward Carpenter. "The Intermediate Sex: A Study of Some Transitional Types of Men and Women" was a book from 1908 and an early work arguing for gay liberation written by Edward Carpenter who was also an influential personality in the foundation of the Fabian Society and the Labour Party. After the Russian Revolution under the leadership of Vladimir Lenin and Leon Trotsky, the Soviet Union abolished previous laws against homosexuality. Harry Hay was an early leader in the American LGBT rights movement as well as a member of the Communist Party USA. He is known for his roles in helping to found several gay organisations, including the Mattachine Society, the first sustained gay rights group in the United States which in its early days had a strong Marxist influence. The "Encyclopedia of Homosexuality" reports that "[a]s Marxists the founders of the group believed that the injustice and oppression which they suffered stemmed from relationships deeply embedded in the structure of American society". Also emerging from a number of events, such as the May 1968 insurrection in France, the anti-Vietnam war movement in the United States and the Stonewall riots of 1969, militant gay liberation organisations began to spring up around the world. Many saw their roots in left radicalism more than in the established homophile groups of the time, though the Gay Liberation Front took an anti-capitalist stance and attacked the nuclear family and traditional gender roles.

Eco-socialism, green socialism or socialist ecology is a political position merging aspects of Marxism, socialism or libertarian socialism with that of green politics, ecology and alter-globalisation. Eco-socialists generally believe that the expansion of the capitalist system is the cause of social exclusion, poverty, war and environmental degradation through globalisation and imperialism, under the supervision of repressive states and transnational structures. Contrary to the depiction of Karl Marx by some environmentalists, social ecologists and fellow socialists as a productivist who favoured the domination of nature, eco-socialists have revisited Marx's writings and believe that he "was a main originator of the ecological world-view". Eco-socialist authors, like John Bellamy Foster and Paul Burkett, point to Marx's discussion of a "metabolic rift" between man and nature, his statement that "private ownership of the globe by single individuals will appear quite absurd as private ownership of one man by another" and his observation that a society must "hand it [the planet] down to succeeding generations in an improved condition". The English socialist William Morris is largely credited with developing key principles of what was later called eco-socialism. During the 1880s and 1890s, Morris promoted his eco-socialist ideas within the Social Democratic Federation and Socialist League. Green anarchism, or ecoanarchism, is a school of thought within anarchism which puts a particular emphasis on environmental issues. An important early influence was the thought of the American anarchist Henry David Thoreau and his book "Walden" and Élisée Reclus.

In the late 19th century, there emerged anarcho-naturism as the fusion of anarchism and naturist philosophies within individualist anarchist circles in France, Spain, Cuba and Portugal. Social ecology is closely related to the work and ideas of Murray Bookchin and influenced by anarchist Peter Kropotkin. Bookchin's first book, "Our Synthetic Environment," was published under the pseudonym Lewis Herber in 1962, a few months before Rachel Carson's "Silent Spring". His groundbreaking essay "Ecology and Revolutionary Thought" introduced ecology as a concept in radical politics. In the 1970s, Barry Commoner, suggesting a left-wing response to the "Limits to Growth" model that predicted catastrophic resource depletion and spurred environmentalism, postulated that capitalist technologies were chiefly responsible for environmental degradation as opposed to population pressures. The 1990s saw the socialist feminists Mary Mellor and Ariel Salleh address environmental issues within an eco-socialist paradigm. With the rising profile of the anti-globalisation movement in the Global South, an "environmentalism of the poor" combining ecological awareness and social justice has also become prominent. In 1994, David Pepper also released his important work, "Ecosocialism: From Deep Ecology to Social Justice", which critiques the current approach of many within green politics, particularly deep ecologists. Currently, many green parties around the world, such as the Dutch Green Left Party (GroenLinks), contain strong eco-socialist elements. Radical red-green alliances have been formed in many countries by eco-socialists, radical greens and other radical left groups. In Denmark, the Red-Green Alliance was formed as a coalition of numerous radical parties. Within the European Parliament, a number of leftist parties from Northern Europe have organised themselves into the Nordic Green Left Alliance.

Syndicalism is a social movement that operates through industrial trade unions and rejects state socialism and the use of establishment politics to establish or promote socialism. They reject using state power to construct a socialist society, favouring strategies such as the general strike. Syndicalists advocate a socialist economy based on federated unions or syndicates of workers who own and manage the means of production. Some Marxist currents advocate syndicalism, such as De-Leonism. Anarcho-syndicalism is a theory of anarchism which views syndicalism as a method for workers in capitalist society to gain control of an economy and with that control influence broader society. The Spanish Revolution largely orchestrated by the anarcho-syndicalist trade union CNT during the Spanish Civil War offers an historical example. The International Workers' Association is an international federation of anarcho-syndicalist labour unions and initiatives.





</doc>
<doc id="26849" url="https://en.wikipedia.org/wiki?curid=26849" title="Sabine River (Texas–Louisiana)">
Sabine River (Texas–Louisiana)

The Sabine River () is a river, long, in the Southern U.S. states of Texas and Louisiana. In its lower course, it forms part of the boundary between the two states and empties into Sabine Lake, an estuary of the Gulf of Mexico. Over the first half of the 19th century, the river formed part of the Spanish–American, Mexican–American, and Texan–American international boundaries. The upper reaches of the river flow through the prairie country of northeast Texas. Along much of its lower reaches, it flows through the pine forests along the Texas–Louisiana border, and the bayou country near the Gulf Coast.

The river drains an area of , of which are in Texas and in Louisiana. It flows through an area of abundant rainfall and discharges the largest volume of any river in Texas. The name Sabine (Sp: "Río de Sabinas") comes from the Spanish word for cypress, in reference to the extensive growth of bald cypresses along the lower river. The river flows through an important petroleum-producing region, and the lower river near the Gulf is among the most industrialized areas of the southeastern United States. The river was often described as the dividing line between the Old South and the New Southwest.

The Sabine rises in northeast Texas by the union of three branches: the Cowleech Fork, Caddo Fork, and South Fork. The Cowleech Fork rises in northwestern Hunt County and flows southeast for . The Caddo Fork, shown as "Caddo Creek" on federal maps, rises in two tributary forks, the East Caddo Fork and the West Caddo Fork, in northwestern Hunt County. The South Fork rises in the southwestern corner of Hunt County and flows east for , joining the Caddo Fork and Cowleech Fork in southeastern Hunt County. The confluence of the forks is now submerged in the Lake Tawakoni reservoir. The combined river flows southeast across northeast Texas and is joined by a fourth branch, Lake Fork Creek, downstream from the reservoir.

In northeast Texas, the river flows past Mineola, Gladewater, Big Sandy, and Longview, the largest city on the river, to southwest of Shreveport at the 32nd parallel north, where it establishes the Texas-Louisiana boundary. It flows south, forming the state line for the remainder of its course. It is impounded west of Leesville, Louisiana, to form the Toledo Bend Reservoir, with the Sabine National Forest along its western bank. South of the reservoir, it passes through the bayou country, surrounded by wetlands, as well as widespread industrial areas near the Gulf Coast. Approximately south of Orange, it meets the Neches River from the west to form the and Sabine Lake, which drains through Sabine Pass to the Gulf of Mexico. The city of Port Arthur, Texas, sits along the western shore of Sabine Lake

Archeological evidence indicates the valley of the river has been inhabited for as long as 12,000 years by indigenous peoples. Starting in the eighth century, the Caddo inhabited the area, building extensive earthwork mounds in complexes expressing their cosmology. The Caddo culture flourished until the late 13th century. Descendants of the Caddo were living along the river when the first European explorers arrived in the 16th century.

The river was named in 1716 by Spanish explorer Domingo Ramón, and appeared as "Río de Sabinas" on a 1721 map. The river was used by French traders, and at various times, the river was claimed by both Spain and France. After the acquisition by Spain of the French territory of Louisiana in 1763, following France's defeat by Great Britain in the Seven Years' War, the capital of the Spanish province of Texas was established on the east side of the river, near present-day Robeline, Louisiana.

After acquiring the French territory west of the Mississippi River in the 1803 Louisiana Purchase, the United States started to exert control in this area. It was at war with Native Americans in Louisiana along the Sabine River from 1836 to 1837, in the period when it was trying to remove the Indians to Indian Territory from the Southeast.

The Sabine River was too deep to ford, and proved to be navigable. Early travelers and settlers would have to swim the river on horseback and cattle would have to be driven into the river to swim across. Ferries were later put into service. By the 1840s, steamboats were travelling from Logansport to Sabine Lake.

Recorded ferry use began 1794, when Louis Chabinan (Sharben), his wife Margarite LaFleur, and their four children settled on the east bank of the Sabine River on land purchased from Vicinte Michele. Chabinan built a ferry landing on the river called "Paso del Chaland." Louisiana State Highway 6 (La 6) and Texas State Highway 21 now meet near here, at the site of the present-day Pendleton Bridge. In 1796, Chabinan was drowned after being kicked by a horse and falling into the Sabine.

Michel Crow married his widow and ran the ferry, until he sold it to James Gaines "circa" 1819; it was renamed Gaines Ferry. This ferry was in service until 1937, when it was replaced by the Pendleton Bridge, built during the Great Depression. Crow also operated a ferry he had started upriver, a 120-foot crossing started in 1796. It linked what became known as Carter's Ferry Road, now Texas FM 276. Carter's ferry was 25 miles from San Augustine and 15 miles from Many, Louisiana. Crow sold the ferry to Carter, who became the namesake. Farther north, and just above Bayou Lanan, was Williamson Ferry. 
Other ferries on the Sabine River:
The main Sabine River crossings were the El Camino Real (King's Highway) from Natchitoches, or "Upper Route" from Shreveport; and the "Lower" Route, from Opelousas called "The Old Beef Trail". It was used to drive thousands of cattle from Texas to Alexandria, Louisiana, for shipment to cities such as New Orleans. Hickman Ferry was a shipping point for areas as far west as Burkeville. Sabine River ports from Sabine Pass in river mileage were "Belgrade", 171 miles; "Stark's Landing" 191 miles; "Loftin Ferry", and "Bayou Lanacoco" 220 miles; "Hickman's Ferry" 252 miles; "Burnham's Landing" 261 miles; and "Burr's Ferry" 281 miles.

The area's geography remained one of the least understood in the region. Various Spanish maps had errors in the naming of the Sabine and Neches, and sometimes showed them flowing independently into the Gulf of Mexico. After the Louisiana Purchase by the United States in 1803, a dispute over the boundary between the U.S. and Spain led to an agreement on November 6, 1806, negotiated by Gen.James Wilkinson and Lt. Col. Simón de Herrera, to establish a neutral territory on both sides of the river. Neither country would put military troops or civil police there.

The indefinite boundary was resolved by the Adams-Onis Treaty of 1819, which established the Sabine River as the boundary from the Gulf to the 32nd parallel. The Spanish delay in the ratification of the treaty, and Mexico gaining independence in 1821, reignited the boundary dispute. The United States, at the insistence of Anthony Butler, claimed for a while that the names of the Sabine and Neches had been reversed, thus they claimed that the treaty established the boundary at the Neches. The first Anglo-American settlers began arriving in the region in the 1820s, soon outnumbering the Mexicans by ten to one. After the independence of the Republic of Texas from Mexico in 1836, the boundary between the U.S. and Texas was firmly established at the Sabine in accordance with the Adams-Onis Treaty. The river served as the western boundary of the United States until it annexed Texas in 1845.

In 1843, Capt. John Clemmons made the first trip up the Sabine in the steamboat "Sabine." Steamboats carried passengers, as well as commodities such as cotton, from as far north as Logansport, Louisiana, down to Sabine Pass.

The pirate Jean Lafitte made many trips up the Sabine and reportedly started the colony of Shacklefoot on the Texas side of the Sabine River, south of Carter's ferry up Bayou Patroon.

During the American Civil War, on September 8, 1863, a small Confederate force thwarted a Union invasion of Texas at the Second Battle of Sabine Pass, fought at the mouth of the river.

In the late 19th and early 20th centuries, the middle course of the river was an area of widespread logging. The discovery of petroleum at nearby Spindletop led to the river basin becoming the scene of widespread oil drilling. The lower river became heavily industrialized, developed with many oil refineries and chemical plants. Such alteration to the wetlands resulted in a degradation of the water quality. Since the late 20th century, there have been federal, state, and local efforts to restore the quality of the river. In addition, draining of wetlands and dredging of bayous has caused decline in the acreage of wetlands, resulting in coastal erosion, and making the area much more vulnerable to hurricane damage.

The lower river, south of Orange to Sabine Lake, forms part of the Intracoastal Waterway, carrying barge traffic and some pleasure boats.

As a young man, Captain Bill McDonald of the Texas Rangers operated a small store at Brown's Bluff (modern-day Elderville) on the Sabine in Gregg County, Texas.

Hadden's Ferry was the site of the ground-breaking ceremony held on October 5, 1961, for the 181,600-acre Toledo Bend Reservoir. Dedicated October 11, 1969, the reservoir is the largest man-made lake in the South. Flooding of lands along the Sabine River behind the dam inundated all the ferry sites within its boundary.

The 1970 Louisiana Legislature passed Acts 90 and 117, creating the Sabine River Diversion Canal, for the purpose of supplying fresh river water to businesses in Lake Charles, Sulphur, Westlake, and what was Mossville (now the Sasol complex), as well as to farmers along the canal, with a total capacity of a day. The canal was completed by the Louisiana Department of Public Works in 1981. The canal is long, with about of underground pipe, and begins on the Old Sabine River north of Niblett's Bluff. Pump station #1 is located 2 miles east of the river. The canal continues running east, piped under roadways such as Louisiana Highway 109 north of Vinton, the Edgerly Big Woods road, and Highway 388, which runs to Dequincy.

Just east of Louisiana Highway 27, the canal forks to the south, running around southern Sulphur. The canal is piped under Louisiana Highway 108, at pumping station #4, providing river water to the business area known as City Service in Westlake, and companies such as Equistar, which has a daily contract for 734,400 gallons a day. Other customers and their gallons of use per day are the city of Westlake (8,640,000 gallons), Air Liquide (129,600), Air Products (1,728,000), CITGO (20,160,000), Phillips 66 (3,600,000), The Axiall subsidiary Eagle US 2 LLC (20,160,000), Entergy (21,600,000), Lake Charles Co-Gen (14,400,000), Louisiana Pigment (3,038,400) that produces Titanium White, another LyondellBasell company (720,000), and Matheson Tri-Gas (175,680).

The main canal continues east, crossing under Highway 27 and joined by the Houston River canal at pumping station #2, continuing to old Mossville. There it tees to the left, providing water to the Krause and Managan canal supplying the Nelson Industrial Steam Company (Nisco), which supplies steam and electricity to area businesses. The right tee of the canal terminates at pumping station #3 on what was 8th street in Mossville, now the Sasol complex, providing 46,080,000 gallons of river water for a total daily contract use of 141,166,000 gallons of river water a day.


Up to 450,000 gallons (about 11,000 bls) of crude oil spilled over the Sabine River when the tanker "Eagle Otome", which was carrying the shipment, struck two chemical-carrying barges due to loss of engine power on January 24, 2010, at 10 am local time.

Severe flooding during the first week of March 2016 was the result of record rainfalls in northern Louisiana and the Sabine River basin, of 18 to more than 24 inches. Toledo Bend Reservoir is considered at "full pool" at 172 ft; before the rains started, it was at 171.5 ft. On March 10, the level reached a record 174.36 ft, and 9 of the 11 gates were opened to 22 ft (two gates were out of commission for repairs). Lake Tawakoni, east of Dallas on the Sabine River, was 2 feet above full pool and Lake Fork Reservoir was 1 1/2 feet above full pool.

When the reservoir level dropped to 173.69 ft, 9 gates were in operation at 20 ft. The previous record level of 173.93 ft was on May 18, 1989. At that time, the spillway gates were opened to 9 ft. The maximum height is 28 ft and with nine 9 gates open, the discharge rate is over 190,000 ft per second, which is equivalent to the flow over Niagara Falls.
The peak water flow from the dam was nearly 208,000 ft per second for 31 hours, equating to 1.5 million gallons per second. Catastrophic flooding was predicted to be from 2 to 5 ft above record floods of 1884 and 1889.

During peak flooding, Deweyville, Texas was surrounded by water, accessible only by air or boat. The flood stage is 24 ft, but reached 33.24 ft on March 10, 2016, which was 9.24 ft above flood stage.

A group of Texas residents who suffered damage in the flooding met March 17, 2016, to discuss a class-action suit against the Sabine River Authority (SRA), based on their belief that it had mismanaged water release. The issue is under review by counsel.

According to local ABC affiliate KBMT-TV, SRA spokesperson Ann Galassi stated that the SRA has guidelines it has to follow and those cannot be altered based on weather forecasts. She said that the guidelines are designed to protect the infrastructure of the dam. After the record flood event, the regulatory commission could possibly review the guidelines, and she said that the SRA would welcome that. The SRA of Texas states, "The Authority was created as a conservation and reclamation district with responsibilities to control, store, preserve, and distribute the waters of the Sabine River and its tributary streams for useful purposes." The site also states, "Toledo Bend Project-since its inception and original development over 50 years ago-has never been a flood-control facility. Rather, the project is regulated, as set forth in the project license, to accommodate a number of public benefits, including water supply, recreation, and hydropower production.".





</doc>
<doc id="26859" url="https://en.wikipedia.org/wiki?curid=26859" title="Synergy">
Synergy

Synergy is the creation of a whole that is greater than the simple sum of its parts. The term "synergy" comes from the Attic Greek word συνεργία ' from ', , meaning "working together".

The words "synergy" and "synergetic" have been used in the field of physiology since at least the middle of the 19th century:

SYN'ERGY, "Synergi'a", "Synenergi'a", (F.) "Synergie"; from "συν", 'with', and "εργον", 'work'. A correlation or concourse of action between different organs in health; and, according to some, in disease.

In 1896, applied the term "synergy" to social psychology by writing "La synergie sociale", in which he argued that Darwinian theory failed to account of "social synergy" or "social love", a collective evolutionary drive. The highest civilizations were the work not only of the elite but of the masses too; those masses must be led, however, because the crowd, a feminine and unconscious force, cannot distinguish between good and evil.

In 1909, Lester Frank Ward defined synergy as the universal constructive principle of nature:

I have characterized the social struggle as centrifugal and social solidarity as centripetal. Either alone is productive of evil consequences. Struggle is essentially destructive of the social order, while communism removes individual initiative. The one leads to disorder, the other to degeneracy. What is not seen—the truth that has no expounders—is that the wholesome, constructive movement consists in the properly ordered combination and interaction of both these principles. This is "social synergy", which is a form of cosmic synergy, the universal constructive principle of nature.
In the natural world, synergistic phenomena are ubiquitous, ranging from physics (for example, the different combinations of quarks that produce protons and neutrons) to chemistry (a popular example is water, a compound of hydrogen and oxygen), to the cooperative interactions among the genes in genomes, the division of labor in bacterial colonies, the synergies of scale in multi-cellular organisms, as well as the many different kinds of synergies produced by socially-organized groups, from honeybee colonies to wolf packs and human societies: compare stigmergy, a mechanism of indirect coordination between agents or actions that results in the self-assembly of complex systems. Even the tools and technologies that are widespread in the natural world represent important sources of synergistic effects. The tools that enabled early hominins to become systematic big-game hunters is a primordial human example.
In the context of organizational behavior, following the view that a cohesive group is more than the sum of its parts, synergy is the ability of a group to outperform even its best individual member. These conclusions are derived from the studies conducted by Jay Hall on a number of laboratory-based group ranking and prediction tasks. He found that effective groups actively looked for the points in which they disagreed and in consequence encouraged conflicts amongst the participants in the early stages of the discussion. In contrast, the ineffective groups felt a need to establish a common view quickly, used simple decision making methods such as averaging, and focused on completing the task rather than on finding solutions they could agree on.
In a technical context, its meaning is a construct or collection of different elements working together to produce results not obtainable by any of the elements alone. The elements, or parts, can include people, hardware, software, facilities, policies, documents: all things required to produce system-level results. The value added by the system as a whole, beyond that contributed independently by the parts, is created primarily by the relationship among the parts, that is, how they are interconnected. In essence, a system constitutes a set of interrelated components working together with a common objective: fulfilling some designated need.

If used in a business application, synergy means that teamwork will produce an overall better result than if each person within the group were working toward the same goal individually. However, the concept of group cohesion needs to be considered. Group cohesion is that property that is inferred from the number and strength of mutual positive attitudes among members of the group. As the group becomes more cohesive, its functioning is affected in a number of ways. First, the interactions and communication between members increase. Common goals, interests and small size all contribute to this. In addition, group member satisfaction increases as the group provides friendship and support against outside threats.

There are negative aspects of group cohesion that have an effect on group decision-making and hence on group effectiveness. There are two issues arising. The risky shift phenomenon is the tendency of a group to make decisions that are riskier than those that the group would have recommended individually. Group Polarisation is when individuals in a group begin by taking a moderate stance on an issue regarding a common value and, after having discussed it, end up taking a more extreme stance.

A second, potential negative consequence of group cohesion is group think. Group think is a mode of thinking that people engage in when they are deeply involved in cohesive group, when the members' striving for unanimity overrides their motivation to appraise realistically the alternative courses of action. Studying the events of several American policy "disasters" such as the failure to anticipate the Japanese attack on Pearl Harbor (1941) and the Bay of Pigs Invasion fiasco (1961), Irving Janis argued that they were due to the cohesive nature of the committees that made the relevant decisions.

That decisions made by committees lead to failure in a simple system is noted by Dr. Chris Elliot. His case study looked at IEEE-488, an international standard set by the leading US standards body; it led to a failure of small automation systems using the IEEE-488 standard (which codified a proprietary communications standard HP-IB). But the external devices used for communication were made by two different companies, and the incompatibility between the external devices led to a financial loss for the company. He argues that systems will be safe only if they are designed, not if they emerge by chance.
The idea of a systemic approach is endorsed by the United Kingdom Health and Safety Executive. The successful performance of the health and safety management depends upon the analyzing the causes of incidents and accidents and learning correct lessons from them. The idea is that all events (not just those causing injuries) represent failures in control, and present an opportunity for learning and improvement. UK Health and Safety Executive, "Successful health and safety management" (1997): this book describes the principles and management practices, which provide the basis of effective health and safety management. It sets out the issues that need to be addressed, and can be used for developing improvement programs, self-audit, or self-assessment. Its message is that organizations must manage health and safety with the same degree of expertise and to the same standards as other core business activities, if they are to effectively control risks and prevent harm to people.

The term synergy was refined by R. Buckminster Fuller, who analyzed some of its implications more fully and coined the term synergetics.


Synergy of various kinds has been advanced by Peter Corning as a causal agency that can explain the progressive evolution of complexity in living systems over the course of time. According to the Synergism Hypothesis, synergistic effects have been the drivers of cooperative relationships of all kinds and at all levels in living systems. The thesis, in a nutshell, is that synergistic effects have often provided functional advantages (economic benefits) in relation to survival and reproduction that have been favored by natural selection. The cooperating parts, elements, or individuals become, in effect, functional “units” of selection in evolutionary change. Similarly, environmental systems may react in a non-linear way to perturbations, such as climate change, so that the outcome may be greater than the sum of the individual component alterations. Synergistic responses are a complicating factor in environmental modeling.

Pest synergy would occur in a biological host organism population, where, for example, the introduction of parasite A may cause 10% fatalities, and parasite B may also cause 10% loss. When both parasites are present, the losses would normally be expected to total less than 20%, yet, in some cases, losses are significantly greater. In such cases, it is said that the parasites in combination have a synergistic effect.

Mechanisms that may be involved in the development of synergistic effects include:

More mechanisms are described in an exhaustive 2009 review.

Toxicological synergy is of concern to the public and regulatory agencies because chemicals individually considered safe might pose unacceptable health or ecological risk in combination. Articles in scientific and lay journals include many definitions of chemical or toxicological synergy, often vague or in conflict with each other. Because toxic interactions are defined relative to the expectation under "no interaction", a determination of synergy (or antagonism) depends on what is meant by "no interaction". The United States Environmental Protection Agency has one of the more detailed and precise definitions of toxic interaction, designed to facilitate risk assessment. In their guidance documents, the no-interaction default assumption is dose addition, so synergy means a mixture response that exceeds that predicted from dose addition. The EPA emphasizes that synergy does not always make a mixture dangerous, nor does antagonism always make the mixture safe; each depends on the predicted risk under dose addition.

For example, a consequence of pesticide use is the risk of health effects. During the registration of pesticides in the United States exhaustive tests are performed to discern health effects on humans at various exposure levels. A regulatory upper limit of presence in foods is then placed on this pesticide. As long as residues in the food stay below this regulatory level, health effects are deemed highly unlikely and the food is considered safe to consume.

However, in normal agricultural practice, it is rare to use only a single pesticide. During the production of a crop, several different materials may be used. Each of them has had determined a regulatory level at which they would be considered individually safe. In many cases, a commercial pesticide is itself a combination of several chemical agents, and thus the safe levels actually represent levels of the mixture. In contrast, a combination created by the end user, such as a farmer, has rarely been tested in that combination. The potential for synergy is then unknown or estimated from data on similar combinations. This lack of information also applies to many of the chemical combinations to which humans are exposed, including residues in food, indoor air contaminants, and occupational exposures to chemicals. Some groups think that the rising rates of cancer, asthma, and other health problems may be caused by these combination exposures; others have alternative explanations. This question will likely be answered only after years of exposure by the population in general and research on chemical toxicity, usually performed on animals. Examples of pesticide synergists include Piperonyl butoxide and MGK 264.

Human synergy relates to human interaction and teamwork. For example, say person A alone is too short to reach an apple on a tree and person B is too short as well. Once person B sits on the shoulders of person A, they are tall enough to reach the apple. In this example, the product of their synergy would be one apple. Another case would be two politicians. If each is able to gather one million votes on their own, but together they were able to appeal to 2.5 million voters, their synergy would have produced 500,000 more votes than had they each worked independently. A song is also a good example of human synergy, taking more than one musical part and putting them together to create a song that has a much more dramatic effect than each of the parts when played individually.

A third form of human synergy is when one person is able to complete two separate tasks by doing one action, for example, if a person were asked by a teacher and his boss at work to write an essay on how he could improve his work. A more visual example of this synergy is a drummer using four separate rhythms to create one drum beat.

Synergy usually arises when two persons with different complementary skills cooperate. In business, cooperation of people with organizational and technical skills happens very often. In general, the most common reason why people cooperate is that it brings a synergy. On the other hand, people tend to specialize just to be able to form groups with high synergy (see also division of labor and teamwork).

Example: Two teams in System Administration working together to combine technical and organizational skills in order to better the client experience, thus creating synergy. Counter-examples can be found in books like The Mythical Man-Month, in which the addition of additional team members is shown to have negative effects on productivity.

Organismic computing is an approach to improving group efficacy by increasing synergy in human groups via technological means.

When synergy occurs in the work place, the individuals involved get to work in a positive and supportive working environment. When individuals get to work in environments such as these, the company reaps the benefits. The authors of "Creating the Best Workplace on Earth" Rob Goffee and Gareth Jones, state that "highly engaged employees are, on average, 50% more likely to exceed expectations that the least-engaged workers. And companies with highly engaged people outperform firms with the most disengaged folks- by 54% in employee retention, by 89% in customer satisfaction, and by fourfold in revenue growth (Goffee & Jones, pg. 100)." Also, those that are able to be open about their views on the company, and have confidence that they will be heard, are likely to be a more organized employee who helps his/ her fellow team members succeed.

Corporate synergy occurs when corporations interact congruently. A corporate synergy refers to a financial benefit that a corporation
expects to realize when it merges with or acquires another corporation. This type of synergy is a nearly ubiquitous feature of a corporate acquisition and is a negotiating point between the buyer and seller that impacts the final price both parties agree to. There are distinct types of corporate synergies, as follows.

A marketing synergy refers to the use of information campaigns, studies, and scientific discovery or experimentation for research or development. This promotes the sale of products for varied use or off-market sales as well as development of marketing tools and in several cases exaggeration of effects. It is also often a meaningless buzzword used by corporate leaders.

A revenue synergy refers to the opportunity of a combined corporate entity to generate more revenue than its two predecessor stand-alone companies would be able to generate. For example, if company A sells product X through its sales force, company B sells product Y, and company A decides to buy company B then the new company could use each sales person to sell products X and Y, thereby increasing the revenue that each sales person generates for the company.

In media revenue, synergy is the promotion and sale of a product throughout the various subsidiaries of a media conglomerate, e.g. films, soundtracks, or video games.

Financial synergy gained by the combined firm is a result of number of benefits which flow to the entity as a consequence of acquisition and merger. These benefits may be:

This is when a firm having number of cash extensive projects acquires a firm which is cash-rich, thus enabling the new combined firm to enjoy the profits from investing the cash of one firm in the projects of the other.

If two firms have no or little capacity to carry debt before individually, it is possible for them to join and gain the capacity to carry the debt through decreased gearing (leverage). This creates value for the firm, as debt is thought to be a cheaper source of finance.

It is possible for one firm to have unused tax benefits which might be offset against the profits of another after combination, thus resulting in less tax being paid. However this greatly depends on the tax law of the country.

Synergy in management and in relation to teamwork refers to the combined effort of individuals as participants of the team. The condition that exists when the organization's parts interact to produce a joint effect that is greater than the sum of the parts acting alone. Positive or negative synergies can exist. In these cases, positive synergy has positive effects such as improved efficiency in operations, greater exploitation of opportunities, and improved utilization of resources. Negative synergy on the other hand has negative effects such as: reduced efficiency of operations, decrease in quality, underutilization of resources and disequilibrium with the external environment.

A cost synergy refers to the opportunity of a combined corporate entity to reduce or eliminate expenses associated with running a business. Cost synergies are realized by eliminating positions that are viewed as duplicate within the merged entity. Examples include the headquarters office of one of the predecessor companies, certain executives, the human resources department, or other employees of the predecessor companies. This is related to the economic concept of economies of scale.

The synergistic action of the economic players lies within the economic phenomenon's profundity. The synergistic action gives different dimensions to competitiveness, strategy and network identity becoming an unconventional "weapon" which belongs to those who exploit the economic systems’ potential in depth.

The synergistic gravity equation (SYNGEq), according to its complex “title”, represents a synthesis of the endogenous and exogenous factors which determine the private and non-private economic decision makers to call to actions of synergistic exploitation of the economic network in which they operate. That is to say, SYNGEq constitutes a big picture of the factors/motivations which determine the entrepreneurs to contour an active synergistic network. SYNGEq includes both factors which character is changing over time (such as the competitive conditions), as well as classics factors, such as the imperative of the access to resources of the collaboration and the quick answers. The synergistic gravity equation (SINGEq) comes to be represented by the formula:

∑SYN.Act = ∑R-*I(CRed+COOP++A)*V(Cust.+Info.)*cc

where:

The synergistic network represents an integrated part of the economic system which, through the coordination and control functions (of the undertaken economic actions), agrees synergies. The networks which promote synergistic actions can be divided in horizontal synergistic networks and vertical synergistic networks.

The synergy effects are difficult (even impossible) to imitate by competitors and difficult to reproduce by their authors because these effects depend on the combination of factors with time-varying characteristics. The synergy effects are often called "synergistic benefits", representing the direct and implied result of the developed/adopted synergistic actions.

Synergy can also be defined as the combination of human strengths and computer strengths, such as advanced chess. Computers can process data much more quickly than humans, but lack the ability to respond meaningfully to arbitrary stimuli.

Etymologically, the "synergy" term was first used around 1600, deriving from the Greek word “synergos”, which means “to work together” or “to cooperate”. If during this period the synergy concept was mainly used in the theological field (describing “the cooperation of human effort with divine will”), in the 19th and 20th centuries, "synergy" was promoted in physics and biochemistry, being implemented in the study of the open economic systems only in the 1960 and 1970s.

In 1938, J. R. R. Tolkien wrote an essay titled "On Fairy Stores", delivered at an Andrew Lang Lecture, and reprinted in his book, "The Tolkien Reader", published in 1966. In it, he made two references to synergy, although he did not use that term. He wrote:
Faerie cannot be caught in a net of words; for it is one of its qualities to be indescribable, though not imperceptible. It has many ingredients, but analysis will not necessarily discover the secret of the whole.
And more succinctly, in a footnote, about the "part of producing the web of an intricate story", he wrote:
It is indeed easier to unravel a single "thread" — an incident, a name, a motive — than to trace the history of any "picture" defined by many threads. For with the picture in the tapestry a new element has come in: the picture is greater than, and not explained by, the sum of the component threads.
Synergy, a book: DION, Eric (2017), "Synergy; A Theoretical Model of Canada's Comprehensive Approach", iUniverse, 308 pp.

The informational synergies which can be applied also in media involve a compression of transmission, access and use of information’s time, the flows, circuits and means of handling information being based on a complementary, integrated, transparent and coordinated use of knowledge.

In media economics, synergy is the promotion and sale of a product (and all its versions) throughout the various subsidiaries of a media conglomerate, e.g. films, soundtracks or video games. Walt Disney pioneered synergistic marketing techniques in the 1930s by granting dozens of firms the right to use his Mickey Mouse character in products and ads, and continued to market Disney media through licensing arrangements. These products can help advertise the film itself and thus help to increase the film's sales. For example, the Spider-Man films had toys of webshooters and figures of the characters made, as well as posters and games. The NBC sitcom 30 Rock often shows the power of synergy, while also poking fun at the use of the term in the corporate world. There are also different forms of synergy in popular card games like , Yu-Gi-Oh!, Cardfight!! Vanguard, and Future Card Buddyfight.

When multiple sources of information taken together provide more information than the sum of the information provided by each source alone, there is said to be a synergy in the sources. This in contrast to the case in which the sources provide less information, in which case there is said to be a redundancy in the sources.



</doc>
<doc id="26860" url="https://en.wikipedia.org/wiki?curid=26860" title="Syntax">
Syntax

In linguistics, syntax () is the set of rules, principles, and processes that govern the structure of sentences (sentence structure) in a given language, usually including word order. The term "syntax" is also used to refer to the study of such principles and processes. The goal of many syntacticians is to discover the syntactic rules common to all languages.

The word "syntax" comes from Ancient Greek: "coordination", which consists of "syn", "together", and "táxis", "an ordering".

One basic description of a language's syntax is the sequence in which the subject (S), verb (V), and object (O) usually appear in sentences. Over 85% of languages usually place the subject first, either in the sequence SVO or the sequence SOV. The other possible sequences are VSO, VOS, OVS, and OSV, the last three of which are rare. In most generative theories of syntax, these surface differences arise from a more complex clausal phrase structure, and each order may be compatible with multiple derivations.

The "Aṣṭādhyāyī" of Pāṇini (c. 4th century BC in Ancient India), is often cited as an example of a premodern work that approaches the sophistication of a modern syntactic theory (as works on grammar were written long before modern syntax came about). In the West, the school of thought that came to be known as "traditional grammar" began with the work of Dionysius Thrax.

For centuries, a framework known as (first expounded in 1660 by Antoine Arnauld in a book of the same title) dominated work in syntax: as its basic premise the assumption that language is a direct reflection of thought processes and therefore there is a single, most natural way to express a thought.

However, in the 19th century, with the development of historical-comparative linguistics, linguists began to realize the sheer diversity of human language and to question fundamental assumptions about the relationship between language and logic. It became apparent that there was no such thing as the most natural way to express a thought, and therefore logic could no longer be relied upon as a basis for studying the structure of language.

The Port-Royal grammar modeled the study of syntax upon that of logic. (Indeed, large parts of the Port-Royal Logic were copied or adapted from the "Grammaire générale".) Syntactic categories were identified with logical ones, and all sentences were analyzed in terms of "subject – copula – predicate". Initially, this view was adopted even by the early comparative linguists such as Franz Bopp.

The central role of syntax within theoretical linguistics became clear only in the 20th century, which could reasonably be called the "century of syntactic theory" as far as linguistics is concerned. (For a detailed and critical survey of the history of syntax in the last two centuries, see the monumental work by Giorgio Graffi (2001).)

There are a number of theoretical approaches to the discipline of syntax. One school of thought, founded in the works of Derek Bickerton, sees syntax as a branch of biology, since it conceives of syntax as the study of linguistic knowledge as embodied in the human mind. Other linguists (e.g., Gerald Gazdar) take a more Platonistic view, since they regard syntax to be the study of an abstract formal system. Yet others (e.g., Joseph Greenberg) consider syntax a taxonomical device to reach broad generalizations across languages.

Dependency grammar is an approach to sentence structure where syntactic units are arranged according to the dependency relation, as opposed to the constituency relation of phrase structure grammars. Dependencies are directed links between words. The (finite) verb is seen as the root of all clause structure and all the other words in the clause are either directly or indirectly dependent on this root. Some prominent dependency-based theories of syntax are:


Lucien Tesnière (1893–1954) is widely seen as the father of modern dependency-based theories of syntax and grammar. He argued vehemently against the binary division of the clause into subject and predicate that is associated with the grammars of his day (S → NP VP) and which remains at the core of most phrase structure grammars. In the place of this division, he positioned the verb as the root of all clause structure.

Categorial grammar is an approach that attributes the syntactic structure not to rules of grammar, but to the properties of the syntactic categories themselves. For example, rather than asserting that sentences are constructed by a rule that combines a noun phrase (NP) and a verb phrase (VP) (e.g., the phrase structure rule S → NP VP), in categorial grammar, such principles are embedded in the category of the head word itself. So the syntactic category for an intransitive verb is a complex formula representing the fact that the verb acts as a function word requiring an NP as an input and produces a sentence level structure as an output. This complex category is notated as (NP\S) instead of V. NP\S is read as "a category that searches to the left (indicated by \) for an NP (the element on the left) and outputs a sentence (the element on the right)." The category of transitive verb is defined as an element that requires two NPs (its subject and its direct object) to form a sentence. This is notated as (NP/(NP\S)) which means "a category that searches to the right (indicated by /) for an NP (the object), and generates a function (equivalent to the VP) which is (NP\S), which in turn represents a function that searches to the left for an NP and produces a sentence."

Tree-adjoining grammar is a categorial grammar that adds in partial tree structures to the categories.

Theoretical approaches to syntax that are based upon probability theory are known as stochastic grammars. One common implementation of such an approach makes use of a neural network or connectionism.

Functionalist models of grammar study the form–function interaction by performing a structural and a functional analysis.


The hypothesis of generative grammar is that language is a biological structure. The difference between structural–functional and generative models is that, in generative grammar, the object is placed into the verb phrase. Generative grammar is meant to be used to describe all human language and to predict whether any given utterance in a hypothetical language would sound correct to a speaker of that language (versus constructions which no human language would use). This approach to language was pioneered by Noam Chomsky. Most generative theories (although not all of them) assume that syntax is based upon the constituent structure of sentences. Generative grammars are among the theories that focus primarily on the form of a sentence, rather than its communicative function.

Among the many generative theories of linguistics, the Chomskyan theories are:

Other theories that find their origin in the generative paradigm are:

The Cognitive Linguistics framework stems from generative grammar, but adheres to evolutionary rather than Chomskyan linguistics. Cognitive models often recognise the generative assumption that the object belongs to the verb phrase. Cognitive frameworks include:







</doc>
<doc id="26861" url="https://en.wikipedia.org/wiki?curid=26861" title="Shamanism">
Shamanism

Shamanism is a practice that involves a practitioner reaching altered states of consciousness in order to perceive and interact with what they believe to be a spirit world and channel these transcendental energies into this world.

Beliefs and practices that have been categorized as "shamanic" have attracted the interest of scholars from a wide variety of disciplines, including anthropologists, archaeologists, historians, religious studies scholars, philosophers and psychologists. Hundreds of books and academic papers on the subject have been produced, with a peer-reviewed academic journal being devoted to the study of shamanism. In the 20th century, many Westerners involved in counter-cultural movements have created modern magico-religious practices influenced by their ideas of indigenous religions from across the world, creating what has been termed "neoshamanism" or the neoshamanic movement. It has affected the development of many neopagan practices, as well as faced a backlash and accusations of cultural appropriation, exploitation and misrepresentation when outside observers have tried to represent cultures to which they do not belong.

The word "shamanism" probably derives from the Manchu-Tungus word , meaning "one who knows". The word "shaman" may also have originated from the Evenki word "šamán", most likely from the southwestern dialect spoken by the Sym Evenki peoples. The Tungusic term was subsequently adopted by Russians interacting with the indigenous peoples in Siberia. It is found in the memoirs of the exiled Russian churchman Avvakum.

The word was brought to Western Europe in the late 17th century by the Dutch traveler Nicolaes Witsen, who reported his stay and journeys among the Tungusic- and Samoyedic-speaking indigenous peoples of Siberia in his book "Noord en Oost Tataryen" (1692). Adam Brand, a merchant from Lübeck, published in 1698 his account of a Russian embassy to China; a translation of his book, published the same year, introduced the word "shaman" to English speakers.

The etymology of the Evenki word is sometimes connected to a Tungus root "ša-" "to know". This has been questioned on linguistic grounds: "The possibility cannot be completely rejected, but neither should it be accepted without reservation since the assumed derivational relationship is phonologically irregular (note especially the vowel quantities)." Other scholars assert that the word comes directly from the Manchu language, and as such would be the only commonly used English word that is a loan from this language.

However, Mircea Eliade noted that the Sanskrit word "śramaṇa", designating a wandering monastic or holy figure, has spread to many Central Asian languages along with Buddhism and could be the ultimate origin of the Tungusic word. This proposal has been thoroughly critiqued since 1917. Ethnolinguist Juha Janhunen regards it as an "anachronism" and an "impossibility" that is nothing more than a "far-fetched etymology".

Twenty-first century anthropologist and archeologist Silvia Tomaskova argues that by the mid-1600s, many Europeans applied the Arabic term "shaitan" (meaning "devil") to the non-Christian practices and beliefs of indigenous peoples beyond the Ural Mountains. She suggests that "shaman" may have entered the various Tungus dialects as a corruption of this term, and then been told to Christian missionaries, explorers, soldiers and colonial administrators with whom the people had increasing contact for centuries.

A (female shaman) is sometimes called a ', which is not an actual indigenous term but simply "shaman" plus the Russian suffix ' (for feminine nouns).

There is no single agreed-upon definition for the word "shamanism" among anthropologists. The English historian Ronald Hutton noted that by the dawn of the 21st century, there were four separate definitions of the term which appeared to be in use. The first of these uses the term to refer to "anybody who contacts a spirit world while in an altered state of consciousness." The second definition limits the term to refer to those who contact a spirit world while in an altered state of consciousness at the behest of others. The third definition attempts to distinguish shamans from other magico-religious specialists who are believed to contact spirits, such as "mediums", "witch doctors", "spiritual healers" or "prophets," by claiming that shamans undertake some particular technique not used by the others. Problematically, scholars advocating the third view have failed to agree on what the defining technique should be. The fourth definition identified by Hutton uses "shamanism" to refer to the indigenous religions of Siberia and neighboring parts of Asia. According to the Golomt Center for Shamanic Studies, a Mongolian organisation of shamans, the Evenk word "shaman" would more accurately be translated as "priest".

The term "shamanism" was first applied by Western anthropologists as outside observers of the ancient religion of the Turks and Mongols, as well as those of the neighbouring Tungusic- and Samoyedic-speaking peoples. Upon observing more religious traditions across the world, some Western anthropologists began to also use the term in a very broad sense. The term was used to describe unrelated magico-religious practices found within the ethnic religions of other parts of Asia, Africa, Australasia and even completely unrelated parts of the Americas, as they believed these practices to be similar to one another.

Mircea Eliade writes, "A first definition of this complex phenomenon, and perhaps the least hazardous, will be: shamanism = 'technique of religious ecstasy'." Shamanism encompasses the premise that shamans are intermediaries or messengers between the human world and the spirit worlds. Shamans are said to treat ailments and illness by mending the soul. Alleviating traumas affecting the soul or spirit are believed to restore the physical body of the individual to balance and wholeness. Shamans also claim to enter supernatural realms or dimensions to obtain solutions to problems afflicting the community. Shamans claim to visit other worlds or dimensions to bring guidance to misguided souls and to ameliorate illnesses of the human soul caused by foreign elements. Shamans operate primarily within the spiritual world, which, they believe, in turn affects the human world. The restoration of balance is said to result in the elimination of the ailment.

A shaman ( , or ) is someone who is regarded as having access to, and influence in, the world of benevolent and malevolent spirits, who typically enters into a trance state during a ritual, and practices divination and healing. The word "shaman" probably originates from the Tungusic Evenki language of North Asia. According to ethnolinguist Juha Janhunen, "the word is attested in all of the Tungusic idioms" such as Negidal, Lamut, Udehe/Orochi, Nanai, Ilcha, Orok, Manchu and Ulcha, and "nothing seems to contradict the assumption that the meaning 'shaman' also derives from Proto-Tungusic" and may have roots that extend back in time at least two millennia. The term was introduced to the west after Russian forces conquered the shamanistic Khanate of Kazan in 1552.

Shamanism is a system of religious practice. Historically, it is often associated with indigenous and tribal societies, and involves belief that shamans, with a connection to the otherworld, have the power to heal the sick, communicate with spirits, and escort souls of the dead to the afterlife. Shamanism is especially associated with the native peoples of Siberia in northern Asia, where shamanic practice has been noted for centuries by Asian and Western visitors. It is an ideology that used to be widely practiced in Europe, Asia, Tibet, North and South America, and Africa. It centered on the belief in supernatural phenomenon such as the world of gods, demons, and ancestral spirits.

Belief in Shamanism has declined and only a few remote tribes still retain its practices. One such tribe is the Inuit people of the Canadian Arctic. Another can be found in the nomadic Tuvan (with an estimated population of just 3000 people surviving from this tribe). Tuva is one of the most isolated tribes in Russia where the art of shamanism has been preserved until today due to its isolated existence, allowing it to be free from the influences of other major religions.

Shamans often claim to have been called through dreams or signs. However, some say their powers are inherited. In traditional societies shamanic training varies in length, but generally takes years.

Turner and colleagues mention a phenomenon called "shamanistic initiatory crisis", a rite of passage for shamans-to-be, commonly involving physical illness or psychological crisis. The significant role of initiatory illnesses in the calling of a shaman can be found in the detailed case history of Chuonnasuan, who was the last master shaman among the Tungus peoples in Northeast China.

The wounded healer is an archetype for a shamanic trial and journey. This process is important to young shamans. They undergo a type of sickness that pushes them to the brink of death. This is said to happen for two reasons:

Shamans claim to gain knowledge and the power to heal in the spiritual world or dimension. Most shamans have dreams or visions that convey certain messages. Shamans may claim to have or have acquired many spirit guides, who they believe guide and direct them in their travels in the spirit world. These spirit guides are always thought to be present within the shaman, although others are said to encounter them only when the shaman is in a trance. The spirit guide energizes the shamans, enabling them to enter the spiritual dimension. Shamans claim to heal within the spiritual dimension by returning lost parts of the human soul from wherever they have gone. Shamans also claim to cleanse excess negative energies, which are said to confuse or pollute the soul.

Shamans act as mediators in their cultures. Shamans claim to communicate with the spirits on behalf of the community, including the spirits of the deceased. Shamans believe they can communicate with both living and dead to alleviate unrest, unsettled issues, and to deliver gifts to the spirits.

Among the Selkups, the sea duck is a spirit animal. Ducks fly in the air and dive in the water and are thus believed to belong to both the upper world and the world below. Among other Siberian peoples, these characteristics are attributed to water fowl in general. The upper world is the afterlife primarily associated with deceased humans and is believed to be accessed by soul journeying through a portal in the sky. The lower world or "world below" is the afterlife primarily associated with animals and is believed to be accessed by soul journeying through a portal in the earth. In shamanic cultures, many animals are regarded as spirit animals.

Shamans perform a variety of functions depending upon their respective cultures; healing, leading a sacrifice, preserving traditions by storytelling and songs, fortune-telling, and acting as a psychopomp ("guide of souls"). A single shaman may fulfill several of these functions.

The functions of a shaman may include either guiding to their proper abode the souls of the dead (which may be guided either one-at-a-time or in a group, depending on culture), and the curing of ailments. The ailments may be either purely physical afflictions—such as disease, which are claimed to be cured by gifting, flattering, threatening, or wrestling the disease-spirit (sometimes trying all these, sequentially), and which may be completed by displaying a supposedly extracted token of the disease-spirit (displaying this, even if "fraudulent", is supposed to impress the disease-spirit that it has been, or is in the process of being, defeated, so that it will retreat and stay out of the patient's body), or else mental (including psychosomatic) afflictions—such as persistent terror, which is likewise believed to be cured by similar methods. In most languages a different term other than the one translated "shaman" is usually applied to a religious official leading sacrificial rites ("priest"), or to a raconteur ("sage") of traditional lore; there may be more of an overlap in functions (with that of a shaman), however, in the case of an interpreter of omens or of dreams.

There are distinct types of shaman who perform more specialized functions. For example, among the Nani people, a distinct kind of shaman acts as a psychopomp. Other specialized shamans may be distinguished according to the type of spirits, or realms of the spirit world, with which the shaman most commonly interacts. These roles vary among the Nenets, Enets, and Selkup shamans.

The assistant of an Oroqen shaman (called "jardalanin", or "second spirit") knows many things about the associated beliefs. He or she accompanies the rituals and interprets the behaviors of the shaman. Despite these functions, the "jardalanin" is not a shaman. For this interpretative assistant, it would be unwelcome to fall into a trance.

Among the Tucano people, a sophisticated system exists for environmental resources management and for avoiding resource depletion through overhunting. This system is conceptualized mythologically and symbolically by the belief that breaking hunting restrictions may cause illness. As the primary teacher of tribal symbolism, the shaman may have a leading role in this ecological management, actively restricting hunting and fishing. The shaman is able to "release" game animals, or their souls, from their hidden abodes. The Piaroa people have ecological concerns related to shamanism. Among the Inuit, shamans fetch the souls of game from remote places, or soul travel to ask for game from mythological beings like the Sea Woman.

The way shamans get sustenance and take part in everyday life varies across cultures. In many Inuit groups, they provide services for the community and get a "due payment", and believe the payment is given to the helping spirits. An account states that the gifts and payments that a shaman receives are given by his partner spirit. Since it obliges the shaman to use his gift and to work regularly in this capacity, the spirit rewards him with the goods that it receives. These goods, however, are only "welcome addenda". They are not enough to enable a full-time shaman. Shamans live like any other member of the group, as a hunter or housewife. Due to the popularity of ayahuasca tourism in South America, there are practitioners in areas frequented by backpackers who make a living from leading ceremonies.

There are many variations of shamanism throughout the world, but several common beliefs are shared by all forms of shamanism. Common beliefs identified by Eliade (1972) are the following:

Shamanism is based on the premise that the visible world is pervaded by invisible forces or spirits which affect the lives of the living. Although the causes of disease lie in the spiritual realm, inspired by malicious spirits, both spiritual and physical methods are used to heal. Commonly, a shaman "enters the body" of the patient to confront the spiritual infirmity and heals by banishing the infectious spirit.

Many shamans have expert knowledge of medicinal plants native to their area, and an herbal treatment is often prescribed. In many places shamans learn directly from the plants, harnessing their effects and healing properties, after obtaining permission from the indwelling or patron spirits. In the Peruvian Amazon Basin, shamans and "curanderos" use medicine songs called "icaros" to evoke spirits. Before a spirit can be summoned it must teach the shaman its song. The use of totemic items such as rocks with special powers and an animating spirit is common.

Such practices are presumably very ancient. Plato wrote in his "Phaedrus" that the "first prophecies were the words of an oak", and that those who lived at that time found it rewarding enough to "listen to an oak or a stone, so long as it was telling the truth".

Belief in witchcraft and sorcery, known as "brujería" in Latin America, exists in many societies. Other societies assert all shamans have the power to both cure and kill. Those with shamanic knowledge usually enjoy great power and prestige in the community, but they may also be regarded suspiciously or fearfully as potentially harmful to others.

By engaging in their work, a shaman is exposed to significant personal risk as shamanic plant materials can be toxic or fatal if misused. Spells are commonly used in an attempt to protect against these dangers, and the use of more dangerous plants is often very highly ritualized.






Generally, shamans traverse the axis mundi and enter the "spirit world" by effecting a transition of consciousness, entering into an ecstatic trance, either autohypnotically or through the use of entheogens or ritual performances. The methods employed are diverse, and are often used together.

An entheogen ("generating the divine within") is a psychoactive substance used in a religious, shamanic, or spiritual context. Entheogens have been used in a ritualized context for thousands of years; their religious significance is well established in anthropological and modern evidences. Examples of traditional entheogens include: peyote, psilocybin and Amanita muscaria (fly agaric) mushrooms, uncured tobacco, cannabis, ayahuasca, "Salvia divinorum", iboga, and Mexican morning glory.

Some shamans observe dietary or customary restrictions particular to their tradition. These restrictions are more than just cultural. For example, the diet followed by shamans and apprentices prior to participating in an ayahuasca ceremony includes foods rich in tryptophan (a biosynthetic precursor to serotonin) as well as avoiding foods rich in tyramine, which could induce hypertensive crisis if ingested with MAOIs such as are found in ayahuasca brews as well as abstinence from alcohol or sex.

Just like shamanism itself, music and songs related to it in various cultures are diverse. In several instances, songs related to shamanism are intended to imitate natural sounds, via onomatopoeia.

Sound mimesis in various cultures may serve other functions not necessarily related to shamanism: practical goals such as luring game in the hunt; or entertainment (Inuit throat singing).


Shamans may have various kinds of paraphernalia in different cultures.

There are two major frameworks among cognitive and evolutionary scientists for explaining shamanism. The first, proposed by anthropologist Michael Winkelman, is known as the "neurotheological theory". According to Winkelman, shamanism develops reliably in human societies because it provides valuable benefits to the practitioner, their group, and individual clients. In particular, the trance states induced by dancing, hallucinogens, and other triggers are hypothesized to have an "integrative" effect on cognition, allowing communication among mental systems that specialize in theory of mind, social intelligence, and natural history. With this cognitive integration, the shaman can better predict the movement of animals, resolve group conflicts, plan migrations, and provide other useful services.

The neurotheological theory contrasts with the "by-product" or "subjective" model of shamanism developed by Harvard anthropologist Manvir Singh. According to Singh, shamanism is a cultural technology that adapts to (or hacks) our psychological biases to convince us that a specialist can influence important but uncontrollable outcomes. Citing work on the psychology of magic and superstition, Singh argues that humans search for ways of influencing uncertain events, such as healing illness, controlling rain, or attracting animals. As specialists compete to help their clients control these outcomes, they drive the evolution of psychologically compelling magic, producing traditions adapted to people's cognitive biases. Shamanism, Singh argues, is the culmination of this cultural evolutionary process—a psychologically appealing method for controlling uncertainty. For example, some shamanic practices exploit our intuitions about humanness: Practitioners use trance and dramatic initiations to seemingly become entities distinct from normal humans and thus more apparently capable of interacting with the invisible forces believed to oversee important outcomes. Influential cognitive and anthropological scientists such as Pascal Boyer and Nicholas Humphrey have endorsed Singh's approach, although other researchers have criticized Singh's dismissal of individual- and group-level benefits.

David Lewis-Williams explains the origins of shamanic practice, and some of its precise forms, through aspects of human consciousness evinced in cave art and LSD experiments alike.

Gerardo Reichel-Dolmatoff relates these concepts to developments in the ways that modern science (systems theory, ecology, new approaches in anthropology and archeology) treats causality in a less linear fashion. He also suggests a cooperation of modern science and indigenous lore.

Shamanic practices may originate as early as the Paleolithic, predating all organized religions, and certainly as early as the Neolithic period. The earliest known undisputed burial of a shaman (and by extension the earliest undisputed evidence of shamans and shamanic practices) dates back to the early Upper Paleolithic era (c. 30,000 BP) in what is now the Czech Republic.

Sanskrit scholar and comparative mythologist Michael Witzel proposes that all of the world's mythologies, and also the concepts and practices of shamans, can be traced to the migrations of two prehistoric populations: the "Gondwana" type (of circa 65,000 years ago) and the "Laurasian" type (of circa 40,000 years ago).

In November 2008, researchers from the Hebrew University of Jerusalem announced the discovery of a 12,000-year-old site in Israel that is perceived as one of the earliest-known shaman burials. The elderly woman had been arranged on her side, with her legs apart and folded inward at the knee. Ten large stones were placed on the head, pelvis and arms. Among her unusual grave goods were 50 complete tortoise shells, a human foot, and certain body parts from animals such as a cow tail and eagle wings. Other animal remains came from a boar, leopard, and two martens. "It seems that the woman … was perceived as being in a close relationship with these animal spirits", researchers noted. The grave was one of at least 28 graves at the site, located in a cave in lower Galilee and belonging to the Natufian culture, but is said to be unlike any other among the Epipaleolithic Natufians or in the Paleolithic period.

A debated etymology of the word "shaman" is "one who knows", implying, among other things, that the shaman is an expert in keeping together the multiple codes of the society, and that to be effective, shamans must maintain a comprehensive view in their mind which gives them certainty of knowledge. According to this view, the shaman uses (and the audience understands) multiple codes, expressing meanings in many ways: verbally, musically, artistically, and in dance. Meanings may be manifested in objects such as amulets. If the shaman knows the culture of their community well, and acts accordingly, their audience will know the used symbols and meanings and therefore trust the shamanic worker.

There are also semiotic, theoretical approaches to shamanism, and examples of "mutually opposing symbols" in academic studies of Siberian lore, distinguishing a "white" shaman who contacts sky spirits for good aims by day, from a "black" shaman who contacts evil spirits for bad aims by night. (Series of such opposing symbols referred to a world-view behind them. Analogously to the way grammar arranges words to express meanings and convey a world, also this formed a cognitive map). Shaman's lore is rooted in the folklore of the community, which provides a "mythological mental map". Juha Pentikäinen uses the concept ""grammar of mind"".

Armin Geertz coined and introduced the hermeneutics, or "ethnohermeneutics", interpretation. Hoppál extended the term to include not only the interpretation of oral and written texts, but that of "visual texts as well (including motions, gestures and more complex rituals, and ceremonies performed, for instance, by shamans)". Revealing the animistic views in shamanism, but also their relevance to the contemporary world, where ecological problems have validated paradigms of balance and protection.

Shamanism is believed to be declining around the world, possibly due to other organised religious influences, like Christianity, that want people who practice shamanism to convert to their own system and doctrine. Another reason is Western views of shamanism as primitive, superstitious, backward and outdated. Whalers who frequently interact with Inuit tribes are one source of this decline in that region.

In many areas, former shamans ceased to fulfill the functions in the community they used to, as they felt mocked by their own community, or regarded their own past as deprecated and were unwilling to talk about it to ethnographers.

Moreover, besides personal communications of former shamans, folklore texts may narrate directly about a deterioration process. For example, a Buryat epic text details the wonderful deeds of the ancient "first shaman" Kara-Gürgän: he could even compete with God, create life, steal back the soul of the sick from God without his consent. A subsequent text laments that shamans of older times were stronger, possessing capabilities like omnividence, fortune-telling even for decades in the future, moving as fast as a bullet.

In most affected areas, shamanic practices ceased to exist, with authentic shamans dying and their personal experiences dying with them. The loss of memories is not always lessened by the fact the shaman is not always the only person in a community who knows the beliefs and motives related to the local shaman-hood. Although the shaman is often believed and trusted precisely because he or she "accommodates" to the beliefs of the community, several parts of the knowledge related to the local shamanhood consist of personal experiences of the shaman, or root in his or her family life, thus, those are lost with his or her death. Besides that, in many cultures, the entire traditional belief system has become endangered (often together with a partial or total language shift), with the other people of the community remembering the associated beliefs and practices (or the language at all) grew old or died, many folklore memories songs, and texts were forgotten—which may threaten even such peoples who could preserve their isolation until the middle of the 20th century, like the Nganasan.

Some areas could enjoy a prolonged resistance due to their remoteness.

After exemplifying the general decline even in the most remote areas, there are revitalizations or tradition-preserving efforts as a response. Besides collecting the memories, there are also tradition-preserving and even revitalization efforts, led by authentic former shamans (for example among the Sakha people and Tuvans). However, according to Richard L. Allen, research and policy analyst for the Cherokee Nation, they are overwhelmed with fraudulent shamans ("plastic medicine people"). "One may assume that anyone claiming to be a Cherokee 'shaman, spiritual healer, or pipe-carrier', is equivalent to a modern day medicine show and snake-oil vendor." One indicator of a plastic shaman might be someone who discusses "Native American spirituality" but does not mention any specific Native American tribe.

Besides tradition-preserving efforts, there are also neoshamanistic movements, these may differ from many traditional shamanistic practice and beliefs in several points. Admittedly, several traditional beliefs systems indeed have ecological considerations (for example, many Inuit peoples), and among Tucano people, the shaman indeed has direct resource-protecting roles.

Today, shamanism survives primarily among indigenous peoples. Shamanic practices continue today in the tundras, jungles, deserts, and other rural areas, and even in cities, towns, suburbs, and shantytowns all over the world. This is especially true for Africa and South America, where "mestizo shamanism" is widespread.

The anthropologist Alice Kehoe criticizes the term "shaman" in her book "Shamans and Religion: An Anthropological Exploration in Critical Thinking". Part of this criticism involves the notion of cultural appropriation. This includes criticism of New Age and modern Western forms of shamanism, which, according to Kehoe, misrepresent or dilute indigenous practices. Kehoe also believes that the term reinforces racist ideas such as the noble savage.

Kehoe is highly critical of Mircea Eliade's work on shamanism as an invention synthesized from various sources unsupported by more direct research. To Kehoe, citing that ritualistic practices (most notably drumming, trance, chanting, entheogens and hallucinogens, spirit communication and healing) as being definitive of shamanism is poor practice. Such citations ignore the fact that those practices exist outside of what is defined as shamanism and play similar roles even in non-shamanic cultures (such as the role of chanting in Judeo-Christian and Islamic rituals) and that in their expression are unique to each culture that uses them. Such practices cannot be generalized easily, accurately, or usefully into a global religion of shamanism. Because of this, Kehoe is also highly critical of the hypothesis that shamanism is an ancient, unchanged, and surviving religion from the Paleolithic period.

Anthropologist Mihály Hoppál also discusses whether the term "shamanism" is appropriate. He notes that for many readers, "-ism" implies a particular dogma, like Buddhism or Judaism. He recommends using the term "shamanhood" or "shamanship" (a term used in old Russian and German ethnographic reports at the beginning of the 20th century) for stressing the diversity and the specific features of the discussed cultures. He believes that this places more stress on the local variations and emphasizes that shamanism is not a religion of sacred dogmas, but linked to the everyday life in a practical way. Following similar thoughts, he also conjectures a contemporary paradigm shift. Piers Vitebsky also mentions that, despite really astonishing similarities, there is no unity in shamanism. The various, fragmented shamanistic practices and beliefs coexist with other beliefs everywhere. There is no record of pure shamanistic societies (although their existence is not impossible). Norwegian social anthropologist Hakan Rydving has likewise argued for the abandonment of the terms "shaman" and "shamanism" as "scientific illusions."

Dulam Bumochir has affirmed the above critiques of "shamanism" as a Western construct created for comparative purposes and, in an extensive article, has documented the role of Mongols themselves, particularly "the partnership of scholars and shamans in the reconstruction of shamanism" in post-1990/post-communist Mongolia. This process has also been documented by Swiss anthropologist Judith Hangartner in her landmark study of Darhad shamans in Mongolia. Historian Karena Kollmar-Polenz argues that the social construction and reification of shamanism as a religious "other" actually began with the 18th-century writings of Tibetan Buddhist monks in Mongolia and later "probably influenced the formation of European discourse on Shamanism".




</doc>
