<doc id="28800" url="https://en.wikipedia.org/wiki?curid=28800" title="Democratic Left Alliance">
Democratic Left Alliance

Democratic Left Alliance (, SLD) is a social-democratic political party in Poland. It was formed in 1991 as an electoral alliance of centre-left parties, and became a single party on 15 April 1999. The party is a member of the Party of European Socialists and Progressive Alliance.

The coalition can be classified as left-wing. However, during the 1990s, it managed to attract voters from the pro-market and even right-wing camp. The main support for SLD came from middle-rank state sector employees, retired people, former Polish United Workers Party (PZPR) and All-Poland Alliance of Trade Unions (OPZZ) members and those who were unlikely to be frequent church-goers. The core of the coalition (Social Democracy of the Republic of Poland) rejected concepts such as lustration and de-communization, supported a parliamentarian regime with only the role of an arbiter for the president and criticized the right-wing camp for introduction of religious education into school. The ex-communists criticized the economic reforms, pointing to the high social costs, without negating the reforms per se.

SdRP, SDU and some other socialist and social-democratic parties had formed the original Democratic Left Alliance as a centre-left coalition just prior to the nation's first free elections in 1991. In 1999 the coalition became a party, but lost some members.

At the time, the coalition's membership drew mostly from the former PZPR. An alliance between the SLD and the Polish People's Party (PSL) ruled Poland in the years 1993–1997. However the coalition lost power to the right-wing Solidarity Electoral Action in the 1997 election as the right-wing opposition was united this time and because of the decline of support for SLD's coalition partner PSL, though the SLD itself actually gained votes.

SLD formed a coalition with Labour Union before the 2001 Polish election and won it overwhelmingly at last by capturing about 5.3 million votes, 42% of the whole and won 200 of 460 seats in the Sejm and 75 of 100 in the Senate. After the elections, the coalition was joined by the Polish People's Party (PSL) in forming a government and Leszek Miller became the Prime Minister. In March 2003, the PSL left the coalition.

By 2004 the support for SLD in the polls had dropped from about 30% to just below 10%, and several high-ranking party members had been accused of taking part in high-profile political scandals by the mainstream press (most notably the Rywin affair: Rywin-gate).

On 6 March 2004, Leszek Miller resigned as party leader and was replaced by Krzysztof Janik. On March 26 the Sejm speaker Marek Borowski, together with other high-ranking SLD officials, announced the creation of a new centre-left party, the Social Democratic Party of Poland. On the next day, Leszek Miller announced he would step down as Prime Minister on 2 May 2004, the day after Poland joined the European Union. Miller proceeded to do so.

In the 2004 European Parliament election, it only received 9% of the votes, giving it 5 of 54 seats reserved for Poland in the European Parliament, as part of the Party of European Socialists.

Wojciech Olejniczak, the former Minister of Agriculture and Rural Development, was elected the president of SLD on 29 May 2004, succeeded Józef Oleksy, who resigned from the post of Polish Prime Minister due to false accusations of links to the KGB.

The 2004 European elections foreshadowed the SLD's huge defeat in the 2005 parliamentary election, in which it won only 11.3% of the vote. This gave the party 55 seats, barely a quarter of what it had had prior to the election. It also lost all of its Senators. In late 2006 a centre-left political alliance called Left and Democrats was created, comprising SLD and smaller centre-left parties, the Labour Union, the Social Democratic Party of Poland, and the liberal Democratic Party – demokraci.pl. The coalition won a disappointing 13% in the 2007 parliamentary election and was dissolved soon after in April 2008. On 31 May 2008, Olejniczak was replaced by Grzegorz Napieralski as SLD leader.

In the 2009 European election the Democratic Left Alliance-Labor Union joint ticket received 12% of the vote and 7 MEPs were elected as part of the newly retitled Socialists & Democrats group.

In the 2011 parliamentary election, SLD received 8.24% of the vote which gave it 27 seats in the Sejm. After the elections, one of the party members, Sławomir Kopyciński, decided to leave SLD and join Palikot's Movement. On December 10, 2011, Leszek Miller was chosen to return as the party leader.

In the 2014 European elections on 25 May 2014, the SLD received 9.4% of the national vote and returned 4 MEPs.

In July 2015 the SLD formed the United Left electoral alliance along with Your Movement (TR), Labour United (UP) and The Greens (PZ) and minor parties to contest the upcoming election.

In the 2015 parliamentary election held on 25 October 2015, the United Left list received 7.6% of the vote, below the 8% threshold (electoral alliances must win at least 8% of the vote, as opposed to the 5% for individual parties), leaving the SLD without parliamentary representation for the first time. Indeed, for the first time since the end of Communism, no centre-left parties won any seats in this election.

In 2017, the party withdrew from the Socialist International, while maintaining ties with the Progressive Alliance.

For the 2019 parliamentary election SLD formed an alliance with Razem and Wiosna, known as The Left. In the 2019 parliamentary election, the alliance won 12.6% of the vote and 49 seats in the Sejm, with the SLD winning 24.

The SLD is usually seen as the face of the standard Polish left, having achieved notable electoral succes during the 90s and benefitting from a strongly organized network of local offices, which span 320 of Poland's 380 administrative counties. For this reason, it was often viewed as the go-to party for left-leaning Poles for the majority of Poland's modern history. The party's monopoly on mainstream left-wing economic ideas in Poland however ended, after the right-wing PiS party adopted many economically interventionist positions, which led a considerable portion of economically left-wing Poles to vote for PiS instead.

Besides self-described left-wingers, the party enjoys the support of many members of the country's police and military, but its largest voting bloc resides among former PZPR members, government officials and civil servants during the PPR period, which are seen as the party's core supporters. The loyal support of this voting bloc enabled the SLD to remain the largest party of the Polish left, even throughout the scandals that rocked the party in the early 2000s.

However, this electoral bloc was seen as unreliable by political observers, as despite the fact that it originally constituted a huge voting bloc, that segment of the population would inevitably shrink as its members steadily age. Following the passage of a "degradation law" by the ruling right-wing PiS party, which cut pensions and disability benefits to thousands of former bureaocrats, however, the party has undergone a revival, as more and more people's primary income came to be threatened by the new government policy. This led many of those affected to support the SLD, thus enlarging and mobilizing the formerly shrinking voting bloc.

The SLD nonetheless made a significant effort to broaden its political appeal by joining forces with two smaller left-wing parties in 2019, creating The Left political alliance, which poses itself as a 'modern' take on leftism.




</doc>
<doc id="28801" url="https://en.wikipedia.org/wiki?curid=28801" title="SLD">
SLD

SLD may refer to:






</doc>
<doc id="28803" url="https://en.wikipedia.org/wiki?curid=28803" title="Segmentation fault">
Segmentation fault

In computing, a segmentation fault (often shortened to segfault) or access violation is a fault, or failure condition, raised by hardware with memory protection, notifying an operating system (OS) the software has attempted to access a restricted area of memory (a memory access violation). On standard x86 computers, this is a form of general protection fault. The OS kernel will, in response, usually perform some corrective action, generally passing the fault on to the offending process by sending the process a signal. Processes can in some cases install a custom signal handler, allowing them to recover on their own, but otherwise the OS default signal handler is used, generally causing abnormal termination of the process (a program crash), and sometimes a core dump.

Segmentation faults are a common class of error in programs written in languages like C that provide low-level memory access. They arise primarily due to errors in use of pointers for virtual memory addressing, particularly illegal access. Another type of memory access error is a bus error, which also has various causes, but is today much rarer; these occur primarily due to incorrect "physical" memory addressing, or due to misaligned memory access – these are memory references that the hardware "cannot" address, rather than references that a process is not "allowed" to address.

Many programming languages may employ mechanisms designed to avoid segmentation faults and improve memory safety. For example, the Rust programming language, which appeared in 2010, employs an 'Ownership' based model to ensure memory safety, and garbage collection has been employed since around 1960, which avoids certain classes of memory errors which could lead to segmentation faults.

A segmentation fault occurs when a program attempts to access a memory location that it is not allowed to access, or attempts to access a memory location in a way that is not allowed (for example, attempting to write to a read-only location, or to overwrite part of the operating system).

The term "segmentation" has various uses in computing; in the context of "segmentation fault", a term used since the 1950s, it refers to the address space of a "program." With memory protection, only the program's own address space is readable, and of this, only the stack and the read-write portion of the data segment of a program are writable, while read-only data and the code segment are not writable. Thus attempting to read outside of the program's address space, or writing to a read-only segment of the address space, results in a segmentation fault, hence the name.

On systems using hardware memory segmentation to provide virtual memory, a segmentation fault occurs when the hardware detects an attempt to refer to a non-existent segment, or to refer to a location outside the bounds of a segment, or to refer to a location in a fashion not allowed by the permissions granted for that segment. On systems using only paging, an invalid page fault generally leads to a segmentation fault, and segmentation faults and page faults are both faults raised by the virtual memory management system. Segmentation faults can also occur independently of page faults: illegal access to a valid page is a segmentation fault, but not an invalid page fault, and segmentation faults can occur in the middle of a page (hence no page fault), for example in a buffer overflow that stays within a page but illegally overwrites memory.

At the hardware level, the fault is initially raised by the memory management unit (MMU) on illegal access (if the referenced memory exists), as part of its memory protection feature, or an invalid page fault (if the referenced memory does not exist). If the problem is not an invalid logical address but instead an invalid physical address, a bus error is raised instead, though these are not always distinguished.

At the operating system level, this fault is caught and a signal is passed on to the offending process, activating the process's handler for that signal. Different operating systems have different signal names to indicate that a segmentation fault has occurred. On Unix-like operating systems, a signal called SIGSEGV (abbreviated from "segmentation violation") is sent to the offending process. On Microsoft Windows, the offending process receives a STATUS_ACCESS_VIOLATION exception.

The conditions under which segmentation violations occur and how they manifest themselves are specific to hardware and the operating system: different hardware raises different faults for given conditions, and different operating systems convert these to different signals that are passed on to processes. The proximate cause is a memory access violation, while the underlying cause is generally a software bug of some sort. Determining the root cause – debugging the bug – can be simple in some cases, where the program will consistently cause a segmentation fault (e.g., dereferencing a null pointer), while in other cases the bug can be difficult to reproduce and depend on memory allocation on each run (e.g., dereferencing a dangling pointer).

The following are some typical causes of a segmentation fault:
These in turn are often caused by programming errors that result in invalid memory access:

In C code, segmentation faults most often occur because of errors in pointer use, particularly in C dynamic memory allocation. Dereferencing a null pointer will always result in a segmentation fault, but wild pointers and dangling pointers point to memory that may or may not exist, and may or may not be readable or writable, and thus can result in transient bugs. For example:

Now, dereferencing any of these variables could cause a segmentation fault: dereferencing the null pointer generally will cause a segfault, while reading from the wild pointer may instead result in random data but no segfault, and reading from the dangling pointer may result in valid data for a while, and then random data as it is overwritten.

The default action for a segmentation fault or bus error is abnormal termination of the process that triggered it. A core file may be generated to aid debugging, and other platform-dependent actions may also be performed. For example, Linux systems using the grsecurity patch may log SIGSEGV signals in order to monitor for possible intrusion attempts using buffer overflows.

Writing to read-only memory raises a segmentation fault. At the level of code errors, this occurs when the program writes to part of its own code segment or the read-only portion of the data segment, as these are loaded by the OS into read-only memory.

Here is an example of ANSI C code that will generally cause a segmentation fault on platforms with memory protection. It attempts to modify a string literal, which is undefined behavior according to the ANSI C standard. Most compilers will not catch this at compile time, and instead compile this to executable code that will crash:

When the program containing this code is compiled, the string "hello world" is placed in the rodata section of the program executable file: the read-only section of the data segment. When loaded, the operating system places it with other strings and constant data in a read-only segment of memory. When executed, a variable, "s", is set to point to the string's location, and an attempt is made to write an "H" character through the variable into the memory, causing a segmentation fault. Compiling such a program with a compiler that does not check for the assignment of read-only locations at compile time, and running it on a Unix-like operating system produces the following runtime error:

Backtrace of the core file from GDB:

This code can be corrected by using an array instead of a character pointer, as this allocates memory on stack and initializes it to the value of the string literal:

Even though string literals should not be modified (this has undefined behavior in the C standard), in C they are of codice_1 type, so there is no implicit conversion in the original code (which points a codice_2 at that array), while in C++ they are of codice_3 type, and thus there is an implicit conversion, so compilers will generally catch this particular error.

In C and C-like languages, null pointers are used to mean "pointer to no object" and as an error indicator, and dereferencing a null pointer (a read or write through a null pointer) is a very common program error. The C standard does not say that the null pointer is the same as the pointer to memory address 0, though that may be the case in practice. Most operating systems map the null pointer's address such that accessing it causes a segmentation fault. This behavior is not guaranteed by the C standard. Dereferencing a null pointer is undefined behavior in C, and a conforming implementation is allowed to assume that any pointer that is dereferenced is not null.

This sample code creates a null pointer, and then tries to access its value (read the value). Doing so causes a segmentation fault at runtime on many operating systems.

Dereferencing a null pointer and then assigning to it (writing a value to a non-existent target) also usually causes a segmentation fault:
The following code includes a null pointer dereference, but when compiled will often not result in a segmentation fault, as the value is unused and thus the dereference will often be optimized away by dead code elimination:

Another example is recursion without a base case:

which causes the stack to overflow which results in a segmentation fault. Infinite recursion may not necessarily result in a stack overflow depending on the language, optimizations performed by the compiler and the exact structure of a code. In this case, the behavior of unreachable code (the return statement) is undefined, so the compiler can eliminate it and use a tail call optimization that might result in no stack usage. Other optimizations could include translating the recursion into iteration, which given the structure of the example function would result in the program running forever, while probably not overflowing its stack.




</doc>
<doc id="28804" url="https://en.wikipedia.org/wiki?curid=28804" title="Signal separation">
Signal separation

Source separation, blind signal separation (BSS) or blind source separation, is the separation of a set of source signals from a set of mixed signals, without the aid of information (or with very little information) about the source signals or the mixing process. It is most commonly applied in digital signal processing and involves the analysis of mixtures of signals; the objective is to recover the original component signals from a mixture signal. The classical example of a source separation problem is the cocktail party problem, where a number of people are talking simultaneously in a room (for example, at a cocktail party), and a listener is trying to follow one of the discussions. The human brain can handle this sort of auditory source separation problem, but it is a difficult problem in digital signal processing.

This problem is in general highly underdetermined, but useful solutions can be derived under a surprising variety of conditions. Much of the early literature in this field focuses on the separation of temporal signals such as audio. However, blind signal separation is now routinely performed on multidimensional data, such as images and tensors, which may involve no time dimension whatsoever.

Several approaches have been proposed for the solution of this problem but development is currently still very much in progress. Some of the more successful approaches are principal components analysis and independent component analysis, which work well when there are no delays or echoes present; that is, the problem is simplified a great deal. The field of computational auditory scene analysis attempts to achieve auditory source separation using an approach that is based on human hearing.

The human brain must also solve this problem in real time. In human perception this ability is commonly referred to as auditory scene analysis or the cocktail party effect.

At a cocktail party, there is a group of people talking at the same time. You have multiple microphones picking up mixed signals, but you want to isolate the speech of a single person. BSS can be used to separate the individual sources by using mixed signals. In the presence of noise, dedicated optimization criteria need to be used

Figure 2 shows the basic concept of BSS. The individual source signals are shown as well as the mixed signals which are received signals. BSS is used to separate the mixed signals with only knowing mixed signals and nothing about original signal or how they were mixed. The separated signals are only approximations of the source signals. The separated images, were separated using Python and the Shogun toolbox using Joint Approximation Diagonalization of Eigen-matrices (JADE) algorithm which is based off independent component analysis, ICA. This toolbox method can be used with multi-dimensions but for an easy visual aspect images(2-D) were used.

One of the practical applications being researched in this area is medical imaging of the brain with magnetoencephalography (MEG). This kind of imaging involves careful measurements of magnetic fields outside the head which yield an accurate 3D-picture of the interior of the head. However, external sources of electromagnetic fields, such as a wristwatch on the subject's arm, will significantly degrade the accuracy of the measurement. Applying source separation techniques on the measured signals can help remove undesired artifacts from the signal.

In electroencephalogram (EEG) and magnetoencephalography (MEG), the interference from muscle activity masks the desired signal from brain activity. BSS, however, can be used to separate the two so an accurate representation of brain activity may be achieved.

Another application is the separation of musical signals. For a stereo mix of relatively simple signals it is now possible to make a fairly accurate separation, although some artifacts remain.

Other applications:

The set of individual source signals, formula_1, is 'mixed' using a matrix, formula_2, to produce a set of 'mixed' signals, formula_3, as follows. Usually, formula_4 is equal to formula_5. If formula_6, then the system of equations is overdetermined and thus can be unmixed using a conventional linear method. If formula_7, the system is underdetermined and a non-linear method must be employed to recover the unmixed signals. The signals themselves can be multidimensional.

formula_8

The above equation is effectively 'inverted' as follows. Blind source separation separates the set of mixed signals, formula_9, through the determination of an 'unmixing' matrix, formula_10, to 'recover' an approximation of the original signals, formula_11.

formula_12

Since the chief difficulty of the problem is its underdetermination, methods for blind source separation generally seek to narrow the set of possible solutions in a way that is unlikely to exclude the desired solution. In one approach, exemplified by principal and independent component analysis, one seeks source signals that are minimally correlated or maximally independent in a probabilistic or information-theoretic sense. A second approach, exemplified by nonnegative matrix factorization, is to impose structural constraints on the source signals. These structural constraints may be derived from a generative model of the signal, but are more commonly heuristics justified by good empirical performance. A common theme in the second approach is to impose some kind of low-complexity constraint on the signal, such as sparsity in some basis for the signal space. This approach can be particularly effective if one requires not the whole signal, but merely its most salient features.

There are different methods of blind signal separation:





</doc>
<doc id="28805" url="https://en.wikipedia.org/wiki?curid=28805" title="Stephen Cole Kleene">
Stephen Cole Kleene

Stephen Cole Kleene ( ; January 5, 1909 – January 25, 1994) was an American mathematician. One of the students of Alonzo Church, Kleene, along with Rózsa Péter, Alan Turing, Emil Post, and others, is best known as a founder of the branch of mathematical logic known as recursion theory, which subsequently helped to provide the foundations of theoretical computer science. Kleene's work grounds the study of computable functions. A number of mathematical concepts are named after him: Kleene hierarchy, Kleene algebra, the Kleene star (Kleene closure), Kleene's recursion theorem and the Kleene fixpoint theorem. He also invented regular expressions in 1951 to describe McCulloch-Pitts neural networks, and made significant contributions to the foundations of mathematical intuitionism.

Kleene was awarded the BA degree from Amherst College in 1930. He was awarded the Ph.D. in mathematics from Princeton University in 1934. His thesis, entitled "A Theory of Positive Integers in Formal Logic", was supervised by Alonzo Church. In the 1930s, he did important work on Church's lambda calculus. In 1935, he joined the mathematics department at the University of Wisconsin–Madison, where he spent nearly all of his career. After two years as an instructor, he was appointed assistant professor in 1937.

While a visiting scholar at the Institute for Advanced Study in Princeton, 1939–40, he laid the foundation for recursion theory, an area that would be his lifelong research interest. In 1941, he returned to Amherst College, where he spent one year as an associate professor of mathematics.

During World War II, Kleene was a lieutenant commander in the United States Navy. He was an instructor of navigation at the U.S. Naval Reserve's Midshipmen's School in New York, and then a project director at the Naval Research Laboratory in Washington, D.C.

In 1946, Kleene returned to Wisconsin, becoming a full professor in 1948 and the Cyrus C. MacDuffee professor of mathematics in 1964. He was chair of the Department of Mathematics and Computer Science, 1962–63, and Dean of the College of Letters and Science from 1969 to 1974. The latter appointment he took on despite the considerable student unrest of the day, stemming from the Vietnam War. He retired from the University of Wisconsin in 1979. In 1999 the mathematics library at the University of Wisconsin was renamed in his honor.

Kleene's teaching at Wisconsin resulted in three texts in mathematical logic, Kleene (1952, 1967) and Kleene and Vesley (1965). The first two are often cited and still in print. Kleene (1952) wrote alternative proofs to the Gödel's incompleteness theorems that enhanced their canonical status and made them easier to teach and understand. Kleene and Vesley (1965) is the classic American introduction to intuitionist logic and mathematics.

Kleene served as president of the Association for Symbolic Logic, 1956–58, and of the International Union of History and Philosophy of Science, 1961. In 1990, he was awarded the National Medal of Science. The importance of Kleene's work led to the saying that "Kleeneness is next to Gödelness."

Kleene and his wife Nancy Elliott had four children. He had a lifelong devotion to the family farm in Maine. An avid mountain climber, he had a strong interest in nature and the environment, and was active in many conservation causes.




At each conference of the Symposium on Logic in Computer Science the Kleene award, in honour of S.C. Kleene, is given for the best student paper.




</doc>
<doc id="28809" url="https://en.wikipedia.org/wiki?curid=28809" title="Shabbat">
Shabbat

Shabbat ( or ; , "rest" or "cessation"), Shabbos (, Ashkenazi Hebrew and ), or the Sabbath, is Judaism's day of rest and seventh day of the week. On this day, religious Jews, Samaritans and certain Christians (such as Seventh-day Adventists, the Church of God (Seventh-Day) and Seventh Day Baptists) remember the biblical creation of the heavens and the earth in six days and look forward to a future Messianic Age.

Shabbat observance entails refraining from work activities, often with great rigor, and engaging in restful activities to honor the day. Judaism's traditional position is that unbroken seventh-day Shabbat originated among the Jewish people, as their first and most sacred institution, though some suggest other origins. Variations upon Shabbat are widespread in Judaism and, with adaptations, throughout the Abrahamic and many other religions.

According to "halakha" (Jewish religious law), Shabbat is observed from a few minutes before sunset on Friday evening until the appearance of three stars in the sky on Saturday night. Shabbat is ushered in by lighting candles and reciting a blessing. Traditionally, three festive meals are eaten: The first one is held on Friday evening, the second is traditionally a lunch meal on Saturday and the third being held later in the afternoon. The evening meal and the early afternoon meal typically begin with a blessing called "kiddush" and another blessing recited over two loaves of challah. The third meal does not have the Kiddush recited but all does have the two loaves. Shabbat is closed Saturday evening with a "havdalah" blessing. 

Shabbat is a festive day when Jews exercise their freedom from the regular labors of everyday life. It offers an opportunity to contemplate the spiritual aspects of life and to spend time with family.

The word "Shabbat" derives from the Hebrew verb "shavat" (). Although frequently translated as "rest" (noun or verb), another accurate translation of these words is "ceasing [from work]", as resting is not necessarily denoted. The related modern Hebrew word "shevita" (labor strike), has the same implication of active rather than passive abstinence from work. The notion of active cessation from labor is also regarded as more consistent with an omnipotent God's activity on the seventh day of Creation according to Genesis. Other significant connotations are to "shevet" (שֶּׁבֶת) which means sitting or staying, and to "sheva" (שֶׁבַע) meaning seven, as Shabbat is the seventh day of the week; the other days of the week do not have names but called by their ordinals.

Sabbath is given special status as a holy day at the very beginning of the Torah in . It is first commanded after the Exodus from Egypt, in (relating to the cessation of manna) and in (relating to the distance one may travel by foot on the Sabbath), as also in (as the fourth of the Ten Commandments). Sabbath is commanded and commended many more times in the Torah and Tanakh; double the normal number of animal sacrifices are to be offered on the day. Sabbath is also described by the prophets Isaiah, Jeremiah, Ezekiel, Hosea, Amos, and Nehemiah.

The longstanding traditional Jewish position is that unbroken seventh-day Shabbat originated among the Jewish people, as their first and most sacred institution. The origins of Shabbat and a seven-day week are not clear to scholars; the Mosaic tradition claims an origin from the Biblical creation.

Seventh-day Shabbat did not originate with the Egyptians, to whom it was unknown; and other origin theories based on the day of Saturn, or on the planets generally, have also been abandoned.

The first non-Biblical reference to Sabbath is in an ostracon found in excavations at Mesad Hashavyahu, which is dated 630 BCE.

For the Babylonian concept of "sapattu" or "sabattu", see here.

Connection to Sabbath observance has been suggested in the designation of the seventh, fourteenth, nineteenth, twenty-first and twenty-eight days of a lunar month in an Assyrian religious calendar as a 'holy day', also called ‘evil days’ (meaning "unsuitable" for prohibited activities). The prohibitions on these days, spaced seven days apart, include abstaining from chariot riding, and the avoidance of eating meat by the King. On these days officials were prohibited from various activities and common men were forbidden to "make a wish", and at least the 28th was known as a "rest-day". The "Universal Jewish Encyclopedia" advanced a theory of Assyriologists like Friedrich Delitzsch (and of Marcello Craveri) that Shabbat originally arose from the lunar cycle in the Babylonian calendar containing four weeks ending in Sabbath, plus one or two additional unreckoned days per month. The difficulties of this theory include reconciling the differences between an unbroken week and a lunar week, and explaining the absence of texts naming the lunar week as Sabbath in any language.

The Tanakh and siddur describe Shabbat as having three purposes:

Judaism accords Shabbat the status of a joyous holy day. In many ways, Jewish law gives Shabbat the status of being the most important holy day in the Jewish calendar:

Honoring Shabbat ("kavod Shabbat") on Preparation Day (Friday) includes bathing, having a haircut and cleaning and beautifying the home (with flowers, for example).
According to Jewish law, Shabbat starts a few minutes before sunset. Candles are lit at this time. It is customary in many communities to light the candles 18 minutes before sundown ("tosefet Shabbat", though sometimes 36 minutes), and most printed Jewish calendars adhere to this custom. The Kabbalat Shabbat service is a prayer service welcoming the arrival of Shabbat. Before Friday night dinner, it is customary to sing two songs, one "greeting" two Shabbat angels into the house (Shalom Aleichem -Peace Be Upon You) and the other praising the woman of the house for all the work she has done over the past week (Aishes Chayil - Women Of Valour). After blessings over the wine and challah, a festive meal is served. Singing is traditional at Sabbath meals. In modern times, many composers have written sacred music for use during the Kabbalat Shabbat observance, including Robert Strassburg and Samuel Adler..

According to rabbinic literature, God via the Torah commands Jews to "observe" (refrain from forbidden activity) and "remember" (with words, thoughts, and actions) Shabbat, and these two actions are symbolized by the customary two Shabbat candles. Candles are lit usually by the woman of the house (or else by a man who lives alone). Some families light more candles, sometimes in accordance with the number of children.

Shabbat is a day of celebration as well as prayer. It is customary to eat three festive meals: Dinner on Shabbat eve (Friday night), lunch on Shabbat day (Saturday), and a third meal (a "Seudah Shlishit"/"Shalosh Seudot") in the late afternoon (Saturday). It is also customary to wear nice clothing (different from during the week) on Shabbat to honor the day.

On June 13, 2014, Am Yisrael Foundation’s White City Shabbat organization set the Guinness World Record for the world's largest Shabbat dinner. Held at Hangar 11 at Tel Aviv Port, the event was attended by 2,226 people, including Alan Dershowitz, Tel Aviv mayor Ron Huldai, Israeli basketball star Tal Brody and former US Ambassador Michael Oren. The event took almost a year of preparation and involved “60 days of crowd-sourced fundraising, 800 bottles of Israeli wine, 80 bottles of vodka, 50 bottles of whiskey, 2,000 challah rolls, 80 long tables, 1,800 pieces of chicken, 1,000 portions of beef and 250 vegetarian meals.” A total of 2,300 diners signed up for the dinner and another 3,000 were placed on the waiting list.

Many Jews attend synagogue services on Shabbat even if they do not do so during the week. Services are held on Shabbat eve (Friday night), Shabbat morning (Saturday morning), and late Shabbat afternoon (Saturday afternoon).

With the exception of Yom Kippur, which is referred to in the Torah (Lev 23:32) as "Shabbat of Shabbatoth", days of public fasting are postponed or advanced if they coincide with Shabbat. Mourners sitting "shivah" (week of mourning subsequent to the death of a spouse or first-degree relative) outwardly conduct themselves normally for the duration of the day and are forbidden to display public signs of mourning.

Although most Shabbat laws are restrictive, the fourth of the Ten Commandments in Exodus is taken by the Talmud and Maimonides to allude to the "positive" commandments of Shabbat. These include:

"Havdalah" (Hebrew: הַבְדָּלָה, "separation") is a Jewish religious ceremony that marks the symbolic end of Shabbat, and ushers in the new week. At the conclusion of Shabbat at nightfall, after the appearance of three stars in the sky, the "havdalah" blessings are recited over a cup of wine, and with the use of fragrant spices and a candle, usually braided. Some communities delay "havdalah" later into the night in order to prolong Shabbat. There are different customs regarding how much time one should wait after the stars have surfaced until the sabbath technically ends. Some people hold by 72 minutes later and other hold longer and shorter than that.

Jewish law (halakha) prohibits doing any form of "melakhah" (מְלָאכָה, plural "melakhoth") on Shabbat, unless an urgent human or medical need is life-threatening. Though "melakhah" is commonly translated as "work" in English, a better definition is "deliberate activity" or "skill and craftmanship". There are 39 categories of prohibited activities ("melakhoth") listed in Mishnah Tractate Shabbat 7:2.

The term "shomer Shabbat" is used for a person (or organization) who adheres to Shabbat laws consistently. The "shomer Shabbat" is an archetype mentioned in Jewish songs (e.g., "Baruch El Elyon") and the intended audience for various treatises on Jewish law and practice for "Shabbat" (e.g., "Shemirat Shabbat ke-Hilkhata").

There are often disagreements between Orthodox Jews and non-Orthodox Jews as to the practical observance of the Sabbath. The (strict) observance of the Sabbath is often seen as a benchmark for orthodoxy and indeed has legal bearing on the way a Jew is seen by an orthodox religious court regarding their affiliation to Judaism. See Yosef Dov Soloveitchik's "Beis HaLevi" commentary on parasha Ki Tissa for further elaboration regarding the legal ramifications.

The 39 categories of "melakhah" are: 

The categories of labors prohibited on Shabbat are exegetically derived – on account of Biblical passages juxtaposing Shabbat observance (Ex. 35:1–3) to making the Tabernacle (Ex. 35:4 ff.) – that they are the kinds of work that were necessary for the construction of the Tabernacle. They are not explicitly listed in the Torah; the Mishnah observes that "the laws of Shabbat ... are like mountains hanging by a hair, for they are little Scripture but many laws". Many rabbinic scholars have pointed out that these labors have in common activity that is "creative", or that exercises control or dominion over one's environment.

Different streams of Judaism view the prohibition on work in different ways. Observant Orthodox and Conservative Jews refrain from performing the 39 prohibited categories of activities. Each "melakhah" has derived prohibitions of various kinds. There are, therefore, many more forbidden activities on Shabbat; all are traced back to one of the 39 above principal "melakhoth".

Given the above, the 39 "melakhoth" are not so much activities as "categories of activity". For example, while "winnowing" usually refers exclusively to the separation of chaff from grain, and "selecting" refers exclusively to the separation of debris from grain, they refer in the Talmudic sense to any separation of intermixed materials which renders edible that which was inedible. Thus, filtering undrinkable water to make it drinkable falls under this category, as does picking small bones from fish (gefilte fish is one solution to this problem).

Orthodox and some Conservative authorities rule that turning electric devices on or off is prohibited as a "melakhah"; however, authorities are not in agreement about exactly which one(s). One view is that tiny sparks are created in a switch when the circuit is closed, and this would constitute lighting a fire (category 37). If the appliance is purposed for light or heat (such as an incandescent bulb or electric oven), then the lighting or heating elements may be considered as a type of fire that falls under both lighting a fire (category 37) and cooking (i.e., baking, category 11). Turning lights off would be extinguishing a fire (category 36).

Another view is that a device plugged into an electrical outlet of a wall becomes part of the building, but is nonfunctional while the switch is off; turning it on would then constitute building (category 35) and turning it off would be demolishing (category 34). Some schools of thought consider the use of electricity to be forbidden only by rabbinic injunction, rather than because it violates one of the original categories.

A common solution to the problem of electricity involves preset timers (Shabbat clocks) for electric appliances, to turn them on and off automatically, with no human intervention on Shabbat itself. Some Conservative authorities reject altogether the arguments for prohibiting the use of electricity. Some Orthodox also hire a "Shabbos goy", a Gentile to perform prohibited tasks (like operating light switches) on Shabbat.

Orthodox and many Conservative authorities completely prohibit the use of automobiles on Shabbat as a violation of multiple categories, including lighting a fire, extinguishing a fire, and transferring between domains (category 39). However, the Conservative movement's Committee on Jewish Law and Standards permits driving to a synagogue on Shabbat, as an emergency measure, on the grounds that if Jews lost contact with synagogue life they would become lost to the Jewish people.

A halakhically authorized Shabbat mode added to a power-operated mobility scooter may be used on the observance of Shabbat for those with walking limitations, often referred to as a Shabbat scooter. It is intended only for individuals whose limited mobility is dependent on a scooter or automobile consistently throughout the week.

Seemingly "forbidden" acts may be performed by modifying technology such that no law is actually violated. In Sabbath mode, a "Sabbath elevator" will stop automatically at every floor, allowing people to step on and off without anyone having to press any buttons, which would normally be needed to work. (Dynamic braking is also disabled if it is normally used, i.e., shunting energy collected from downward travel, and thus the gravitational potential energy of passengers, into a resistor network.) However, many rabbinical authorities consider the use of such elevators by those who are otherwise capable as a violation of Shabbat, with such workarounds being for the benefit of the frail and handicapped and not being in the spirit of the day.

Many observant Jews avoid the prohibition of carrying by use of an eruv. Others make their keys into a tie bar, part of a belt buckle, or a brooch, because a legitimate article of clothing or jewelry may be worn rather than carried. An elastic band with clips on both ends, and with keys placed between them as integral links, may be considered a belt.

Shabbat lamps have been developed to allow a light in a room to be turned on or off at will while the electricity remains on. A special mechanism blocks out the light when the off position is desired without violating Shabbat.

The Shabbos App is a proposed Android app claimed by its creators to enable Orthodox Jews, and all Jewish Sabbath-observers, to use a smartphone to text on the Jewish Sabbath. It has met with resistance from some authorities.

In the event that a human life is in danger (pikuach nefesh), a Jew is not only allowed, but required, to violate any halakhic law that stands in the way of saving that person (excluding murder, idolatry, and forbidden sexual acts). The concept of life being in danger is interpreted broadly: for example, it is mandated that one violate Shabbat to bring a woman in active labor to a hospital. Lesser rabbinic restrictions are often violated under much less urgent circumstances (a patient who is ill but not critically so).

Various other legal principles closely delineate which activities constitute desecration of Shabbat. Examples of these include the principle of "shinui" ("change" or "deviation"): A violation is not regarded as severe if the prohibited act was performed in a way that would be considered abnormal on a weekday. Examples include writing with one's nondominant hand, according to many rabbinic authorities. This legal principle operates "bedi'avad" ("ex post facto") and does not cause a forbidden activity to be permitted barring extenuating circumstances.

Generally, adherents of Reform and Reconstructionist Judaism believe that the individual Jew determines whether to follow Shabbat prohibitions or not. For example, some Jews might find activities, such as writing or cooking for leisure, to be enjoyable enhancements to Shabbat and its holiness, and therefore may encourage such practices. Many Reform Jews believe that what constitutes "work" is different for each person, and that only what the person considers "work" is forbidden. The radical Reform rabbi Samuel Holdheim advocated moving Sabbath to Sunday for many no longer observed it, a step taken by dozens of congregations in the United States in late 19th century.

More rabbinically traditional Reform and Reconstructionist Jews believe that these "halakhoth" in general may be valid, but that it is up to each individual to decide how and when to apply them. A small fraction of Jews in the Progressive Jewish community accept these laws much the same way as Orthodox Jews.

All Jewish denominations encourage the following activities on Shabbat:

Special Shabbatot are the Shabbatot that precede important Jewish holidays: e.g., "Shabbat HaGadol" (Shabbat preceding Pesach), "Shabbat Zachor" (Shabbat preceding Purim), and "Shabbat Shuvah" (Shabbat between Rosh Hashanah and Yom Kippur).

Most Christians do not observe Saturday Sabbath, but instead observe a weekly day of worship on Sunday, which is often called the "Lord's Day". Several Christian denominations, such as the Seventh-day Adventist Church, the Church of God (7th Day), the Seventh Day Baptists, and many others, observe seventh-day Sabbath. This observance is celebrated from Friday sunset to Saturday sunset.

The principle of weekly Sabbath also exists in other beliefs. Examples include the Babylonian calendar, the Buddhist "uposatha", and the Unification Church's Ahn Shi Il.



</doc>
<doc id="28810" url="https://en.wikipedia.org/wiki?curid=28810" title="Saki">
Saki

Hector Hugh Munro (18 December 1870 – 14 November 1916), better known by the pen name Saki and also frequently as H. H. Munro, was a British writer whose witty, mischievous and sometimes macabre stories satirize Edwardian society and culture. He is considered a master of the short story, and often compared to O. Henry and Dorothy Parker. Influenced by Oscar Wilde, Lewis Carroll and Rudyard Kipling, he himself influenced A. A. Milne, Noël Coward and P. G. Wodehouse.

Besides his short stories (which were first published in newspapers, as was customary at the time, and then collected into several volumes), he wrote a full-length play, "The Watched Pot", in collaboration with Charles Maude; two one-act plays; a historical study, "The Rise of the Russian Empire" (the only book published under his own name); a short novel, "The Unbearable Bassington"; the episodic "The Westminster Alice" (a parliamentary parody of "Alice in Wonderland"); and "When William Came", subtitled "A Story of London Under the Hohenzollerns", a fantasy about a future German invasion and occupation of Britain.

Hector Hugh Munro was born in Akyab, British Burma, which was then still part of British India, and was governed from Calcutta, under the authority of the Viceroy of India. Saki was the son of Charles Augustus Munro, an Inspector General for the Indian Imperial Police, and his wife, Mary Frances Mercer (1843–1872), the daughter of Rear Admiral Samuel Mercer. Her nephew Cecil William Mercer later became a well-known novelist, under the name of Dornford Yates.

In 1872, on a home visit to England, Mary Munro was charged by a cow, and the shock caused her to miscarry. She never recovered and soon died.

After his wife's death Charles Munro sent his children, including two-year-old Hector, home to England. The children were sent to Broadgate Villa, in Pilton village near Barnstaple, North Devon, to be raised by their grandmother and paternal maiden aunts Charlotte and Augusta in a strict and puritanical household. It is said that his aunts were most likely models for some of his characters, notably the aunt in 'The Lumber Room' and the guardian in 'Sredni Vashtar': Munro's sister Ethel said that the aunt in "The Lumber Room" was an almost perfect portrait of Aunt Augusta. Munro and his siblings led slightly insular lives during their early years and were educated by governesses. At the age of 12 the young Hector Munro was educated at Pencarwick School in Exmouth and then as a boarder at Bedford School.

In 1887, after his retirement, his father returned from Burma and embarked upon a series of European travels with Hector and his siblings.

Hector followed his father in 1893 into the Indian Imperial Police and was posted to Burma, but successive bouts of fever caused his return home after only fifteen months.

In 1896, he decided to move to London to make a living as a writer.

Munro started his writing career as a journalist for newspapers such as the "Westminster Gazette", the "Daily Express", the "Morning Post", and magazines such as the "Bystander" and "Outlook". His first book "The Rise of the Russian Empire", a historical study modeled upon Edward Gibbon's "The Decline and Fall of the Roman Empire", appeared in 1900, under his real name, but proved to be something of a false start.

While writing "The Rise of the Russian Empire", he made his first foray into short story writing and published a piece called 'Dogged' in "St Paul's" in February 1899. He then moved into the world of political satire in 1900 with a collaboration with Francis Carruthers Gould entitled "Alice in Westminster". Gould produced the sketches, and Munro wrote the text accompanying them, using the pen-name "Saki" for the first time. The series lampooned political figures of the day ('Alice in Downing Street' begins with the memorable line, '"Have you ever seen an Ineptitude?"' - referring to a zoomorphised Arthur Balfour), and was published in the Liberal "Westminster Gazette."

In 1902 he moved to "The Morning Post", described as one of the 'organs of intransigence' by Stephen Koss, to work as a foreign correspondent, first in the Balkans, and then in Russia, where he was witness to the 1905 revolution in St Petersburg. He then went on to Paris, before returning to London in 1908, where 'the agreeable life of a man of letters with a brilliant reputation awaited him.' In the intervening period "Reginald" had been published in 1904, the stories having first appeared in the "Westminster Gazette", and all this time he was writing sketches for the "Morning Post", the "Bystander," and the "Westminster Gazette". He kept a place in Mortimer Street, wrote, played bridge at the Cocoa Tree Club, and lived simply. "Reginald in Russia" appeared in 1910, and "The Chronicles of Clovis" was published in 1911, and "Beasts and Super-Beasts" in 1914, along with many other short stories that appeared in newspapers not published in collections in his lifetime.

He also produced two novels, "The Unbearable Bassington" (1912) and "When William Came" (1913).

At the start of the First World War Munro was 43 and officially over-age to enlist, but he refused a commission and joined the 2nd King Edward's Horse as an ordinary trooper. He later transferred to the 22nd Battalion of the Royal Fusiliers, in which he rose to the rank of lance sergeant. More than once he returned to the battlefield when officially still too sick or injured. In November 1916 he was sheltering in a shell crater near Beaumont-Hamel, France, during the Battle of the Ancre, when he was killed by a German sniper. According to several sources, his last words were "Put that bloody cigarette out!"

Munro has no known grave. He is commemorated on Pier and Face 8C 9A and 16A of the Thiepval Memorial.

In 2003 English Heritage marked Munro's flat at 97 Mortimer Street, in Fitzrovia with a blue plaque.

After his death his sister Ethel destroyed most of his papers and wrote her own account of their childhood, which appeared at the beginning of "The Square Egg and Other Sketches" (1924). Rothay Reynolds, a close friend, wrote a relatively lengthy memoir in "The Toys of Peace" (1919), but aside from this, the only other biographies of Munro are "Saki: A Life of Hector Hugh Munro" (1982) by A. J. Langguth, and "The Unbearable Saki" (2007) by Sandie Byrne. All later biographies have had to draw heavily upon Ethel's account of her brother's life.

Munro was homosexual at a time when in Britain sexual activity between men was a crime. The Cleveland Street scandal (1889), followed by the downfall of Oscar Wilde (1895), meant "that side of [Munro's] life had to be secret".

The pen name "Saki" is most commonly assumed to be a reference to the cupbearer in the "Rubáiyát of Omar Khayyam." Both Rothay Reynolds and Ethel Munro confirm this.

This reference is stated as fact by Emlyn Williams in his introduction to a Saki anthology published in 1978. However, "Saki" may also or instead be a reference to the South American monkey of that name, which at least two commentators, Tom Sharpe and Will Self, have connected to the "small, long-tailed monkey from the Western Hemisphere" that is a central character in .

Much of Saki's work contrasts the conventions and hypocrisies of Edwardian England with the ruthless but straightforward life-and-death struggles of nature. Writing in "The Guardian" to mark the centenary of Saki's death, Stephen Moss noted, "In many of his stories, stuffy authority figures are set against forces of nature – polecats, hyenas, tigers. Even if they are not eaten, the humans rarely have the best of it".

"The Interlopers" is a story about two men, Georg Znaeym and Ulrich von Gradwitz, whose families have fought over a forest in the eastern Carpathian Mountains for generations. Ulrich's family legally owns the land, and so considers Georg an interloper when he hunts in the forest. But Georg, believing that the forest rightfully belongs to his family, hunts there often and believes that Ulrich is the real interloper for trying to stop him. One winter night, Ulrich catches Georg hunting in the forest. Neither man can shoot the other without warning, as they would soil their family’s honor, so they hesitate to acknowledge one another. In an "act of God", a tree branch suddenly falls on each of them, trapping them both under a log. Gradually they realize the futility of their quarrel, become friends and end the feud. They then call out for their men’s assistance and, after a brief period, Ulrich makes out nine or ten figures approaching over a hill. The story ends with Ulrich’s realization that the approaching figures on the hill are actually hungry wolves. The wolves, it seems, are the true owners of the forest, while both humans are interlopers.

"Gabriel-Ernest" starts with a warning: "There is a wild beast in your woods …" Gabriel, a naked boy sunbathing by the river, is "adopted" by well-meaning Townspeople. Lovely and charming, but also rather vague and distant, he seems bemused by his "benefactors." Asked how he managed by himself in the woods, he replies that he hunts "on four legs," which they take to mean that he has a dog. The climax comes when a small child disappears while walking home from Sunday school. A pursuit ensues, but Gabriel and the child disappear near a river. The only items found are Gabriel's clothes, and the two are never seen again. The story includes many of the author's favorite themes: good intentions gone awry, the banality of polite society, the attraction of the sinister, and the allure of the wild and the forbidden. There is also a recognition of basic decency, upheld when the story’s protagonist ‘flatly refuses’ to subscribe to a Gabriel-Ernst memorial, for his supposedly gallant attempt to save a drowning child, and tragically drowning himself, as well. He realises that Gabriel-Ernst was actually a werewolf, who had eaten the child, then run off.

At a railway station an arrogant and overbearing woman, Mrs Quabarl, mistakes the mischievous Lady Carlotta, who has been inadvertently left behind by a train, for the governess, Miss Hope, whom she has been expecting, Miss Hope having erred about the date of her arrival. Lady Carlotta decides not to correct the mistake, acknowledges herself as Miss Hope, a proponent of "the Schartz-Metterklume method" of making children understand history by acting it out themselves, and chooses the Rape of the Sabine Women (exemplified by a washerwoman's two girls) as the first lesson. After creating chaos for two days, she departs, explaining that her delayed luggage will include a leopard cub.

Preferring not to give her young sons toy soldiers or guns, and having taken away their toy depicting the Siege of Adrianople, Eleanor instructs her brother Harvey to give them innovative "peace toys" as an Easter present. When the packages are opened young Bertie shouts "It's a fort!" and is disappointed when his uncle replies "It's a municipal dustbin." The boys are initially baffled as to how to obtain any enjoyment from models of a school of art and a public library, or from little figures of John Stuart Mill, Felicia Hemans and Sir John Herschel. Youthful inventiveness finds a way, however, as the boys combine their history lessons on Louis XIV with a lurid and violent play-story about the invasion of Britain and the storming of the Young Women's Christian Association. The end of the story has Harvey reporting failure to Eleanor, explaining "We have begun too late.", not realising he was doomed to failure whenever he had begun.

An aunt is travelling by train with her two nieces and a nephew. The children are inquisitive and mischievous. A bachelor is also travelling in the same compartment. The aunt starts telling a moralistic story, but is unable to satisfy the children's curiosity. The bachelor butts in and tells a story in which a "good" person ends up being devoured by a wolf, to the children's delight. The bachelor is amused by the thought that in the future the children will embarrass their guardian by begging to be told "an improper story."

Framton Nuttel, a nervous man, has come to stay in the country for his health. His sister, who thinks he should socialise while he is there, has given him letters of introduction to families in the neighbourhood whom she got to know when she was staying there a few years previously. Framton goes to visit Mrs Sappleton and, while he is waiting for her to come down, is entertained by her fifteen-year-old, witty niece. The niece tells him that the French window is kept open, even though it is October, because Mrs Sappleton believes that her husband and her brothers, who were drowned in a bog three years before, will come back one day. When Mrs Sappleton comes down she talks about her husband and her brothers, and how they are going to come back from shooting soon, and Framton, believing that she is deranged, tries to distract her by talking about his health. Then, to his horror, Mrs Sappleton points out that her husband and her brothers are coming, and he sees them walking towards the window with their dog. He thinks he is seeing ghosts and runs away. Mrs Sappleton cannot understand why he has run away and, when her husband and her brothers come in, she tells them about the odd man who has just left. The niece explains that Framton Nuttel ran away because of the spaniel: he is afraid of dogs since he was hunted by a pack of stray dogs in India and had to spend a night in the newly dug grave with creatures grinning and foaming just above him. The last line summarizes the story, saying of the niece, "Romance at short notice was her speciality."

Saki's recurring hero Clovis Sangrail, a clever, mischievous young man, overhears the complacent middle-aged Huddle complaining of his own addiction to routine and aversion to change. Huddle's friend makes the wry suggestion that he needs an "unrest-cure" (the opposite of a rest cure), to be performed, if possible, in the home. Clovis takes it upon himself to "help" the man and his sister by involving them in an invented outrage that will be a "blot on the twentieth century".
A baroness tells Clovis a story about a hyena that she and her friend Constance encountered while out fox hunting. Later, the hyena follows them, stopping briefly to eat a gypsy child. Shortly after this, the hyena is killed by a motorcar. The baroness immediately claims the corpse as her beloved dog Esmé, and the guilty owner of the car gets his chauffeur to bury the animal and later sends her a diamond brooch to make up for her loss.

A sickly child named Conradin is raised by his aunt and guardian, Mrs De Ropp, who "would never... have confessed to herself that she disliked Conradin, though she might have been dimly aware that thwarting him 'for his good' was a duty which she did not find particularly irksome". Conradin rebels against his aunt and her choking authority. He invents a religion in which his pole-cat ferret is imagined as a vengeful deity, and Conradin prays that "Sredni Vashtar" will deliver retribution upon De Ropp. When De Ropp attempts to dispose of the animal, it attacks and kills her. The entire household is shocked and alarmed; Conradin calmly butters another piece of toast.

At a country-house party, one guest, Cornelius Appin, announces to the others that he has perfected a procedure for teaching animals human speech. He demonstrates this on his host's cat, Tobermory. Soon it is clear that animals are permitted to view many private things on the assumption that they will remain silent, such as the host Sir Wilfred's commentary on one guest's intelligence and the hope that she will buy his car, or the implied sexual activities of some of the other guests. The guests are angered, especially when Tobermory runs away to pursue a rival cat, but plans to poison him fail when Tobermory is instead killed by the rival cat. "An archangel ecstatically proclaiming the Millennium, and then finding that it clashed unpardonably with Henley and would have to be indefinitely postponed, could hardly have felt more crestfallen than Cornelius Appin at the reception of his wonderful achievement." Appin is killed shortly afterwards when attempting to teach an elephant in a zoo in Dresden to speak German.
His fellow house party guest, Clovis Sangrail (Saki's recurring hero) remarks callously that if he was teaching "the poor beast" irregular German verbs, he deserved no pity.

Tom Yorkfield, a farmer, receives a visit from his half-brother Laurence. Tom has no great liking for Laurence or respect for his profession as a painter of animals. Tom shows Laurence his prize bull and expects him to be impressed, but Laurence nonchalantly tells Tom that he has sold a painting of a different bull, which Tom has seen and does not like, for three hundred pounds. Tom is angry that a mere picture of a bull should be worth more than his real bull. This and Laurence's condescending attitude give him the urge to strike him. Laurence, running away across the field, is attacked by the bull, but is saved by Tom from serious injury. Tom, looking after Laurence as he recovers, feels no more rancour because he knows that, however valuable Laurence's painting might be, only a real bull like his can attack someone.

This is a "rediscovered" short story that was previously cited as a play. A house party is beset by a fire in the middle of the night in the east wing of the house. Begged by their hostess to save "my poor darling Eva – Eva of the golden hair," Lucien demurs, on the grounds that he has never even met her. It is only on discovering that Eva is not a flesh-and-blood daughter but Mrs Gramplain's painting of the daughter she wished that she had had, and which she has faithfully updated with the passing years, that Lucien declares a willingness to forfeit his life to rescue her, since "death in this case is more beautiful," a sentiment endorsed by the Major. As the two men disappear into the blaze, Mrs Gramplain recollects that she "sent Eva to Exeter to be cleaned". The two men have lost their lives for nothing.


Posthumous publications:

The 5th broadcast of Orson Welles' series for CBS Radio, "The Mercury Theatre on the Air", from 8 August 1938, dramatizes three short stories rather than one long story. The second of the three stories is "The Open Window."

"The Open Window" is also adapted (by John Allen) in the 1962 Golden Records release "Alfred Hitchcock Presents: Ghost Stories for Young People", a record album of six ghost stories for children.

A dramatisation of "The Schartz-Metterklume Method" was an episode in the series "Alfred Hitchcock Presents" in 1960.

"Saki: The Improper Stories of H. H. Munro" (a reference to the ending of "The Story Teller") was an eight-part series produced by Philip Mackie for Granada Television in 1962. Actors involved included Mark Burns as Clovis, Fenella Fielding as Mary Drakmanton, Heather Chasen as Agnes Huddle, Richard Vernon as the Major, Rosamund Greenwood as Veronique and Martita Hunt as Lady Bastable.

"Who Killed Mrs De Ropp?", a BBC TV production in 2007, starring Ben Daniels and Gemma Jones, showcased three of Saki's short stories, "The Storyteller", "The Lumber Room" and "Sredni Vashtar".





</doc>
<doc id="28811" url="https://en.wikipedia.org/wiki?curid=28811" title="Static program analysis">
Static program analysis

Static program analysis is the analysis of computer software that is performed without actually executing programs, in contrast with dynamic analysis, which is analysis performed on programs while they are executing. In most cases the analysis is performed on some version of the source code, and in the other cases, some form of the object code.

The term is usually applied to the analysis performed by an automated tool, with human analysis being called program understanding, program comprehension, or code review. Software inspections and software walkthroughs are also used in the latter case.

The sophistication of the analysis performed by tools varies from those that only consider the behaviour of individual statements and declarations, to those that include the complete source code of a program in their analysis. The uses of the information obtained from the analysis vary from highlighting possible coding errors (e.g., the lint tool) to formal methods that mathematically prove properties about a given program (e.g., its behaviour matches that of its specification).

Software metrics and reverse engineering can be described as forms of static analysis. Deriving software metrics and static analysis are increasingly deployed together, especially in creation of embedded systems, by defining so-called "software quality objectives".

A growing commercial use of static analysis is in the verification of properties of software used in safety-critical computer systems and
locating potentially vulnerable code. For example, the following industries have identified the use of static code analysis as a means of improving the quality of increasingly sophisticated and complex software:


A study in 2012 by VDC Research reports that 28.7% of the embedded software engineers surveyed currently use static analysis tools and 39.7% expect to use them within 2 years.
A study from 2010 found that 60% of the interviewed developers in European research projects made at least use of their basic IDE built-in static analyzers. However, only about 10% employed an additional other (and perhaps more advanced) analysis tool.

In the application security industry the name "Static Application Security Testing" (SAST) is also used. SAST is an important part of Security Development Lifecycles (SDLs) such as the SDL defined by Microsoft and a common practice in software companies.

The OMG (Object Management Group) published a study regarding the types of software analysis required for software quality measurement and assessment. This document on "How to Deliver Resilient, Secure, Efficient, and Easily Changed IT Systems in Line with CISQ Recommendations" describes three levels of software analysis.

A further level of software analysis can be defined.


Formal methods is the term applied to the analysis of software (and computer hardware) whose results are obtained purely through the use of rigorous mathematical methods. The mathematical techniques used include denotational semantics, axiomatic semantics, operational semantics, and abstract interpretation.

By a straightforward reduction to the halting problem, it is possible to prove that (for any Turing complete language), finding all possible run-time errors in an arbitrary program (or more generally any kind of violation of a specification on the final result of a program) is undecidable: there is no mechanical method that can always answer truthfully whether an arbitrary program may or may not exhibit runtime errors. This result dates from the works of Church, Gödel and Turing in the 1930s (see: Halting problem and Rice's theorem). As with many undecidable questions, one can still attempt to give useful approximate solutions.

Some of the implementation techniques of formal static analysis include:

Data-driven static analysis uses large amounts of code to infer coding rules. For instance, one can use all Java open-source packages on GitHub to learn a good analysis strategy. The rule inference can use machine learning techniques. For instance, it has been shown that when one deviates too much in the way one uses an object-oriented API, it is likely to be a bug. It is also possible to learn from a large amount of past fixes and warnings.





</doc>
<doc id="28812" url="https://en.wikipedia.org/wiki?curid=28812" title="Samuel Mudd">
Samuel Mudd

Samuel Alexander Mudd (December 20, 1833 – January 10, 1883) was an American physician who was imprisoned for conspiring with John Wilkes Booth in the assassination of President Abraham Lincoln.

Mudd worked as a doctor and tobacco farmer in Southern Maryland. The Civil War seriously damaged his business, especially when Maryland abolished slavery in 1864. That year, he first met Booth, who was planning to kidnap Lincoln, and Mudd was seen in company with three of the conspirators. However, his part in the plot, if any, remains unclear.

After mortally wounding Lincoln on April 14, 1865, Booth rode with conspirator David Herold to Mudd's home in the early hours of April 15 for surgery on his fractured leg before he crossed into Virginia. Sometime that day, Mudd must have learned of the assassination but did not report Booth's visit to the authorities for another 24 hours. That appeared to link him to the crime, as did his various changes of story under interrogation. A military commission found him guilty of aiding and conspiring in a murder, and he was sentenced to life imprisonment, escaping the death penalty by a single vote.

Mudd was pardoned by President Andrew Johnson and released from prison in 1869. Despite repeated attempts by family members and others to have it expunged, his conviction has not been overturned.

Born in Charles County, Maryland, Mudd was the fourth of 10 children of Henry Lowe and Sarah Ann Reeves Mudd. He grew up on Oak Hill, his father's tobacco plantation of several hundred acres, southeast of Washington, DC, and worked by 89 slaves.

At 15, after several years of home tutoring, Mudd went off to boarding school at St. John's Literary Institute, now known as Saint John's Catholic Prep School in Frederick, Maryland. Two years later, he enrolled at Georgetown College in Washington, DC. He then studied medicine at the University of Maryland, Baltimore, writing his thesis on dysentery.

Upon graduation in 1856, Mudd returned to Charles County to practice medicine, marrying his childhood sweetheart, Sarah Frances (Frankie) Dyer Mudd one year later.

As a wedding present, Mudd's father gave the couple of his best farmland and a new house named St. Catherine. While the house was under construction, the young Mudds lived with Frankie's bachelor brother, Jeremiah Dyer, finally moving into their new home in 1859. They had nine children in all: four before Mudd's arrest and five after his release from prison. To supplement his income from his medical practice, Mudd became a small scale tobacco grower, using five slaves according to the 1860 census. Mudd believed that slavery was divinely ordained and wrote a letter to the theologian Orestes Brownson to that effect.

With the advent of the American Civil War in 1861, the Southern Maryland slave system and the economy that it supported rapidly began to collapse. In 1863, the Union Army established Camp Stanton, just from the Mudd farm to enlist black freedmen and runaway slaves. Six regiments totaling over 8,700 black soldiers, many from Southern Maryland, were trained there. In 1864, Maryland, which was exempt from Lincoln's 1863 Emancipation Proclamation, abolished slavery, making it difficult for growers like Mudd to operate their plantations. As a result, Mudd considered selling his farm and depending on his medical practice. As Mudd pondered his alternatives, he was introduced to someone who said he might be interested in buying his property, a 26-year-old actor, John Wilkes Booth.

Many historians agree that President Abraham Lincoln's future assassin, John Wilkes Booth, visited Bryantown, Maryland, in November and December 1864, claiming to look for real estate investments. Bryantown is about from Washington, DC, and about from Mudd's farm. The real estate story was merely a cover; Booth's true purpose was to plan an escape route as part of a plan to kidnap Lincoln. Booth believed the federal government would ransom Lincoln by releasing a large number of Confederate prisoners of war.
Historians agree that Booth met Mudd at St. Mary's Catholic Church in Bryantown during one of those visits, probably in November. Booth visited Mudd at his farm the next day, and stayed there overnight. The following day, Booth purchased a horse from Mudd's neighbor and returned to Washington. Some historians believe that Booth used his visit to Bryantown to recruit Mudd to his kidnapping plot, but others believe that Mudd would have had no interest in such a scheme.

A short time later, on December 23, 1864, Mudd went to Washington where he met Booth again. Some historians believe the meeting had been arranged, but others disagree. The two men, as well as John Surratt, Jr., and Louis J. Weichmann, had a conversation and drinks together, first at Booth's hotel and later at Mudd's.

According to a statement made by associated conspirator George Atzerodt, found long after his death and taken down while he was in federal custody on May 1, 1865, Mudd knew in advance about Booth's plans; Atzerodt was sure the doctor knew, he said, because Booth had "sent (as he told me) liquors & provisions... about two weeks before the murder to Dr. Mudd's."

Although that is true, some historians believe that there may be other reasons behind Mudd's relationship to Booth. The trial brought forth many theories of Mudd's involvement in the assassination of Lincoln. One theory posits that Mudd was involved in a completely different conspiracy to gain an upper hand for the southern states. Prior to the assassination of Lincoln, Booth originally intended to kidnap the president and hold him and other political affiliates of the Union for a large sum of money. The plan was in effect until the night of the assassination, when Booth met up with Atzerodt, David Herold and Lewis Powell (who gave his name as Lewis Payne when he was arrested at Mary Surratt's house days after the murder) and disclosed the plot to assassinate the president. Following the assassination, Powell came forth by stating that Booth had not told him until the meeting and that the other men did not know about the plot until the night of the assassination. That supports the theory that Mudd may have been an accomplice to the plot to kidnap the president but not a conspirator to the assassination.

After Booth shot Lincoln on April 14, 1865, he broke his left fibula while fleeing Ford's Theater. Booth met up with Herold and both made for Virginia, via southern Maryland. They stopped at Mudd's house around 4 a.m. on April 15. Mudd splinted Booth's leg, and gave him a shoe to wear. He also arranged for a carpenter, John Best, to make a pair of crutches for Booth. Booth paid Mudd $25 in greenbacks for his services. He and Herold spent between twelve and fifteen hours at Mudd's house. They slept in the front bedroom on the second floor. It is unclear whether Mudd had yet been informed that Booth had killed Lincoln.

Mudd went to Bryantown during the day on April 15 to run errands; if he did not already know the news of the assassination from Booth, he certainly learned of it on the trip. He returned home that evening, and accounts differ as to whether Booth and Herold had already left, whether Mudd met them as they were leaving, and whether they left at Mudd's urging and with his assistance.

It is certain that Mudd did not immediately contact the authorities. When questioned, he stated that he had not wanted to leave his family alone in the house in case the assassins returned and found him absent and his family unprotected. He waited until Mass the following day, Easter Sunday, when he asked his second cousin, Dr. George Mudd, a resident of Bryantown, to notify the 13th New York Cavalry in Bryantown, under the command of Lieutenant David Dana. The delay in contacting the authorities drew suspicion and was a significant factor in tying Mudd to the conspiracy.

During his initial investigative interview on April 18, Mudd stated that he had never seen either of the parties before. In his sworn statement of April 22, he told about Booth's visit to Bryantown in November 1864
but then said, "I have never seen Booth since that time to my knowledge until last Saturday morning." He hid his meeting with Booth in Washington in December 1864. In prison, Mudd admitted the Washington meeting and said he ran into Booth by chance during a Christmas shopping trip. Mudd's failure to mention the meeting in his interview with detectives was a big mistake. When Weichmann later told the authorities of the meeting, they realized that Mudd had misled them and immediately began to treat him as a suspect, rather than a witness.

During the conspiracy trial, Lieutenant Alexander Lovett testified, "On Friday, the 21st of April, I went to Mudd's again, for the purpose of arresting him. When he found we were going to search the house, he said something to his wife, and she went upstairs and brought down a boot. Mudd said he had cut it off the man's leg. I turned down the top of the boot, and saw the name 'J. Wilkes' written in it."

After Booth's death on April 26, 1865, Mudd was arrested and charged with conspiracy to murder Lincoln. Representative Frederick Stone was the senior defense counsel for Mudd.

On May 1, 1865, President Johnson ordered the formation of a nine-man military commission to try the conspirators. Mudd was represented by General Thomas Ewing, Jr.. The trial began on May 10, 1865. Mary Surratt, Lewis Powell, George Atzerodt, David Herold, Samuel Mudd, Michael O'Laughlen, Edmund Spangler and Samuel Arnold were all charged with conspiring to murder Lincoln. The prosecution called 366 witnesses.

The defense sought to prove that Mudd was a loyal citizen, citing his self-description as a "Union man" and asserting that he was "a deeply religious man, devoted to family, and a kind master to his slaves." The prosecution presented witnesses who testified that he had shot one of his slaves in the leg and threatened to send others to Richmond, Virginia, to assist in the construction of Confederate defenses. The prosecution also contended that he had been a member of a Confederate communications distribution agency and had sheltered Confederate soldiers on his plantation.

On June 29, 1865, Mudd was found guilty with the others. The testimony of Louis J. Weichmann was crucial in obtaining the convictions. According to historian Edward Steers, the testimony presented by former slaves was also crucial, but it faded from public memory. Mudd escaped the death penalty by one vote and was sentenced to life imprisonment. Four of the defendants (Surratt, Powell, Atzerodt and Herold) were hanged at the Old Penitentiary at the Washington Arsenal on July 7, 1865.

Mudd, O'Laughlen, Arnold, and Spangler were imprisoned at Fort Jefferson, in the Dry Tortugas, about west of Key West, Florida. The fort housed Union Army deserters and held about 600 prisoners when Mudd and the others arrived. Prisoners lived on the second tier of the fort, in unfinished, open-air gun rooms called casemates. Mudd and his three companions lived in the casemate directly above the fort's main entrance, called the sally port.

In September 1865, two months after Mudd arrived, the control of Fort Jefferson was transferred from the 161st New York Volunteer Infantry Regiment to the 82nd US Colored Troops. On September 25, 1865, he attempted to escape from Fort Jefferson by stowing away on the transport "Thomas A. Scott".

He was quickly discovered and placed, along with Arnold, O'Laughlen, Spangler, and George St. Leger Grenfell, in a large empty ground-level gunroom that soldiers referred to as "the dungeon." The men were let out of the dungeon every working day for 12 hours and were required to wear leg irons. However, following a December 22 letter from his wife to President Johnson, the War Department ordered the discontinuance of the shackles and the move to better quarters, which was accomplished by January.

After three months in the dungeon, Mudd and the others were returned to the general prison population. However, because of his attempted escape, Mudd lost his privilege of working in the prison hospital and was assigned to work in the prison carpentry shop with Spangler.

There was an outbreak of yellow fever in the fall of 1867 at the fort. O'Laughlen eventually died of it on September 23. The prison doctor died, and Mudd agreed to take over the position. He was able to help stem the spread of the disease. The soldiers in the fort wrote a petition to Johnson in October 1867 stating the degree of Mudd's assistance: "He inspired the hopeless with courage and by his constant presence in the midst of danger and infection... [Many] doubtless owe their lives to the care and treatment they received at his hands." Probably as a reward for his work in the yellow fever epidemic, Mudd was reassigned from the carpentry shop to a clerical job in the Provost Marshal's office, where he remained until his pardon.

The influence of his defense attorney, Thomas Ewing Jr., who was also influential in the President's administration, was one reason why Mudd was pardoned by Johnson on February 8, 1869. He was released from prison on March 8, 1869, and returned to his home in Maryland on March 20, 1869. On March 2, 1869, three weeks after he pardoned Mudd, Johnson also pardoned Spangler and Arnold.

When Mudd returned home, well-wishing friends and strangers, as well as inquiring newspaper reporters, besieged him. Mudd was very reluctant to talk to the press because he felt it had misquoted him in the past. He gave one interview to the "New York Herald" after his release but immediately regretted it and complained that the article had several factual errors and misrepresented his work during the yellow fever epidemic. On the whole, though, Mudd continued to enjoy the support of his friends and neighbors. He resumed his medical practice and slowly brought the family farm back to productivity.

In 1873, Spangler traveled to the Mudd farm, where Mudd and his wife welcomed him. Spangler lived with the Mudd family for about 18 months, earning his keep by doing carpentry, gardening, and other farm chores, until his death on February 7, 1875.

Mudd always had an interest in politics. In prison, he learned about political happenings by reading the newspapers that were sent to him. After his release, he again became active in community affairs. In 1874, he was elected chief officer of the local farmers association, the Bryantown Grange. In 1876, he was elected Vice President of the local Democratic Tilden-Hendricks presidential election committee. Tilden lost that year to Republican Rutherford B. Hayes in a hotly-disputed election. The next year, Mudd ran as a Democratic candidate for the Maryland House of Delegates but was defeated by the popular Republican William Mitchell.

Mudd's ninth child, Mary Eleanor "Nettie" Mudd, was born in 1878.

In 1880, the Port Tobacco Times reported that Mudd's barn which contained almost 8000 lb. of tobacco, two horses, a wagon, and farm implements was destroyed by fire.

Mudd was just 49 years old when he died of pneumonia, on January 10, 1883, and was buried in the cemetery at St. Mary's Catholic Church in Bryantown, the same church in which he once met Booth.

The degree of Samuel Mudd's culpability has remained controversial ever since. Some, including Mudd's grandson Richard Mudd, claimed that Mudd was innocent of any wrongdoing and that he had merely been imprisoned for treating a man who came to his house late at night with a fractured leg. Over a century after the assassination, Presidents Jimmy Carter and Ronald Reagan both wrote letters to Richard Mudd in which they agreed that his grandfather had committed no crime. However, others, including authors Edward Steers, Jr. and James Swanson, assert evidence that Samuel Mudd visited Booth three times in the months before the failed kidnapping attempt. The first time was November 1864 when Booth, who was looking for help in his kidnapping plot, was directed to Mudd by agents of the Confederate Secret Service. In December, Booth again met with Mudd and spent the night at his farm. Later that December, Mudd went to Washington and introduced Booth to a Confederate agent whom he knew: John Surratt. Additionally, George Atzerodt testified that Booth sent supplies to Mudd's house in preparation for the kidnapping plan. Mudd lied to the authorities who came to his house after the assassination, claiming that he did not recognize the man who showed up on his doorstep in need of treatment and giving them false information about where Booth and Herold went. He also hid the monogrammed boot that he had cut off Booth's injured leg behind a panel in his attic, but the thorough search of Mudd's house soon revealed this further piece of evidence which was later used against him. One hypothesis is that Dr. Mudd was originally complicit in the kidnapping plot, likely as the person who the conspirators would have turned to for medical treatment in case Lincoln was injured, and that Booth thus remembered the doctor and went to his house to get help in the early hours of April 15.
Mudd's grandson, Richard Mudd, unsuccessfully tried to clear his grandfather's name from the stigma of aiding Booth. In 1951, he published "The Mudd Family of the United States", an encyclopedic two-volume history of the Mudd family in America, beginning with Thomas Mudd, who arrived from England in 1665. A second edition was published in 1969. Following his death in 2002, his papers, which detailed his attempts to clear his grandfather's name, were donated to Georgetown University's Lauinger Library. They are available to the public in the Special Collections Department.

Richard Mudd petitioned several successive presidents, receiving replies from Presidents Jimmy Carter and Ronald Reagan. Carter, while sympathetic, responded by stating that he had no authority under law to set aside the conviction; Reagan responded by stating that he had come to believe that Samuel Mudd was innocent of any wrongdoing. In 1992, Representatives Steny Hoyer and Thomas W. Ewing introduced House Bill 1885 to overturn the conviction, but it failed in committee. Mudd then turned to the Army Board for Correction of Military Records, which recommended that the conviction be overturned on the basis that Mudd should have been tried by a civilian court. The recommendation was rejected by Acting Army Assistant Secretary William D. Clark.

Several other legal venues were attempted, ending in 2003 when the US Supreme Court refused to hear the case because the deadline for filing it had been missed.

St. Catharine, also known as the Dr. Samuel A. Mudd House, was listed on the National Register of Historic Places in 1974.

Mudd's life was the subject of a 1936 John Ford-directed film "The Prisoner of Shark Island", based on a script by Nunnally Johnson. A radio adaptation of "The Prisoner of Shark Island" aired, as an episode of the radio series Lux Radio Theater, with Gary Cooper as Dr. Mudd, on May 2, 1938, in which significant dramatic license was used by introducing fictional characters and altering several of the known facts of the case for melodramatic expediency. For example, Fort Jefferson was never called "Shark Island."

Another production, with the same title, aired on the radio series Encore Theatre in 1946. Another film, "The Ordeal of Dr. Mudd", was made in 1980. It starred Dennis Weaver as Mudd. At the end, a written message appears incorrectly stating that President Carter gave Mudd a posthumous pardon. All of these productions espoused the point of view that Mudd was essentially innocent of any conspiracy.

Roger Mudd, an Emmy Award-winning journalist, television host and former CBS, NBC, and PBS news anchor is related to Samuel Mudd, but he is not a descendant, as has mistakenly been reported.

Samuel Mudd's life was also the subject of an episode of the TV western "Laramie", "Time of the Traitor" which aired in 1962.

On the episode "Swiss Diplomacy" on "The West Wing", the First Lady and cardiac surgeon, Dr. Abby Bartlet commented on the duty of a physician to treat an injured patient despite potential legal repercussions. She responded to Mudd's conviction: "So that's the way it goes. You set the leg."

Samuel Mudd's name is sometimes given as the origin of the phrase "your name is mud," as in, for example, the 2007 feature film "". However, according to an online etymology dictionary, the phrase has its earliest known recorded instance in 1823, ten years before Mudd's birth, and it is based on an obsolete sense of the word "mud" meaning "a stupid twaddling fellow." 



</doc>
<doc id="28814" url="https://en.wikipedia.org/wiki?curid=28814" title="Secure Shell">
Secure Shell

Secure Shell (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network. Typical applications include remote command-line, login, and remote command execution, but any network service can be secured with SSH.

SSH provides a secure channel over an unsecured network in a client–server architecture, connecting an SSH client application with an SSH server. The protocol specification distinguishes between two major versions, referred to as SSH-1 and SSH-2. The standard TCP port for SSH is 22. SSH is generally used to access Unix-like operating systems, but it can also be used on Microsoft Windows. Windows 10 uses OpenSSH as its default SSH client.

SSH was designed as a replacement for Telnet and for unsecured remote shell protocols such as the Berkeley rlogin, rsh, and rexec protocols. Those protocols send information, notably passwords, in plaintext, rendering them susceptible to interception and disclosure using packet analysis. The encryption used by SSH is intended to provide confidentiality and integrity of data over an unsecured network, such as the Internet, although files leaked by Edward Snowden indicate that the National Security Agency can sometimes decrypt SSH, allowing them to read the contents of SSH sessions.

SSH uses public-key cryptography to authenticate the remote computer and allow it to authenticate the user, if necessary. There are several ways to use SSH; one is to use automatically generated public-private key pairs to simply encrypt a network connection, and then use password authentication to log on.

Another is to use a manually generated public-private key pair to perform the authentication, allowing users or programs to log in without having to specify a password. In this scenario, anyone can produce a matching pair of different keys (public and private). The public key is placed on all computers that must allow access to the owner of the matching private key (the owner keeps the private key secret). While authentication is based on the private key, the key itself is never transferred through the network during authentication. SSH only verifies whether the same person offering the public key also owns the matching private key. In all versions of SSH it is important to verify unknown public keys, i.e. associate the public keys with identities, before accepting them as valid. Accepting an attacker's public key without validation will authorize an unauthorized attacker as a valid user.

On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is allowed to log in remotely, in the file ~/.ssh/authorized_keys. This file is respected by SSH only if it is not writable by anything apart from the owner and root. When the public key is present on the remote end and the matching private key is present on the local end, typing in the password is no longer required (some software like Message Passing Interface (MPI) stack may need this password-less access to run properly). However, for additional security the private key itself can be locked with a passphrase.

The private key can also be looked for in standard places, and its full path can be specified as a command line setting (the option "-i" for ssh). The ssh-keygen utility produces the public and private keys, always in pairs.

SSH also supports password-based authentication that is encrypted by automatically generated keys. In this case, the attacker could imitate the legitimate server side, ask for the password, and obtain it (man-in-the-middle attack). However, this is possible only if the two sides have never authenticated before, as SSH remembers the key that the server side previously used. The SSH client raises a warning before accepting the key of a new, previously unknown server. Password authentication can be disabled.

SSH is typically used to log into a remote machine and execute commands, but it also supports tunneling, forwarding TCP ports and X11 connections; it can transfer files using the associated SSH file transfer (SFTP) or secure copy (SCP) protocols. SSH uses the client-server model.

The standard TCP port 22 has been assigned for contacting SSH servers.

An SSH client program is typically used for establishing connections to an SSH daemon accepting remote connections. Both are commonly present on most modern operating systems, including macOS, most distributions of Linux, OpenBSD, FreeBSD, NetBSD, Solaris and OpenVMS. Notably, versions of Windows prior to Windows 10 version 1709 do not include SSH by default. Proprietary, freeware and open source (e.g. PuTTY, and the version of OpenSSH which is part of Cygwin) versions of various levels of complexity and completeness exist. File managers for UNIX-like systems (e.g. Konqueror) can use the FISH protocol to provide a split-pane GUI with drag-and-drop. The open source Windows program WinSCP provides similar file management (synchronization, copy, remote delete) capability using PuTTY as a back-end. Both WinSCP and PuTTY are available packaged to run directly off a USB drive, without requiring installation on the client machine. Setting up an SSH server in Windows typically involves enabling a feature in Settings app. In Windows 10 version 1709, an official Win32 port of OpenSSH is available.

SSH is important in cloud computing to solve connectivity problems, avoiding the security issues of exposing a cloud-based virtual machine directly on the Internet. An SSH tunnel can provide a secure path over the Internet, through a firewall to a virtual machine.

In 1995, Tatu Ylönen, a researcher at Helsinki University of Technology, Finland, designed the first version of the protocol (now called SSH-1) prompted by a password-sniffing attack at his university network. The goal of SSH was to replace the earlier rlogin, TELNET, FTP and rsh protocols, which did not provide strong authentication nor guarantee confidentiality. Ylönen released his implementation as freeware in July 1995, and the tool quickly gained in popularity. Towards the end of 1995, the SSH user base had grown to 20,000 users in fifty countries.

In December 1995, Ylönen founded SSH Communications Security to market and develop SSH. The original version of the SSH software used various pieces of free software, such as GNU libgmp, but later versions released by SSH Communications Security evolved into increasingly proprietary software.

It was estimated that by the year 2000 the number of users had grown to 2 million.

"Secsh" was the official Internet Engineering Task Force's (IETF) name for the IETF working group responsible for version 2 of the SSH protocol. In 2006, a revised version of the protocol, SSH-2, was adopted as a standard. This version is incompatible with SSH-1. SSH-2 features both security and feature improvements over SSH-1. Better security, for example, comes through Diffie–Hellman key exchange and strong integrity checking via message authentication codes. New features of SSH-2 include the ability to run any number of shell sessions over a single SSH connection. Due to SSH-2's superiority and popularity over SSH-1, some implementations such as libssh (v0.8.0+), Lsh and Dropbear support only the SSH-2 protocol.

In January 2006, well after version 2.1 was established, RFC 4253 specified that an SSH server which supports both 2.0 and prior versions of SSH should identify its protoversion as 1.99. This is not an actual version but a method to identify backward compatibility.

In 1999, developers, wanting a free software version to be available, went back to the older 1.2.12 release of the original SSH program, which was the last released under an open source license. Björn Grönvall's OSSH was subsequently developed from this codebase. Shortly thereafter, OpenBSD developers forked Grönvall's code and did extensive work on it, creating OpenSSH, which shipped with the 2.6 release of OpenBSD. From this version, a "portability" branch was formed to port OpenSSH to other operating systems.

, OpenSSH was the single most popular SSH implementation, coming by default in a large number of operating systems. OSSH meanwhile has become obsolete. OpenSSH continues to be maintained and supports the SSH-2 protocol, having expunged SSH-1 support from the codebase with the OpenSSH 7.6 release.

SSH is a protocol that can be used for many applications across many platforms including most Unix variants (Linux, the BSDs including Apple's macOS, and Solaris), as well as Microsoft Windows. Some of the applications below may require features that are only available or compatible with specific SSH clients or servers. For example, using the SSH protocol to implement a VPN is possible, but presently only with the OpenSSH server and client implementation.

The Secure Shell protocols are used in several file transfer mechanisms.

The SSH-2 protocol has an internal architecture (defined in RFC 4251) with well-separated layers, namely:


This open architecture provides considerable flexibility, allowing the use of SSH for a variety of purposes beyond a secure shell. The functionality of the transport layer alone is comparable to Transport Layer Security (TLS); the user-authentication layer is highly extensible with custom authentication methods; and the connection layer provides the ability to multiplex many secondary sessions into a single SSH connection, a feature comparable to BEEP and not available in TLS.

These are intended for performance enhancements of SSH products:

In 1998 a vulnerability was described in SSH 1.5 which allowed the unauthorized insertion of content into an encrypted SSH stream due to insufficient data integrity protection from CRC-32 used in this version of the protocol. A fix known as SSH Compensation Attack Detector was introduced into most implementations. Many of these updated implementations contained a new integer overflow vulnerability that allowed attackers to execute arbitrary code with the privileges of the SSH daemon, typically root.

In January 2001 a vulnerability was discovered that allows attackers to modify the last block of an IDEA-encrypted session. The same month, another vulnerability was discovered that allowed a malicious server to forward a client authentication to another server.

Since SSH-1 has inherent design flaws which make it vulnerable, it is now generally considered obsolete and should be avoided by explicitly disabling fallback to SSH-1. Most modern servers and clients support SSH-2.

In November 2008, a theoretical vulnerability was discovered for all versions of SSH which allowed recovery of up to 32 bits of plaintext from a block of ciphertext that was encrypted using what was then the standard default encryption mode, CBC. The most straightforward solution is to use CTR, counter mode, instead of CBC mode, since this renders SSH resistant to the attack.

On December 28, 2014 Der Spiegel published classified information leaked by whistleblower Edward Snowden which suggests that the National Security Agency may be able to decrypt some SSH traffic. The technical details associated with such a process were not disclosed.

An analysis in 2017 of the hacking tools BothanSpy & Gyrfalcon suggested that the SSH protocol itself was not compromised.

The following RFC publications by the IETF "secsh" working group document SSH-2 as a proposed Internet standard.

It was later modified and expanded by the following publications.

In addition, the OpenSSH project includes several vendor protocol specifications/extensions:





</doc>
<doc id="28819" url="https://en.wikipedia.org/wiki?curid=28819" title="Generalissimo Francisco Franco is still dead">
Generalissimo Francisco Franco is still dead

"Generalissimo Francisco Franco is still dead" is a catchphrase that originated in 1975 during the first season of "NBC's Saturday Night" (now called "Saturday Night Live", or "SNL") and which mocked the weeks-long media reports of the Spanish Caudillo's impending death. It was one of the first catchphrases from the series to enter the general lexicon.

The death of Spanish Caudillo Francisco Franco during the first season of "NBC's Saturday Night" served as the source of the phrase. Franco lingered near death for weeks before dying. On slow news days, United States network television newscasters sometimes noted that Franco was still alive, or not yet dead. The imminent death of Franco was a headline story on NBC News for a number of weeks prior to his death on November 20, 1975.

After Franco's death, Chevy Chase, reader of the news on "NBC's Saturday Night"'s comedic news segment "Weekend Update", announced Franco's death and read a quotation from Richard Nixon: "General Franco was a loyal friend and ally of the United States. He earned worldwide respect for Spain through firmness and fairness." As an ironic counterpoint to this, a picture was displayed behind Chase, showing Franco giving the Roman salute alongside Adolf Hitler.

In subsequent weeks Chase developed the joke into a parody of the earlier news coverage of Franco's illness, treating his death as the top story. "This breaking news just in", Chase would announce – "Generalissimo Francisco Franco is "still" dead!" Occasionally, Chase would change the wording slightly in order to keep the joke fresh, e.g. "Generalissimo Francisco Franco is still valiantly holding on in his fight to remain dead." The joke was sometimes combined with another running gag in which, rather than having a sign language interpreter visually presenting the news to aid the deaf, the show would provide assistance from Garrett Morris, "head of the New York School for the Hard of Hearing", whose "aid" involved cupping his hands around his mouth and shouting the news as Chase read it. The gag ran until early 1977, with occasional callbacks in later seasons.

The phrase has remained in use since Franco's death. James Taranto's "Best of the Web Today" column at OpinionJournal.com uses the phrase as a tag for newspaper headlines that indicate something is still happening when it should be obvious. On February 8, 2007, during Jack Cafferty's segment on CNN's "The Situation Room" on the day of the death of Anna Nicole Smith, he asked of CNN correspondent Wolf Blitzer "Is Anna Nicole Smith still dead, Wolf?" It was also used now and then on "NBC News Overnight" in the early 1980s, and Keith Olbermann occasionally used it on "Countdown". In 2013, it experienced a brief resurgence in a different context, when it began appearing on social media a few days after the death of Spanish filmmaker Jesus Franco.

"The Wall Street Journal" used the headline "Generalísimo Francisco Franco Is Still Dead – And His Statues Are Next" on its front page March 2, 2009. The newspaper used it once again on its front page in the headline "Generalísimo Francisco Franco Is Still Dead – But for some not dead enough" on August 21, 2015 when it reported about critics calling to enforce a 2007 anti-Franco law in Madrid and to rename streets and plazas, after the last election had ended the 24-year reign of conservatives in the city council.

Although "SNL"s use is perhaps the most widely known, it is predated by the "'John Garfield Still Dead' syndrome," which originated as a result of extensive coverage in the wake of the actor John Garfield's death and funeral in 1952.

After a brief "in memoriam" during "SNL"s 40th Anniversary Special on February 15, 2015, Bill Murray ended the segment with the famous phrase which "just came in from Spain."

Notes
Citations


</doc>
<doc id="28820" url="https://en.wikipedia.org/wiki?curid=28820" title="Son House">
Son House

Eddie James "Son" House, Jr. (March 21, 1902 – October 19, 1988) was an American delta blues singer and guitarist, noted for his highly emotional style of singing and slide guitar playing.

After years of hostility to secular music, as a preacher and for a few years also as a church pastor, he turned to blues performance at the age of 25. He quickly developed a unique style by applying the rhythmic drive, vocal power and emotional intensity of his preaching to the newly learned idiom. In a short career interrupted by a spell in Parchman Farm penitentiary, he developed to the point that Charley Patton, the foremost blues artist of the Mississippi Delta region, invited him to share engagements and to accompany him to a 1930 recording session for Paramount Records.

Issued at the start of the Great Depression, the records did not sell and did not lead to national recognition. Locally, House remained popular, and in the 1930s, together with Patton's associate Willie Brown, he was the leading musician of Coahoma County. There he was a formative influence on Robert Johnson and Muddy Waters. In 1941 and 1942, House and the members of his band were recorded by Alan Lomax and John W. Work for the Library of Congress and Fisk University. The following year, he left the Delta for Rochester, New York, and gave up music.

In 1964, a group of young record collectors discovered House, whom they knew of from his records issued by Paramount and by the Library of Congress. With their encouragement, he relearned his repertoire and established a career as an entertainer, performing for young, mostly white audiences in coffeehouses, at folk festivals and on concert tours during the American folk music revival, billed as a "folk blues" singer. He recorded several albums, and some informally taped concerts have also been issued as albums. House died in 1988. In 2017, his single, "Preachin' the Blues" was inducted in to the Blues Hall of Fame.

House was born in the hamlet of Lyon, north of Clarksdale, Mississippi, the second of three brothers, and lived in the rural Mississippi Delta until his parents separated, when he was about seven or eight years old. His father, Eddie House, Sr., was a musician, playing the tuba in a band with his brothers and sometimes playing the guitar. He was a church member but also a drinker; he left the church for a time, on account of his drinking, but then gave up alcohol and became a Baptist deacon. Young Eddie House adopted the family commitment to religion and churchgoing. He also absorbed the family love of music but confined himself to singing, showing no interest in the family instrumental band, and hostile to the blues on religious grounds.

When House's parents separated, his mother took him to Tallulah, Louisiana, across the Mississippi River from Vicksburg, Mississippi. When he was in his early teens, they moved to Algiers, New Orleans. Recalling these years, he would later speak of his hatred of blues and his passion for churchgoing (he described himself as "churchy" and "churchified"). At fifteen, probably while living in Algiers, he began preaching sermons.

At the age of nineteen, while living in the Delta, he married Carrie Martin, an older woman from New Orleans. This was a significant step for House; he married in church and against family opposition. The couple moved to her hometown of Centerville, Louisiana, to help run her father's farm. After a couple of years, feeling used and disillusioned, House recalled, "I left her hanging on the gatepost, with her father tellin' me to come back so we could plow some more." Around the same time, probably 1922, House's mother died. In later years, he was still angry about his marriage and said of Carrie, "She wasn't nothin' but one of them New Orleans whores".

House's resentment of farming extended to the many menial jobs he took as a young adult. He moved frequently, on one occasion taking off to East Saint Louis to work in a steel plant. The one job he enjoyed was on a Louisiana horse ranch, which later he celebrated by wearing a cowboy hat in his performances. He found an escape from manual labor when, following a conversion experience ("getting religion") in his early twenties, he was accepted as a paid pastor, first in the Baptist Church and then in the Colored Methodist Episcopal Church. However, he fell into habits which conflicted with his calling—drinking like his father and probably also womanizing. This led him after several years of conflict to leave the church, ceasing his full-time commitment, although he continued to preach sermons from time to time.

In 1927, at the age of 25, House underwent a change of musical perspective as rapid and dramatic as a religious conversion. In a hamlet south of Clarksdale, he heard one of his drinking companions, either James McCoy or Willie Wilson (his recollections differed), playing bottleneck guitar, a style he had never heard before. He immediately changed his attitude about the blues, bought a guitar from a musician called Frank Hoskins, and within weeks was playing with Hoskins, McCoy and Wilson. Two songs he learned from McCoy would later be among his best known: "My Black Mama" and "Preachin' the Blues". Another source of inspiration was Rube Lacey, a much better known performer who had recorded for Columbia Records in 1927 (no titles were released) and for Paramount Records in 1928 (two titles were released). In an astonishingly short time, with only these four musicians as models, House developed to a professional standard a blues style based on his religious singing and simple bottleneck guitar style.

Around 1927 or 1928, he had been playing in a juke joint when a man went on a shooting spree, wounding House in the leg, and he allegedly shot the man dead. House received a 15-year sentence at the Mississippi State Penitentiary (Parchman Farm), of which he served two years between 1928 and 1929. He credited his re-examination and release to an appeal by his family, but also spoke of the intervention by the influential white planter for whom they worked. The date of the killing and the duration of his sentence are unclear; House gave different accounts to different interviewers, and searches by his biographer Daniel Beaumont found no details in the court records of Coahoma County or in the archive of the Mississippi Department of Corrections.

Upon his release in 1929 or early 1930, House was strongly advised to leave Clarksdale and stay away. He walked to Jonestown and caught a train to the small town of Lula, Mississippi, sixteen miles north of Clarksdale and eight miles from the blues hub of Helena, Arkansas. Coincidentally, the great star of Delta blues, Charley Patton, was also in virtual exile in Lula, having been expelled from his base on the Dockery Plantation. With his partner Willie Brown, Patton dominated the local market for professional blues performance. Patton watched House busking when he arrived penniless at Lula station, but did not approach him. He observed House's showmanship attracting a crowd to the café and bootleg whiskey business of a woman called Sara Knight. Patton invited House to be a regular musical partner with him and Brown. House formed a liaison with Knight, and both musicians profited from association with her bootlegging activities. The musical partnership is disputed by Patton's biographers Stephen Calt and Gayle Dean Wardlow. They consider that House's musicianship was too limited to play with Patton and Brown, who were also rumoured to be estranged at the time. They also cite one statement by House that he did not play for dances in Lula. Beaumont concluded that House became a friend of Patton's, traveling with him to gigs but playing separately.

In 1930, Art Laibly of Paramount Records traveled to Lula to persuade Patton to record several more sides in Grafton, Wisconsin. Along with Patton came House, Brown, and the pianist Louise Johnson, all of whom recorded sides for the label. House recorded nine songs during that session, eight of which were released, but they were commercial failures. He did not record again commercially for 35 years, but he continued to play with Patton and Brown, and with Brown after Patton's death in 1934. During this time, House worked as a tractor driver for various plantations in the Lake Cormorant area.

Alan Lomax recorded House for the Library of Congress in 1941. Willie Brown, the mandolin player Fiddlin' Joe Martin, and the harmonica player Leroy Williams played with House on these recordings. Lomax returned to the area in 1942, where he recorded House once more.

House then faded from the public view, moving to Rochester, New York, in 1943, and working as a railroad porter for the New York Central Railroad and as a chef.

In 1964, after a long search of the Mississippi Delta region by Nick Perls, Dick Waterman and Phil Spiro, House was "rediscovered" in Rochester, New York. He had been retired from the music business for many years and was unaware of the 1960s folk blues revival and international enthusiasm for his early recordings.

He subsequently toured extensively in the United States and Europe and recorded for CBS Records. Like Mississippi John Hurt, he was welcomed into the music scene of the 1960s and played at the Newport Folk Festival in 1964, the New York Folk Festival in July 1965, and the October 1967 European tour of the American Folk Festival, along with Skip James and Bukka White.

The young guitarist Alan Wilson (later of Canned Heat) was a fan of House's. The producer John Hammond asked Wilson, who was just 22 years old, to teach "Son House how to play like Son House," because Wilson had such a good knowledge of blues styles. House subsequently recorded the album "Father of Folk Blues", later reissued as a 2-CD set "Father of Delta Blues: The Complete 1965 Sessions". House performed with Wilson live, as can be heard on "Levee Camp Moan" on the album "John the Revelator: The 1970 London Sessions".

House appeared in Seattle on Mar 19, 1968, arranged by the Seattle Folklore Society. The concert was recorded by Bob West and issued on Acola Records as a CD in 2006. The Arcola CD also included an interview of House recorded on November 15, 1969 in Seattle.

In the summer of 1970, House toured Europe once again, including an appearance at the Montreux Jazz Festival; a recording of his London concerts was released by Liberty Records. He also played at the two Days of Blues Festival in Toronto in 1974. On an appearance on the TV arts show "Camera Three", he was accompanied by the blues guitarist Buddy Guy.

Ill health plagued House in his later years, and in 1974 he retired once again. He later moved to Detroit, Michigan, where he remained until his death from cancer of the larynx. He had been married five times. He was buried at the Mt. Hazel Cemetery. Members of the Detroit Blues Society raised money through benefit concerts to put a monument on his grave.

In addition to his early influence on Robert Johnson and Muddy Waters, he was an inspiration to John Hammond, Alan Wilson (of Canned Heat), Bonnie Raitt, Jack White of the White Stripes, Dallas Green and John Mooney.

In 2017, his single, "Preachin' the Blues" was inducted in to the Blues Hall of Fame.

78-RPM recordings

Recorded May 28, 1930, in Grafton, Wisconsin, for Paramount Records


Recordings for Library of Congress and Fisk University

Recorded August 1941, at Klack's Store, Lake Cormorant, Mississippi.
There are some railway noises in the background on some titles, as the store (which had electricity necessary for the recording) was close to a branch line between Lake Cormorant and Robinsonville.

Recorded July 17, 1942, Robbinsonville, Mississippi

The music from both sessions and most of the recorded interviews have been reissued on LP and CD.

Singles

Other albums

This list is incomplete. For a complete list, see external links.



</doc>
<doc id="28822" url="https://en.wikipedia.org/wiki?curid=28822" title="Sex worker">
Sex worker

A sex worker is a person who is employed in the sex industry. The term is used in reference to all those in all areas of the sex industry, including those who provide direct sexual services as well as the staff and management of such industries. Some sex workers are paid to engage in sex acts or sexually explicit behavior which involves varying degrees of physical contact with clients (prostitutes and some but not all professional dominants); pornographic models and actors engage in sexually explicit behavior which is filmed or photographed. Phone sex operators have sexually-oriented conversations with clients, and may do verbal sexual roleplay.

Other sex workers are paid to engage in live sexual performance, such as webcam sex and performers in live sex shows. Some sex workers perform erotic dances and other acts for an audience. These include: striptease, go-go dancing, lap dancing, neo-burlesque, and peep shows. Sexual surrogates work with psychoanalysts to engage in sexual activity as part of therapy with their clients. Thus, although the term "sex worker" is sometimes viewed as a synonym or euphemism for "prostitute", it is more general. "Sex worker" can refer to individuals who do not directly engage in sexual activity such as pole dancers, sex toy testers, and strip club managers. Another example of sex workers that would not fall under the term "prostitute" would be an adult talent manager, who negotiates and secures pornographic roles for clients. There are also erotic photographers who shoot and edit for adult media and porn reviewers who watch and rate adult films.

Some people use the term "sex worker" to avoid invoking the stigma associated with the word "prostitute". Using the term "sex worker" rather than "prostitute" also allows more members of the sex industry to be represented and helps ensure that individuals who are actually prostitutes are not singled out and associated with the negative connotations of "prostitute." In addition, choosing to use the term "sex worker" rather than "prostitute" shows ownership over the individuals' career choice. Some argue that those who prefer the term "sex worker" wish to separate their occupation from their person. Describing someone as a sex worker recognizes that the individual may have many different facets, and are not necessarily defined by their job.

According to one view, sex work is different from sexual exploitation, or the forcing of a person to commit sexual acts, in that sex work is voluntary "and is seen as the commercial exchange of sex for money or goods". In an attempt to further clarify the broad term that "sex work" is, John E. Exner, an American psychologist, worked with his colleagues to create five distinct classes for categorizing sex workers. One scholarly article details the classes as follows: "specifically, the authors articulated Class I, or the upper class of the profession, consisting of call girls; Class II was referred to as the middle class, consisting of 'in-house girls' who typically work in an establishment on a commission basis; Class III, the lower middle class, were 'streetwalkers' whose fees and place of work fluctuate considerably; Class IV sex workers have been known as 'commuter housewives', and they are typically involved in sex work to supplement family income; and Class V consists of 'streetwalker addicts', or 'drugs-for-sex streetwalkers' who are considered the lower class of the profession."

The term "sex worker" was coined in 1978 by sex worker activist Carol Leigh. Its use became popularized after publication of the anthology, "Sex Work: Writings By Women In The Sex Industry" in 1987, edited by Frédérique Delacoste and Priscilla Alexander.</ref> The term "sex worker" has since spread into much wider use, including in academic publications, by NGOs and labor unions, and by governmental and intergovernmental agencies, such as the World Health Organization. The term is listed in the Oxford English Dictionary and Merriam-Webster's Dictionary.

The term is strongly opposed, however, by many who are morally opposed to the sex industry, such as social conservatives, anti-prostitution feminists, and other prohibitionists. Such groups view prostitution variously as a crime or as victimization, and see the term "sex work" as legitimizing criminal activity or exploitation as a type of labor.

Sex workers may be any gender and exchange sexual services or favors for money or other gifts. The motives of sex workers vary widely and can include debt, coercion, survival, or simply as a way to earn a living. Sexual empowerment is another possible reasons why people engage in sex work. One Canadian study found that a quarter of the sex workers interviewed started sex work because they found it "appealing". The flexibility to choose hours of work and ability to select their own client base may also contribute the appeal of sex work when compared to other service industry jobs. Sex work may also be a way to fund addiction. This line of work can be fueled by an individual's addiction to illegal substances before entering the industry or being introduced to these substances after entering the industry. These motives also align with varying climates surrounding sex work in different communities and cultures. In some cases, sex work is linked to tourism. Sex work can take the form of prostitution, stripping or lap dancing, performance in pornography, phone or internet sex, or any other exchange of sexual services for financial or material gain. The variety in the tasks encompassed by sex work lead to a large range in both severity and nature of risks that sex workers face in their occupations. Sex workers can act independently as individuals, work for a company or corporation, or work as part of a brothel. All of the above can be undertaken either by free choice or by coercion, or, as some argue, along a continuum between conflict and agency. Sex workers may also be hired to be companions on a trip or to perform sexual services within the context of a trip; either of these can be voluntary or forced labor. Transgender people are more likely than the general population to do sex work, particularly trans women and trans people of color. In a study of female Indian sex workers, illiteracy and lower social status were more prevalent than among the general female population.

Many studies struggle to gain demographic information about the prevalence of sex work, as many countries or cities have laws prohibiting prostitution or other sex work. In addition, sex trafficking, or forced sex work, is also difficult to quantify due to its underground and covert nature. In addition, finding a representative sample of sex workers in a given city can be nearly impossible because the size of the population itself is unknown. Maintaining privacy and confidentiality in research is also difficult because many sex workers may face prosecution and other consequences if their identities are revealed.

While demographic characteristics of sex workers vary by region and are hard to measure, some studies have attempted to estimate the composition of the sex work communities in various places. For example, one study of sex work in Tijuana, Mexico found that the majority of sex workers there are young, female and heterosexual. Many of these studies attempt to use smaller samples of sex workers and pimps in order to extrapolate about larger populations of sex workers. One report on the underground sex trade in the United States used known data on the illegal drug and weapon trades and interviews with sex workers and pimps in order to draw conclusions about the number of sex workers in eight American cities. However, studies like this one can come under scrutiny for a perceived emphasis on the activities and perspectives of pimps and other sex work managers rather than those of sex work providers themselves. Another criticism is that sex trafficking may not be adequately assessed in its relation to sex work in these studies.

Sex workers may be stereotyped as deviant, hypersexual, sexually risky, and substance abusive. Sex workers cope with this stigmatization, or othering, in ways such as hiding their occupation from non-sex workers, social withdrawal, and creating a false self to perform at work. Sex-work-related stigma perpetuates rape culture and leads to slut-shaming.

Globally, sex workers encounter barriers in accessing health care, legislation, legal resources, and labor rights. In a study of U.S sex workers, 43% of interview participants reported exposure to intimate-partner violence, physical violence, armed physical violence, and sexual violence in the forms of sexual coercion and rape. In this same study, a sex worker reported, "in this lifestyle nothing’s safe". Sex workers experience police abuse as well. Police use their authority to intimidate sex workers. Police officers have been reported to exploit street-based sex workers’ fear of incarceration to force them to have sex with the police without payment, sometimes still arresting them after the coerced sex. Police also compromise sex workers safety, often holding sex workers responsible for crimes acted against them because of the stigma attached to their occupation, also known as victim-blaming. The effects of whorephobia impacts sex workers’ agency, safety, and mental health. There is growth in advocacy organizations to reduce and erase prejudice and stigma against sex work, and to provide more support and resources for sex workers.

Depending on local law, sex workers' activities may be regulated, controlled, tolerated, or prohibited. In most countries, even those where sex work is legal, sex workers may be stigmatized and marginalized, which may prevent them from seeking legal redress for discrimination (e.g., racial discrimination by a strip club owner), non-payment by a client, assault or rape. Sex worker advocates have identified this as whorephobia.

The legality of different types of sex work varies within and between regions of the world. For example, while pornography is legal in the United States, prostitution is illegal in most parts of the US. However, in other regions of the world, both pornography and prostitution are illegal; in others, both are legal. One example of a country in which pornography, prostitution, and all professions encompassed under the umbrella of sex work are all legal is New Zealand. Under the Prostitution Reform Act of New Zealand, laws and regulations have been put into place in order to ensure the safety and protection of its sex workers. For example, since the implementation of the Prostitution Reform Act, "any person seeking to open a larger brothel, where more than four sex workers will be working requires a Brothel Operators Certificate, which certifies them as a suitable person to exercise control over sex workers in the workplace. [In addition,] sex workers operating in managed premises have access to labour rights and human rights protection and can pursue claims before the courts, like any other worker or employee." In regions where sex work is illegal, advocates for sex workers' rights argue that the covert nature of illegal prostitution is a barrier to access to legal resources. However, some who oppose the legalization of prostitution argue that sex work is inherently exploitative and can never be legalized or practiced in a way that respects the rights of those who perform it.

There are many arguments against legalizing prostitution/sex work. In one study, women involved in sex work were interviewed and asked if they thought it should be made legal. They answered that they thought it should not, as it would put women at higher risk from violent customers if it were considered legitimate work, and they would not want their friends or family entering the sex industry to earn money. Another argument is that legalizing sex work would increase the demand for it, and women should not be treated as sexual merchandise. A study showed that in countries that have legalized prostitution, there was an increase in child prostitution. An argument against legalizing sex work is to keep children from being involved in this industry. The studies also showed that legalizing sex work lead to an increase in sex trafficking, which is another reason people give for making sex work illegal.

There are also arguments for legalizing prostitution/sex work. One major argument for legalizing prostitution is that women should have a right to do what they want with their own bodies. The government should not have a say in what they do for work, and if they want to sell their bodies it is their own decision. Another common argument for legalizing prostitution is that enforcing prostitution laws is a waste of money. This is because prostitution has always, and will continue to persist despite whatever laws and regulations are implemented against it. In arguing for the decriminalization of sex work, the Minister of Justice of the Netherlands expanded upon this argument in court when stating that, "prostitution has existed for a long time and will continue to do so…Prohibition is not the way to proceed…One should allow for voluntary prostitution. The authorities can then regulate prostitution, [and] it can become healthy, safe, transparent, and cleansed from criminal side-effects." People who wish to legalize prostitution do not see enforcing laws against sex work as effective and think the money is better spent elsewhere. Many people also argue that legalization of prostitution will lead to less harm for the sex workers. They argue that the decriminalization of sex work will decrease the exploitation of sex workers by third parties such as pimps and managers. A final argument for the legalization of sex work is that prostitution laws are unconstitutional. Some argue that these laws go against people's rights to free speech, privacy, etc.

Risk reduction in sex work is a highly debated topic. "Abolitionism" and "nonabolitionism" or "empowerment" are regarded as opposing ways in which risk reduction is approached. While abolitionism would call for an end to all sex work, empowerment would encourage the formation of networks among sex workers and enable them to prevent STIs and other health risks by communicating with each other. Both approaches aim to reduce rates of disease and other negative effects of sex work.

In addition, sex workers themselves have disputed the dichotomous nature of abolitionism and nonabolitionism, advocating instead a focus on sex workers' rights. In 1999, the Network of Sex Worker Projects claimed that "Historically, anti-trafficking measures have been more concerned with protecting 'innocent' women from becoming prostitutes than with ensuring the human rights of those in the sex industry. Penelope Saunders, a sex workers' rights advocate, claims that the sex workers' rights approach considers more of the historical context of sex work than either abolitionism or empowerment. In addition, Jo Doezema has written that the dichotomy of the voluntary and forced approaches to sex work has served to deny sex workers agency.

Sex workers are unlikely to disclose their work to healthcare providers. This can be due to embarrassment, fear of disapproval, or a disbelief that sex work can have effects on their health. The criminalization of sex work in many places can also lead to a reluctance to disclose for fear of being turned in for illegal activities. There are very few legal protections for sex workers due to criminalization; thus, in many cases, a sex worker reporting violence to a healthcare provider may not be able to take legal action against their aggressor.

Health risks of sex work relate primarily to sexually transmitted infections and to drug use. In one study, nearly 40% of sex workers who visited a health center reported illegal drug use. In general, transgender women sex workers have a higher risk of contracting HIV than male and female sex workers and transgender women who are not sex workers.

The reason transgender women are at higher risk for developing HIV is their combination of risk factors. They face biological, personal, relational, and structural risks that all increase their chances of getting HIV. Biological factors include incorrect condom usage because of erectile dysfunction from hormones taken to become more feminine and receptive anal intercourse without a condom which is a high risk for developing HIV. Personal factors include mental health issues that lead to increased sexual risk, such as anxiety, depression, and substance abuse provoked through lack of support, violence, etc. Structural risks include involvement in sex work being linked to poverty, substance abuse, and other factors that are more prevalent in transgender women based on their tendency to be socially marginalized and not accepted for challenging gender norms. The largest risk for HIV is unprotected sex with male partners, and studies have been emerging that show men who have sex with transgender women are more likely to use drugs than men that do not.

Condom use is one way to mitigate the risk of contracting an STI. However, negotiating condom use with one's clients and partners is often an obstacle to practicing safer sex. While there is not much data on rates of violence against sex workers, many sex workers do not use condoms due to the fear of resistance and violence from clients. Some countries also have laws prohibiting condom possession; this reduces the likelihood that sex workers will use condoms. Increased organization and networking among sex workers has been shown to increase condom use by increasing access to and education about STI prevention. Brothels with strong workplace health practices, including the availability of condoms, have also increased condom use among their workers.

Health Concerns of Exotic Dancers<br>"Mental Health and Stigma"<br>In order to protect themselves from the stigma of sex work, many dancers resort to othering themselves. Othering involves constructing oneself as superior to one's peers, and the dancer persona provides an internal boundary that separates the "authentic" from the stripper self. This practice creates a lot of stress for the dancers, in turn leading many to resort to using drugs and alcohol to cope. Since it is so widespread, the use of drugs has become normalized in the exotic dance scene.<br>Despite this normalization, passing as nonusers, or covering as users of less maligned drugs, is necessary. This is because strippers concurrently attribute a strong moral constitution to those that resist the drug atmosphere; it is a testament to personal strength and will power. It is also an occasion for dancers to "other" fellow strippers. Valorizing resistance to the drug space discursively positions "good" strippers against such a drug locale and indicates why dancers are motivated to closet hard drug use.<br>Stigma causes strippers to hide their lifestyles from friends and family alienating themselves from a support system. Further, the stress of trying to hide their lifestyles from others due to fear of scrutiny affects the mental health of dancers. Stigma is a difficult area to address because it is more abstract, but it would be helpful to work toward normalizing sex work as a valid way of making a living. This normalization of sex work would relieve the stress many dancers experience increasing the likelihood that they will be open about their work. Being open will allow them access to a viable support system and reduce the othering and drug use so rampant in the sex industry.

Forced sex work is when an individual enters into any sex trade due to coercion rather than by choice. Forced sex work increases the likelihood that a sex worker will contract HIV/AIDS or another sexually transmitted infection, particularly when an individual enters sex work before the age of 18. In addition, even when sex workers do consent to certain sex acts, they are often forced or coerced into others (often anal intercourse) by clients. Sex workers may also experience strong resistance to condom use by their clients, which may extend into a lack of consent by the worker to any sexual act performed in the encounter; this risk is magnified when sex workers are trafficked or forced into sex work.

Forced sex work often involves deception - workers are told that they can make a living and are then not allowed to leave. This deception can cause ill effects on the mental health of many sex workers. In addition, an assessment of studies estimates that between 40% and 70% of sex workers face violence within a year. Currently, there is little support for migrant workers in many countries, including those who have been trafficked to a location for sex.

Sex worker's rights advocates argue that sex workers should have the same basic human and labor rights as other working people. For example, the Canadian Guild for Erotic Labour calls for the legalization of sex work, the elimination of state regulations that are more repressive than those imposed on other workers and businesses, the right to recognition and protection under labour and employment laws, the right to form and join professional associations or unions, and the right to legally cross borders to work. Advocates also want to see changes in legal practices involving sex work, the Red Umbrella Project has pushed for the decriminalization of condoms and changes to New York's sex workers diversion program. Advocacy for the interests of sex workers can come from a variety of sources, including non-governmental organizations, labor rights organizations, governments, or sex workers themselves. Each year in London The Sexual Freedom Awards is held to honor the most notable advocates and pioneers of sexual freedom and sex workers' rights in the UK, where sex work is essentially legal.

The unionization of sex workers is a recent development. The first organization within the contemporary sex workers' rights movement was Call Off Your Old Tired Ethics (COYOTE), founded in 1973 in San Francisco, California. Many organizations in Western countries were established in the decade after the founding of COYOTE. Currently, a small number of sex worker unions exist worldwide. One of the largest is the International Union of Sex Workers, headquartered in the United Kingdom. The IUSW advocates for the rights of all sex workers, whether they chose freely or were coerced to enter the trade, and promotes policies that benefit the interests of sex workers both in the UK and abroad. Many regions are home to sex worker unions, including Latin America, Brazil, Canada, Europe, and Africa.

In unionizing, many sex workers face issues relating to communication and to the legality of sex work. Because sex work is illegal in many places where they wish to organize, it is difficult to communicate with other sex workers in order to organize. There is also concern with the legitimacy of sex work as a career and an activity that merits formal organizing, largely because of the sexism often present in sex work and the devaluation of sex work as not comparable to other paid labor and employment.

A factor affecting the unionization of sex work is that many sex workers belong to populations that historically have not had a strong representation in labor unions. While this unionization can be viewed as a way of empowering sex workers and granting them agency within their profession, it is also criticized as implicitly lending its approval to sexism and power imbalances already present in sex work. Unionization also implies a submission to or operation within the systems of capitalism, which is of concern to some feminists.

"Independent contractor vs Employee"<br>Performers in general are problematic to categorize because they often exercise a high level of control over their work product, one characteristic of an independent contractor. Additionally, their work can be artistic in nature and often done on a freelance basis. Often, the work of performers does not possess the obvious attributes of employees such as regular working hours, places or duties. Consequently, employers misclassify them because they are unsure of their workers' status, or they purposely misclassify them to take advantage of independent contractors' low costs. Exotic dance clubs are one such employer that purposely misclassify their performers as independent contractors.

There are additional hurdles in terms of self-esteem and commitment to unionize. On the most basic level, dancers themselves must have the desire to unionize for collective action. For those who wish not to conform to group activity or want to remain independent, a union may seem as controlling as club management since joining a union would obligate them to pay dues and abide by decisions made through majority vote, with or without their personal approval.

In the "Lusty Lady" case study, this strip club was the first all-woman-managed club to successfully unionize in 1996. Some of the working conditions they were able to address included "protest[ing] racist hiring practices, customers being allowed to videotape dancers without their consent via one-way mirrors, inconsistent disciplinary policies, lack of health benefits, and an overall dearth of job security". Unionizing exotic dancers can certainly bring better work conditions and fair pay, but it is difficult to do at times because of their dubious employee categorization. Also, as is the case with many other unions, dancers are often reluctant to join them. This reluctance can be due to many factors, ranging from the cost of joining a union to the dancers believing they do not need union support because they will not be exotic dancers for a long enough period of time to justify joining a union.

NGOs often play a large role in outreach to sex workers, particularly in HIV and STI prevention efforts. However, NGO outreach to sex workers for HIV prevention is sometimes less coordinated and organized than similar HIV prevention programs targeted at different groups (such as men who have sex with men). This lack of organization may be due to the legal status of prostitution and other sex work in the country in question; in China, many sex work and drug abuse NGOs do not formally register with the government and thus run many of their programs on a small scale and discreetly.

While some NGOs have increased their programming to improve conditions within the context of sex work, these programs are criticized at times due to their failure to dismantle the oppressive structures of prostitution, particularly forced trafficking. Some scholars believe that advocating for rights within the institution of prostitution is not enough; rather, programs that seek to empower sex workers must empower them to leave sex work as well as improve their rights within the context of sex work.











</doc>
<doc id="28824" url="https://en.wikipedia.org/wiki?curid=28824" title="Stéphane Mallarmé">
Stéphane Mallarmé

Stéphane Mallarmé ( , ; 18 March 1842 – 9 September 1898), pen name of Étienne Mallarmé, was a French poet and critic. He was a major French symbolist poet, and his work anticipated and inspired several revolutionary artistic schools of the early 20th century, such as Cubism, Futurism, Dadaism, and Surrealism.

Stéphane Mallarmé was born in Paris. He was a boarder at the "Pensionnat des Frères des écoles chrétiennes à Passy" between 6 or 9 October 1852 and March 1855. He worked as an English teacher and spent much of his life in relative poverty but was famed for his "salons", occasional gatherings of intellectuals at his house on the rue de Rome for discussions of poetry, art and philosophy. The group became known as "les Mardistes," because they met on Tuesdays (in French, "mardi"), and through it Mallarmé exerted considerable influence on the work of a generation of writers. For many years, those sessions, where Mallarmé held court as judge, jester, and king, were considered the heart of Paris intellectual life. Regular visitors included W.B. Yeats, Rainer Maria Rilke, Paul Valéry, Stefan George, Paul Verlaine, and many others. 

Along with other members of "La Revue Blanche" such as Jules Renard, Julien Benda and Ioannis Psycharis, Mallarmé was a Dreyfusard.

On 10 August 1863, he married Maria Christina Gerhard. Their daughter, (Stéphanie Françoise) Geneviève Mallarmé, was born on 19 November 1864. Mallarmé died in Valvins (present-day Vulaines-sur-Seine) September 9, 1898.

Mallarmé's earlier work owes a great deal to the style of Charles Baudelaire who was recognised as the forerunner of literary Symbolism. Mallarmé's later "fin de siècle" style, on the other hand, anticipates many of the fusions between poetry and the other arts that were to blossom in the next century. Most of this later work explored the relationship between content and form, between the text and the arrangement of words and spaces on the page. This is particularly evident in his last major poem, "Un coup de dés jamais n'abolira le hasard" ('A roll of the dice will never abolish chance') of 1897.

Some consider Mallarmé one of the French poets most difficult to translate into English. The difficulty is due in part to the complex, multilayered nature of much of his work, but also to the important role that the sound of the words, rather than their meaning, plays in his poetry. When recited in French, his poems allow alternative meanings which are not evident on reading the work on the page. For example, Mallarmé's "Sonnet en '-yx"' opens with the phrase "ses purs ongles" ('her pure nails'), whose first syllables when spoken aloud sound very similar to the words "c'est pur son" ('it's pure sound'). Indeed, the 'pure sound' aspect of his poetry has been the subject of musical analysis and has inspired musical compositions. These phonetic ambiguities are very difficult to reproduce in a translation which must be faithful to the meaning of the words.

Mallarmé's poetry has been the inspiration for several musical pieces, notably Claude Debussy's "Prélude à l'après-midi d'un faune" (1894), a free interpretation of Mallarmé's poem "L'après-midi d'un faune" (1876), which creates powerful impressions by the use of striking but isolated phrases. Maurice Ravel set Mallarmé's poetry to music in "Trois poèmes de Mallarmé" (1913). Other composers to use his poetry in song include Darius Milhaud ("Chansons bas de Stéphane Mallarmé", 1917) and Pierre Boulez ("Pli selon pli", 1957–62).

Man Ray's last film, entitled "Les Mystères du Château de Dé (The Mystery of the Chateau of Dice)" (1929), was greatly influenced by Mallarmé's work, prominently featuring the line "A roll of the dice will never abolish chance".

Mallarmé is referred to extensively in the latter section of Joris-Karl Huysmans' "À rebours", where Des Esseintes describes his fervour-infused enthusiasm for the poet: "These were Mallarmé's masterpieces and also ranked among the masterpieces of prose poetry, for they combined a style so magnificently that in itself it was as soothing as a melancholy incantation, an intoxicating melody, with irresistibly suggestive thoughts, the soul-throbs of a sensitive artist whose quivering nerves vibrate with an intensity that fills you with a painful ecstasy." [p. 198, Robert Baldick translation]

The critic and translator Barbara Johnson has emphasized Mallarmé's influence on twentieth-century French criticism and theory: "It was largely by learning the lesson of Mallarmé that critics like Roland Barthes came to speak of 'the death of the author' in the making of literature. Rather than seeing the text as the emanation of an individual author's intentions, structuralists and deconstructors followed the paths and patterns of the linguistic signifier, paying new attention to syntax, spacing, intertextuality, sound, semantics, etymology, and even individual letters. The theoretical styles of Jacques Derrida, Julia Kristeva, Maurice Blanchot, and especially Jacques Lacan also owe a great deal to Mallarmé's 'critical poem.'"

It has been suggested that "much of Mallarmé's work influenced the conception of hypertext, with his purposeful use of blank space and careful placement of words on the page, allowing multiple non-linear readings of the text. This becomes very apparent in his work "Un coup de dés"."

On the publishing of "Un Coup de Dés" and its mishaps after the death of Mallarmé, consult the notes and commentary of for his edition of the complete works of Mallarmé, Volume 1, Bibliothèque de la Pléiade, Gallimard 1998. To delve more deeply, consult "Igitur, Divagations, Un Coup de Dés," edited by Bertrand Marchal with a preface by Yves Bonnefoy, nfr Poésie/Gallimard.

In 1990, Greenhouse Review Press published D. J. Waldie's American translation of "Un Coup de Dés" in a letterpress edition of 60 copies, its typography and format based on examination of the final (or near final) corrected proofs of the poem in the collection of Harvard's Houghton Library.

Prior to 2004, "Un Coup de Dés" was never published in the typography and format conceived by Mallarmé. In 2004, 90 copies on vellum of a new edition were published by Michel Pierson et Ptyx. This edition reconstructs the typography originally designed by Mallarmé for the projected Vollard edition in 1897 and which was abandoned after the sudden death of the author in 1898. All the pages are printed in the format (38 cm by 28 cm) and in the typography chosen by the author. The reconstruction has been made from the proofs which are kept in the Bibliothèque Nationale de France, taking into account the written corrections and wishes of Mallarmé and correcting certain errors on the part of the printers Firmin-Didot.

A copy of this new edition can be consulted in the Bibliothèque François-Mitterrand. Copies have been acquired by the Bibliothèque littéraire Jacques-Doucet and University of California - Irvine, as well as by private collectors. A copy has been placed in the Museum Stéphane Mallarmé at Vulaines-sur-Seine, Valvins, where Mallarmé lived and died and where, according to Paul Valéry, he made his final corrections on the proofs prior to the projected printing of the poem.

The poet and visual artist Marcel Broodthaers created a purely graphical version of "Un coup de Dés", using Mallarmé's typographical layout but with the words replaced by black bars.

In 2012, the French philosopher Quentin Meillassoux published "The Number and the Siren", a rigorous attempt at 'deciphering' the poem on the basis of a unique interpretation of the phrase 'the unique Number, which cannot be another.'

In 2015, Wave Books published "A Roll of the Dice Will Never Abolish Chance", a definitive dual-language edition of the poem, translated by Robert Bononno and Jeff Clark (designer). Readers may also consider Henry Weinfield's translation (in dual-language edition) to merit consideration as "definitive"—or, indeed, each generation will find its own definitive translation.

In 2018, Apple Pie Editions published "un coup de des jamais n'abolira le hasard: translations" by Eric Zboya, an English edition that transforms the poem not only through erasure, but through graphic imaging software.







</doc>
<doc id="28825" url="https://en.wikipedia.org/wiki?curid=28825" title="Submarine">
Submarine

A submarine (or sub) is a watercraft capable of independent operation underwater. It differs from a submersible, which has more limited underwater capability. It is also sometimes used historically or colloquially to refer to remotely operated vehicles and robots, as well as medium-sized or smaller vessels, such as the midget submarine and the wet sub. Submarines are referred to as "boats" rather than "ships" irrespective of their size.

Although experimental submarines had been built before, submarine design took off during the 19th century, and they were adopted by several navies. Submarines were first widely used during World War I (1914–1918), and are now used in many navies large and small. Military uses include attacking enemy surface ships (merchant and military), or other submarines, aircraft carrier protection, blockade running, ballistic missile submarines as part of a nuclear strike force, reconnaissance, conventional land attack (for example using a cruise missile), and covert insertion of special forces. Civilian uses for submarines include marine science, salvage, exploration and facility inspection and maintenance. Submarines can also be modified to perform more specialized functions such as search-and-rescue missions or undersea cable repair. Submarines are also used in tourism and undersea archaeology.

Most large submarines consist of a cylindrical body with hemispherical (or conical) ends and a vertical structure, usually located amidships, which houses communications and sensing devices as well as periscopes. In modern submarines, this structure is the "sail" in American usage and "fin" in European usage. A "conning tower" was a feature of earlier designs: a separate pressure hull above the main body of the boat that allowed the use of shorter periscopes. There is a propeller (or pump jet) at the rear, and various hydrodynamic control fins. Smaller, deep-diving and specialty submarines may deviate significantly from this traditional layout. Submarines use diving planes and also change the amount of water and air in ballast tanks to change buoyancy for submerging and surfacing.

Submarines have one of the widest ranges of types and capabilities of any vessel. They range from small autonomous examples and one- or two-person subs that operate for a few hours to vessels that can remain submerged for six months—such as the Russian , the biggest submarines ever built. Submarines can work at greater depths than are survivable or practical for human divers. Modern deep-diving submarines derive from the bathyscaphe, which in turn evolved from the diving bell.

Whereas the principal meaning of "submarine" is an armed, submersible warship, the more general meaning is for any type of submersible craft. The definition as of 1899 was for any type of "submarine boat". By naval tradition, submarines are still usually referred to as "boats" rather than as "ships", regardless of their size. In other navies with a history of large submarine fleets they are also "boats"; in German it is an or (under-sea boat) and in Russian it is a (underwater boat). Although referred to informally as "boats", U.S. submarines employ the designation USS (United States Ship) at the beginning of their names, such as .

According to a report in "Opusculum Taisnieri" published in 1562:
In 1578, the English mathematician William Bourne recorded in his book "Inventions or Devises" one of the first plans for an underwater navigation vehicle. A few years later the Scottish mathematician and theologian John Napier wrote in his "Secret Inventions" (1596) that "These inventions besides devises of sayling under water with divers, other devises and strategems for harming of the enemyes by the Grace of God and worke of expert Craftsmen I hope to perform." It's unclear whether he ever carried out his idea.

The first submersible of whose construction there exists reliable information was designed and built in 1620 by Cornelis Drebbel, a Dutchman in the service of James I of England. It was propelled by means of oars.

By the mid-18th century, over a dozen patents for submarines/submersible boats had been granted in England. In 1747, Nathaniel Symons patented and built the first known working example of the use of a ballast tank for submersion. His design used leather bags that could fill with water to submerge the craft. A mechanism was used to twist the water out of the bags and cause the boat to resurface. In 1749, the Gentlemen's Magazine reported that a similar design had initially been proposed by Giovanni Borelli in 1680. Further design improvement stagnated for over a century, until application of new technologies for propulsion and stability.

The first military submarine was (1775), a hand-powered acorn-shaped device designed by the American David Bushnell to accommodate a single person. It was the first verified submarine capable of independent underwater operation and movement, and the first to use screws for propulsion.

In 1800, France built a human-powered submarine designed by American Robert Fulton, . The French eventually gave up on the experiment in 1804, as did the British when they later considered Fulton's submarine design.

In 1864, late in the American Civil War, the Confederate navy's became the first military submarine to sink an enemy vessel, the Union sloop-of-war . In the aftermath of its successful attack against the ship, "H. L. Hunley" also sank, possibly because it was too close to its own exploding torpedo.

In 1866, was the first submarine to successfully dive, cruise underwater, and resurface under the control of the crew. The design by German American Julius H. Kroehl (in German, "Kröhl") incorporated elements that are still used in modern submarines.

In 1866, was built at the request of the Chilean government, by Karl Flach, a German engineer and immigrant. It was the fifth submarine built in the world and, along with a second submarine, was intended to defend the port of Valparaiso against attack by the Spanish Navy during the Chincha Islands War.

The first submarine not relying on human power for propulsion was the French ("Diver"), launched in 1863, which used compressed air at . Narcís Monturiol designed the first air-independent and combustion-powered submarine, , which was launched in Barcelona, Spain in 1864.

The submarine became a potentially viable weapon with the development of the Whitehead torpedo, designed in 1866 by British engineer Robert Whitehead, the first practical self-propelled or 'locomotive' torpedo. The spar torpedo that had been developed earlier by the Confederate States Navy was considered to be impracticable, as it was believed to have sunk both its intended target, and probably "H. L. Hunley", the submarine that deployed it.

Discussions between the English clergyman and inventor George Garrett and the Swedish industrialist Thorsten Nordenfelt led to the first practical steam-powered submarines, armed with torpedoes and ready for military use. The first was "Nordenfelt I", a 56-tonne, vessel similar to Garrett's ill-fated (1879), with a range of , armed with a single torpedo, in 1885.

A reliable means of propulsion for the submerged vessel was only made possible in the 1880s with the advent of the necessary electric battery technology. The first electrically powered boats were built by Isaac Peral y Caballero in Spain (who built ), Dupuy de Lôme (who built ) and Gustave Zédé (who built "Sirène") in France, and James Franklin Waddington (who built "Porpoise") in England. Peral's design featured torpedoes and other systems that later became standard in submarines.

Submarines were not put into service for any widespread or routine use by navies until the early 1900s. This era marked a pivotal time in submarine development, and several important technologies appeared. A number of nations built and used submarines. Diesel electric propulsion became the dominant power system and equipment such as the periscope became standardized. Countries conducted many experiments on effective tactics and weapons for submarines, which led to their large impact in World War I.

The Irish inventor John Philip Holland built a model submarine in 1876 and a full-scale version in 1878, which were followed by a number of unsuccessful ones. In 1896 he designed the Holland Type VI submarine, which used internal combustion engine power on the surface and electric battery power underwater. Launched on 17 May 1897 at Navy Lt. Lewis Nixon's Crescent Shipyard in Elizabeth, New Jersey, "Holland VI" was purchased by the United States Navy on 11 April 1900, becoming the Navy's first commissioned submarine, christened .

Commissioned in June 1900, the French steam and electric employed the now typical double-hull design, with a pressure hull inside the outer shell. These 200-ton ships had a range of over underwater. The French submarine "Aigrette" in 1904 further improved the concept by using a diesel rather than a gasoline engine for surface power. Large numbers of these submarines were built, with seventy-six completed before 1914.

The Royal Navy commissioned five s from Vickers, Barrow-in-Furness, under licence from the Holland Torpedo Boat Company from 1901 to 1903. Construction of the boats took longer than anticipated, with the first only ready for a diving trial at sea on 6 April 1902. Although the design had been purchased entirely from the US company, the actual design used was an untested improvement to the original Holland design using a new petrol engine.

These types of submarines were first used during the Russo-Japanese War of 1904–05. Due to the blockade at Port Arthur, the Russians sent their submarines to Vladivostok, where by 1 January 1905 there were seven boats, enough to create the world's first "operational submarine fleet". The new submarine fleet began patrols on 14 February, usually lasting for about 24 hours each. The first confrontation with Japanese warships occurred on 29 April 1905 when the Russian submarine "Som" was fired upon by Japanese torpedo boats, but then withdrew.

Military submarines first made a significant impact in World War I. Forces such as the U-boats of Germany saw action in the First Battle of the Atlantic, and were responsible for sinking , which was sunk as a result of unrestricted submarine warfare and is often cited among the reasons for the entry of the United States into the war.

At the outbreak of the war, Germany had only twenty submarines immediately available for combat, although these included vessels of the diesel-engined "U-19" class, which had a sufficient range of and speed of to allow them to operate effectively around the entire British coast. By contrast the Royal Navy had a total of 74 submarines, though of mixed effectiveness. In August 1914, a flotilla of ten U-boats sailed from their base in Heligoland to attack Royal Navy warships in the North Sea in the first submarine war patrol in history.

The U-boats' ability to function as practical war machines relied on new tactics, their numbers, and submarine technologies such as combination diesel-electric power system developed in the preceding years. More submersibles than true submarines, U-boats operated primarily on the surface using regular engines, submerging occasionally to attack under battery power. They were roughly triangular in cross-section, with a distinct keel to control rolling while surfaced, and a distinct bow. During World War I more than 5,000 Allied ships were sunk by U-boats.

The British tried to catch up to the Germans in terms of submarine technology with the creation of the K-class submarines. However, these were extremely large and often collided with each other forcing the British to scrap the K-class design shortly after the war.

During World War II, Germany used submarines to devastating effect in the Battle of the Atlantic, where it attempted to cut Britain's supply routes by sinking more merchant ships than Britain could replace. (Shipping was vital to supply Britain's population with food, industry with raw material, and armed forces with fuel and armaments.) While U-boats destroyed a significant number of ships, the strategy ultimately failed. Although the U-boats had been updated in the interwar years, the major innovation was improved communications, encrypted using the famous Enigma cipher machine. This allowed for mass-attack naval tactics ("Rudeltaktik", commonly known as "wolfpack"), but was also ultimately the U-boats' downfall. By the end of the war, almost 3,000 Allied ships (175 warships, 2,825 merchantmen) had been sunk by U-boats. Although successful early in the war, ultimately Germany's U-boat fleet suffered heavy casualties, losing 793 U-boats and about 28,000 submariners out of 41,000, a casualty rate of about 70%.

The Imperial Japanese Navy operated the most varied fleet of submarines of any navy, including "Kaiten" crewed torpedoes, midget submarines ( and es), medium-range submarines, purpose-built supply submarines and long-range fleet submarines. They also had submarines with the highest submerged speeds during World War II (s) and submarines that could carry multiple aircraft (s). They were also equipped with one of the most advanced torpedoes of the conflict, the oxygen-propelled Type 95. Nevertheless, despite their technical prowess, Japan chose to use its submarines for fleet warfare, and consequently were relatively unsuccessful, as warships were fast, maneuverable and well-defended compared to merchant ships.

The submarine force was the most effective anti-ship weapon in the American arsenal. Submarines, though only about 2 percent of the U.S. Navy, destroyed over 30 percent of the Japanese Navy, including 8 aircraft carriers, 1 battleship and 11 cruisers. US submarines also destroyed over 60 percent of the Japanese merchant fleet, crippling Japan's ability to supply its military forces and industrial war effort. Allied submarines in the Pacific War destroyed more Japanese shipping than all other weapons combined. This feat was considerably aided by the Imperial Japanese Navy's failure to provide adequate escort forces for the nation's merchant fleet.

During World War II, 314 submarines served in the US Navy, of which nearly 260 were deployed to the Pacific. When the Japanese attacked Hawaii in December 1941, 111 boats were in commission; 203 submarines from the , , and es were commissioned during the war. During the war, 52 US submarines were lost to all causes, with 48 directly due to hostilities. US submarines sank 1,560 enemy vessels, a total tonnage of 5.3 million tons (55% of the total sunk).

The Royal Navy Submarine Service was used primarily in the classic Axis blockade. Its major operating areas were around Norway, in the Mediterranean (against the Axis supply routes to North Africa), and in the Far East. In that war, British submarines sank 2 million tons of enemy shipping and 57 major warships, the latter including 35 submarines. Among these is the only documented instance of a submarine sinking another submarine while both were submerged. This occurred when engaged ; the "Venturer" crew manually computed a successful firing solution against a three-dimensionally maneuvering target using techniques which became the basis of modern torpedo computer targeting systems. Seventy-four British submarines were lost, the majority, forty-two, in the Mediterranean.

The first launch of a cruise missile (SSM-N-8 Regulus) from a submarine occurred in July 1953, from the deck of , a World War II fleet boat modified to carry the missile with a nuclear warhead. "Tunny" and its sister boat, , were the United States' first nuclear deterrent patrol submarines. In the 1950s, nuclear power partially replaced diesel-electric propulsion. Equipment was also developed to extract oxygen from sea water. These two innovations gave submarines the ability to remain submerged for weeks or months. Most of the naval submarines built since that time in the US, the Soviet Union/Russian Federation, Britain, and France have been powered by nuclear reactors.

In 1959–1960, the first ballistic missile submarines were put into service by both the United States () and the Soviet Union () as part of the Cold War nuclear deterrent strategy.

During the Cold War, the US and the Soviet Union maintained large submarine fleets that engaged in cat-and-mouse games. The Soviet Union lost at least four submarines during this period: was lost in 1968 (a part of which the CIA retrieved from the ocean floor with the Howard Hughes-designed ship "Glomar Explorer"), in 1970, in 1986, and in 1989 (which held a depth record among military submarines—). Many other Soviet subs, such as (the first Soviet nuclear submarine, and the first Soviet sub to reach the North Pole) were badly damaged by fire or radiation leaks. The US lost two nuclear submarines during this time: due to equipment failure during a test dive while at its operational limit, and due to unknown causes.

During India's intervention in the Bangladesh Liberation War, the Pakistan Navy's sank the Indian frigate . This was the first sinking by a submarine since World War II. During the same war, , a "Tench"-class submarine on loan to Pakistan from the US, was sunk by the Indian Navy. It was the first submarine combat loss since World War II. In 1982 during the Falklands War, the Argentine cruiser was sunk by the British submarine , the first sinking by a nuclear-powered submarine in war.

Before and during World War II, the primary role of the submarine was anti-surface ship warfare. Submarines would attack either on the surface, using deck guns or submerged, using torpedoes. They were particularly effective in sinking Allied transatlantic shipping in both World Wars, and in disrupting Japanese supply routes and naval operations in the Pacific in World War II.

Mine-laying submarines were developed in the early part of the 20th century. The facility was used in both World Wars. Submarines were also used for inserting and removing covert agents and military forces in special operations, for intelligence gathering, and to rescue aircrew during air attacks on islands, where the airmen would be told of safe places to crash-land so the submarines could rescue them. Submarines could carry cargo through hostile waters or act as supply vessels for other submarines.

Submarines could usually locate and attack other submarines only on the surface, although managed to sink with a four torpedo spread while both were submerged. The British developed a specialized anti-submarine submarine in WWI, the R class. After WWII, with the development of the homing torpedo, better sonar systems, and nuclear propulsion, submarines also became able to hunt each other effectively.

The development of submarine-launched ballistic missile and submarine-launched cruise missiles gave submarines a substantial and long-ranged ability to attack both land and sea targets with a variety of weapons ranging from cluster bombs to nuclear weapons.

The primary defense of a submarine lies in its ability to remain concealed in the depths of the ocean. Early submarines could be detected by the sound they made. Water is an excellent conductor of sound (much better than air), and submarines can detect and track comparatively noisy surface ships from long distances. Modern submarines are built with an emphasis on stealth. Advanced propeller designs, extensive sound-reducing insulation, and special machinery help a submarine remain as quiet as ambient ocean noise, making them difficult to detect. It takes specialized technology to find and attack modern submarines.

Active sonar uses the reflection of sound emitted from the search equipment to detect submarines. It has been used since WWII by surface ships, submarines and aircraft (via dropped buoys and helicopter "dipping" arrays), but it reveals the emitter's position, and is susceptible to counter-measures.

A concealed military submarine is a real threat, and because of its stealth, can force an enemy navy to waste resources searching large areas of ocean and protecting ships against attack. This advantage was vividly demonstrated in the 1982 Falklands War when the British nuclear-powered submarine sank the Argentine cruiser . After the sinking the Argentine Navy recognized that they had no effective defense against submarine attack, and the Argentine surface fleet withdrew to port for the remainder of the war, though an Argentine submarine remained at sea.

Although the majority of the world's submarines are military, there are some civilian submarines, which are used for tourism, exploration, oil and gas platform inspections, and pipeline surveys. Some are also used in illegal activities.

The Submarine Voyage ride opened at Disneyland in 1959, but although it ran under water it was not a true submarine, as it ran on tracks and was open to the atmosphere. The first tourist submarine was , which went into service in 1964 at Expo64. By 1997 there were 45 tourist submarines operating around the world. Submarines with a crush depth in the range of are operated in several areas worldwide, typically with bottom depths around , with a carrying capacity of 50 to 100 passengers.

In a typical operation a surface vessel carries passengers to an offshore operating area and loads them into the submarine. The submarine then visits underwater points of interest such as natural or artificial reef structures. To surface safely without danger of collision the location of the submarine is marked with an air release and movement to the surface is coordinated by an observer in a support craft.

A recent development is the deployment of so-called narco submarines by South American drug smugglers to evade law enforcement detection. Although they occasionally deploy true submarines, most are self-propelled semi-submersibles, where a portion of the craft remains above water at all times. In September 2011, Colombian authorities seized a 16-meter-long submersible that could hold a crew of 5, costing about $2 million. The vessel belonged to FARC rebels and had the capacity to carry at least 7 tonnes of drugs.


All surface ships, as well as surfaced submarines, are in a positively buoyant condition, weighing less than the volume of water they would displace if fully submerged. To submerge hydrostatically, a ship must have negative buoyancy, either by increasing its own weight or decreasing its displacement of water. To control their displacement, submarines have ballast tanks, which can hold varying amounts of water and air.

For general submersion or surfacing, submarines use the forward and aft tanks, called Main Ballast Tanks (MBT), which are filled with water to submerge or with air to surface. Submerged, MBTs generally remain flooded, which simplifies their design, and on many submarines these tanks are a section of interhull space. For more precise and quick control of depth, submarines use smaller Depth Control Tanks (DCT)—also called hard tanks (due to their ability to withstand higher pressure), or trim tanks. The amount of water in depth control tanks can be controlled to change depth or to maintain a constant depth as outside conditions (chiefly water density) change. Depth control tanks may be located either near the submarine's center of gravity, or separated along the submarine body to prevent affecting trim.

When submerged, the water pressure on a submarine's hull can reach for steel submarines and up to for titanium submarines like , while interior pressure remains relatively unchanged. This difference results in hull compression, which decreases displacement. Water density also marginally increases with depth, as the salinity and pressure are higher. This change in density incompletely compensates for hull compression, so buoyancy decreases as depth increases. A submerged submarine is in an unstable equilibrium, having a tendency to either sink or float to the surface. Keeping a constant depth requires continual operation of either the depth control tanks or control surfaces.

Submarines in a neutral buoyancy condition are not intrinsically trim-stable. To maintain desired trim, submarines use forward and aft trim tanks. Pumps can move water between the tanks, changing weight distribution and pointing the sub up or down. A similar system is sometimes used to maintain stability.
The hydrostatic effect of variable ballast tanks is not the only way to control the submarine underwater. Hydrodynamic maneuvering is done by several surfaces, which can be moved to create hydrodynamic forces when a submarine moves at sufficient speed. The stern planes (hydroplanes in UK), located near the propeller and normally horizontal, serve the same purpose as the trim tanks, controlling the trim, and are commonly used, while other control surfaces may not be present on all submarines. The fairwater planes on the sail and/or bow planes on the main body, both also horizontal, are closer to the center of gravity, and are used to control depth with less effect on the trim.

When a submarine performs an emergency surfacing, all depth and trim methods are used simultaneously, together with propelling the boat upwards. Such surfacing is very quick, so the sub may even partially jump out of the water, potentially damaging submarine systems.

Modern submarines are cigar-shaped. This design, visible in early submarines, is sometimes called a "teardrop hull". It reduces the hydrodynamic drag when submerged, but decreases the sea-keeping capabilities and increases drag while surfaced. Since the limitations of the propulsion systems of early submarines forced them to operate surfaced most of the time, their hull designs were a compromise. Because of the slow submerged speeds of those subs, usually well below 10 kt (18 km/h), the increased drag for underwater travel was acceptable. Late in World War II, when technology allowed faster and longer submerged operation and increased aircraft surveillance forced submarines to stay submerged, hull designs became teardrop shaped again to reduce drag and noise. was a unique research submarine that pioneered the American version of the teardrop hull form (sometimes referred to as an "Albacore hull") of modern submarines. On modern military submarines the outer hull is covered with a layer of sound-absorbing rubber, or anechoic plating, to reduce detection.

The occupied pressure hulls of deep diving submarines such as are spherical instead of cylindrical. This allows a more even distribution of stress at the great depth. A titanium frame is usually affixed to the pressure hull, providing attachment for ballast and trim systems, scientific instrumentation, battery packs, syntactic flotation foam, and lighting.

A raised tower on top of a submarine accommodates the periscope and electronics masts, which can include radio, radar, electronic warfare, and other systems including the snorkel mast. In many early classes of submarines (see history), the control room, or "conn", was located inside this tower, which was known as the "conning tower". Since then, the conn has been located within the hull of the submarine, and the tower is now called the "sail". The conn is distinct from the "bridge", a small open platform in the top of the sail, used for observation during surface operation.

"Bathtubs" are related to conning towers but are used on smaller submarines. The bathtub is a metal cylinder surrounding the hatch that prevents waves from breaking directly into the cabin. It is needed because surfaced submarines have limited freeboard, that is, they lie low in the water. Bathtubs help prevent swamping the vessel.

Modern submarines and submersibles, as well as the oldest ones, usually have a single hull. Large submarines generally have an additional hull or hull sections outside. This external hull, which actually forms the shape of submarine, is called the outer hull ("casing" in the Royal Navy) or light hull, as it does not have to withstand a pressure difference. Inside the outer hull there is a strong hull, or pressure hull, which withstands sea pressure and has normal atmospheric pressure inside.

As early as World War I, it was realized that the optimal shape for withstanding pressure conflicted with the optimal shape for seakeeping and minimal drag, and construction difficulties further complicated the problem. This was solved either by a compromise shape, or by using two hulls; internal for holding pressure, and external for optimal shape. Until the end of World War II, most submarines had an additional partial cover on the top, bow and stern, built of thinner metal, which was flooded when submerged. Germany went further with the Type XXI, a general predecessor of modern submarines, in which the pressure hull was fully enclosed inside the light hull, but optimized for submerged navigation, unlike earlier designs that were optimized for surface operation.
After World War II, approaches split. The Soviet Union changed its designs, basing them on German developments. All post-World War II heavy Soviet and Russian submarines are built with a double hull structure. American and most other Western submarines switched to a primarily single-hull approach. They still have light hull sections in the bow and stern, which house main ballast tanks and provide a hydrodynamically optimized shape, but the main cylindrical hull section has only a single plating layer. Double hulls are being considered for future submarines in the United States to improve payload capacity, stealth and range.

The pressure hull is generally constructed of thick high-strength steel with a complex structure and high strength reserve, and is separated with watertight bulkheads into several compartments. There are also examples of more than two hulls in a submarine, like the , which has two main pressure hulls and three smaller ones for control room, torpedoes and steering gear, with the missile launch system between the main hulls.

The dive depth cannot be increased easily. Simply making the hull thicker increases the weight and requires reduction of onboard equipment weight, ultimately resulting in a "bathyscaphe". This is acceptable for civilian research submersibles, but not military submarines.

WWI submarines had hulls of carbon steel, with a maximum depth. During WWII, high-strength alloyed steel was introduced, allowing depths. High-strength alloy steel remains the primary material for submarines today, with depths, which cannot be exceeded on a military submarine without design compromises. To exceed that limit, a few submarines were built with titanium hulls. Titanium can be stronger than steel, lighter, and is not ferromagnetic, important for stealth. Titanium submarines were built by the Soviet Union, which developed specialized high-strength alloys. It has produced several types of titanium submarines. Titanium alloys allow a major increase in depth, but other systems must be redesigned to cope, so test depth was limited to for the , the deepest-diving combat submarine. An may have successfully operated at , though continuous operation at such depths would produce excessive stress on many submarine systems. Titanium does not flex as readily as steel, and may become brittle after many dive cycles. Despite its benefits, the high cost of titanium construction led to the abandonment of titanium submarine construction as the Cold War ended. Deep-diving civilian submarines have used thick acrylic pressure hulls.

The deepest deep-submergence vehicle (DSV) to date is "Trieste". On 5 October 1959, "Trieste" departed San Diego for Guam aboard the freighter "Santa Maria" to participate in "Project Nekton", a series of very deep dives in the Mariana Trench. On 23 January 1960, "Trieste" reached the ocean floor in the Challenger Deep (the deepest southern part of the Mariana Trench), carrying Jacques Piccard (son of Auguste) and Lieutenant Don Walsh, USN. This was the first time a vessel, manned or unmanned, had reached the deepest point in the Earth's oceans. The onboard systems indicated a depth of , although this was later revised to and more accurate measurements made in 1995 have found the Challenger Deep slightly shallower, at .

Building a pressure hull is difficult, as it must withstand pressures at its required diving depth. When the hull is perfectly round in cross-section, the pressure is evenly distributed, and causes only hull compression. If the shape is not perfect, the hull is bent, with several points heavily strained. Inevitable minor deviations are resisted by stiffener rings, but even a one-inch (25 mm) deviation from roundness results in over 30 percent decrease of maximal hydrostatic load and consequently dive depth. The hull must therefore be constructed with high precision. All hull parts must be welded without defects, and all joints are checked multiple times with different methods, contributing to the high cost of modern submarines. (For example, each attack submarine costs US$2.6 billion, over US$200,000 per ton of displacement.)

The first submarines were propelled by humans. The first mechanically driven submarine was the 1863 French , which used compressed air for propulsion. Anaerobic propulsion was first employed by the Spanish "Ictineo II" in 1864, which used a solution of zinc, manganese dioxide, and potassium chlorate to generate sufficient heat to power a steam engine, while also providing oxygen for the crew. A similar system was not employed again until 1940 when the German Navy tested a hydrogen peroxide-based system, the Walter turbine, on the experimental V-80 submarine and later on the naval and type XVII submarines.

Until the advent of nuclear marine propulsion, most 20th-century submarines used batteries for running underwater and gasoline (petrol) or diesel engines on the surface, and for battery recharging. Early submarines used gasoline, but this quickly gave way to kerosene (paraffin), then diesel, because of reduced flammability. Diesel-electric became the standard means of propulsion. The diesel or gasoline engine and the electric motor, separated by clutches, were initially on the same shaft driving the propeller. This allowed the engine to drive the electric motor as a generator to recharge the batteries and also propel the submarine. The clutch between the motor and the engine would be disengaged when the submarine dived, so that the motor could drive the propeller. The motor could have multiple armatures on the shaft, which could be electrically coupled in series for slow speed and in parallel for high speed (these connections were called "group down" and "group up", respectively).

Early submarines used a direct mechanical connection between the engine and propeller, switching between diesel engines for surface running, and battery-driven electric motors for submerged propulsion.

In 1928, the United States Navy's Bureau of Engineering proposed a diesel-electric transmission. Instead of driving the propeller directly while running on the surface, the submarine's diesel drove a generator that could either charge the submarine's batteries or drive the electric motor. This made electric motor speed independent of diesel engine speed, so the diesel could run at an optimum and non-critical speed. One or more diesel engines could be shut down for maintenance while the submarine continued to run on the remaining engine or battery power. The US pioneered this concept in 1929, in the S-class submarines , , and . The first production submarines with this system were the "Porpoise" class of the 1930s, and it was used on most subsequent US diesel submarines through the 1960s. No other navy adopted the system before 1945, apart from the Royal Navy's U-class submarines, though some submarines of the Imperial Japanese Navy used separate diesel generators for low speed running.

Other advantages of such an arrangement were that a submarine could travel slowly with the engines at full power to recharge the batteries quickly, reducing time on the surface or on snorkel. It was then possible to isolate the noisy diesel engines from the pressure hull, making the submarine quieter. Additionally, diesel-electric transmissions were more compact.

During World War II the Germans experimented with the idea of the "schnorchel" (snorkel) from captured Dutch submarines, but didn't see the need for them until rather late in the war. The "schnorchel" was a retractable pipe that supplied air to the diesel engines while submerged at periscope depth, allowing the boats to cruise and recharge their batteries while maintaining a degree of stealth. It was far from a perfect solution, however. There were problems with the device's valve sticking shut or closing as it dunked in rough weather; since the system used the entire pressure hull as a buffer, the diesels would instantaneously suck huge volumes of air from the boat's compartments, and the crew often suffered painful ear injuries. Speed was limited to , lest the device snap from stress. The "schnorchel" also created noise that made the boat easier to detect with sonar, yet more difficult for the on-board sonar to detect signals from other vessels. Finally, Allied radar eventually became sufficiently advanced that the "schnorchel" mast could be detected beyond visual range.

While the snorkel renders a submarine far less detectable, it is not perfect. In clear weather, diesel exhaust can be seen on the surface to a distance of about three miles, while "periscope feather" (the wave created by the snorkel or periscope moving through the water) is visible from far off in calm sea conditions. Modern radar is also capable of detecting a snorkel in calm sea conditions.

The problem of the diesels causing a vacuum in the submarine when the head valve is submerged still exists in later model diesel submarines, but is mitigated by high-vacuum cut-off sensors that shut down the engines when the vacuum in the ship reaches a pre-set point. Modern snorkel induction masts use a fail-safe design using compressed air, controlled by a simple electrical circuit, to hold the "head valve" open against the pull of a powerful spring. Seawater washing over the mast shorts out exposed electrodes on top, breaking the control, and shutting the "head valve" while it is submerged. US submarines did not adopt the use of snorkels until after WWII.

One new technology that is being introduced starting with the Japanese Navy's eleventh "Sōryū"-class submarine (JS "Ōryū") is a more modern battery, the lithium-ion battery. These batteries have about double the electric storage of traditional batteries, and by changing out the lead-acid batteries in their normal storage areas plus filling up the large hull space normally devoted to AIP engine and fuel tanks with many tons of lithium-ion batteries, modern submarines can actually return to a "pure" diesel-electric configuration yet have the added underwater range and power normally associated with AIP equipped submarines.

During World War II, German Type XXI submarines (also known as ""Elektroboote"") were the first submarines designed to operate submerged for extended periods. Initially they were to carry hydrogen peroxide for long-term, fast air-independent propulsion, but were ultimately built with very large batteries instead. At the end of the War, the British and Soviets experimented with hydrogen peroxide/kerosene (paraffin) engines that could run surfaced and submerged. The results were not encouraging. Though the Soviet Union deployed a class of submarines with this engine type (codenamed by NATO), they were considered unsuccessful.

The United States also used hydrogen peroxide in an experimental midget submarine, X-1. It was originally powered by a hydrogen peroxide/diesel engine and battery system until an explosion of her hydrogen peroxide supply on 20 May 1957. X-1 was later converted to use diesel-electric drive.

Today several navies use air-independent propulsion. Notably Sweden uses Stirling technology on the and s. The Stirling engine is heated by burning diesel fuel with liquid oxygen from cryogenic tanks. A newer development in air-independent propulsion is hydrogen fuel cells, first used on the German Type 212 submarine, with nine 34 kW or two 120 kW cells and soon to be used in the new Spanish s.

Steam power was resurrected in the 1950s with a nuclear-powered steam turbine driving a generator. By eliminating the need for atmospheric oxygen, the time that a submarine could remain submerged was limited only by its food stores, as breathing air was recycled and fresh water distilled from seawater. More importantly, a nuclear submarine has unlimited range at top speed. This allows it to travel from its operating base to the combat zone in a much shorter time and makes it a far more difficult target for most anti-submarine weapons. Nuclear-powered submarines have a relatively small battery and diesel engine/generator powerplant for emergency use if the reactors must be shut down.

Nuclear power is now used in all large submarines, but due to the high cost and large size of nuclear reactors, smaller submarines still use diesel-electric propulsion. The ratio of larger to smaller submarines depends on strategic needs. The US Navy, French Navy, and the British Royal Navy operate only nuclear submarines, which is explained by the need for distant operations. Other major operators rely on a mix of nuclear submarines for strategic purposes and diesel-electric submarines for defense. Most fleets have no nuclear submarines, due to the limited availability of nuclear power and submarine technology.

Diesel-electric submarines have a stealth advantage over their nuclear counterparts. Nuclear submarines generate noise from coolant pumps and turbo-machinery needed to operate the reactor, even at low power levels. Some nuclear submarines such as the American can operate with their reactor coolant pumps secured, making them quieter than electric subs. A conventional submarine operating on batteries is almost completely silent, the only noise coming from the shaft bearings, propeller, and flow noise around the hull, all of which stops when the sub hovers in mid-water to listen, leaving only the noise from crew activity. Commercial submarines usually rely only on batteries, since they operate in conjunction with a mother ship.

Several serious nuclear and radiation accidents have involved nuclear submarine mishaps. The reactor accident in 1961 resulted in 8 deaths and more than 30 other people were over-exposed to radiation. The reactor accident in 1968 resulted in 9 fatalities and 83 other injuries. The accident in 1985 resulted in 10 fatalities and 49 other radiation injuries.

Oil-fired steam turbines powered the British K-class submarines, built during World War I and later, to give them the surface speed to keep up with the battle fleet. The K-class subs were not very successful, however.

Toward the end of the 20th century, some submarines—such as the British "Vanguard" class—began to be fitted with pump-jet propulsors instead of propellers. Though these are heavier, more expensive, and less efficient than a propeller, they are significantly quieter, providing an important tactical advantage.

The success of the submarine is inextricably linked to the development of the torpedo, invented by Robert Whitehead in 1866. His invention is essentially the same now as it was 140 years ago. Only with self-propelled torpedoes could the submarine make the leap from novelty to a weapon of war. Until the perfection of the guided torpedo, multiple "straight-running" torpedoes were required to attack a target. With at most 20 to 25 torpedoes stored on board, the number of attacks was limited. To increase combat endurance most World War I submarines functioned as submersible gunboats, using their deck guns against unarmed targets, and diving to escape and engage enemy warships. The importance of guns encouraged the development of the unsuccessful Submarine Cruiser such as the French and the Royal Navy's and M-class submarines. With the arrival of Anti-submarine warfare (ASW) aircraft, guns became more for defense than attack. A more practical method of increasing combat endurance was the external torpedo tube, loaded only in port.

The ability of submarines to approach enemy harbours covertly led to their use as minelayers. Minelaying submarines of World War I and World War II were specially built for that purpose. Modern submarine-laid mines, such as the British Mark 5 Stonefish and Mark 6 Sea Urchin, can be deployed from a submarine's torpedo tubes.

After World War II, both the US and the USSR experimented with submarine-launched cruise missiles such as the SSM-N-8 Regulus and P-5 Pyatyorka. Such missiles required the submarine to surface to fire its missiles. They were the forerunners of modern submarine-launched cruise missiles, which can be fired from the torpedo tubes of submerged submarines, for example the US BGM-109 Tomahawk and Russian RPK-2 Viyuga and versions of surface-to-surface anti-ship missiles such as the Exocet and Harpoon, encapsulated for submarine launch. Ballistic missiles can also be fired from a submarine's torpedo tubes, for example missiles such as the anti-submarine SUBROC. With internal volume as limited as ever and the desire to carry heavier warloads, the idea of the external launch tube was revived, usually for encapsulated missiles, with such tubes being placed between the internal pressure and outer streamlined hulls.

The strategic mission of the SSM-N-8 and the P-5 was taken up by submarine-launched ballistic missile beginning with the US Navy's Polaris missile, and subsequently the Poseidon and Trident missiles.

Germany is working on the torpedo tube-launched short-range IDAS missile, which can be used against ASW helicopters, as well as surface ships and coastal targets.

A submarine can have a variety of sensors, depending on its missions. Modern military submarines rely almost entirely on a suite of passive and active sonars to locate targets. Active sonar relies on an audible "ping" to generate echoes to reveal objects around the submarine. Active systems are rarely used, as doing so reveals the sub's presence. Passive sonar is a set of sensitive hydrophones set into the hull or trailed in a towed array, normally trailing several hundred feet behind the sub. The towed array is the mainstay of NATO submarine detection systems, as it reduces the flow noise heard by operators. Hull mounted sonar is employed in addition to the towed array, as the towed array can't work in shallow depth and during maneuvering. In addition, sonar has a blind spot "through" the submarine, so a system on both the front and back works to eliminate that problem. As the towed array trails behind and below the submarine, it also allows the submarine to have a system both above and below the thermocline at the proper depth; sound passing through the thermocline is distorted resulting in a lower detection range.

Submarines also carry radar equipment to detect surface ships and aircraft. Submarine captains are more likely to use radar detection gear than active radar to detect targets, as radar can be detected far beyond its own return range, revealing the submarine. Periscopes are rarely used, except for position fixes and to verify a contact's identity.

Civilian submarines, such as the or the Russian "Mir" submersibles, rely on small active sonar sets and viewing ports to navigate. The human eye cannot detect sunlight below about underwater, so high intensity lights are used to illuminate the viewing area.

Early submarines had few navigation aids, but modern subs have a variety of navigation systems. Modern military submarines use an inertial guidance system for navigation while submerged, but drift error unavoidably builds over time. To counter this, the crew occasionally uses the Global Positioning System to obtain an accurate position. The periscope—a retractable tube with a prism system that provides a view of the surface—is only used occasionally in modern submarines, since the visibility range is short. The and s use photonics masts rather than hull-penetrating optical periscopes. These masts must still be deployed above the surface, and use electronic sensors for visible light, infrared, laser range-finding, and electromagnetic surveillance. One benefit to hoisting the mast above the surface is that while the mast is above the water the entire sub is still below the water and is much harder to detect visually or by radar.

Military submarines use several systems to communicate with distant command centers or other ships. One is VLF (very low frequency) radio, which can reach a submarine either on the surface or submerged to a fairly shallow depth, usually less than . ELF (extremely low frequency) can reach a submarine at greater depths, but has a very low bandwidth and is generally used to call a submerged sub to a shallower depth where VLF signals can reach. A submarine also has the option of floating a long, buoyant wire antenna to a shallower depth, allowing VLF transmissions by a deeply submerged boat.

By extending a radio mast, a submarine can also use a "burst transmission" technique. A burst transmission takes only a fraction of a second, minimizing a submarine's risk of detection.

To communicate with other submarines, a system known as Gertrude is used. Gertrude is basically a sonar telephone. Voice communication from one submarine is transmitted by low power speakers into the water, where it is detected by passive sonars on the receiving submarine. The range of this system is probably very short, and using it radiates sound into the water, which can be heard by the enemy.

Civilian submarines can use similar, albeit less powerful systems to communicate with support ships or other submersibles in the area.

With nuclear power or air-independent propulsion, submarines can remain submerged for months at a time. Conventional diesel submarines must periodically resurface or run on snorkel to recharge their batteries. Most modern military submarines generate breathing oxygen by electrolysis of water (using a device called an "Electrolytic Oxygen Generator"). Atmosphere control equipment includes a CO scrubber, which uses an amine absorbent to remove the gas from air and diffuse it into waste pumped overboard. A machine that uses a catalyst to convert carbon monoxide into carbon dioxide (removed by the CO scrubber) and bonds hydrogen produced from the ship's storage battery with oxygen in the atmosphere to produce water, is also used. An atmosphere monitoring system samples the air from different areas of the ship for nitrogen, oxygen, hydrogen, R-12 and R-114 refrigerants, carbon dioxide, carbon monoxide, and other gases. Poisonous gases are removed, and oxygen is replenished by use of an oxygen bank located in a main ballast tank. Some heavier submarines have two oxygen bleed stations (forward and aft). The oxygen in the air is sometimes kept a few percent less than atmospheric concentration to reduce fire risk.

Fresh water is produced by either an evaporator or a reverse osmosis unit. The primary use for fresh water is to provide feedwater for the reactor and steam propulsion plants. It is also available for showers, sinks, cooking and cleaning once propulsion plant needs have been met. Seawater is used to flush toilets, and the resulting "black water" is stored in a sanitary tank until it is blown overboard using pressurized air or pumped overboard by using a special sanitary pump. The blackwater-discharge system is difficult to operate, and the German Type VIIC boat was lost with casualties because of human error while using this system. Water from showers and sinks is stored separately in "grey water" tanks and discharged overboard using drain pumps.

Trash on modern large submarines is usually disposed of using a tube called a Trash Disposal Unit (TDU), where it is compacted into a galvanized steel can. At the bottom of the TDU is a large ball valve. An ice plug is set on top of the ball valve to protect it, the cans atop the ice plug. The top breech door is shut, and the TDU is flooded and equalized with sea pressure, the ball valve is opened and the cans fall out assisted by scrap iron weights in the cans. The TDU is also flushed with seawater to ensure it is completely empty and the ball valve is clear before closing the valve.

A typical nuclear submarine has a crew of over 80; conventional boats typically have fewer than 40. The conditions on a submarine can be difficult because crew members must work in isolation for long periods of time, without family contact. Submarines normally maintain radio silence to avoid detection. Operating a submarine is dangerous, even in peacetime, and many submarines have been lost in accidents.

Most navies prohibited women from serving on submarines, even after they had been permitted to serve on surface warships. The Royal Norwegian Navy became the first navy to allow women on its submarine crews in 1985. The Royal Danish Navy allowed female submariners in 1988. Others followed suit including the Swedish Navy (1989), the Royal Australian Navy (1998), the Spanish Navy (1999), the German Navy (2001) and the Canadian Navy (2002). In 1995, Solveig Krey of the Royal Norwegian Navy became the first female officer to assume command on a military submarine, HNoMS "Kobben".

On 8 December 2011, British Defence Secretary Philip Hammond announced that the UK's ban on women in submarines was to be lifted from 2013. Previously there were fears that women were more at risk from a build-up of carbon dioxide in the submarine. But a study showed no medical reason to exclude women, though pregnant women would still be excluded. Similar dangers to the pregnant woman and her fetus barred women from submarine service in Sweden in 1983, when all other positions were made available for them in the Swedish Navy. Today, pregnant women are still not allowed to serve on submarines in Sweden. However, the policymakers thought that it was discriminatory with a general ban and demanded that women should be tried on their individual merits and have their suitability evaluated and compared to other candidates. Further, they noted that a woman complying with such high demands is unlikely to become pregnant. In May 2014, three women became the RN's first female submariners.

Women have served on US Navy surface ships since 1993, and , began serving on submarines for the first time. Until presently, the Navy allowed only three exceptions to women being on board military submarines: female civilian technicians for a few days at most, women midshipmen on an overnight during summer training for Navy ROTC and Naval Academy, and family members for one-day dependent cruises. In 2009, senior officials, including then-Secretary of the Navy Ray Mabus, Joint Chief of Staff Admiral Michael Mullen, and Chief of Naval Operations Admiral Gary Roughead, began the process of finding a way to implement women on submarines. The US Navy rescinded its "no women on subs" policy in 2010.

Both the US and British navies operate nuclear-powered submarines that deploy for periods of six months or longer. Other navies that permit women to serve on submarines operate conventionally powered submarines, which deploy for much shorter periods—usually only for a few months. Prior to the change by the US, no nation using nuclear submarines permitted women to serve on board.

In 2011, the first class of female submarine officers graduated from Naval Submarine School's Submarine Officer Basic Course (SOBC) at the Naval Submarine Base New London. Additionally, more senior ranking and experienced female supply officers from the surface warfare specialty attended SOBC as well, proceeding to fleet Ballistic Missile (SSBN) and Guided Missile (SSGN) submarines along with the new female submarine line officers beginning in late 2011. By late 2011, several women were assigned to the "Ohio"-class ballistic missile submarine . On 15 October 2013, the US Navy announced that two of the smaller "Virginia"-class attack submarines, and , would have female crew-members by January 2015.

In an emergency, submarines can transmit a signal to other ships. The crew can use Submarine Escape Immersion Equipment to abandon the submarine. The crew can prevent a lung injury from the pressure change known as pulmonary barotrauma by exhaling during the ascent. Following escape from a pressurized submarine, the crew is at risk of developing decompression sickness. An alternative escape means is via a Deep Submergence Rescue Vehicle that can dock onto the disabled submarine.


1900/Russo-Japanese War 1904–1905




</doc>
<doc id="28827" url="https://en.wikipedia.org/wiki?curid=28827" title="Second Epistle to the Thessalonians">
Second Epistle to the Thessalonians

The Second Epistle to the Thessalonians, commonly referred to as Second Thessalonians or 2 Thessalonians is a book from the New Testament of the Christian Bible. It is traditionally attributed to Paul the Apostle, with Timothy as a co-author. Modern biblical scholarship is divided on whether the epistle was written by Paul; many scholars reject its authenticity based on what they see as differences in style and theology between this and the First Epistle to the Thessalonians.

Scholars who support its authenticity view it as having been written around 51–52 AD, shortly after the First Epistle. Those who see it as a later composition assign a date of around 80–115 AD.

The authenticity of this epistle is still in widespread dispute. As Professor Ernest Best, New Testament scholar, explains the problem;

The structures of the two letters (to which Best refers) include opening greetings ("1 Thess." 1:1, "2 Thess." 1:1–2) and closing benedictions ("1 Thess." 5:28, "2 Thess." 3:16d–18) which frame two, balancing, sections (AA'). In "2 Thessalonians" these begin with similar successions of nine Greek words, at 1:3 and 2:13. The opening letter section (1:3–2:12) itself comprises two halves, 1:3–12 (where the introductory piece, A, is 1:3–5; the first development, B, is 1:6–10; and the paralleling and concluding development, B', is 1:11–12) and 2:1–12 (with pieces: A 2:1–4, B 2:5–7, B' 2:8–12).

The second, balancing, letter section (2:13–3:16c) also comprises two halves: 2:13–3:5 (with pieces: A 2:13–14, B 2:15–17, B' 3:1–5) and 3:6–16c (with pieces: A 3:6–9, B 3:10–12, B' 3:13-16c). Of the twelve pieces in "2 Thessalonians" seven begin with 'brother' introductions. Of the eighteen pieces in "1 Thessalonians" fourteen begin with 'brother' introductions. In both letters, the sections balance in size and focus, and in many details. In "2 Thessalonians", in 2:5 and 3:10, for example, there is a structural balance of the use of 'when I was with you...' and 'when we were with you...'.

One piece of evidence for the authenticity of the epistle is that it was included in Marcion's canon and the Muratorian fragment. It was also mentioned by name by Irenaeus, and quoted by Ignatius, Justin, and Polycarp.

G. Milligan argued that a church which possessed an authentic letter of Paul would be unlikely to accept a fake addressed to them. So also Colin Nicholl who has put forward a substantial argument for the authenticity of Second Thessalonians. He points out that 'the pseudonymous view is ... more vulnerable than most of its advocates conceded. ... The lack of consensus regarding a date and destination ... reflects a dilemma for this position: on the one hand, the date needs to be early enough for the letter to be have been accepted as Pauline ... [on] the other hand, the date and destination need to be such that the author could be confident that no contemporary of 1 Thessalonians ... could have exposed 2 Thessalonians as a ... forgery.'. pp. 5–6

Another scholar who argues for the authenticity of this letter is Jerome Murphy-O'Connor. Admitting that there are stylistic problems between "Second Thessalonians" and "First Thessalonians", he argues that part of the problem is due to the composite nature of "First Thessalonians" (Murphy-O'Connor is only one of many scholars who argue that the current text of "Second Thessalonians" is the product of merging two or more authentic letters of Paul). Once the text of this interpolated letter is removed and the two letters compared, Murphy-O'Connor asserts that this objection is "drastically weakened", and concludes, "The arguments against the authenticity of 2 Thessalonians are so weak that it is preferable to accept the traditional ascription of the letter to Paul."

Those who believe Paul was the author of "Second Thessalonians" also note how Paul drew attention to the authenticity of the letter by signing it himself: "I, Paul, write this greeting with my own hand, which is how I write in every letter.". Bruce Metzger writes, "Paul calls attention to his signature, which was added by his own hand as a token of genuineness to every letter of his (3:17)."

Other scholars who hold to authenticity include Beale, Green, Jones, Morris, Witherington, and Kretzmann. According to Leon Moris in 1986, the majority of current scholars at that time still held to Paul's authorship of 2 Thessalonians.

At least as early as 1798, when J.E.C. Schmidt published his opinion, Paul's authorship of this epistle was questioned. More recent challenges to this traditional belief came from scholars such as William Wrede in 1903 and Alfred Loisy in 1933, who challenged the traditional view of the authorship.

In his book "Forged", New Testament scholar Bart D. Ehrman puts forward some of the most common arguments against the authenticity of 2 Thessalonians. For example, he argues that the views concerning the Second Coming of Christ expressed in 2 Thessalonians differ so strikingly from those found in 1 Thessalonians that they cannot be written by the same author:

Ehrman also argues that the self-referencing signature at the end of 2 Thessalonians was likely used by the forger of the epistle to authenticate what he had written. If Paul had actually written the letter, Ehrman reasons, he would not have needed to include such an autograph:

Many modern scholars agree with Ehrman that 2 Thessalonians was not written by Paul but by an associate or disciple after his death. See, for example, Beverly Roberts Gaventa, Vincent Smiles, Udo Schnelle, Eugene Boring, and Joseph Kelly. Norman Perrin observes, "The best understanding of 2 Thessalonians ... is to see it as a deliberate imitation of 1 Thessalonians, updating the apostle's thought." Perrin bases this claim on his hypothesis that prayer at the time usually treated God the Father as ultimate judge, rather than Jesus.

Thessalonica was the second city in Europe where Paul helped to create an organized Christian community. At some point after the first letter was sent, probably soon, some of the Thessalonicans grew concerned over whether those who had died would share in the parousia. This letter was written in response to this concern. The problem then arises, as Raymond Brown points out, whether this letter is an authentic writing of Paul written by one of his followers in his name.

If this letter is authentic, then it might have been written soon after Paul's first letter to this community—or possibly years later. Brown notes that Paul "most likely visited Thessalonica several times in his journeys to Macedonia". However, if the letter is not authentic, Brown notes that "in some ways interpretation becomes more complex." Brown believes that the majority of scholars who advocate pseudonymity would place it towards the end of the first century, the same time that Revelation was written. These scholars emphasize the appearance of "man of sin" in the second chapter of this letter, whether this personage is identified with the Antichrist of 1 John and Revelation, or with a historical person like Caligula.

The traditional view is that the second epistle to the Thessalonians was probably written from Corinth not many months after the first.

Biblical commentator and pastor John Macarthur writes, "The emphasis is on how to maintain a church with an effective testimony in proper response to sound eschatology and obedience to the truth."

Paul opens the letter praising this church for their faithfulness and perseverance in the face of persecution:

"We ought always to give thanks to God for you, brethren, as is only fitting, because your faith is greatly enlarged, and the love of each one of you toward one another grows ever greater; therefore, we ourselves speak proudly of you among the churches of God for your perseverance and faith in the midst of all your persecutions and afflictions which you endure" (2 Thess 1:3–5 [NASB]).

The letter contains a whole chapter regarding the second advent of Christ, among other themes and instructions.

From the inference of 2:1–2, the Thessalonians were faced with a false teaching, saying that Christ had already returned. This error is corrected in chapter 2 (2:1–12), where Paul tells the Thessalonians that a great tribulation must occur before Christ's return. Seeing as how this series of events has not yet happened, his argument reads, Christ cannot have returned yet. He then expresses thanks that his readers were the elect of God, chosen for salvation and saved by His grace through faith, and thus not susceptible to the deception of the "Great Apostasy," (2 Thess 2:13–14) first mentioned here as is the "Katechon" (2 Thess 2:6–7).

In 2 Thess 2:15, Paul instructs his readers to "[h]old fast to the traditions (, ) which you were taught, whether by word of mouth or by our letter." Quoting this verse, in his "On the Holy Spirit", Basil the Great writes, "These [traditions] have been passed on by word of mouth from Paul or from the other apostles, without necessarily being written down," and mentions the Trinitarian confession of faith as an example of "unwritten tradition". Cyril of Jerusalem shares a similar view in his "Catechetical Lectures", argues that the traditions stated by Paul should be preserved and memorized, at a minimum in the form of the Creed. In his homily on this verse, John Chrysostom differentiates oral tradition from written tradition. At that time, the oral tradition has been defined as the "tradition" and the written tradition as "Scripture", united together in "the authenticity of their apostolic origin". Everett Ferguson says Paul's reference to tradition implicates that "what was delivered was from the Lord", and John Stott calls the tradition (, "paradosis") "apostolic 'tradition.

The letter continues by encouraging the Thessalonian church to stand firm in their faith, and to "keep away from every brother who leads an unruly life and not according to the tradition which you received from us... do not associate with him, so that he will be put to shame. Yet do not regard him as an enemy, but admonish him as a brother" (2 Thess 3:6–7, 14–15).

Paul ends this letter by saying, "I, Paul, write this greeting with my own hand, and this is a distinguishing mark in every letter; this is the way I write. The grace of our Lord Jesus Christ be with you all" (2 Thess 3:17–18). Macarthur writes, "Paul added an identifying signature (cf. 1 Cor. 16:21; Col. 4:18) so his readers could be sure he was truly the author."

A passage from this book reading "For even when we were with you, this we commanded you, that if any would not work, neither should he eat", (2 Thess. 3:10), was later adapted by Vladimir Lenin as an adage of the Soviet Union, He who does not work, neither shall he eat.



Online translations of the Second Epistle to the Thessalonians:

Exegetical Papers on Second Thessalonians:


</doc>
<doc id="28828" url="https://en.wikipedia.org/wiki?curid=28828" title="Poetry slam">
Poetry slam

A poetry slam is a competition arts event, in which poets perform spoken word poetry before a live audience and a panel of judges. Culturally, poetry slams are a break with the past image of poetry as an elitist or rigid art form. While formats can vary, slams are often loud and lively, with audience participation, cheering and dramatic delivery. Hip-hop music and urban culture are strong influences, and backgrounds of participants tend to be diverse.

Poetry slams began in Chicago in 1984, with the first slam competition designed to move poetry recitals from academia to a popular audience. American poet Marc Smith, believing the poetry scene at the time was "too structured and stuffy", began experimenting by attending open microphone poetry readings, and then turning them into slams by introducing the element of competition.

The performances at a poetry slam are judged as much on enthusiasm and style as content, and poets may compete as individuals or in teams. The judging is often handled by a panel of judges, typically five, who are usually selected from the audience. Sometimes the poets are judged by audience response.

American poet Marc Smith is credited with starting the poetry slam at the Get Me High Lounge in Chicago in November 1984. In July 1986, the original slam moved to its permanent home, the Green Mill Jazz Club. In 1987 the Ann Arbor Poetry Slam was founded by Vince Keuter and eventually made its home at the Heidelberg (moving later 2010, 2013, and 2015 to its new home at Espresso Royale). In August 1988, the first poetry slam held in New York City was hosted by Bob Holman at the Nuyorican Poet's Cafe. In 1990, the first National Poetry Slam took place at Fort Mason, San Francisco. This slam included teams from Chicago and San Francisco, and an individual poet from New York. Soon afterward, poetry slam increased popularity allowed some poets to make full-time careers in performance and competition, touring the United States and eventually the world.

In 2001, the grounding of aircraft following the September 11 attacks left a number of performers stranded in cities they had been performing in. After the attacks, a new wave of poetry slam started within New York City with a community focus on poets coming together to speak about the terrorist attacks.

, the National Poetry Slam featured 72 certified teams, culminating in five days of competition.

Today, there are poetry slam competitions in a number of countries around the globe.

Poetry Slam Inc. sanctions three major annual poetry competitions (for poets 18+) on a national and international scale: the National Poetry Slam (NPS), the individual World Poetry Slam (iWPS), and the Women of the World Poetry Slam (WoWPS)

In a poetry slam, members of the audience are chosen by an emcee or host to act as judges for the event. In the national slam, there are five judges, but smaller slams generally have three. After each poet performs, each judge awards a score to that poem. Scores generally range between zero and ten. The highest and lowest score are dropped, giving each performance a rating between zero and thirty points.

Before the competition begins, the host will often bring up a "sacrificial" poet, whom the judges will score in order to calibrate their judging.

A single round at a standard slam consists of performances by all eligible poets. Most slams last multiple rounds, and many involve the elimination of lower-scoring poets in successive rounds. An elimination rubric might run 8-4-2; eight poets in the first round, four in the second, and two in the last. Some slams do not eliminate poets at all. The Green Mill usually runs its slams with 6 poets in the first round. At the end of the slam, the poet with the highest number of points earned is the winner.

The Boston Poetry Slam takes a different approach; it uses the 8-4-2 three-round rubric, but the poets go head-to-head in separate bouts within the round.

Props, costumes, and music are forbidden in slams, which differs greatly from its immediate predecessor, performance poetry. Hedwig Gorski, the founder of performance poetry as a distinct genre, saw props, costumes, and music as essential for a complete theatrical experience while also following theorist Jerzy Grotowski's Poor Theater by blurring lines between the real person, actor, and speakers in scripted literary art. Other rules for slams enforce a time limit of three minutes (and a grace period of ten seconds), after which a poet's score may be docked according to how long the poem exceeded the limit. Many youth slams, however, allow the poets up to three and a half minutes on stage.

In an "Open Slam", the most common slam type, competition is open to all who wish to compete, given the number of slots available. In an "Invitational Slam", only those invited to do so may compete.

Poetry Slam, Inc. holds several national and international competitions, including the Individual World Poetry Slam, the National Poetry Slam and The Women of the World Poetry Slam. The current (2013) IWPS champion is Ed Mabrey. Ed Mabrey is the only three-time IWPS champion in the history of the event. The current (2013) National Poetry Slam Team champions are Slam New Orleans (SNO), who have won the competition for the second year in a row. The current (2014) Women of the World Poetry Slam Champion is Dominique Christina.

From 10–11 December 2016 Salzburg, Austria held a world-record poetry slam competition (28 hours of classic slam poetry) and broke the so-far-record of Nuremberg, Germany (25 hours) by Michl Jakob. The winner of the competition (Friedrich Herrmann) scored one point better in the finals than the second ranked (Darryl Kiermeier). The event was organized by Lukas Wagner (Slamlabor) and took place in the SN-Saal of the Salzburger Nachrichten.

A "Theme Slam" is one in which all performances must conform to a specified theme, genre, or formal constraint. Themes may include Nerd, Erotica, Queer, Improv, or other conceptual limitations. In theme slams, poets can sometimes be allowed to break "traditional" slam rules. For instance, they sometimes allow performance of work by another poet (e.g. the "Dead Poet Slam", in which all work must be by a deceased poet). They can also allow changes on the restrictions on costumes or props (e.g. the Swedish "Triathlon" slams that allow for a poet, musician, and dancer to all take the stage at the same time), changing the judging structure (e.g. having a specific guest judge), or changing the time limits (e.g. a "1-2-3" slam with three rounds of one minute, two minutes, and three minutes, respectively).

Although theme slams may seem restricting in nature, slam venues frequently use them to advocate participation by particular and perhaps underrepresented demographics (which vary from slam to slam), like younger poets and women.

Poetry slams can feature a broad range of voices, styles, cultural traditions, and approaches to writing and performance. The originator of performance poetry, Hedwig Gorski, credits slam poetry for carrying on the poetics of ancient oral poetry designed to grab attention in barrooms and public squares.
Some poets are closely associated with the vocal delivery style found in hip-hop music and draw heavily on the tradition of dub poetry, a rhythmic and politicized genre belonging to black and particularly West Indian culture. Others employ an unrhyming narrative formula. Some use traditional theatrical devices including shifting voices and tones, while others may recite an entire poem in ironic monotone. Some poets use nothing but their words to deliver a poem, while others stretch the boundaries of the format, tap-dancing or beatboxing or using highly choreographed movements.

What is a dominant / successful style one year may not be passed to the next. Cristin O'Keefe Aptowicz, slam poet and author of "Words In Your Face: A Guided Tour Through Twenty Years of the New York City Poetry Slam", was quoted in an interview on the Best American Poetry blog as saying:

One of the more interesting end products (to me, at least) of this constant shifting is that poets in the slam always worry that something—a style, a project, a poet—will become so dominant that it will kill the scene, but it never does. Ranting hipsters, freestyle rappers, bohemian drifters, proto-comedians, mystical shamans and gothy punks have all had their time at the top of the slam food chain, but in the end, something different always comes along and challenges the poets to try something new.
One of the goals of a poetry slam is to challenge the authority of anyone who claims absolute authority over literary value. No poet is beyond critique, as everyone is dependent upon the goodwill of the audience. Since only the poets with the best cumulative scores advance to the final round of the night, the structure assures that the audience gets to choose from whom they will hear more poetry. Audience members furthermore become part of each poem's presence, thus breaking down the barriers between poet/performer, critic, and audience.

Bob Holman, a poetry activist and former slammaster of the Nuyorican Poets Cafe, once called the movement "the democratization of verse". In 2005, Holman was also quoted as saying: "The spoken word revolution is led a lot by women and by poets of color. It gives a depth to the nation's dialogue that you don't hear on the floor of Congress. I want a floor of Congress to look more like a National Poetry Slam. That would make me happy."

At the 1993 National Poetry Slam in San Francisco, a participating team from Canada (Kedrick James, Alex Ferguson and John Sobol) wrote, printed and circulated an instant broadside titled "Like Lambs to the Slammer", that criticized what they perceived as the complacency, conformity, and calculated tear-jerking endemic to the poetry slam scene.

In an interview in the "Paris Review," literary critic Harold Bloom said about slamming:
I can't bear these accounts I read in the "Times" and elsewhere of these poetry slams, in which various young men and women in various late-spots are declaiming rant and nonsense at each other. The whole thing is judged by an applause meter which is actually not there, but might as well be. This isn't even silly; it is the death of art.

Poet and lead singer of King Missile, John S. Hall has also long been a vocal opponent, taking issue with such factors as its inherently competitive nature and what he considers its lack of stylistic diversity. In his 2005 interview in "Words In Your Face: A Guided Tour Through Twenty Years of the New York City Poetry Slam," he recalls seeing his first slam, at the Nuyorican Poets Café: "...I hated it. And it made me really uncomfortable and ... it was very much like a sport, and I was interested in poetry in large part because it was like the antithesis of sports. ... [I]t seemed to me like a very macho, masculine form of poetry and not at all what I was interested in."

The poet Tim Clare offers a "for and against" account of the phenomenon in "Slam: A Poetic Dialogue".

Ironically, slam poetry movement founder Marc Smith has been critical of the commercially successful Def Poetry television and Broadway live stage shows produced by Russell Simmons, decrying it as "an exploitive entertainment [program that] diminished the value and aesthetic of performance poetry".

As of 2011, four poets who have competed at National Poetry Slam have won National Endowment of the Arts (NEA) Fellowships for Literature:

As of 2017, one poet who has competed at National Poetry Slam has won the Pulitzer Prize for Poetry:

A number of poets belong to both academia and slam: 

Some renowned poets have competed in slams, with less successful results. Henry Taylor, winner of the 1985 Pulitzer Prize for Poetry, competed in the 1997 National Poetry Slam as an individual and placed 75th out of 150.

While slam poetry has often been ignored in traditional higher learning institutions, it slowly is finding its way into courses and programs of study. For example, at Berklee College of Music, in Boston, slam poetry is now available as a Minor course of study.

Slam poetry has found popularity as a form of self-expression among many teenagers. Young Chicago Authors (YCA) provides workshops, mentoring, and competition opportunities to youth in the Chicago area. Every year YCA presents Louder Than a Bomb, the world's largest team-based youth slam and subject of a documentary by the same name. The youth poetry slam movement was the focus of a documentary film series produced by HBO and released in 2009. It featured poets from Youth Speaks, Urban Word, Louder than a Bomb and other related youth poetry slam organizations.

In a 2005 interview, one of slam's best known poets Saul Williams praised the youth poetry slam movement, explaining:
In 2012, more than 12,000 young people took part in an England-wide youth slam "Shake the Dust", organised by Apples and Snakes as part of the London 2012 Festival.
An Open Letter to Honey Singh, a rap video featuring Rene Sharanya Verma performing at Delhi Poetry Slam, went viral on YouTube receiving over 1.5 million hits.

Slam Poetry has been in Egypt since the twentieth century and was introduced by Hussain Shafiq al-Misry; who was the editor of a sarcastic magazine. According to al-Misry, having different jobs gave him the experience to understand the struggles of Egyptian people in different classes of life. He had good knowledge of Arabic literature, grammar and some commonly used foreign words as well as slang; which he used to form Halamantishi poetry. Muhammad Ragab Bayyoumi in 1986 wrote an article entitled Hussein Shafiq al-Misry: Ustaz la Tilmeeth lah" (Hussein Shafiq al-Misry: A Teacher with No Student of His) in which he introduced al-Misry's poems and explained al-Misry's literary poetry techniques. In Egypt Performance Poetry is new in popularity, the term "Ash-Shi'r al-Mu'adda" was recently introduced as the term for performance poetry. Poets such as  Bayram At-Tunisi, Ahmad Rami, and Kamel Ash-Shennawy paved the way after al-Misry with lyrical slam poems that use a melodic rhythm to attract the audience.

In Japan, Professor Katsunori Kusunoki, a professor of communications at Toyo University found a way to incorporate slam poetry into his students lives; allowing them to showcase their competitiveness and love of poetry by putting together “poetry boxing” matches. Professor Kusunoki created annual “poetry boxing” tournaments in order to provide an medium for expression and social interaction . The rules are “16 boxers face off in pairs in competitions of stand-up verse that last for three minutes. Winners compete in series of challenges such as timed presentation and a round of improvised jousting.” A MC adds to the event by providing nicknames for the competitors. Professor Kusunoki's goal was to try to get his students to open up by breaking language barriers and allowing them to express themselves through Slam Poetry.



</doc>
<doc id="28829" url="https://en.wikipedia.org/wiki?curid=28829" title="Sestina">
Sestina

A sestina (Italian: "sestina", from "sesto", sixth; Old Occitan: "cledisat" ; also known as "sestine", "sextine", "sextain") is a fixed verse form consisting of six stanzas of six lines each, normally followed by a three-line envoi. The words that end each line of the first stanza are used as line endings in each of the following stanzas, rotated in a set pattern.

The invention of the form is usually attributed to Arnaut Daniel, a troubadour of 12th-century Provence, and the first sestinas were written in the Occitan language of that region. The form was cultivated by his fellow troubadours, then by other poets across Continental Europe in the subsequent centuries; they contributed to what would become the "standard form" of the sestina. The earliest example of the form in English appeared in 1579, though they were rarely written in Britain until the end of the 19th century. The sestina remains a popular poetic form, and many sestinas continue to be written by contemporary poets.

The oldest-known sestina is "Lo ferm voler qu'el cor m'intra", written around 1200 by Arnaut Daniel, a troubadour of Aquitanian origin; he refers to it as "cledisat", meaning, more or less, "interlock". Hence, Daniel is generally considered the form's inventor, though it has been suggested that he may only have innovated an already existing form. Nevertheless, two other original troubadouric sestinas are known, the best known being "Eras, pus vey mon benastruc" by Guilhem Peire Cazals de Caortz; there are also two contrafacta built on the same end-words, the best known being "Ben gran avoleza intra" by Bertran de Born. These early sestinas were written in Old Occitan; the form started spilling into Italian with Dante in the 13th century; by the 15th, it was used in Portuguese by Luís de Camões.

The involvement of Dante and Petrarch in establishing the sestina form, together with the contributions of others in the country, account for its classification as an Italian verse form—despite not originating there. The result was that the sestina was re-imported into France from Italy in the 16th century. Pontus de Tyard was the first poet to attempt the form in French, and the only one to do so prior to the 19th century; he introduced a partial rhyme scheme in his sestina.

The first appearance of the sestina in English print is "Ye wastefull woodes", comprising lines 151–89 of the August Æglogue in Edmund Spenser's "Shepherd's Calendar", published in 1579. It is in unrhymed iambic pentameter, but the order of end-words in each stanza is non-standard – ending 123456, 612345, etc. – each stanza promoting the previous final end-word to the first line, but otherwise leaving the order intact; the envoi order is (1) 2 / (3) 4 / (5) 6. This scheme was set by the Spaniard Gutierre de Cetina.

Although they appeared in print later, Philip Sidney's three sestinas may have been written earlier, and are often credited as the first in English. The first published (toward the end of Book I of "The Countess of Pembroke's Arcadia", 1590) is the double sestina "Ye Goatherd Gods". In this variant the standard end-word pattern is repeated for twelve stanzas, ending with a three-line envoi, resulting in a poem of 75 lines. Two others were published in subsequent editions of the "Arcadia". The second, "Since wailing is a bud of causeful sorrow", is in the "standard" form. Like "Ye Goatherd Gods" it is written in unrhymed iambic pentameter and uses exclusively feminine endings, reflecting the Italian "endecasillabo". The third, "Farewell, O sun, Arcadia's clearest light", is the first rhyming sestina in English: it is in iambic pentameters and follows the standard end-word scheme, but rhymes ABABCC in the first stanza (the rhyme scheme necessarily changes in each subsequent stanza, a consequence of which is that the 6th stanza is in rhyming couplets). Sidney uses the same envoi structure as Spenser. William Drummond of Hawthornden published two sestinas (which he called "sextains") in 1616, which copy the form of Sidney's rhyming sestina. After this, there is an absence of notable sestinas for over 250 years, with John Frederick Nims noting that, "... there is not a single sestina in the three volumes of the Oxford anthologies that cover the seventeenth, eighteenth and nineteenth centuries."

In the 1870s, there was a revival of interest in French forms, led by Andrew Lang, Austin Dobson, Edmund Gosse, W. E. Henley, John Payne, and others. The earliest sestina of this period is Algernon Charles Swinburne's "Sestina". It is in iambic pentameter rhyming ABABAB in the first stanza; each stanza begins by repeating the previous end-words 6 then 1, but the following 4 lines repeat the remaining end-words "ad lib"; the envoi is (1) 4 / (2) 3 / (5) 6. In the same volume ("Poems and Ballads, Second Series", 1878) Swinburne introduces a "double sestina" ("The Complaint of Lisa") that is unlike Sidney's: it comprises 12 stanzas of 12 iambic pentameter lines each, the first stanza rhyming ABCABDCEFEDF. Similar to his "Sestina", each stanza first repeats end-words 12 then 1 of the previous stanza; the rest are "ad lib". The envoi is (12) 10 / (8) 9 / (7) 4 / (3) 6 / (2) 1 / (11) 5.

From the 1930s, a revival of the form took place across the English-speaking world, led by poets such as W. H. Auden, and the 1950s were described as the "age of the sestina" by James E. B. Breslin. "Sestina: Altaforte" by Ezra Pound and "Paysage moralisé" by W. H. Auden are distinguished modern examples of the sestina. The sestina remains a popular closed verse form, and many sestinas continue to be written by contemporary poets; notable examples include "The Guest Ellen at the Supper for Street People" by David Ferry and "IVF" by Kona Macphee.

Although the sestina has been subject to many revisions throughout its development, there remain several features that define the form. The sestina is composed of six stanzas of six lines (sixains), followed by a stanza of three lines (a tercet). There is no rhyme within the stanzas; instead the sestina is structured through a recurrent pattern of the words that end each line, a technique known as "lexical repetition".

In the original form composed by Daniel, each line is of ten syllables, except the first of each stanza which are of seven. The established form, as developed by Petrarch and Dante, was in hendecasyllables. Since then, changes to the line length have been a relatively common variant, such that Stephanie Burt has written: "sestinas, as the form exists today, [do not] require expertise with inherited meter ...".

The pattern that the line-ending words follow is often explained if the numbers 1 to 6 are allowed to stand for the end-words of the first stanza. Each successive stanza takes its pattern based upon a bottom-up pairing of the lines of the preceding stanza (i.e., last and first, then second-from-last and second, then third-from-last and third). Given that the pattern for the first stanza is 123456, this produces 615243 in the second stanza.

Another way of visualising the pattern of line-ending words for each stanza is by the procedure known as "retrogradatio cruciata", which may be rendered as "backward crossing". The second stanza can be seen to have been formed from three sets of pairs (6–1, 5–2, 4–3), or two triads (1–2–3, 4–5–6). The 1–2–3 triad appears in its original order, but the 4–5–6 triad is reversed and superimposed upon it.

The pattern of the line-ending words in a sestina is represented both numerically and alphabetically in the following table:

The sixth stanza is followed by a tercet that is known variably by the French term envoi, the Occitan term tornada, or, with reference to its size in relation to the preceding stanzas, a "half-stanza". It consists of three lines that include all six of the line-endings words of the preceding stanzas. This should take the pattern of 2–5, 4–3, 6–1 (numbers relative to the first stanza); the first end-word of each pair can occur anywhere in the line, while the second must end the line. However, the end-word order of the envoi is no longer strictly enforced.

The sestina has been subject to some variations, with changes being made to both the size and number of stanzas, and also to individual line length. A "double sestina" is the name given to either: two sets of six six-line stanzas, with a three-line envoy (for a total of 75 lines), or twelve twelve-line stanzas, with a six-line envoy (for a total of 150 lines). Examples of either variation are rare; "Ye Goatherd Gods" by Philip Sidney is a notable example of the former variation, while "The Complaint of Lisa" by Algernon Charles Swinburne is a notable example of the latter variation. In the former variation, the original pattern of line-ending words, i.e. that of the first stanza, recurs in the seventh stanza, and thus the entire change of pattern occurs twice throughout. In the second variation, the pattern of line-ending words returns to the starting sequence in the eleventh stanza; thus it does not, unlike the "single" sestina, allow for every end-word to occupy each of the stanza ends; end-words 5 and 10 fail to couple between stanzas.

The structure of the sestina, which demands adherence to a strict and arbitrary order, produces several effects within a poem. Stephanie Burt notes that, "The sestina has served, historically, as a complaint", its harsh demands acting as "signs for deprivation or duress". The structure can enhance the subject matter that it orders; in reference to Elizabeth Bishop's "A Miracle for Breakfast", David Caplan suggests that the form's "harshly arbitrary demands echo its subject's". Nevertheless, the form's structure has been criticised; Paul Fussell considers the sestina to be of "dubious structural expressiveness" when composed in English and, irrespective of how it is used, "would seem to be [a form] that gives more structural pleasure to the contriver than to the apprehender."

Margaret Spanos highlights "a number of corresponding levels of tension and resolution" resulting from the structural form, including: structural, semantic and aesthetic tensions. She believes that the aesthetic tension, which results from the ""conception" of its mathematical completeness and perfection", set against the ""experiences" of its labyrinthine complexities" can be resolved in the apprehension of the "harmony of the whole."

The strength of the sestina, according to Stephen Fry, is the "repetition and recycling of elusive patterns that cannot be quite held in the mind all at once". For Shanna Compton, these patterns are easily discernible by newcomers to the form; she says that: "Even someone unfamiliar with the form's rules can tell by the end of the second stanza ... what's going on ...".

The 1972 television play "Between Time and Timbuktu", based on the writings of Kurt Vonnegut, was about a poet-astronaut who wanted to compose a sestina in outer space. Vonnegut wrote a sestina for the production.




</doc>
<doc id="28830" url="https://en.wikipedia.org/wiki?curid=28830" title="Song">
Song

A song is a musical composition intended to be vocally performed by the human voice. This is often done at distinct and fixed pitches (melodies) using patterns of sound and silence. Songs contain various forms, such as those including the repetition and variation of sections. 

Written words created specifically for music or for which music is specifically created, are called lyrics. If a pre-existing poem is set to composed music in classical music it is an art song. Songs that are sung on repeated pitches without distinct contours and patterns that rise and fall are called chants. Songs in a simple style that are learned informally "by ear" are often referred to as folk songs. Songs that are composed for professional singers who sell their recordings or live shows to the mass market are called popular songs. These songs, which have broad appeal, are often composed by professional songwriters, composers and lyricists. Art songs are composed by trained classical composers for concert or recital performances. Songs are performed live and recorded on audio or video (or, in some cases, a song may be performed live and simultaneously recorded). Songs may also appear in plays, musical theatre, stage shows of any form, and within operas, films, and TV shows.

A song may be for a solo singer, a lead singer supported by background singers, a duet, trio, or larger ensemble involving more voices singing in harmony, although the term is generally not used for large classical music vocal forms including opera and oratorio, which use terms such as aria and recitative instead. A song can be sung without accompaniment by instrumentalists (a cappella) or accompanied by instruments. In popular music, a singer may perform with an acoustic guitarist, pianist, organist, accordionist, or a backing band. In jazz, a singer may perform with a single pianist, a small combo (such as a trio or quartet), or with a big band. A Classical singer may perform with a single pianist, a small ensemble, or an orchestra. In jazz and blues, singers often learn songs "by ear" and they may improvise some melody lines. In Classical music, melodies are written by composers in sheet music format, so singers learn to read music.

Songs with more than one voice to a part singing in polyphony or harmony are considered choral works. Songs can be broadly divided into many different forms and types, depending on the criteria used. Through semantic widening, a broader sense of the word "song" may refer to instrumentals, such as 19th century "Song Without Words" pieces for solo piano.

Art songs are songs created for performance by classical artists, often with piano or other instrumental accompaniment, although they can be sung solo. Art songs require strong vocal technique, understanding of language, diction and poetry for interpretation. Though such singers may also perform popular or folk songs on their programs, these characteristics and the use of poetry are what distinguish art songs from popular songs. Art songs are a tradition from most European countries, and now other countries with classical music traditions. German-speaking communities use the term art song ("Kunstlied") to distinguish so-called "serious" compositions from folk song ("Volkslied"). The lyrics are often written by a poet or lyricist and the music separately by a composer. Art songs may be more formally complicated than popular or folk songs, though many early Lieder by the likes of Franz Schubert are in simple strophic form. The accompaniment of European art songs is considered as an important part of the composition. Some art songs are so revered that they take on characteristics of national identification.

Art songs emerge from the tradition of singing romantic love songs, often to an ideal or imaginary person and from religious songs. The troubadours and bards of Europe began the documented tradition of romantic songs, continued by the Elizabethan lutenists. Some of the earliest art songs are found in the music of Henry Purcell. The tradition of the romance, a love song with a flowing accompaniment, often in triple meter, entered opera in the 19th century, and spread from there throughout Europe. It spread into popular music and became one of the underpinnings of popular songs. While a romance generally has a simple accompaniment, art songs tend to have complicated, sophisticated accompaniments that underpin, embellish, illustrate or provide contrast to the voice. Sometimes the accompaniment performer has the melody, while the voice sings a more dramatic part.

Folk songs are songs of often anonymous origin (or are public domain) that are transmitted orally. They are frequently a major aspect of national or cultural identity. Art songs often approach the status of folk songs when people forget who the author was. Folk songs are also frequently transmitted non-orally (that is, as sheet music), especially in the modern era. Folk songs exist in almost every culture. Popular songs may eventually become folk songs by the same process of detachment from its source. Folk songs are more-or-less in the public domain by definition, though there are many folk song entertainers who publish and record copyrighted original material. This tradition led also to the singer-songwriter style of performing, where an artist has written confessional poetry or personal statements and sings them set to music, most often with guitar accompaniment.

There are many genres of popular songs, including torch songs, ballads, novelty songs, anthems, rock, blues and soul songs as well as indie music. Other commercial genres include rapping. Folk songs include ballads, lullabies, love songs, mourning songs, dance songs, work songs, ritual songs and many more.


Jean Nicolas De Surmont (2017), From vocal poetry to song, toward a Theory of Song Obects" with a foreword by Geoff Stahl, Stuttgart, Ibidem.


</doc>
<doc id="28833" url="https://en.wikipedia.org/wiki?curid=28833" title="Sir Gawain and the Green Knight">
Sir Gawain and the Green Knight

Sir Gawain and the Green Knight (Middle English: "Sir Gawayn and þe Grene Knyȝt") is a late 14th-century Middle English chivalric romance. It is one of the best known Arthurian stories, with its plot combining two types of folk motifs, the beheading game and the exchange of winnings. Written in stanzas of alliterative verse, each of which ends in a rhyming bob and wheel, it draws on Welsh, Irish and English stories, as well as the French chivalric tradition. It is an important example of a chivalric romance, which typically involves a hero who goes on a quest which tests his prowess. It remains popular in modern English renderings from J. R. R. Tolkien, Simon Armitage and others, as well as through film and stage adaptations.

It describes how Sir Gawain, a knight of King Arthur's Round Table, accepts a challenge from a mysterious "Green Knight" who dares any knight to strike him with his axe if he will take a return blow in a year and a day. Gawain accepts and beheads him with his blow, at which the Green Knight stands up, picks up his head and reminds Gawain of the appointed time. In his struggles to keep his bargain, Gawain demonstrates chivalry and loyalty until his honour is called into question by a test involving Lady Bertilak, the lady of the Green Knight's castle.

The poem survives in one manuscript, "Cotton Nero A.x.", which also includes three religious narrative poems: "Pearl", "Purity" and "Patience". All are thought to have been written by the same author, dubbed the "Pearl Poet" or "Gawain Poet", since all four are written in a North West Midland dialect of Middle English.

In Camelot on New Year's Day, King Arthur's court is exchanging gifts and waiting for the feasting to start when the king asks to see or hear of an exciting adventure. A gigantic figure, entirely green in appearance and riding a green horse, rides unexpectedly into the hall. He wears no armour but bears an axe in one hand and a holly bough in the other. Refusing to fight anyone there on the grounds that they are all too weak to take him on, he insists he has come for a friendly christmas game: someone is to strike him once with his axe on the condition that the Green Knight may return the blow in a year and a day. The splendid axe will belong to whoever accepts this deal. Arthur himself is prepared to accept the challenge when it appears no other knight will dare, but Sir Gawain, youngest of Arthur's knights and his nephew, asks for the honour instead. The giant bends and bares his neck before him and Gawain neatly beheads him in one stroke. However, the Green Knight neither falls nor falters, but instead reaches out, picks up his severed head and remounts, holding up his bleeding head to Queen Guinevere while its writhing lips remind Gawain that the two must meet again at the Green Chapel. He then rides away. Gawain and Arthur admire the axe, hang it up as a trophy and encourage Guinevere to treat the whole matter lightly.

As the date approaches, Sir Gawain sets off to find the Green Chapel and keep his side of the bargain. Many adventures and battles are alluded to (but not described) until Gawain comes across a splendid castle where he meets Bertilak de Hautdesert, the lord of the castle, and his beautiful wife, who are pleased to have such a renowned guest. Also present is an old and ugly lady, unnamed but treated with great honour by all. Gawain tells them of his New Year's appointment at the Green Chapel and that he only has a few days remaining. Bertilak laughs, explaining that there is a path that will take him to there less than two miles away, and proposes that Gawain rest at the castle until then. Relieved and grateful, Gawain agrees.

Before going hunting the next day, Bertilak proposes a bargain: he will give Gawain whatever he catches on the condition that Gawain give him whatever he might gain during the day. Gawain accepts. After Bertilak leaves, Lady Bertilak visits Gawain's bedroom and behaves seductively, but despite her best efforts he yields nothing but a single kiss in his unwillingness to offend her. When Bertilak returns and gives Gawain the deer he has killed, his guest gives a kiss to Bertilak without divulging its source. The next day the lady comes again, Gawain again courteously foils her advances, and later that day there is a similar exchange of a hunted boar for two kisses. She comes once more on the third morning, but once her advances are denied, she offers Gawain a gold ring as a keepsake. He gently but steadfastly refuses but she pleads that he at least take her belt, a girdle of green and gold silk. The belt, the lady assures him, is charmed and will keep him from all physical harm. Tempted, as he may otherwise die the next day, Gawain accepts it, and they exchange three kisses. The lady has Gawain swear that he will keep the gift secret from Bertilak. That evening, Bertilak returns with a fox, which he exchanges with Gawain for the three kisses – but Gawain says nothing of the girdle.

The next day, Gawain binds the belt twice around his waist. He finds the Green Knight sharpening an axe and, as promised, Gawain bends his bared neck to receive his blow. At the first swing, Gawain flinches slightly and the Green Knight belittles him for it. Ashamed of himself, Gawain doesn't flinch with the second swing; but again the Green Knight withholds the full force of his blow. The knight explains he was testing Gawain's nerve. Angrily Gawain tells him to deliver his blow and so the knight does, causing only a slight wound on Gawain's neck. The game is over. Gawain seizes his sword, helmet and shield, but the Green Knight, laughing, reveals himself to be the lord of the castle, Bertilak de Hautdesert, transformed by magic. He explains that the entire adventure was a trick of the "elderly lady" Gawain saw at the castle, who is actually the sorceress Morgan le Fay, Arthur's sister, who intended to test Arthur's knights and frighten Guinevere to death. Gawain is ashamed to have behaved deceitfully but the Green Knight laughs and professes him the most blameless knight in all the land. The two part on cordial terms. Gawain returns to Camelot wearing the girdle as a token of his failure to keep his promise. The Knights of the Round Table absolve him of blame and decide that henceforth each will wear a green sash in recognition of Gawain's adventure and as a reminder to be always honest.

Though the real name of "The "Gawain" Poet" (or poets) is unknown, some inferences about him can be drawn from an informed reading of his works. The manuscript of "Gawain" is known in academic circles as "Cotton Nero A.x.", following a naming system used by one of its owners, Robert Cotton, a collector of Medieval English texts. Before the "Gawain" manuscript came into Cotton's possession, it was in the library of Henry Savile in Yorkshire. Little is known about its previous ownership, and until 1824, when the manuscript was introduced to the academic community in a second edition of Thomas Warton's "History" edited by Richard Price, it was almost entirely unknown. Even then, the "Gawain" poem was not published in its entirety until 1839. Now held in the British Library, it has been dated to the late 14th century, meaning the poet was a contemporary of Geoffrey Chaucer, author of "The Canterbury Tales", though it is unlikely that they ever met. The three other works found in the same manuscript as "Gawain" (commonly known as "Pearl", "Patience", and "Purity" or "Cleanliness") are often considered to be written by the same author. However, the manuscript containing these poems was transcribed by a copyist and not by the original poet. Although nothing explicitly suggests that all four poems are by the same poet, comparative analysis of dialect, verse form, and diction have pointed towards single authorship.

What is known today about the poet is largely general. As J. R. R. Tolkien and E. V. Gordon, after reviewing the text's allusions, style, and themes, concluded in 1925:
The most commonly suggested candidate for authorship is John Massey of Cotton, Cheshire. He is known to have lived in the dialect region of the Gawain Poet and is thought to have written the poem "St. Erkenwald", which some scholars argue bears stylistic similarities to "Gawain". "St. Erkenwald", however, has been dated by some scholars to a time outside the Gawain Poet's era. Thus, ascribing authorship to John Massey is still controversial and most critics consider the Gawain Poet an unknown.

The 2,530 lines and 101 stanzas that make up "Sir Gawain and the Green Knight" are written in what linguists call the "Alliterative Revival" style typical of the 14th century. Instead of focusing on a metrical syllabic count and rhyme, the alliterative form of this period usually relied on the agreement of a pair of stressed syllables at the beginning of the line and another pair at the end. Each line always includes a pause, called a "caesura", at some point after the first two stresses, dividing it into two half-lines. Although he largely follows the form of his day, the Gawain poet was somewhat freer with convention than his or her predecessors. The poet broke the alliterative lines into variable-length groups and ended these nominal stanzas with a rhyming section of five lines known as the "bob and wheel", in which the "bob" is a very short line, sometimes of only two syllables, followed by the "wheel," longer lines with internal rhyme.

The earliest known story to feature a beheading game is the 8th-century Middle Irish tale "Bricriu's Feast". This story parallels "Gawain" in that, like the Green Knight, Cú Chulainn's antagonist feints three blows with the axe before letting his target depart without injury. A beheading exchange also appears in the late 12th-century "Life of Caradoc", a Middle French narrative embedded in the anonymous First Continuation of Chrétien de Troyes' "Perceval, the Story of the Grail". A notable difference in this story is that Caradoc's challenger is his father in disguise, come to test his honour. Lancelot is given a beheading challenge in the early 13th-century "Perlesvaus", in which a knight begs him to chop off his head or else put his own in jeopardy. Lancelot reluctantly cuts it off, agreeing to come to the same place in a year to put his head in the same danger. When Lancelot arrives, the people of the town celebrate and announce that they have finally found a true knight, because many others had failed this test of chivalry.

The stories "The Girl with the Mule" (alternately titled "The Mule Without a Bridle") and "Hunbaut" feature Gawain in beheading game situations. In "Hunbaut," Gawain cuts off a man's head and, before he can replace it, removes the magic cloak keeping the man alive, thus killing him. Several stories tell of knights who struggle to stave off the advances of voluptuous women sent by their lords as a test; these stories include "Yder", the Lancelot-Grail, "Hunbaut", and "The Knight of the Sword". The last two involve Gawain specifically. Usually the temptress is the daughter or wife of a lord to whom the knight owes respect, and the knight is tested to see whether or not he will remain chaste in trying circumstances.

In the first branch of the medieval Welsh collection of tales known as The Four Branches of the Mabinogi, Pwyll exchanges places for a year with Arawn, the lord of Annwn (the Otherworld). Despite having his appearance changed to resemble Arawn exactly, Pwyll does not have sexual relations with Arawn's wife during this time, thus establishing a lasting friendship between the two men. This story may, then, provide a background to Gawain's attempts to resist the wife of the Green Knight; thus, the story of Sir Gawain and the Green Knight may be seen as a tale which combines elements of the Celtic beheading game and seduction test stories. Additionally, in both stories a year passes before the completion of the conclusion of the challenge or exchange. Some scholars disagree with this interpretation, however, as Arawn seems to have accepted the notion that Pwyll may reciprocate with his wife, making it less of a "seduction test" per se, as seduction tests typically involve a Lord and Lady conspiring to seduce a knight, seemingly "against" the wishes of the Lord.

After the writing of "Sir Gawain and the Green Knight", several similar stories followed. "The Greene Knight" (15th–17th century) is a rhymed retelling of nearly the same tale. In it, the plot is simplified, motives are more fully explained, and some names are changed. Another story, "The Turke and Gowin" (15th century), begins with a Turk entering Arthur's court and asking, "Is there any will, as a brother, To give a buffett and take another?" At the end of this poem the Turk, rather than buffeting Gawain back, asks the knight to cut off his head, which Gawain does. The Turk then praises Gawain and showers him with gifts. "The Carle of Carlisle" (17th century) also resembles "Gawain" in a scene in which the Carle (Churl), a lord, takes Sir Gawain to a chamber where two swords are hanging and orders Gawain to cut off his head or suffer his own to be cut off. Gawain obliges and strikes, but the Carle rises, laughing and unharmed. Unlike the "Gawain" poem, no return blow is demanded or given.

At the heart of "Sir Gawain and the Green Knight" is the test of Gawain's adherence to the code of chivalry. The typical temptation fable of medieval literature presents a series of tribulations assembled as tests or "proofs" of moral virtue. The stories often describe several individuals' failures after which the main character is tested. Success in the proofs will often bring immunity or good fortune. Gawain's ability to pass the tests of his host are of utmost importance to his survival, though he does not know it. It is only by fortuity or "instinctive-courtesy" that Sir Gawain is able to pass his test. Gawain does not realize, however, that these tests are all orchestrated by Sir Bertilak.
In addition to the laws of chivalry, Gawain must respect another set of laws concerning courtly love. The knight's code of honour requires him to do whatever a damsel asks. Gawain must accept the girdle from the Lady, but he must also keep the promise he has made to his host that he will give whatever he gains that day. Gawain chooses to keep the girdle out of fear of death, thus breaking his promise to the host but honouring the lady. Upon learning that the Green Knight is actually his host (Bertilak), he realises that although he has completed his quest, he has failed to be virtuous. This test demonstrates the conflict between honour and knightly duties. In breaking his promise, Gawain believes he has lost his honour and failed in his duties.

Scholars have frequently noted the parallels between the three hunting scenes and the three seduction scenes in "Gawain". They are generally agreed that the fox chase has significant parallels to the third seduction scene, in which Gawain accepts the girdle from Bertilak's wife. Gawain, like the fox, fears for his life and is looking for a way to avoid death from the Green Knight's axe. Like his counterpart, he resorts to trickery in order to save his skin. The fox uses tactics so unlike the first two animals, and so unexpectedly, that Bertilak has the hardest time hunting it. Similarly, Gawain finds the Lady's advances in the third seduction scene more unpredictable and challenging to resist than her previous attempts. She changes her evasive language, typical of courtly love relationships, to a more assertive style. Her dress, relatively modest in earlier scenes, is suddenly voluptuous and revealing.

The deer- and boar-hunting scenes are less clearly connected, although scholars have attempted to link each animal to Gawain's reactions in the parallel seduction scene. Attempts to connect the deer hunt with the first seduction scene have unearthed a few parallels. Deer hunts of the time, like courtship, had to be done according to established rules. Women often favoured suitors who hunted well and skinned their animals, sometimes even watching while a deer was cleaned. The sequence describing the deer hunt is relatively unspecific and nonviolent, with an air of relaxation and exhilaration. The first seduction scene follows in a similar vein, with no overt physical advances and no apparent danger; the entire exchange is humorously portrayed.

The boar-hunting scene is, in contrast, laden with detail. Boars were (and are) much more difficult to hunt than deer; approaching one with only a sword was akin to challenging a knight to single combat. In the hunting sequence, the boar flees but is cornered before a ravine. He turns to face Bertilak with his back to the ravine, prepared to fight. Bertilak dismounts and in the ensuing fight kills the boar. He removes its head and displays it on a pike. In the seduction scene, Bertilak's wife, like the boar, is more forward, insisting that Gawain has a romantic reputation and that he must not disappoint her. Gawain, however, is successful in parrying her attacks, saying that surely she knows more than he about love. Both the boar hunt and the seduction scene can be seen as depictions of a moral victory: both Gawain and Bertilak face struggles alone and emerge triumphant.
Masculinity has also been associated with hunting. The theme of masculinity is present throughout. In an article by Vern L. Bullough, "Being a Male in the Middle Ages," he discusses Sir Gawain and how normally, masculinity is often viewed in terms of being sexually active. He notes that Sir Gawain is not part of this normalcy.

Some argue that nature represents a chaotic, lawless order which is in direct confrontation with the civilisation of Camelot throughout "Sir Gawain and the Green Knight". The green horse and rider that first invade Arthur's peaceful halls are iconic representations of nature's disturbance. Nature is presented throughout the poem as rough and indifferent, constantly threatening the order of men and courtly life. Nature invades and disrupts order in the major events of the narrative, both symbolically and through the inner nature of humanity. This element appears first with the disruption caused by the Green Knight, later when Gawain must fight off his natural lust for Bertilak's wife, and again when Gawain breaks his vow to Bertilak by choosing to keep the green girdle, valuing survival over virtue. Represented by the sin-stained girdle, nature is an underlying force, forever within man and keeping him imperfect (in a chivalric sense). In this view, Gawain is part of a wider conflict between nature and chivalry, an examination of the ability of man's order to overcome the chaos of nature.

Several critics have made exactly the opposite interpretation, reading the poem as a comic critique of the Christianity of the time, particularly as embodied in the Christian chivalry of Arthur's court. In its zeal to extirpate all traces of paganism, Christianity had cut itself off from the sources of life in nature and the female. The green girdle represents all the pentangle lacks. The Arthurian enterprise is doomed unless it can acknowledge the unattainability of the ideals of the Round Table, and, for the sake of realism and wholeness, recognize and incorporate the pagan values represented by the Green Knight.

The chivalry that is represented within 'Gawain' is one which was constructed by court nobility. The violence that is part of this chivalry is steeply contrasted by the fact that King Arthur's court is Christian and the initial beheading event takes place while celebrating Christmas. The violence of an act of beheading seems to be counterintuitive to chivalric and Christian ideals, and yet it is seen as part of knighthood.

The question of politeness and chivalry is a main theme during Gawain's interactions with Bertilak's wife. He cannot accept her advances or else lose his honour, and yet he cannot utterly refuse her advances or else risk upsetting his hostess. Gawain plays a very fine line and the only part where he appears to fail is when he conceals the green girdle from Bertilak.

The word "gomen" (game) is found 18 times in "Gawain". Its similarity to the word "gome" (man), which appears 21 times, has led some scholars to see men and games as centrally linked. Games at this time were seen as tests of worthiness, as when the Green Knight challenges the court's right to its good name in a "Christmas game". The "game" of exchanging gifts was common in Germanic cultures. If a man received a gift, he was obliged to provide the giver with a better gift or risk losing his honour, almost like an exchange of blows in a fight (or in a "beheading game"). The poem revolves around two games: an exchange of beheading and an exchange of winnings. These appear at first to be unconnected. However, a victory in the first game will lead to a victory in the second. Elements of both games appear in other stories; however, the linkage of outcomes is unique to "Gawain".

Times, dates, seasons, and cycles within "Gawain" are often noted by scholars because of their symbolic nature. The story starts on New Year's Eve with a beheading and culminates on the next New Year's Day. Gawain leaves Camelot on All Saints Day and arrives at Bertilak's castle on Christmas Eve. Furthermore, the Green Knight tells Gawain to meet him at the Green Chapel in "a year and a day"—a period of time seen often in medieval literature. Some scholars interpret the yearly cycles, each beginning and ending in winter, as the poet's attempt to convey the inevitable fall of all things good and noble in the world. Such a theme is strengthened by the image of Troy, a powerful nation once thought to be invincible which, according to the "Aeneid", fell to the Greeks due to pride and ignorance. The Trojan connection shows itself in the presence of two virtually identical descriptions of Troy's destruction. The poem's first line reads: "Since the siege and the assault were ceased at Troy" and the final stanzaic line (before the bob and wheel) is "After the siege and the assault were ceased at Troy".

The entire 'Gawain' poem follows one individual experiencing highly emotional situations. He participates in the beheading contest, watches as a man he has beheaded walks away unscathed, prepares for a journey where he will then also receive a blow that will behead him, is tempted by the sexual advances of Sir Bertilak's wife, decides what to do with the moral conundrum that is the girdle, suffering humiliation, and returning to court to retell his entire adventure. These events invite the reader to empathize with Gawain, the flawed hero, and understand that he is also human.

Humans experience an emotional contagion, which was defined by psychologists Elaine Hatfield, John Cacioppo, and Richard Rapson as 'the tendency to automatically mimic and synchronize expressions, vocalizations, postures, and movements with those of another person, and, consequently, to converge emotionally.' Amy Coplan explains that emotional contagion is something that happens so quickly and automatically that we are typically unaware of it happening. The poet capitalized on this emotional reaction and thus has the reader empathizing with Sir Gawain almost without realizing it.

Given the varied and even contradictory interpretations of the colour green, its precise meaning in the poem remains ambiguous. In English folklore and literature, green was traditionally used to symbolise nature and its associated attributes: fertility and rebirth. Stories of the medieval period also used it to allude to love and the base desires of man. Because of its connection with faeries and spirits in early English folklore, green also signified witchcraft, devilry and evil. It can also represent decay and toxicity. When combined with gold, as with the Green Knight and the girdle, green was often seen as representing youth's passing. In Celtic mythology, green was associated with misfortune and death, and therefore avoided in clothing. The green girdle, originally worn for protection, became a symbol of shame and cowardice; it is finally adopted as a symbol of honour by the knights of Camelot, signifying a transformation from good to evil and back again; this displays both the spoiling and regenerative connotations of the colour green.

Scholars have puzzled over the Green Knight's symbolism since the discovery of the poem. British medievalist C. S. Lewis said the character was "as vivid and concrete as any image in literature" and J. R. R. Tolkien said he was the "most difficult character" to interpret in "Sir Gawain". His major role in Arthurian literature is that of a judge and tester of knights, thus he is at once terrifying, friendly, and mysterious. He appears in only two other poems: "The Greene Knight" and "King Arthur and King Cornwall". Scholars have attempted to connect him to other mythical characters, such as Jack in the green of English tradition and to Al-Khidr, but no definitive connection has yet been established.

However, there is a possibility, as Alice Buchanan has argued, that the colour green is erroneously attributed to the Green Knight due to the poet's mistranslation or misunderstanding of the Irish word "glas", which could either mean grey or green. In the Death of Curoi (one of the Irish stories from Bricriu's Feast), Curoi stands in for Bertilak, and is often called "the man of the grey mantle". Though the words usually used for grey in the Death of Curoi are "lachtna" or "odar", roughly meaning milk-coloured and shadowy respectively, in later works featuring a green knight, the word "glas" is used and may have been the basis of misunderstanding.

The girdle's symbolic meaning, in "Sir Gawain and the Green Knight", has been construed in a variety of ways. Interpretations range from sexual to spiritual. Those who argue for the sexual inference view the girdle as a "trophy". It is not entirely clear if the "winner" is Sir Gawain or Lady Bertilak. The girdle is given to Gawain by Lady Bertilak to keep him safe when he confronts the Green Knight. When Lord Bertilak comes home from his hunting trip, Gawain does not reveal the girdle to his host, instead he hides it. This introduces the spiritual interpretation, that Gawain's acceptance of the girdle is a sign of his faltering faith in God, at least in the face of death. To some, the Green Knight is Christ, who overcomes death, while Gawain is the Every Christian, who in his struggles to follow Christ faithfully, chooses the easier path. In "Sir Gawain", the easier choice is the girdle, which promises what Gawain most desires. Faith in God, alternatively, requires one's acceptance that what one most desires does not always coincide with what God has planned. It is arguably best to view the girdle not as an either–or situation, but as a complex, multi-faceted symbol that acts to test Gawain in more ways than one. While Gawain is able to resist Bertilak's wife's sexual advances, he is unable to resist the powers of the girdle. Gawain is operating under the laws of chivalry which, evidently, have rules that can contradict each other. In the story of "Sir Gawain", Gawain finds himself torn between doing what a damsel asks (accepting the girdle) and keeping his promise (returning anything given to him while his host is away). 

The poem contains the first recorded use of the word "pentangle" in English. It contains the only representation of such a symbol on Gawain's shield in the Gawain literature. What is more, the poet uses a total of 46 lines in order to describe the meaning of the pentangle; no other symbol in the poem receives as much attention or is described in such detail. The poem describes the pentangle as a symbol of faithfulness and an "endless knot". From lines 640 to 654, the five points of the pentangle relate directly to Gawain in five ways: five senses, his five fingers, his faith found in the five wounds of Christ, the five joys of Mary (whose face was on the inside of the shield) and finally friendship, fraternity, purity, politeness and pity (traits that Gawain possessed around others). In line 625, it is described as "a sign by Solomon". Solomon, the third king of Israel, in the 10th century BC, was said to have the mark of the pentagram on his ring, which he received from the archangel Michael. The pentagram seal on this ring was said to give Solomon power over demons.

Along these lines, some academics link the Gawain pentangle to magical traditions. In Germany, the symbol was called a "Drudenfuß" and was placed on household objects to keep out evil. The symbol was also associated with magical charms that, if recited or written on a weapon, would call forth magical forces. However, concrete evidence tying the magical pentagram to Gawain's pentangle is scarce.

Gawain's pentangle also symbolises the "phenomenon of physically endless objects signifying a temporally endless quality." Many poets use the symbol of the circle to show infinity or endlessness, but Gawain's poet insisted on using something more complex. In medieval number theory, the number five is considered a "circular number", since it "reproduces itself in its last digit when raised to its powers". Furthermore, it replicates itself geometrically; that is, every pentangle has a smaller pentagon that allows a pentangle to be embedded in it and this "process may be repeated forever with decreasing pentangles". Thus, by reproducing the number five, which in medieval number symbolism signified incorruptibility, Gawain's pentangle represents his eternal incorruptibility.

Gawain's refusal of the Lady Bertilak's ring has major implications for the remainder of the story. While the modern student may tend to pay more attention to the girdle as the eminent object offered by the lady, readers in the time of Gawain would have noticed the significance of the offer of the ring as they believed that rings, and especially the embedded gems, had talismanic properties similarly done by the Gawain-poet in "Pearl". This is especially true of the lady's ring as scholars believe it to be a ruby or carbuncle, indicated when the Gawain-Poet describes it as a "brygt sunne" (line 1819), a "fiery sun." This red colour can be seen as symbolizing royalty, divinity, and the Passion of the Christ, something that Gawain as a knight of the Round Table would strive for, but this colour could also represent the negative qualities of temptation and covetousness. Given the importance of magic rings in Arthurian romance, this remarkable ring would also have been believed to protect the wearer from harm just as Lady Bertilak claims the girdle will.

The poet highlights number symbolism to add symmetry and meaning to the poem. For example, three kisses are exchanged between Gawain and Bertilak's wife; Gawain is tempted by her on three separate days; Bertilak goes hunting three times, and the Green Knight swings at Gawain three times with his axe. The number two also appears repeatedly, as in the two beheading scenes, two confession scenes, and two castles. The five points of the pentangle, the poet adds, represent Gawain's virtues, for he is "faithful five ways and five times each". The poet goes on to list the ways in which Gawain is virtuous: all five of his senses are without fault; his five fingers never fail him, and he always remembers the five wounds of Christ, as well as the five joys of the Virgin Mary. The fifth five is Gawain himself, who embodies the five moral virtues of the code of chivalry: "friendship, generosity, chastity, courtesy, and piety". All of these virtues reside, as the poet says, in the "Endless Knot" of the pentangle, which forever interlinks and is never broken. This intimate relationship between symbol and faith allows for rigorous allegorical interpretation, especially in the physical role that the shield plays in Gawain's quest. Thus, the poet makes Gawain the epitome of perfection in knighthood through number symbolism.
The number five is also found in the structure of the poem itself. "Sir Gawain" is 101 stanzas long, traditionally organised into four 'Fits' of 21, 24, 34, and 22 stanzas. These divisions, however, have since been disputed; scholars have begun to believe that they are the work of the copyist and not of the poet. The surviving manuscript features a series of capital letters added after the fact by another scribe, and some scholars argue that these additions were an attempt to restore the original divisions. These letters divide the manuscript into nine parts. The first and last parts are 22 stanzas long. The second and second-to-last parts are only one stanza long, and the middle five parts are eleven stanzas long. The number eleven is associated with transgression in other medieval literature (being one more than ten, a number associated with the Ten Commandments). Thus, this set of five elevens (55 stanzas) creates the perfect mix of transgression and incorruption, suggesting that Gawain is faultless in his faults.

At the story's climax, Gawain is wounded superficially in the neck by the Green Knight's axe. During the medieval period, the body and the soul were believed to be so intimately connected that wounds were considered an outward sign of inward sin. The neck, specifically, was believed to correlate with the part of the soul related to will, connecting the reasoning part (the head) and the courageous part (the heart). Gawain's sin resulted from using his will to separate reasoning from courage. By accepting the girdle from the lady, he employs reason to do something less than courageous—evade death in a dishonest way. Gawain's wound is thus an outward sign of an internal wound. The Green Knight's series of tests shows Gawain the weakness that has been in him all along: the desire to use his will pridefully for personal gain, rather than submitting his will in humility to God. The Green Knight, by engaging with the greatest knight of Camelot, also reveals the moral weakness of pride in all of Camelot, and therefore all of humanity. However, the wounds of Christ, believed to offer healing to wounded souls and bodies, are mentioned throughout the poem in the hope that this sin of prideful "stiffneckedness" will be healed among fallen mortals.

Many critics argue that "Sir Gawain and the Green Knight" should be viewed, above all, as a romance. Medieval romances typically recount the marvellous adventures of a chivalrous, heroic knight, often of super-human ability, who abides by chivalry's strict codes of honour and demeanour, embarks upon a quest and defeats monsters, thereby winning the favour of a lady. Thus, medieval romances focus not on love and sentiment (as the term "romance" implies today), but on adventure.

Gawain's function, as medieval scholar Alan Markman says, "is the function of the romance hero … to stand as the champion of the human race, and by submitting to strange and severe tests, to demonstrate human capabilities for good or bad action." Through Gawain's adventure, it becomes clear that he is merely human. The reader becomes attached to this human view in the midst of the poem's romanticism, relating to Gawain's humanity while respecting his knightly qualities. Gawain "shows us what moral conduct is. We shall probably not equal his behaviour, but we admire him for pointing out the way."

In viewing the poem as a medieval romance, many scholars see it as intertwining chivalric and courtly love laws under the English Order of the Garter. The group's motto, 'honi soit qui mal y pense', or "Shamed be he who finds evil here," is written at the end of the poem. Some critics describe Gawain's peers wearing girdles of their own as evidence of the origin of the Order of the Garter. However, in the parallel poem "The Greene Knight", the lace is white, not green, and is considered the origin of the collar worn by the knights of the Bath, not the Order of the Garter. The motto on the poem was probably written by a copyist and not by the original author. Still, the connection made by the copyist to the Order is not extraordinary.

The poem is in many ways deeply Christian, with frequent references to the fall of Adam and Eve and to Jesus Christ. Scholars have debated the depth of the Christian elements within the poem by looking at it in the context of the age in which it was written, coming up with varying views as to what represents a Christian element of the poem and what does not. For example, some critics compare "Sir Gawain" to the other three poems of the "Gawain" manuscript. Each has a heavily Christian theme, causing scholars to interpret "Gawain" similarly. Comparing it to the poem "Cleanness" (also known as "Purity"), for example, they see it as a story of the apocalyptic fall of a civilisation, in "Gawain's" case, Camelot. In this interpretation, Sir Gawain is like Noah, separated from his society and warned by the Green Knight (who is seen as God's representative) of the coming doom of Camelot. Gawain, judged worthy through his test, is spared the doom of the rest of Camelot. King Arthur and his knights, however, misunderstand Gawain's experience and wear garters themselves. In "Cleanness" the men who are saved are similarly helpless in warning their society of impending destruction.

One of the key points stressed in this interpretation is that salvation is an individual experience difficult to communicate to outsiders. In his depiction of Camelot, the poet reveals a concern for his society, whose inevitable fall will bring about the ultimate destruction intended by God. "Gawain" was written around the time of the Black Death and Peasants' Revolt, events which convinced many people that their world was coming to an apocalyptic end and this belief was reflected in literature and culture. However, other critics see weaknesses in this view, since the Green Knight is ultimately under the control of Morgan le Fay, often viewed as a figure of evil in Camelot tales. This makes the knight's presence as a representative of God problematic.

While the character of the Green Knight is usually not viewed as a representation of Christ in "Sir Gawain and the Green Knight", critics do acknowledge a parallel. Lawrence Besserman, a specialist in medieval literature, explains that "the Green Knight is not a figurative representative of Christ. But the idea of Christ's divine/human nature provides a medieval conceptual framework that supports the poet's serious/comic account of the Green Knight's supernatural/human qualities and actions." This duality exemplifies the influence and importance of Christian teachings and views of Christ in the era of the Gawain Poet.

Furthermore, critics note the Christian reference to Christ's crown of thorns at the conclusion of "Sir Gawain and the Green Knight". After Gawain returns to Camelot and tells his story regarding the newly acquired green sash, the poem concludes with a brief prayer and a reference to "the thorn-crowned God". Besserman theorises that "with these final words the poet redirects our attention from the circular girdle-turned-sash (a double image of Gawain's "yntrawpe/renoun") to the circular Crown of Thorns (a double image of Christ's humiliation turned triumph)."

Throughout the poem, Gawain encounters numerous trials testing his devotion and faith in Christianity. When Gawain sets out on his journey to find the Green Chapel, he finds himself lost, and only after praying to the Virgin Mary does he find his way. As he continues his journey, Gawain once again faces anguish regarding his inevitable encounter with the Green Knight. Instead of praying to Mary, as before, Gawain places his faith in the girdle given to him by Bertilak's wife. From the Christian perspective, this leads to disastrous and embarrassing consequences for Gawain as he is forced to reevaluate his faith when the Green Knight points out his betrayal. Another interpretation sees the work in terms of the perfection of virtue, with the pentangle representing the moral perfection of the connected virtues, the Green Knight as Christ exhibiting perfect fortitude, and Gawain as slightly imperfect in fortitude by virtue of flinching when under the threat of death.

An analogy is also made between Gawain's trial and the Biblical test that Adam encounters in the Garden of Eden. Adam succumbs to Eve just as Gawain surrenders to Bertilak's wife by accepting the girdle. Although Gawain sins by putting his faith in the girdle and not confessing when he is caught, the Green Knight pardons him, thereby allowing him to become a better Christian by learning from his mistakes. Through the various games played and hardships endured, Gawain finds his place within the Christian world.

Feminist literary critics see the poem as portraying women's ultimate power over men. Morgan le Fay and Bertilak's wife, for example, are the most powerful characters in the poem—Morgan especially, as she begins the game by enchanting the Green Knight. The girdle and Gawain's scar can be seen as symbols of feminine power, each of them diminishing Gawain's masculinity. Gawain's misogynist passage, in which he blames all of his troubles on women and lists the many men who have fallen prey to women's wiles, further supports the feminist view of ultimate female power in the poem.

In contrast, others argue that the poem focuses mostly on the opinions, actions, and abilities of men. For example, on the surface, it appears that Bertilak's wife is a strong leading character. By adopting the masculine role, she appears to be an empowered individual, particularly in the bedroom scene. This is not entirely the case, however. While the Lady is being forward and outgoing, Gawain's feelings and emotions are the focus of the story, and Gawain stands to gain or lose the most. The Lady "makes the first move", so to speak, but Gawain ultimately decides what is to become of those actions. He, therefore, is in charge of the situation and even the relationship.

In the bedroom scene, both the negative and positive actions of the Lady are motivated by her desire. Her feelings cause her to step out of the typical female role and into that of the male, thus becoming more empowered. At the same time, those same actions make the Lady appear adulterous; some scholars compare her with Eve in the Bible. By forcing Gawain to take her girdle, i.e. the apple, the pact made with Bertilak—and therefore the Green Knight—is broken. In this sense, it is clear that at the hands of the Lady, Gawain is a "good man seduced".

From 1350 to 1400—the period in which the poem is thought to have been written—Wales experienced several raids at the hands of the English, who were attempting to colonise the area. The Gawain poet uses a North West Midlands dialect common on the Welsh–English border, potentially placing him in the midst of this conflict. Patricia Clare Ingham is credited with first viewing the poem through the lens of postcolonialism, and since then a great deal of dispute has emerged over the extent to which colonial differences play a role in the poem. Most critics agree that gender plays a role, but differ about whether gender supports the colonial ideals or replaces them as English and Welsh cultures interact in the poem.

A large amount of critical debate also surrounds the poem as it relates to the bi-cultural political landscape of the time. Some argue that Bertilak is an example of the hybrid Anglo-Welsh culture found on the Welsh–English border. They therefore view the poem as a reflection of a hybrid culture that plays strong cultures off one another to create a new set of cultural rules and traditions. Other scholars, however, argue that historically much Welsh blood was shed well into the 14th century, creating a situation far removed from the more friendly hybridisation suggested by Ingham. To support this argument further, it is suggested that the poem creates an "us versus them" scenario contrasting the knowledgeable civilised English with the uncivilised borderlands that are home to Bertilak and the other monsters that Gawain encounters.

In contrast to this perception of the colonial lands, others argue that the land of Hautdesert, Bertilak's territory, has been misrepresented or ignored in modern criticism. They suggest that it is a land with its own moral agency, one that plays a central role in the story. Bonnie Lander, for example, argues that the denizens of Hautdesert are "intelligently immoral", choosing to follow certain codes and rejecting others, a position which creates a "distinction … of moral insight versus moral faith". Lander thinks that the border dwellers are more sophisticated because they do not unthinkingly embrace the chivalric codes but challenge them in a philosophical, and—in the case of Bertilak's appearance at Arthur's court—literal sense. Lander's argument about the superiority of the denizens of Hautdesert hinges on the lack of self-awareness present in Camelot, which leads to an unthinking populace that frowns on individualism. In this view, it is not Bertilak and his people, but Arthur and his court, who are the monsters.

Several scholars have attempted to find a real-world correspondence for Gawain's journey to the Green Chapel. The Anglesey islands, for example, are mentioned in the poem. They exist today as a single island off the coast of Wales. In line 700, Gawain is said to pass the "Holy Head", believed by many scholars to be either Holywell or the Cistercian abbey of Poulton in Pulford. Holywell is associated with the beheading of Saint Winifred. As the story goes, Winifred was a virgin who was beheaded by a local leader after she refused his sexual advances. Her uncle, another saint, put her head back in place and healed the wound, leaving only a white scar. The parallels between this story and Gawain's make this area a likely candidate for the journey.

Gawain's trek leads him directly into the centre of the Pearl Poet's dialect region, where the candidates for the locations of the Castle at Hautdesert and the Green Chapel stand. Hautdesert is thought to be in the area of Swythamley in northwest Midland, as it lies in the writer's dialect area and matches the topographical features described in the poem. The area is also known to have housed all of the animals hunted by Bertilak (deer, boar, fox) in the 14th century. The Green Chapel is thought to be in either Lud's Church or Wetton Mill, as these areas closely match the descriptions given by the author. Ralph Elliott located the chapel ("two myle henne" v1078) from the old manor house at Swythamley Park at the bottom of a valley ("bothm of the brem valay" v2145) on a hillside ("loke a littel on the launde, on thi lyfte honde" v2147) in an enormous fissure ("an olde caue,/or a creuisse of an olde cragge" v2182-83). Several have tried to replicate this expedition and others such as Michael Twomey have created a virtual tour of Gawain's journey entitled 'Travels with Sir Gawain' that include photographs of landscapes mentioned and particular views mentioned in the text.

According to medieval scholar Richard Zeikowitz, the Green Knight represents a threat to homosocial friendship in his medieval world. Zeikowitz argues that the narrator of the poem seems entranced by the Knight's beauty, homoeroticising him in poetic form. The Green Knight's attractiveness challenges the homosocial rules of King Arthur's court and poses a threat to their way of life. Zeikowitz also states that Gawain seems to find Bertilak as attractive as the narrator finds the Green Knight. Bertilak, however, follows the homosocial code and develops a friendship with Gawain. Gawain's embracing and kissing Bertilak in several scenes thus represents not a homosexual but a homosocial expression. Men of the time often embraced and kissed and this was acceptable under the chivalric code. Nonetheless, the Green Knight blurs the lines between homosociality and homosexuality, representing the difficulty medieval writers sometimes had in separating the two.

Carolyn Dinshaw argues that the poem may have been a response to accusations that Richard II had a male lover—an attempt to reestablish the idea that heterosexuality was the Christian norm. Around the time the poem was written, the Catholic Church was beginning to express concerns about kissing between males. Many religious figures were trying to make the distinction between strong trust and friendship between males and homosexuality. Still, the Pearl Poet seems to have been simultaneously entranced and repulsed by homosexual desire. In his other poem "Cleanness", he points out several grievous sins, but spends lengthy passages describing them in minute detail. His obsession seems to carry into "Gawain" in his descriptions of the Green Knight.

Beyond this, Dinshaw proposes that Gawain can be read as a woman-like figure. He is the passive one in the advances of Lady Bertilak, as well as in his encounters with Lord Bertilak, where he acts the part of a woman in kissing the man. However, while the poem does have homosexual elements, these elements are brought up by the poet in order to establish heterosexuality as the normal lifestyle of Gawain's world. The poem does this by making the kisses between Lady Bertilak and Gawain sexual in nature, but rendering the kisses between Gawain and Lord Bertilak "unintelligible" to the medieval reader. In other words, the poet portrays kisses between a man and a woman as having the possibility of leading to sex, while in a heterosexual world kisses between a man and a man are portrayed as having no such possibility.

Though the surviving manuscript dates from the fourteenth century, the first published version of the poem did not appear until as late as 1839, when Sir Frederic Madden of the British Museum recognized the poem as worth reading. Madden's scholarly, Middle English edition of the poem was followed in 1898 by the first Modern English translation – a prose version by literary scholar Jessie L. Weston. In 1925, J. R. R. Tolkien and E. V. Gordon published a scholarly edition of the Middle English text of "Sir Gawain and the Green Knight"; a revised edition of this text was prepared by Norman Davis and published in 1967. The book, featuring a text in Middle English with extensive scholarly notes, is frequently confused with the translation into Modern English that Tolkien prepared, along with translations of "Pearl" and "Sir Orfeo", late in his life. Many editions of the latter work, first published in 1975, shortly after his death, list Tolkien on the cover as author rather than translator.

For students, especially undergraduate students, the text is usually given in translation. Notable translators include Jessie Weston, whose 1898 prose translation and 1907 poetic translation took many liberties with the original; Theodore Banks, whose 1929 translation was praised for its adaptation of the language to modern usage; and Marie Borroff, whose imitative translation was first published in 1967 and "entered the academic canon" in 1968, in the second edition of the "Norton Anthology of English Literature". In 2010, her (slightly revised) translation was published as a Norton Critical Edition, with a foreword by Laura Howes. In 2007, Simon Armitage, who grew up near the Gawain poet's purported residence, published a translation which attracted attention in the US and the United Kingdom, and was published in the United States by Norton, which replaced Borroff's translation with Armitage's for the ninth edition of the "Norton Anthology of English Literature". Other modern translations include those by Larry Benson, Brian Stone, James Winny, Helen Cooper, W. S. Merwin, Jacob Rosenberg, William Vantuono, Joseph Glaser, Bernard O'Donoghue, John Gardner, and Francis Ingledew. In 2015, Zach Weinersmith published "Augie and the Green Knight", a children's adaptation in which the protagonist is a young girl. In 2017, the graphic novel adaptation "Gawain and the Green Knight" was self-published by Emily Cheeseman. An illustrated contextual translation of the work by historian and printmaker Michael Smith was published in July 2018 by Unbound. The young adult novel "The Squire, His Knight, and His Lady" by Gerald Morris includes a faithful adaptation of the story.

The poem has been adapted to film twice, on both occasions by writer-director Stephen Weeks: first as "Gawain and the Green Knight" in 1973 and again in 1984 as "", featuring Miles O'Keeffe as Gawain and Sean Connery as the Green Knight. Both films have been criticised for deviating from the poem's plot. Also, Bertilak and the Green Knight are never connected. French/Australian director Martin Beilby directed a short (30') film adaptation in 2014. There have been at least two television adaptations, "Gawain and the Green Knight" in 1991 and the animated "Sir Gawain and the Green Knight" in 2002. The BBC broadcast a documentary presented by Simon Armitage in which the journey depicted in the poem is traced, using what are believed to be the actual locations. On 5 November 2018, it was announced that a new film adaptation titled "Green Knight" is in the works, to be directed by American filmmaker David Lowery for A24.

The poem was also adapted for the "Adventure Time" episode "Seventeen" where the Green Knight arrives on Finn's 17th birthday, issuing him the challenge, as well as a series of follow-up games.

The Tyneside Theatre company presented a stage version of "Sir Gawain and the Green Knight" at the University Theatre, Newcastle at Christmas 1971. It was directed by Michael Bogdanov and adapted for the stage from the translation by Brian Stone. The music and lyrics were composed by Iwan Williams using medieval carols, such as the "Boar's Head Carol", as inspiration and folk instruments such as the Northumbrian pipes, whistles and bhodran to create a "rough" feel. Stone had referred Bogdanov to "Cuchulain and the Beheading Game", a sequence which is contained in The Grenoside Sword dance. Bogdanov found the pentangle theme to be contained in most sword dances, and so incorporated a long sword dance while Gawain lay tossing uneasily before getting up to go to the Green Chapel. The dancers made the knot of the pentangle around his drowsing head with their swords. The interlacing of the hunting and wooing scenes was achieved by frequent cutting of the action from hunt to bed-chamber and back again, while the locale of both remained on-stage.

In 1992 Simon Corble created an adaptation with medieval songs and music for The Midsommer Actors' Company. performed as walkabout productions in the summer 1992 at Thurstaston Common and Beeston Castle and in August 1995 at Brimham Rocks, North Yorkshire. Corble later wrote a substantially revised version which was produced indoors at the O'Reilly Theatre, Oxford in February 2014.

"Sir Gawain and the Green Knight" was first adapted as an opera in 1978 by the composer Richard Blackford on commission from the village of Blewbury, Oxfordshire. The libretto was written for the adaptation by the children's novelist John Emlyn Edwards. The "Opera in Six Scenes" was subsequently recorded by Decca between March and June 1979 and released on the Argo label in November 1979.

"Sir Gawain and the Green Knight" was adapted into an opera called "Gawain" by Harrison Birtwistle, first performed in 1991. Birtwistle's opera was praised for maintaining the complexity of the poem while translating it into lyric, musical form. Another operatic adaptation is Lynne Plowman's "Gwyneth and the Green Knight", first performed in 2002. This opera uses "Sir Gawain" as the backdrop but refocuses the story on Gawain's female squire, Gwyneth, who is trying to become a knight. Plowman's version was praised for its approachability, as its target is the family audience and young children, but criticised for its use of modern language and occasional preachy nature.




</doc>
<doc id="28834" url="https://en.wikipedia.org/wiki?curid=28834" title="Sultan Bashiruddin Mahmood">
Sultan Bashiruddin Mahmood

Sultan Bashiruddin Mahmood(; born 1940; "SI"), is a Pakistani nuclear engineer and a scholar on Islamic studies. He was the subject of a criminal probe launched by the Federal Investigation Agency (FIA) over unauthorized travel in Afghanistan prior to the terrorist attacks in 2001.

Having spent a distinguished career in PAEC, he founded the Ummah Tameer-e-Nau (UTN) in 1999– a right-wing organization that was banned and sanctioned by the United States in 2001. Mehmood was among those who were listed and sanctioned by the al-Qaeda sanction committee in December 2001. Having been cleared by the FIA, he has been living in "anonymity" in Islamabad, authoring books on the relationship between Islam and science.

Mahmood was born in Amritsar, Punjab, British India to the Punjabi family. There are conflicting reports on concerning his date of birth; his personal admission noted the birth year as 1940, while the UN reports estimated as 1938. His father, Chaudhry Muhammad Sharif, was a local "Zamindar" (lit. feudal lord). His family emigrated from India to Pakistan in an events following the Religious violence in India in 1947; the family settled in Lahore, Punjab.

After graduating with distinctions from a local high school standing at top of his class, Mehmood was awarded scholarship and enrolled at the famed Government College University to study electrical engineering. After spending a semester, he made a transfer to University of Engineering and Technology in Lahore, and graduated with bachelor's degree with honors in electrical engineering in 1960. His credentials led him to join the Pakistan Atomic Energy Commission (PAEC) where he gained scholarship to study in the United Kingdom.

In 1962, he went to attend the University of Manchester where he studied for double master's degree. First completing masters' program in control systems in 1965, then Mehmood received his another master's degree in nuclear engineering in 1969 from the Manchester University. While in Manchester, Mehmood was an expert on Manhattan Project and was reportedly in contacts with South African scientists in discussing the jet-nozzle method for uranium-enrichment. However, it remains unclear how much interaction was taken place during that time.

Mehmood joined the PAEC in 1968, joining the Nuclear Physics Division at the Institute of Nuclear Science and Technology working under dr. Naeem Ahmad Khan. His collaboration took place with Samar Mubarakmand, Hafeez Qureshi and was a vital member of the group before it got discontinued in 1970. Mahmood was one of the foremost experts on civilian reactor technology and was a senior engineer at the KANUPP I— the first commercial nuclear power plant of the country. He gained notability and publicity in the physics community for inventing the scientific instrument, the "SBM probe" to detect leaks in steam pipes, a problem that was affecting nuclear plants all over the world and is still used worldwide.

After witnessing the war with India which saw the unconditional surrender of Pakistan in 1971, Mahmood attended the winter seminar at Multan and delivered a speech on atomic science. On 20 January 1972, President Zulfikar Ali Bhutto approved the crash program under Munir Ahmad Khan for a sake of "national survivor." Though, he continued his work at the KANUPP I engineering division.

In the aftermath of surprise nuclear test conducted by India, Munir Ahmad appointed Mehmood as the director of the enrichment division at the PAEC where majority of the calculations were conducted by dr. Khalil Qureshi– a physical chemist. Mehmood analyzed the diffusion, gas-centrifuge, jet-nozzle and laser methods for the uranium-enrichment; recommending the gas-centrifuge method as economical. After submitting the report, Mehmood was asked to depart to the Netherlands to interview Dr. Abdul Qadeer Khan on behalf of President Bhutto in 1974. In 1975, his proposal was approved and the work on uranium project started with Mahmood being its director, a move that irked more qualified but more difficult to manage Dr. Abdul Qadeer Khan who had coveted the job for himself. His relations with dr. Khan remains extremely tense and the pairs disagreed with each other and developed differences at great height. In private meetings with Munir Ahmad, Mehmood often complained and pictured him as "egomaniac". In 1976, Mahmood was removed from the enrichment division as Dr. Abdul Qadeer Khan had him ejected and moving the enrichment division at the ERL under military control.

Eventually, Munir Ahmad removed him from other classified works and posted him back at the KANUPP-I with no reason given. In 1980s, Munir Ahmad secured him a job as project manager for the construction of the Khushab-I where he served as chief engineer and aided with the designing the coolant systems. In 1998, he was promoted as a director of the nuclear power division and held that position until 1999.

After the reactor went critical in April 1998, Mahmood in an interview had said: ""This reactor (can produce enough plutonium for two to three nuclear weapons per year) Pakistan had "acquired the capability to produce... boosted thermonuclear weapons and hydrogen bombs"." In 1998, Mahmood was honored with Sitara-e-Imtiaz in a colourful ceremony by the Prime Minister, Nawaz Sharif.

In 1998, he was promoted as a director of the nuclear power division and held that position until 1999.

Endorsing publicly the decision of nuclear tests by Prime Minister Nawaz Sharif in 1998, Mahmood began appearing in news channels as an outspoken opponent of Prime Minister Sharif, as he vehemently opposed Pakistan becoming the signatory state of the NPT and CTBT. At country's popular news channels and newspapers, Mahmood gave numerous interviews, wrote articles, and lobbied against Prime Minister Sharif when learning that Prime Minister Sharif had been willing to be a signatory of anti-nuclear weapon treaties, prompting the government forcefully transferring Mahmood at the non-technical position in the PAEC.

Seeking premature retirement from PAEC in 1999, Mahmood moved towards publishing books and articles involving the relationship between Science and Islam. Mahmood founded the Ummah Tameer-e-Nau (UTN)– a rightwing organization– with his close associates. In 2000, he began attending the lectures and religious sessions with Dr. Israr Ahmed who would later influenced in his political views and philosophy. Through UTN, he steps in the more radical politics, and began visiting Afghanistan where he wanted to be focused on rebuilding educational institutions, hospitals, and relief work.

In August 2001, Mahmood and one of his colleagues at the UTN met with Osama bin Laden and Ayman al-Zawahiri in Kandahar, Afghanistan. Describing the meeting, the "New York Times" editorial quoted:""There is little doubt that Mahmood talked to the two al-Qaeda leaders about nuclear weapons, or that Al Qaeda desperately wanted the bomb"".

Since 1999 and 2000 onwards, Pakistan's intelligence community had been tracking and monitoring Mehmood whose bushy beard advertised his deep attachment to Taliban. After the terrorist attacks in the United States, the FIA launched an active criminal investigations against him, leveling charges on unauthorized traveled to Afghanistan. Director CIA George Tenet later described intelligence reports of his meeting with Al Qaeda as ""frustratingly vague"." When asked by Pakistani and American investigators about nature of UTN's work and discussions, Mahmood told that he had nothing to do with the al-Qaeda and was only working on humanitarian issues like food, health and education. Investigators from ISI and CIA were astonished and surprised when finding out that Mahmood knew nothing on nuclear weapons as contrary of being a nuclear engineer, and were unable to construct one by themselves.

During his debriefing, his son Dr. Asim Mahmood, who's a family medicine doctor told ISI officials that: "My father [Mahmood] did meet with Osama bin Laden and Osama Bin Laden seemed interested in that matter but my father showed no interest in the matter as he met him for food, water and healthcare matters on which his charity was working".

The FIA criminal probe continued for four months and yielded no concrete results. Pressure from the civil society and court inquiries against FIA's criminal probe led to his release in 2001. His family did confirm his release but had been constantly under surveillance by the FIA; his name was placed in the "Exit Control List" in which he is not allowed to travel out of Pakistan and since his release, Mehmood has been out of the public eye and lives a very quiet life in Islamabad, devoting most of his time to writing books and doing research work on Islam and science.

Dr. Bashir Syed, former president of the Association of Pakistani Scientists and Engineers of North America (APSENA), said: "I know both of these persons and can tell you there is not an iota of truth that both these respected scientists and friends will do anything to harm the interest of their own country."

He has written over fifteen books, the most well-known being ""The Mechanics of Doomsday and Life After Death"", which is an analysis of the events leading to doomsday in light of scientific theories and Quranic knowledge. However, his scientific arguments and theories have been challenged by some prominent scientists in Pakistan. His religiosity and eccentricity began troubling the Pakistan's Physics Society; his peers often quoted him as "a rather strange man".

In 1988, Mehmood was invited through an invitation at the University of Islamabad to deliver a lecture on science. During his lecture at the university's ""Physics Hall"", he and several other academcians have debated on his book. While debating, a well known Dr. Pervez Hoodbhoy and Sultan Bashiruddin Mahmood had an acrimonious public debate in 1988 at the University of Islamabad's Physics Hall. Dr. Pervez Hoodbhoy had severely criticised Mr. Bashiruddin Mahmood's theories and the notion of Islamic science in general, calling it "ludicrous science." Bashiruddin Mahmood protested that Dr. Pervez Hoodbhoy misrepresented his views, quoting: "This is crossing all limits of decency," he wrote. "But should one expect any honesty or decency from anti-Islamic sources?"

In his writings and speeches, Mahmood has advocated for nuclear sharing with other Islamic nations which he believed would give rise to Muslim dominance in the world. He has also written a Tafseer of the Quran in English.

Mahmood is reported to be fascinated "with the role sunspots played in triggering the French and Russian Revolutions, World War II and assorted anti-colonial uprisings." According to his book ""Cosmology and Human Destiny"", Mahmood argued that sunspots have influenced major human events, including the French Revolution, the Russian Revolution, and World War II. He concluded that governments across the world ""are already being subjected to great emotional aggression under the catalytic effect of the abnormally high sunspot activity under which they are most likely to adapt aggression as the natural solution for their problems"". In this book which was first published in 1998, he predicts that the period from 2007 to 2014 would be of great turmoil and destruction in the world. Other books written by him include a biography of the Islamic prophet Muhammad titled ""First and the Last"", while his other books are focused more on the relation between Islam and science like "Miraculous Quran", "Life After Death and Doomsday", and "Kitab-e-Zindagi" (in Urdu).

One passage of the book reportedly states: ""At the international level, terrorism will rule; and in this scenario use of mass destruction weapons cannot be ruled out. Millions, by 2020, may die through mass destruction weapons, hunger, disease, street violence, terrorist attacks, and suicide.""

Mahmood's lifelong friend, Parliamentarian Farhatullah Babar, who is currently serving as a spokesperson of President of Pakistan, while talking to media, said: "Mahmood predicted in Cosmology and Human Destiny that "the year 2002 was likely to be a year of maximum sunspot activity. It means upheaval, particularly on the South Asia, with the possibility of nuclear exchanges"."

Mahmood has published papers concerning djinni, which are described in the Qur'an as beings made of fire. He has proposed that djinni could be tapped to solve the energy crisis. "I think that if we develop our souls, we can develop communication with them," Mr. Bashiruddin Mahmood said about djinni in The Wall Street Journal in an interview in 1988. "Every new idea has its opponents," he added. "But there is no reason for this controversy over Islam and science because there is no conflict between Islam and science."

The New York Times has described Mahmood as "an autodidact intellectual with grand aspirations," and noted that "his fellow scientists at PAEC began to wonder if Mahmood was mentally sound." Mahmood made it clear that he believed Pakistan's bomb was "the property of the whole Ummah," referring to the worldwide Muslim community. "This guy was our ultimate nightmare," an American intelligence official told the Times in late 2001. He has also been awarded Gold Medal by the Pakistan Academy of Sciences.






</doc>
<doc id="28837" url="https://en.wikipedia.org/wiki?curid=28837" title="Siege tower">
Siege tower

A siege tower or breaching tower (or in the Middle Ages, a belfry) is a specialized siege engine, constructed to protect assailants and ladders while approaching the defensive walls of a fortification. The tower was often rectangular with four wheels with its height roughly equal to that of the wall or sometimes higher to allow archers to stand on top of the tower and shoot arrows into the fortification. Because the towers were wooden and thus flammable, they had to have some non-flammable covering of iron or fresh animal skins.

Used since the 11th century BC by the Babylonians and Assyrians in the ancient Near East, the 4th century BC in Europe and also in antiquity in the Far East, siege towers were of unwieldy dimensions and, like trebuchets, were therefore mostly constructed on site of the siege. Taking considerable time to construct, siege towers were mainly built if the defense of the opposing fortification could not be overcome by ladder assault ("escalade"), by mining or by breaking walls or gates.

The siege tower sometimes housed spearmen, pikemen, swordsmen, archers or crossbowmen who shot arrows and quarrels at the defenders. Because of the size of the tower it would often be the first target of large stone catapults but it had its own projectiles with which to retaliate.

Siege towers were used to get troops over an enemy curtain wall. When a siege tower was near a wall, it would drop a gangplank between it and the wall. Troops could then rush onto the walls and into the castle or city.

The oldest known siege towers were used by the armies of the Neo-Assyrian Empire in the 9th century BC, under Ashurnasirpal II (r. 884 BC – 859 BC). Reliefs from his reign, and subsequent reigns, depict siege towers in use with a number of other siege works, including ramps and battering rams.

Centuries after they were employed in Assyria, the use of the siege tower spread throughout the Mediterranean. The biggest siege towers of antiquity, such as the "Helepolis" (meaning ""The Taker of Cities"") of the siege of Rhodes in 305 BC, could be as high as 135 feet and as wide as 67.5 feet. Such large engines would require a rack and pinion to be moved effectively. It was manned by 200 soldiers and was divided into nine stories; the different levels housed various types of catapults and ballistae. Subsequent siege towers down through the centuries often had similar engines.

But this huge tower was defeated by the defenders by flooding the ground in front of the wall, creating a moat that caused the tower to get bogged in the mud. The siege of Rhodes illustrates the important point that the larger siege towers needed level ground. Many castles and hill-top towns and forts were virtually invulnerable to siege tower attack simply due to topography. Smaller siege towers might be used on top of siege-mounds, made of earth, rubble and timber mounds in order to overtop a defensive wall. The remains of such a siege-ramp at Masada, for example, has survived almost 2,000 years and can still be seen today.

On the other hand, almost all the largest cities were on large rivers, or the coast, and so did have part of their circuit wall vulnerable to these towers. Furthermore, the tower for such a target might be prefabricated elsewhere and brought dismantled to the target city by water. In some rare circumstances, such towers were mounted on ships to assault the coastal wall of a city: at the siege of Cyzicus during the Third Mithridatic War, for example, towers were used in conjunction with more conventional siege weapons.

One of the oldest references to the mobile siege tower in Ancient China was a written dialogue primarily discussing naval warfare. In the Chinese "Yuejueshu" (Lost Records of the State of Yue) written by the later Han Dynasty author Yuan Kang in the year 52 AD, Wu Zixu (526 BC – 484 BC) purportedly discussed different ship types with King Helü of Wu (r. 514 BC – 496 BC) while explaining military preparedness. Before labeling the types of warships used, Zixu said:

With the collapse of the Roman Empire in the West into independent states, and the Eastern Roman Empire on the defensive, the use of siege towers reached its height during the medieval period. Siege towers were used when the Avars laid siege unsuccessfully to Constantinople in 626, as the "Chronicon Paschale" recounts:

At this siege the attackers also made use of "sows" – mobile armoured shelters which were used throughout the medieval period, and allowed workers to fill in moats with protection from the defenders (thus levelling the ground for the siege towers to be moved to the walls). However, the construction of a sloping talus at the base of a castle wall (as was common in Crusader fortification) could have reduced the effectiveness of this tactic to an extent.

Siege towers also became more elaborate during the medieval period; at the Siege of Kenilworth Castle in 1266, for example, 200 archers and 11 catapults operated from a single tower. Even then, the siege lasted almost a year, making it the longest siege in all of English history. They were not invulnerable either, as during the Fall of Constantinople in 1453, Ottoman siege towers were sprayed by the defenders with Greek fire.

Siege towers became vulnerable and obsolete with the development of large cannon. They had only ever existed to get assaulting troops over high walls and towers and large cannon also made high walls obsolete as fortification took a new direction. However, later constructions known as battery-towers took on a similar role in the gunpowder age; like siege-towers, these were built out of wood on-site for mounting siege artillery. One of these was built by the Russian military engineer Ivan Vyrodkov during the siege of Kazan in 1552 (as part of the Russo-Kazan Wars), and could hold ten large-calibre cannon and 50 lighter cannons.

On 1 March 2007, police officers entered Ungdomshuset in Copenhagen, Denmark by being lifted to the upper levels of an illegally occupied structure using small boom cranes for a purpose similar to that for which siege towers were constructed. The officers were placed in containers that crane operators raised and placed against the structure's windows, enabling the officers to gain access to the structure.


</doc>
<doc id="28840" url="https://en.wikipedia.org/wiki?curid=28840" title="Sharia">
Sharia

Sharia (, ), Islamic law or sharia law is a religious law forming part of the Islamic tradition. It is derived from the religious precepts of Islam, particularly the Quran and the hadith. In Arabic, the term "sharīʿah" refers to God's immutable divine law and is contrasted with "fiqh", which refers to its human scholarly interpretations. The manner of its application in modern times has been a subject of dispute between Muslim fundamentalists and modernists.

Traditional theory of Islamic jurisprudence recognizes four sources of sharia: the Quran, "sunnah" (authentic hadith), "qiyas" (analogical reasoning), and "ijma" (juridical consensus). Different legal schools—of which the most prominent are Hanafi, Maliki, Shafi'i, Hanbali and Jafari—developed methodologies for deriving sharia rulings from scriptural sources using a process known as "ijtihad". Traditional jurisprudence ("fiqh") distinguishes two principal branches of law, "ʿibādāt" (rituals) and "muʿāmalāt" (social relations), which together comprise a wide range of topics. Its rulings are concerned with ethical standards as much as with legal norms, assigning actions to one of five categories: mandatory, recommended, neutral, abhorred, and prohibited. Thus, some areas of sharia overlap with the Western notion of law while others correspond more broadly to living life in accordance with God’s will.

Classical jurisprudence was elaborated by private religious scholars, largely through legal opinions (fatwas) issued by qualified jurists (muftis). It was historically applied in sharia courts by ruler-appointed judges, who dealt mainly with civil disputes and community affairs. Sultanic courts, the police and market inspectors administered criminal justice, which was influenced by sharia but not bound by its rules. Non-Muslim (dhimmi) communities had legal autonomy to adjudicate their internal affairs. Over the centuries, Sunni muftis were gradually incorporated into state bureaucracies, and fiqh was complemented by various economic, criminal and administrative laws issued by Muslim rulers. The Ottoman civil code of 1869–1876 was the first partial attempt to codify sharia.

In the modern era, traditional laws in the Muslim world have been widely replaced by statutes inspired by European models. Judicial procedures and legal education were likewise brought in line with European practice. While the constitutions of most Muslim-majority states contain references to sharia, its classical rules were largely retained only in personal status (family) laws. Legislators who codified these laws sought to modernize them without abandoning their foundations in traditional jurisprudence. The Islamic revival of the late 20th century brought along calls by Islamist movements for full implementation of sharia, including "hudud" corporal punishments, such as stoning. In some cases, this resulted in traditionalist legal reform, while other countries witnessed juridical reinterpretation of sharia advocated by progressive reformers. Some Muslim-minority countries recognize the use of sharia-based family laws for their Muslim populations. Sharia also continues to influence other aspects of private and public life.

The role of sharia has become a contested topic around the world. Introduction of sharia-based laws sparked intercommunal violence in Nigeria and may have contributed to the breakup of Sudan. Some jurisdictions in North America have passed bans on use of sharia, framed as restrictions on religious or foreign laws. There are ongoing debates as to whether sharia is compatible with democracy, human rights, freedom of thought, women's rights, LGBT rights, and banking.

The word "sharīʿah" is used by Arabic-speaking peoples of the Middle East to designate a prophetic religion in its totality. For example, "sharīʿat Mūsā" means law or religion of Moses and "sharīʿatu-nā" can mean "our religion" in reference to any monotheistic faith. Within Islamic discourse, "šarīʿah" refers to religious regulations governing the lives of Muslims. For many Muslims, the word means simply "justice," and they will consider any law that promotes justice and social welfare to conform to sharia.

Jan Michiel Otto distinguishes four senses conveyed by the term "sharia" in religious, legal and political discourse:

A related term "" (, Islamic law), which was borrowed from European usage in the late 19th century, is used in the Muslim world to refer to a legal system in the context of a modern state.

The primary range of meanings of the Arabic word "šarīʿah", derived from the root "š-r-ʕ", is related to religion and religious law. The lexicographical tradition records two major areas of use where the word "šarīʿah" can appear without religious connotation. In texts evoking a pastoral or nomadic environment, the word, and its derivatives refer to watering animals at a permanent water-hole or to the seashore, with special reference to animals who come there. Another area of use relates to notions of stretched or lengthy. This range of meanings is cognate with the Hebrew "saraʿ" and is likely to be the origin of the meaning "way" or "path". Both these areas have been claimed to have given rise to aspects of the religious meaning.

Some scholars describe the word "šarīʿah" as an archaic Arabic word denoting "pathway to be followed" (analogous to the Hebrew term Halakhah ["The Way to Go"]), or "path to the water hole" and argue that its adoption as a metaphor for a divinely ordained way of life arises from the importance of water in an arid desert environment.

In the Quran, "šarīʿah" and its cognate "širʿah" occur once each, with the meaning "way" or "path". The word "šarīʿah" was widely used by Arabic-speaking Jews during the Middle Ages, being the most common translation for the word "torah" in the 10th-century Arabic translation of the Torah by Saʿadya Gaon. A similar use of the term can be found in Christian writers. The Arabic expression "Sharīʿat Allāh" (شريعة الله "God’s Law") is a common translation for תורת אלוהים (‘God’s Law’ in Hebrew) and νόμος τοῦ θεοῦ (‘God’s Law’ in Greek in the New Testament [Rom. 7: 22]). In Muslim literature, "šarīʿah" designates the laws or message of a prophet or God, in contrast to "fiqh", which refers to a scholar's interpretation thereof.

In older English-language law-related works in the late 19th/early 20th centuries, the word used for sharia was sheri. It, along with the French variant "chéri", was used during the time of the Ottoman Empire, and is from the Turkish "şer’"("i").

According to the traditional Muslim view, the major precepts of sharia were passed down directly from the Islamic prophet Muhammad without "historical development," and the emergence of Islamic jurisprudence ("fiqh") also goes back to the lifetime of Muhammad. In this view, his companions and followers took what he did and approved of as a model (sunnah) and transmitted this information to the succeeding generations in the form of hadith. These reports led first to informal discussion and then systematic legal thought, articulated with greatest success in the eighth and ninth centuries by the master jurists Abu Hanifah, Malik ibn Anas, Al-Shafi‘i, and Ahmad ibn Hanbal, who are viewed as the founders of the Hanafi, Maliki, Shafiʿi, and Hanbali legal schools ("madhhabs") of Sunni jurisprudence.

Modern historians have presented alternative theories of the formation of fiqh. At first Western scholars accepted the general outlines of the traditional account. In the late 19th century, an influential revisionist hypothesis was advanced by Ignac Goldziher and elaborated by Joseph Schacht in the mid-20th century. Schacht and other scholars argued that having conquered much more populous agricultural and urban societies with already existing laws and legal needs unknown to the desert-dwelling conquerors, 
the initial Muslim efforts to formulate legal norms
regarded the Quran and Muhammad's hadiths as just one sources of law, with jurist personal opinions, the legal practice of conquered peoples, and the decrees and decisions of the caliphs also being valid sources. At least one source (historian Tom Holland) has argued that the strong scholarly tradition of Mobad among the conquered Zoroastrians of Persia and rabbis among the conquered Jews influenced the law of their largely illiterate warrior conquerors; and that this can explain such issues as why the Quran mentions only three prayers (24:58) while Muslims pray five times a day (Zoroastrians prayed five times a day) and why the Quran commands adulterers be lashed, while sharia calls for their execution by stoning (Deuteronomy 22:21 of the Jewish Torah calls for stoning to death of women who have been found to have had sex before marriage).

According to this theory, most canonical hadiths did not originate with Muhammad but were actually created at a later date, despite the efforts of hadith scholars to weed out fabrications.
After it became accepted that legal norms must be formally grounded in scriptural sources, proponents of rules of jurisprudence supported by the hadith would extend the chains of transmission of the hadith back to Muhammad's companions. In his view, the real architect of Islamic jurisprudence was Al-Shafi‘i (d. 820 CE/204 AH), who formulated this idea (that legal norms must be formally grounded in scriptural sources) and other elements of classical legal theory in his work "al-risala", but who was preceded by a body of Islamic law not based on primacy of Muhammad's hadiths. 

While the origin of hadith remains a subject of scholarly controversy, this theory (of Goldziher and Schacht) has given rise to objections, and modern historians generally adopt more cautious, intermediate positions, 
and it is generally accepted that early Islamic jurisprudence developed out of a combination of administrative and popular practices shaped by the religious and ethical precepts of Islam. It continued some aspects of pre-Islamic laws and customs of the lands that fell under Muslim rule in the aftermath of the early conquests and modified other aspects, aiming to meet the practical need of establishing Islamic norms of behavior and adjudicating disputes arising in the early Muslim communities. Juristic thought gradually developed in study circles, where independent scholars met to learn from a local master and discuss religious topics. At first, these circles were fluid in their membership, but with time distinct regional legal schools crystallized around shared sets of methodological principles. As the boundaries of the schools became clearly delineated, the authority of their doctrinal tenets came to be vested in a master jurist from earlier times, who was henceforth identified as the school's founder. In the course of the first three centuries of Islam, all legal schools came to accept the broad outlines of classical legal theory, according to which Islamic law had to be firmly rooted in the Quran and hadith.

Fiqh is traditionally divided into the fields of "uṣūl al-fiqh" (lit. the roots of fiqh), which studies the theoretical principles of jurisprudence, and "furūʿ al-fiqh" (lit. the branches of fiqh), which is devoted to elaboration of rulings on the basis of these principles.

Classical jurists held that human reason is a gift from God which should be exercised to its fullest capacity. However, they believed that use of reason alone is insufficient to distinguish right from wrong, and that rational argumentation must draw its content from the body of transcendental knowledge revealed in the Quran and through the sunnah of Muhammad.

Traditional theory of Islamic jurisprudence elaborates how scriptures should be interpreted from the standpoint of linguistics and rhetoric. It also comprises methods for establishing authenticity of hadith and for determining when the legal force of a scriptural passage is abrogated by a passage revealed at a later date. In addition to the Quran and sunnah, the classical theory of Sunni fiqh recognizes two other sources of law: juristic consensus ("ijmaʿ") and analogical reasoning ("qiyas"). It therefore studies the application and limits of analogy, as well as the value and limits of consensus, along with other methodological principles, some of which are accepted by only certain legal schools. This interpretive apparatus is brought together under the rubric of ijtihad, which refers to a jurist's exertion in an attempt to arrive at a ruling on a particular question. The theory of Twelver Shia jurisprudence parallels that of Sunni schools with some differences, such as recognition of reason ("ʿaql") as a source of law in place of "qiyas" and extension of the notion of sunnah to include traditions of the imams.


The classical process of ijtihad combined these generally recognized principles with other methods, which were not adopted by all legal schools, such as "istihsan" (juristic preference), "istislah" (consideration of public interest) and "istishab" (presumption of continuity). A jurist who is qualified to practice ijtihad is known as a "mujtahid". The use of independent reasoning to arrive at a ruling is contrasted with "taqlid" (imitation), which refers to following the rulings of a mujtahid. By the beginning of the 10th century, development of Sunni jurisprudence prompted leading jurists to state that the main legal questions had been addressed and the scope of ijtihad was gradually restricted. From the 18th century on, leading Muslim reformers began calling for abandonment of taqlid and renewed emphasis on ijtihad, which they saw as a return to the vitality of early Islamic jurisprudence.

Fiqh is concerned with ethical standards as much as with legal norms, seeking to establish not only what is and is not legal, but also what is morally right and wrong. Sharia rulings fall into one of five categories known as “the five decisions” ("al-aḥkām al-khamsa"): mandatory ("farḍ" or "wājib"), recommended ("mandūb" or "mustaḥabb"), neutral ("mubāḥ"), reprehensible ("makrūh"), and forbidden ("ḥarām"). It is a sin or a crime to perform a forbidden action or not to perform a mandatory action. Reprehensible acts should be avoided, but they are not considered to be sinful or punishable in court. Avoiding reprehensible acts and performing recommended acts is held to be subject of reward in the afterlife, while neutral actions entail no judgement from God. Jurists disagree on whether the term "ḥalāl" covers the first three or the first four categories. The legal and moral verdict depends on whether the action is committed out of necessity ("ḍarūra") and on the underlying intention ("niyya"), as expressed in the legal maxim "acts are [evaluated according] to intention."

"Maqāṣid" (aims or purposes) of sharia and "maṣlaḥa" (welfare or public interest) are two related classical doctrines which have come to play an increasingly prominent role in modern times. They were first clearly articulated by al-Ghazali (d. 1111), who argued that "maslaha" was God's general purpose in revealing the divine law, and that its specific aim was preservation of five essentials of human well-being: religion, life, intellect, offspring, and property. Although most classical-era jurists recognized "maslaha" and "maqasid" as important legal principles, they held different views regarding the role they should play in Islamic law. Some jurists viewed them as auxiliary rationales constrained by scriptural sources and analogical reasoning. Others regarded them as an independent source of law, whose general principles could override specific inferences based on the letter of scripture. While the latter view was held by a minority of classical jurists, in modern times it came to be championed in different forms by prominent scholars who sought to adapt Islamic law to changing social conditions by drawing on the intellectual heritage of traditional jurisprudence. These scholars expanded the inventory of "maqasid" to include such aims of sharia as reform and women's rights (Rashid Rida); justice and freedom (Mohammed al-Ghazali); and human dignity and rights (Yusuf al-Qaradawi).

The domain of "furūʿ al-fiqh" (lit. branches of fiqh) is traditionally divided into "ʿibādāt" (rituals or acts of worship) and "muʿāmalāt" (social relations). Many jurists further divided the body of substantive jurisprudence into "the four quarters", called rituals, sales, marriage and injuries. Each of these terms figuratively stood for a variety of subjects. For example, the quarter of sales would encompass partnerships, guaranty, gifts, and bequests, among other topics. Juristic works were arranged as a sequence of such smaller topics, each called a "book" ("kitab"). The special significance of ritual was marked by always placing its discussion at the start of the work.

Some historians distinguish a field of Islamic criminal law, which combines several traditional categories. Several crimes with scripturally prescribed punishments are known as "hudud". Jurists developed various restrictions which in many cases made them virtually impossible to apply. Other crimes involving intentional bodily harm are judged according to a version of "lex talionis" that prescribes a punishment analogous to the crime ("qisas"), but the victims or their heirs may accept a monetary compensation ("diya") or pardon the perpetrator instead; only "diya" is imposed for non-intentional harm. Other criminal cases belong to the category of "taʿzīr", where the goal of punishment is correction or rehabilitation of the culprit and its form is largely left to the judge's discretion. In practice, since early on in Islamic history, criminal cases were usually handled by ruler-administered courts or local police using procedures which were only loosely related to sharia.

The two major genres of "furūʿ" literature are the "mukhtasar" (concise summary of law) and the "mabsut" (extensive commentary). "Mukhtasars" were short specialized treatises or general overviews that could be used in a classroom or consulted by judges. A "mabsut", which usually provided a commentary on a "mukhtasar" and could stretch to dozens of large volumes, recorded alternative rulings with their justifications, often accompanied by a proliferation of cases and conceptual distinctions. The terminology of juristic literature was conservative and tended to preserve notions which had lost their practical relevance. At the same time, the cycle of abridgement and commentary allowed jurists of each generation to articulate a modified body of law to meet changing social conditions. Other juristic genres include the "qawāʿid" (succinct formulas meant to aid the student remember general principles) and collections of fatwas by a particular scholar.

Classical jurisprudence has been described as "one of the major intellectual achievements of Islam" and its importance in Islam has been compared to that of theology in Christianity.

The main Sunni schools of law ("madhhabs") are the Hanafi, Maliki, Shafi'i and Hanbali madhhabs. They emerged in the ninth and tenth centuries and by the twelfth century almost all jurists aligned themselves with a particular madhhab. These four schools recognize each other's validity and they have interacted in legal debate over the centuries. Rulings of these schools are followed across the Muslim world without exclusive regional restrictions, but they each came to dominate in different parts of the world. For example, the Maliki school is predominant in North and West Africa; the Hanafi school in South and Central Asia; the Shafi'i school in Lower Egypt, East Africa, and Southeast Asia; and the Hanbali school in North and Central Arabia. The first centuries of Islam also witnessed a number of short-lived Sunni madhhabs. The Zahiri school, which is commonly identified as extinct, continues to exert influence over legal thought. The development of Shia legal schools occurred along the lines of theological differences and resulted in formation of the Twelver, Zaidi and Ismaili madhhabs, whose differences from Sunni legal schools are roughly of the same order as the differences among Sunni schools. The Ibadi legal school, distinct from Sunni and Shia madhhabs, is predominant in Oman.

The transformations of Islamic legal institutions in the modern era have had profound implications for the madhhab system. Legal practice in most of the Muslim world has come to be controlled by government policy and state law, so that the influence of the madhhabs beyond personal ritual practice depends on the status accorded to them within the national legal system. State law codification commonly utilized the methods of "takhayyur" (selection of rulings without restriction to a particular madhhab) and "talfiq" (combining parts of different rulings on the same question). Legal professionals trained in modern law schools have largely replaced traditional ulema as interpreters of the resulting laws. Global Islamic movements have at times drawn on different madhhabs and at other times placed greater focus on the scriptural sources rather than classical jurisprudence. The Hanbali school, with its particularly strict adherence to the Quran and hadith, has inspired conservative currents of direct scriptural interpretation by the Salafi and Wahhabi movements. Other currents, such as networks of Indonesian ulema and Islamic scholars residing in Muslim-minority countries, have advanced liberal interpretations of Islamic law without focusing on traditions of a particular madhhab.

Sharia was traditionally interpreted by muftis. During the first few centuries of Islam, muftis were private legal specialists who normally also held other jobs. They issued fatwas (legal opinions), generally free of charge, in response to questions from laypersons or requests for consultation coming from judges, which would be stated in general terms. Fatwas were regularly upheld in courts, and when they were not, it was usually because the fatwa was contradicted by a more authoritative legal opinion. The stature of jurists was determined by their scholarly reputation. The majority of classical legal works, written by author-jurists, were based in large part on fatwas of distinguished muftis. These fatwas functioned as a form of legal precedent, unlike court verdicts, which were valid only for the given case. Although independent muftis never disappeared, from the 12th century onward Muslim rulers began to appoint salaried muftis to answer questions from the public. Over the centuries, Sunni muftis were gradually incorporated into state bureaucracies, while Shia jurists in Iran progressively asserted an autonomous authority starting from the early modern era.
Islamic law was initially taught in study circles that gathered in mosques and private homes. The teacher, assisted by advanced students, provided commentary on concise treatises of law and examined the students' understanding of the text. This tradition continued to be practiced in "madrasas", which spread during the 10th and 11th centuries. Madrasas were institutions of higher learning devoted principally to study of law, but also offering other subjects such as theology, medicine, and mathematics. The madrasa complex usually consisted of a mosque, boarding house, and a library. It was maintained by a "waqf" (charitable endowment), which paid salaries of professors, stipends of students, and defrayed the costs of construction and maintenance. At the end of a course, the professor granted a license ("ijaza") certifying a student's competence in its subject matter. Students specializing in law would complete a curriculum consisting of preparatory studies, the doctrines of a particular madhhab, and training in legal disputation, and finally write a dissertation, which earned them a license to teach and issue fatwas.

A judge (qadi) was in charge of the qadi's court ("mahkama"), also called the sharia court. Qadis were trained in Islamic law, though not necessarily to a level required to issue fatwas. Court personnel also included a number of assistants performing various roles. Judges were theoretically independent in their decisions, though they were appointed by the ruler and often experienced pressure from members of the ruling elite where their interests were at play. The role of qadis was to evaluate the evidence, establish the facts of the case, and issue a verdict based on the applicable rulings of Islamic jurisprudence. The qadi was supposed to solicit a fatwa from a mufti if it was unclear how the law should be applied to the case. Since Islamic legal theory does not recognize the distinction between private and public law, court procedures were identical for civil and criminal cases, and required a private plaintiff to produce evidence against the defendant. The main type of evidence was oral witness testimony. The standards of evidence for criminal cases were so strict that a conviction was often difficult to obtain even for apparently clear-cut cases. Most historians believe that because of these stringent procedural norms, qadi's courts at an early date lost their jurisdiction over criminal cases, which were instead handled in other types of courts.

If an accusation did not result in a verdict in a qadi's court, the plaintiff could often pursue it in another type of court called the "mazalim" court, administered by the ruler's council. The rationale for "mazalim" (lit. wrongs, grievances) courts was to address the "wrongs" that sharia courts were unable to address, including complaints against government officials. Islamic jurists were commonly in attendance and a judge often presided over the court as a deputy of the ruler. "Mazalim" verdicts were supposed to conform to the "spirit" of sharia, but they were not bound by the letter of the law or the procedural restrictions of qadi's courts.

The police ("shurta"), which took initiative in preventing and investigating crime, operated its own courts. Like the mazalim courts, police courts were not bound by the rules of sharia and had the powers to inflict discretionary punishments. Another office for maintaining public order was the "muhtasib" (market inspector), who was charged with preventing fraud in economic transactions and infractions against public morality. The "muhtasib" took an active role in pursuing these types of offenses and meted out punishments based on local custom.

The social fabric of pre-modern Islamic societies was largely defined by close-knit communities organized around kinship groups and local neighborhoods. Conflicts between individuals had the potential to escalate into a conflict between their supporting groups and disrupt the life of the entire community. Court litigation was seen as a last resort for cases where informal mediation had failed. This attitude was reflected in the legal maxim "amicable settlement is the best verdict" ("al-sulh sayyid al-ahkam"). In court disputes, qadis were generally less concerned with legal theory than with achieving an outcome that enabled the disputants to resume their previous social relationships. This could be accomplished by avoiding a total loss for the losing side or simply giving them a chance to articulate their position in public and obtain a measure of psychological vindication. Islamic law required judges to be familiar with local customs, and they exercised a number of other public functions in the community, including mediation and arbitration, supervision of public works, auditing waqf finances, and looking after the interests of orphans.

Unlike pre-modern cultures where the ruling dynasty promulgated the law, Islamic law was formulated by religious scholars without involvement of the rulers. The law derived its authority not from political control, but rather from the collective doctrinal positions of the legal schools (madhhabs) in their capacity as interpreters of the scriptures. The ulema (religious scholars) were involved in management of communal affairs and acted as representatives of the Muslim population vis-à-vis the ruling dynasties, who before the modern era had limited capacity for direct governance. Military elites relied on the ulema for religious legitimation, with financial support for religious institutions being one of the principal means through which these elites established their legitimacy. In turn, the ulema depended on the support of the ruling elites for the continuing operation of religious institutions. Although the relationship between secular rulers and religious scholars underwent a number of shifts and transformations in different times and places, this mutual dependence characterized Islamic history until the start of the modern era. Additionally, since sharia contained few provisions in several areas of public law, Muslim rulers were able to legislate various collections of economic, criminal and administrative laws outside the jurisdiction of Islamic jurists, the most famous of which is the "qanun" promulgated by Ottoman sultans beginning from the 15th century. The Mughal emperor Aurangzeb (r. 1658-1707) issued a hybrid body of law known as Fatawa-e-Alamgiri, based on Hanafi fatwas as well as decisions of Islamic courts, and made it applicable to all religious communities on the Indian subcontinent. This early attempt to turn Islamic law into semi-codified state legislation sparked rebellions against Mughal rule.

In both the rules of civil disputes and application of penal law, classical sharia distinguishes between men and women, between Muslims and non-Muslims, and between free persons and slaves.
Traditional Islamic law assumes a patriarchal society with a man at the head of the household. Different legal schools formulated a variety of legal norms which could be manipulated to the advantage of men or women, but women were generally at a disadvantage with respect to the rules of inheritance, blood money ("diya"), and witness testimony, where a woman's value is effectively treated as half of that of a man. In economic terms women enjoyed greater advantages under Islamic law than under other Mediterranean and Middle Eastern legal systems, including the right to own personal property and dispose of it freely, which women in the West did not possess until "quite recently". Various financial obligations imposed on the husband acted as a deterrent against unilateral divorce and commonly gave the wife financial leverage in divorce proceedings. Women were active in sharia courts as both plaintiffs and defendants in a wide variety of cases, though some opted to be represented by a male relative.

Sharia was intended to regulate affairs of the Muslim community. Non-Muslims residing under Islamic rule had the legal status of dhimmi, which entailed a number of protections, restrictions, freedoms and legal inequalities, including payment of the jizya tax. Dhimmi communities had legal autonomy to adjudicate their internal affairs. Cases involving litigants from two different religious groups fell under jurisdiction of sharia courts, where (unlike in secular courts) testimony of non-Muslim witnesses against a Muslim was inadmissible in criminal cases or at all. This legal framework was implemented with varying degree of rigor. In some periods or towns, all inhabitants apparently used the same court without regard for their religious affiliation. The Mughal emperor Aurangzeb imposed Islamic law on all his subjects, including provisions traditionally applicable only to Muslims, while some of his predecessors and successors are said to have abolished jizya. According to Ottoman records, non-Muslim women took their cases to a sharia court when they expected a more favorable outcome on marital, divorce and property questions than in Christian and Jewish courts.

Classical fiqh acknowledges and regulates slavery as a legitimate institution. It granted slaves certain rights and protections, improving their status relative to Greek and Roman law, and restricted the scenarios under which people could be enslaved. However, slaves could not inherit or enter into a contract, and were subject to their master's will in a number of ways. The labor and property of slaves were owned by the master, who was also entitled to sexual submission of his unmarried slaves.

Formal legal disabilities for some groups coexisted with a legal culture that viewed sharia as a reflection of universal principles of justice, which involved protection of the weak against injustices committed by the strong. This conception was reinforced by the historical practice of sharia courts, where peasants "almost always" won cases against oppressive landowners, and non-Muslims often prevailed in disputes against Muslims, including such powerful figures as the governor of their province. In family matters the sharia court was seen as a place where the rights of women could be asserted against their husband's transgressions.

Starting from the 17th century, European powers began to extend political influence over lands ruled by Muslim dynasties, and by the end of the 19th century, much of the Muslim world came under colonial domination. The first areas of Islamic law to be impacted were usually commercial and criminal laws, which impeded colonial administration and were soon replaced by European regulations. Islamic commercial laws were also replaced by European (mostly French) laws in Muslim states which retained formal independence, because these states increasingly came to rely on Western capital and could not afford to lose the business of foreign merchants who refused to submit to Islamic regulations.
The first significant changes to the legal system of British India were initiated in the late 18th century by the governor of Bengal Warren Hastings. Hastings' plan of legal reform envisioned a multi-tiered court system for the Muslim population, with a middle tier of British judges advised by local Islamic jurists, and a lower tier of courts operated by qadis. Hastings also commissioned a translation of the classic manual of Hanafi fiqh, "Al-Hidayah", from Arabic into Persian and then English, later complemented by other texts. These translations enabled British judges to pass verdicts in the name of Islamic law based on a combination of sharia rules and common law doctrines, and eliminated the need to rely on consultation by local ulema, whom they mistrusted. In the traditional Islamic context, a concise text like "Al-Hidayah" would be used as a basis for classroom commentary by a professor, and the doctrines thus learned would be mediated in court by judicial discretion, consideration of local customs and availability of different legal opinions that could fit the facts of the case. The British use of "Al-Hidayah", which amounted to an inadvertent codification of sharia, and its interpretation by judges trained in Western legal traditions anticipated later legal reforms in the Muslim world.

British administrators felt that sharia rules too often allowed criminals to escape punishment, as exemplified by Hastings' complaint that Islamic law was "founded on the most lenient principles and on an abhorrence of bloodshed". In the course of the 19th century, criminal laws and other aspects of the Islamic legal system in India were supplanted by British law, with the exception of sharia rules retained in family laws and some property transactions. Among other changes, these reforms brought about abolition of slavery, prohibition of child marriage, and a much more frequent use of capital punishment. The resulting legal system, known as "Anglo-Muhammadan law", was treated by the British as a model for legal reforms in their other colonies. Like the British in India, colonial administrations typically sought to obtain precise and authoritative information about indigenous laws, which prompted them to prefer classical Islamic legal texts over local judicial practice. This, together with their conception of Islamic law as a collection of inflexible rules, led to an emphasis on traditionalist forms of sharia that were not rigorously applied in the pre-colonial period and served as a formative influence on the modern identity politics of the Muslim world.

During the colonial era, Muslim rulers concluded that they could not resist European pressure unless they modernized their armies and built centrally administered states along the lines of Western models. In the Ottoman empire, the first such changes in the legal sphere involved placing the formerly independent waqfs under state control. This reform, passed in 1826, enriched the public treasury at the expense of the waqfs, thereby depleting the financial support for traditional Islamic legal education. Over the second half of the 19th century, a new hierarchical system of secular courts was established to supplement and eventually replace most religious courts. Students hoping to pursue legal careers in the new court system increasingly preferred attending secular schools over the traditional path of legal education with its dimming financial prospects. The Tanzimat reforms of the 19th century saw reorganization of both Islamic civil law and sultanic criminal law after the model of the Napoleonic Code. In the 1870s, a codification of civil law and procedure (excepting marriage and divorce), called the "Mecelle", was produced for use in both sharia and secular courts. It adopted the Turkish language for the benefit of the new legal class who no longer possessed competence in the Arabic idiom of traditional jurisprudence. The code was based on Hanafi law, and its authors selected minority opinions over authoritative ones when they were felt to better "suit the present conditions". The Mecelle was promulgated as a "qanun" (sultanic code), which represented an unprecedented assertion of the state's authority over Islamic civil law, traditionally the preserve of the ulema. The 1917 Ottoman Law of Family Rights adopted an innovative approach of drawing rules from minority and majority opinions of all Sunni madhhabs with a modernizing intent. The Republic of Turkey, which emerged after the dissolution of the Ottoman Empire, abolished its sharia courts and replaced Ottoman civil laws with the Swiss Civil Code, but Ottoman civil laws remained in force for several decades in Jordan, Lebanon, Palestine, Syria, and Iraq.

Westernization of legal institutions and expansion of state control in all areas of law, which began during the colonial era, continued in nation-states of the Muslim world. Sharia courts at first continued to exist alongside state courts as in earlier times, but the doctrine that sultanic courts should implement the ideals of sharia was gradually replaced by legal norms imported from Europe. Court procedures were also brought in line with European practice. Though the Islamic terms "qadi" and "mahkama" (qadi's/sharia court) were preserved, they generally came to mean judge and court in the Western sense. While in the traditional sharia court all parties represented themselves, in modern courts they are represented by professional lawyers educated in Western-style law schools, and the verdicts are subject to review in an appeals court. In the 20th century, most countries abolished a parallel system of sharia courts and brought all cases under a national civil court system.

In most Muslim-majority countries, traditional rules of classical fiqh have been largely preserved only in family law. In some countries religious minorities such as Christians or Shia Muslims have been subject to separate systems of family laws. Many Muslims today believe that contemporary sharia-based laws are an authentic representation of the pre-modern legal tradition. In reality, they generally represent the result of extensive legal reforms made in the modern era. As traditional Islamic jurists lost their role as authoritative interpreters of the laws applied in courts, these laws were codified by legislators and administered by state systems which employed a number of devices to effect changes, including:

The most powerful influence on liberal reformist thought came from the work of the Egyptian Islamic scholar Muhammad ʿAbduh (1849–1905). Abduh viewed only sharia rules pertaining to religious rituals as inflexible, and argued that the other Islamic laws should be adapted based on changing circumstances in consideration of social well-being. Following precedents of earlier Islamic thinkers, he advocated restoring Islam to its original purity by returning to the Quran and the sunna instead of following the medieval schools of jurisprudence. He championed a creative approach to ijtihad that involved direct interpretation of scriptures as well as the methods of "takhayyur" and "talfiq".

One of the most influential figures in modern legal reforms was the Egyptian legal scholar Abd El-Razzak El-Sanhuri (1895–1971), who possessed expertise in both Islamic and Western law. Sanhuri argued that reviving Islamic legal heritage in a way that served the needs of contemporary society required its analysis in light of the modern science of comparative law. He drafted the civil codes of Egypt (1949) and Iraq (1951) based on a variety of sources, including classical fiqh, European laws, existing Arab and Turkish codes, and the history of local court decisions. Sanhuri's Egyptian code incorporated few classical sharia rules, but he drew on traditional jurisprudence more frequently for the Iraqi code. Sanhuri's codes were subsequently adopted in some form by most Arab countries.

Aside from the radical reforms of Islamic family law carried out in Tunisia (1956) and Iran (1967), governments often preferred to make changes that made a clear break from traditional sharia rules by imposing administrative hurdles rather than changing the rules themselves, in order to minimize objections from religious conservatives. Various procedural changes have been made in a number of countries to restrict polygamy, give women greater rights in divorce, and eliminate child marriage. Inheritance has been the legal domain least susceptible to reform, as legislators have been generally reluctant to tamper with the highly technical system of Quranic shares. Some reforms have faced strong conservative opposition. For example, the 1979 reform of Egyptian family law, promulgated by Anwar Sadat through presidential decree, provoked an outcry and was annulled in 1985 by the supreme court on procedural grounds, to be later replaced by a compromise version. The 2003 reform of Moroccan family law, which sought to reconcile universal human rights norms and the country's Islamic heritage, was drafted by a commission that included parliamentarians, religious scholars and feminist activists, and the result has been praised by international rights groups as an example of "progressive" legislation achieved within an Islamic framework.

The Islamic revival of the late 20th century brought the topic of sharia to international attention in the form of numerous political campaigns in the Muslim world calling for full implementation of sharia. A number of factors have contributed to the rise of these movements, classified under the rubric of Islamism or political Islam, including the failure of authoritarian secular regimes to meet the expectations of their citizens, and a desire of Muslim populations to return to more culturally authentic forms of socio-political organization in the face of a perceived cultural invasion from the West. Islamist leaders such as Ayatollah Khomeini drew on leftist anticolonialist rhetoric by framing their call for sharia as a resistance struggle. They accused secular leaders of corruption and predatory behavior, and claimed that a return to sharia would replace despotic rulers with pious leaders striving for social and economic justice. In the Arab world these positions are often encapsulated in the slogan "Islam is the solution" ("al-Islam huwa al-hall").

Full implementation of sharia theoretically refers to expanding its scope to all fields of law and all areas of public life. In practice, Islamization campaigns have focused on a few highly visible issues associated with the conservative Muslim identity, particularly women's hijab and the "hudud" criminal punishments (whipping, stoning and amputation) prescribed for certain crimes. For many Islamists, "hudud" punishments are at the core of the divine sharia because they are specified by the letter of scripture rather than by human interpreters. Modern Islamists have often rejected, at least in theory, the stringent procedural constraints developed by classical jurists to restrict their application. To the broader Muslim public, the calls for sharia often represent, even more than any specific demands, a vague vision of their current economic and political situation being replaced by a "just utopia".

A number of legal reforms have been made under the influence of these movements, starting from the 1970s when Egypt and Syria amended their constitutions to specify sharia as the basis of legislation. The Iranian Revolution of 1979 represented a watershed for Islamization advocates, demonstrating that it was possible to replace a secular regime with a theocracy. Several countries, including Iran, Pakistan, Sudan, and some Nigerian states have incorporated hudud rules into their criminal justice systems, which, however, retained fundamental influences of earlier Westernizing reforms. In practice, these changes were largely symbolic, and aside from some cases brought to trial to demonstrate that the new rules were being enforced, hudud punishments tended to fall into disuse, sometimes to be revived depending on the local political climate. The supreme courts of Sudan and Iran have rarely approved verdicts of stoning or amputation, and the supreme courts of Pakistan and Nigeria have never done so. Nonetheless, Islamization campaigns have also had repercussions in several other areas of law, leading to curtailment of rights of women and religious minorities, and in the case of Sudan contributing to the breakout of a civil war.

Advocates of Islamization have often been more concerned with ideology than traditional jurisprudence and there is no agreement among them as to what form a modern sharia-based "Islamic state" should take. This is particularly the case for the theorists of Islamic economics and Islamic finance, who have advocated both free-market and socialist economic models. The notion of "sharia-compliant" finance has become an active area of doctrinal innovation and its development has had a major impact on business operations around the world.

The legal systems of most Muslim-majority countries can be classified as either secular or mixed. Sharia plays no role in secular legal systems. In mixed legal systems, sharia rules are allowed to influence some national laws, which are codified and may be based on European or Indian models, and the central legislative role is played by politicians and modern jurists rather than the ulema (traditional Islamic scholars). Saudi Arabia and some other Gulf states possess what may be called classical sharia systems, where national law is largely uncodified and formally equated with sharia, with ulema playing a decisive role in its interpretation. Iran has adopted some features of classical sharia systems, while also maintaining characteristics of mixed systems, like codified laws and a parliament.

Constitutions of many Muslim-majority countries refer to sharia as a source or the main source of law, though these references are not in themselves indicative of how much the legal system is influenced by sharia, and whether the influence has a traditionalist or modernist character. The same constitutions usually also refer to universal principles such as democracy and human rights, leaving it up to legislators and the judiciary to work out how these norms are to be reconciled in practice. Conversely, some countries (e.g., Algeria), whose constitution does not mention sharia, possess sharia-based family laws. Nisrine Abiad identifies Bahrain, Iran, Pakistan, and Saudi Arabia as states with "strong constitutional consequences" of sharia "on the organization and functioning of power".

Except for secular systems, Muslim-majority countries possess sharia-based laws dealing with family matters (marriage, inheritance, etc). These laws generally reflect influence of various modern-era reforms and tend to be characterized by ambiguity, with traditional and modernist interpretations often manifesting themselves in the same country, both in legislation and court decisions. In some countries (e.g., parts of Nigeria and Greece), people can choose whether to pursue a case in a sharia or secular court.

Countries in the Muslim world generally have criminal codes influenced by French law or common law, and in some cases a combination of Western legal traditions. Saudi Arabia has never adopted a criminal code and Saudi judges still follow traditional Hanbali jurisprudence. In the course of Islamization campaigns, several countries (Libya, Pakistan, Iran, Sudan, Mauritania, and Yemen) inserted Islamic criminal laws into their penal codes, which were otherwise based on Western models. In some countries only "hudud" penalties were added, while others also enacted provisions for "qisas" (law of retaliation) and "diya" (monetary compensation). Iran subsequently issued a new "Islamic Penal Code". The criminal codes of Afghanistan and United Arab Emirates contain a general provision that certain crimes are to be punished according to Islamic law, without specifying the penalties. Some Nigerian states have also enacted Islamic criminal laws. Laws in the Indonesian province of Aceh provide for application of discretionary ("ta'zir") punishments for violation of Islamic norms, but explicitly exclude "hudud" and "qisas". Brunei has been implementing a "Sharia Penal Code", which includes provisions for stoning and amputation, in stages since 2014. The countries where "hudud" penalties are legal do not use stoning and amputation routinely, and generally apply other punishments instead.

Sharia also plays a role beyond religious rituals and personal ethics in some countries with Muslim minorities. For example, in Israel sharia-based family laws are administered for the Muslim population by the Ministry of Justice through the Sharia Courts. In India, the Muslim Personal Law (Shariat) Application Act provides for the use of Islamic law for Muslims in several areas, mainly related to family law. In England, the Muslim Arbitration Tribunal makes use of sharia family law to settle disputes, though this limited adoption of sharia is controversial.

Sharia courts traditionally do not rely on lawyers; plaintiffs and defendants represent themselves. In Saudi Arabia and Qatar, which have preserved traditional procedure in sharia courts, trials are conducted solely by the judge, and there is no jury system. There is no pre-trial discovery process, and no cross-examination of witnesses. Unlike common law, judges' verdicts do not set binding precedents under the principle of "stare decisis", and unlike civil law, sharia is left to the interpretation in each case and has no formally codified universal statutes.

The rules of evidence in sharia courts traditionally prioritize oral testimony, and witnesses must be Muslim. Male Muslim witnesses are deemed more reliable than female Muslim witnesses, and non-Muslim witnesses considered unreliable and receive no priority in a sharia court. In civil cases in some countries, a Muslim woman witness is considered half the worth and reliability than a Muslim man witness. In criminal cases, women witnesses are unacceptable in stricter, traditional interpretations of sharia, such as those found in Hanbali jurisprudence, which forms the basis of law in Saudi Arabia.

A confession, an oath, or the oral testimony of Muslim witnesses are the main evidence admissible in traditional sharia courts for hudud crimes, i.e., the religious crimes of adultery, fornication, rape, accusing someone of illicit sex but failing to prove it, apostasy, drinking intoxicants and theft. According to classical jurisprudence, testimony must be from at least two free Muslim male witnesses, or one Muslim male and two Muslim females, who are not related parties and who are of sound mind and reliable character. Testimony to establish the crime of adultery, fornication or rape must be from four Muslim male witnesses, with some fiqhs allowing substitution of up to three male with six female witnesses; however, at least one must be a Muslim male. Forensic evidence ("i.e.", fingerprints, ballistics, blood samples, DNA etc.) and other circumstantial evidence may likewise rejected in hudud cases in favor of eyewitnesses in some modern interpretations. In the case of regulations that were part of local Malaysian legislation that did not go into effect, this could cause severe difficulties for women plaintiffs in rape cases. In Pakistan, DNA evidence is rejected in paternity cases on the basis of legislation that favors the presumption of children's legitimacy, while in sexual assault cases DNA evidence is regarded as equivalent to expert opinion and evaluated on a case-by-case basis.

 recommends written financial contracts with reliable witnesses, although there is dispute about equality of female testimony.

Marriage is solemnized as a written financial contract, in the presence of two Muslim male witnesses, and it includes a brideprice (Mahr) payable from a Muslim man to a Muslim woman. The brideprice is considered by a sharia court as a form of debt. Written contracts were traditionally considered paramount in sharia courts in the matters of dispute that are debt-related, which includes marriage contracts. Written contracts in debt-related cases, when notarized by a judge, is deemed more reliable.

In commercial and civil contracts, such as those relating to exchange of merchandise, agreement to supply or purchase goods or property, and others, oral contracts and the testimony of Muslim witnesses historically triumphed over written contracts. Islamic jurists traditionally held that written commercial contracts may be forged. Timur Kuran states that the treatment of written evidence in religious courts in Islamic regions created an incentive for opaque transactions, and the avoidance of written contracts in economic relations. This led to a continuation of a "largely oral contracting culture" in Muslim-majority nations and communities.

In lieu of written evidence, oaths are traditionally accorded much greater weight; rather than being used simply to guarantee the truth of ensuing testimony, they are themselves used as evidence. Plaintiffs lacking other evidence to support their claims may demand that defendants take an oath swearing their innocence, refusal thereof can result in a verdict for the plaintiff. Taking an oath for Muslims can be a grave act; one study of courts in Morocco found that lying litigants would often "maintain their testimony right up to the moment of oath-taking and then to stop, refuse the oath, and surrender the case." Accordingly, defendants are not routinely required to swear before testifying, which would risk casually profaning the Quran should the defendant commit perjury; instead oaths are a solemn procedure performed as a final part of the evidence process.

In classical jurisprudence monetary compensation for bodily harm ("diya" or blood money) is assessed differently for different classes of victims. For example, for Muslim women the amount was half that assessed for a Muslim man. "Diya" for the death of a free Muslim man is twice as high as for Jewish and Christian victims according to the Maliki and Hanbali madhhabs and three times as high according to Shafi'i rules. Several legals schools assessed "diya" for Magians ("majus") at one-fifteenth the value of a free Muslim male.

Modern countries which incorporate classical "diya" rules into their legal system treat them in different ways. The Pakistan Penal Code modernized the Hanafi doctrine by eliminating distinctions between Muslims and non-Muslims. In Iran, "diya" for non-Muslim victims professing one of the faiths protected under the constitution (Jews, Christians, and Zoroastrians) was made equal to "diya" for Muslims in 2004, though according to a 2006 US State Department report, the penal code still discriminates against other religious minorities and women. According to Human Rights Watch and the US State Department, in Saudi Arabia Jewish or Christian male plaintiffs are entitled to half the amount a Muslim male would receive, while for all other non-Muslim males the proportion is one-sixteenth.

The spread of codified state laws and Western-style legal education in the modern Muslim world has displaced traditional muftis from their historical role of clarifying and elaborating the laws applied in courts. Instead, fatwas have increasingly served to advise the general public on other aspects of sharia, particularly questions regarding religious rituals and everyday life. Modern fatwas deal with topics as diverse as insurance, sex-change operations, moon exploration and beer drinking. Most Muslim-majority states have established national organizations devoted to issuing fatwas, and these organizations to a considerable extent replaced independent muftis as religious guides for the general population. State-employed muftis generally promote a vision of Islam that is compatible with state law of their country.

Modern public and political fatwas have addressed and sometimes sparked controversies in the Muslim world and beyond. Ayatollah Khomeini's proclamation condemning Salman Rushdie to death for his novel "The Satanic Verses" is credited with bringing the notion of fatwa to world's attention, although some scholars have argued that it did not qualify as one. Together with later militant fatwas, it has contributed to the popular misconception of the fatwa as a religious death warrant.

Modern fatwas have been marked by an increased reliance on the process of "ijtihad", i.e. deriving legal rulings based on an independent analysis rather than conformity with the opinions of earlier legal authorities ("taqlid"), and some of them are issued by individuals who do not possess the qualifications traditionally required of a mufti. The most notorious examples are the fatwas of militant extremists. When Osama Bin Laden and his associates issued a fatwa in 1998 proclaiming "jihad against Jews and Crusaders", many Islamic jurists, in addition to denouncing its content, stressed that bin Laden was not qualified to either issue a fatwa or proclaim a jihad. New forms of ijtihad have also given rise to fatwas that support such notions as gender equality and banking interest, which are at variance with classical jurisprudence.

In the internet age, a large number of websites provide fatwas in response to queries from around the world, in addition to radio shows and satellite television programs offering call-in fatwas. Erroneous and sometimes bizarre fatwas issued by unqualified or eccentric individuals in recent times have sometimes given rise to complaints about a "chaos" in the modern practice of issuing fatwas. There exists no international Islamic authority to settle differences in interpretation of Islamic law. An International Islamic Fiqh Academy was created by the Organisation of Islamic Cooperation, but its legal opinions are not binding. The vast amount of fatwas produced in the modern world attests to the importance of Islamic authenticity to many Muslims. However, there is little research available to indicate to what extent Muslims acknowledge the authority of different muftis or heed their rulings in real life.

The classical doctrine of "hisba", associated with the Quranic injunction of "enjoining good and forbidding wrong", refers to the duty of Muslims to promote moral rectitude and intervene when another Muslim is acting wrongly. Historically, its legal implementation was entrusted to a public official called "muhtasib" (market inspector), who was charged with preventing fraud, disturbance of public order and infractions against public morality. This office disappeared in the modern era everywhere in the Muslim world, but it was revived in Arabia by the first Saudi state, and later instituted as a government committee responsible for supervising markets and public order. It has been aided by volunteers enforcing attendance of daily prayers, gender segregation in public places, and a conservative notion of hijab. Committee officers were authorized to detain violators before a 2016 reform. With the rising international influence of Wahhabism, the conception of "hisba" as an individual obligation to police religious observance has become more widespread, which led to the appearance of activists around the world who urge fellow Muslims to observe Islamic rituals, dress code, and other aspects of sharia.

In Iran, "hisba" was enshrined in the constitution after the 1979 Revolution as a "universal and reciprocal duty", incumbent upon both the government and the people. Its implementation has been carried out by official committees as well as volunteer forces ("basij"). Elsewhere, policing of various interpretations of sharia-based public morality has been carried out by the Kano State Hisbah Corps in the Nigerian state of Kano, by "Polisi Perda Syariah Islam" in the Aceh province of Indonesia, by the Committee for the Propagation of Virtue and the Prevention of Vice in the Gaza Strip, and by the Taleban during their 1996-2001 rule of Afghanistan. Religious police organizations tend to have support from conservative currents of public opinion, but their activities are often disliked by other segments of the population, especially liberals, urban women, and younger people.

In Egypt, a law based on the doctrine of hisba had for a time allowed a Muslim to sue another Muslim over beliefs that may harm society, though because of abuses it has been amended so that only the state prosecutor may bring suit based on private requests. Before the amendment was passed, a hisba suit brought by a group of Islamists against the liberal theologian Nasr Abu Zayd on charges of apostasy led to annulment of his marriage. The law was also invoked in an unsuccessful blasphemy suit against the feminist author Nawal El Saadawi. Hisba has also been invoked in several Muslim-majority countries as rationale for blocking pornographic content on the internet and for other forms of faith-based censorship.

A 2013 survey based on interviews of 38,000 Muslims, randomly selected from urban and rural parts in 39 countries using area probability designs, by the Pew Forum on Religion and Public Life found that a majority—in some cases "overwhelming" majority—of Muslims in a number of countries support making "sharia" or "Islamic law" the law of the land, including Afghanistan (99%), Iraq (91%), Niger (86%), Malaysia (86%), Pakistan (84%), Morocco (83%), Bangladesh (82%), Egypt (74%), Indonesia (72%), Jordan (71%), Uganda (66%), Ethiopia (65%), Mali (63%), Ghana (58%), and Tunisia (56%). In Muslim regions of Southern-Eastern Europe and Central Asia, the support is less than 50%: Russia (42%), Kyrgyzstan (35%), Tajikistan (27%), Kosovo (20%), Albania (12%), Turkey (12%), Kazakhstan (10%), Azerbaijan (8%). Regional averages of support were 84% in South Asia, 77% in Southeast Asia, 74% in the Middle-East/North Africa, 64%, in Sub-Saharan Africa, 18% in Southern-Eastern Europe, and 12% in Central Asia .

However, while most of those who support implementation of sharia favor using it in family and property disputes, fewer supported application of severe punishments such as whippings and cutting off hands, and interpretations of some aspects differed widely. According to the Pew poll, among Muslims who support making sharia the law of the land, most do not believe that it should be applied to non-Muslims. In the Muslim-majority countries surveyed this proportion varied between 74% (of 74% in Egypt) and 19% (of 10% in Kazakhstan), as percentage of those who favored making sharia the law of the land.

In all of the countries surveyed, respondents were more likely to define sharia as "the revealed word of God" rather than as "a body of law developed by men based on the word of God". In analyzing the poll, Amaney Jamal has argued that there is no single, shared understanding of the notions "sharia" and "Islamic law" among the respondents. In particular, in countries where Muslim citizens have little experience with rigid application of sharia-based state laws, these notions tend to be more associated with Islamic ideals like equality and social justice than with prohibitions. Other polls have indicated that for Egyptians, the word "sharia" is associated with notions of political, social and gender justice.

In 2008, Rowan Williams, the Archbishop of Canterbury, has suggested that Islamic and Orthodox Jewish courts should be integrated into the British legal system alongside ecclesiastical courts to handle marriage and divorce, subject to agreement of all parties and strict requirements for protection of equal rights for women. His reference to the sharia sparked a controversy. Later that year, Nicholas Phillips, then Lord Chief Justice of England and Wales, stated that there was "no reason why sharia principles [...] should not be the basis for mediation or other forms of alternative dispute resolution." A 2008 YouGov poll in the United Kingdom found 40% of Muslim students interviewed supported the introduction of sharia into British law for Muslims. Michael Broyde, professor of law at Emory University specializing in alternative dispute resolution and Jewish law, has argued that sharia courts can be integrated into the American religious arbitration system, provided that they adopt appropriate institutional requirements as American rabbinical courts have done.

In the Western world, sharia has been called a source of "hysteria", "more controversial than ever", the one aspect of Islam that inspires "particular dread". On the Internet, "dozens of self-styled counter-jihadis" emerged to campaign against sharia law, describing it in strict interpretations resembling those of Salafi Muslims. Also, fear of sharia law and of the ideology of extremism among Muslims as well as certain congregations donating money to terrorist organizations within the Muslim community reportedly spread to mainstream conservative Republicans in the United States. Former House Speaker Newt Gingrich won ovations calling for a federal ban on sharia law.
The issue of "liberty versus Sharia" was called a "momentous civilisational debate" by right-wing pundit Diana West.
In 2008 in Britain, the future Prime Minister (David Cameron) declared his opposition to "any expansion of Sharia law in the UK." In Germany, in 2014, the Interior Minister (Thomas de Maizière) told a newspaper ("Bild"), "Sharia law is not tolerated on German soil."

Some countries and jurisdictions have explicit bans on sharia law. In Canada, for example, sharia law has been explicitly banned in Quebec by a 2005 unanimous vote of the National Assembly, while the province of Ontario allows family law disputes to be arbitrated only under Ontario law. In the U.S., opponents of Sharia have sought to ban it from being considered in courts, where it has been routinely used alongside traditional Jewish and Catholic laws to decide legal, business, and family disputes subject to contracts drafted with reference to such laws, as long as they do not violate secular law or the U.S. constitution. After failing to gather support for a federal law making observing Sharia a felony punishable by up to 20 years in prison, anti-Sharia activists have focused on state legislatures. By 2014, bills aimed against use of Sharia have been introduced in 34 states and passed in 11. These bills have generally referred to banning foreign or religious law in order to thwart legal challenges.

According to Jan Michiel Otto, Professor of Law and Governance in Developing Countries at Leiden University, "[a]nthropological research shows that people in local communities often do not distinguish clearly whether and to what extent their norms and practices are based on local tradition, tribal custom, or religion. Those who adhere to a confrontational view of sharia tend to ascribe many undesirable practices to sharia and religion overlooking custom and culture, even if high-ranking religious authorities have stated the opposite."

Esposito and DeLong-Bas distinguish four attitudes toward sharia and democracy prominent among Muslims today:


Polls conducted by Gallup and PEW in Muslim-majority countries indicate that most Muslims see no contradiction between democratic values and religious principles, desiring neither a theocracy, nor a secular democracy, but rather a political model where democratic institutions and values can coexist with the values and principles of sharia.

Muslih and Browers identify three major perspectives on democracy among prominent Muslims thinkers who have sought to develop modern, distinctly Islamic theories of socio-political organization conforming to Islamic values and law:


In 1998 the Constitutional Court of Turkey banned and dissolved Turkey's Refah Party over its announced intention to introduce sharia-based laws, ruling that it would change Turkey's secular order and undermine democracy. On appeal by Refah the European Court of Human Rights determined that "sharia is incompatible with the fundamental principles of democracy". Refah's sharia-based notion of a "plurality of legal systems, grounded on religion" was ruled to contravene the European Convention for the Protection of Human Rights and Fundamental Freedoms. It was determined that it would "do away with the State's role as the guarantor of individual rights and freedoms" and "infringe the principle of non-discrimination between individuals as regards their enjoyment of public freedoms, which is one of the fundamental principles of democracy". In an analysis, Maurits S. Berger found the ruling to be "nebulous" and surprising from a legal point of view, since the Court neglected to define what it meant by "sharia" and would not, for example, be expected to regard sharia rules for Islamic rituals as contravening European human rights values. Kevin Boyle also criticized the decision for not distinguishing between extremist and mainstream interpretations of Islam and implying that peaceful advocacy of Islamic doctrines ("an attitude which fails to respect [the principle of secularism]") is not protected by the European Convention provisions for freedom of religion.

Governments of several predominantly Muslim countries have criticized the Universal Declaration of Human Rights (UDHR) for its perceived failure to take into account the cultural and religious context of non-Western countries. Iran declared in the UN assembly that UDHR was "a secular understanding of the Judeo-Christian tradition", which could not be implemented by Muslims without trespassing the Islamic law. Islamic scholars and Islamist political parties consider 'universal human rights' arguments as imposition of a non-Muslim culture on Muslim people, a disrespect of customary cultural practices and of Islam. In 1990, the Organisation of Islamic Cooperation, a group representing all Muslim-majority nations, met in Cairo to respond to the UDHR, then adopted the Cairo Declaration on Human Rights in Islam.

Ann Elizabeth Mayer points to notable absences from the Cairo Declaration: provisions for democratic principles, protection for religious freedom, freedom of association and freedom of the press, as well as equality in rights and equal protection under the law. Article 24 of the Cairo declaration states that "all the rights and freedoms stipulated in this Declaration are subject to the Islamic "shari'a"".

In 2009, the journal "Free Inquiry" summarized the criticism of the Cairo Declaration in an editorial: "We are deeply concerned with the changes to the Universal Declaration of Human Rights by a coalition of Islamic states within the United Nations that wishes to prohibit any criticism of religion and would thus protect Islam's limited view of human rights. In view of the conditions inside the Islamic Republic of Iran, Egypt, Pakistan, Saudi Arabia, the Sudan, Syria, Bangladesh, Iraq, and Afghanistan, we should expect that at the top of their human rights agenda would be to rectify the legal inequality of women, the suppression of political dissent, the curtailment of free expression, the persecution of ethnic minorities and religious dissenters – in short, protecting their citizens from egregious human rights violations. Instead, they are worrying about protecting Islam."

H. Patrick Glenn states that sharia is structured around the concept of mutual obligations of a collective, and it considers individual human rights as potentially disruptive and unnecessary to its revealed code of mutual obligations. In giving priority to this religious collective rather than individual liberty, the Islamic law justifies the formal inequality of individuals (women, non-Islamic people). Bassam Tibi states that sharia framework and human rights are incompatible. Abdel al-Hakeem Carney, in contrast, states that sharia is misunderstood from a failure to distinguish "sharia" from "siyasah" (politics).

In classical fiqh, blasphemy refers to any form of cursing, questioning or annoying God, Muhammad or anything considered sacred in Islam, including denying one of the Islamic prophets or scriptures, insulting an angel or refusing to accept a religious commandment. Jurists of different schools prescribed different punishment for blasphemy against Islam, by Muslims and non-Muslims, ranging from imprisonment or fines to the death penalty. In some cases, sharia allows non-Muslims to escape death by converting and becoming a devout follower of Islam. In the modern Muslim world, the laws pertaining to blasphemy vary by country, and some countries prescribe punishments consisting of fines, imprisonment, flogging, hanging, or beheading.

Blasphemy laws were rarely enforced in pre-modern Islamic societies, but in the modern era some states and radical groups have used charges of blasphemy in an effort to burnish their religious credentials and gain popular support at the expense of liberal Muslim intellectuals and religious minorities.

Blasphemy, as interpreted under sharia, is controversial. Representatives of the Organisation of Islamic Cooperation have petitioned the United Nations to condemn "defamation of religions" because "Unrestricted and disrespectful freedom of opinion creates hatred and is contrary to the spirit of peaceful dialogue". The Cairo Declaration on Human Rights in Islam subjects free speech to unspecified sharia restrictions: Article 22(a) of the Declaration states that "Everyone shall have the right to express his opinion freely in such manner as would not be contrary to the principles of the Shariah." Others, in contrast, consider blasphemy laws to violate freedom of speech, stating that freedom of expression is essential to empowering both Muslims and non-Muslims, and point to the abuse of blasphemy laws in prosecuting members of religious minorities, political opponents, and settling personal scores. In Pakistan, blasphemy laws have been used to convict more than a thousand people, about half of them Ahmadis and Christians. While none have been legally executed, two Pakistani politicians, Shahbaz Bhatti and Salmaan Taseer, have been assassinated over their criticism of the blasphemy laws. Although the laws were inherited from British colonial legislation and then expanded and "Islamized" in the 1980s, many Pakistanis believe that they are taken directly from the Quran.

According to the classical doctrine, apostasy from Islam is a crime as well as a sin, punishable with the death penalty, typically after a waiting period to allow the apostate time to repent and to return to Islam. Wael Hallaq writes that "[in] a culture whose lynchpin is religion, religious principles and religious morality, apostasy is in some way equivalent to high treason in the modern nation-state". Early Islamic jurists set the standard for apostasy from Islam so high that practically no apostasy verdict could be passed before the 11th century, but later jurists lowered the bar for applying the death penalty, allowing judges to interpret the apostasy law in different ways, which they did sometimes leniently and sometimes strictly. In the late 19th century, the use of criminal penalties for apostasy fell into disuse, although civil penalties were still applied.

According to Abdul Rashied Omar, the majority of modern Islamic jurists continue to regard apostasy as a crime deserving the death penalty. This view is dominant in conservative societies like Saudi Arabia and Pakistan. A number of liberal and progressive Islamic scholars have argued that apostasy should not be viewed as a crime.

Twenty-three Muslim-majority countries, , penalized apostasy from Islam through their criminal laws. 
, apostasy from Islam was a capital offense in Afghanistan, Brunei, Mauritania, Qatar, Saudi Arabia, Sudan, the United Arab Emirates, and Yemen. In other countries, sharia courts could use family laws to void the Muslim apostate's marriage and to deny child-custody rights as well as inheritance rights. In the years 1985-2006, four individuals were legally executed for apostasy from Islam: "one in Sudan in 1985; two in Iran, in 1989 and 1998; and one in Saudi Arabia in 1992." While modern states have rarely prosecuted apostasy, the issue has a "deep cultural resonance" in some Muslim societies and Islamists have tended to exploit it for political gain. In a 2008-2012 Pew Research Center poll, public support for capital punishment for apostasy among Muslims ranged from 78% in Afghanistan to less than 1% in Kazakhstan, reaching over 50% in 6 of the 20 countries surveyed.

Homosexual intercourse is illegal in classical sharia, with different penalties, including capital punishment, stipulated depending of the situation and legal school. In pre-modern Islam, the penalties prescribed for homosexual acts were "to a large extent theoretical", owing in part to stringent procedural requirements for their harsher ("hudud") forms and in part to prevailing social tolerance toward same-sex relationships. Historical instances of prosecution for homosexual acts are rare, and those which followed sharia rules are even rarer. Public attitudes toward homosexuality in the Muslim world turned more negative starting from the 19th century under the influence of sexual notions prevalent in Europe at that time. A number of Muslim-majority countries have retained criminal penalties for homosexual acts enacted under colonial rule. In recent decades, prejudice against LGBT individuals in the Muslim world has been exacerbated by increasingly conservative attitudes and the rise of Islamist movements, resulting in sharia-based penalties enacted in several countries. The death penalty for homosexual acts is currently a legal punishment in Brunei, Iran, Mauritania, some northern states in Nigeria, Pakistan, Qatar, Saudi Arabia, parts of Somalia, Sudan, and Yemen, all of which have sharia-based criminal laws. It is unclear whether the laws of Afghanistan and United Arab Emirates provide for the death penalty for gay sex. Criminalization of consensual homosexual acts and especially making them liable to capital punishment has been condemned by international rights groups. According to polls, the level of social acceptance for homosexuality ranges from 52% among Muslims in the U.S. to less than 10% in a number of Muslim-majority nations.

Some extremists have used their interpretation of Islamic scriptures and sharia, in particular the doctrine of jihad, to justify acts of war and terror against Muslim as well as non-Muslim individuals and governments.

In classical fiqh, the term "jihad" refers to armed struggle against unbelievers. Classical jurists developed an elaborate set of rules pertaining to jihad, including prohibitions on harming those who are not engaged in combat. According to Bernard Lewis, "[a]t no time did the classical jurists offer any approval or legitimacy to what we nowadays call terrorism" and the terrorist practice of suicide bombing "has no justification in terms of Islamic theology, law or tradition". In the modern era the notion of jihad has lost its jurisprudential relevance and instead gave rise to an ideological and political discourse. While modernist Islamic scholars have emphasized defensive and non-military aspects of jihad, some radical Islamists have advanced aggressive interpretations that go beyond the classical theory. For al-Qaeda ideologues, in jihad all means are legitimate, including targeting Muslim non-combatants and the mass killing of non-Muslim civilians. According to these interpretations, Islam does not discriminate between military and civilian targets, but rather between Muslims and nonbelievers, whose blood can be legitimately spilled.

Some modern ulema, such as Yusuf al-Qaradawi and Sulaiman Al-Alwan, have supported suicide attacks against Israeli civilians, arguing that they are army reservists and hence should be considered as soldiers, while Hamid bin Abdallah al-Ali declared that suicide attacks in Chechnya were justified as a "sacrifice". Many prominent Islamic scholars, including al-Qaradawi himself, have issued condemnations of terrorism in general terms. For example, Abdul-Aziz ibn Abdullah Al ash-Sheikh, the Grand Mufti of Saudi Arabia has stated that "terrorizing innocent people [...] constitute[s] a form of injustice that cannot be tolerated by Islam", while Muhammad Sayyid Tantawy, Grand Imam of al-Azhar and former Grand Mufti of Egypt has stated that "attacking innocent people is not courageous; it is stupid and will be punished on the Day of Judgment".

According to some interpretations, sharia condones certain forms of domestic violence against women, when a husband suspects "nushuz" (disobedience, disloyalty, rebellion, ill conduct) in his wife. Others believe that wife beating is not consistent with modern perspectives of the Quran.

One of the verses of the Quran relating to permissibility of domestic violence is Surah 4:34, which has been subject to varied interpretations. Traditional interpretations of sharia have been criticized as inconsistent with women's rights in domestic abuse cases. Musawah, CEDAW, KAFA and other organizations have proposed ways to modify sharia-inspired laws to improve women's rights in Muslim-majority nations, including women's rights in domestic abuse cases.

Shari'a is the basis for personal status laws in most Islamic-majority nations. These personal status laws determine rights of women in matters of marriage, divorce and child custody. A 2011 UNICEF report concludes that sharia law provisions are discriminatory against women from a human rights perspective. In many countries, in legal proceedings relating to sharia-based personal status law, a woman’s testimony is worth half of a man’s before a court.

The 1917 codification of Islamic family law in the Ottoman empire distinguished between the age of competence for marriage, which was set at 18 for boys and 17 for girls, and the minimum age for marriage, which followed the traditional Hanafi limits of 12 for boys and 9 for girls. Marriage below the age of competence was permissible only if proof of sexual maturity was accepted in court, while marriage under the minimum age was forbidden. During the 20th century, most countries in the Middle East followed the Ottoman precedent in defining the age of competence, while raising the minimum age to 15 or 16 for boys and 13-16 for girls. Marriage below the age of competence is subject to approval by a judge and the legal guardian of the adolescent. Egypt diverged from this pattern by setting the age limits of 18 for boys and 16 for girls, without a distinction between competence for marriage and minimum age. Many senior clerics in Saudi Arabia have opposed setting a minimum age for marriage, arguing that a woman reaches adulthood at puberty.

Rape is considered a crime in all countries of the North Africa and Middle East region, but as of 2011, sharia-based or secular laws in some countries, including Bahrain, Iraq, Jordan, Libya, Morocco, Syria and Tunisia, allowed a rapist to escape punishment by marrying his victim, while in other countries, including Libya, Oman, Saudi Arabia and United Arab Emirates, rape victims who press charges risk being prosecuted for extramarital sex ("zina").

Islamic law granted Muslim women certain legal rights, such as property rights which women in the West did not possess until "comparatively recent times". Starting with the 20th century, Western legal systems evolved to expand women's rights, but women's rights in the Muslim world have to varying degree remained tied to the Quran, hadiths and their traditional interpretations by Islamic jurists. Sharia grants women the right to inherit property from other family members, and these rights are detailed in the Quran. A woman's inheritance is unequal and less than a man's, and dependent on many factors. For instance, a daughter's inheritance is usually half that of her brother's.

Sharia recognizes the basic inequality between master and women slave, between free women and slave women, between Believers and non-Believers, as well as their unequal rights. Sharia authorized the institution of slavery, using the words "abd" (slave) and the phrase "ma malakat aymanukum" ("that which your right hand owns") to refer to women slaves, seized as captives of war. Under Islamic law, Muslim men could have sexual relations with female captives and slaves. Slave women under sharia did not have a right to own property or to move freely. Sharia, in Islam's history, provided a religious foundation for enslaving non-Muslim women (and men), but allowed for the manumission of slaves. However, manumission required that the non-Muslim slave first convert to Islam. A slave woman who bore a child to her Muslim master ("umm al-walad") could not be sold, becoming legally free upon her master's death, and the child was considered free and a legitimate heir of the father.

Islamic legal tradition has a number of parallels with Judaism. In both religions, revealed law holds a central place, in contrast to Christianity which does not possess a body of revealed law, and where theology rather than law is considered to be the principal field of religious study. Both Islamic and Jewish law ("Halakha") are derived from formal textual revelations (Quran and Pentateuch) as well as less formal, orally transmitted prophetic traditions (hadith and "mishna"). According to some scholars, the words "sharia" and "halakha" both mean literally "the path to follow". The "fiqh" literature parallels rabbinical law developed in the Talmud, with fatwas being analogous to rabbinic "responsa". However, the emphasis on "qiyas" in classical Sunni legal theory is both more explicitly permissive than Talmudic law with respect to authorizing individual reason as a source of law, and more implicitly restrictive, in excluding other, unauthorized forms of reasoning.

Early Islamic law developed a number of legal concepts that anticipated similar such concepts that later appeared in English common law. Similarities exist between the royal English contract protected by the action of debt and the Islamic "Aqd", between the English assize of novel disseisin and the Islamic "Istihqaq", and between the English jury and the Islamic "Lafif" in classical Maliki jurisprudence. The law schools known as Inns of Court also parallel Madrasahs. The methodology of legal precedent and reasoning by analogy ("Qiyas") are also similar in both the Islamic and common law systems, as are the English trust and agency institutions to the Islamic "Waqf" and "Hawala" institutions, respectively.

Elements of Islamic law also have other parallels in Western legal systems. For example, the influence of Islam on the development of an international law of the sea can be discerned alongside that of the Roman influence.

George Makdisi has argued that the madrasa system of attestation paralleled the legal scholastic system in the West, which gave rise to the modern university system. The triple status of "faqih" ("master of law"), "mufti" ("professor of legal opinions") and "mudarris" ("teacher"), conferred by the classical Islamic legal degree, had its equivalents in the medieval Latin terms "magister", "professor" and "doctor", respectively, although they all came to be used synonymously in both East and West. Makdisi suggested that the medieval European doctorate, "licentia docendi" was modeled on the Islamic degree "ijazat al-tadris wa-l-ifta’", of which it is a word-for-word translation, with the term "ifta’" (issuing of fatwas) omitted. He also argued that these systems shared fundamental freedoms: the freedom of a professor to profess his personal opinion and the freedom of a student to pass judgement on what he is learning.

There are differences between Islamic and Western legal systems. For example, sharia classically recognizes only natural persons, and never developed the concept of a legal person, or corporation, i.e., a legal entity that limits the liabilities of its managers, shareholders, and employees; exists beyond the lifetimes of its founders; and that can own assets, sign contracts, and appear in court through representatives. Interest prohibitions imposed secondary costs by discouraging record keeping and delaying the introduction of modern accounting. Such factors, according to Timur Kuran, have played a significant role in retarding economic development in the Middle East.






</doc>
<doc id="28841" url="https://en.wikipedia.org/wiki?curid=28841" title="Sunnah">
Sunnah

Sunnah (, , plural ), also sunna or sunnat, is the body of literature which discusses and prescribes the traditional customs and practices of the Islamic community, both social and legal, often but not necessarily based on the verbally transmitted record of the teachings, deeds and sayings, silent permissions (or disapprovals) of the Islamic prophet Muhammad, as well as various reports about Muhammad's companions. The Quran (the holy book of Islam) and the "sunnah" make up the two primary sources of Islamic theology and law. The "sunnah" is also defined as "a path, a way, a manner of life"; "all the traditions and practices" of the Islamic prophet that "have become models to be followed" by Muslims.

In the pre-Islamic period, the word "sunnah" was used with the meaning "manner of acting", whether good or bad. During the early Islamic period, the term came to refer to any good precedent set by people of the past, including Muhammad. Under the influence of Al-Shafi‘i, who argued for priority of Muhammad's example as recorded in hadith over precedents set by other authorities, the term "al-sunnah" eventually came to be viewed as synonymous with the "sunnah" of Muhammad.

The "sunnah" of Muhammad includes his specific words ("Sunnah Qawliyyah"), habits, practices ("Sunnah Fiiliyyah"), and silent approvals ("Sunnah Taqririyyah"). According to Muslim belief, Muhammad was the best exemplar for Muslims, and his practices are to be adhered to in fulfilling the divine injunctions, carrying out religious rites, and moulding life in accord with the will of God. Instituting these practices was, as the Quran states, a part of Muhammad's responsibility as a messenger of God. Recording the "sunnah" was an Arabian tradition and, once people converted to Islam, they brought this custom to their religion.

The word ""sunnah"" is also used to refer to religious duties that are optional, such as "Sunnah salat".

' ( , plural ' ) is an Arabic word that means "habit" or "usual practice".

Sunni Muslims are also referred to as "Ahl as-Sunnah wa'l-Jamā'ah" ("people of the tradition and the community (of Muhammad)") or "Ahl as-Sunnah" for short. Some early Sunnî Muslim scholars (such as Abu Hanifa, al-Humaydî, Ibn Abî `Âsim, Abû Dâwûd, and Abû Nasr al-Marwazî) reportedly used the term "the sunnah" narrowly to refer to Sunni Doctrine as opposed to the creeds of Shia and other non-Sunni sects. Sunnah literally means face, nature , lifestyle etc. In the time of prophet Muhammad's companion, newly converted muslims accepted and rejected some set of creed by using reason. So many early muslim scholar started writing books on creed entitled as 'sunnah'. 

According to scholars such as Joseph Schacht and Ignác Goldziher the pre-Islamic definition of sunnah was simply "precedent" or "way of life". It was first used with the meaning of "law" in the Syro-Roman law book before it became widely used in Islamic jurisprudence.

Early schools of Islamic jurisprudence also had a more flexible definition of sunnah than was used later, that being "acceptable norms" or "custom", and was not limited to “traditions traced back to the Prophet Muhammad himself” ("sunna al-nabawiyyah"). It included examples of the Prophet's Companions, the rulings of the Caliphs, and practices that “had gained general acceptance among the jurists of that school”. Evidence of the use of other “sunnas” at this time is found in the hadith comment made about a report on the difference in the number of lashes used to punish alcohol consumption (Muhammad and Abu Bakr ordered 40 lashes, Umar 80) — “All this is sunna”; and also on Umar’s deathbed instructions on where Muslims should seek guidance: from the Qur’an, the early Muslims ("muhajirun") who emigrated to Medina with Muhammad, the Medina residents who welcomed and supported the "muhajirun" (the "ansar"), the people of the desert, and the protected communities of Jews and Christians ("ahl al-dhimma").

It was Abū ʿAbdullāh Muhammad ibn Idrīs al-Shāfiʿī (150-204 AH), known as al-Shafi'i, who argued against this practice, emphasizing the final authority of a hadith of Muhammad, so that even the Qur'an was "to be interpreted in the light of traditions (i.e. hadith), and not vice versa." While the sunnah has often been called "second to the Quran", (it has also been said to "rule over and interpret the Quran") 
Al-Shafi'i "forcefully argued" that the sunnah stands "on equal footing with the Quran", (according to scholar Daniel Brown) for (as Al-Shafi'i put it) “the command of the Prophet is the command of God.”

His success was such that later writers “hardly ever thought of sunna as comprising anything but that of the Prophet”.

In the 1960s, Fazlur Rahman Malik, an Islamic modernist and former head of Pakistan's Central Institute for Islamic Research, advanced another idea for how the (Prophetic) sunnah should be understood: as the normative example of the Prophet, but not "filled with absolutely specific content". Rather it should be "a general umbrella concept" that could and should evolve as a "living and on-going process". He argued that Muhammad had come as a "moral reformer" and not a "pan-legit", and that the community of his followers would agree on the specifics of the sunna. If Western and Muslim scholars found that the isnad (chain of transmitters) and content of ahadith had been tampered by someone trying to prove the Muhammad had made a specific statement, this did not mean they were fraudulent. "Hadith verbally speaking does not go back to the Prophet, its spirit certainly does". If hadith changed from the early schools to the time of al-Shafi'i, and then through tampering from al-Shafi'i to the collections of ahadith of al-Bukhari and al-Muslim's, they actually formed a kind of "ijma" (consensus or agreement of the Muslim scholars). According to Rahman they were "materially identical" to "ijma".

Basic features of the sunnah — such as worship rituals like "salat" (ritual prayer), "zakat" (ritual tithing), "hajj" (pilgrimage to Mecca), "sawm" (dawn to dusk fasting during Ramadan) — are known to Muslim from being passed down `from the many to the many` (according to scholars of fiqh such as Al-Shafi'i), rather than from consultation with books of hadith, (more often used to consult for answers to details not agreed upon or not frequently practiced).

According to Javed Ahmad Ghamidi, another Modernist, this passing down by continuous practice of the Muslim community (which indicates consensus) was similar to how the Qur’ān has been "received by the "ummah"" (Muslim community) through the consensus of the Prophet's Companions and through their perpetual recitation. Consequently, Ghamidi sees this continuous practice sunnah as the true sunnah — equally authentic to the Quran, but shedding orthodox sunnah and avoiding problematic basis of the hadith.

According to the view of some Sufi Muslims who incorporate both the outer and inner reality of Muhammad, the deeper and true sunnah are the noble characteristics and inner state of Muhammad. To them Muhammad's attitude, his piety, the quality of his character constitute the truer and deeper aspect of what it means by sunnah in Islam, rather than the external aspects alone. They argue that the external customs of Muhammad loses its meaning without the inner attitude and also many hadiths are simply custom of the Arabs, not something that is unique to Muhammad. and "Khuluqin Azim" or 'Exalted Character' in the Quran, real sunnah cannot be upheld.


In addition to being "the way" of Islam or the traditional social and legal custom and practice of the Islamic community, sunnah is often used as a synonym for “"mustahabb" (encouraged)” rather than "wajib"/"fard" (obligatory) regarding some commendable action (usually the saying of a prayer). "Mustahabb"/sunnah deeds are those that earn a reward in the afterlife for those who do them, but will not bring any punishment for those who neglect them. According to Islam Q&A website of Muhammed Salih Al-Munajjid this second definition of sunna is used by "scholars of usool and fiqh" for acts that are “"mustahabb" (encouraged)”, in the five categories of Sharia rulings (known as “the five decisions” or five akram).

Salât as-Sunnah (Arabic: صلاة السنة) are optional prayers performed in addition to the five daily compulsory Salât prayers. Some are done at the same time as the compulsory prayers, some are done only at certain times, e.g. late at night, and some are only done for specific occasions such as during a drought. They are called sunnah because how they are practiced is based on stories, narrations, interpretations, traditions of Muhammad by his companions.
"Examples include al-Sunan al-Rawaatib" (sunnah prayers which Muhammad did regularly), ""Salaat al-Duhaa" and so on."
"Sunnah Mu’akkadah" are actions Muhammad "never omitted to do, whether he was travelling or not," such as the prayers Sunnat al-Fajr and al-Witr.

The word “Sunna” appears several times in the Qur’an, but there is no specific mention of "sunnat al-rasool" (sunna of the messenger) or "sunnat al-nabi" or "sunna al-nabawiyyah" (sunna of the prophet), i.e. the way/practice of Prophet Muhammad. (There are several verses calling on Muslims to obey Muhammad—see below.) Four verses (8.38, 15.13, 18.55) use the expression “"sunnat al-awwalin"”, which is thought to mean “the way or practice of the ancients.” It is described as something "that has passed away" or prevented unbelievers from accepting God. “"Sunnat Allah"” (the “way of God”) appears eight times in five verses. In addition, verse 17.77 talks of both the way of other, earlier Muslim messengers (Ibrahim, Musa, etc.), and of "our way", i.e. God's way.
[This is] the way ("sunna") of those whom we sent [as messengers] before you, and you will not find any change in Our way ("sunnatuna").

This indicates to some scholars (such as Javed Ahmad Ghamidi) that sunnah predates both the Quran and Muhammad, and is actually the tradition of the prophets of God, specifically the tradition of Abraham. Christians, Jews and the Arab descendants of Ishmael, the Arabized Arabs or Ishmaelites, when Muhammad reinstituted this practice as an integral part of Islam.

The Qur'an contains numerous commands to follow the Prophet. Among the Quranic verses quoted as demonstrating the importance of hadith/sunnah to Muslims are 
Say: Obey Allah and obey the Messenger,

Which appears in several verses: 3:32, 5:92, 24:54, 64:12

Your companion [Muhammad] has not strayed, nor has he erred, Nor does he speak from [his own] inclination or desire.

"A similar (favour have ye already received) in that We have sent among you a Messenger of your own, rehearsing to you Our Signs, and sanctifying you, and instructing you in Scripture and Wisdom, and in new knowledge.

"Ye have indeed in the Messenger of Allah a beautiful pattern (of conduct) for any one whose hope is in Allah and the Final Day, and who engages much in the Praise of Allah."

The teachings of "wisdom" have been declared to be a function of Muhammad along with the teachings of the scripture. Several Quranic verses mention "wisdom" ("hikmah") coupled with "scripture" or "the book" (i.e. the Quran), and it is thought that in this context, "wisdom" means the "sunnah". 
Surah 4 (An-Nisa), ayah 113 states: "For Allah hath sent down to thee the Book and wisdom and taught thee what thou Knewest not (before): And great is the Grace of Allah unto thee." 
Surah 2 (Al-Baqara), ayah 231: "...but remember Allah's grace upon you and that which He hath revealed unto you of the Scripture and of wisdom, whereby He doth exhort you." 
Surah 33 (Al-Ahzab), ayah 34: "And bear in mind which is recited in your houses of the revelations of God and of wisdom".

Therefore, along with the Quran the "sunnah" was revealed. Modern Sunni scholars have examined both the "sira" and the "hadith" in order to justify modifications to jurisprudence ("fiqh"). 
For Muslims the imitation of Muhammad helps one to know and be loved by God.

According to John Burton, paraphrasing Al-Shafi'i, "it must be remembered that the Quran text are couched in very general terms which it is the function of the sunnah to expand and elucidate, to make God's meaning absolutely clear."
There are a number of verses in the Quran where "to understand the context, as well as the meaning", Muslims need to refer to the record of the life and example of the Prophet.

It is thought that verses 16:44 and 64 indicate that Muhammed's mission "is not merely that of a deliveryman who simply delivers the revelation from Allah to us, rather, he has been entrusted with the most important task of explaining and illustrating" the Quran. 
And We have also sent down unto you (O Muhammad) the reminder and the advice (the Quran), that you may explain clearly to men what is sent down to them, and that they may give thought.

And We have not sent down the Book (the Quran) to you (O Muhammad), except that you may explain clearly unto them those things in which they differ, and (as) a guidance and a mercy for a folk who believe. [Quran 16:64]

For example, while the Quran presents the general principles of praying, fasting, paying zakat, or making pilgrimage, they are presented "without the illustration found in Hadith, for these acts of worship remain as abstract imperatives in the Qur’an".

There are three types of sunnah:

In the terminology of "fiqh" (Islamic jurisprudence), sunnah denotes whatever though not obligatory, is "firmly established ("thabata") as called for ("matlub")" in Islam "on the basis of a legal proof ("dalîl shar`î").

Abd Allah ibn 'Amr was one of the first companions to write down the hadith, after receiving permission from prophet Muhammad to do so. Abu Hurayrah memorized the hadith.

According to scholar Khaled Abou El Fadl, unlike the Quran, the Sunnah was not recorded and written during the Prophet's lifetime, but was systematically collected and documented beginning at least two centuries after the death of Muhammad (i.e. the ninth century of the Christian era). He states: "the late documentation of the Sunna meant that many of the reports attributed to the Prophet are apocryphal or at least are of dubious historical authenticity. In fact, one of the most complex disciplines in Islamic jurisprudence is one which attempts to differentiate between authentic and inauthentic traditions."

According to scholar Gibril Fouad Haddad, the "sciences of the Sunnah" ("`ulûm as-Sunna") refer to:
the biography of the Prophet ("as-sîra"), the chronicle of his battles ("al-maghâzî"), his everyday sayings and acts or "ways" ("sunan"), his personal and moral qualities ("ash-shamâ'il"), and the host of the ancillary hadîth sciences such as the circumstances of occurrence ("asbâb al-wurûd"), knowledge of the abrogating and abrogated hadîth, difficult words ("gharîb al-hadîth"), narrator criticism ("al-jarh wat-ta`dîl"), narrator biographies ("al-rijâl"), etc., as discussed in great detail in the authoritative books of al-Khatîb al-Baghdâdî.

Originally Muslim lawyers "felt no obligation" to provide documentation of hadith when arguing their case. Over the course of the second century under the influence of Imam Al-Shafi‘i (the founder of the Shafi'i school of jurisprudence), this changed 
so that now there is "rather broad agreement that Hadith must be the basis for authentication of any Sunnah," and the "particular textual source for Sunnah is Hadith", according to M.O. Farooq.

The Saudi Arabian Islam Question and Answer (Supervised by Muhammad Al-Munajjid) states that while:


In the context of biographical records of Muhammad, "sunnah" often stands synonymous with "hadith" since most of the personality traits of Muhammad are known from descriptions of him, his sayings and his actions after becoming a prophet at the age of forty. "Sunnah", which consists not only of sayings, but of what Muhammad believed, implied, or tacitly approved, was recorded by his companions in "hadith". Allegiance to the tribal "sunnah" had been partially replaced by submission to a new universal authority and the sense of brotherhood among Muslims.

Early Sunni scholars often considered "sunnah" equivalent to the biography of Muhammed ("sira"). As the "hadith" came to be better documented and the scholars who validated them gained prestige, the "sunnah" came often to be known mostly through the "hadith", especially as variant or fictional biographies of Muhammad spread.

Classical Islam often equates the "sunnah" with the "hadith". Scholars who studied the narrations according to their context ("matn") as well as their transmission ("isnad") in order to discriminate between them were influential in the development of early Muslim philosophy. In the context of sharia, Malik ibn Anas and the Hanafi scholars are assumed to have differentiated between the two: for example Malik is said to have rejected some traditions that reached him because, according to him, they were against the "established practice of the people of Medina".

Shia Islam does not use the "Kutub al-Sittah" (six major "hadith" collections) followed by Sunni Islam, therefore the Sunnah of Shia Islam and the Sunnah of Sunni Islam refer to different collections of religious canonical literature.

The primary collections of Sunnah of Shia Islam were written by three authors known as the 'Three Muhammads', and they are:


Unlike Akhbari Twelver Shiites, Usuli Twelver Shiite scholars do not believe that everything in the four major books of the Sunnah of Shia Islam is authentic.

In Shia "hadees" one often finds sermons attributed to Ali in The Four Books or in the Nahj al-Balagha.





</doc>
<doc id="28845" url="https://en.wikipedia.org/wiki?curid=28845" title="Safe sex">
Safe sex

Safe sex is sexual activity using methods or devices (such as condoms) to reduce the risk of transmitting or acquiring sexually transmitted infections (STIs), especially HIV. "Safe sex" is also sometimes referred to as safer sex or protected sex to indicate that some safe sex practices do not completely eliminate STI risks. It is also sometimes used colloquially to describe methods aimed at preventing pregnancy that may or may not also lower STI risks.

The concept of "safe sex" emerged in the 1980s as a response to the global AIDS epidemic, and possibly more specifically to the AIDS crisis in the US. Promoting safe sex is now one of the main aims of sex education and STI prevention, especially reducing new HIV infections. Safe sex is regarded as a harm reduction strategy aimed at reducing the risk of STI transmission.

Although some safe sex practices (like condoms) can also be used as birth control ("contraception"), most forms of contraception do not protect against STIs. Likewise, some safe sex practices, such as partner selection and low-risk sex behavior, might not be effective forms of contraception.

Although strategies for avoiding STIs like syphilis and gonorrhea have existed for centuries and the term "safe sex" existed in English as early as the 1930s, the use of the term to refer to STI-risk reduction dates to the mid-1980s in the United States. It emerged in response to the HIV/AIDS crisis.

A year before the HIV virus was isolated and named, the San Francisco chapter of the Sisters of Perpetual Indulgence published a small pamphlet titled "Play Fair!" out of concern over widespread STIs among the city's gay male population. It specifically named illnesses (Kaposi's sarcoma and pneumocystis pneumonia) that would later be understood as symptoms of advanced HIV disease (or AIDS). The pamphlet advocated a range of safe-sex practices, including abstinence, condoms, personal hygiene, use of personal lubricants, and STI testing/treatment. It took a casual, sex-positive approach while also emphasizing personal and social responsibility. In May 1983—the same month HIV was isolated and named in France—the New York City-based HIV/AIDS activists Richard Berkowitz and Michael Callen published similar advice in their booklet, "". Neither publication used the term "safe sex" but both included recommendations that are now standard advice for reducing STI (including HIV) risks.

Safe sex as a form of STI risk reduction appeared in journalism as early as 1984, in the British publication 'The Intelligencer': ""The goal is to reach about 50 million people with messages about safe sex and AIDS education."

Although "safe sex" is used by individuals to refer to protection against both pregnancy and HIV/AIDS or other STI transmissions, the term was born in response to the HIV/AIDS epidemic. It is believed that the term "safe sex" was used in the professional literature in 1984, in the content of a paper on the psychological effect that HIV/AIDS may have on gay and bisexual men.

A year later, the same term appeared in an article in "The New York Times." This article emphasized that most specialists advised their AIDS patients to practice safe sex. The concept included limiting the number of sexual partners, using prophylactics, avoiding bodily fluid exchange, and resisting the use of drugs that reduced inhibitions for high-risk sexual behavior. Moreover, in 1985, the first safe sex guidelines were established by the 'Coalition for Sexual Responsibilities'. According to these guidelines, safe sex was practiced by using condoms also when engaging in anal or oral sex.

Although the term "safe sex" was primarily used in reference to sexual activity between men, in 1986 the concept was spread to the general population. Various programs were developed with the aim of promoting safe sex practices among college students. These programs were focused on promoting the use of the condom, a better knowledge about the partner's sexual history and limiting the number of sexual partners. The first book on this subject appeared in the same year. The book was entitled "Safe Sex in the Age of AIDS", it had 88 pages and it described both positive and negative approaches to sexual life. Sexual behavior could be either safe (kissing, hugging, massage, body-to-body rubbing, mutual masturbation, exhibitionism, phone sex, and use of separate sex toys); possibly safe (use of condoms); and unsafe.

In 1997, specialists in this matter promoted the use of condoms as the most accessible safe sex method (besides abstinence) and they called for TV commercials featuring condoms. During the same year, the Catholic Church in the United States issued their own "safer sex" guidelines on which condoms were listed, though two years later the Vatican urged chastity and heterosexual marriage, attacking the American Catholic bishops' guidelines.

A study carried out in 2006 by Californian specialists showed that the most common definitions of safe sex are condom use (68% of the interviewed subjects), abstinence (31.1% of the interviewed subjects), monogamy (28.4% of the interviewed subjects) and safe partner (18.7% of the interviewed subjects).

The term "safer sex" in Canada and the United States has gained greater use by health workers, reflecting that risk of transmission of sexually transmitted infections in various sexual activities is a continuum. The term "safe sex" is still in common use in the United Kingdom, Australia and New Zealand.

"Safer sex" is thought to be a more aggressive term which may make it more obvious to individuals that any type of sexual activity carries a certain degree of risk.

The term "safe love" has also been used, notably by the French Sidaction in the promotion of men's underpants incorporating a condom pocket and including the red ribbon symbol in the design, which were sold to support the charity.

A range of safe-sex practices are commonly recommended by sexual health educators and public health agencies. Many of these practices can reduce (but not completely eliminate) risk of transmitting or acquiring STIs.

Sexual activities, such as phone sex, cybersex, and sexting, that do not include direct contact with the skin or bodily fluids of sexual partners, carry no STI risks and, thus, are forms of safe sex.

A range of sex acts called "non-penetrative sex" or "outercourse" can significantly reduce STI risks. Non-penetrative sex includes practices such as kissing, mutual masturbation, rubbing or stroking. According to the Health Department of Western Australia, this sexual practice may prevent pregnancy and most STIs. However, non-penetrative sex may not protect against infections that can be transmitted via skin-to-skin contact, such as herpes and human papilloma virus. Mutual or partnered masturbation carries some STI risk, especially if there is skin contact or shared bodily fluids with sexual partners, although the risks are significantly lower than many other sexual activities.

Barriers, such as condoms, dental dams, and medical gloves can prevent contact with body fluids (such as blood, vaginal fluid, semen, rectal mucus), and other means of transmitting STIs (like skin, hair and shared objects) during sexual activity.

Oil-based lubrication can break down the structure of latex condoms, dental dams or gloves, reducing their effectiveness for STI protection.

While use of external condoms can reduce STI risks during sexual activity, they are not 100% effective. One study has suggested condoms might reduce HIV transmission by 85% to 95%; effectiveness beyond 95% was deemed unlikely because of slippage, breakage, and incorrect use. It also said, "In practice, inconsistent use may reduce the overall effectiveness of condoms to as low as 60–70%".

Pre-exposure prophylaxis (often abbreviated as "PrEP") is the use of prescription drugs by those who do not have HIV to prevent HIV infection. PrEP drugs are taken "prior" to HIV exposure to prevent the transmission of the virus, usually between sexual partners. PrEP drugs do not prevent other STI infections or pregnancy.

As of 2018, the most-widely approved form of "PrEP" combines two drugs (tenofovir and emtricitabine) in one pill. That drug combination is sold under the brand name Truvada by Gilead Sciences. It is also sold in generic formulations worldwide. Other drugs are also being studied for use as PrEP.

Different countries have approved different protocols for using the tenofovir/emtricitabine-combination drug as "PrEP". That two-drug combination has been shown to prevent HIV infection in different populations when taken daily, intermittently, and on demand. Numerous studies have found the tenofovir/emtricitabine combination to be over 90% effective at preventing HIV transmission between sexual partners.

Treatment as Prevention (often abbreviated as "TasP") is the practice of testing for and treating HIV infection as a way to prevent further spread of the virus. Those having knowledge of their HIV-positive status can use safe-sex practices to protect themselves and their partners (such as using condoms, sero-sorting partners, or choosing less-risky sexual activities). And, because HIV-positive people with durably suppressed or undetectable amounts of HIV in their blood "cannot transmit HIV to sexual partners", sexual activity with HIV-positive partners on effective treatment is a form of safe sex (to prevent HIV infection). This fact has given rise to the concept of "U=U" ("Undetectable = Untransmittable").

Other methods proven effective at reducing STI risks during sexual activity are:

Most methods of contraception are not effective at preventing the spread of STIs. This includes birth control pills, vasectomy, tubal ligation, periodic abstinence, IUDs and many non-barrier methods of pregnancy prevention. However, condoms are highly effective for birth control and STI prevention.

The spermicide Nonoxynol-9 has been claimed to reduce the likelihood of STI transmission. However, a technical report by the World Health Organization has shown that Nonoxynol-9 is an irritant and can produce tiny tears in mucous membranes, which may increase the risk of transmission by offering pathogens more easy points of entry into the system. They reported that Nonoxynol-9 lubricant do not have enough spermicide to increase contraceptive effectiveness cautioned they should not be promoted. There is no evidence that spermicidal condoms are better at preventing STD transmission compared to condoms that do not have spermicide. If used properly, spermicidal condoms can prevent pregnancy, but there is still an increased risk that nonoxynyl-9 can irritate the skin, making it more susceptible for infections.

The use of a diaphragm or contraceptive sponge provides some women with better protection against certain sexually transmitted diseases, but they are not effective for all STIs.

Hormonal methods of preventing pregnancy (such as oral contraceptives [i.e. 'The pill'], depoprogesterone, hormonal IUDs, the vaginal ring, and the patch) offer no protection against STIs. The copper intrauterine device and the hormonal intrauterine device provide an up to 99% protection against pregnancies but no protection against STIs. Women with copper intrauterine device "may" be subject to greater risk of infection from bacterial infectious such as gonorrhea or chlamydia, although this is debated.

Coitus interruptus (or "pulling out"), in which the penis is removed from the vagina, anus, or mouth before ejaculation, may reduce transmission of STIs but still carries significant risk. This is because pre-ejaculate, a fluid that oozes from the penile urethra before ejaculation, may contain STI pathogens. Additionally, the microbes responsible for some diseases, including genital warts and syphilis, can be transmitted through skin-to-skin or mucous membrane contact.

Unprotected anal penetration is considered a high-risk sexual activity because the thin tissues of the anus and rectum can be easily damaged. Slight injuries can allow the passage of bacteria and viruses, including HIV. This includes penetration of the anus by fingers, hands, or sex toys such as dildos. Also, condoms may be more likely to break during anal sex than during vaginal sex, increasing the risk of STI transmission.

The main risk which individuals are exposed to when performing anal sex is the transmission of HIV. Other possible infections include Hepatitis A, B and C; intestinal parasite infections like "Giardia"; and bacterial infections such as "Escherichia coli."

Anal sex should be avoided by couples in which one of the partners has been diagnosed with an STI until the treatment has proven to be effective.

In order to make anal sex safer, the couple must ensure that the anal area is clean and the bowel empty and the partner on whom anal penetration occurs should be able to relax. Regardless of whether anal penetration occurs by using a finger or the penis, the condom is the best barrier method to prevent transmission of STI. Enemas should be not be used as they can increase the risk of HIV infection and lymphogranuloma venereum proctitis.

Since the rectum can be easily damaged, the use of lubricants is highly recommended even when penetration occurs by using the finger. Especially for beginners, using a condom on the finger is both a protection measure against STI and a lubricant source. Most condoms are lubricated and they allow less painful and easier penetration. Oil-based lubricants damage latex and should not be used with condoms; water-based and silicone-based lubricants are available instead. Non-latex condoms are available for people who are allergic to latex made out of polyurethane or polyisoprene. Polyurethane condoms can safely be used with oil-based lubricant. The "female condom" may also be used effectively by the anal receiving partner.

Anal stimulation with a sex toy requires similar safety measures to anal penetration with a penis, in this case using a condom on the sex toy in a similar way.

It is important that the man washes and cleans his penis after anal intercourse if he intends to penetrate the vagina. Bacteria from the rectum are easily transferred to the vagina, which may cause vaginal and urinary tract infections.

When anal-oral contact occurs, protection is required since this is a risky sexual behavior in which illnesses as Hepatitis A or STIs can be easily transmitted, as well as enteric infections. The dental dam or the plastic wrap are effective protection means whenever anilingus is performed.

Putting a condom on a sex toy provides better sexual hygiene and can help to prevent transmission of infections if the sex toy is shared, provided the condom is replaced when used by a different partner. Some sex toys are made of porous materials, and pores retain viruses and bacteria, which makes it necessary to clean sex toys thoroughly, preferably with use of cleaners specifically for sex toys. Glass is non-porous and medical grade glass sex toys more easily sterilized between uses.

In cases in which one of the partners is treated for an STI, it is recommended that the couple not use sex toys until the treatment has proved to be effective.

All sex toys have to be properly cleaned after use. The way in which a sex toy is cleaned varies on the type of material it is made of. Some sex toys can be boiled or cleaned in a dishwasher. Most of the sex toys come with advice on the best way to clean and store them and these instructions should be carefully followed. A sex toy should be cleaned not only when it is shared with other individuals but also when it is used on different parts of the body (such as mouth, vagina or anus).

A sex toy should regularly be checked for scratches or breaks that can be breeding ground for bacteria. It is best if the damaged sex toy is replaced by a new undamaged one. Even more hygiene protection should be considered by pregnant women when using sex toys. Sharing any type of sex toy that may draw blood, like whips or needles, is not recommended, and is not safe.

When using sex toys in the anus, sex toys "...can easily get lost" as "rectal muscles contract and can suck an object up and up, potentially obstructing the colon"; to prevent this serious problem, sex toy users are advised to use sex "...toys with a flared base or a string".

Sexual abstinence reduces STIs and pregnancy risks associated with sexual contact, but STIs may also be transmitted through non-sexual means, or by rape. HIV may be transmitted through contaminated needles used in tattooing, body piercing, or injections. Medical or dental procedures using contaminated instruments can also spread HIV, while some health-care workers have acquired HIV through occupational exposure to accidental injuries with needles. Evidence does not support the use of abstinence-only sex education. Abstinence-only sex education programs have been found to be ineffective in decreasing rates of HIV infection in the developed world and unplanned pregnancy. Abstinence-only sex education primarily relies on the consequences of character and morality while health care professionals are concerned about matters regarding health outcomes and behaviors. Though abstinence is the best course to prevent pregnancy and STIs, in reality, it is unrealistic so intentions to abstain from sexual activity are often unsuccessful. It leaves young people without the information and skills they need to avoid unwanted pregnancies and STIs.




</doc>
<doc id="28846" url="https://en.wikipedia.org/wiki?curid=28846" title="STD">
STD

STD may refer to:





</doc>
<doc id="28848" url="https://en.wikipedia.org/wiki?curid=28848" title="Scabies">
Scabies

Scabies, also known as the seven-year itch, is a contagious skin infestation by the mite "Sarcoptes scabiei". The most common symptoms are severe itchiness and a pimple-like rash. Occasionally, tiny burrows may be seen in the skin. In a first-ever infection a person will usually develop symptoms in between two and six weeks. During a second infection symptoms may begin in as little as 24 hours. These symptoms can be present across most of the body or just certain areas such as the wrists, between fingers, or along the waistline. The head may be affected, but this is typically only in young children. The itch is often worse at night. Scratching may cause skin breakdown and an additional bacterial infection of the skin.
Scabies is caused by infection with the female mite "Sarcoptes scabiei "var." hominis", an ectoparasite. The mites burrow into the skin to live and deposit eggs. The symptoms of scabies are due to an allergic reaction to the mites. Often, only between 10 and 15 mites are involved in an infection. Scabies is most often spread during a relatively long period of direct skin contact with an infected person (at least 10 minutes) such as that which may occur during sex or living together. Spread of disease may occur even if the person has not developed symptoms yet. Crowded living conditions, such as those found in child-care facilities, group homes, and prisons, increase the risk of spread. Areas with a lack of access to water also have higher rates of disease. Crusted scabies is a more severe form of the disease. It typically only occurs in those with a poor immune system and people may have millions of mites, making them much more contagious. In these cases, spread of infection may occur during brief contact or by contaminated objects. The mite is very small and usually not directly visible. Diagnosis is based on the signs and symptoms.
A number of medications are available to treat those infected, including permethrin, crotamiton, and lindane creams and ivermectin pills. Sexual contacts within the last month and people who live in the same house should also be treated at the same time. Bedding and clothing used in the last three days should be washed in hot water and dried in a hot dryer. As the mite does not live for more than three days away from human skin, more washing is not needed. Symptoms may continue for two to four weeks following treatment. If after this time symptoms continue, retreatment may be needed.
Scabies is one of the three most common skin disorders in children, along with ringworm and bacterial skin infections. As of 2015, it affects about 204 million people (2.8% of the world population). It is equally common in both sexes. The young and the old are more commonly affected. It also occurs more commonly in the developing world and tropical climates. The word scabies is from "", "to scratch". Other animals do not spread human scabies. Infection in other animals is typically caused by slightly different but related mites and is known as sarcoptic mange.

The characteristic symptoms of a scabies infection include intense itching and superficial burrows. The burrow tracks are often linear, to the point that a neat "line" of four or more closely placed and equally developed mosquito-like "bites" is almost diagnostic of the disease. Because the host develops the symptoms as a reaction to the mites' presence over time, typically a delay of four to six weeks occurs between the onset of infestation and the onset of itching. Similarly, symptoms often persist for one to several weeks after successful eradication of the mites. As noted, those re-exposed to scabies after successful treatment may exhibit symptoms of the new infestation in a much shorter period—as little as one to four days.

In the classic scenario, the itch is made worse by warmth, and is usually experienced as being worse at night, possibly because distractions are fewer. As a symptom, it is less common in the elderly.

The superficial burrows of scabies usually occur in the area of the finger webs, feet, ventral wrists, elbows, back, buttocks, and external genitals. Except in infants and the immunosuppressed, infection generally does not occur in the skin of the face or scalp. The burrows are created by excavation of the adult mite in the epidermis.

In most people, the trails of the burrowing mites are linear or S-shaped tracks in the skin often accompanied by rows of small, pimple-like mosquito or insect bites. These signs are often found in crevices of the body, such as on the webs of fingers and toes, around the genital area, in stomach folds of the skin, and under the breasts of women.

Symptoms typically appear two to six weeks after infestation for individuals never before exposed to scabies. For those having been previously exposed, the symptoms can appear within several days after infestation. However, symptoms may appear after several months or years. Acropustulosis, or blisters and pustules on the palms and soles of the feet, are characteristic symptoms of scabies in infants.
The elderly, disabled, and people with an impaired immune system, such as those with HIV, cancer, or those on immunosuppressive medications, are susceptible to crusted scabies (also called Norwegian scabies). On those with weaker immune systems, the host becomes a more fertile breeding ground for the mites, which spread over the host's body, except the face. The mites in crusted scabies are not more virulent than in noncrusted scabies; however, they are much more numerous (up to two million). People with crusted scabies exhibit scaly rashes, slight itching, and thick crusts of skin that contain a large numbers of scabies mites. For this reason, persons with crusted scabies are more contagious to others than those with typical scabies. Such areas make eradication of mites particularly difficult, as the crusts protect the mites from topical miticides/scabicides, necessitating prolonged treatment of these areas.

In the 18th century, Italian biologists Giovanni Cosimo Bonomo and Diacinto Cestoni (1637–1718) described the mite now called "Sarcoptes scabiei", variety "hominis", as the cause of scabies. "Sarcoptes" is a genus of skin parasites and part of the larger family of mites collectively known as scab mites. These organisms have eight legs as adults, and are placed in the same phylogenetic class (Arachnida) as spiders and ticks.

"S. scabiei" mites are under 0.5 mm in size, but are sometimes visible as pinpoints of white. Gravid females tunnel into the dead, outermost layer (stratum corneum) of a host's skin and deposit eggs in the shallow burrows. The eggs hatch into larvae in three to ten days. These young mites move about on the skin and molt into a "nymphal" stage, before maturing as adults, which live three to four weeks in the host's skin. Males roam on top of the skin, occasionally burrowing into the skin. In general, the total number of adult mites infesting a healthy hygienic person with noncrusted scabies is small, about 11 females in burrows, on average.

The movement of mites within and on the skin produces an intense itch, which has the characteristics of a delayed cell-mediated inflammatory response to allergens. IgE antibodies are present in the serum and the site of infection, which react to multiple protein allergens in the body of the mite. Some of these cross-react to allergens from house dust mites. Immediate antibody-mediated allergic reactions (wheals) have been elicited in infected persons, but not in healthy persons; immediate hypersensitivity of this type is thought to explain the observed far more rapid allergic skin response to reinfection seen in persons having been previously infected (especially having been infected within the previous year or two).

Scabies is contagious and can be contracted through prolonged physical contact with an infested person. This includes sexual intercourse, although a majority of cases are acquired through other forms of skin-to-skin contact. Less commonly, scabies infestation can happen through the sharing of clothes, towels, and bedding, but this is not a major mode of transmission; individual mites can survive for only two to three days, at most, away from human skin at room temperature. As with lice, a latex condom is ineffective against scabies transmission during intercourse, because mites typically migrate from one individual to the next at sites other than the sex organs.

Healthcare workers are at risk of contracting scabies from patients, because they may be in extended contact with them.

The symptoms are caused by an allergic reaction of the host's body to mite proteins, though exactly which proteins remains a topic of study. The mite proteins are also present from the gut, in mite feces, which are deposited under the skin. The allergic reaction is both of the delayed (cell-mediated) and immediate (antibody-mediated) type, and involves IgE (antibodies are presumed to mediate the very rapid symptoms on reinfection). The allergy-type symptoms (itching) continue for some days, and even several weeks, after all mites are killed. New lesions may appear for a few days after mites are eradicated. Nodular lesions from scabies may continue to be symptomatic for weeks after the mites have been killed.

Rates of scabies are negatively related to temperature and positively related to humidity.

Scabies may be diagnosed clinically in geographical areas where it is common when diffuse itching presents along with either lesions in two typical spots or itchiness is present in another household member. The classical sign of scabies is the burrow made by a mite within the skin. To detect the burrow, the suspected area is rubbed with ink from a fountain pen or a topical tetracycline solution, which glows under a special light. The skin is then wiped with an alcohol pad. If the person is infected with scabies, the characteristic zigzag or S pattern of the burrow will appear across the skin; however, interpreting this test may be difficult, as the burrows are scarce and may be obscured by scratch marks. A definitive diagnosis is made by finding either the scabies mites or their eggs and fecal pellets. Searches for these signs involve either scraping a suspected area, mounting the sample in potassium hydroxide and examining it under a microscope, or using dermoscopy to examine the skin directly.

Symptoms of early scabies infestation mirror other skin diseases, including dermatitis, syphilis, erythema multiforme, various urticaria-related syndromes, allergic reactions, ringworm-related diseases, and other ectoparasites such as lice and fleas.

Mass-treatment programs that use topical permethrin or oral ivermectin have been effective in reducing the prevalence of scabies in a number of populations. No vaccine is available for scabies. The simultaneous treatment of all close contacts is recommended, even if they show no symptoms of infection (asymptomatic), to reduce rates of recurrence. Since mites can survive for only two to three days without a host, other objects in the environment pose little risk of transmission except in the case of crusted scabies. Therefore cleaning is of little importance. Rooms used by those with crusted scabies require thorough cleaning.

A number of medications are effective in treating scabies. Treatment should involve the entire household, and any others who have had recent, prolonged contact with the infested individual. Options to control itchiness include antihistamines and prescription anti-inflammatory agents. Bedding, clothing and towels used during the previous three days should be washed in hot water and dried in a hot dryer.

Permethrin, a pyrethroid insecticide, is the most effective treatment for scabies, and remains the treatment of choice. It is applied from the neck down, usually before bedtime, and left on for about eight to 14 hours, then washed off in the morning. Care should be taken to coat the entire skin surface, not just symptomatic areas; any patch of skin left untreated can provide a "safe haven" for one or more mites to survive. One application is normally sufficient, as permethrin kills eggs and hatchlings, as well as adult mites, though many physicians recommend a second application three to seven days later as a precaution. Crusted scabies may require multiple applications, or supplemental treatment with oral ivermectin (below). Permethrin may cause slight irritation of the skin that is usually tolerable.

Oral ivermectin is effective in eradicating scabies, often in a single dose. It is the treatment of choice for crusted scabies, and is sometimes prescribed in combination with a topical agent. It has not been tested on infants, and is not recommended for children under six years of age.

Topical ivermectin preparations have been shown to be effective for scabies in adults, though only one such formulation is available in the United States at present, and it is not FDA-approved as a scabies treatment. It has also been useful for sarcoptic mange (the veterinary analog of human scabies). conditions.

Other treatments include lindane, benzyl benzoate, crotamiton, malathion, and sulfur preparations. Lindane is effective, but concerns over potential neurotoxicity have limited its availability in many countries. It is banned in California, but may be used in other states as a second-line treatment. Sulfur ointments or benzyl benzoate are often used in the developing world due to their low cost; Some 10% sulfur solutions have been shown to be effective, and sulfur ointments are typically used for at least a week, though many people find the odor of sulfur products unpleasant. Crotamiton has been found to be less effective than permethrin in limited studies. Crotamiton or sulfur preparations are sometimes recommended instead of permethrin for children, due to concerns over dermal absorption of permethrin.

Scabies is endemic in many developing countries, where it tends to be particularly problematic in rural and remote areas. In such settings, community-wide control strategies are required to reduce the rate of disease, as treatment of only individuals is ineffective due to the high rate of reinfection. Large-scale mass drug administration strategies may be required where coordinated interventions aim to treat whole communities in one concerted effort. Although such strategies have shown to be able to reduce the burden of scabies in these kinds of communities, debate remains about the best strategy to adopt, including the choice of drug.

The resources required to implement such large-scale interventions in a cost-effective and sustainable way are significant. Furthermore, since endemic scabies is largely restricted to poor and remote areas, it is a public health issue that has not attracted much attention from policy makers and international donors.

Scabies is one of the three most common skin disorders in children, along with tinea and pyoderma. As of 2010, it affects about 100 million people (1.5% of the population) and its frequency is not related to gender. The mites are distributed around the world and equally infect all ages, races, and socioeconomic classes in different climates. Scabies is more often seen in crowded areas with unhygienic living conditions. Globally as of 2009, an estimated 300 million cases of scabies occur each year, although various parties claim the figure is either over- or underestimated. About 1–10% of the global population is estimated to be infected with scabies, but in certain populations, the infection rate may be as high as 50–80%.

Scabies has been observed in humans since ancient times. Archeological evidence from Egypt and the Middle East suggests scabies was present as early as 494 BC. The first recorded reference to scabies is believed to be from the Bible – it may be a type of "leprosy" mentioned in Leviticus c. 1200 BC or be mentioned among the curses of Deuteronomy 28. In the fourth century BC, Aristotle reported on "lice" that "escape from little pimples if they are pricked" – a description consistent with scabies.

The Roman encyclopedist and medical writer Aulus Cornelius Celsus (c. 25 BC – 50 AD) is credited with naming the disease "scabies" and describing its characteristic features. The parasitic etiology of scabies was documented by the Italian physician Giovanni Cosimo Bonomo (1663–1696) in his 1687 letter, "Observations concerning the fleshworms of the human body". Bonomo's description established scabies as one of the first human diseases with a well-understood cause.

In Europe in the late 19th through mid-20th centuries, a sulfur-bearing ointment called by the medical eponym of Wilkinson's ointment was widely used for topical treatment of scabies. The contents and origins of several versions of the ointment were detailed in correspondence published in the "British Medical Journal" in 1945.

The International Alliance for the Control of Scabies was started in 2012, and brings together over 150 researchers, clinicians, and public-health experts from more than 15 different countries. It has managed to bring the global health implications of scabies to the attention of the World Health Organization. Consequently, the WHO has included scabies on its official list of neglected tropical diseases and other neglected conditions.

Scabies may occur in a number of domestic and wild animals; the mites that cause these infestations are of different subspecies from the one typically causing the human form. These subspecies can infest animals that are not their usual hosts, but such infections do not last long. Scabies-infected animals suffer severe itching and secondary skin infections. They often lose weight and become frail.

The most frequently diagnosed form of scabies in domestic animals is sarcoptic mange, caused by the subspecies "Sarcoptes scabiei canis", most commonly in dogs and cats. Sarcoptic mange is transmissible to humans who come into prolonged contact with infested animals, and is distinguished from human scabies by its distribution on skin surfaces covered by clothing. Scabies-infected domestic fowl suffer what is known as "scaly leg". Domestic animals that have gone feral and have no veterinary care are frequently afflicted with scabies and a host of other ailments. Nondomestic animals have also been observed to suffer from scabies. Gorillas, for instance, are known to be susceptible to infection by contact with items used by humans.

Moxidectin is being evaluated as a treatment for scabies. It is established in veterinary medicine to treat a range of parasites, including sarcoptic mange. Its advantage over ivermectin is its longer half life in humans and, thus, potential duration of action. Tea tree oil appears to be effective in the laboratory setting.


</doc>
<doc id="28849" url="https://en.wikipedia.org/wiki?curid=28849" title="Shiva">
Shiva

Shiva (; , ISO: , ) also known as Mahadeva () is one of the principal deities of Hinduism. He is the supreme being within Shaivism, one of the major traditions within contemporary Hinduism.

Shiva is known as "The Destroyer" within the Trimurti, the Hindu trinity that includes Brahma and Vishnu. In Shaivism tradition, Shiva is one of the supreme beings who creates, protects and transforms the universe. In the Shaktism tradition, the Goddess, or Devi, is described as one of the supreme, yet Shiva is revered along with Vishnu and Brahma. A goddess is stated to be the energy and creative power (Shakti) of each, with Parvati (Sati) the equal complementary partner of Shiva. He is one of the five equivalent deities in Panchayatana puja of the Smarta tradition of Hinduism.

According to the Shaivism sect, the highest form of Ishvar is formless, limitless, transcendent and unchanging absolute Brahman, and the primal Atman (soul, self) of the universe. There are many both benevolent and fearsome depictions of Shiva. In benevolent aspects, he is depicted as an omniscient Yogi who lives an ascetic life on Mount Kailash as well as a householder with wife Parvati and his two children, Ganesha and Kartikeya. In his fierce aspects, he is often depicted slaying demons. Shiva is also known as Adiyogi Shiva, regarded as the patron god of yoga, meditation and arts.

The iconographical attributes of Shiva are the serpent around his neck, the adorning crescent moon, the holy river Ganga flowing from his matted hair, the third eye on his forehead, the trishula or trident, as his weapon, and the damaru drum. He is usually worshipped in the aniconic form of lingam. Shiva is a pan-Hindu deity, revered widely by Hindus, in India, Nepal and Sri Lanka.

Shiva is also called as "Bhrahman" which can also be said as Parabhrahman. Shiva means nothingness. The word "shivoham" means the consciousness of one individual, the lord says that he is omnipotent, omnipresent, as he is present in the form of one's consciousness. In Tamil, he was called by different names other than Sivan. Nataraja (Dancing form of Shiva), Rudra (Enraged form of Shiva), and Dhakshinamoorthy (Yoga form of Shiva). Nataraja is the only form of Shiva worshipped in a human figure format. Elsewhere he is worshipped in Lingam figure. Pancha Bootha temples are located in south India. Pancha Bhoota Stalam refers to five temples dedicated to Shiva. Tamil literature is enriched by Shiva devotees called 63 Nayanmars (Nayanars).
The Sanskrit word "" (, also transliterated as "shiva") means, states Monier Monier-Williams, "auspicious, propitious, gracious, benign, kind, benevolent, friendly". The roots of in folk etymology are "śī" which means "in whom all things lie, pervasiveness" and "va" which means "embodiment of grace".

The word Shiva is used as an adjective in the Rig Veda (approximately 1700–1100 BC), as an epithet for several Rigvedic deities, including Rudra. The term Shiva also connotes "liberation, final emancipation" and "the auspicious one", this adjective sense of usage is addressed to many deities in Vedic layers of literature. The term evolved from the Vedic "Rudra-Shiva" to the noun "Shiva" in the Epics and the Puranas, as an auspicious deity who is the "creator, reproducer and dissolver".

Sharva, sharabha presents another etymology with the Sanskrit root "-", which means "to injure" or "to kill", interprets the name to connote "one who can kill the forces of darkness".

The Sanskrit word ' means "relating to the god Shiva", and this term is the Sanskrit name both for one of the principal sects of Hinduism and for a member of that sect. It is used as an adjective to characterize certain beliefs and practices, such as Shaivism.

Some authors associate the name with the Tamil word ' meaning "red", noting that Shiva is linked to the Sun (', "the Red one", in Tamil) and that Rudra is also called "Babhru" (brown, or red) in the Rigveda. The "Vishnu sahasranama" interprets "Shiva" to have multiple meanings: "The Pure One", and "the One who is not affected by three Guṇas of Prakṛti (Sattva, Rajas, and Tamas)".

Shiva is known by many names such as Viswanatha (lord of the universe), Mahadeva, Mahandeo, Mahasu, Mahesha, Maheshvara, Shankara, Shambhu, Rudra, Hara, Trilochana, Devendra (chief of the gods), Neelakanta, Subhankara, Trilokinatha (lord of the three realms), and Ghrneshwar (lord of compassion). The highest reverence for Shiva in Shaivism is reflected in his epithets ' ("Great god"; ' "Great" and "deva" "god"), ' ("Great Lord"; ' "great" and "" "lord"), and "" ("Supreme Lord").

Sahasranama are medieval Indian texts that list a thousand names derived from aspects and epithets of a deity. There are at least eight different versions of the "Shiva Sahasranama", devotional hymns ("stotras") listing many names of Shiva. The version appearing in Book 13 () of the "Mahabharata" provides one such list. Shiva also has "Dasha-Sahasranamas" (10,000 names) that are found in the "Mahanyasa". The "Shri Rudram Chamakam", also known as the "Śatarudriya", is a devotional hymn to Shiva hailing him by many names.

The Shiva-related tradition is a major part of Hinduism, found all over the Indian subcontinent, such as India, Nepal, Sri Lanka, and Southeast Asia, such as Bali, Indonesia. Scholars have interpreted early prehistoric paintings at the Bhimbetka rock shelters, carbon dated to be from pre-10,000 BCE period, as Shiva dancing, Shiva's trident, and his mount Nandi. Rock paintings from Bhimbetka, depicting a figure with a trishul, have been described as Nataraja by Erwin Neumayer, who dates them to the mesolithic.

Of several Indus valley seals that show animals, one seal that has attracted attention shows a large central figure, either horned or wearing a horned headdress and possibly ithyphallic, seated in a posture reminiscent of the Lotus position, surrounded by animals. This figure was named by early excavators of Mohenjo-daro as "Pashupati" (Lord of Animals, Sanskrit '), an epithet of the later Hindu deities Shiva and Rudra.

Sir John Marshall and others suggested that this figure is a prototype of Shiva, with three faces, seated in a "yoga posture" with the knees out and feet joined. Semi-circular shapes on the head were interpreted as two horns. Scholars such as Gavin Flood, John Keay and Doris Meth Srinivasan have expressed doubts about this suggestion.

Gavin Flood states that it is not clear from the seal that the figure has three faces, is seated in a yoga posture, or even that the shape is intended to represent a human figure. He characterizes these views as "speculative", but adds that it is nevertheless possible that there are echoes of Shaiva iconographic themes, such as half-moon shapes resembling the horns of a bull. John Keay writes that "he may indeed be an early manifestation of Lord Shiva as Pashu-pati", but a couple of his specialties of this figure does not match with Rudra. Writing in 1997, Srinivasan interprets what John Marshall interpreted as facial as not human but more bovine, possibly a divine buffalo-man.

The interpretation of the seal continues to be disputed. McEvilley, for example, states that it is not possible to "account for this posture outside the yogic account". Asko Parpola states that other archaeological finds such as the early Elamite seals dated to 3000-2750 BCE show similar figures and these have been interpreted as "seated bull" and not a yogi, and the bovine interpretation is likely more accurate. Gregory L. Possehl in 2002, associated it with the water buffalo, and concluded that while it would be appropriate to recognize the figure as a deity, and its posture as one of ritual discipline, regarding it as a proto-Shiva would "go too far".

The Vedic literature refers to a minor atmospheric deity, with fearsome powers called Rudra. The Rigveda, for example, has 3 out of 1,028 hymns dedicated to Rudra, and he finds occasional mention in other hymns of the same text. The term Shiva also appears in the Rigveda, but simply as an epithet, that means "kind, auspicious", one of the adjectives used to describe many different Vedic deities. While fierce ruthless natural phenomenon and storm-related Rudra is feared in the hymns of the Rigveda, the beneficial rains he brings are welcomed as Shiva aspect of him. This healing, nurturing, life-enabling aspect emerges in the Vedas as Rudra-Shiva, and in post-Vedic literature ultimately as Shiva who combines the destructive and constructive powers, the terrific and the gentle, as the ultimate recycler and rejuvenator of all existence.

The similarities between the iconography and theologies of Shiva with Greek and European deities have led to proposals for an Indo-European link for Shiva, or lateral exchanges with ancient central Asian cultures. His contrasting aspects such as being terrifying or blissful depending on the situation, are similar to those of the Greek god Dionysus, as are their iconic associations with bull, snakes, anger, bravery, dancing and carefree life. The ancient Greek texts of the time of Alexander the Great call Shiva as "Indian Dionysus", or alternatively call Dionysus as ""god of the Orient"". Similarly, the use of phallic symbol as an icon for Shiva is also found for Irish, Nordic, Greek (Dionysus) and Roman deities, as was the idea of this aniconic column linking heaven and earth among early Indo-Aryans, states Roger Woodward. Others contest such proposals, and suggest Shiva to have emerged from indigenous pre-Aryan tribal origins.

Shiva as we know him today shares many features with the Vedic god Rudra, and both Shiva and Rudra are viewed as the same personality in Hindu scriptures. The two names are used synonymously. Rudra, the god of the roaring storm, is usually portrayed in accordance with the element he represents as a fierce, destructive deity.

The oldest surviving text of Hinduism is the Rig Veda, which is dated to between 1700 and 1100 BC based on linguistic and philological evidence. A god named Rudra is mentioned in the Rig Veda. The name Rudra is still used as a name for Shiva. In RV 2.33, he is described as the "Father of the Rudras", a group of storm gods.

The hymn 10.92 of the Rigveda states that deity Rudra has two natures, one wild and cruel (Rudra), another that is kind and tranquil (Shiva). The Vedic texts do not mention bull or any animal as the transport vehicle ("vahana") of Rudra or other deities. However, post-Vedic texts such as the Mahabharata and the Puranas state the Nandi bull, the Indian zebu, in particular, as the vehicle of Rudra and of Shiva, thereby unmistakably linking them as same.

Rudra and Agni have a close relationship. The identification between Agni and Rudra in the Vedic literature was an important factor in the process of Rudra's gradual development into the later character as Rudra-Shiva. The identification of Agni with Rudra is explicitly noted in the "Nirukta", an important early text on etymology, which says, "Agni is also called Rudra." The interconnections between the two deities are complex, and according to Stella Kramrisch:
In the "Śatarudrīya", some epithets of Rudra, such as ("Of golden red hue as of flame") and ("Flaming bright"), suggest a fusing of the two deities. Agni is said to be a bull, and Lord Shiva possesses a bull as his vehicle, Nandi. The horns of Agni, who is sometimes characterized as a bull, are mentioned. In medieval sculpture, both Agni and the form of Shiva known as Bhairava have flaming hair as a special feature.

According to Wendy Doniger, the Puranic Shiva is a continuation of the Vedic Indra. Doniger gives several reasons for her hypothesis. Both are associated with mountains, rivers, male fertility, fierceness, fearlessness, warfare, the transgression of established mores, the Aum sound, the Supreme Self. In the Rig Veda the term "" is used to refer to Indra. (2.20.3, 6.45.17, and 8.93.3.) Indra, like Shiva, is likened to a bull. In the Rig Veda, Rudra is the father of the Maruts, but he is never associated with their warlike exploits as is Indra.

The Vedic beliefs and practices of the pre-classical era were closely related to the hypothesised Proto-Indo-European religion, and the pre-Islamic Indo-Iranian religion. The earliest iconic artworks of Shiva may be from Gandhara and northwest parts of ancient India. There is some uncertainty as the artwork that has survived is damaged and they show some overlap with meditative Buddha-related artwork, but the presence of Shiva's trident and phallic symbolism in this art suggests it was likely Shiva. Numismatics research suggests that numerous coins of the ancient Kushan Empire that have survived, were images of a god who is probably Shiva. The Shiva in Kushan coins is referred to as Oesho of unclear etymology and origins, but the simultaneous presence of Indra and Shiva in the Kushan era artwork suggest that they were revered deities by the start of the Kushan Empire.

The texts and artwork of Jainism show Indra as a dancer, although not identical generally resembling the dancing Shiva artwork found in Hinduism, particularly in their respective mudras. For example, in the Jain caves at Ellora, extensive carvings show dancing Indra next to the images of Tirthankaras in a manner similar to Shiva Nataraja. The similarities in the dance iconography suggests that there may be a link between ancient Indra and Shiva.

Rudra's evolution from a minor Vedic deity to a supreme being is first evidenced in the "Shvetashvatara Upanishad" (400–200 BC), according to Gavin Flood. Prior to it, the Upanishadic literature is monistic, and the "Shvetashvatara" text presents the earliest seeds of theistic devotion to Rudra-Shiva. Here Rudra-Shiva is identified as the creator of the cosmos and liberator of souls from the birth-rebirth cycle. The period of 200 BC to 100 AD also marks the beginning of the Shaiva tradition focused on the worship of Shiva as evidenced in other literature of this period. Shaiva devotees and ascetics are mentioned in Patanjali's "Mahābhāṣya" (2nd-century BC) and in the "Mahabharata". Other scholars such as Robert Hume and Doris Srinivasan state that the "Shvetashvatara Upanishad" presents pluralism, pantheism, or henotheism, rather than being a text just on Shiva theism.
The Shaiva Upanishads are a group of 14 minor Upanishads of Hinduism variously dated from the last centuries of the 1st millennium BCE through the 17th century. These extol Shiva as the metaphysical unchanging reality Brahman and the Atman (soul, self), and include sections about rites and symbolisms related to Shiva.

A few texts such as "Atharvashiras Upanishad" mention Rudra, and assert all gods are Rudra, everyone and everything is Rudra, and Rudra is the principle found in all things, their highest goal, the innermost essence of all reality that is visible or invisible. The "Kaivalya Upanishad" similarly, states Paul Deussen – a German Indologist and professor of Philosophy, describes the self-realized man as who "feels himself only as the one divine essence that lives in all", who feels identity of his and everyone's consciousness with Shiva (highest Atman), who has found this highest Atman within, in the depths of his heart.

The Shaiva Puranas, particularly the Shiva Purana and the Linga Purana, present the various aspects of Shiva, mythologies, cosmology and pilgrimage ("Tirtha") associated with him. The Shiva-related Tantra literature, composed between the 8th and 11th centuries, are regarded in devotional dualistic Shaivism as Sruti. Dualistic Shaiva Agamas which consider soul within each living being and Shiva as two separate realities (dualism, "dvaita"), are the foundational texts for Shaiva Siddhanta. Other Shaiva Agamas teach that these are one reality (monism, "advaita"), and that Shiva is the soul, the perfection and truth within each living being. In Shiva related sub-traditions, there are ten dualistic Agama texts, eighteen qualified monism-cum-dualism Agama texts and sixty-four monism Agama texts.

Shiva-related literature developed extensively across India in the 1st millennium CE and through the 13th century, particularly in Kashmir and Tamil Shaiva traditions. The monist Shiva literature posit absolute oneness, that is Shiva is within every man and woman, Shiva is within every living being, Shiva is present everywhere in the world including all non-living being, and there is no spiritual difference between life, matter, man and Shiva. The various dualistic and monist Shiva-related ideas were welcomed in medieval southeast Asia, inspiring numerous Shiva-related temples, artwork and texts in Indonesia, Myanmar, Cambodia, Laos, Vietnam, Thailand and Malaysia, with syncretic integration of local pre-existing theologies.

The figure of Shiva as we know him today may be an amalgamation of various older deities into a single figure. How the persona of Shiva converged as a composite deity is not understood, a challenge to trace and has attracted much speculation. According to Vijay Nath, for example: 
An example of assimilation took place in Maharashtra, where a regional deity named Khandoba is a patron deity of farming and herding castes. The foremost center of worship of Khandoba in Maharashtra is in Jejuri. Khandoba has been assimilated as a form of Shiva himself, in which case he is worshipped in the form of a lingam. Khandoba's varied associations also include an identification with Surya and Karttikeya.

Shaivism is one of the four major sects of Hinduism, the others being Vaishnavism, Shaktism and the Smarta Tradition. Followers of Shaivism, called "Shaivas", revere Shiva as the Supreme Being. Shaivas believe that Shiva is All and in all, the creator, preserver, destroyer, revealer and concealer of all that is. He is not only the creator in Shaivism, but he is also the creation that results from him, he is everything and everywhere. Shiva is the primal soul, the pure consciousness and Absolute Reality in the Shaiva traditions.

The Shaivism theology is broadly grouped into two: the popular theology influenced by Shiva-Rudra in the Vedas, Epics and the Puranas; and the esoteric theology influenced by the Shiva and Shakti-related Tantra texts. The Vedic-Brahmanic Shiva theology includes both monist ("Advaita") and devotional traditions ("Dvaita") such as Tamil Shaiva Siddhanta and Lingayatism with temples featuring items such as linga, Shiva-Parvati iconography, bull Nandi within the premises, relief artwork showing mythologies and aspects of Shiva.

The Tantric Shiva tradition ignored the mythologies and Puranas related to Shiva, and depending on the sub-school developed a spectrum of practices. For example, historical records suggest the tantric Kapalikas (literally, the 'skull-men') co-existed with and shared many Vajrayana Buddhist rituals, engaged in esoteric practices that revered Shiva and Shakti wearing skulls, begged with empty skulls, used meat, alcohol, and sexuality as a part of ritual. In contrast, the esoteric tradition within Kashmir Shaivism has featured the "Krama" and "Trika" sub-traditions. The Krama sub-tradition focussed on esoteric rituals around Shiva-Kali pair. The Trika sub-tradition developed a theology of triads involving Shiva, combined it with an ascetic lifestyle focusing on personal Shiva in the pursuit of monistic self liberation.

The Vaishnava (Vishnu-oriented) literature acknowledges and discusses Shiva. Like Shaiva literature that presents Shiva as supreme, the Vaishnava literature presents Vishnu as supreme. However, both traditions are pluralistic and revere both Shiva and Vishnu (along with Devi), their texts do not show exclusivism, and Vaishnava texts such as the "Bhagavata Purana" while praising Krishna as the Ultimate Reality, also present Shiva and Shakti as a personalized form an equivalent to the same Ultimate Reality. The texts of Shaivism tradition similarly praise Vishnu. The Skanda Purana, for example, states:

Mythologies of both traditions include legends about who is superior, about Shiva paying homage to Vishnu, and Vishnu paying homage to Shiva. However, in texts and artwork of either tradition, the mutual salutes are symbolism for complementarity. The Mahabharata declares the unchanging Ultimate Reality (Brahman) to be identical to Shiva and to Vishnu, that Vishnu is the highest manifestation of Shiva, and Shiva is the highest manifestation of Vishnu.

The goddess-oriented Shakti tradition of Hinduism is based on the premise that the Supreme Principle and the Ultimate Reality called Brahman is female (Devi), but it treats the male as her equal and complementary partner. This partner is Shiva.

The earliest evidence of the tradition of reverence for the feminine with Rudra-Shiva context, is found in the Hindu scripture "Rigveda", in a hymn called the Devi Sukta:

The "Devi Upanishad" in its explanation of the theology of Shaktism, mentions and praises Shiva such as in its verse 19. Shiva, along with Vishnu, is a revered god in the "Devi Mahatmya", a text of Shaktism considered by the tradition to be as important as the "Bhagavad Gita". The Ardhanarisvara concept co-mingles god Shiva and goddess Shakti by presenting an icon that is half man and half woman, a representation and theme of union found in many Hindu texts and temples.

In the Smarta tradition of Hinduism, Shiva is a part of its Panchayatana puja. This practice consists of the use of icons or anicons of five deities considered equivalent, set in a quincunx pattern. Shiva is one of the five deities, others being Vishnu, Devi (such as Parvati), Surya and Ganesha or Skanda or any personal god of devotee's preference (Ishta Devata).

Philosophically, the Smarta tradition emphasizes that all idols (murti) are icons to help focus on and visualize aspects of Brahman, rather than distinct beings. The ultimate goal in this practice is to transition past the use of icons, recognize the Absolute symbolized by the icons, on the path to realizing the nondual identity of one's Atman (soul, self) and the Brahman. Popularized by Adi Shankara, many Panchayatana mandalas and temples have been uncovered that are from the Gupta Empire period, and one Panchayatana set from the village of Nand (about 24 kilometers from Ajmer) has been dated to belong to the Kushan Empire era (pre-300 CE). The Kushan period set includes Shiva, Vishnu, Surya, Brahma and one deity whose identity is unclear.

Shiva is considered the Great Yogi who is totally absorbed in himself – the transcendental reality. He is the Lord of Yogis, and the teacher of Yoga to sages. As Shiva Dakshinamurthi, states Stella Kramrisch, he is the supreme guru who "teaches in silence the oneness of one's innermost self ("atman") with the ultimate reality ("brahman")."

The theory and practice of Yoga, in different styles, has been a part of all major traditions of Hinduism, and Shiva has been the patron or spokesperson in numerous Hindu Yoga texts. These contain the philosophy and techniques for Yoga. These ideas are estimated to be from or after the late centuries of the 1st millennium CE, and have survived as Yoga texts such as the "Isvara Gita" (literally, 'Shiva's song'), which Andrew Nicholson – a professor of Hinduism and Indian Intellectual History – states have had "a profound and lasting influence on the development of Hinduism".

Other famed Shiva-related texts influenced Hatha Yoga, integrated monistic ("Advaita Vedanta") ideas with Yoga philosophy and inspired the theoretical development of Indian classical dance. These include the "Shiva Sutras", the "Shiva Samhita", and those by the scholars of Kashmir Shaivism such as the 10th-century scholar Abhinavagupta. Abhinavagupta writes in his notes on the relevance of ideas related to Shiva and Yoga, by stating that "people, occupied as they are with their own affairs, normally do nothing for others", and Shiva and Yoga spirituality helps one look beyond, understand interconnectedness, and thus benefit both the individual and the world towards a more blissful state of existence.

The Trimurti is a concept in Hinduism in which the cosmic functions of creation, maintenance, and destruction are personified by the forms of Brahma the creator, Vishnu the maintainer or preserver and Shiva the destroyer or transformer. These three deities have been called "the Hindu triad" or the "Great Trinity". However, the ancient and medieval texts of Hinduism feature many triads of gods and goddesses, some of which do not include Shiva.


According to Gavin Flood, "Shiva is a god of ambiguity and paradox," whose attributes include opposing themes. The ambivalent nature of this deity is apparent in some of his names and the stories told about him.

In Yajurveda, two contrary sets of attributes for both malignant or terrifying (Sanskrit: ') and benign or auspicious (Sanskrit: ') forms can be found, leading Chakravarti to conclude that "all the basic elements which created the complex Rudra-Śiva sect of later ages are to be found here". In the Mahabharata, Shiva is depicted as "the standard of invincibility, might, and terror", as well as a figure of honor, delight, and brilliance.

The duality of Shiva's fearful and auspicious attributes appears in contrasted names. The name Rudra reflects Shiva's fearsome aspects. According to traditional etymologies, the Sanskrit name "Rudra" is derived from the root "rud-", which means "to cry, howl". Stella Kramrisch notes a different etymology connected with the adjectival form "raudra", which means "wild, of "rudra" nature", and translates the name "Rudra" as "the wild one" or "the fierce god". R. K. Sharma follows this alternate etymology and translates the name as "terrible". Hara is an important name that occurs three times in the Anushasanaparvan version of the "Shiva sahasranama", where it is translated in different ways each time it occurs, following a commentorial tradition of not repeating an interpretation. Sharma translates the three as "one who captivates", "one who consolidates", and "one who destroys". Kramrisch translates it as "the ravisher". Another of Shiva's fearsome forms is as "time" and "great time", which ultimately destroys all things. The name appears in the "Shiva Sahasranama", where it is translated by Ram Karan Sharma as "(the Supreme Lord of) Time". Bhairava "terrible" or "frightful" is a fierce form associated with annihilation. In contrast, the name , "beneficent" or "conferring happiness" reflects his benign form. This name was adopted by the great Vedanta philosopher Adi Shankara (c. 788–820), who is also known as Shankaracharya. The name (Sanskrit: swam-on its own; bhu-burn/shine) "self-shining/ shining on its own", also reflects this benign aspect.

Shiva is depicted as both an ascetic yogi and as a householder (grihasta), roles which have been traditionally mutually exclusive in Hindu society. When depicted as a yogi, he may be shown sitting and meditating. His epithet Mahāyogi ("the great Yogi: "" = "great", "Yogi" = "one who practices Yoga") refers to his association with yoga. While Vedic religion was conceived mainly in terms of sacrifice, it was during the Epic period that the concepts of tapas, yoga, and asceticism became more important, and the depiction of Shiva as an ascetic sitting in philosophical isolation reflects these later concepts.

As a family man and householder, he has a wife, Parvati and two sons, Ganesha and Kartikeya. His epithet ("The husband of ") refers to this idea, and Sharma notes that two other variants of this name that mean the same thing, and , also appear in the "sahasranama". in epic literature is known by many names, including the benign . She is identified with Devi, the Divine Mother; Shakti (divine energy) as well as goddesses like Tripura Sundari, Durga, Kali, Kamakshi and Minakshi. The consorts of Shiva are the source of his creative energy. They represent the dynamic extension of Shiva onto this universe. His son Ganesha is worshipped throughout India and Nepal as the Remover of Obstacles, Lord of Beginnings and Lord of Obstacles. Kartikeya is worshipped in South India (especially in Tamil Nadu, Kerala and Karnataka) by the names Subrahmanya, Subrahmanyan, Shanmughan, Swaminathan and Murugan, and in Northern India by the names Skanda, Kumara, or Karttikeya.

Some regional deities are also identified as Shiva's children. As one story goes, Shiva is enticed by the beauty and charm of Mohini, Vishnu's female avatar, and procreates with her. As a result of this union, Shasta – identified with regional deities Ayyappan and Aiyanar – is born. In outskirts of Ernakulam in Kerala, a deity named Vishnumaya is stated to be offspring of Shiva and invoked in local exorcism rites, but this deity is not traceable in Hindu pantheon and is possibly a local tradition with "vaguely Chinese" style rituals, states Saletore. In some traditions, Shiva has daughters like the serpent-goddess Manasa and Ashokasundari. According to Doniger, two regional stories depict demons Andhaka and Jalandhara as the children of Shiva who war with him, and are later destroyed by Shiva.

The depiction of Shiva as Nataraja (Sanskrit: ', "Lord of Dance") is popular. The names Nartaka ("dancer") and Nityanarta ("eternal dancer") appear in the Shiva Sahasranama. His association with dance and also with music is prominent in the Puranic period. In addition to the specific iconographic form known as Nataraja, various other types of dancing forms (Sanskrit: ') are found in all parts of India, with many well-defined varieties in Tamil Nadu in particular. The two most common forms of the dance are the Tandava, which later came to denote the powerful and masculine dance as Kala-Mahakala associated with the destruction of the world. When it requires the world or universe to be destroyed, Shiva does it by the Tandava, and Lasya, which is graceful and delicate and expresses emotions on a gentle level and is considered the feminine dance attributed to the goddess Parvati. "Lasya" is regarded as the female counterpart of "Tandava". The "Tandava"-"Lasya" dances are associated with the destruction-creation of the world.

Dakshinamurthy "()" literally describes a form (') of Shiva facing south ('). This form represents Shiva in his aspect as a teacher of yoga, music, and wisdom and giving exposition on the "shastras". This iconographic form for depicting Shiva in Indian art is mostly from Tamil Nadu. Elements of this motif can include Shiva seated upon a deer-throne and surrounded by sages who are receiving his instruction.

An iconographic representation of Shiva called Ardhanarishvara ("") shows him with one half of the body as male and the other half as female. According to Ellen Goldberg, the traditional Sanskrit name for this form is best translated as "the lord who is half woman", not as "half-man, half-woman".

Shiva is often depicted as an archer in the act of destroying the triple fortresses, "Tripura", of the Asuras. Shiva's name Tripurantaka ( ""), "ender of Tripura", refers to this important story.

Apart from anthropomorphic images of Shiva, he is also represented in aniconic form of a lingam. These are depicted in various designs. One common form is the shape of a vertical rounded column in the centre of a lipped, disk-shaped object, the "yoni", symbolism for the goddess Shakti. In Shiva temples, the "linga" is typically present in its sanctum sanctorum and is the focus of votary offerings such as milk, water, flower petals, fruit, fresh leaves, and rice. According to Monier Williams and Yudit Greenberg, "linga" literally means 'mark, sign or emblem', and also refers to a "mark or sign from which the existence of something else can be reliably inferred". It implies the regenerative divine energy innate in nature, symbolized by Shiva. Some scholars, such as Wendy Doniger, view "linga" merely as an erotic phallic symbol, although this interpretation is criticized by others, including Swami Vivekananda, Sivananda Saraswati, and S. N. Balagangadhara. According to Moriz Winternitz, the "linga" in the Shiva tradition is "only a symbol of the productive and creative principle of nature as embodied in Shiva", and it has no historical trace in any obscene phallic cult.

The worship of the lingam originated from the famous hymn in the "Atharva-Veda Samhitâ" sung in praise of the "Yupa-Stambha", the sacrificial post. In that hymn, a description is found of the beginningless and endless "Stambha" or "Skambha", and it is shown that the said "Skambha" is put in place of the eternal Brahman. Just as the Yajna (sacrificial) fire, its smoke, ashes, and flames, the "Soma" plant, and the ox that used to carry on its back the wood for the Vedic sacrifice gave place to the conceptions of the brightness of Shiva's body, his tawny matted hair, his blue throat, and the riding on the bull of the Shiva, the "Yupa-Skambha" gave place in time to the "Shiva-Linga". In the text "Linga Purana", the same hymn is expanded in the shape of stories, meant to establish the glory of the great Stambha and the superiority of Shiva as Mahadeva.

The oldest known archaeological "linga" as an icon of Shiva is the Gudimallam lingam from 3rd-century BCE. In Shaivism pilgrimage tradition, twelve major temples of Shiva are called Jyotirlinga, which means "linga of light", and these are located across India.

Five is a sacred number for Shiva. One of his most important mantras has five syllables ().

Shiva's body is said to consist of five mantras, called the . As forms of God, each of these have their own names and distinct iconography:

These are represented as the five faces of Shiva and are associated in various texts with the five elements, the five senses, the five organs of perception, and the five organs of action. Doctrinal differences and, possibly, errors in transmission, have resulted in some differences between texts in details of how these five forms are linked with various attributes. The overall meaning of these associations is summarized by Stella Kramrisch:
According to the "Pañcabrahma Upanishad":
Puranic scriptures contain occasional references to "ansh" – literally 'portion, or avatars of Shiva', but the idea of Shiva avatars is not universally accepted in Saivism. The Linga Purana mentions twenty-eight forms of Shiva which are sometimes seen as avatars, however such mention is unusual and the avatars of Shiva is relatively rare in Shaivism compared to the well emphasized concept of Vishnu avatars in Vaishnavism.
Some Vaishnava literature reverentially link Shiva to characters in its mythologies. For example, in the "Hanuman Chalisa", Hanuman is identified as the eleventh avatar of Shiva. The "Bhagavata Purana" and the "Vishnu Purana" claim sage Durvasa to be a portion of Shiva. Some medieval era writers have called the Advaita Vedanta philosopher Adi Shankara an incarnation of Shiva.

There is a "Shivaratri" in every lunar month on its 13th night/14th day, but once a year in late winter (February/March) and before the arrival of spring, marks "Maha Shivaratri" which means "the Great Night of Shiva".

Maha Shivaratri is a major Hindu festival, but one that is solemn and theologically marks a remembrance of "overcoming darkness and ignorance" in life and the world, and meditation about the polarities of existence, of Shiva and a devotion to humankind. It is observed by reciting Shiva-related poems, chanting prayers, remembering Shiva, fasting, doing Yoga and meditating on ethics and virtues such as self-restraint, honesty, noninjury to others, forgiveness, introspection, self-repentance and the discovery of Shiva. The ardent devotees keep awake all night. Others visit one of the Shiva temples or go on pilgrimage to Jyotirlingam shrines. Those who visit temples, offer milk, fruits, flowers, fresh leaves and sweets to the lingam. Some communities organize special dance events, to mark Shiva as the lord of dance, with individual and group performances. According to Jones and Ryan, Maha Sivaratri is an ancient Hindu festival which probably originated around the 5th-century.

Another major festival involving Shiva worship is Kartik Purnima, commemorating Shiva's victory on the demons Tripurasura. Across India, various Shiva temples are illuminated throughout the night. Shiva icons are carried in procession in some places.

Regional festivals dedicated to Shiva include the Chittirai festival in Madurai around April/May, one of the largest festivals in South India, celebrating the wedding of Minakshi (Parvati) and Shiva. The festival is one where both the Vaishnava and Shaiva communities join the celebrations, because Vishnu gives away his sister Minakshi in marriage to Shiva.

Some Shaktism-related festivals revere Shiva along with the goddess considered primary and Supreme. These include festivals dedicated to Annapurna such as "Annakuta" and those related to Durga. In Himalayan regions such as Nepal, as well as in northern, central and western India, the festival of Teej is celebrated by girls and women in the monsoon season, in honor of goddess Parvati, with group singing, dancing and by offering prayers in Parvati-Shiva temples.

The ascetic, Vedic and Tantric sub-traditions related to Shiva, such as those that became ascetic warriors during the Islamic rule period of India, celebrate the Kumbha Mela festival. This festival cycles every 12 years, in four pilgrimage sites within India, with the event moving to the next site after a gap of three years. The biggest is in Prayaga (renamed Allahabad during the Mughal rule era), where millions of Hindus of different traditions gather at the confluence of rivers Ganges and Yamuna. In the Hindu tradition, the Shiva-linked ascetic warriors ("Nagas") get the honor of starting the event by entering the "Sangam" first for bathing and prayers.

In Shaivism of Indonesia, the popular name for Shiva has been "Batara Guru", which is derived from Sanskrit "Bhattaraka" which means “noble lord". He is conceptualized as a kind spiritual teacher, the first of all Gurus in Indonesian Hindu texts, mirroring the Dakshinamurti aspect of Shiva in the Indian subcontinent. However, the Batara Guru has more aspects than the Indian Shiva, as the Indonesian Hindus blended their spirits and heroes with him. Batara Guru's wife in southeast Asia is the same Hindu deity Durga, who has been popular since ancient times, and she too has a complex character with benevolent and fierce manifestations, each visualized with different names such as Uma, Sri, Kali and others. Shiva has been called Sadasiva, Paramasiva, Mahadeva in benevolent forms, and Kala, Bhairava, Mahakala in his fierce forms. The Indonesian Hindu texts present the same philosophical diversity of Shaivism traditions found on the subcontinent. However, among the texts that have survived into the contemporary era, the more common are of those of Shaiva Siddhanta (locally also called Siwa Siddhanta, Sridanta).

In the pre-Islamic period on the island of Java, Shaivism and Buddhism were considered very close and allied religions, though not identical religions. The medieval era Indonesian literature equates Buddha with Siwa (Shiva) and Janardana (Vishnu). This tradition continues in predominantly Hindu Bali Indonesia in the modern era, where Buddha is considered the younger brother of Shiva.

The worship of Shiva became popular in Central Asia through the Hephthalite Empire, and Kushan Empire. Shaivism was also popular in Sogdia and the Kingdom of Yutian as found from the wall painting from Penjikent on the river Zervashan. In this depiction, Shiva is portrayed with a sacred halo and a sacred thread ("Yajnopavita"). He is clad in tiger skin while his attendants are wearing Sogdian dress. A panel from Dandan Oilik shows Shiva in His Trimurti form with Shakti kneeling on her right thigh. Another site in the Taklamakan Desert depicts him with four legs, seated cross-legged on a cushioned seat supported by two bulls. It is also noted that Zoroastrian wind god Vayu-Vata took on the iconographic appearance of Shiva.

Daikokuten, one of the Seven Lucky Gods in Japan, is considered to be evolved from Shiva. The god enjoys an exalted position as a household deity in Japan and is worshipped as the god of wealth and fortune. The name is the Japanese equivalent of Mahākāla, the Buddhist name for Shiva. Shiva is also mentioned in Buddhist Tantra. Shiva as "Upaya" and Shakti as "Prajna". In cosmologies of Buddhist tantra, Shiva is depicted as passive, with Shakti being his active counterpart. In Mahayana Buddhist cosmology, Shiva resides in Akaniṣṭha, highest of Śuddhāvāsa (Pure Abodes) where Anāgāmi ("Non-returners") who are already on the path to Arhat-hood and who will attain enlightenment are born in.

The Japuji Sahib of the Guru Granth Sahib says, "The Guru is Shiva, the Guru is Vishnu and Brahma; the Guru is Paarvati and Lakhshmi." In the same chapter, it also says, "Shiva speaks, and the Siddhas listen." In Dasam Granth, Guru Gobind Singh has mentioned two avtars of Rudra: Dattatreya Avtar and Parasnath Avtar.

In contemporary culture, Shiva is depicted in films, books, tattoos and art. He has been referred to as "the god of cool things" and a "bonafide rock hero".

Popular films include the Gujarati language movie "Har Har Mahadev", the Kannada movie "Gange Gowri" and well-known books include Amish Tripathi's "Shiva Trilogy", which has sold over a million copies. On television, "Devon Ke Dev...Mahadev", a television serial about Lord Shiva on the Life OK channel was among the most watched shows at its peak popularity.

In the "Final Fantasy" videogame series, Shiva is often depicted as a benevolent ancient being of Ice Element who frequently aids the heroes against mighty foes (via summoning). Shiva is also a character in the video game "Dark Souls", with the name Shiva of the East.


 


</doc>
<doc id="28851" url="https://en.wikipedia.org/wiki?curid=28851" title="Sap beetle">
Sap beetle

The sap beetles are a family (Nitidulidae) of beetles.

They are small (2–6 mm) ovoid, usually dull-coloured beetles, with knobbed antennae. Some have red or yellow spots or bands. They feed mainly on decaying vegetable matter, over-ripe fruit, and sap. There are a few pest species. An example of a pest species is the strawberry sap beetle that infest crops in Brazil between the months of August and February. 

The family includes these genera:



</doc>
<doc id="28852" url="https://en.wikipedia.org/wiki?curid=28852" title="Syphilis">
Syphilis

Syphilis is a sexually transmitted infection caused by the bacterium "Treponema pallidum" subspecies "pallidum". The signs and symptoms of syphilis vary depending in which of the four stages it presents (primary, secondary, latent, and tertiary). The primary stage classically presents with a single chancre (a firm, painless, non-itchy skin ulceration usually between 1 cm and 2 cm in diameter) though there may be multiple sores. In secondary syphilis, a diffuse rash occurs, which frequently involves the palms of the hands and soles of the feet. There may also be sores in the mouth or vagina. In latent syphilis, which can last for years, there are few or no symptoms. In tertiary syphilis, there are gummas (soft, non-cancerous growths), neurological problems, or heart symptoms. Syphilis has been known as "the great imitator" as it may cause symptoms similar to many other diseases.
Syphilis is most commonly spread through sexual activity. It may also be transmitted from mother to baby during pregnancy or at birth, resulting in congenital syphilis. Other diseases caused by the "Treponema" bacteria include yaws (subspecies "pertenue"), pinta (subspecies "carateum"), and nonvenereal endemic syphilis (subspecies "endemicum"). These three diseases are not typically sexually transmitted. Diagnosis is usually made by using blood tests; the bacteria can also be detected using dark field microscopy. The Centers for Disease Control and Prevention (U.S.) recommend all pregnant women be tested.
The risk of sexual transmission of syphilis can be reduced by using a latex or polyurethane condom. Syphilis can be effectively treated with antibiotics. The preferred antibiotic for most cases is benzathine benzylpenicillin injected into a muscle. In those who have a severe penicillin allergy, doxycycline or tetracycline may be used. In those with neurosyphilis, intravenous benzylpenicillin or ceftriaxone is recommended. During treatment people may develop fever, headache, and muscle pains, a reaction known as Jarisch–Herxheimer.
In 2015, about 45.4 million people were infected with syphilis, with 6 million new cases. During 2015, it caused about 107,000 deaths, down from 202,000 in 1990. After decreasing dramatically with the availability of penicillin in the 1940s, rates of infection have increased since the turn of the millennium in many countries, often in combination with human immunodeficiency virus (HIV). This is believed to be partly due to increased promiscuity, prostitution, decreasing use of condoms, and unsafe sexual practices among men who have sex with men. In 2015, Cuba became the first country to eliminate mother-to-child transmission of syphilis.
Syphilis can present in one of four different stages: primary, secondary, latent, and tertiary, and may also occur congenitally. It was referred to as "the great imitator" by Sir William Osler due to its varied presentations.

Primary syphilis is typically acquired by direct sexual contact with the infectious lesions of another person. Approximately 3 to 90 days after the initial exposure (average 21 days) a skin lesion, called a chancre, appears at the point of contact. This is classically (40% of the time) a single, firm, painless, non-itchy skin ulceration with a clean base and sharp borders approximately 0.3–3.0 cm in size. The lesion may take on almost any form. In the classic form, it evolves from a macule to a papule and finally to an erosion or ulcer. Occasionally, multiple lesions may be present (~40%), with multiple lesions being more common when coinfected with HIV. Lesions may be painful or tender (30%), and they may occur in places other than the genitals (2–7%). The most common location in women is the cervix (44%), the penis in heterosexual men (99%), and anally and rectally in men who have sex with men (34%). Lymph node enlargement frequently (80%) occurs around the area of infection, occurring seven to 10 days after chancre formation. The lesion may persist for three to six weeks if left untreated.

Secondary syphilis occurs approximately four to ten weeks after the primary infection. While secondary disease is known for the many different ways it can manifest, symptoms most commonly involve the skin, mucous membranes, and lymph nodes. There may be a symmetrical, reddish-pink, non-itchy rash on the trunk and extremities, including the palms and soles. The rash may become maculopapular or pustular. It may form flat, broad, whitish, wart-like lesions on mucous membranes, known as condyloma latum. All of these lesions harbor bacteria and are infectious. Other symptoms may include fever, sore throat, malaise, weight loss, hair loss, and headache. Rare manifestations include liver inflammation, kidney disease, joint inflammation, periostitis, inflammation of the optic nerve, uveitis, and interstitial keratitis. The acute symptoms usually resolve after three to six weeks; about 25% of people may present with a recurrence of secondary symptoms. Many people who present with secondary syphilis (40–85% of women, 20–65% of men) do not report previously having had the classical chancre of primary syphilis.

Latent syphilis is defined as having serologic proof of infection without symptoms of disease. It is further described as either early (less than 1 year after secondary syphilis) or late (more than 1 year after secondary syphilis) in the United States. The United Kingdom uses a cut-off of two years for early and late latent syphilis. Early latent syphilis may have a relapse of symptoms in 25% of cases. Late latent syphilis is asymptomatic, and not as contagious as early latent syphilis.

Tertiary syphilis may occur approximately 3 to 15 years after the initial infection, and may be divided into three different forms: gummatous syphilis (15%), late neurosyphilis (6.5%), and cardiovascular syphilis (10%). Without treatment, a third of infected people develop tertiary disease. People with tertiary syphilis are not infectious.

Gummatous syphilis or late benign syphilis usually occurs 1 to 46 years after the initial infection, with an average of 15 years. This stage is characterized by the formation of chronic gummas, which are soft, tumor-like balls of inflammation which may vary considerably in size. They typically affect the skin, bone, and liver, but can occur anywhere.

Neurosyphilis refers to an infection involving the central nervous system. It may occur early, being either asymptomatic or in the form of syphilitic meningitis, or late as meningovascular syphilis, general paresis, or tabes dorsalis, which is associated with poor balance and lightning pains in the lower extremities. Late neurosyphilis typically occurs 4 to 25 years after the initial infection. Meningovascular syphilis typically presents with apathy and seizures, and general paresis with dementia and tabes dorsalis. Also, there may be Argyll Robertson pupils, which are bilateral small pupils that constrict when the person focuses on near objects (accommodation reflex) but do not constrict when exposed to bright light (pupillary reflex).

Cardiovascular syphilis usually occurs 10–30 years after the initial infection. The most common complication is syphilitic aortitis, which may result in aortic aneurysm formation.

Congenital syphilis is that which is transmitted during pregnancy or during birth. Two-thirds of syphilitic infants are born without symptoms. Common symptoms that develop over the first couple of years of life include enlargement of the liver and spleen (70%), rash (70%), fever (40%), neurosyphilis (20%), and lung inflammation (20%). If untreated, late congenital syphilis may occur in 40%, including saddle nose deformation, Higouménakis' sign, saber shin, or Clutton's joints among others. Infection during pregnancy is also associated with miscarriage. The three main dental defects in congenital syphilis are Hutchinson's incisors (screwdriver shaped incisors), Moon's molars or bud molars, and Fournier's molars or mulberry molars (molars with abnormal occlusal anatomy resembling a mulberry).

"Treponema pallidum" subspecies" pallidum" is a spiral-shaped, Gram-negative, highly mobile bacterium. Three other human diseases are caused by related "Treponema pallidum" subspecies, including yaws (subspecies "pertenue"), pinta (subspecies "carateum") and bejel (subspecies "endemicum"). Unlike subspecies "pallidum", they do not cause neurological disease. Humans are the only known natural reservoir for subspecies "pallidum". It is unable to survive more than a few days without a host. This is due to its small genome (1.14Mbp) failing to encode the metabolic pathways necessary to make most of its macronutrients. It has a slow doubling time of greater than 30 hours. The bacterium is known for its ability to evade the immune system and its invasiveness.

Syphilis is transmitted primarily by sexual contact or during pregnancy from a mother to her baby; the spirochete is able to pass through intact mucous membranes or compromised skin. It is thus transmissible by kissing near a lesion, as well as oral, vaginal, and anal sex. Approximately 30% to 60% of those exposed to primary or secondary syphilis will get the disease. Its infectivity is exemplified by the fact that an individual inoculated with only 57 organisms has a 50% chance of being infected. Most (60%) of new cases in the United States occur in men who have sex with men; and in this population 20% of syphilis were due to oral sex alone. Syphilis can be transmitted by blood products, but the risk is low due to screening of donated blood in many countries. The risk of transmission from sharing needles appears limited.

It is not generally possible to contract syphilis through toilet seats, daily activities, hot tubs, or sharing eating utensils or clothing. This is mainly because the bacteria die very quickly outside of the body, making transmission by objects extremely difficult.

Syphilis is difficult to diagnose clinically during early infection. Confirmation is either via blood tests or direct visual inspection using dark field microscopy. Blood tests are more commonly used, as they are easier to perform. Diagnostic tests are unable to distinguish between the stages of the disease.

Blood tests are divided into nontreponemal and treponemal tests.

Nontreponemal tests are used initially, and include venereal disease research laboratory (VDRL) and rapid plasma reagin (RPR) tests. False positives on the nontreponemal tests can occur with some viral infections, such as varicella (chickenpox) and measles. False positives can also occur with lymphoma, tuberculosis, malaria, endocarditis, connective tissue disease, and pregnancy.

Because of the possibility of false positives with nontreponemal tests, confirmation is required with a treponemal test, such as treponemal pallidum particle agglutination (TPHA) or fluorescent treponemal antibody absorption test (FTA-Abs). Treponemal antibody tests usually become positive two to five weeks after the initial infection. Neurosyphilis is diagnosed by finding high numbers of leukocytes (predominately lymphocytes) and high protein levels in the cerebrospinal fluid in the setting of a known syphilis infection.

Dark field microscopy of serous fluid from a chancre may be used to make an immediate diagnosis. Hospitals do not always have equipment or experienced staff members, and testing must be done within 10 minutes of acquiring the sample. Sensitivity has been reported to be nearly 80%; therefore the test can only be used to confirm a diagnosis, not to rule one out. Two other tests can be carried out on a sample from the chancre: direct fluorescent antibody (DFA) and polymerase chain reaction (PCR) tests. DFA uses antibodies tagged with fluorescein, which attach to specific syphilis proteins, while PCR uses techniques to detect the presence of specific syphilis genes. These tests are not as time-sensitive, as they do not require living bacteria to make the diagnosis.

, there is no vaccine effective for prevention. Several vaccines based on treponemal proteins reduce lesion development in an animal model but research continues.

Condom use reduces the likelihood of transmission during sex, but does not completely eliminate the risk. The Centers for Disease Control and Prevention (CDC) states, "Correct and consistent use of latex condoms can reduce the risk of syphilis only when the infected area or site of potential exposure is protected. However, a syphilis sore outside of the area covered by a latex condom can still allow transmission, so caution should be exercised even when using a condom."

Abstinence from intimate physical contact with an infected person is effective at reducing the transmission of syphilis. The CDC states, "The surest way to avoid transmission of sexually transmitted diseases, including syphilis, is to abstain from sexual contact or to be in a long-term mutually monogamous relationship with a partner who has been tested and is known to be uninfected."

Congenital syphilis in the newborn can be prevented by screening mothers during early pregnancy and treating those who are infected. The United States Preventive Services Task Force (USPSTF) strongly recommends universal screening of all pregnant women, while the World Health Organization (WHO) recommends all women be tested at their first antenatal visit and again in the third trimester. If they are positive, it is recommend their partners also be treated. Congenital syphilis is still common in the developing world, as many women do not receive antenatal care at all, and the antenatal care others receive does not include screening. It still occasionally occurs in the developed world, as those most likely to acquire syphilis are least likely to receive care during pregnancy. Several measures to increase access to testing appear effective at reducing rates of congenital syphilis in low- to middle-income countries. Point-of-care testing to detect syphilis appeared to be reliable although more research is needed to assess its effectiveness and into improving outcomes in mothers and babies.

The CDC recommends that sexually active men who have sex with men be tested at least yearly. The USPSTF also recommends screening among those at high risk.

Syphilis is a notifiable disease in many countries, including Canada the European Union, and the United States. This means health care providers are required to notify public health authorities, which will then ideally provide partner notification to the person's partners. Physicians may also encourage patients to send their partners to seek care. Several strategies have been found to improve follow-up for STI testing, including email and text messaging of reminders for appointments.

The first-line treatment for uncomplicated syphilis remains a single dose of intramuscular benzathine benzylpenicillin. Doxycycline and tetracycline are alternative choices for those allergic to penicillin; due to the risk of birth defects, these are not recommended for pregnant women. Resistance to macrolides, rifampicin, and clindamycin is often present. Ceftriaxone, a third-generation cephalosporin antibiotic, may be as effective as penicillin-based treatment. It is recommended that a treated person avoid sex until the sores are healed.

For neurosyphilis, due to the poor penetration of benzathine penicillin into the central nervous system, those affected are given large doses of intravenous penicillin for a minimum of 10 days. If a person is allergic to penicillin, ceftriaxone may be used or penicillin desensitization attempted. Other late presentations may be treated with once-weekly intramuscular benzathine penicillin for three weeks. Treatment at this stage solely limits further progression of the disease and has a limited effect on damage which has already occurred.

One of the potential side effects of treatment is the Jarisch–Herxheimer reaction. It frequently starts within one hour and lasts for 24 hours, with symptoms of fever, muscle pains, headache, and a fast heart rate. It is caused by cytokines released by the immune system in response to lipoproteins released from rupturing syphilis bacteria.

Penicillin is an effective treatment for syphilis in pregnancy but there is no agreement on which dose or route of delivery is most effective.

In 2012, about 0.5% of adults were infected with syphilis, with 6 million new cases. In 1999, it is believed to have infected 12 million additional people, with greater than 90% of cases in the developing world. It affects between 700,000 and 1.6 million pregnancies a year, resulting in spontaneous abortions, stillbirths, and congenital syphilis. During 2015, it caused about 107,000 deaths, down from 202,000 in 1990. In sub-Saharan Africa, syphilis contributes to approximately 20% of perinatal deaths. Rates are proportionally higher among intravenous drug users, those who are infected with HIV, and men who have sex with men. In the United States about 55,400 people are newly infected each year. In the United States, rates of syphilis as of 2007 were six times greater in men than in women; they were nearly equal ten years earlier. African Americans accounted for almost half of all cases in 2010. As of 2014, syphilis infections continue to increase in the United States.

Syphilis was very common in Europe during the 18th and 19th centuries. Flaubert found it universal among nineteenth-century Egyptian prostitutes. In the developed world during the early 20th century, infections declined rapidly with the widespread use of antibiotics, until the 1980s and 1990s. Since 2000, rates of syphilis have been increasing in the US, Canada, the UK, Australia and Europe, primarily among men who have sex with men. Rates of syphilis among US women have remained stable during this time, while rates among UK women have increased, but at a rate less than that of men. Increased rates among heterosexuals have occurred in China and Russia since the 1990s. This has been attributed to unsafe sexual practices, such as sexual promiscuity, prostitution, and decreasing use of barrier protection.

Left untreated, it has a mortality rate of 8% to 58%, with a greater death rate among males. The symptoms of syphilis have become less severe over the 19th and 20th centuries, in part due to widespread availability of effective treatment, and partly due to virulence of the bacteria. With early treatment, few complications result. Syphilis increases the risk of HIV transmission by two to five times, and coinfection is common (30–60% in some urban centers). In 2015, Cuba became the first country in the world to eradicate mother to child transmission of syphilis.

The origin of syphilis is disputed. Syphilis was present in the Americas before European contact, and it may have been carried from the Americas to Europe by the returning crewmen from Christopher Columbus's voyage to the Americas, or it may have existed in Europe previously but gone unrecognized until shortly after Columbus's return. These are the "Columbian" and "pre-Columbian" hypotheses, respectively, with the "Columbian" hypothesis better supported by the evidence.

The first written records of an outbreak of syphilis in Europe occurred in 1494 or 1495 in Naples, Italy, during a French invasion (Italian War of 1494–98). Since it was claimed to have been spread by French troops, it was initially called the "French disease" by the people of Naples. In 1530, the pastoral name "syphilis" (the name of a character) was first used by the Italian physician and poet Girolamo Fracastoro as the title of his Latin poem in dactylic hexameter describing the ravages of the disease in Italy. It was also called the "Great Pox".

In the 16th through 19th centuries, syphilis was one of the largest public health burdens in prevalence, symptoms, and disability, although records of its true prevalence were generally not kept because of the fearsome and sordid status of sexually transmitted diseases in those centuries. At the time the causative agent was unknown but it was well known that it was spread sexually and also often from mother to child. Its association with sex, especially sexual promiscuity and prostitution, made it an object of fear and revulsion and a taboo. The magnitude of its morbidity and mortality in those centuries reflected that, unlike today, there was no adequate understanding of its pathogenesis and no truly effective treatments. Its damage was caused not so much by great sickness or death early in the course of the disease but rather by its gruesome effects decades after infection as it progressed to neurosyphilis with tabes dorsalis. Mercury compounds and isolation were commonly used, with treatments often worse than the disease. 

The causative organism, "Treponema pallidum", was first identified by Fritz Schaudinn and Erich Hoffmann, in 1905. The first effective treatment for syphilis was Arsphenamine, discovered by Sahachiro Hata in 1909, during a survey of hundreds of newly synthesized organic arsenical compounds led by Paul Ehrlich. It was manufactured and marketed from 1910 under the trade name Salvarsan by Hoechst AG. This organoarsenic compound was the first modern chemotherapeutic agent.

During the 20th century, as both microbiology and pharmacology advanced greatly, syphilis, like many other infectious diseases, became more of a manageable burden than a scary and disfiguring mystery, at least in developed countries among those people who could afford to pay for timely diagnosis and treatment. Penicillin was discovered in 1928, and effectiveness of treatment with penicillin was confirmed in trials in 1943, at which time it became the main treatment.

Many famous historical figures, including Franz Schubert, Arthur Schopenhauer, Édouard Manet, Charles Baudelaire, and Guy de Maupassant are believed to have had the disease. Friedrich Nietzsche was long believed to have gone mad as a result of tertiary syphilis, but that diagnosis has recently come into question. It has been proposed that this was a posthumous smear campaign by anti-Nazis. 
The earliest known depiction of an individual with syphilis is Albrecht Dürer's "Syphilitic Man", a woodcut believed to represent a Landsknecht, a Northern European mercenary. The myth of the "femme fatale" or "poison women" of the 19th century is believed to be partly derived from the devastation of syphilis, with classic examples in literature including John Keats' "La Belle Dame sans Merci".

The Flemish artist Stradanus designed a print called "Preparation and Use of Guayaco for Treating Syphilis", a scene of a wealthy man receiving treatment for syphilis with the tropical wood guaiacum sometime around 1590.

The "Tuskegee Study of Untreated Syphilis in the Negro Male" was an infamous, unethical, and racist clinical study conducted between 1932 and 1972 by the U.S. Public Health Service. The purpose of this study was to observe the natural history of untreated syphilis; the African-American men in the study were told they were receiving free health care from the United States government.

The Public Health Service started working on this study in 1932 in collaboration with Tuskegee University, a historically black college in Alabama. Investigators enrolled in the study a total of 600 impoverished, African-American sharecroppers from Macon County, Alabama. Of these men, 399 had previously contracted syphilis before the study began, and 201 did not have the disease. The men were given free medical care, meals, and free burial insurance for participating in the study. The men were told that the study was only going to last six months, but it actually lasted 40 years. After funding for treatment was lost, the study was continued without informing the men that they would never be treated. None of the men infected were ever told that they had the disease, and none were treated with penicillin even after the antibiotic was proven to successfully treat syphilis. According to the Centers for Disease Control, the men were told that they were being treated for "bad blood", a colloquialism that described various conditions such as syphilis, anemia, and fatigue. "Bad blood"—specifically the collection of illnesses the term included—was a leading cause of death within the southern African-American community.

The 40-year study was controversial for reasons related to ethical standards. Researchers knowingly failed to treat patients appropriately after the 1940s validation of penicillin was found as an effective cure for the disease that they were studying. The revelation in 1972 of study failures by a whistleblower, Peter Buxtun, led to major changes in U.S. law and regulation on the protection of participants in clinical studies. Now studies require informed consent, communication of diagnosis, and accurate reporting of test results.
Similar experiments were carried out in Guatemala from 1946 to 1948. It was done during the administration of American President Harry S. Truman and Guatemalan President Juan José Arévalo with the cooperation of some Guatemalan health ministries and officials. Doctors infected soldiers, prostitutes, prisoners and mental patients with syphilis and other sexually transmitted diseases, without the informed consent of the subjects, and treated most subjects with antibiotics. The experiment resulted in at least 83 deaths. In October 2010, the U.S. formally apologized to Guatemala for the ethical violations that took place. Clinton and Sebelius stated "Although these events occurred more than 64 years ago, we are outraged that such reprehensible research could have occurred under the guise of public health. We deeply regret that it happened, and we apologize to all the individuals who were affected by such abhorrent research practices." The experiments were led by physician John Charles Cutler who also participated in the late stages of the Tuskegee syphilis experiment.

First called "grande verole" or the "great pox" by the French. Other historical names have included "button scurvy", sibbens, frenga and dichuchwa, among others.



</doc>
<doc id="28853" url="https://en.wikipedia.org/wiki?curid=28853" title="Smiling Buddha">
Smiling Buddha

Smiling Buddha (MEA designation: Pokhran-I) was the assigned code name of India's first successful nuclear bomb test on 18 May 1974. The bomb was detonated on the Pokhran Test Range (PTR), in Rajasthan, by the Indian Army under the supervision of several key .

"Pokhran-I" was also the first confirmed nuclear weapons test by a nation outside the five permanent members of the United Nations Security Council. Officially, the Indian Ministry of External Affairs (MEA) characterised this test as a "peaceful nuclear explosion". A series of nuclear tests was carried out in 1998 under the name Pokhran-II.

India started its own nuclear programme in 1944 when Homi Jehangir Bhabha founded the Tata Institute of Fundamental Research. Physicist Raja Ramanna played an essential role in nuclear weapons technology research; he expanded and supervised scientific research on nuclear weapons and was the first directing officer of the small team of scientists that supervised and carried out the test.<ref name = "http://nuclearweaponarchive.org"></ref>

After Indian independence from the British Empire, Indian Prime Minister Jawaharlal Nehru authorised the development of a nuclear programme headed by Homi Bhabha. The "Atomic Energy Act" of 1948 focused on peaceful development. India was heavily involved in the development of the Nuclear Non-Proliferation Treaty, but ultimately opted not to sign it.
In 1954, Homi Jehangir Bhabha steered the nuclear programme in the direction of weapons design and production. Two important infrastructure projects were commissioned. The first established Trombay Atomic Energy Establishment at Mumbai (Bombay). The other created a governmental secretariat, Department of Atomic Energy (DAE), of which Bhabha was the first secretary. From 1954 to 1959, the nuclear programme grew swiftly. By 1958, the DAE had one-third of the defence budget for research purposes. In 1954, India reached a verbal understanding with Canada and the United States under the Atoms for Peace programme; Canada and the United States ultimately agreed to provide and establish the CIRUS research reactor also at Trombay. The acquisition of CIRUS was a watershed event in nuclear proliferation with the understanding between India and the United States that the reactor would be used for peaceful purposes only. CIRUS was an ideal facility to develop a plutonium device, and therefore Nehru refused to accept nuclear fuel from Canada and started the programme to develop an indigenous nuclear fuel cycle.

In July 1958, Nehru authorised "Project Phoenix" to build a reprocessing plant with a capacity of 20 tons of fuel a year – a size to match the production capacity of CIRUS. The plant used the PUREX process and was designed by the Vitro Corporation of America. Construction of the plutonium plant began at Trombay on 27 March 1961, and it was commissioned in mid-1964.

The nuclear programme continued to mature, and by 1960, Nehru made the critical decision to move the programme into production. At about the same time, Nehru held discussions with the American firm Westinghouse Electric to construct India's first nuclear power plant in Tarapur, Maharashtra. Kenneth Nichols, a US Army engineer, recalls from a meeting with Nehru, "it was that time when Nehru turned to Bhabha and asked Bhabha for the timeline of the development of a nuclear weapon". Bhabha estimated he would need about a year to accomplish the task.

By 1962, the nuclear programme was still developing, but at a slow rate. Nehru was distracted by the Sino-Indian War, during which India lost territory to China. Nehru turned to the Soviet Union for help, but the Soviet Union was preoccupied with the Cuban Missile Crisis. The Soviet Politburo turned down Nehru's request for arms and continued backing the Chinese. India concluded that the Soviet Union was an unreliable ally, and this conclusion strengthened India's determination to create a nuclear deterrent. Design work began in 1965 under Bhabha and proceeded under Raja Ramanna who took over the programme after the former's death.

Bhabha was now aggressively lobbying for nuclear weapons and made several speeches on Indian radio. In 1964, Bhabha told the Indian public via Indian radio that "such nuclear weapons are remarkably cheap" and supported his arguments by referring to the economical cost of American nuclear testing programme ("Plowshare"). Bhabha stated to the politicians that a 10 kt device would cost around $350,000, and $600,000 for a 2 mt. From this, he estimated that "a stockpile" of around 50 atomic bombs would cost under $21 million and a stockpile of 50 two-megaton hydrogen bombs would cost around $31.5 million." Bhabha did not realise, however, that the U.S. "Plowshare" cost-figures were produced by a vast industrial complex costing tens of billions of dollars, which had already manufactured nuclear weapons numbering in the tens of thousands. The delivery systems for nuclear weapons typically cost several times as much as the weapons themselves.

The nuclear programme was partially slowed down when Lal Bahadur Shastri became the prime minister. In 1965, Shastri faced another war with Pakistan. Shastri appointed physicist Vikram Sarabhai as the head of the nuclear programme but because of his Gandhian beliefs Sarabhai directed the programme toward peaceful purposes rather than military development.

In 1967, Indira Gandhi became the prime minister and work on the nuclear programme resumed with renewed vigour. Homi Sethna, a chemical engineer, played a significant role in the development of weapon-grade plutonium while Ramanna designed and manufactured the whole nuclear device. The first nuclear bomb project did not employ more than 75 scientists because of its sensitivity. The nuclear weapons programme was now directed towards the production of plutonium rather than uranium.

In 1968–69, P. K. Iyengar visited the Soviet Union with three colleagues and toured the nuclear research facilities at Dubna, Russia. During his visit, Iyengar was impressed by the plutonium-fueled pulsed fast reactor. Upon his return to India, Iyengar set about developing plutonium reactors approved by the Indian political leadership in January 1969. The secret plutonium plant was known as "Purnima", and construction began in March 1969. The plant's leadership included Iyengar, Ramanna, Homi Sethna, and Sarabhai. Sarabhai's presence indicates that, with or without formal approval, the work on nuclear weapons at Trombay had been commenced.

India continued to harbour ambivalent feelings about nuclear weapons, and accorded low priority to their production until the Indo-Pakistani War of 1971. In December 1971, Richard Nixon sent a carrier battle group led by the into the Bay of Bengal in an attempt to intimidate India. The Soviet Union responded by sending a submarine armed with nuclear missiles from Vladivostok to trail the US task force. The Soviet response demonstrated the deterrent value and significance of nuclear weapons and ballistic missile submarines to Indira Gandhi. India gained the military and political initiative over Pakistan after acceding to the treaty that divided Pakistan into two different political entities.

On 7 September 1972, near the peak of her post-war popularity, Indira Gandhi authorised the Bhabha Atomic Research Centre (BARC) to manufacture a nuclear device and prepare it for a test. Although the Indian Army was not fully involved in the nuclear testing, the army's highest command was kept fully informed of the test preparations. The preparations were carried out under the watchful eyes of the Indian political leadership, with civilian scientists assisting the Indian Army.

The device was formally called the "Peaceful Nuclear Explosive", but it was usually referred to as the "Smiling Buddha". The device was detonated on 18 May 1974, Buddha Jayanti (a festival day in India marking the birth of Gautama Buddha). Indira Gandhi maintained tight control of all aspects of the preparations of the "Smiling Buddha" test, which was conducted in extreme secrecy; besides Gandhi, only advisers Parmeshwar Haksar and Durga Dhar were kept informed. Scholar Raj Chengappa asserts the Indian Defence Minister Jagjivan Ram was not provided with any knowledge of this test and came to learn of it only after it was conducted. Swaran Singh, the Minister of External Affairs, was given 48 hours advance notice. The Indira Gandhi administration employed no more than 75 civilian scientists while General G. G. Bewoor, Indian army chief, and the commander of Indian Western Command were the only military commanders kept informed.

The head of this entire nuclear bomb project was the director of the BARC, Raja Ramanna. In later years, his role in the nuclear programme would be more deeply integrated as he remained head of the nuclear programme most of his life. The designer and creator of the bomb was P. K. Iyengar, who was the second in command of this project. Iyengar's work was further assisted by the chief metallurgist, R. Chidambaram, and by Nagapattinam Sambasiva Venkatesan of the Terminal Ballistics Research Laboratory, who developed and manufactured the high explosive implosion system. The explosive materials and the detonation system were developed by Waman Dattatreya Patwardhan of the High Energy Materials Research Laboratory.

The overall project was supervised by chemical engineer Homi Sethna, Chairman of the Atomic Energy Commission of India. Chidambaram, who would later coordinate work on the Pokhran-II tests, began work on the equation of state of plutonium in late 1967 or early 1968. To preserve secrecy, the project employed no more than 75 scientists and engineers from 1967–74. Abdul Kalam also arrived at the test site as the representative of the DRDO.

The device was of the implosion-type design and had a close resemblance to the American nuclear bomb called the "Fat Man". The implosion system was assembled at the Terminal Ballistics Research Laboratory (TBRL) of the DRDO in Chandigarh. The detonation system was developed at the High Energy Materials Research Laboratory (HEMRL) of the DRDO in Pune, Maharashtra State. The 6 kg of plutonium came from the CIRUS reactor at BARC. The neutron initiator was of the polonium–beryllium type and code-named "Flower". The complete nuclear bomb was engineered and finally assembled by Indian engineers at Trombay before transportation to the test site.

The fully assembled device had a hexagonal cross section, 1.25 metres in diameter, and weighed 1400 kg. The device was mounted on a hexagonal metal tripod, and was transported to the shaft on rails which the army kept covered with sand. The device was detonated when Dr Pranab R. Dastidar pushed the firing button at 8.05 a.m.; it was in a shaft 107 m under the army Pokhran test range in the Thar Desert (or Great Indian Desert), Rajasthan.

The nuclear yield of this test still remains controversial, with unclear data provided by Indian sources, although Indian politicians have given the country's press a range from 2 kt to 20 kt. The official yield was initially set at 12 kt; post-Operation Shakti claims have raised it to 13 kt. Independent seismic data from outside and analysis of the crater features indicate a lower figure. Analysts usually estimate the yield at 4 to 6 kt, using conventional seismic magnitude-to-yield conversion formulas. In recent years, both Homi Sethna and P. K. Iyengar have conceded the official yield to be an exaggeration.

Iyengar has variously stated that the yield was 8–10 kt, that the device was designed to yield 10 kt, and that the yield was 8 kt "exactly as predicted". Although seismic scaling laws lead to an estimated yield range between 3.2 kt and 21 kt, an analysis of hard rock cratering effects suggests a narrow range of around 8 kt for the yield, which is within the uncertainties of the seismic yield estimate.

Indian Prime Minister Indira Gandhi had already gained much popularity and publicity after her successful military campaign against Pakistan in the 1971 war. The test caused an immediate revival of Indira Gandhi's popularity, which had flagged considerably from its height after the 1971 war. The overall popularity and image of the Congress Party was enhanced and the Congress Party was well received in the Indian Parliament. In 1975, Homi Sethna, a chemical engineer and the chairman of the Indian Atomic Energy Commission (AECI), Raja Ramanna of BARC, and Basanti Nagchaudhuri of DRDO, all were honoured with the "Padma Vibhushan", India's second highest civilian award. Five other project members received the "Padma Shri", India's fourth highest civilian award. India consistently maintained that this was a peaceful nuclear bomb test and that it had no intentions of militarising its nuclear programme. However, according to independent monitors, this test was part of an accelerated Indian nuclear programme.
In 1997 Raja Ramanna, speaking to the "Press Trust of India", maintained:

While India continued to state that the test was for peaceful purposes, it encountered opposition from many quarters. The Nuclear Suppliers Group (NSG) was formed in reaction to the Indian tests to check international nuclear proliferation. The NSG decided in 1992 to require full-scope IAEA safeguards for any new nuclear export deals, which effectively ruled out nuclear exports to India, but in 2008 it waived this restriction on nuclear trade with India as part of the Indo-US civilian nuclear agreement.

Pakistan did not view the test as a "peaceful nuclear explosion", and cancelled talks scheduled for 10 June on normalisation of relations. Pakistan's Prime Minister Zulfikar Ali Bhutto vowed in June 1974 that he would never succumb to "nuclear blackmail" or accept "Indian hegemony or domination over the subcontinent". The chairman of the Pakistan Atomic Energy Commission, Munir Ahmed Khan, said that the test would force Pakistan to test its own nuclear bomb. Pakistan's leading nuclear physicist, Pervez Hoodbhoy, stated in 2011 that he believed the test "pushed [Pakistan] further into the nuclear arena".

The plutonium used in the test was created in the CIRUS reactor supplied by Canada and using heavy water supplied by the United States. Both countries reacted negatively, especially in light of then ongoing negotiations on the Nuclear Non-Proliferation Treaty and the economic aid both countries had provided to India. Canada concluded that the test violated a 1971 understanding between the two states, and froze nuclear energy assistance for the two heavy water reactors then under construction. The United States concluded that the test did not violate any agreement and proceeded with a June 1974 shipment of enriched uranium for the Tarapur reactor.

France sent a congratulatory telegram to India but later withdrew it.

Despite many proposals, India did not carry out further nuclear tests until 1998. After the 1998 general elections, Operation Shakti (also known as Pokhran-II) was carried out at the Pokhran test site, using devices designed and built over the preceding two decades.



</doc>
<doc id="28855" url="https://en.wikipedia.org/wiki?curid=28855" title="Shea Stadium">
Shea Stadium

Shea Stadium (; formally known as William A. Shea Municipal Stadium) was a stadium in Flushing Meadows–Corona Park, Queens, New York City. Built as a multi-purpose stadium, it was the home park of Major League Baseball's New York Mets for 45 seasons as well as the New York Jets football team from 1964 to 1983.

The venue was named in honor of William A. Shea, the man who was most responsible for bringing National League baseball back to New York after the Dodgers and Giants left for California in 1957. It was demolished in 2009 to create additional parking for the adjacent Citi Field, the current home of the Mets.

The origins of Shea Stadium go back to the Brooklyn Dodgers' and the New York Giants' relocations to the U.S. west coast in 1958, which left New York without a National League baseball team for the next four years.

Prior to the Dodgers' departure, New York City official Robert Moses tried to interest owner Walter O'Malley in the site as the location for a new stadium, but O'Malley refused, unable to agree on location, ownership, and lease terms. O'Malley preferred to pay construction costs himself so he could own the stadium outright. He wanted total control over revenue from parking, concessions, and other events.

New York City, in contrast, wanted to build the stadium, rent it, and retain the ancillary revenue rights to pay off its construction bonds. Additionally, O'Malley wanted to build his new stadium in Brooklyn, while Moses insisted on Flushing Meadows. When Los Angeles offered O'Malley what the City of New York wouldn't—complete ownership of the facility—he left for southern California in a preemptive bid to install the Dodgers there before a new or existing major league franchise could beat him to it. At the same time, Horace Stoneham moved his New York Giants to San Francisco (although he originally considered moving them to Minneapolis), ensuring that there would be two National League teams in California, and preserving the longstanding rivalry with the Dodgers that continues to this day.

In , the National League agreed to grant an expansion franchise to the owners of the New York franchise in the abortive Continental League, provided that a new stadium be built. Mayor Robert Wagner, Jr. had to personally wire all National League owners and assure them that the city would build a stadium.

On October 6, 1961, the Mets signed a 30-year stadium lease, with an option for a 10-year renewal. Rent for what was originally budgeted as a $9 million facility was set at $450,000 annually, with a reduction of $20,000 each year until it reached $300,000 annually.

In their inaugural season in 1962, the expansion Mets played in the Polo Grounds, with original plans to move to a new stadium in 1963. In October 1962, Mets official Tom Meany said, "Only a series of blizzards or some other unforeseen trouble might hamper construction."{ That unforeseen trouble surfaced in a number of ways: the severe winter of 1962–1963, along with the bankruptcies of two subcontractors and labor issues. The end result was that both the Mets and Jets played at the Polo Grounds for one more year.
It was originally to be called "Flushing Meadow Park Municipal Stadium" – the name of the public park within which it was built – but a movement was launched to name it in honor of William Shea, the New York attorney who brought National League baseball back to New York.

After 29 months of construction and $28.5 million spent, Shea Stadium opened on April 17, 1964, with the Pittsburgh Pirates beating the Mets before a crowd of There were no prior exhibition games or events, and the stadium was barely finished in time for the home opener. Because of a jurisdictional dispute between Local 3 of the International Brotherhood of Electrical Workers and Local 1106 of the Communications Workers of America, the telephone and telegraph wiring was not finished in time for opening day. The stadium opened five days before the 1964-65 New York World's Fair, across Roosevelt Avenue. Although not officially part of the fair grounds, the stadium sported steel panels on its exterior in the blue-and-orange colors of the Fair, the same team colors of the Mets. The panels were removed in 1980.

In accordance with New York City law, in 2009 Shea Stadium was dismantled, rather than imploded. The company with the rights to sell memorabilia was given two weeks after the final game to remove seats, signage and other potentially saleable and collectable items before demolition was to begin. The seats were the first ($869 per pair plus tax, a combination of '86 and '69, the team's two World Series championship years), followed by other memorabilia such as the foul poles, dugouts, stadium signage, and the giant letters that spelled out "SHEA" at the front of the building.

After salvaging operations concluded, demolition of the ballpark began on October 14, 2008. On October 18, the scoreboard in right field was demolished, with the bleachers, batter's eye and bullpens shortly thereafter.

By November 10, the field, dugouts and the rest of the field level seats had been demolished.

On January 31, Mets fans all over New York came to Shea Stadium for one final farewell. Fans took a tour of the site, told stories, and sang songs. The last remaining section of seats was demolished on February 18. Fans stood in awe as the remaining structure of Shea Stadium (one section of ramps) was torn down at 11:22 am.

The locations of Shea's home plate, pitcher's mound, and bases are marked in Citi Field's parking lot. The plaques feature engravings of the neon baseball players that once graced the exterior of the stadium.

On October 9, 2013, the New York City Council approved a plan to build a mall and entertainment center called Willets West in the Citi Field parking lot where Shea Stadium stood, as part of an effort by the city to redevelop the nearby neighborhood of Willets Point. However, in 2015, the Appellate Division of the New York State Supreme Court ruled that the site, considered parkland, could not be used for commercial development without permission from the New York state government.

Shea Stadium was the home of the New York Mets starting in 1964, and it hosted its only All-Star Game that first year, with Johnny Callison of the Philadelphia Phillies hitting a home run in the ninth inning to win the only Mid-Summer Classic held in the Queens ballpark. A month earlier, on Father's Day, Callison's teammate, future Hall of Fame member and U.S. Senator Jim Bunning, pitched a perfect game against the Mets.

The stadium was often criticized by baseball purists for many reasons, even though it was retrofitted to be a baseball-only stadium after the Jets left. The upper deck was one of the highest in the majors. The lower boxes were farther from the field than similar seats in other parks because they were still on the rails that swiveled the boxes into position for football. Outfield seating was sparse, in part because the stadium was designed to be fully enclosed.

At one time, Shea's foul territory was one of the most expansive in the majors. This was very common for ballparks built during the 1960s, in part due to the need to accommodate the larger football field. However, seats added over the years in the lower level greatly reduced the size of foul territory by the dawn of the 21st century. On the plus side, Shea always used a natural grass surface, in contrast to other multi-purpose stadiums such as Three Rivers Stadium, Veterans Stadium, and Riverfront Stadium, which were built in the same era and style and had artificial turf.

Shea Stadium hosted postseason baseball in 1969, 1973, 1986, 1988, 1999, 2000, and 2006; it hosted the World Series in , , , and . It had the distinction of being the home of the 1969 "Miracle Mets"— led by former Brooklyn Dodger Gil Hodges that defied 100–1 odds and won the World Series, after seven straight seasons in last or next-to-last place. Shea became famous for the bedlam that took place after the Mets won the decisive Game 5 of the World Series, as fans stormed the field in celebration. Similar scenes took place a few weeks earlier after the Mets clinched the National League East title, and then defeated the Atlanta Braves in the first National League Championship Series to win the pennant.

Tommie Agee, Lenny Dykstra, Todd Pratt, Robin Ventura, and Benny Agbayani hit post-season, walk-off home runs at Shea (although, while the ball hit by Ventura over the fence may have been the most famous of the postseason walk off hits, it was famously called "the grand slam single", because when he hit the game winning ball over the fence, he was mobbed by his teammates before he could reach second base, and never wound up touching second-base, third-base or Homeplate. It was not ruled a homerun as he never circled the bases. It probably made Ventura, known for his penchant for hitting Grand slam home runs, even more famous, and the hit itself more famous, because of the very fact that he never circled the bases, technically not making it a homer).

Agee was the only player in the history of the ballpark to hit a fair ball into the upper deck in left field. The spot was marked with a sign featuring Agee's number 20 and the date, which was April 10, 1969. Teammate Cleon Jones said the ball was still rising when it hit the seats, so it very likely could have been the longest home run ever hit at Shea. It came in the second inning, and Agee hit another in the seventh over the center field wall; both solo shots were off of Montreal Expos starter Larry Jaster, and the Mets 

In 1971, Dave Kingman – then with the San Francisco Giants and later to play for the Mets on two occasions – hit a home run that smashed off the windshield of the Giants' team bus, parked behind the left field bullpen.

For many years, the Mets' theme song, "Meet the Mets", was played at Shea before every home game. Jane Jarvis, a local jazz artist, played the popular songs on the Thomas organ at Mets games for many years at the stadium.

On October 3, 2004, it was the venue for the last game in the history of the Montreal Expos, and the Mets won Montreal's major league story ended where it had started 35 years earlier: at Shea Stadium. The following year, the Expos relocated to Washington, D.C. and became the Nationals.

The last game played at Shea Stadium was a loss to the Florida Marlins on September 28, 2008. However, the Mets were in the thick of the playoff chase until the last day. A win would have meant another game for Shea as the Mets were scheduled to play the Milwaukee Brewers in a one-game playoff for the National League Wild Card berth. Following the game, there was a "Shea Goodbye" tribute in which many players from the Mets' glory years entered the stadium and touched home plate one final time so that fans could pay their last respects to the players and the stadium the Mets called home for 45 years. The ceremony ended with Tom Seaver throwing a final pitch to Mike Piazza, then, as the Beatles' "In My Life" played on the stadium speakers the two former Met stars walked out of the centerfield gate and closed it behind them, followed by a display of blue and orange fireworks.

Three National League Division Series were played at Shea Stadium. The Mets won all three, and never lost a Division Series game at Shea.

Seven National League Championship Series were played at Shea Stadium.

The decisive seventh game of this series was played at Shea Stadium, marking the only time that the Mets ever lost the deciding game of a National League Championship Series at Shea.

Four World Series were played in Shea Stadium.

The Yankees' World Series win in 2000 was the only time that a visiting team won a World Series at Shea Stadium. The Mets won both their World Series titles at Shea Stadium (in Game 5 in 1969, and Game 7 in 1986).
The New York Yankees played their home games in Shea Stadium during the 1974 and 1975 seasons while Yankee Stadium was being renovated. The move to Shea had been proposed earlier in the decade, but the Mets, as Shea's primary tenants, refused to sign off on the deal. However, when the city stepped in to pay for renovating Yankee Stadium, the Mets had little choice but to agree to share Shea with the Yankees.

On the afternoon of April 15, 1998, the Yankees also played one home game at Shea, against the Anaheim Angels after a beam collapsed at Yankee Stadium two days before, destroying several rows of seats. With the Mets playing a game at Shea that evening against the Chicago Cubs, the Yankees used the visitor's locker room and dugout and the Angels used the home dugout and old locker room of the New York Jets. Former Mets star Darryl Strawberry, then playing for the Yankees, hit a home run during the game. Stadium operators partially raised the Mets' home run apple signal before lowering it back down, to the delight of the crowd.

Shea Stadium also hosted the first extra-inning regular season baseball opener ever played in New York, on March 31, 1998, when the Mets opened their season against their rival Philadelphia Phillies, playing the longest scoreless opening day game in the National League and the longest one in Major League Baseball since . The Mets won the game 1–0 in the bottom of the 14th inning.

During the 1977 New York City blackout the stadium was plunged into darkness at approximately 9:30 p.m. during a game between the Mets and the Chicago Cubs. It occurred during the bottom of the sixth inning, with the Mets losing 2–1 and Lenny Randle at bat. Jane Jarvis, Shea's organist (affectionately known as Shea's "Queen of Melody") played "Jingle Bells" and "White Christmas". The game was eventually completed on September 16, with the Cubs winning 5–2.

Shea Stadium held boxing matches in the mid-1960s.

The New York Jets of the American Football League and later, the National Football League played at Shea for 20 seasons, from 1964 through 1983 (excluding their first home game in 1977, played at Giants Stadium). The stadium hosted three Jets playoff games: the American Football League Championship in 1968 (beat the Oakland Raiders, 27–23), an AFL Divisional Playoff in 1969 (lost 13–6 to the Kansas City Chiefs) and the 1981 AFC Wild Card Playoff game (lost 31–27 to the Buffalo Bills).

For most of the Jets' tenure at Shea, they were burdened by onerous lease terms imposed at the insistence of the Mets. Until 1978, the Jets could not play their first home game until the Mets' season was finished. For instance, in 1969, the defending Super Bowl champion Jets didn't play a home game until October 20 due to the Mets advancing to (and winning) the World Series. As a result, the 1969 Jets opened with five consecutive road games, and then played all seven home games in consecutive weeks before closing with two road games. Even after 1978, the Mets' status as Shea's primary tenants would require the Jets to go on long road trips (switching Shea from baseball to football configuration was a complex process involving electrical, plumbing, field, and other similar work). The stadium was also not well maintained in the 1970s. The Jets moved to Giants Stadium for the 1984 season, enticed by the more than 15,000 additional seats there. Fans ripped apart Shea after the last game of the 1983 season, which also was the last game for Hall of Fame quarterback Terry Bradshaw, who threw two touchdown passes to lead the Pittsburgh Steelers to a 34–7 victory. Even the scoreboard operator had a field day, displaying the home team as the "N.J. Jets".
It was at Shea Stadium on December 16, 1973 that O.J. Simpson became the first running back to gain 2,000 yards in a single season (and, to date, the only player to do it in 14 games or fewer). In the 1983 season, a Jets game against the Los Angeles Rams featured an 85-yard touchdown run by rookie Eric Dickerson, as well as a brawl between Rams offensive tackle Jackie Slater and Jets defensive end Mark Gastineau when Slater blindsided Gastineau after the Jet performed his infamous "Sack Dance" over fallen Rams quarterback Vince Ferragamo.

The NFL's New York Giants played their 1975 season at Shea while Giants Stadium was being built. The Giants were that year (2–5 at Shea). Their coach was Bill Arnsparger and their quarterback was Craig Morton. The Giants played their final five home games of 1973 and all seven in 1974 at the Yale Bowl in New Haven, Connecticut; Yankee Stadium was closed in October 1973 for a massive renovation, which was completed in time for the 1976 baseball season.

On the night of October 9, 1965, Shea Stadium hosted the football rivalry between Army and Notre Dame for the first and only time. The Fighting Irish blanked the Cadets, 17-0, beginning a 15-game winning streak for Notre Dame in the storied series.

In 1966, the Brooklyn Dodgers of the minor Continental Football League unsuccessfully sued the Jets in an attempt to use the stadium; the team wound up playing on Randall's Island and soon folded. In 1974, the New York Stars of the nascent World Football League also made inquiries to play at Shea, whose schedule was already overcrowded by the Mets, Jets and Yankees (and the following year, the Giants; see below). The Stars also moved out to Randall's Island, playing only a handful of games before shifting to Charlotte.

The football field at Shea extended from around home plate to centerfield, with the baseline seating rotating out to fill left and right fields.

The first soccer game at Shea Stadium occurred during International Soccer League tournament play on June 17, 1965.

The original New York Cosmos beat the Washington Diplomats, 2-0, in an NASL playoff game at Shea on August 17, 1976.

New York United of the American Soccer League called Shea home in 1980.

On Sunday, August 15, 1965, the Beatles opened their 1965 North American tour there to a record audience of 55,600. "Beatlemania" was at one of its peaks at their Shea concert. Film footage shows many teenagers and women crying, screaming, and even fainting. The crowd noise was such that security guards can be seen covering their ears as the Beatles entered the field. The sound of the crowd was so deafening that none of the Beatles (or anyone else) could hear what they were playing. Nevertheless, it was the first concert to be held at a major stadium and set records for attendance and revenue generation, demonstrating that outdoor concerts on a large scale could be successful and profitable, and led the Beatles to return to Shea for a successful encore on August 23, 1966. The attendance record stood until 1973 when it was broken by Led Zeppelin with 56,800 fans at Tampa Stadium.

The next major music event to play Shea Stadium after the Beatles successful appearances was the Summer Festival for Peace on August 6, 1970. It was a day-long fundraiser, which featured many of the era's biggest selling and seminal rock, folk, blues and jazz performers including: Janis Joplin, Paul Simon, Creedence Clearwater Revival, Steppenwolf, The James Gang, Miles Davis, Tom Paxton, John Sebastian, and others.

The next music show at Shea Stadium was the historic concert by Grand Funk Railroad in 1971, which broke the Beatles' then-record for fastest ticket sales. Humble Pie was the opening band. The same filmmakers for the documentary of the Rolling Stones concert at Altamont were commissioned to film it, but to date, a final film has not been released.

The stadium subsequently hosted numerous concerts, including Jethro Tull with opening act Robin Trower in July 1976 (billed as Tull v. Boeing because of the proximity to LaGuardia Airport), The Who with opening act The Clash in October 1982, and Simon & Garfunkel in August 1983. On August 18, 1983, The Police played in front of 70,000 fans at Shea, a concert that the band's singer and bassist Sting described as "like playing the top of Everest", and announced near the end of the concert: "We'd like to thank the Beatles for lending us their stadium." The Rolling Stones performed at Shea for a six-night run in October 1989, and Elton John & Eric Clapton played a concert in August 1992. Bruce Springsteen and the E Street Band performed at Shea in early October 2003.

The last concert event was a two-night engagement by Billy Joel on July 16 and July 18, 2008. The concert was dubbed "The Last Play at Shea", and featured many special guest appearances, including former Beatle Paul McCartney who closed the second show with an emotional rendition of the Beatles classic "Let It Be". Other artists that joined Joel on stage for the shows were former Shea performer Roger Daltrey of The Who, Tony Bennett, Don Henley, John Mayer, John Mellencamp, Garth Brooks, and Steven Tyler of Aerosmith. The concert was the subject of a documentary film of the same name, which is used along with Shea's history to tell the story of changes in American suburban life.

The 1978 International Convention of Jehovah's Witnesses was held at Shea Stadium from July 12 to July 16, 1978.

During his tour of America in October 1979, Pope John Paul II was also among those hosted by Shea Stadium. On the morning of the Pontiff's visit, Shea Stadium was awash in torrential rain, causing ankle-deep mud puddles, and threatened to ruin the event. But as the Popemobile entered the stadium, the rain stopped although the deep mud remained.

On December 9, 1979, as part of the halftime show of a National Football League game between the New York Jets and New England Patriots, a model airplane group put on a remote control airplane display. The grand finale was a remote control airplane, weighing 40 lbs, made to look like a red flying lawnmower. The pilot lost control of the airplane, and it crashed into the stands, hitting Kevin Rourke, of Lynn, Massachusetts and John Bowen of Nashua, New Hampshire. Both suffered serious head injuries; Rourke survived but Bowen died four days later.

Between 1972 and 1980, Shea also hosted a Showdown at Shea event three separate times, by the then World Wrestling Federation. In 1980, it hosted a simulcast of the first fight between Roberto Duran and Sugar Ray Leonard, won by Duran.

From 1970 to 1987, the Cape Cod Baseball League (CCBL) played its annual all-star game at various major league stadiums. The games were interleague contests between the CCBL and the Atlantic Collegiate Baseball League (ACBL). The 1982 and 1986 games were played at Shea. The 1986 contest starred game MVP and future Cincinnati Reds all-star pitcher Jack Armstrong.

In the aftermath of the September 11 attacks, the stadium became a staging area for rescuers, its parking lots filled with food, water, medical supplies, even makeshift shelters where relief workers could sleep. Ten days later Shea reopened for the first post-attack sporting event in New York where the Mets beat the Braves, behind a dramatic home run by Mets catcher Mike Piazza.

In the television serial drama "Mad Men", the main character, Don Draper, has his secretary buy a pair of tickets for the Beatles' concert at Shea Stadium in 1965.

Shea Stadium was parodied as "Che Stadium" ("named for the Cuban guerilla leader, Che Guevara") for The Rutles film "All You Need is Cash" for a sequence that spoofed the Beatles' concert at the stadium.

Shea Stadium was parodied as Spray Stadium in an episode of Batman 66.

In 1987, Marvel Comics rented Shea Stadium to re-enact the wedding of Spider-Man/Peter Parker and Mary Jane Watson.

Recently on VH1's documentary series "7 Ages of Rock", Shea Stadium was named the most hallowed venue in all of rock music.

In "", the stadium was destroyed in a fight between Godzilla and Crackler.

Shea Stadium was used in the 1970s for filming the 1973 movie "Bang The Drum Slowly" starring Robert De Niro and Michael Moriarty and the 1978 film "The Wiz". In the latter film, the exterior pedestrian ramps were used for a motorcycle chase scene with Michael Jackson & Diana Ross.

A scene in the 2002 movie "Two Weeks Notice" takes place at Shea.

In "Men in Black", a Mets game at Shea was featured in the film, with outfielder Bernard Gilkey dropping a fly ball after being distracted by an alien spacecraft in the sky. Shea was also featured in "Men in Black 3" which is where K and J intercept Griffin and the ArcNet in 1969 before Boris the Animal can capture it.

Shea Stadium was also the setting for two episodes of "The King of Queens": "Doug Out" (1999) and "Catching Hell" (2005).

The Mets, Yankees, Jets and Giants all called Shea home in 1975, the only time in professional sports history that two baseball teams and two football teams shared the same facility in the same year.

As Yankee Stadium was being renovated and Giants Stadium was nearing completion, there were scheduling clashes between the four teams once the calendar turned to September. Neither the Jets nor the Giants could play "home" games at Shea Stadium until the baseball season ended for the Mets and Yankees. The matter was simplified when neither baseball team qualified for the postseason; still, there was a one-week overlap as the NFL season started on Sunday, September 21 while the MLB campaign ended on Sunday, September 28. This meant the Jets opened at home on Sunday, October 5, the third week of the season, and the Giants on Sunday, October 12, the season's fourth week. It also meant that the Giants and Jets had to play a combined 14 home games in the final 12 weeks of the 14-week NFL season. To do so, the Giants played two Saturday afternoon home games, neither of which were televised, and both of which were played the day before a Jets' Sunday home game. New York football fans thus enjoyed either the Jets or the Giants hosting a Sunday home game every weekend from October 5 through December 21. Shea wound up hosting all four teams on consecutive Sundays: Mets (September 21), Yankees (September 28), Jets (October 5) and Giants (October 12).

In total, the "Big Four" drew 3,738,546 customers to Shea: 1,730,566 by the Mets (76 home dates); 1,288,048 by the Yankees (71 home dates); 361,102 by the Jets (seven home games) and 358,830 by the Giants (also seven). Having both the Giants and Jets share Shea Stadium for one season foreshadowed what was to come in the future with the Meadowlands (a.k.a. Giants Stadium), after the Jets left Flushing Meadows for New Jersey following the 1983 NFL season. 

Shea was a circular stadium, with the grandstand forming about two-thirds of a circle around the field and ending a short distance beyond the foul lines. The remainder of the perimeter was mostly empty space beyond the outfield fences. This space was occupied by the bullpens, scoreboards, and a section of bleachers beyond the left field fence. The stadium boasted 54 restrooms, 21 escalators, seats for 57,343 fans (although as seating configuration changed constantly over the life of the stadium, that number varied often, dropping to 55,601 by the 1986 World Series, and then increased again over following years to between approximately 56,000 and 57,000, until its closing), and a massive 86' x 175' scoreboard. Also, rather than the standard light towers, Shea featured lamps along its upper reaches. Some deemed Shea a showplace, praised for its convenience, even its "elegance". The stadium's scoreboard in right field, one of the largest in MLB when it opened, weighed over 60 tons. One of its distinctive features was a giant rearview slide projector screen on the top center of the scoreboard; it was intended to display a picture of the current player at bat (a groundbreaking innovation at the time); however, due to lighting issues (it only worked at night when the light was really low; during day games, the picture would not show up at all), it was not used very often and was eventually covered with a giant Mets logo (or a Jets logo when they played).

The stadium was located close to LaGuardia Airport. For many years, interruptions for planes flying overhead were common at Shea; the noise was so loud that radio and television broadcasts could not be heard. Players would usually ask for time during noisy flight approaches and takeoffs.
Shea was originally designed with two motor-operated stands that allow the field level seats to rotate on underground tracks, allowing the stadium to be converted between a baseball and an American football/soccer configuration. In 1982, a new Mitsubishi DiamondVision screen was installed in left field. After the New York Jets football team moved to Giants Stadium in East Rutherford, New Jersey in 1984, the Mets took over operation of the stadium and retrofitted it for exclusive baseball use. As part of the refitting, Shea Stadium's exterior was painted blue and neon signs of baseball player silhouettes were added to the windscreens prior to the 1988 season. The original scoreboard was removed, and a new one installed in its place (fitting into the shell left behind by the old one) allowing for a much greater space for information and entertainment, in 1988. 
Also, after years of injuries to players crashing into the wooden outfield wall, most notably to 1973 star player Rusty Staub, where one injury caused a dislocated shoulder and forced him to miss or play severely injured during that Championship Season, the original wall finally had padding added to it, as most in baseball already did, greatly reducing injuries to outfielders.
Banks of ramps that provided access from the ground to the upper levels were built around the outside circumference of the stadium. The ramps were not walled in and were visible from outside the stadium. The ramps were originally partly covered with many rectangular panels in blue and orange, the Mets' colors. These panels can be seen in the 1970s movie "The Wiz", which used the exterior pedestrian ramps for a motorcycle chase scene with Michael Jackson and Diana Ross. The 1960s-style decorations were removed in 1980. The banks of ramps resulted in the outer wall of the stadium jutting out where the banks existed.

The design also allowed for Shea Stadium to be expandable to 90,000 seats, simply by completely enclosing the grandstand. It was also designed to be later enclosed by a dome if warranted. In March 1965, a plan was formally announced to add a glass dome and add 15,000 seats. The Mets strongly objected to the proposal. The idea was later dropped after engineering studies concluded that the stadium's foundation would be unable to support the weight of the dome.

The distances to the right and left field foul poles were initially both . There was a horizontal orange line that determined where a batted ball was a home run or still in play. In 1978, Manager Joe Torre suggested moving in the fences to in the corners with a wall in front of the original brick wall, to decrease the number of disputed calls.

Originally, all of the seats were wooden, with each level having a different color. The field boxes were yellow, the loge level seats were brown, the mezzanine seats were blue, and the upper deck seats were green. Each level above the field level was divided into box seats below the entrance/exit portals and reserved seats above the portals. The box seats were a darker shade than the reserved seats. The game ticket was the same color as the seat that it represented, and the signs in the lobby for that section were the same color as the seat and the ticket. Before the 1980 baseball season, they were replaced with red (upper deck), green (mezzanine), blue (loge), and orange (field level) plastic seats.
Unlike Yankee Stadium, Shea was built on an open field, so there was no need to have it conform to the surrounding streets.

Before Shea Stadium closed in 2008, it was the only stadium in the major leagues with orange foul poles. This tradition is carried on at Citi Field as the foul poles there are the same color.

After the Jets left Shea, the exterior of the stadium was painted blue and white, two of the Mets' team colors.

In 2003, large murals celebrating the Mets' two world championships were added, covering the two ends of the grandstand. The 1986 mural was removed after the 2006 season because of deterioration (the wall was re-painted solid blue, and a window was opened on the mezzanine level where fans could view the progress of Citi Field), but the 1969 mural survived until the final game at the end of .

With its refurbishment in 1988, the scoreboard was topped by a representation of the New York Skyline, a prominent part of the team logo. After the September 11 terrorist attacks, the Twin Towers of the World Trade Center were kept unlit, with a red-white-and-blue ribbon placed over them. The scoreboard was demolished in October 2008, but the skyline was preserved and is now located on the Shake Shack in Citi Field's "Taste Of The City" food court behind the giant scoreboard in center field.

During the 2007 and 2008 seasons, the construction of Citi Field was visible beyond the left and center field walls of Shea.

From 1973 to 1979, fans could estimate the distance of home run balls, since there were several signs beyond the outfield wall giving the distance in feet from home plate, in addition to the nine markers within the field.

The Home Run Apple came out of a magic hat after every Mets home run at Shea Stadium. It was first installed in May 1980 as a symbol of the Mets' advertising slogan "The Magic Is Back!" (the hat originally said "Mets Magic" in script but was changed in the mid-1980s to a simple "Home Run" in block capital letters). A bigger apple was placed in center field at Citi Field. The original apple was installed inside Citi Field's bullpen gate and was visible from outside, on 126th Street. In 2010, the original Shea apple was relocated outside the Citi Field, in front of the Jackie Robinson Rotunda.

Four players in the National League named their children after Shea Stadium.

Actor Kevin James, a devoted Mets fan, named his youngest daughter Shea Joelle.

Minor League Baseball social media wizard Andie Kruczkowski has the middle name Shea in tribute.


 

 


</doc>
<doc id="28857" url="https://en.wikipedia.org/wiki?curid=28857" title="Signal transduction">
Signal transduction

Signal transduction is the process by which a chemical or physical signal is transmitted through a cell as a series of molecular events, most commonly protein phosphorylation catalyzed by protein kinases, which ultimately results in a cellular response. Proteins responsible for detecting stimuli are generally termed receptors, although in some cases the term sensor is used. The changes elicited by ligand binding (or signal sensing) in a receptor give rise to a biochemical cascade, which is a chain of biochemical events known as a signaling pathway.

When signaling pathways interact with one another they form networks, which allow cellular responses to be coordinated, often by combinatorial signaling events. At the molecular level, such responses include changes in the transcription or translation of genes, and post-translational and conformational changes in proteins, as well as changes in their location. These molecular events are the basic mechanisms controlling cell growth, proliferation, metabolism and many other processes. In multicellular organisms, signal transduction pathways have evolved to regulate cell communication in a wide variety of ways.

Each component (or node) of a signaling pathway is classified according to the role it plays with respect to the initial stimulus. Ligands are termed "first messengers", while receptors are the "signal transducers", which then activate "primary effectors". Such effectors are often linked to second messengers, which can activate "secondary effectors", and so on. Depending on the efficiency of the nodes, a signal can be amplified (a concept known as signal gain), so that one signaling molecule can generate a response involving hundreds to millions of molecules. As with other signals, the transduction of biological signals is characterised by delay, noise, signal feedback and feedforward and interference, which can range from negligible to pathological. With the advent of computational biology, the analysis of signaling pathways and networks has become an essential tool to understand cellular functions and disease, including signaling rewiring mechanisms underlying responses to acquired drug resistance.

The basis for signal transduction is the transformation of a certain stimulus into a biochemical signal. The nature of such stimuli can vary widely, ranging from extracellular cues, such as the presence of EGF, to intracellular events, such as the DNA damage resulting from replicative telomere attrition. Traditionally, signals that reach the central nervous system are classified as senses. These are transmitted from neuron to neuron in a process called synaptic transmission. Many other intercellular signal relay mechanisms exist in multicellular organisms, such as those that govern embryonic development.

The majority of signal transduction pathways involve the binding of signaling molecules, known as ligands, to receptors that trigger events inside the cell. The binding of a signaling molecule with a receptor causes a change in the conformation of the receptor, known as "receptor activation". Most ligands are soluble molecules from the extracellular medium which bind to cell surface receptors. These include growth factors, cytokines and neurotransmitters. Components of the extracellular matrix such as fibronectin and hyaluronan can also bind to such receptors (integrins and CD44, respectively). In addition, some molecules such as steroid hormones are lipid-soluble and thus cross the plasma membrane to reach nuclear receptors. In the case of steroid hormone receptors, their stimulation leads to binding to the promoter region of steroid-responsive genes.

Not all classifications of signaling molecules take into account the molecular nature of each class member. For example, odorants belong to a wide range of molecular classes, as do neurotransmitters, which range in size from small molecules such as dopamine to neuropeptides such as endorphins. Moreover, some molecules may fit into more than one class, e.g. epinephrine is a neurotransmitter when secreted by the central nervous system and a hormone when secreted by the adrenal medulla.

Some receptors such as HER2 are capable of ligand-independent activation when overexpressed or mutated. This leads to constituitive activation of the pathway, which may or may not be overturned by compensation mechanisms. In the case of HER2, which acts as a dimerization partner of other EGFRs, constituitive activation leads to hyperproliferation and cancer.

The prevalence of basement membranes in the tissues of Eumetazoans means that most cell types require attachment to survive. This requirement has led to the development of complex mechanotransduction pathways, allowing cells to sense the stiffness of the substratum. Such signaling is mainly orchestrated in focal adhesions, regions where the integrin-bound actin cytoskeleton detects changes and transmits them downstream through YAP1. Calcium-dependent cell adhesion molecules such as cadherins and selectins can also mediate mechanotransduction. Specialised forms of mechanotransduction within the nervous system are responsible for mechanosensation: hearing, touch, proprioception and balance.

Cellular and systemic control of osmotic pressure (the difference in osmolarity between the cytosol and the extracellular medium) is critical for homeostasis. There are three ways in which cells can detect osmotic stimuli: as changes in macromolecular crowding, ionic strength, and changes in the properties of the plasma membrane or cytoskeleton (the latter being a form of mechanotransduction). These changes are detected by proteins known as osmosensors or osmoreceptors. In humans, the best characterised osmosensors are transient receptor potential channels present in the primary cilium of human cells. In yeast, the HOG pathway has been extensively characterised.

The sensing of temperature in cells is known as thermoception and is primarily mediated by transient receptor potential channels. Additionally, animal cells contain a conserved mechanism to prevent high temperatures from causing cellular damage, the heat-shock response. Such response is triggered when high temperatures cause the dissociation of inactive HSF1 from complexes with heat shock proteins Hsp40/Hsp70 and Hsp90. With help from the ncRNA "hsr1", HSF1 then trimerizes, becoming active and upregulating the expression of its target genes. Many other thermosensory mechanisms exist in both prokaryotes and eukaryotes.

In mammals, light controls the sense of sight and the circadian clock by activating light-sensitive proteins in photoreceptor cells in the eye's retina. In the case of vision, light is detected by rhodopsin in rod and cone cells. In the case of the circadian clock, a different photopigment, melanopsin, is responsible for detecting light in intrinsically photosensitive retinal ganglion cells.

Receptors can be roughly divided into two major classes: intracellular and extracellular receptors.

Extracellular receptors are integral transmembrane proteins and make up most receptors. They span the plasma membrane of the cell, with one part of the receptor on the outside of the cell and the other on the inside. Signal transduction occurs as a result of a ligand binding to the outside region of the receptor (the ligand does not pass through the membrane). Ligand-receptor binding induces a change in the conformation of the inside part of the receptor, a process sometimes called "receptor activation". This results in either the activation of an enzyme domain of the receptor or the exposure of a binding site for other intracellular signaling proteins within the cell, eventually propagating the signal through the cytoplasm.

In eukaryotic cells, most intracellular proteins activated by a ligand/receptor interaction possess an enzymatic activity; examples include tyrosine kinase and phosphatases. Often such enzymes are covalently linked to the receptor. Some of them create second messengers such as cyclic AMP and IP, the latter controlling the release of intracellular calcium stores into the cytoplasm. Other activated proteins interact with adaptor proteins that facilitate signaling protein interactions and coordination of signaling complexes necessary to respond to a particular stimulus. Enzymes and adaptor proteins are both responsive to various second messenger molecules.

Many adaptor proteins and enzymes activated as part of signal transduction possess specialized protein domains that bind to specific secondary messenger molecules. For example, calcium ions bind to the EF hand domains of calmodulin, allowing it to bind and activate calmodulin-dependent kinase. PIP and other phosphoinositides do the same thing to the Pleckstrin homology domains of proteins such as the kinase protein AKT.

G protein–coupled receptors (GPCRs) are a family of integral transmembrane proteins that possess seven transmembrane domains and are linked to a heterotrimeric G protein. With nearly 800 members, this is the largest family of membrane proteins and receptors in mammals. Counting all animal species, they add up to over 5000. Mammalian GPCRs are classified into 5 major families: rhodopsin-like, secretin-like, metabotropic glutamate, adhesion and frizzled/smoothened, with a few GPCR groups being difficult to classify due to low sequence similarity, e.g. vomeronasal receptors. Other classes exist in eukaryotes, such as the "Dictyostelium" cyclic AMP receptors and fungal mating pheromone receptors.

Signal transduction by a GPCR begins with an inactive G protein coupled to the receptor; the G protein exists as a heterotrimer consisting of Gα, Gβ, and Gγ subunits. Once the GPCR recognizes a ligand, the conformation of the receptor changes to activate the G protein, causing Gα to bind a molecule of GTP and dissociate from the other two G-protein subunits. The dissociation exposes sites on the subunits that can interact with other molecules. The activated G protein subunits detach from the receptor and initiate signaling from many downstream effector proteins such as phospholipases and ion channels, the latter permitting the release of second messenger molecules. The total strength of signal amplification by a GPCR is determined by the lifetimes of the ligand-receptor complex and receptor-effector protein complex and the deactivation time of the activated receptor and effectors through intrinsic enzymatic activity; e.g. via protein kinase phosphorylation or b-arrestin-dependent internalization.

A study was conducted where a point mutation was inserted into the gene encoding the chemokine receptor CXCR2; mutated cells underwent a malignant transformation due to the expression of CXCR2 in an active conformation despite the absence of chemokine-binding. This meant that chemokine receptors can contribute to cancer development.

Receptor tyrosine kinases (RTKs) are transmembrane proteins with an intracellular kinase domain and an extracellular domain that binds ligands; examples include growth factor receptors such as the insulin receptor. To perform signal transduction, RTKs need to form dimers in the plasma membrane; the dimer is stabilized by ligands binding to the receptor. The interaction between the cytoplasmic domains stimulates the autophosphorylation of tyrosine residues within the intracellular kinase domains of the RTKs, causing conformational changes. Subsequent to this, the receptors' kinase domains are activated, initiating phosphorylation signaling cascades of downstream cytoplasmic molecules that facilitate various cellular processes such as cell differentiation and metabolism. Many Ser/Thr and dual-specificity protein kinases are important for signal transduction, either acting downstream of [receptor tyrosine kinases], or as membrane-embedded or cell-soluble versions in their own right. The process of signal transduction involves around 560 known protein kinases and pseudokinases, encoded by the human kinome 

As is the case with GPCRs, proteins that bind GTP play a major role in signal transduction from the activated RTK into the cell. In this case, the G proteins are members of the Ras, Rho, and Raf families, referred to collectively as small G proteins. They act as molecular switches usually tethered to membranes by isoprenyl groups linked to their carboxyl ends. Upon activation, they assign proteins to specific membrane subdomains where they participate in signaling. Activated RTKs in turn activate small G proteins that activate guanine nucleotide exchange factors such as SOS1. Once activated, these exchange factors can activate more small G proteins, thus amplifying the receptor's initial signal. The mutation of certain RTK genes, as with that of GPCRs, can result in the expression of receptors that exist in a constitutively activated state; such mutated genes may act as oncogenes.

Histidine-specific protein kinases are structurally distinct from other protein kinases and are found in prokaryotes, fungi, and plants as part of a two-component signal transduction mechanism: a phosphate group from ATP is first added to a histidine residue within the kinase, then transferred to an aspartate residue on a receiver domain on a different protein or the kinase itself, thus activating the aspartate residue.

Integrins are produced by a wide variety of cells; they play a role in cell attachment to other cells and the extracellular matrix and in the transduction of signals from extracellular matrix components such as fibronectin and collagen. Ligand binding to the extracellular domain of integrins changes the protein's conformation, clustering it at the cell membrane to initiate signal transduction. Integrins lack kinase activity; hence, integrin-mediated signal transduction is achieved through a variety of intracellular protein kinases and adaptor molecules, the main coordinator being integrin-linked kinase. As shown in the adjacent picture, cooperative integrin-RTK signaling determines the timing of cellular survival, apoptosis, proliferation, and differentiation.

Important differences exist between integrin-signaling in circulating blood cells and non-circulating cells such as epithelial cells; integrins of circulating cells are normally inactive. For example, cell membrane integrins on circulating leukocytes are maintained in an inactive state to avoid epithelial cell attachment; they are activated only in response to stimuli such as those received at the site of an inflammatory response. In a similar manner, integrins at the cell membrane of circulating platelets are normally kept inactive to avoid thrombosis. Epithelial cells (which are non-circulating) normally have active integrins at their cell membrane, helping maintain their stable adhesion to underlying stromal cells that provide signals to maintain normal functioning.

In plants, there are no bona fide integrin receptors identified to date; nevertheless, several integrin-like proteins were proposed based on structural homology with the metazoan receptors. Plants contain integrin-linked kinases that are very similar in their primary structure with the animal ILKs. In the experimental model plant "Arabidopsis thaliana", one of the integrin-linked kinase genes, "ILK1", has been shown to be a critical element in the plant immune response to signal molecules from bacterial pathogens and plant sensitivity to salt and osmotic stress. ILK1 protein interacts with the high-affinity potassium transporter HAK5 and with the calcium sensor CML9.

When activated, toll-like receptors (TLRs) take adapter molecules within the cytoplasm of cells in order to propagate a signal. Four adaptor molecules are known to be involved in signaling, which are Myd88, TIRAP, TRIF, and TRAM. These adapters activate other intracellular molecules such as IRAK1, IRAK4, TBK1, and IKKi that amplify the signal, eventually leading to the induction or suppression of genes that cause certain responses. Thousands of genes are activated by TLR signaling, implying that this method constitutes an important gateway for gene modulation.

A ligand-gated ion channel, upon binding with a ligand, changes conformation to open a channel in the cell membrane through which ions relaying signals can pass. An example of this mechanism is found in the receiving cell of a neural synapse. The influx of ions that occurs in response to the opening of these channels induces action potentials, such as those that travel along nerves, by depolarizing the membrane of post-synaptic cells, resulting in the opening of voltage-gated ion channels.

An example of an ion allowed into the cell during a ligand-gated ion channel opening is Ca; it acts as a second messenger initiating signal transduction cascades and altering the physiology of the responding cell. This results in amplification of the synapse response between synaptic cells by remodelling the dendritic spines involved in the synapse.

Intracellular receptors, such as nuclear receptors and cytoplasmic receptors, are soluble proteins localized within their respective areas. The typical ligands for nuclear receptors are non-polar hormones like the steroid hormones testosterone and progesterone and derivatives of vitamins A and D. To initiate signal transduction, the ligand must pass through the plasma membrane by passive diffusion. On binding with the receptor, the ligands pass through the nuclear membrane into the nucleus, altering gene expression.

Activated nuclear receptors attach to the DNA at receptor-specific hormone-responsive element (HRE) sequences, located in the promoter region of the genes activated by the hormone-receptor complex. Due to their enabling gene transcription, they are alternatively called inductors of gene expression. All hormones that act by regulation of gene expression have two consequences in their mechanism of action; their effects are produced after a characteristically long period of time and their effects persist for another long period of time, even after their concentration has been reduced to zero, due to a relatively slow turnover of most enzymes and proteins that would either deactivate or terminate ligand binding onto the receptor.

Nucleic receptors have DNA-binding domains containing zinc fingers and a ligand-binding domain; the zinc fingers stabilize DNA binding by holding its phosphate backbone. DNA sequences that match the receptor are usually hexameric repeats of any kind; the sequences are similar but their orientation and distance differentiate them. The ligand-binding domain is additionally responsible for dimerization of nucleic receptors prior to binding and providing structures for transactivation used for communication with the translational apparatus.

Steroid receptors are a subclass of nuclear receptors located primarily within the cytosol. In the absence of steroids, they associate in an aporeceptor complex containing chaperone or heatshock proteins (HSPs). The HSPs are necessary to activate the receptor by assisting the protein to fold in a way such that the signal sequence enabling its passage into the nucleus is accessible. Steroid receptors, on the other hand, may be repressive on gene expression when their transactivation domain is hidden. Receptor activity can be enhanced by phosphorylation of serine residues at their N-terminal as a result of another signal transduction pathway, a process called crosstalk.

Retinoic acid receptors are another subset of nuclear receptors. They can be activated by an endocrine-synthesized ligand that entered the cell by diffusion, a ligand synthesised from a precursor like retinol brought to the cell through the bloodstream or a completely intracellularly synthesised ligand like prostaglandin. These receptors are located in the nucleus and are not accompanied by HSPs. They repress their gene by binding to their specific DNA sequence when no ligand binds to them, and vice versa.

Certain intracellular receptors of the immune system are cytoplasmic receptors; recently identified NOD-like receptors (NLRs) reside in the cytoplasm of some eukaryotic cells and interact with ligands using a leucine-rich repeat (LRR) motif similar to TLRs. Some of these molecules like NOD2 interact with RIP2 kinase that activates NF-κB signaling, whereas others like NALP3 interact with inflammatory caspases and initiate processing of particular cytokines like interleukin-1β.

First messengers are the signaling molecules (hormones, neurotransmitters, and paracrine/autocrine agents) that reach the cell from the extracellular fluid and bind to their specific receptors. Second messengers are the substances that enter the cytoplasm and act within the cell to trigger a response. In essence, second messengers serve as chemical relays from the plasma membrane to the cytoplasm, thus carrying out intracellular signal transduction.

The release of calcium ions from the endoplasmic reticulum into the cytosol results in its binding to signaling proteins that are then activated; it is then sequestered in the smooth endoplasmic reticulum and the mitochondria. Two combined receptor/ion channel proteins control the transport of calcium: the InsP-receptor that transports calcium upon interaction with inositol triphosphate on its cytosolic side; and the ryanodine receptor named after the alkaloid ryanodine, similar to the InsP receptor but having a feedback mechanism that releases more calcium upon binding with it. The nature of calcium in the cytosol means that it is active for only a very short time, meaning its free state concentration is very low and is mostly bound to organelle molecules like calreticulin when inactive.

Calcium is used in many processes including muscle contraction, neurotransmitter release from nerve endings, and cell migration. The three main pathways that lead to its activation are GPCR pathways, RTK pathways, and gated ion channels; it regulates proteins either directly or by binding to an enzyme.

Lipophilic second messenger molecules are derived from lipids residing in cellular membranes; enzymes stimulated by activated receptors activate the lipids by modifying them. Examples include diacylglycerol and ceramide, the former required for the activation of protein kinase C.

Nitric oxide (NO) acts as a second messenger because it is a free radical that can diffuse through the plasma membrane and affect nearby cells. It is synthesised from arginine and oxygen by the NO synthase and works through activation of soluble guanylyl cyclase, which when activated produces another second messenger, cGMP. NO can also act through covalent modification of proteins or their metal co-factors; some have a redox mechanism and are reversible. It is toxic in high concentrations and causes damage during stroke, but is the cause of many other functions like relaxation of blood vessels, apoptosis, and penile erections.

In addition to nitric oxide, other electronically activated species are also signal-transducing agents in a process called redox signaling. Examples include superoxide, hydrogen peroxide, carbon monoxide, and hydrogen sulfide. Redox signaling also includes active modulation of electronic flows in semiconductive biological macromolecules.

Gene activations and metabolism alterations are examples of cellular responses to extracellular stimulation that require signal transduction. Gene activation leads to further cellular effects, since the products of responding genes include instigators of activation; transcription factors produced as a result of a signal transduction cascade can activate even more genes. Hence, an initial stimulus can trigger the expression of a large number of genes, leading to physiological events like the increased uptake of glucose from the blood stream and the migration of neutrophils to sites of infection. The set of genes and their activation order to certain stimuli is referred to as a genetic program.

Mammalian cells require stimulation for cell division and survival; in the absence of growth factor, apoptosis ensues. Such requirements for extracellular stimulation are necessary for controlling cell behavior in unicellular and multicellular organisms; signal transduction pathways are perceived to be so central to biological processes that a large number of diseases are attributed to their disregulation.
Three basic signals determine cellular growth:
The combination of these signals are integrated in an altered cytoplasmic machinery which leads to altered cell behaviour.

Following are some major signaling pathways, demonstrating how ligands binding to their receptors can affect second messengers and eventually result in altered cellular responses.


The earliest notion of signal transduction can be traced back to 1855, when Claude Bernard proposed that ductless glands such as the spleen, the thyroid and adrenal glands, were responsible for the release of "internal secretions" with physiological effects. Bernard's "secretions" were later named "hormones" by Ernest Starling in 1905. Together with William Bayliss, Starling had discovered secretin in 1902. Although many other hormones, most notably insulin, were discovered in the following years, the mechanisms remained largely unknown.

The discovery of nerve growth factor by Rita Levi-Montalcini in 1954, and epidermal growth factor by Stanley Cohen in 1962, led to more detailed insights into the molecular basis of cell signaling, in particular growth factors. Their work, together with Earl Wilbur Sutherland's discovery of cyclic AMP in 1956, prompted the redefinition of endocrine signaling to include only signaling from glands, while the terms autocrine and paracrine began to be used. Sutherland was awarded the 1971 Nobel Prize in Physiology or Medicine, while Levi-Montalcini and Cohen shared it in 1986.

In 1970, Martin Rodbell examined the effects of glucagon on a rat's liver cell membrane receptor. He noted that guanosine triphosphate disassociated glucagon from this receptor and stimulated the G-protein, which strongly influenced the cell's metabolism. Thus, he deduced that the G-protein is a transducer that accepts glucagon molecules and affects the cell. For this, he shared the 1994 Nobel Prize in Physiology or Medicine with Alfred G. Gilman. Thus, the characterization of RTKs and GPCRs led to the formulation of the concept of "signal transduction", a word first used in 1972. Some early articles used the terms "signal transmission" and "sensory transduction". In 2007, a total of 48,377 scientific papers—including 11,211 review papers—were published on the subject. The term first appeared in a paper's title in 1979. Widespread use of the term has been traced to a 1980 review article by Rodbell: Research papers focusing on signal transduction first appeared in large numbers in the late 1980s and early 1990s.




</doc>
<doc id="28858" url="https://en.wikipedia.org/wiki?curid=28858" title="Stone–Weierstrass theorem">
Stone–Weierstrass theorem

In mathematical analysis, the Weierstrass approximation theorem states that every continuous function defined on a closed interval can be uniformly approximated as closely as desired by a polynomial function. Because polynomials are among the simplest functions, and because computers can directly evaluate polynomials, this theorem has both practical and theoretical relevance, especially in polynomial interpolation. The original version of this result was established by Karl Weierstrass in 1885 using the Weierstrass transform.

Marshall H. Stone considerably generalized the theorem and simplified the proof . His result is known as the Stone–Weierstrass theorem. The Stone–Weierstrass theorem generalizes the Weierstrass approximation theorem in two directions both regressive and progressive: instead of the real interval , an arbitrary compact Hausdorff space is considered, and instead of the algebra of polynomial functions, approximation with elements from more general subalgebras of is investigated. The Stone–Weierstrass theorem is a vital result in the study of the algebra of continuous functions on a compact Hausdorff space.

Further, there is a generalization of the Stone–Weierstrass theorem to noncompact Tychonoff spaces, namely, any continuous function on a Tychonoff space is approximated uniformly on compact sets by algebras of the type appearing in the Stone–Weierstrass theorem and described below.

A different generalization of Weierstrass' original theorem is Mergelyan's theorem, which generalizes it to functions defined on certain subsets of the complex plane.

The statement of the approximation theorem as originally discovered by Weierstrass is as follows:

A constructive proof of this theorem using Bernstein polynomials is outlined on that page.

As a consequence of the Weierstrass approximation theorem, one can show that the space is separable: the polynomial functions are dense, and each polynomial function can be uniformly approximated by one with rational coefficients; there are only countably many polynomials with rational coefficients. Since is Hausdorff and separable it follows that has cardinality at most . (Remark: This cardinality result also follows from the fact that a continuous function on the reals is uniquely determined by its restriction to the rationals.)

The set of continuous real-valued functions on , together with the supremum norm , is a Banach algebra, (that is, an associative algebra and a Banach space such that for all ). The set of all polynomial functions forms a subalgebra of (that is, a vector subspace of that is closed under multiplication of functions), and the content of the Weierstrass approximation theorem is that this subalgebra is dense in .

Stone starts with an arbitrary compact Hausdorff space and considers the algebra of real-valued continuous functions on , with the topology of uniform convergence. He wants to find subalgebras of which are dense. It turns out that the crucial property that a subalgebra must satisfy is that it "separates points": a set of functions defined on is said to separate points if, for every two different points and in there exists a function in with . Now we may state:

This implies Weierstrass’ original statement since the polynomials on form a subalgebra of which contains the constants and separates points.

A version of the Stone–Weierstrass theorem is also true when is only locally compact. Let be the space of real-valued continuous functions on which vanish at infinity; that is, a continuous function is in if, for every , there exists a compact set such that on . Again, is a Banach algebra with the supremum norm. A subalgebra of is said to vanish nowhere if not all of the elements of simultaneously vanish at a point; that is, for every in , there is some in such that . The theorem generalizes as follows:

This version clearly implies the previous version in the case when is compact, since in that case . There are also more general versions of the Stone–Weierstrass that weaken the assumption of local compactness.

The Stone–Weierstrass theorem can be used to prove the following two statements which go beyond Weierstrass's result.


The theorem has many other applications to analysis, including:


Slightly more general is the following theorem, where we consider the algebra formula_1 of complex-valued continuous functions on the compact space formula_2, again with the topology of uniform convergence. This is a C*-algebra with the *-operation given by pointwise complex conjugation.

The complex unital *-algebra generated by formula_4 consists of all those functions that can be obtained from the elements of formula_4 by throwing in the constant function and adding them, multiplying them, conjugating them, or multiplying them with complex scalars, and repeating finitely many times.

This theorem implies the real version, because if a sequence of complex-valued functions uniformly approximate a given function formula_10, then the real parts of those functions uniformly approximate the real part of formula_10. As in the real case, an analog of this theorem is true for locally compact Hausdorff spaces.

Following : consider the algebra of quaternion-valued continuous functions on the compact space , again with the topology of uniform convergence. If a quaternion "q" is written in the form "q" = "a" + "ib";+ "jc" + "kd" then the scalar part a is the real number ("q" − "iqi" − "jqj" − "kqk")/4. Likewise being the scalar part of −"qi", −"qj" and −"qk" : b,c and d are respectively the real numbers (−"qi" − "iq" + "jqk" − "kqj")/4,
(−"qj" − "iqk" − "jq" + "kqi")/4 and (−"qk" + "iqj" − "jqk" − "kq")/4. Then we may state :

The space of complex-valued continuous functions on a compact Hausdorff space "X" i.e. C("X", C) is the canonical example of a unital commutative C*-algebra formula_12. The space "X" may be viewed as the space of pure states on formula_12, with the weak-* topology. Following the above cue, a non-commutative extension of the Stone–Weierstrass theorem, which has remain unsolved, is as follows:

In 1960, Jim Glimm proved a weaker version of the above conjecture.

Let be a compact Hausdorff space. Stone's original proof of the theorem used the idea of lattices in . A subset of is called a lattice if for any two elements , the functions also belong to . The lattice version of the Stone–Weierstrass theorem states:

The above versions of Stone–Weierstrass can be proven from this version once one realizes that the lattice property can also be formulated using the absolute value which in turn can be approximated by polynomials in . A variant of the theorem applies to linear subspaces of closed under max :

More precise information is available:

Another generalization of the Stone–Weierstrass theorem is due to Errett Bishop. Bishop's theorem is as follows :

Nachbin's theorem gives an analog for Stone–Weierstrass theorem for algebras of complex valued smooth functions on a smooth manifold . Nachbin's theorem is as follows :



The historical publication of Weierstrass (in German language) is freely available from the digital online archive of the "Berlin Brandenburgische Akademie der Wissenschaften":


Important historical works of Stone include:


</doc>
<doc id="28860" url="https://en.wikipedia.org/wiki?curid=28860" title="Simile">
Simile

A simile () is a figure of speech that directly "compares" two things. Similes differ from metaphors by highlighting the similarities between two things that must use "like" and "as", while metaphors create an implicit comparison (i.e. saying something "is" something else). This distinction is evident in the etymology of the words: simile derives from the Latin word "similis" ("similar, like"), while metaphor derives from the Greek word "metapherein" ("to transfer"). While similes are mainly used in forms of poetry that compare the inanimate and the living, there are also terms in which similes are used for humorous purposes and comparison.



Similes are used extensively in British comedy, notably in the slapstick era of the 1960s and 1970s. In comedy, the simile is often used in negative style: "he was as daft as a brush." They are also used in comedic context where a sensitive subject is broached, and the comedian will test the audience with response to subtle implicit simile before going deeper. The sitcom "Blackadder" featured the use of extended similes, normally said by the title character. For example:

Given that similes emphasize affinities between different objects, they occur in many cultures and languages.

Sayf al-Din al-Amidi discussed Arabic similes in 1805: "On Substantiation Through Transitive Relations".

Thuy Nga Nguyen and Ghil'ad Zuckermann (2012) classify Vietnamese similes into two types: Meaning Similes and Rhyming Similes.

The following is an example:
<br>
<br>Nghèo như con mèo
<br>/ŋɛu ɲɯ kɔn mɛu/ 
<br>"Poor as a cat"
<br>

Whereas the above Vietnamese example is of a rhyming simile, the English simile "(as) poor as a church mouse" is only a semantic simile.



</doc>
<doc id="28861" url="https://en.wikipedia.org/wiki?curid=28861" title="Serengeti">
Serengeti

The Serengeti ( ) ecosystem is a geographical region in Africa, spanning northern Tanzania and some of southwestern Kenya. The protected area within the region includes approximately of land, including the Serengeti National Park and several game reserves. The Serengeti hosts the second largest terrestrial mammal migration in the world, which helps secure it as one of the Seven Natural Wonders of Africa, and as one of the ten natural travel wonders of the world. 

The Serengeti is also renowned for its large lion population and is one of the best places to observe prides in their natural environment. Approximately 70 large mammal and 500 bird species are found there. This high diversity is a function of diverse habitats, including riverine forests, swamps, kopjes, grasslands, and woodlands. Blue wildebeests, gazelles, zebras, and buffalos are some of the commonly found large mammals in the region. 

The Serengeti also contains the Serengeti District of Tanzania. There has been controversy about a proposal to build a road through the Serengeti.

The name "Serengeti" is often said to be derived from the word from "seringit" in the Maasai language, Maa, meaning "endless plains". However, this etymology does not appear in Maa dictionaries.

Much of the Serengeti was known to outsiders as Maasailand. The Maasai are known as fierce warriors and live alongside most wild animals with an aversion to eating game and birds, subsisting exclusively on their cattle. Historically, their strength and reputation kept the newly arrived Europeans from exploiting the animals and resources of most of their land. A rinderpest epidemic and drought during the 1890s greatly reduced the numbers of both Maasai and animal populations. The Tanzanian government later in the 20th century re-settled the Maasai around the Ngorongoro Crater. Poaching and the absence of fires, which had been the result of human activity, set the stage for the development of dense woodlands and thickets over the next 30–50 years. Tsetse fly populations now prevented any significant human settlement in the area.

By the mid-1970s, wildebeest and the Cape buffalo populations had recovered and were increasingly cropping the grass, reducing the amount of fuel available for fires. The reduced intensity of fires has allowed acacia to once again become established.

In the 21st century, mass rabies vaccination programmes for domestic dogs in the Serengeti have not only indirectly prevented hundreds of human deaths, but also protected wildlife species such as the endangered African wild dog.

Each year around the same time, the circular great wildebeest migration begins in the Ngorongoro Conservation Area of the southern Serengeti in Tanzania and loops in a clockwise direction through the Serengeti National Park and north towards the Masai Mara reserve in Kenya. This migration is a natural phenomenon determined by the availability of grazing. The initial phase lasts from approximately January to March, when the calving season begins – a time when there is plenty of rain-ripened grass available for the 260,000 zebra that precede 1.7 million wildebeest and the following hundreds of thousands of other plains game, including around 470,000 gazelles.

During February, the wildebeest spend their time on the short grass plains of the southeastern part of the ecosystem, grazing and giving birth to approximately 500,000 calves within a 2 to 3-week period. Few calves are born ahead of time and of these, hardly any survive. The main reason is that very young calves are more noticeable to predators when mixed with older calves from the previous year. As the rains end in May, the animals start moving northwest into the areas around the Grumeti River, where they typically remain until late June. The crossings of the Grumeti and Mara rivers beginning in July are a popular safari attraction because crocodiles are lying in wait. The herds arrive in Kenya in late July / August, where they stay for the remainder of the dry season, except that the Thomson's and Grant's gazelles move only east/west. In early November, with the start of the short rains the migration starts moving south again, to the short grass plains of the southeast, usually arriving in December in plenty of time for calving in February.

About 250,000 wildebeest die during the journey from Tanzania to the Maasai Mara National Reserve in southwestern Kenya, a total of . Death is usually from thirst, hunger, exhaustion, or predation.

The Serengeti has some of East Africa's finest game areas. Besides being known for the great migration, the Serengeti is also famous for its abundant large predators. The ecosystem is home to over 3,000 lions ("Panthera leo"), 1,000 leopards ("Panthera pardus"), and 7,700 to 8,700 spotted hyenas ("Crocuta crocuta).".The East African cheetah are also present in Serengeti.

Wild dogs are relatively scarce in much of the Serengeti. This is particularly true in places such as Serengeti National Park (where they became extinct in 1992), in which lions and spotted hyenas, predators that steal wild dog kills and are a direct cause of wild dog mortality, are abundant.

The Serengeti is also home to a diversity of grazers, including African buffalo, warthogs, Grant's gazelle, eland, waterbuck, and topi. The Serengeti can support this remarkable variety of grazers only because each species, even those that are closely related, has a different diet. For example, wildebeests prefer to consume shorter grasses, while zebras prefer taller ones. Similarly, dik-diks eat the lowest leaves of a tree, impalas eat the leaves that are higher up, and giraffes eat leaves that are even higher.The governments of Tanzania and Kenya maintain a number of protected areas, including national parks, conservation areas, and game reserves, that give legal protection to over 80 percent of the Serengeti.

The southeastern area lies in the rain shadow of the Ngorongoro Conservation Area's highlands and is composed of shortgrass treeless plains with abundant small dicots. Soils are high in nutrients, overlying a shallow calcareous hardpan due to natrocarbonatite eruptions from Ol Doinyo Lengai. A gradient of soil depth northwestward across the plains results in changes in the herbaceous community and taller grass. About west, acacia woodlands appear suddenly and stretch west to Lake Victoria and north to the Loita Plains, north of the Maasai Mara National Reserve. The sixteen acacia species vary over this range, their distribution determined by edaphic conditions. Near Lake Victoria, floodplains have developed from ancient lakebeds.

In the far northwest, acacia woodlands are replaced by broadleaved Terminalia-Combretum woodlands, caused by a change in geology. This area has the highest rainfall in the system and forms a refuge for the migrating ungulates at the end of the dry season.Altitudes in the Serengeti range from with mean temperatures varying from . Although the climate is usually warm and dry, rainfall occurs in two rainy seasons: March to May, and a shorter season in October and November. Rainfall amounts vary from a low of in the lee of the Ngorongoro highlands to a high of on the shores of Lake Victoria. The highlands, which are considerably cooler than the plains and are covered by montane forest, mark the eastern border of the basin in which the Serengeti lies.

The Serengeti plain is punctuated by granite and gneiss outcroppings known as kopjes. These outcroppings are the result of volcanic activity. Kopjes provide a microhabitat for non-plains wildlife. One kopje likely to be seen by visitors to the Serengeti is the Simba Kopje (Lion Kopje).

The area is also home to the Ngorongoro Conservation Area, which contains Ngorongoro Crater and the Olduvai Gorge, where some of the oldest hominin fossils have been found.




</doc>
<doc id="28863" url="https://en.wikipedia.org/wiki?curid=28863" title="Sea of Marmara">
Sea of Marmara

The Sea of Marmara (; ; , ), also known as the Sea of Marmora or the Marmara Sea, and in the context of classical antiquity as the Propontis, is the inland sea, entirely within the borders of Turkey, that connects the Black Sea to the Aegean Sea, thus separating Turkey's Asian and European parts. The Bosphorus strait connects it to the Black Sea and the Dardanelles strait to the Aegean Sea. The former also separates Istanbul into its Asian and European sides. The Sea of Marmara is a small sea with an area of , and dimensions . Its greatest depth is .

The sea takes its name from Marmara Island, which is rich in sources of marble, from the Greek ("mármaron"), "marble".

The sea's ancient Greek name "Propontis" derives from "pro-" (before) and "pontos" (sea), deriving from the fact that the Greeks sailed through it to reach the Black Sea, "Pontos". In Greek mythology, a storm on Propontis brought the Argonauts back to an island they had left, precipitating a battle where either Jason or Heracles killed King Cyzicus, who mistook them for his Pelasgian enemies.

The surface salinity of the sea averages about 22 parts per thousand, which is slightly greater than that of the Black Sea, but only about two-thirds that of most oceans. The water is much more saline at the sea bottom, averaging salinities of around 38 parts per thousand, similar to that of the Mediterranean Sea. This high-density saline water, like that of the Black Sea, does not migrate to the surface. Water from the Susurluk, Biga (Granicus) and Gonen Rivers also reduces the salinity of the sea, though with less influence than on the Black Sea. With little land in Thrace draining southward, almost all of these rivers flow from Anatolia.

The sea contains the archipelago of the Prince Islands and Marmara Island, Avşa and Paşalimanı.

The south coast of the sea is heavily indented, and includes the Gulf of İzmit (), the Gulf of Gemlik (), Gulf of Bandırma () and the Gulf of Erdek (). During a storm on December 29, 1999, the Russian oil tanker "Volgoneft" broke in two in the Sea of Marmara, and more than 1,500 tonnes of oil were spilled into the water.

The North Anatolian Fault, which has triggered many major earthquakes in recent years, such as the August and November 1999 earthquakes in Izmit and Düzce, respectively, runs under the sea.

The International Hydrographic Organization defines the limits of the Sea of Marmara as follows:

Towns and cities on the Marmara Sea coast include:



</doc>
<doc id="28866" url="https://en.wikipedia.org/wiki?curid=28866" title="Saint John, New Brunswick">
Saint John, New Brunswick

Saint John is a port city on the Bay of Fundy in the Canadian province of New Brunswick. The port is Canada's third largest port by tonnage with a cargo base that includes dry and liquid bulk, break bulk, containers, and cruise. Historically New Brunswick's largest city, in 2016 the city fell to second place, with a population of 67,575 over an area of . Greater Saint John covers a land area of across the Caledonia Highlands, with a growing population of 126,202 (as of 2016). Saint John is the oldest incorporated city in Canada. During the reign of George III, the municipality was created by royal charter in 1785.

Saint John is the oldest of five chartered cities in Canada along with Montreal, Winnipeg, Vancouver, and Lloydminster (a city that straddles both Alberta and Saskatchewan).

French colonist Samuel de Champlain landed at Saint John Harbour on June 24, 1604 (the feast of St. John the Baptist) and is where the Saint John River gets its name although Mi'kmaq and Maliseet peoples lived in the region for thousands of years prior calling the river Wolastoq. The Saint John area was an important area for trade and defence for Acadia during the French colonial era and Fort La Tour, in the city's harbour, was a pivotal battleground during the Acadian Civil War.. After over a century of ownership disputes over the land surrounding Saint John between the French and English, the English deported the French colonists in 1755 and constructed Fort Howe above the harbour in 1779. In 1785, the City of Saint John was established by uniting the two towns of Parrtown and Carleton on each side of the harbour after the arrival of thousands of refugees from the American Revolution who wished to remain British and were forced to leave their U.S. homes. Over the next century, waves of immigration via Partridge Island, especially during the Great Famine, would fundamentally change the city's demographics and culture.

Predated by the Maritime Archaic Indian civilization, the northwestern coastal region of the Bay of Fundy is believed to have been inhabited by the Passamaquoddy Nation several thousand years ago, while the Saint John River valley north of the bay became the domain of the Maliseet Nation. The Mi'kmaq also ventured into the territory and named the area <nowiki>"Měnagwĕs"</nowiki>, which means "where they collect the dead seals."

Samuel de Champlain landed at Saint John Harbour in 1604, though he did not settle the area. Saint John was a key area for trade and defence for Acadia during the French colonial era. Moreover, Fort La Tour in the city's harbour, was a pivotal battleground during the Acadian Civil War. The region was conquered by the British by the end of the Seven Years' War. After being incorporated as a city in 1785 with an influx of British Loyalists from the northern of the former Thirteen Colonies and immigrants from Ireland, the city grew as a global hub for shipping and shipbuilding. After the partitioning of the colony of Nova Scotia in 1784, the new colony of New Brunswick was thought to be named 'New Ireland' with the capital to be in Saint John before being vetoed by George III. In 1851 the city cemented itself as a global shipbuilding hub when the , built from a Saint John yard, became the fastest in the world.

However, the city would also struggle with its success. From 1840 to 1860 sectarian violence was rampant in Saint John resulting in some of the worst urban riots in Canadian history. The city experienced a cholera outbreak in 1854 with the death of over 1,500 people, as well as a great fire in 1877 that destroyed 40% of the city and left 20,000 people homeless.


Situated in the south-central portion of the province, along the north shore of the Bay of Fundy at the mouth of the Saint John River, the city is split by the south-flowing river and the east side is bordered on the north by the Kennebecasis River where it meets the Saint John River at Grand Bay. Saint John Harbour, where the two rivers meet the Bay of Fundy, is a deep water port and ice-free all year long. Partridge Island is in the harbour.

Stonehammer UNESCO Geopark, the first Geopark in North America, is centred around Saint John. The Geopark has been recognized by UNESCO as having exceptional geological significance. The park contains rock formations throughout the Saint John region ranging billions of years.

The Saint John River itself flows into the Bay of Fundy through a narrow gorge several hundred feet wide at the centre of the city. It contains a unique phenomenon called the Reversing Falls where the diurnal tides of the bay reverse the water flow of the river for several kilometres. A series of underwater ledges at the narrowest point of this gorge also create a series of rapids.

The topography surrounding Saint John is hilly; a result of the influence of two coastal mountain ranges which run along the Bay of Fundy – the "St. Croix Highlands" and the "Caledonia Highlands". The soil throughout the region is extremely rocky with frequent granite outcrops. The coastal plain hosts numerous freshwater lakes in the eastern, western and northern parts of the city.

In Saint John the height difference from low to high tide is approximately 8 metres (28 ft) due to the funnelling effect of the Bay of Fundy as it narrows. The Reversing Falls in Saint John, actually an area of strong rapids, provides one example of the power of these tides; at every high tide, ocean water is pushed through a narrow gorge in the middle of the city and forces the Saint John River to reverse its flow for several hours.

Saint John is a city of neighbourhoods, with residents closely identifying with their particular area.

The central peninsula on the east side of Saint John Harbour and the area immediately opposite on the west side are the sites of the original city, which resulted from the merger of Parrtown and Carleton. The western side of the central peninsula subsequently saw increased development and includes the central business district (CBD) and the Trinity Royal Heritage Conservation Area, which together are referred to as "Uptown" by residents throughout the city. The term "Uptown" came about as people at the slips would go up the hill to the city. In addition, most of the central peninsula is situated on a hill. This central area of Saint John is only rarely called "Downtown." The south end of the central peninsula, south of Duke Street, is appropriately called the South End.

The area north of the Highway #1 from the South Central Peninsula is called the North End; both areas being predominantly urban residential older housing which is undergoing gentrification. Much of the North End is made up of the former city of Portland and comprises another former working class area which is slowly undergoing gentrification at the eastern end of Douglas Avenue; immediately north of Portland and upstream from the Reversing Falls is the former community of Indiantown.

Vessels navigating the Saint John River can only transit the Reversing Falls gorge at slack tide, thus Indiantown became a location during the 19th and 20th centuries where tugboats and paddle wheelers could dock to wait. Being at the beginning of the navigable part of the Saint John River, Indiantown also became a major terminal for vessels departing to ply their trade upriver.

Further north of the central part of the city, and northeast of the North End and Portland, along the southern bank of the Kennebecasis River is the area of Millidgeville which is generally considered a neighbourhood separate from the North End. The boundary of Millidgeville is typically thought to begin at the "Y" intersection of Somerset Street and Millidge Ave or right after Tartan St. It is a middle to upper-class neighbourhood. Located here is University of New Brunswick, as well as New Brunswick's largest health care centre, the Saint John Regional Hospital, and Saint John's only completely French school and community centre, Centre Scolaire Communautaire Samuel-de-Champlain.

The eastern area of the North End plays host to the city's largest park, and one of Canada's largest urban parks. Rockwood Park encompasses of upland Acadian mixed forest, many hills and several caves, as well as several freshwater lakes, with an extensive trail network, a golf course, and the Cherry Brook Zoo. The park was designed by Calvert Vaux in the mid-to-late 19th century. Mount Pleasant borders the park, and is generally seen as distinct from the traditionally poorer North End.

To the east of the Courtney Bay / Forebay and south of New Brunswick Route 1 is the East Side, where the city has experienced its greatest suburban sprawl in recent decades with commercial retail centres and residential subdivisions. There has been significant and consistent commercial and retail development in the Westmorland Road-McAllister Drive-Consumer's Drive-Major's Brook Drive-Retail Drive corridor since the 1970s, including McAllister Place, the city's largest shopping mall, which opened in 1978, and with active year-to-year development since 1994. The city's airport is further east on the coastal plain among several lakes at the far eastern edge of the municipality. Far east side is Loch Lomond, including several urban neighbourhoods found here, including Forest Hills, Champlain Heights, and Lakewood Heights. The malls were built by filling in Major's Brook (a tributary to Marsh Creek), making the area prone to flooding.

The portion of the city west of the Saint John River is collectively referred to as West Side, although West Saint Johners divide this area into several neighbourhoods. As mentioned previously, the Lower West Side is the former working-class neighbourhood known as Carleton at the time of the city's formation in 1785. West and north of the Lower West Side is the former city of Lancaster (commonly referred to as Saint John West), which was amalgamated into Saint John in 1967. The boundary was City Line Street, with the streets east considered to be the "West Side", and the streets west of City Kine Street having been renamed from Lancaster, NB to Saint John West, NB.

The southern part of Lancaster abutting Saint John Harbour and the Bay of Fundy is Bayshore and the location of Canadian Pacific Railway's Bayshore Yard. The north end of Lancaster, known as Fairville, is home to Moosehead Brewery and older neighbourhoods clustered along Manawagonish Road. North of Fairville are the communities of Milford and Randolph. Randolph, which is home to Dominion Park Beach, includes land on the city's largest island, and is joined by the Canal Bridge over Mosquito Cove on Greenhead Road. The area also contains the Irving Pulp and Paper mill, a highly visible manufacturing plant that sits next to the Reversing Falls and is owned and operated by J. D. Irving, Ltd.

West of Lancaster, the city hosts its second largest park, and one of the largest coastal urban parks in the country. The Irving Nature Park, along Saints' Rest Beach, sits on an extensive peninsula called Taylor's Island extending into the western part of the harbour into the Bay of Fundy. The park is partially open to vehicles in summer and features ocean views and walking trails through mixed forests.

Saint John's suburbs, just on the edge of the city limit, are Rothesay, Quispamsis, and Grand Bay–Westfield. Mainly residential, the suburbs have attracted many of Saint John's residents, leading the city's population to shrink.

The climate of Saint John is humid continental (Köppen climate classification ""). The Bay of Fundy never fully freezes, thus moderating the winter temperatures compared with inland locations. Even so, with the prevailing wind blowing from the west (from land to sea), the average January temperature is about . Summers are usually warm to hot, and daytime temperatures often exceed . The highest temperature recorded in a given year is usually . The confluence of cold Bay of Fundy air and inland warmer temperatures often creates onshore winds that bring periods of fog and cooler temperatures during the summer months.

Precipitation in Saint John totals about annually and is well distributed throughout the year, although the late autumn and early winter are typically the wettest time of year. Snowfalls can often be heavy, but rain is as common as snow in winter, and it is not unusual for the ground to be snow-free even in mid-winter.

The highest temperature ever recorded in Saint John was on June 22, 1941, August 15, 1944, and August 22, 1976. The coldest temperature ever recorded was on February 11, 1948.


There are 13 National Historic Sites of Canada in Saint John.

The population of the city declined from the 1970s to the early 21st century. This trend reversed itself and the city's population increased in the 2011 census, but then declined again by 2016. Saint John was New Brunswick's largest city until 2016.

In 2011, the population of the Greater Saint John area was 127,761, of whom 49% were male and 51% female. Children under fifteen accounted for approximately 16% of the population. People 65 and over accounted for approximately 15% of the population. When the census was taken in May 2011 the population of the Saint John metropolitan area was 127,761 compared with 122,389 in 2006.

Historically, as one of Canada's main ports, Saint John has been a centre for immigration from all over the world. The city was incorporated in the late 1700s after more than 3,300 Black Loyalist refugees came to Saint John along with more than 10,000 white refugees after the American Revolution. In the years between 1815 and 1867, when immigration of that era passed its peak, more than 150,000 immigrants from Ireland came to Saint John dramatically changing the city. Those who came in the earlier period were largely tradesmen, and many stayed in Saint John, becoming the backbone of its builders. But when the Great Irish Potato Famine raged between 1845 and 1852, huge waves of famine refugees flooded the city's shores. It is estimated that between 1845 and 1847, some 30,000 arrived, more people than were living in the city at the time. In 1847, dubbed "Black 47", one of the worst years of the famine, some 16,000 immigrants, most of them from Ireland, arrived at Partridge Island, the immigration and quarantine station at the mouth of Saint John Harbour.

By 1850, the Irish Catholic community constituted Saint John's largest ethnic group. In the census of 1851, over half the heads of households in the city registered themselves as natives of Ireland. By 1871, 55 per cent of Saint John's residents were Irish natives or children of Irish-born mothers or fathers. There were violent confrontations between the Catholic and the Protestant ("Orange") Irish especially in the 1840s.

As of the 2016 census, approximately 87.7% of the residents were white, while 7% were visible minorities and 5.3% were aboriginal. The largest visible minority groups were Black (2.1%), Chinese (1.4%), Arab (0.9%), and South Asian (0.7%).

With regard to religion, 89.2% identify as Christian (47.6% Protestant, 40.3% Roman Catholic, and 1.3% other Christian, mostly Orthodox and independent churches). 10.1% state no religious affiliation, and other religions including Islam, Judaism, Buddhism, and Hinduism together comprise less than 1%.

While New Brunswick is a bilingual province, the Greater Saint John area is overwhelmingly anglophone: of its 127,761 residents in 2011, only 5,520 (4.3%) were native French speakers, a much lower percentage than for the province as a whole (31.9%).

Saint John is one of five chartered cities in Canada, giving it unique legislative powers.

Saint John is governed by a body of elected officials, referred to as "Common Council", whose responsibilities include:

The Common Council consists of:
One is elected by the council to serve as Deputy Mayor.

As of 2017, the Council's members are:

In the October 9, 2007 Plebiscite, it was decided that as of the May 2008 quadrennial municipal elections, the city will be divided into four wards of approximately equal population, with two councillors to be elected by the voters in that ward, and two councillors to be elected at large.

Saint John derived its economy from maritime industries such as shipping, fishing and shipbuilding. Saint John has a long history of shipbuilding at the city's dry dock, which is one of the largest in the world. Since 2003 shipbuilding has ended on the scale it once was, forcing the city to adopt a new economic strategy. The University of New Brunswick, the New Brunswick Museum and the New Brunswick Community College are important institutions, and along with Radian6, Horizon Health Network and many others, they are a part of Saint John's fast-growing research and information-technology sectors. As the city moves away from its industrial past it now begins to capitalize on the growing sector of tourism, hosting over 1.5 million visitors a year and 200,000 cruise ship visitors a year, creating a renaissance in the city's historic downtown (locally known as uptown). Many small businesses have moved into Uptown and large scale waterfront developments are underway, such as the Fundy Quay (condo, hotel and office space), Saint John Law Courts, and the Three Sisters Harbourfront condos.

The arts and culture sector plays a large role in Saint John's economy. The Imperial Theatre is home to the highly acclaimed Saint John Theatre Company, and the Symphony New Brunswick and hosts a large collection of plays, concerts and other stage productions year-round. Harbour Station entertainment complex is home to the Saint John Sea Dogs of the QMJHL and the Saint John Riptide of the NBL.

Art galleries in Saint John cover the uptown, more than any other Atlantic Canadian city. Artists like Miller Brittain and Fred Ross have made Uptown Saint John their home, and now the torch has been passed to artists like Gerard Collins, Cliff Turner and Peter Salmon and their respective galleries. Uptown art galleries also include the Trinity Galleries, Citadel Gallery, Handworks Gallery and the Saint John Arts Centre (SJAC). The SJAC in the Carnegie Building hosts art exhibits, workshops, local songwriters' circles and other shows too small to be featured at the grand Imperial Theatre.

Saint John maintains industrial infrastructure in the city's East side such as Canada's largest oil refinery. Capitalist K.C. Irving and his family built his unfettered industrial conglomerate in the city by buying up mills, shipyards, media outlets, and other industrial infrastructure during the 20th century, and still continue to this day. Today Irving dominates the city and province with stakes in oil, forestry, shipbuilding, media and transportation. Irving companies remain dominant employers in the region with North America's first deepwater oil terminal, a pulp mill, a paper mill and a tissue paper plant.

Other important economic activity in the city is generated by the Port of Saint John.

Saint John has a long history of brewers, such as Simeon Jones, The Olands, and James Ready. The city is now home to Moosehead Breweries, James Ready Brewing Co., Big Tide Brewing Co., Picaroon's and other craft brewers. The Moosehead Brewery (established in 1867, is Canada's only nationally distributed independent brewery [M. Nicholson]), James Ready Brewing Co., the New Brunswick Power Corporation which operates three electrical generating stations in the region including the Point Lepreau Nuclear Generating Station, Bell Aliant which operates out of the former New Brunswick Telephone headquarters, the Horizon Health Network, which operates 5 hospitals in the Saint John area, and numerous information technology companies. There are also a number of call centres which were established in the 1990s under provincial government incentives.

Until the first decade of the 21st century, Canada's largest shipyard (Irving Shipbuilding) had been an important employer in the city. During the 1980s-early 1990s the shipyard was responsible for building 9 of the 12 multi-purpose patrol frigates for the Canadian Navy. However, the Irvings closed the shipyard in 2003 and centralized in Halifax.

Prior to the opening of the St. Lawrence Seaway in 1959, the Port of Saint John functioned as the winter port for Montreal, Quebec when shipping was unable to traverse the sea ice in the Gulf of St. Lawrence and St. Lawrence River. The Canadian Pacific Railway opened a line to Saint John from Montreal in 1889 across the state of Maine and transferred the majority of its trans-Atlantic passenger and cargo shipping to the port during the winter months. The port fell into decline following the seaway opening and the start of year-round icebreaker services in the 1960s. In 1994 CPR left Saint John when it sold the line to shortline operator New Brunswick Southern Railway. The Canadian National Railway still services Saint John with a secondary mainline from Moncton. Despite these setbacks, Port Saint John is the largest port by volume in Eastern Canada, at about 28 million metric tonnes of cargo per year, including containers and bulk cargo.

Besides being the location of several historical forts, such as Fort Howe, Fort Dufferin, Fort Latour, and the Carleton Martello Tower, Saint John is the location of a number of reserve units of the Canadian Forces.


Saint John is often described as the birthplace of unionism in Canada and is one of the few pre-capitalist colonial settlements in North America. The city has a history of labour achievements and sparked the Canadian labour movement with Canada's first trade union, the Labourers' Benevolent Association (now International Longshoremen's Association Local 273). In 1849 the union was formed when Saint John's longshoremen banded together to lobby for regular pay and a shorter workday. One of their first resolutions was to apply to the city council for permission to erect the bell, which would announce the beginning and end of the labourers' 10-hour workday. As the bell shears were hardly finished when capitalists and merchants in the city objected to the bell and successfully lobbied city hall to keep the bell from being put up. But then, citizens and longshoremen defied the order and erected a larger bell and merchants withdrew their opposition to the "Labourers' Bell". ILA Local 273 remain one of the city's strongest trade unions to this day.

The 1914 Saint John street railway strike (sometimes called the "Saint John street railwaymen's strike") was a strike by workers on the street railway system in the city which lasted from July 22–24, 1914, with rioting by Saint John inhabitants occurring on July 23 and 24. The strike was important for shattering the image of Saint John as a conservative town dominated primarily by ethnic and religious (rather than class) divisions, and highlighting tensions between railway industrialists and the local working population.

The 1949 Canadian Seamen's Union (CSU) strike for many shows a striking bit of history in Canadian labour and is a story about anti-union shipping companies who had a clear disregard for the law. The companies demanded the removal from the contract of the hiring hall. This concession was totally unacceptable to the union as it would mean the end of the CSU. When the union discovered the shipping companies were signing back-door agreements with the Seafarer's International Union (SIU) they had implemented strike action. With SIU crews operating the ships and the longshoremen handling the cargo, the CSU strike in Saint John was, for all intents and purposes, over. The union had been destroyed by a corrupt American union, led by gangster Hal Banks, who was supported by the American labour movement, the Canadian government and the Shipping Federation of Canada. The strike was now ineffective in Saint John, but the vibrations from the strike would be felt in the city for many years.

The Saint John General Strike of 1976 was a result of the Bill C-73 passed by Prime Minister of Canada, Pierre Elliott Trudeau, and the House of Commons in Ottawa on October 14, 1975. This bill limited wage increases to 8% the first year, 6% the second year, and 4% the third year after its enactment. Most provinces of Canada accepted the bill by spring of 1976, but within eighteen months they began to withdraw from the program. After its introduction in 1975, it was not until 1976 that the Anti-Inflation Board (AIB) began to roll back workers' wages. The employees of Irving Pulp and Paper, members of the Canadian Paper Workers Union, were among the first to experience the roll backs implemented by the AIB. The paper workers were required to give back to the employer 9.8% of their previous wage increase the first year, and 11% the second year. The Atlantic Sugar Refinery workers of the Bakery and Confectionary Workers International Union of America soon felt the burden as well. The majority of workers within Saint John were influenced by the AIB by January 1976. On February 5, 1976, the Saint John District and the Labour Council held a conference to plan an organized opposition to the AIB. Fifty-two people came to the meeting as representatives of twenty-six unions in Saint John. The council was led by the Labour Council president, George Vair. They began by educating those present on wage control legislation, but swiftly transitioned into rallying and demonstrating in opposition throughout the city. Five thousand marched from numerous ends of the town to King Square. All major industries in Saint John were shut down.

On May 12, 1994, at 4:30 pm, members of Local 691 of the Communications, Energy and Paperworkers (CEP) union at the Irving Oil Ltd. Refinery went on strike. At this time the refinery's management took over its operations. Irving had argued the refinery might have to shut down and had to bring in a bevy of rollbacks to the workers’ pay and benefits and other changes to the collective agreement. Local 691 argued Irving simply wished to lengthen the work week without paying workers overtime rates. The strike lasted 27 months and was based on Irving’s demands for flexibility of the workers to ensure the refinery was competitive. The strike is seen as symbolic of a rollback of labour and democratic collective bargaining rights that have been in decline across North America.

Air service into Saint John is provided by the Saint John Airport/Aéroport de Saint-Jean, near Loch Lomond east northeast of the central business district or approximately by road northeast of the city centre. Flights are offered by Sunwing Airlines (seasonal) and Air Canada (Air Canada Express and Air Canada Rouge). In 2011, WestJet decided to withdraw from the Saint John Airport. Quebec-based Pascan Aviation announced its expansion into Saint John in late 2012, with direct flights from Saint John to Quebec City, Newfoundland, and other destinations beginning in September 2012. Porter Airlines flies once daily from Saint John, to Ottawa and Toronto Island Airport.

The main highway in the city is the Saint John Throughway (Route 1). Route 1 extends west to St. Stephen, and northeast towards Moncton. A second major highway, Route 7, connects Saint John with Fredericton. There are two main road crossings over the Saint John River: the Harbour Bridge and the Reversing Falls Bridge, approximately upstream.

The Reversing Falls Railway Bridge carries rail traffic for the New Brunswick Southern Railway on the route from Saint John to Maine. Saint John was serviced by the "Atlantic" Line of Via Rail passenger service. Passenger rail service in Saint John was discontinued in December 1994, although the Canadian National Railway and New Brunswick Southern Railway continue to provide freight service.

Bay Ferries operates a ferry service, , across the Bay of Fundy to Digby, Nova Scotia. The Summerville to Millidgeville Ferry, a free propeller (as opposed to cable) ferry service operated by the New Brunswick Department of Transportation, connects the Millidgeville neighbourhood with Summerville, New Brunswick, across the Kennebecasis River on the Kingston Peninsula.

Bus service is provided by Saint John Transit (Greater Saint John Area) and Maritime Bus (Inter-city). Acadian Lines used to operate regular inter-city bus services between New Brunswick, Prince Edward Island, Nova Scotia, Bangor, as well as Rivière-du-Loup, Quebec (connecting with Orléans Express). In November 2012, Acadian Lines ceased operations.

The presence of Irish, British and French heritage is very apparent along with maritime traditions. Saint John's economy has long ties to the fisheries and shipbuilding, and is known for the "Marco Polo" as its flagship vessel. However, the city's economy has begun a transition from resource-based sectors and manufacturing to include growing IT and research sectors. The city has always been a traditional hub for the arts on the east coast, boasting many notable artists, actors and musicians, including Walter Pidgeon, Donald Sutherland, Louis B. Mayer, and Miller Brittain.

What is considered the golden age of the Saint John arts community was during the post-war era from 1940 to 1970 when the city produced renowned artists and writers such as poet Kay Smith, painters Jack Humphrey, Miller Brittain, Bruno Bobak, Fred Ross, and sculptor John Hooper and folk-singer Tom Connors. Poet Bliss Carman once wrote about Saint John, "All the beauty and mystery Of life were there, adventure bold, Youth, and the glamour of the sea, And all its sorrows old." 

Dance, music, and theatre ensembles in the city include:


Saint John has several small private art galleries, as well as concert series hosted by local churches and schools. Cultural festivals and venues include:


Museums in Saint John include, among others:

National Historic Sites of Canada in Saint John include, among others:

Early French, British, Black Loyalists, and Irish settlers influenced music in Saint John from the time the area had been a series of forts for the English and French. Working class fishers, labourers and shipbuilders carried Maritime traditions and folk songs with kitchen parties and outdoor gatherings. But musical high-culture was captured by the wealthy. New Brunswick's solicitor-general 1784–1808, Ward Chipman Sr was known to have fancy soirées at his home with all the latest songs from London. A notable Loyalist musician, Stephen Humbert, moved in 1783 from New Jersey to Saint John and opened a Sacred Vocal Music School. In 1801 Humbert published Union Harmony, the first Canadian music book in English. The Mechanics' Institute, built in 1840, was the first large-scale platform for comic opera and concerts. In 1950 The Saint John Symphony was founded by Kelsey Jones; by 1983 the organization became Symphony New Brunswick. Some musicians from Saint John include Berkley Chadwick, Stompin' Tom Connors, Stevedore Steve, Jane Coop, Bruce Holder, Frances James, the songwriter Michael F. Kelly, Ned Landry, the composer and teacher Edward Betts Manning, Paul Murray, Catherine McKinnon, Patricia Rideout, Philip Thomson, and the tenor and choir conductor Gordon Wry.

Music festivals have long been a part of the city's cultural scene. New Brunswick's Music Festival was held in Saint John every Spring in the early to mid 20th Century. As the city's music changed with the times so did its festivals. Other popular festivals included, the now defunct, Festival By The Sea, and Salty Jam catering to various genres of pop music. 

Area 506 music festival is held every New Brunswick Day long-weekend at Long Wharf on Saint John Harbour. The festival is set up with shipping containers from the port with vendors from New Brunswick companies to promote local business. A main stage area is also set up for night time shows with local acts as well as major groups. Major bands to have played Area 506 included, Tegan and Sara, Stars, Bahamas, Interpol, and Arkells. Each year the festival also includes a bevy of bands coming out of the Saint John music scene.

Quality Block Party music festival hosts independent New Brunswick musicians in smaller venues throughout uptown Saint John. The festival gets its name from the old quality block on Germain Street.

The following teams are based in Saint John:

The following sporting events have been held here:

Saint John is also home to Exhibition Park Raceway, a harness racing facility that has been hosting this form of horse racing for over the past 120 years. Prior to 1950 it was known as Moosepath Park.

In 1964, the University of New Brunswick created UNB Saint John in buildings throughout the uptown CBD. In 1968 UNBSJ opened a new campus in the city's Tucker Park neighbourhood. This campus has undergone expansion over the years and is the fastest growing component of the UNB system with many new buildings constructed from the 1970s to the first decade of the 21st century. A trend in recent years has been a growth in the number of international students. The city also hosts a New Brunswick Community College campus in the East End of the city. There has also been a satellite campus of Dalhousie Medical School added within the UNBSJ campus in 2010, instructing 30 medical students each year.

In the fall of 2007, a report commissioned by the provincial government recommended UNBSJ and the NBCC be reformed and consolidated into a new polytechnic post-secondary institute. The proposal immediately came under heavy criticism and led to the organizing of several protests in the uptown area, citing the diminishment of UNB as a nationally accredited university, the reduction in accessibility to receive degrees – and these are only a couple of the reasons why the community was enraged by the recommendation. Support for keeping UNBSJ as it was, and expanding the university under its current structure, fell slightly below 90%. Seeing too much political capital would be lost, and several Saint John MPs were likely not to support the initiative if the policies recommended by the report were legislated, the government abandoned the commission's report and created an intra-provincial post-secondary commission.

Saint John is served by two school boards: Anglophone South School District schools and Francophone Sud School District (based out of Dieppe, New Brunswick) for the city's only Francophone school, Centre-Scolaire-Communautaire Samuel-de-Champlain. Saint John is also home to Canada's oldest publicly funded school, Saint John High School. The other high schools in the city are Harbour View High School, St. Malachy's High School, and Simonds High School.





</doc>
<doc id="28867" url="https://en.wikipedia.org/wiki?curid=28867" title="Sigyn">
Sigyn

In Norse mythology, Sigyn (Old Norse "victorious girl-friend") is a goddess and is the wife of Loki. Sigyn is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In the "Poetic Edda", little information is provided about Sigyn other than her role in assisting Loki during his captivity. In the "Prose Edda", her role in helping her husband through his time spent in bondage is stated again, she appears in various kennings, and her status as a goddess is mentioned twice. Sigyn may appear on the Gosforth Cross and has been the subject of an amount of theory and cultural references.

Sigyn is attested in the following works:

In stanza 35 of the "Poetic Edda" poem "Völuspá", a völva tells Odin that, amongst many other things, she sees Sigyn sitting very unhappily with her bound husband, Loki, under a "grove of hot springs". Sigyn is mentioned a second (and final) time in the ending prose section of the poem "Lokasenna". In the prose, Loki has been bound by the gods with the guts of his son Nari, his son Váli is described as having been turned into a wolf, and the goddess Skaði fastens a venomous snake over Loki's face, from which venom drips. Sigyn, again described as Loki's wife, holds a basin under the dripping venom. The basin grows full, and she pulls it away, during which time venom drops on Loki, causing him to writhe so violently that earthquakes occur that shake the entire earth.

Sigyn appears in the books "Gylfaginning" and "Skáldskaparmál" in the "Prose Edda". In "Gylfaginning", Sigyn is introduced in chapter 31. There, she is introduced as the wife of Loki, and that they have a son by the name of "Nari or Narfi". Sigyn is mentioned again in "Gylfaginning" in chapter 50, where events are described differently than in "Lokasenna". Here, the gods have captured Loki and his two sons, who are stated as Váli, described as a son of Loki, and "Nari or Narfi", the latter earlier described as also a son of Sigyn. Váli is changed into a wolf by the gods, and rips apart his brother "Nari or Narfi". The guts of "Nari or Narfi" are then used to tie Loki to three stones, after which the guts turn to iron, and Skaði places a snake above Loki. Sigyn places herself beside him, where she holds out a bowl to catch the dripping venom. However, when the bowl becomes full she leaves to pour out the venom. As a result, Loki is again described as shaking so violently that the planet shakes, and this process repeats until he breaks free, setting Ragnarök into motion.

Sigyn is introduced as a goddess, an ásynja, in the "Prose Edda" book "Skáldskaparmál", where the gods are holding a grand feast for the visiting Ægir, and in kennings for Loki: "husband of Sigyn", "cargo [Loki] of incantation-fetter's [Sigyn's] arms", and in a passage quoted from the 9th-century "Haustlöng", "the burden of Sigyn's arms". The final mention of Sigyn in "Skáldskaparmál" is in the list of ásynjur in the appended Nafnaþulur section, chapter 75.

The mid-11th century Gosforth Cross located in Cumbria, England, has been interpreted as featuring various figures from Norse mythology. The bottom portion of the west side of the cross features a depiction of a long-haired female, kneeling figure holding an object above another prostrate, bound figure. Above and to their left is a knotted serpent. This has been interpreted as Sigyn soothing the bound Loki.

While the name "Sigyn" is found as a female personal name in Old Norse sources (Old Norse "sigr" meaning 'victory' and "vina" meaning 'girl-friend'), and though in surviving sources she is largely restricted to a single role, she appears in the 9th century skaldic poem "Haustlöng" from pagan times, written by the skald Þjóðólfr of Hvinir. Due to this early connection with Loki, Sigyn has been theorized as being a goddess dating back to an older form of Germanic paganism.

The scene of Sigyn Loki has been depicted on a number of paintings, including "Loke och Sigyn" (1850) by Nils Blommér, "Loke och Sigyn" (1863) by Mårten Eskil Winge, "Loki och Sigyn (1879) by Oscar Wergeland, and the illustration "Loki und Sigyn; Hel mit dem Hunde Garm" (1883) by K. Ehrenberg. Various objects and places have been named after Sigyn in modern times, including the Norwegian stiff-straw winter wheat varieties "Sigyn I" and "Sigyn II", a Marvel Comics character (1978) of the same name, the Swedish vessel MS Sigyn, which transports spent nuclear fuel in an allusion to Sigyn holding a bowl beneath the venom to spare Loki, and the antarctic Sigyn Glacier.



</doc>
<doc id="28868" url="https://en.wikipedia.org/wiki?curid=28868" title="Saudi Arabian–Iraqi neutral zone">
Saudi Arabian–Iraqi neutral zone

The Saudi–Iraqi neutral zone was an area of on the border between Saudi Arabia and Iraq within which the border between the two countries had not been settled. The neutral zone came into existence following the Uqair Protocol of 1922 which defined the border between Iraq and the Sultanate of Nejd (Saudi Arabia's predecessor state). The neutral zone ended on 26 December 1981, when Iraq and Saudi Arabia agreed on the partition of the zone, but this wasn't filed with the United Nations until June 1991.

The Treaty of Muhammarah (Khorramshahr), 5 May 1922, forestalled the imminent conflict between the United Kingdom, which held the mandate for Iraq, and the Kingdom of Nejd, which later became Saudi Arabia (when combined with the Kingdom of Hejaz). The treaty specifically avoided defining boundaries. Following further negotiations, the Protocol of Uqair (Uqayr), 2 December 1922, defined most of the borders between them and created the neutral zone.

No military or permanent buildings were to be built in or near the neutral zone and the nomads of both countries were to have unimpeded access to its pastures and wells.

Administrative division of the zone was achieved in 1975, and a border treaty concluded in 1981. For unknown reasons, the treaty was not filed with the United Nations and nobody outside Iraq and Saudi Arabia was notified of the change or shown maps with details of the new boundary. As the Persian Gulf War approached in early 1991, Iraq cancelled all international agreements with Saudi Arabia since 1968. Saudi Arabia responded by registering all previous boundary agreements negotiated with Iraq at the United Nations in June 1991. That ended the legal existence of the Saudi Arabian–Iraqi neutral zone. Iraqi dictator Saddam Hussein was toppled in 2003.

Most official maps no longer show the diamond-shaped neutral zone but rather draw the border line approximately through the centre of the territory. For example, the United States Office of the Geographer regarded the area as having only an approximate boundary rather than a precise one.

The Saudi Arabian–Iraqi neutral zone formerly had the ISO 3166-1 codes NT and NTZ. The codes were discontinued in 1993. The FIPS 10-4 code for the Saudi Arabian–Iraqi neutral zone was IY; that code was deleted in 1992.




</doc>
<doc id="28869" url="https://en.wikipedia.org/wiki?curid=28869" title="Solidarity (Polish trade union)">
Solidarity (Polish trade union)

Solidarity (, ; full name: Independent Self-governing Trade Union "Solidarity"—"Niezależny Samorządny Związek Zawodowy "Solidarność"" ) is a trade union founded in August-September 1980 at the Lenin Shipyard in Gdańsk, Poland. Subsequently, it was the first independent union in a Warsaw Pact country to be recognised by the state. The union's membership peaked at 10 million in September 1981, representing one-third of the country's working-age population. Solidarity's leader, Lech Wałęsa was awarded the Nobel Peace Prize in 1983 and the union is widely recognised as having played a central role in the end of communist rule in Poland. 

In the 1980s, Solidarity was a broad anti-bureaucratic social movement, using methods of civil resistance to advance the causes of workers' rights and social change. Government attempts in the early 1980s to destroy the union through the imposition of martial law and the use of political repression failed. Operating underground, with significant financial support from the Vatican and the United States, estimated to be as much as US$50 million, the union survived and by the latter 1980s had entered into negotiations with the government. 

The 1989 round table talks between the government and the Solidarity-led opposition produced agreement for the 1989 legislative elections, the country's first pluralistic election since 1947. By the end of August, a Solidarity-led coalition government was formed and in December 1990, Wałęsa was elected President of Poland.

Following Poland's transition to liberal capitalism in the 1990s and the extensive privatization of state assets, Solidarity's membership and influence declined significantly; by 2010, 30 years after being founded, the union had lost more than 90% of its original membership.

In the 1970s Poland's government raised food prices while wages were stagnant. This and other stresses led to protests in 1976 and a subsequent government crackdown on dissent. The KOR, the ROPCIO and other groups began to form underground networks to monitor and oppose the government's behavior. Labour unions formed an important part of this network. In 1979, the Polish economy shrank for the first time since World War II, by 2 percent. Foreign debt reached around $18 billion by 1980.

Anna Walentynowicz was fired from the Gdańsk Shipyard on 7 August 1980, five months before she was due to retire, for participation in the illegal trade union. This management decision enraged the workers of the shipyard, who staged a strike action on 14 August defending Anna Walentynowicz and demanding her return. She and Alina Pienkowska transformed a strike over bread and butter issues into a solidarity strike in sympathy with strikes on other establishments.

Solidarity emerged on 31 August 1980 at the Gdańsk Shipyard when the communist government of Poland signed the agreement allowing for its existence. On 17 September 1980, over twenty Inter-factory Founding Committees of free trade unions merged at the congress into one national organization NSZZ Solidarity. It officially registered on 10 November 1980.

Lech Wałęsa and others formed a broad anti-Soviet social movement ranging from people associated with the Catholic Church to members of the anti-Soviet left. Solidarity advocated non-violence in its members' activities. In September 1981, Solidarity's first national congress elected Wałęsa as a president and adopted a republican program, the "Self-governing Republic". The government attempted to destroy the union with the martial law of 1981 and several years of repression, but in the end it had to start negotiating with the union.

Roundtable Talks between the government and Solidarity-led opposition led to semi-free elections in 1989. By the end of August a Solidarity-led coalition government was formed, and in December Tadeusz Mazowiecki was elected Prime Minister. Since 1989, Solidarity has become a more traditional trade union, and had relatively little impact on the political scene of Poland in the early 1990s. A political arm founded in 1996 as Solidarity Electoral Action (AWS) won the parliamentary election in 1997, but lost the following 2001 election. Currently, as a political party "Solidarity" has little influence on modern Polish politics.

In the year leading up to martial law, Reagan Administration policies supported the Solidarity movement, waging a public relations campaign to deter what the Carter administration had seen as "an imminent move by large Soviet military forces into Poland." Michael Reisman from Yale Law School named operations in Poland as one of the covert regime change actions of the CIA during the Cold War. Colonel Ryszard Kukliński, a senior officer on the Polish General Staff was secretly sending reports to CIA officer David Forden. The Central Intelligence Agency (CIA) transferred around $2 million yearly in cash to Solidarity, for a total of $10 million over five years. There were no direct links between the CIA and Solidarnosc, and all money was channeled through third parties. CIA officers were barred from meeting Solidarity leaders, and the CIA's contacts with Solidarnosc activists were weaker than those of the AFL-CIO, which raised $300,000 from its members, which were used to provide material and cash directly to Solidarity, with no control of Solidarity's use of it. The U.S. Congress authorized the National Endowment for Democracy to promote democracy, and the NED allocated $10 million to Solidarity.

When the Polish government launched martial law in December 1981, however, Solidarity was not alerted. Potential explanations for this vary; some believe that the CIA was caught off guard, while others suggest that American policy-makers viewed an internal crackdown as preferable to an "inevitable Soviet intervention." CIA support for Solidarity included money, equipment and training, which was coordinated by Special Operations. Henry Hyde, U.S. House intelligence committee member, stated that the USA provided "supplies and technical assistance in terms of clandestine newspapers, broadcasting, propaganda, money, organizational help and advice". Initial funds for covert actions by CIA were $2 million, but soon after authorization were increased and by 1985 CIA successfully infiltrated Poland.

In 2017, Solidarity backed a proposal to implement blue laws to prohibit Sunday shopping, a move supported by Polish bishops. A 2018 new Polish law banning almost all trade on Sundays has taken effect, with large supermarkets and most other retailers closed for the first time since liberal shopping laws were introduced in the 1990s. The Law and Justice party passed the legislation with the support of Prime Minister Mateusz Morawiecki.

Although Leszek Kołakowski's works were officially banned in Poland, and he lived outside the country from the late 1960s, the philosopher's ideas nonetheless exerted an influence on the Solidarity movement. Underground copies of his books and essays shaped the opinions of the Polish intellectual opposition. His 1971 essay "Theses on Hope and Hopelessness", which suggested that self-organized social groups could gradually expand the spheres of civil society in a totalitarian state, helped inspire the dissident movements of the 1970s that led to the creation of Solidarity and provided a philosophical underpinning for the movement.

Kołakowski later described Solidarity as "perhaps [the] closest to the working class revolution" that Karl Marx had predicted in the mid-1800s. Ironically, however, Solidarity featured many elements contrary to socialism as conceived by Marx: "[workers organized] against the exploiters, that is to say, the state. And this solitary example of a working class revolution (if even this may be counted) was directed against a socialist state, and carried out under the sign of the cross, with the blessing of the Pope."

The survival of Solidarity was an unprecedented event not only in Poland, a satellite state of the USSR ruled (in practice) by a one-party Communist regime, but the whole of the Eastern bloc. It meant a break in the hard-line stance of the communist Polish United Workers' Party, which had bloodily ended a 1970 protest with machine gun fire (killing over thirty and injuring over 1,000), and the broader Soviet communist regime in the Eastern Bloc, which had quelled both the 1956 Hungarian Uprising and the 1968 Prague Spring with Soviet-led invasions.

Solidarity's influence led to the intensification and spread of anti-communist ideals and movements throughout the countries of the Eastern Bloc, weakening their communist governments. As a result of the Round Table Agreement between the Polish government and the Solidarity-led opposition, elections were held in Poland on 4 June 1989, in which the opposition were allowed to field candidates against the Communist Party—the first free elections in any Soviet bloc country. A new upper chamber (the Senate) was created in the Polish parliament and all of its 100 seats were contestable in the election, as well as one third of the seats in the more important lower chamber (the Sejm). Solidarity won 99 of the 100 Senate seats and all 161 contestable seats in the Sejm—a victory that also triggered a chain reaction across the Soviet Union's satellite states, leading to almost entirely peaceful anti-communist revolutions in Central and Eastern Europe known as the Revolutions of 1989 ("Jesień Ludów" or "Wiosna Obywatelów"), which ended in the overthrow of each Moscow-imposed regime, and ultimately to the dissolution of the Soviet Union in the early 1990s.

Given the union's support from many western governments, relations with trade unions in capitalist countries could be complicated. For example, during the UK miners' strike of 1984–85, Wałęsa said that "The miners should fight, but with common sense—not with destruction" and said of Margaret Thatcher "With such a wise and brave woman, Britain will find a solution to the strike." However, David Jastrzębski, the president of Upper Silesia Solidarity, voiced his support of the striking miners: "Neither the British government's mounted police charges nor its truncheon blows, any more than the Polish junta's tanks or rifle fire, can break our common will to struggle for a better future for the working class." This was despite the fact that Arthur Scargill, president of the British National Union of Mineworkers had been highly critical of Solidarity, condemning it as an "anti-socialist organization which desires the overthrow of a socialist state".

In 2005, the trade union Solidarity – The Union for British Workers was created in honour of the original Polish union by the far-right British National Party.

In late 2008, several democratic opposition groups in the Russian Federation formed a Solidarity movement.

In the United States, the American Solidarity Party (formerly the Christian Democratic Party USA), a Christian democratic political party, attributes its namesake to Solidarity.

In a 2011 essay "The Jacobin Spirit" in the American magazine Jacobin, philosopher Slavoj Zizek called Solidarnosc' one of the "free spaces at a distance from state power" that used "defensive violence" to protect itself from state control. The notion of "defensive violence" runs in the vein of ideas postulated by Alain Badiou.

The union was officially founded on 17 September 1980, the union's supreme powers were vested in a legislative body, the "Convention of Delegates" ("Zjazd Delegatów"). The executive branch was the National Coordinating Commission ("Krajowa Komisja Porozumiewawcza"), later renamed the National Commission ("Komisja Krajowa"). The Union had a regional structure, comprising 38 regions ("region") and two districts ("okręg"). At its highest, the Union had over 10 million members, which became the largest union membership in the world. During the communist era the 38 regional delegates were arrested and jailed when martial law came into effect on 13 December 1981 under General Wojciech Jaruzelski. After a one-year prison term the high-ranking members of the union were offered one way trips to any country accepting them (including Canada, the United States, and nations in the Middle East).

Solidarity was organized as an industrial union, or more specifically according to the One Big Union principle, along the lines of the Industrial Workers of the World and the Spanish Confederación Nacional del Trabajo (workers in every trade were organized by region, rather than by craft).

In 2010, Solidarity had more than 400,000 members. National Commission of Independent Self-Governing Trade Union is located in Gdańsk and is composed of Delegates from Regional General Congresses.

Solidarity is divided into 37 regions, and the territorial structure to a large degree reflects the shape of Polish voivodeships, established in 1975 and annulled in 1998 (see: Administrative division of People's Republic of Poland). The regions are:

The network of Solidarity branches of the key factories of Poland was created on 14 April 1981 in Gdańsk. It was made of representatives of seventeen factories; each stood for the most important factory of every voivodeship of the pre-1975 Poland (see: Administrative division of People's Republic of Poland). However, there were two exceptions. There was no representative of the Koszalin Voivodeship, and the Katowice Voivodeship was represented by two factories:





</doc>
<doc id="28876" url="https://en.wikipedia.org/wiki?curid=28876" title="Surtsey">
Surtsey

Surtsey ("Surtr's island" in Icelandic, ) is a volcanic island located in the Vestmannaeyjar archipelago off the southern coast of Iceland. At , Surtsey is the southernmost point of Iceland. It was formed in a volcanic eruption which began below sea level, and reached the surface on 14 November 1963. The eruption lasted until 5 June 1967, when the island reached its maximum size of . Since then, wave erosion has caused the island to steadily diminish in size: , its surface area was . The most recent survey (2007) shows the island's maximum elevation at above sea level.

The new island was named after "Surtr", a fire "jötunn" or giant from Norse mythology. It was intensively studied by volcanologists during its eruption, and afterwards by botanists and other biologists as life forms gradually colonised the originally barren island. The undersea vents that produced Surtsey are part of the "Vestmannaeyjar" submarine volcanic system, part of the fissure of the sea floor called the Mid-Atlantic Ridge. Vestmannaeyjar also produced the famous eruption of "Eldfell" on the island of Heimaey in 1973. The eruption that created Surtsey also created a few other small islands along this volcanic chain, such as "Jólnir" and other unnamed peaks. Most of these eroded away fairly quickly. It is estimated that Surtsey will remain above sea level for another 100 years.

The eruption was unexpected, and almost certainly began some days before it became apparent at the surface. The sea floor at the eruption site is below sea level, and at this depth volcanic emissions and explosions would be suppressed, quenched and dissipated by the water pressure and density. Gradually, as repeated flows built up a mound of material that approached sea level, the explosions could no longer be contained, and activity broke the surface.

The first noticeable indications of volcanic activity were recorded at the seismic station in Kirkjubæjarklaustur, Iceland from 6 to 8 November, which detected weak tremors emanating from an epicentre approximately west-south-west at a distance of , the location of Surtsey. Another station in Reykjavík recorded even weaker tremors for ten hours on 12 November at an undetermined location, when seismic activity ceased until 21 November. That same day, people in the coastal town of Vík away noticed a smell of hydrogen sulphide. On 13 November, a fishing vessel in search of herring, equipped with sensitive thermometers, noted sea temperatures SW of the eruption center were higher than surrounding waters.

At 07:15 UTC on 14 November 1963, the cook of "Ísleifur II", a trawler sailing these same waters, spotted a rising column of dark smoke southwest of the boat. The captain thought it might have been a boat on fire, and ordered his crew to investigate. Instead, they encountered explosive eruptions giving off black columns of ash, indicating that a volcanic eruption had begun to penetrate the surface of the sea. By 11:00 the same day, the eruption column had reached several kilometres in height. At first the eruptions took place at three separate vents along a northeast by southwest trending fissure, but by the afternoon the separate eruption columns had merged into one along the erupting fissure. Over the next week, explosions were continuous, and after just a few days the new island, formed mainly of scoria, measured over in length and had reached a height of .

As the eruptions continued, they became concentrated at one vent along the fissure and began to build the island into a more circular shape. By 24 November, the island measured about . The violent explosions caused by the meeting of lava and sea water meant that the island consisted of a loose pile of volcanic rock (scoria), which was eroded rapidly by North Atlantic storms during the winter. However, eruptions more than kept pace with wave erosion, and by February 1964, the island had a maximum diameter of over .

The explosive phreatomagmatic eruptions caused by the easy access of water to the erupting vents threw rocks up to a kilometre (0.6 mi) away from the island, and sent ash clouds as high as up into the atmosphere. The loose pile of unconsolidated tephra would quickly have been washed away had the supply of fresh magma dwindled, and large clouds of dust were often seen blowing away from the island during this stage of the eruption.

The new island was named after the fire jötunn Surtur from Norse mythology ("Surts" is the genitive case of "Surtur", plus -ey, "island"). Three French journalists representing the magazine "Paris Match" notably landed there on 6 December 1963, staying for about 15 minutes before violent explosions encouraged them to leave. The journalists jokingly claimed French sovereignty over the island, but Iceland quickly asserted that the new island belonged to it.

By early 1964, though, the continuing eruptions had built the island to such a size that sea water could no longer easily reach the vents, and the volcanic activity became much less explosive. Instead, lava fountains and flows became the main form of activity. These resulted in a hard cap of extremely erosion-resistant rock being laid down on top of much of the loose volcanic pile, which prevented the island from being washed away rapidly. Effusive eruptions continued until 1965, by which time the island had a surface area of .

On 28 December 1963, submarine activity to the northeast of Surtsey caused the formation of a ridge high on the sea floor. This seamount was named Surtla, but never reached sea level. Eruptions at Surtla ended on 6 January 1964, and it has since been eroded from its minimum depth of below sea level.

In 1965, the activity on the main island diminished, but at the end of May that year an eruption began at a vent off the northern shore. By 28 May, an island had appeared, and was named Syrtlingur (Little Surtsey). The new island was washed away during early June, but reappeared on 14 June. Eruptions at Syrtlingur were much smaller in scale than those that had built Surtsey, with the average rate of emission of volcanic materials being about a tenth of the rate at the main vent. Activity was short-lived, continuing until the beginning of October 1965, by which time the islet had an area of . Once the eruptions had ceased, wave erosion rapidly wore the island away, and it disappeared beneath the waves on 24 October.

During December 1965, more submarine activity occurred southwest of Surtsey, and another island was formed. It was named Jólnir, and over the following eight months it appeared and disappeared several times, as wave erosion and volcanic activity alternated in dominance. Activity at Jólnir was much weaker than the activity at the main vent, and even weaker than that seen at Syrtlingur, but the island eventually grew to a maximum size of in height, covering an area of , during July and early August 1966. Like Syrtlingur, though, after activity ceased on 8 August 1966, it was rapidly eroded, and dropped below sea level during October 1966.

Effusive eruptions on the main island returned on 19 August 1966, with fresh lava flows giving it further resistance to erosion. The eruption rate diminished steadily, though, and on 5 June 1967, the eruption ended. The volcano has been dormant ever since. The total volume of lava emitted during the three-and-a-half-year eruption was about one cubic kilometre (0.24 cu mi), and the island's highest point was above sea level at that time.

Since the end of the eruption, erosion has seen the island diminish in size. A large area on the southeast side has been eroded away completely, while a sand spit called "Norðurtangi" (north point) has grown on the north side of the island. It is estimated that about of material has been lost due to erosion—this represents about a quarter of the original above-sea-level volume of the island. Its maximum elevation has diminished to .

Following the end of the eruption, scientists established a grid of benchmarks against which they measured the change in the shape of the island. In the 20 years following the end of the eruption, measurements revealed that the island was steadily subsiding and had lost about one metre in height. The rate of subsidence was initially about per year but slowed to a year by the 1990s. It had several causes: settling of the loose tephra forming the bulk of the volcano, compaction of sea floor sediments underlying the island, and downward warping of the lithosphere due to the weight of the volcano.

Volcanoes in the Vestmannaeyjar archipelago are typically monogenetic, and so the island is unlikely to be enlarged in the future by further eruptions. The heavy seas around the island have been eroding it ever since the island appeared, and since the end of the eruption almost half of its original area has been lost. The island currently loses about of its surface area each year.

This island is unlikely to disappear entirely in the near future. The eroded area consisted mostly of loose tephra, easily washed away. Most of the remaining area is capped by hard lava flows, which are much more resistant to erosion. In addition, complex chemical reactions within the loose tephra within the island have gradually formed highly erosion resistant tuff material, in a process known as palagonitization. On Surtsey this process has happened quite rapidly, due to high temperatures not far below the surface.

Estimates of how long Surtsey will survive are based on the rate of erosion seen up to the present day. Assuming that the current rate does not change, the island will be mostly at or below sea level by 2100. However, the rate of erosion is likely to slow as the tougher core of the island is exposed: an assessment assuming that the rate of erosion will slow exponentially suggests that the island will survive for many centuries. An idea of what it will look like in the future is given by the other small islands in the Vestmannaeyjar archipelago, which formed in the same way as Surtsey several thousand years ago, and have eroded away substantially since they were formed.

A classic site for the study of biocolonisation from founder populations that arrive from outside ("allochthonous"), Surtsey was declared a nature reserve in 1965, while the eruption was still in active progress. Today only a few scientists are permitted to land on Surtsey; the only way anyone else can see it closely is from a small plane. This allows the natural ecological succession for the island to proceed without outside interference. In 2008, UNESCO declared the island a World Heritage Site, in recognition of its great scientific value.

In the spring of 1965, the first vascular plant was found growing on the northern shore of Surtsey, mosses became visible in 1967, and lichens were first found on the Surtsey lava in 1970. Plant colonisation on Surtsey has been closely studied, the vascular plants in particular as they have been of far greater significance than mosses, lichens and fungi in the development of vegetation.

Mosses and lichens now cover much of the island. During the island's first 20 years, 20 species of plants were observed at one time or another, but only 10 became established in the nutrient-poor sandy soil.
As birds began nesting on the island, soil conditions improved, and more vascular plant species were able to survive. In 1998, the first bush was found on the island—a tea-leaved willow ("Salix phylicifolia"), which can grow to heights of up to . By 2008, 69 species of plant had been found on Surtsey, of which about 30 had become established. This compares to the approximately 490 species found on mainland Iceland. More species continue to arrive, at a typical rate of roughly 2–5 new species per year.

The expansion of bird life on the island has both relied on and helped to advance the spread of plant life. Birds use the plants for nesting material, but also continue to assist in the spreading of seeds, and fertilize the soil with their guano. Birds first began nesting on Surtsey three years after the eruptions ended, with fulmar and guillemot the first species to set up home. Twelve species are now regularly found on the island.

A gull colony has been present since 1984, although gulls were seen briefly on the shores of the new island only weeks after it first appeared. The gull colony has been particularly important in developing the plant life on Surtsey, and the gulls have had much more of an impact on plant colonisation than other breeding species due to their abundance. An expedition in 2004 found the first evidence of nesting Atlantic puffins, which are extremely common in the rest of the archipelago.

As well as providing a home for some species of birds, Surtsey has also been used as a stopping-off point for migrating birds, particularly those en route between Europe and Iceland. Species that have been seen briefly on the island include whooper swans, various species of geese, and common ravens. Although Surtsey lies to the west of the main migration routes to Iceland, it has become a more common stopping point as its vegetation has improved. In 2008, the 14th bird species was detected with the discovery of a common raven's nest.

According to a 30 May 2009 report, a golden plover was nesting on the island with four eggs.

Soon after the island's formation, seals were seen around the island. They soon began basking there, particularly on the northern spit, which grew as the waves eroded the island. Seals were found to be breeding on the island in 1983, and a group of up to 70 made the island their breeding spot. Grey seals are more common on the island than harbour seals, but both are now well established. The presence of seals attracts orcas, which are frequently seen in the waters around the Vestmannaeyjar archipelago and now frequent the waters around Surtsey.

On the submarine portion of the island, many marine species are found. Starfish are abundant, as are sea urchins and limpets. The rocks are covered in algae, and seaweed covers much of the submarine slopes of the volcano, with its densest cover between below sea level.

Insects arrived on Surtsey soon after its formation, and were first detected in 1964. The original arrivals were flying insects, carried to the island by winds and their own power. Some were believed to have been blown across from as far away as mainland Europe. Later insect life arrived on floating driftwood, and both live animals and carcasses washed up on the island. When a large, grass-covered tussock was washed ashore in 1974, scientists took half of it for analysis and discovered 663 land invertebrates, mostly mites and springtails, the great majority of which had survived the crossing.

The establishment of insect life provided some food for birds, and birds in turn helped many species to become established on the island. The bodies of dead birds provide sustenance for carnivorous insects, while the fertilisation of the soil and resulting promotion of plant life provides a viable habitat for herbivorous insects.

Some higher forms of land life are now colonising the soil of Surtsey. The first earthworm was found in a soil sample in 1993, probably carried over from Heimaey by a bird. However, the next year earthworms were not found. Slugs were found in 1998, and appeared to be similar to varieties found in the southern Icelandic mainland. Spiders and beetles have also become established.

The only significant human impact is a small prefabricated hut which is used by researchers while staying on the island. The hut includes a few bunk beds and a solar power source to drive an emergency radio and other key electronics. All visitors check themselves and belongings to ensure no seeds are accidentally introduced by humans to this ecosystem. It is believed that some young boys tried to introduce potatoes, which were promptly dug up once discovered. An improperly handled human defecation resulted in a tomato plant taking root which was also destroyed.
In 2009 a weather station for weather observations and a webcam were installed on Surtsey.




</doc>
<doc id="28878" url="https://en.wikipedia.org/wiki?curid=28878" title="Software Engineering Body of Knowledge">
Software Engineering Body of Knowledge

The Software Engineering Body of Knowledge (SWEBOK) is an international standard ISO/IEC TR 19759:2005 specifying a guide to the generally accepted Software Engineering Body of Knowledge.

The Guide to the Software Engineering Body of Knowledge (SWEBOK Guide) has been created through cooperation among several professional bodies and members of industry and is published by the IEEE Computer Society (IEEE). The standard can be accessed freely from the IEEE Computer Society. In late 2013, SWEBOK V3 was approved for publication and released. In 2016, the IEEE Computer Society kicked off the SWEBoK Evolution effort to develop future iterations of the body of knowledge.

The published version of SWEBOK V3 has the following 15 knowledge areas (KAs) within the field of software engineering:


It also recognized, but did not define, these related disciplines:


The 2004 edition of the SWEBOK guide defined ten knowledge areas (KAs) within the field of software engineering:

The SWEBOK also defines disciplines related to software engineering:

A similar effort to define a body of knowledge for software engineering is the "Computing Curriculum Software Engineering (CCSE)," officially named Software Engineering 2004 (SE2004). The curriculum largely overlaps with the 2004 SWEBOK V2 because the SWEBOK has been used as one of its sources; however, it is more directed towards academia. Whereas the SWEBOK Guide defines the software engineering knowledge that practitioners should have after four years of practice, SE2004 defines the knowledge that an undergraduate software engineering student should possess upon graduation (including knowledge of mathematics, general engineering principles, and other related areas). SWEBOK V3 aims to address these intersections.



</doc>
<doc id="28884" url="https://en.wikipedia.org/wiki?curid=28884" title="SOE">
SOE

SOE may refer to:






</doc>
<doc id="28887" url="https://en.wikipedia.org/wiki?curid=28887" title="Scottish National Party">
Scottish National Party

The Scottish National Party (SNP; , ) is a Scottish nationalist, social-democratic political party in Scotland. The SNP supports and campaigns for Scottish independence within the European Union. It is the third-largest political party by membership in the United Kingdom, behind the Labour Party and the Conservative Party; it is the third-largest by overall representation in the House of Commons, behind the Conservative Party and the Labour Party; and it is the largest political party in Scotland, where it has the most seats in the Scottish Parliament and 47 out of the 59 Scottish seats in the House of Commons of the Parliament of the United Kingdom. The current Scottish National Party leader, Nicola Sturgeon, has served as First Minister of Scotland since November 2014.

Founded in 1934 with the amalgamation of the National Party of Scotland and the Scottish Party, the party has had continuous parliamentary representation in Westminster since Winnie Ewing won the 1967 Hamilton by-election. With the establishment of the devolved Scottish Parliament in 1999, the SNP became the second-largest party, serving two terms as the opposition. The SNP gained power at the 2007 Scottish Parliament election, forming a minority government, before going on to win the 2011 Parliament election, after which it formed Holyrood's first majority government. It was reduced back to being a minority government at the 2016 election.

The SNP is the largest political party in Scotland in terms of both seats in the Westminster and Holyrood parliaments, and membership, reaching 125,482 members as of December 2019, 47 MPs and over 400 local councillors. The SNP is a member of the European Free Alliance (EFA). The party does not have any members of the House of Lords, as it has always maintained a position of objecting to an unelected upper house.

The SNP was formed in 1934 through the merger of the National Party of Scotland and the Scottish Party, with The Duke of Montrose and Robert Bontine Cunninghame Graham as its first, joint, president. Sir Alexander MacEwen was its first chairman. Professor Douglas Young, who was the leader of the Scottish National Party from 1942 to 1945 campaigned for the Scottish people to refuse conscription and his activities were popularly vilified as undermining the British war effort against the Axis powers. Young was imprisoned for refusing to be conscripted.

The SNP first won a parliamentary seat at the Motherwell by-election in 1945, but Robert McIntyre MP lost the seat at the general election three months later. They next won a seat in 1967, when Winnie Ewing was the surprise winner of a by-election in the previously safe Labour seat of Hamilton. This brought the SNP to national prominence, leading to the establishment of the Kilbrandon Commission.

The SNP hit a high point in the October 1974 general election, polling almost a third of all votes in Scotland and returning 11 MPs to Westminster. However, the party experienced a large drop in its support at the 1979 general election, followed by a further drop at the 1983 election. The success of the October 1974 general election was not surpassed until the 2015 general election.

In the 2007 Scottish Parliament election, the SNP emerged as the largest party with 47 of 129 seats, narrowly ousting the Scottish Labour Party with 46 seats and Alex Salmond became Scottish First Minister. The Scottish Green Party supported Salmond's election as First Minister, and his subsequent appointments of ministers, in return for early tabling of the climate change bill and the SNP nominating a Green MSP to chair a parliamentary committee.

In May 2011, the SNP won an overall majority in the Scottish Parliament with 69 seats. This was a significant feat as the additional member system used for Scottish Parliament elections was specifically designed to prevent one party from winning an outright majority.

Based on their 2011 majority, the SNP government held a referendum on Scottish independence in 2014. The "No" vote prevailed in a close-fought campaign, prompting the resignation of First Minister Alex Salmond. Forty-five percent of Scottish voters cast their ballots for independence, with the "Yes" side receiving less support than late polling predicted.

The SNP rebounded from the loss in the independence referendum at the May 2015 UK general election, led by Salmond's successor as First Minister Nicola Sturgeon. The party went from holding six seats in the House of Commons to 56, mostly at the expense of the Labour Party. All but three of the fifty-nine constituencies in the country elected an SNP candidate. BBC News described the historic result as a "Scots landslide".

At the 2016 Scottish Parliament election, the SNP lost a net total of 6 seats, losing its overall majority in the Scottish Parliament, but returning for a third consecutive term as a minority government. The party actually gained an additional 1.1% of the constituency vote from the 2011 election, but lost 2.3% of the regional list vote. On the constituency vote, the SNP gained 11 seats from Labour, but lost the Edinburgh Southern constituency to the party. The Conservatives and Liberal Democrats each gained two constituency seats from the SNP on 2011 (Aberdeenshire West and Edinburgh Central for the Conservatives and Edinburgh Western and North East Fife for the Liberal Democrats).

At the 2017 United Kingdom general election the SNP underperformed compared to polling expectations, losing 21 seats to bring their number of Westminster MPs down to 35. This was largely attributed by many, including former Deputy First Minister John Swinney, to their stance on holding a second Scottish independence referendum and saw a swing to the Unionist parties, with seats being picked up by the Conservatives, Labour and the Liberal Democrats and a reduction in their majorities in the other seats. Stephen Gethins, MP for North East Fife, came out of this election with a majority of just 2 to the Liberal Democrat candidate. High-profile losses included SNP Commons leader Angus Robertson in Moray and former party leader and First Minister Alex Salmond in Gordon. However, the SNP still held the majority of the country's Westminster parliamentary seats, with a majority of 11.

The SNP achieved its best ever European Parliament result in the final election before Brexit, the party taking its MEP total to 3 or half of Scottish seats. Later that year the SNP experienced a surge in the 2019 general election, winning 45.0% of the vote. Although the party suffered a loss to the Liberal Democrats, it gained the seat of its then UK leader Jo Swinson, along with 7 from the Conservatives and 6 from Labour. Overall the party finished with 48 out of 59, or 81% of Scotland's Westminster seats.

The local Branches are the primary level of organisation in the SNP. All of the Branches within each Scottish Parliament constituency form a Constituency Association, which coordinates the work of the Branches within the constituency, coordinates the activities of the party in the constituency, and acts as a point of liaison between an MSP or MP and the party. Constituency Associations are composed of delegates from all of the Branches within the constituency.

The annual National Conference is the supreme governing body of the SNP, and is responsible for determining party policy and electing the National Executive Committee. The National Conference is composed of:

The National Council serves as the SNP's governing body between National Conferences, and its decisions are binding, unless rescinded or modified by the National Conference. There are also regular meetings of the National Assembly, which provides a forum for detailed discussion of party policy by party members.

The party has an active youth wing, the Young Scots for Independence, as well as a student wing, the Federation of Student Nationalists. There is also an SNP Trade Union Group. There is an independently owned monthly newspaper, "The Scots Independent", which is highly supportive of the party.

The SNP's leadership is vested in its National Executive Committee (NEC), which is made up of the party's elected office bearers and six elected members (voted for at conference). The SNP parliamentarians (Scottish, Westminster and European) and councillors have representation on the NEC, as do the Trade Union Group, the youth wing and the student wing.

The National Executive Committee is composed of:

Since 18 September 2014 (the day of the Scottish independence referendum), party membership has more than quadrupled (from 25,642), surpassing the Liberal Democrats and Conservatives to become the second-largest political party in the United Kingdom in terms of membership. As of August 2018, the Party has 125,482 members.

The SNP retains close links with Plaid Cymru, its counterpart in Wales. MPs from both parties co-operate closely with each other and work as a single parliamentary group within the House of Commons. The SNP and Plaid Cymru were involved in joint campaigning during the 2005 general election campaign. Both the SNP and Plaid Cymru, along with Mebyon Kernow from Cornwall, are members of the European Free Alliance (EFA), a European political party comprising regionalist political parties. The EFA co-operates with the larger European Green Party to form The Greens–European Free Alliance (Greens/EFA) group in the European Parliament.

Prior to its affiliation with The Greens–European Free Alliance, the SNP had previously been allied with the European Progressive Democrats (1979–1984), Rainbow Group (1989–1994) and European Radical Alliance (1994–1999).

The Scottish National Party did not have a clear ideological position until the 1970s, when it sought to explicitly present itself as a social democratic party in terms of party policy and publicity. During the period from its foundation until the 1960s, the SNP was essentially a moderate centrist party. Debate within the party focused more on the SNP being distinct as an all-Scotland national movement, with it being neither of the left nor the right, but constituting a new politics that sought to put Scotland first.

The SNP was formed through the merger of the centre-left National Party of Scotland (NPS) and the centre-right Scottish Party. The SNP's founders were united over self-determination in principle, though not its exact nature, or the best strategic means to achieve self-government. From the mid-1940s onwards, SNP policy was radical and redistributionist in relation to land and in favour of ‘the diffusion of economic power’, including the decentralisation of industries such as coal to include the involvement of local authorities and regional planning bodies to control industrial structure and development. Party policies supported the economic and social policy status quo of the post-war welfare state.

By the 1960s, the SNP was starting to become defined ideologically, with a social democratic tradition emerging as the party grew in urban, industrial Scotland, and its membership experienced an influx of social democrats from the Labour Party, the trade unions and the Campaign for Nuclear Disarmament. The emergence of Billy Wolfe as a leading figure in the SNP also contributed to the leftwards shift. By this period, the Labour Party were also the dominant party in Scotland, in terms of electoral support and representation. Targeting Labour through emphasising left-of-centre policies and values was therefore electorally logical for the SNP, as well as tying in with the ideological preferences of many new party members. In 1961, the SNP conference expressed the party's opposition to the siting of the US Polaris submarine base at the Holy Loch. This policy was followed in 1963 by a motion opposed to nuclear weapons: a policy that has remained in place ever since. The 1964 policy document, "SNP & You", contained a clear centre-left policy platform, including commitments to full employment, government intervention in fuel, power and transport, a state bank to guide economic development, encouragement of cooperatives and credit unions, extensive building of council houses (social housing) by central and local government, pensions adjusted to cost of living, a minimum wage and an improved national health service.

The 1960s also saw the beginnings of the SNP's efforts to establish an industrial organisation and mobilise amongst trade unionists in Scotland, with the establishment of the SNP Trade Union Group, and identifying the SNP with industrial campaigns, such as the Upper-Clyde Shipbuilders Work-in and the attempt of the workers at the Scottish "Daily Express" to run as a co-operative. For the party manifestos for the two 1974 general elections, the SNP finally self-identified as a social democratic party, and proposed a range of social democratic policies. There was also an unsuccessful proposal at the 1975 party conference to rename the party as the "Scottish National Party (Social Democrats)". In the UK wide referendum on Britain’s membership of the European Economic Community (EEC) in the same year as the aforementioned attempted name change, the SNP campaigned for Britain to leave the EEC.

There were further ideological and internal struggles after 1979, with the 79 Group attempting to move the SNP further to the left, away from being what could be described a "social-democratic" party, to an expressly "socialist" party. Members of the 79 Group - including future party leader and First Minister Alex Salmond - were expelled from the party. This produced a response in the shape of the Campaign for Nationalism in Scotland from those who wanted the SNP to remain a "broad church", apart from arguments of left vs. right. The 1980s saw the SNP further define itself as a party of the political left, such as campaigning against the introduction of the poll tax in Scotland in 1989; one year before the tax was imposed on the rest of the UK.

Ideological tensions inside the SNP are further complicated by arguments between the so-called SNP gradualists and SNP fundamentalists. In essence, gradualists seek to advance Scotland to independence through further devolution, in a "step-by-step" strategy. They tend to be in the moderate left grouping, though much of the 79 Group was gradualist in approach. However, this 79 Group gradualism was as much a reaction against the fundamentalists of the day, many of whom believed the SNP should not take a clear left or right position.

The SNP's policy base is mostly in the mainstream Western European social democratic tradition. Among its policies are commitments to same-sex marriage, reducing the voting age to sixteen years, unilateral nuclear disarmament, progressive personal taxation, the eradication of poverty; the building of affordable social housing, government-subsidised higher education, the abolition of Air Passenger Duty, and a pay increase for nurses.

The SNP is against the renewal of Trident and wants to continue providing free university education in Scotland.

The SNP would like to see an independent Scotland as a member of the European Union.

It has been noted that the party contains a broader spectrum of opinion regarding economic issues than most political parties in the UK due to its status as "the only viable vehicle for Scottish independence", with the party's parliamentary group at Westminster consisting of socialists such as Tommy Sheppard and Mhairi Black as well as supporters of tax cuts like Stewart Hosie and former Conservative Tasmina Ahmed-Sheikh.

At the 2017 SNP Conference, on 10 October, Nicola Sturgeon made several commitments, including:

Sturgeon has also condemned the EU for failing to act to protect the rights of EU citizens in Catalonia, following the use of violence on the Catalan public by Spanish police while attempting to prevent the 2017 Catalan independence referendum, and condemned the later arrests of pro-independence Catalan ministers by the Spanish Government.








As of June 2018, the Cabinet of the Scottish Government is as follows:
As of January 2020, the Shadow Cabinet of the SNP in Westminster was as follows.

The SNP had 431 councillors in Local Government elected from the 2017 Scottish local elections.

The viewpoint within the Scottish National Party is the idea that Scottish independence can be won by the accumulation by the Scottish Parliament of powers that the UK Parliament currently has over a protracted period of time. It is also a philosophy that emphasises the election of an SNP government should bring about trust in the Scottish people in the ability of Scotland to govern herself, thus bringing increased support for independence.

Gradualism stands in opposition to the so-called fundamentalist point of view, which believes that the SNP should emphasise independence more widely in order to achieve it. The argument goes that if the SNP is unprepared to argue for its central policy then it is unlikely ever to persuade the public of its worthiness.







</doc>
<doc id="28889" url="https://en.wikipedia.org/wiki?curid=28889" title="Scotch-Irish">
Scotch-Irish

Scotch-Irish or Scots-Irish may refer to:



</doc>
<doc id="28891" url="https://en.wikipedia.org/wiki?curid=28891" title="Snowy Mountains">
Snowy Mountains

The Snowy Mountains, known informally as "The Snowies", is an IBRA subregion and the highest mountain range on the continent of mainland Australia. It contains the Australian mainland's highest mountain, Mount Kosciuszko, which reaches to a height of above sea level. The range also contains the five highest peaks on the Australian mainland (including Mount Kosciuszko), all of which are above . They are located in southern New South Wales and are part of the larger Australian Alps and Great Dividing Range. Unusual for Australia, the mountain range experiences large natural snowfalls every winter. Snow normally falls during June, July, August and early September, with the snow cover melting by late spring. The Tasmanian highlands makes up the other (major) alpine region present in Australia.

The range is host to the mountain plum-pine, a low-lying type of conifer. It is considered to be one of the centres of the Australian ski industry during the winter months, with all four snow resorts in New South Wales being located in the region.

The Alpine Way and Snowy Mountains Highway are the major roads through the Snowy Mountains region.

The mountain range is thought to have had Aboriginal occupation for 20,000 years. Large scale intertribal gatherings were held in the High Country during summer for collective feasting on the Bogong moth. This practice continued until around 1865.

The area was first explored by Europeans in 1835, and in 1840, Edmund Strzelecki ascended Mount Kosciuszko and named it after the Polish patriot. High country stockmen followed who used the Snowy Mountains for grazing during the summer months. Banjo Paterson's famous poem The Man From Snowy River recalls this era. The cattle graziers have left a legacy of mountain huts scattered across the area. Today these huts are maintained by the National Parks and Wildlife Service or volunteer organisations like the Kosciuszko Huts Association.

In the 19th century gold was mined on the high plains near Kiandra. At its height this community had a population of about 4,000 people, and ran 14 hotels. Since the last resident left in 1974, Kiandra has become a ghost town of ruins and abandoned diggings.

The Kosciuszko National Park came into existence as the National Chase Snowy Mountains on 5 December 1906. In 1944 this became the Kosciuszko State Park, and then the Kosciuszko National Park in 1967.

Recreational skiing began at Kiandra in the 1860s and experienced a boom in the 20th century following the commencement of the construction of the Snowy Mountains Hydro-Electric Scheme between 1949 and 1976 which brought many European workers to the district and opened up access to the ranges.

The discovery of gold at Kiandra (elevation ), in 1859, briefly enticed a population of thousands above the snowline and saw the introduction of recreational skiing to the Snowy Mountains around 1861. The Kiandra Goldrush was short-lived, but the township remained a service centre for recreational and survival skiing for over a century. Australia's first T-Bar was installed at Kiandra in 1957, but the ski facilities were finally shifted up the hill to Selwyn Snowfields in 1978. Steeper slopes and more reliable snows lie further to the south and in the 20th Century, the focus of recreational skiing in New South Wales shifted southward, to the Mount Kosciuszko region.

The Kosciuszko Chalet was built at Charlotte Pass in 1930, giving relatively comfortable access to Australia's highest terrain. In 1964, Australia briefly boasted the "World's Longest Chairlift", designed to carry skiers from the Thredbo Valley to Charlotte Pass, but technical difficulties soon closed the facility. At , Charlotte Pass has the highest village base elevation of any Australia ski resort and can only be accessed via over-snow transport in winter. The growing number of ski enthusiasts heading to Charlotte Pass led to the establishment of a cafe at Smiggin Holes around 1939, where horse-drawn sleighs would deliver skiers to begin the arduous ski to the Kosciusko Chalet. It was the construction of the vast Snowy Mountains Hydro-Electric Scheme from 1949 that really opened up the Snowy Mountains for large scale development of a ski industry and led to the establishment of Thredbo and Perisher as leading Australian resorts. The Construction of Guthega Dam brought skiers to the isolated Guthega district and a rope tow was installed there in 1957.

Skifields up by Kosciusko's side were also established during this period, though their existence is now little realised. The Australian Alpine Club was founded in 1950 by Charles Anton. Huts were constructed in the "Backcountry" close to Mount Kosciusko, including Kunama Hut, which opened for the 1953 season. A rope tow was installed on Mount Northcote at the site and opened in 1954. The site proved excellent for speed skiing, but the hut was destroyed in an avalanche, which also killed one person, in 1956.

Anton also recognised the potential of the Thredbo Valley for construction of a major resort and village, with good vertical terrain. Construction began in 1957. Today, Thredbo has 14 ski-lifts and possesses Australia's longest ski resort run, the 5.9 km from Karel's T-Bar to Friday Flat; Australia's greatest vertical drop of 672 m; and the highest lifted point in Australia at 2037 m.

The last establishment of a major skifield in NSW came with the development of Mount Blue Cow in the 1980s. In 1987 the Swiss designed Skitube Alpine Railway opened to deliver skiers from Bullocks Flat, on the Alpine Way, to Perisher Valley and to Blue Cow, which also opened in 1987. The operators of Blue Cow purchased Guthega in 1991, and the new combined resort later merged with Perisher-Smiggins to become the largest ski resort in the Southern Hemisphere. In 2009 Perisher had 48 lifts covering 1,245 hectares and four village base areas: Perisher Valley, Blue Cow, Smiggin Holes and Guthega.

The Snowy Mountains also feed the Murrumbidgee and Murray rivers from the Tooma River, Whites River and Yarrangobilly River. The range is perhaps best known for the Snowy Mountains Scheme, a project to dam the Snowy River, providing both water for irrigation and hydroelectricity.

The project began in 1949 employing 100,000 men, two-thirds of whom came from thirty other countries during the post-World War II years. Socially this project symbolises a period during which Australia became an ethnic "melting pot" of the twentieth century but which also changed Australia's character and increased its appreciation for a wide range of cultural diversity.

The Scheme built several temporary towns for its construction workers, several of which have become permanent: Cabramurra (the highest town in Australia); and Khancoban. Additionally, the economy of Cooma has been sustained by the Scheme. Townships at Adaminaby, Jindabyne and Talbingo were inundated by the construction of Lakes Eucumbene, Jindabyne and Talbingo. Improved vehicular access to the High Country enabled ski-resort villages to be constructed at Thredbo and Guthega in the 1950s by ex-Snowy Scheme workers who realised the potential for expansion of the Australian ski industry.

By 1974, of tunnels and of aqueducts connected the sixteen dams, seven power stations (two underground), and one pumping station. The American Society of Civil Engineers has rated the Snowy Scheme as "a world-class civil engineering project".

The principal lakes created by the scheme include: Lake Eucumbene, Blowering Dam, Talbingo Dam, Lake Jindabyne and Tantangara Dam.

The higher regions of the park experience an alpine climate which is unusual on mainland Australia. However, only the peaks of the main range are subject to consistent heavy winter snow. The climate station at Charlotte Pass recorded Australia's lowest temperature of -23.0 °C on 28 June 1994.

Part of the mountains known as Main Range contains mainland Australia's five glacial lakes. The largest of these lakes is Blue Lake, one of the headwaters of the Snowy River. The other four glacial lakes are Lake Albina, Lake Cootapatamba, Club Lake and Headley Tarn.

During the last ice age, which peaked about 20,000 years ago in the Pleistocene epoch, the highest peaks of the main range near Mount Kosciuszko experienced a climate which favoured the formation of glaciers, evidence of which can still be seen today. Cirques moraines, tarn lakes, roche moutonnées and other glacial features can all be seen in the area. Lake Cootapatamba, which was formed by an ice spilling from Mount Kosciuszko's southern flank, is the highest lake on the Australian mainland. Lake Albina, Club Lake, Blue Lake, and Hedley Tarn also have glacial origins.

There is some disagreement as to exactly how widespread Pleistocene glaciation was on the main range, and little or no evidence from earlier glacial periods exists. The 'David Moraine', a one kilometre long ridge running across Spencers Creek valley seems to indicate a larger glacier existed in this area at some time, however the glacial origin of this feature is disputed.

There is evidence of periglacial activity in the area. Solifluction appears to have created terraces on the north west flank of Mount Northcote. Frost heave is also a significant agent of soil erosion in the Kosciuszko Area.

The Snowy Mountains cover a variety of climatic regions which support several distinct ecosystems. The alpine area above the tree line is one of the most fragile and covers the smallest area. This area is a patchwork of alpine heaths, herbfields, feldmarks, bogs and fens. The windswept feldmark ecotope is endemic to the alpine region, and covers a mere . It is most vulnerable to the wandering footsteps of unmindful tourists.

Many rare or threatened plant and animal species occur within the Snowy Mountains. The Kosciuszko National Park is home to one of Australia's most threatened species (the corroboree frog). The endangered mountain pygmy possum and the more common dusky antechinus are located in the high country of the park.

By 2008, wild horse numbers in the National Park had reached 1,700 with that figure growing by 300 each year, resulting in park authorities coordinating their culling and relocation.

The high country is dominated by alpine woodlands, characterised by the snow gum. Montane and wet sclerophyll forests also occur across the ranges, supporting large stands of alpine ash and mountain gum. In the southern Byadbo wilderness area, dry sclerophyll and wattle forests predominate. Amongst the many different native trees in the park, the large Chinese elm has become naturalised.

In summer 2003, the Australian Alps experienced their largest bushfires for over 60 years with an estimated 1.73 million hectares burning. The bushfires burnt across Victoria, New South Wales (NSW) and the Australian Capital Territory (ACT) during a drought that ranks as one of the worst in 103 years of official Australian weather records. Fires are a natural feature of the park's ecosystem, but it will take some time for the region to return to its pre-2003 condition.

In November 2004, a committee "The Snowy Mountains Bushfire Recovery Taskforce" was set up by the NSW State Premier's Department to help residents in the region recover from the fires. The Taskforce commissioned Louise Darmody from Sound Memories to produce a documentary involving 26 people from the Snowy Mountains to talk about their experiences. The interviewees included farmers, school children, volunteers and employees from the NSW Rural Fire Service and National Park Snowy Hydro.




</doc>
<doc id="28892" url="https://en.wikipedia.org/wiki?curid=28892" title="Skara Brae">
Skara Brae

Skara Brae is a stone-built Neolithic settlement, located on the Bay of Skaill on the west coast of Mainland, the largest island in the Orkney archipelago of Scotland. Consisting of eight clustered houses, it was occupied from roughly 3180 BC to about 2500 BC and is Europe's most complete Neolithic village. Skara Brae gained UNESCO World Heritage Site status as one of four sites making up "The Heart of Neolithic Orkney". Older than Stonehenge and the Great Pyramids, it has been called the "Scottish Pompeii" because of its excellent preservation.

In the winter of 1850 a severe storm hit Scotland causing widespread damage and over 200 deaths. In the Bay of Skaill the storm stripped the earth from a large irregular knoll known as "Skerrabra". When the storm cleared local villagers found the outline of a village consisting of a number of small houses without roofs. William Watt of Skaill, the local laird, began an amateur excavation of the site but after four houses were uncovered the work was abandoned in 1868. The site remained undisturbed until 1913 when during a single weekend the site was plundered by a party with shovels who took away an unknown quantity of artefacts. In 1924 another storm swept away part of one of the houses and it was determined the site should be secured and properly investigated. The job was given to the University of Edinburgh’s Professor V. Gordon Childe, who travelled to Skara Brae for the first time in mid-1927.

The inhabitants of Skara Brae were makers and users of grooved ware, a distinctive style of pottery that had recently appeared in northern Scotland. The houses used earth sheltering, being sunk into the ground. They were sunk into mounds of pre-existing prehistoric domestic waste known as middens. This provided the houses with a stability and also acted as insulation against Orkney's harsh winter climate. On average, each house measures with a large square room containing a stone hearth used for heating and cooking. Given the number of homes, it seems likely that no more than fifty people lived in Skara Brae at any given time.

It is not clear what material the inhabitants burned in their hearths. Childe was sure that the fuel was peat, but a detailed analysis of vegetation patterns and trends suggests that climatic conditions conducive to the development of thick beds of peat did not develop in this part of Orkney until after Skara Brae was abandoned. Other possible fuels include driftwood and animal dung. There is evidence that dried seaweed may have been used significantly. At some sites in Orkney, investigators have found a glassy, slag-like material called "kelp" or "cramp" that may be residual burnt seaweed.

The dwellings contain a number of stone-built pieces of furniture, including cupboards, dressers, seats, and storage boxes. Each dwelling was entered through a low doorway that had a stone slab door that could be closed "by a bar that slid in bar-holes cut in the stone door jambs". A sophisticated drainage system was incorporated into the village's design. It included a primitive form of toilet in each dwelling.

Seven of the houses have similar furniture, with the beds and dresser in the same places in each house. The dresser stands against the wall opposite the door, and was the first thing seen by anyone entering the dwelling. Each of these houses had the larger bed on the right side of the doorway and the smaller on the left. Lloyd Laing noted that this pattern accorded with Hebridean custom up to the early 20th century suggesting that the husband's bed was the larger and the wife's was the smaller. The discovery of beads and paint-pots in some of the smaller beds may support this interpretation. Additional support may come from the recognition that stone boxes lie to the left of most doorways, forcing the person entering the house to turn to the right-hand, "male", side of the dwelling. At the front of each bed lie the stumps of stone pillars that may have supported a canopy of fur; another link with recent Hebridean style.
House 8 has no storage boxes or dresser and has been divided into something resembling small cubicles. Fragments of stone, bone and antler were excavated suggesting the house may have been used to make tools such as bone needles or flint axes. The presence of heat-damaged volcanic rocks and what appears to be a flue, support this interpretation. House 8 is distinctive in other ways as well: it is a stand-alone structure not surrounded by midden, instead it is above ground with walls over thick and has a "porch" protecting the entrance.

The site provided the earliest known record of the human flea ("Pulex irritans") in Europe.

The Grooved Ware People who built Skara Brae were primarily pastoralists who raised cattle and sheep. Childe originally believed that the inhabitants did not practice agriculture, but excavations in 1972 unearthed seed grains from a midden suggesting that barley was cultivated. Fish bones and shells are common in the middens indicating that dwellers ate seafood. Limpet shells are common and may have been fish-bait that was kept in stone boxes in the homes. The boxes were formed from thin slabs with joints carefully sealed with clay to render them waterproof.

This pastoral lifestyle is in sharp contrast to some of the more exotic interpretations of the culture of the Skara Brae people. Euan MacKie suggested that Skara Brae might be the home of a privileged theocratic class of wise men who engaged in astronomical and magical ceremonies at nearby Ring of Brodgar and the Standing Stones of Stenness. Graham and Anna Ritchie cast doubt on this interpretation noting that there is no archaeological evidence for this claim, although a Neolithic "low road" that goes from Skara Brae passes near both these sites and ends at the chambered tomb of Maeshowe. Low roads connect Neolithic ceremonial sites throughout Britain.
Originally, Childe believed that the settlement dated from around 500 BC. This interpretation was coming under increasing challenge by the time new excavations in 1972–73 settled the question. Radiocarbon results obtained from samples collected during these excavations indicate that occupation of Skara Brae began about 3180 BC with occupation continuing for about six hundred years. Around 2500 BC, after the climate changed, becoming much colder and wetter, the settlement may have been abandoned by its inhabitants. There are many theories as to why the people of Skara Brae left; particularly popular interpretations involve a major storm. Evan Hadingham combined evidence from found objects with the storm scenario to imagine a dramatic end to the settlement:
Anna Ritchie strongly disagrees with catastrophic interpretations of the village's abandonment:
The site was farther from the sea than it is today, and it is possible that Skara Brae was built adjacent to a fresh water lagoon protected by dunes. Although the visible buildings give an impression of an organic whole, it is certain that an unknown quantity of additional structures had already been lost to sea erosion before the site's rediscovery and subsequent protection by a seawall. Uncovered remains are known to exist immediately adjacent to the ancient monument in areas presently covered by fields, and others, of uncertain date, can be seen eroding out of the cliff edge a little to the south of the enclosed area.

A number of enigmatic carved stone balls have been found at the site and some are on display in the museum. Similar objects have been found throughout northern Scotland. The spiral ornamentation on some of these "balls" has been stylistically linked to objects found in the Boyne Valley in Ireland. Similar symbols have been found carved into stone lintels and bed posts. These symbols, sometimes referred to as "runic writings", have been subjected to controversial translations. For example, Castleden suggested that "colons" found punctuating vertical and diagonal symbols may represent separations between words.

Lumps of red ochre found here and at other Neolithic sites have been interpreted as evidence that body painting may have been practised. Nodules of haematite with highly polished surfaces have been found as well; the shiny surfaces suggest that the nodules were used to finish leather.

Other artefacts excavated on site made of animal, fish, bird, and whalebone, whale and walrus ivory, and killer whale teeth included awls, needles, knives, beads, adzes, shovels, small bowls and, most remarkably, ivory pins up to long. These pins are very similar to examples found in passage graves in the Boyne Valley, another piece of evidence suggesting a linkage between the two cultures. So-called Skaill knives were commonly used tools in Skara Brae; these consist of large flakes knocked off sandstone cobbles. Skaill knives have been found throughout Orkney and Shetland.

The 1972 excavations reached layers that had remained waterlogged and had preserved items that otherwise would have been destroyed. These include a twisted skein of Heather, one of a very few known examples of Neolithic rope, and a wooden handle.

A comparable, though smaller, site exists at Rinyo on Rousay. Unusually, no Maeshowe-type tombs have been found on Rousay and although there are a large number of Orkney–Cromarty chambered cairns, these were built by Unstan ware people.

Knap of Howar, on the Orkney island of Papa Westray, is a well-preserved Neolithic farmstead. Dating from 3500 BC to 3100 BC, it is similar in design to Skara Brae, but from an earlier period, and it is thought to be the oldest preserved standing building in northern Europe.

There is also a site currently under excavation at Links of Noltland on Westray that appears to have similarities to Skara Brae.

"The Heart of Neolithic Orkney" was inscribed as a World Heritage site in December 1999. In addition to Skara Brae the site includes Maeshowe, the Ring of Brodgar, the Standing Stones of Stenness and other nearby sites. It is managed by Historic Environment Scotland, whose "Statement of Significance" for the site begins:

In 2019, a risk assessment was performed to assess the site's vulnerability to climate change. The report by Historic Environment Scotland, the Orkney Islands Council and others concludes that the entire Heart of Neolithic Orkney World Heritage Site, and in particular Skara Brae, is "extremely vulnerable" to climate change due to rising sea levels, increased rainfall and other factors; it also highlights the risk that Skara Brae could be partially destroyed by one unusually severe storm.




</doc>
<doc id="28893" url="https://en.wikipedia.org/wiki?curid=28893" title="Sinners in the Hands of an Angry God">
Sinners in the Hands of an Angry God

"Sinners in the Hands of an Angry God" is a sermon written by British Colonial Christian theologian Jonathan Edwards, preached to his own congregation in Northampton, Massachusetts, to unknown effect, and again on July 8, 1741 in Enfield, Connecticut. Like Edwards' other works, it combines vivid imagery of Hell with observations of the world and citations of the scripture. It is Edwards' most famous written work, is a fitting representation of his preaching style, and is widely studied by Christians and historians, providing a glimpse into the theology of the First Great Awakening of c. 1730–1755.

This is a typical sermon of the Great Awakening, emphasizing the teaching that Hell is real—a place that actually exists. Edwards hoped that the imagery and language of his sermon would awaken audiences to the horrific reality of hell that awaits them should they continue living without calling on Christ to be saved. The underlying point is that God has given humans a chance to confess their sins. Edwards said that it is the mere will of God that keeps wicked men from the depths of Hell. This act of restraint has given humans a chance to believe and trust in Christ.

""There is nothing that keeps wicked men at any one moment out of hell, but the mere pleasure of God.""

Most of the sermon's text consists of ten "considerations":

One church in Enfield, Connecticut, had been largely unaffected during the First Great Awakening of New England. Edwards was invited by the pastor of the church to preach to them. Edwards's aim was to teach his listeners about the horrors of hell, the dangers of sin, and the terrors of being lost. Edwards described the position of those who do not follow Christ's urgent call to receive forgiveness.

In the final section of "Sinners in the Hands of an Angry God," Edwards shows that his theological argument holds throughout scripture and biblical history. He invokes stories and examples throughout the whole Bible. Edwards ends the sermon with one final appeal: "Therefore let everyone that is out of Christ, now awake and fly from the wrath to come." According to Edwards, only by returning to Christ can one escape the stark fate he outlines.

Edwards was interrupted many times during the sermon by people moaning and crying out, "What shall I do to be saved?". Although the sermon has received criticism, Edwards' words have endured and are still read to this day. Edwards' sermon continues to be the leading example of a First Great Awakening sermon and is still used in religious and academic studies.

Since the 1950s, a number of critical perspectives were used to analyse the sermon. The first comprehensive academic analysis of "Sinners in the Hands of an Angry God" was published by Edwin Cady in 1949, who comments on the imagery of the sermon and distinguishes between the "cliché" and "fresh" figurative images, stressing how the former related to the colonial life. Lee Stuart questions that the message of the sermon was solely negative and attributes its success to the final passages in which the sinners are actually "comforted". Rosemary Hearn argues that it is the logical structure of the sermon that constitutes its most important persuasive element. Lemay looks into the changes in the syntactic categories, like grammatical tenses, in the text of the sermon. Lukasik stresses how in the sermon Edwards appropriates Newtonian physics, especially the image of the gravitational pull that would relentlessly bring the sinners down. Gallagher focuses on the "beat" of the sermon, and on how the consecutive structural elements of the sermon serve different persuasive aims. Choiński suggests that the rhetorical success of the sermon consists in the use of the "deictic shift" that transported the hearers mentally into the figurative images of hell.





</doc>
<doc id="28894" url="https://en.wikipedia.org/wiki?curid=28894" title="Scottish Highlands">
Scottish Highlands

The Highlands (; , 'the place of the Gaels') is a historic region of Scotland. Culturally, the Highlands and the Lowlands diverged from the later Middle Ages into the modern period, when Lowland Scots replaced Scottish Gaelic throughout most of the Lowlands. The term is also used for the area north and west of the Highland Boundary Fault, although the exact boundaries are not clearly defined, particularly to the east. The Great Glen divides the Grampian Mountains to the southeast from the Northwest Highlands. The Scottish Gaelic name of "" literally means "the place of the Gaels" and traditionally, from a Gaelic-speaking point of view, includes both the Western Isles and the Highlands.

The area is very sparsely populated, with many mountain ranges dominating the region, and includes the highest mountain in the British Isles, Ben Nevis. Before the middle of the 19th century the Highlands was home to a much larger population, but from "circa" 1841 and for the next 160 years, the natural increase in population was exceeded by emigration (mostly to Canada, the United States, Australia and New Zealand, and migration to the industrial cities of Scotland and England.) The area is now one of the most sparsely populated in Europe. At 9.1 per km (23.6 per square mile) in 2012, the population density in the Highlands and Islands is less than one seventh of Scotland's as a whole, comparable with that of Bolivia, Chad and Russia.

The Highland Council is the administrative body for much of the Highlands, with its administrative centre at Inverness. However, the Highlands also includes parts of the council areas of Aberdeenshire, Angus, Argyll and Bute, Moray, North Ayrshire, Perth and Kinross, Stirling and West Dunbartonshire.

The Scottish highlands is the only area in the British Isles to have the taiga biome as it features concentrated populations of Scots pine forest: see Caledonian Forest.

Between the 15th century and the mid-20th century, the area differed from most of the Lowlands in terms of language. In Scottish Gaelic, the region is known as the "", because it was traditionally the Gaelic-speaking part of Scotland, although the language is now largely confined to The Hebrides. The terms are sometimes used interchangeably but have different meanings in their respective languages. Scottish English (in its Highland form) is the predominant language of the area today, though Highland English has been influenced by Gaelic speech to a significant extent. Historically, the "Highland line" distinguished the two Scottish cultures. While the Highland line broadly followed the geography of the Grampians in the south, it continued in the north, cutting off the north-eastern areas, that is Eastern Caithness, Orkney and Shetland, from the more Gaelic Highlands and Hebrides.

Historically, the major social unit of the Highlands was the clan. Scottish kings, particularly James VI, saw clans as a challenge to their authority; the Highlands was seen by many as a lawless region. Following the Union of the Crowns, James VI had the military strength to back up any attempts to impose some control. The result was, in 1609, the Statutes of Iona which started the process of integrating clan leaders into Scottish society. The gradual changes continued into the 19th century, as clan chiefs thought of themselves less as patriarchal leaders of their people and more as commercial landlords. The first effect on the clansmen who were their tenants was the change to rents being payable in money rather than in kind. Later, rents were increased as Highland landowners sought to increase their income. This was followed, mostly in the period 1760–1850, by agricultural improvement that often (particularly in the Western Highlands) involved clearance of the population to make way for large scale sheep farms. Displaced tenants were set up in crofting communities in the process. The crofts were intended not to provide all the needs of their occupiers; they were expected to work in other industries such as kelping and fishing. Crofters came to rely substantially on seasonal migrant work, particularly in the Lowlands. This gave impetus to the learning of English, which was seen by many rural Gaelic speakers to be the essential "language of work".

Older historiography attributes the collapse of the clan system to the aftermath of the Jacobite risings. This is now thought less influential by historians. Following the Jacobite rising of 1745 the British government enacted a series of laws to try to suppress the clan system, including bans on the bearing of arms and the wearing of tartan, and limitations on the activities of the Scottish Episcopal Church. Most of this legislation was repealed by the end of the 18th century as the Jacobite threat subsided. There was soon a rehabilitation of Highland culture. Tartan was adopted for Highland regiments in the British Army, which poor Highlanders joined in large numbers in the era of the Revolutionary and Napoleonic Wars (1790–1815). Tartan had largely been abandoned by the ordinary people of the region, but in the 1820s, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe. The international craze for tartan, and for idealising a romanticised Highlands, was set off by the Ossian cycle, and further popularised by the works of Walter Scott. His "staging" of the visit of King George IV to Scotland in 1822 and the king's wearing of tartan resulted in a massive upsurge in demand for kilts and tartans that could not be met by the Scottish woollen industry. Individual clan tartans were largely designated in this period and they became a major symbol of Scottish identity. This "Highlandism", by which all of Scotland was identified with the culture of the Highlands, was cemented by Queen Victoria's interest in the country, her adoption of Balmoral as a major royal retreat, and her interest in "tartenry".

Recurrent famine affected the Highlands for much of its history, with significant instances as late as 1817 in the Eastern Highlands and the early 1850s in the West. Over the 18th century, the region had developed a trade of black cattle into Lowland markets, and this was balanced by imports of meal into the area. There was a critical reliance on this trade to provide sufficient food, and it is seen as an essential prerequisite for the population growth that started in the 18th century. Most of the Highlands, particularly in the North and West was short of the arable land that was essential for the mixed, run rig based, communal farming that existed before agricultural improvement was introduced into the region. Between the 1760s and the 1830s there was a substantial trade in unlicensed whisky that had been distilled in the Highlands. Lowland distillers (who were not able to avoid the heavy taxation of this product) complained that Highland whisky made up more than half the market. The development of the cattle trade is taken as evidence that the pre-improvement Highlands was not an immutable system, but did exploit the economic opportunities that came its way. The illicit whisky trade demonstrates the entrepreneurial ability of the peasant classes.

Agricultural improvement reached the Highlands mostly over the period 1760 to 1850. Agricultural advisors, factors, land surveyors and others educated in the thinking of Adam Smith were keen to put into practice the new ideas taught in Scottish universities. Highland landowners, many of whom were burdened with chronic debts, were generally receptive to the advice they offered and keen to increase the income from their land. In the East and South the resulting change was similar to that in the Lowlands, with the creation of larger farms with single tenants, enclosure of the old run rig fields, introduction of new crops (such as turnips), land drainage and, as a consequence of all this, eviction, as part of the Highland clearances, of many tenants and cottars. Some of those cleared found employment on the new, larger farms, others moved to the accessible towns of the Lowlands.

In the West and North, evicted tenants were usually given tenancies in newly created crofting communities, whilst their former holdings were converted into large sheep farms. Sheep farmers could pay substantially higher rents than the run rig farmers and were much less prone to falling into arrears. Each croft was limited in size so that the tenants would have to find work elsewhere. The major alternatives were fishing and the kelp industry. Landlords took control of the kelp shores, deducting the wages earned by their tenants from the rent due and retaining the large profits that could be earned at the high prices paid for the processed product during the Napoleonic wars.

When the Napoleonic wars finished in 1815, the Highland industries were affected by the return to a peacetime economy. The price of black cattle fell, nearly halving between 1810 and the 1830s. Kelp prices had peaked in 1810, but reduced from £9 a ton in 1823 to £3 13s 4d a ton in 1828. Wool prices were also badly affected. This worsened the financial problems of debt-encumbered landlords. Then, in 1846, potato blight arrived in the Highlands, wiping out the essential subsistence crop for the overcrowded crofting communities. As the famine struck, the government made clear to landlords that it was their responsibility to provide famine relief for their tenants. The result of the economic downturn had been that a large proportion of Highland estates were sold in the first half of the 19th century. T M Devine points out that in the region most affected by the potato famine, by 1846, 70 per cent of the landowners were new purchasers who had not owned Highland property before 1800. More landlords were obliged to sell due to the cost of famine relief. Those who were protected from the worst of the crisis were those with extensive rental income from sheep farms. Government loans were made available for drainage works, road building and other improvements and many crofters became temporary migrants – taking work in the Lowlands. When the potato famine ceased in 1856, this established a pattern of more extensive working away from the Highlands.

The unequal concentration of land ownership remained an emotional and controversial subject, of enormous importance to the Highland economy, and eventually became a cornerstone of liberal radicalism. The poor crofters were politically powerless, and many of them turned to religion. They embraced the popularly oriented, fervently evangelical Presbyterian revival after 1800. Most joined the breakaway "Free Church" after 1843. This evangelical movement was led by lay preachers who themselves came from the lower strata, and whose preaching was implicitly critical of the established order. The religious change energised the crofters and separated them from the landlords; it helped prepare them for their successful and violent challenge to the landlords in the 1880s through the Highland Land League.
Violence erupted, starting on the Isle of Skye, when Highland landlords cleared their lands for sheep and deer parks. It was quietened when the government stepped in, passing the Crofters' Holdings (Scotland) Act, 1886 to reduce rents, guarantee fixity of tenure, and break up large estates to provide crofts for the homeless. This contrasted with the Irish Land War under way at the same time, where the Irish were intensely politicised through roots in Irish nationalism, while political dimensions were limited. In 1885 three Independent Crofter candidates were elected to Parliament, which listened to their pleas. The results included explicit security for the Scottish smallholders in the "crofting counties"; the legal right to bequeath tenancies to descendants; and the creation of a Crofting Commission. The Crofters as a political movement faded away by 1892, and the Liberal Party gained their votes.
Today, the Highlands are the largest of Scotland's whisky producing regions; the relevant area runs from Orkney to the Isle of Arran in the south and includes the northern isles and much of Inner and Outer Hebrides, Argyll, Stirlingshire, Arran, as well as sections of Perthshire and Aberdeenshire. (Other sources treat The Islands, except Islay, as a separate whisky producing region.) This massive area has over 30 distilleries, or 47 when the Islands sub-region is included in the count. According to one source, the top five are The Macallan, Glenfiddich, Aberlour, Glenfarclas and Balvenie. While Speyside is geographically within the Highlands, that region is specified as distinct in terms of whisky productions. Speyside single malt whiskies are produced by about 50 distilleries.

According to "Visit Scotland", Highlands whisky is "fruity, sweet, spicy, malty". Another review states that Northern Highlands single malt is "sweet and full-bodied", the Eastern Highlands and Southern Highlands whiskies tend to be "lighter in texture" while the distilleries in the Western Highlands produce single malts with a "much peatier influence".

The Scottish Reformation achieved partial success in the Highlands. Roman Catholicism remained strong in some areas, owing to remote locations and the efforts of Franciscan missionaries from Ireland, who regularly came to celebrate Mass. There remain significant Catholic strongholds within the Highlands and Islands such as Moidart and Morar on the mainland and South Uist and Barra in the southern Outer Hebrides.
The remoteness of the region and the lack of a Gaelic-speaking clergy undermined the missionary efforts of the established church. The later 18th century saw somewhat greater success, owing to the efforts of the SSPCK missionaries and to the disruption of traditional society after the Battle of Culloden in 1746. In the 19th century, the evangelical Free Churches, which were more accepting of Gaelic language and culture, grew rapidly, appealing much more strongly than did the established church.

For the most part, however, the Highlands are considered predominantly Protestant, loyal to the Church of Scotland. In contrast to the Catholic southern islands, the northern Outer Hebrides islands (Lewis, Harris and North Uist) have an exceptionally high proportion of their population belonging to the Protestant Free Church of Scotland or the Free Presbyterian Church of Scotland. The Outer Hebrides have been described as the last bastion of Calvinism in Britain and the Sabbath remains widely observed. Inverness and the surrounding area has a majority Protestant population, with most locals belonging to either The Kirk or the Free Church of Scotland. The church maintains a noticeable presence within the area, with church attendance notably higher than in other Scottish cities. Religion continues to play an important role in Highland culture, with Sabbath observance still widely practised, particularly in the Hebrides.

In traditional Scottish geography, the Highlands refers to that part of Scotland north-west of the Highland Boundary Fault, which crosses mainland Scotland in a near-straight line from Helensburgh to Stonehaven. However the flat coastal lands that occupy parts of the counties of Nairnshire, Morayshire, Banffshire and Aberdeenshire are often excluded as they do not share the distinctive geographical and cultural features of the rest of the Highlands. The north-east of Caithness, as well as Orkney and Shetland, are also often excluded from the Highlands, although the Hebrides are usually included. The Highland area, as so defined, differed from the Lowlands in language and tradition, having preserved Gaelic speech and customs centuries after the anglicisation of the latter; this led to a growing perception of a divide, with the cultural distinction between Highlander and Lowlander first noted towards the end of the 14th century. In Aberdeenshire, the boundary between the Highlands and the Lowlands is not well defined. There is a stone beside the A93 road near the village of Dinnet on Royal Deeside which states 'You are now in the Highlands', although there are areas of Highland character to the east of this point.

A much wider definition of the Highlands is that used by the Scotch Whisky industry. Highland Single Malts are produced at distilleries north of an imaginary line between Dundee and Greenock, thus including all of Aberdeenshire and Angus.

Inverness is regarded as the Capital of the Highlands, although less so in the Highland parts of Aberdeenshire, Angus, Perthshire and Stirlingshire which look more to Aberdeen, Dundee, Perth, and Stirling as their commercial centres. 

The Highland Council area, created as one of the local government regions of Scotland, has been a unitary council area since 1996. The council area excludes a large area of the southern and eastern Highlands, and the Western Isles, but includes Caithness. "Highlands" is sometimes used, however, as a name for the council area, as in "Highlands and Islands Fire and Rescue Service". "Northern", as in "Northern Constabulary", is also used to refer to the area covered by the fire and rescue service. This area consists of the Highland council area and the island council areas of Orkney, Shetland and the Western Isles.

Highland Council signs in the Pass of Drumochter, between Glen Garry and Dalwhinnie, say "Welcome to the Highlands".

Much of the Highlands area overlaps the Highlands and Islands area. An electoral region called "Highlands and Islands" is used in elections to the Scottish Parliament: this area includes Orkney and Shetland, as well as the Highland Council local government area, the Western Isles and most of the Argyll and Bute and Moray local government areas. "Highlands and Islands" has, however, different meanings in different contexts. It means Highland (the local government area), Orkney, Shetland, and the Western Isles in "Highlands and Islands Fire and Rescue Service". "Northern", as in "Northern Constabulary", refers to the same area as that covered by the fire and rescue service.

There have been trackways from the Lowlands to the Highlands since prehistoric times. Many traverse the Mounth, a spur of mountainous land that extends from the higher inland range to the North Sea slightly north of Stonehaven. The most well-known and historically important trackways are the Causey Mounth, Elsick Mounth, Cryne Corse Mounth and Cairnamounth.

Although most of the Highlands is geographically on the British mainland, it is somewhat less accessible than the rest of Britain; thus most UK couriers categorise it separately, alongside Northern Ireland, the Isle of Man, and other offshore islands. They thus charge additional fees for delivery to the Highlands, or exclude the area entirely. Whilst the physical remoteness from the largest population centres inevitably leads to higher transit cost, there is confusion and consternation over the scale of the fees charged and the effectiveness of their communication, and the use of the word Mainland in their justification. Since the charges are often based on postcode areas, many far less remote areas, including some which are traditionally considered part of the lowlands, are also subject to these charges. Royal Mail is the only delivery network bound by a Universal Service Obligation to charge a uniform tariff across the UK. This, however, applies only to mail items and not larger packages which are dealt with by its Parcelforce division.

The Highlands lie to the north and west of the Highland Boundary Fault, which runs from Arran to Stonehaven. This part of Scotland is largely composed of ancient rocks from the Cambrian and Precambrian periods which were uplifted during the later Caledonian Orogeny. Smaller formations of Lewisian gneiss in the northwest are up to 3 billion years old. The overlying rocks of the Torridon Sandstone form mountains in the Torridon Hills such as Liathach and Beinn Eighe in Wester Ross.

These foundations are interspersed with many igneous intrusions of a more recent age, the remnants of which have formed mountain massifs such as the Cairngorms and the Cuillin of Skye. A significant exception to the above are the fossil-bearing beds of Old Red Sandstone found principally along the Moray Firth coast and partially down the Highland Boundary Fault. The Jurassic beds found in isolated locations on Skye and Applecross reflect the complex underlying geology. They are the original source of much North Sea oil. The Great Glen is formed along a transform fault which divides the Grampian Mountains to the southeast from the Northwest Highlands.

The entire region was covered by ice sheets during the Pleistocene ice ages, save perhaps for a few nunataks. The complex geomorphology includes incised valleys and lochs carved by the action of mountain streams and ice, and a topography of irregularly distributed mountains whose summits have similar heights above sea-level, but whose bases depend upon the amount of denudation to which the plateau has been subjected in various places.





</doc>
<doc id="28896" url="https://en.wikipedia.org/wiki?curid=28896" title="Scotch whisky">
Scotch whisky

Scotch whisky (; often simply called whisky or Scotch) is malt whisky or grain whisky (or a blend of the two), made in Scotland. As of 2018, there were 133 Scotch whisky distilleries operating in Scotland. Scotch whisky must be made in a manner specified by law.

All Scotch whisky was originally made from malted barley. Commercial distilleries began introducing whisky made from wheat and rye in the late 18th century. Scotch whisky is divided into five distinct categories: single malt Scotch whisky, single grain Scotch whisky, blended malt Scotch whisky (formerly called "vatted malt" or "pure malt"), blended grain Scotch whisky, and blended Scotch whisky.

All Scotch whisky must be aged in oak barrels for at least three years. Any age statement on a bottle of Scotch whisky, expressed in numerical form, must reflect the age of the youngest whisky used to produce that product. A whisky with an age statement is known as guaranteed-age whisky. A whisky without an age statement is known as a no age statement (NAS) whisky, the only guarantee being that all whisky contained in that bottle is at least three years old. The minimum bottling strength according to the regulation is 40% alcohol by volume.

The first known written mention of Scotch whisky is in the Exchequer Rolls of Scotland, 1495. Scotch whisky was known as "aqua vitae," Latin for "water of life."

Many Scotch whisky drinkers refer to a unit for drinking as a dram.

As of 23 November 2009, the Scotch Whisky Regulations 2009 (SWR) define and regulate the production, labelling, packaging as well as the advertising of Scotch whisky in the United Kingdom. They replace previous regulations that focused solely on production, including the Scotch Whisky Act 1988.

The previous act primarily focused on production standards and so it was repealed and superseded by the 2009 Regulations. The SWR includes broader definitions and requirements for the crafting, bottling, labelling, branding, and selling of "Scotch Whisky". International trade agreements have the effect of making some provisions of the SWR apply in various other countries as well as in the UK. The SWR define "Scotch whisky" as whisky that is:

A Scotch whisky label comprises several elements that indicate aspects of production, age, bottling, and ownership. Some of these elements are regulated by the SWR, and some reflect tradition and marketing. The spelling of the term "whisky" is often debated by journalists and consumers. Scottish, English, Welsh, Australian and Canadian whiskies use "whisky", Irish whiskies use "whiskey", while American and other styles vary in their spelling of the term.

The label always features a declaration of the malt or grain whiskies used. A single malt Scotch whisky is one that is entirely produced from malt in one distillery. One may also encounter the term "single cask", signifying the bottling comes entirely from one cask. The term "blended malt" signifies that single malt whisky from different distilleries are blended in the bottle. The Cardhu distillery also began using the term "pure malt" for the same purpose, causing a controversy in the process over clarity in labelling—the Glenfiddich distillery was using the term to describe some single malt bottlings. As a result, the Scotch Whisky Association declared that a mixture of single malt whiskies must be labelled a "blended malt". The use of the former terms "vatted malt" and "pure malt" is prohibited. The term "blended malt" is still debated, as some bottlers maintain that consumers confuse the term with "blended Scotch whisky", which contains some proportion of grain whisky.

The brand name featured on the label is usually the same as the distillery name (for example, the Talisker distillery labels its whiskies with the Talisker name). Indeed, the SWR prohibit bottlers from using a distillery name when the whisky was not made there. A bottler name may also be listed, sometimes independent of the distillery. In addition to requiring that Scotch whisky be distilled in Scotland, the SWR require that it also be bottled and labelled in Scotland. Labels may also indicate the region of the distillery (for example, Islay or Speyside).

Alcoholic strength is expressed on the label with "Alcohol By Volume" ("ABV") or sometimes simply "Vol". Typically, bottled whisky is between 40% and 46% ABV. Whisky is considerably stronger when first emerging from the cask—normally 60–63% ABV. Water is then added to create the desired bottling strength. If the whisky is not diluted before bottling, it can be labelled as cask strength.

A whisky's age may be listed on the bottle providing a guarantee of the youngest whisky used. An age statement on the bottle, in the form of a number, must reflect the age of the youngest whisky used to produce that product. A whisky with an age statement is known as guaranteed age whisky. Scotch whisky without an age statement may, by law, be as young as three years old. In the early 21st century, such "No age statement" whiskies have become more common, as distilleries respond to the depletion of aged stocks caused by improved sales. A label may carry a distillation date or a bottling date. Whisky does not mature once bottled, so if no age statement is provided, one may calculate the age of the whisky if both the distillation date and bottling date are given.

Labels may also carry various declarations of filtration techniques or final maturation processes. A Scotch whisky labelled as "natural" or "non-chill-filtered" has not been through a filtration process during bottling that removes compounds that some consumers see as desirable. Whisky is aged in various types of casks—and often in used sherry or port casks—during distinct portions of the maturation process, and will take on characteristics, flavour and aromas from such casks. Special casks are sometimes used at the end of the maturation process, and such whiskies may be labelled as "wood finished", "sherry/port finished", and so on.

According to the Scotch Whisky Association, Scotch whisky evolved from a Scottish drink called "uisge beatha", which means "water of life". The earliest record of distillation in Scotland occurred as long ago as 1494, as documented in the "Exchequer Rolls", which were records of royal income and expenditure. The quote above records eight bolls of malt given to Friar John Cor wherewith to make aqua vitae (Latin for "water of life," = "uisge beatha") over the previous year. This would be enough for 1,500 bottles, which suggests that distillation was well-established by the late 15th century.

Aqua vitae (in the form of wine or spirits) was used when making gunpowder to moisten the slurry of saltpetre, charcoal and sulphur. As a drink, Scotch whisky was a favourite of King James IV of Scotland but after he was defeated in 1513, the monasteries were dissolved by King Henry VIII of England. Some monks with distillery experience moved to other locations where they continued producing the spirit. At the time, that endeavour was considered to be illegal under British rule. Nonetheless, the Crown imposed a tax on malt in the early 1700s in order to generate income.

Whisky production was first taxed in 1644, causing a rise in illicit whisky distilling in the country. Between the 1760s and the 1830s a substantial unlicensed trade originated from the Highlands, forming a significant part of the region's export economy. In 1782, more than 1,000 illegal stills were seized in the Highlands: these can only have been a fraction of those in operation. The Lowland distillers, who had no opportunity to avoid taxation, complained that un-taxed Highland whisky made up more than half the market. The heavy taxation during the Napoleonic Wars gave the illicit trade a big advantage, but their product was also considered better quality, commanding a higher price in the Lowlands. This was due to the method of taxation: malt was subject to tax (at a rate that climbed substantially between the 1790s and 1822). The licensed distillers therefore used more raw grain in an effort to reduce their tax bill.

The Highland magistrates, themselves members of the landowning classes, had a lenient attitude to unlicensed distillers—all of whom would be tenants in the local area. They understood that the trade supported the rents paid. Imprisoned tenants would not be able to pay any rent.

In 1823, Parliament eased restrictions on licensed distilleries with the "Excise Act", while at the same time making it harder for the illegal stills to operate. Magistrates found counsel for the Crown appearing in their courts, so forcing the maximum penalties to be applied, with some cases removed to the Court of Exchequer in Edinburgh for tougher sentences. Highland landowners were now happy to remove tenants who were distillers in clearances on their estates. These changes ushered in the modern era of Scotch production: in 1823 2,232,000 gallons of whisky had duty paid on it; in 1824 this increased to 4,350,000 gallons.

A farmer, George Smith, working under landlord the Duke of Gordon, was the first person in Scotland to take out a licence for a distillery under the new Act, founding the Glenlivet Distillery in 1824, to make single malt Scotch. Some of the distilleries which started legal operations in the next few years included Bowmore, Strathisla, Balblair, and Glenmorangie; all remain in business today.

Two events helped to increase whisky's popularity: first, the introduction in 1831 of the column still. Aeneas Coffey patented a refined version of a design originally created by Robert Stein, based on early innovations by Sir Anthony Perrier, for the new type of still which produced whisky much more efficiently than the traditional pot stills. The column still allowed for continuous distillation, without the need for cleaning after each batch was made. This process made manufacturing more affordable by performing the equivalent of multiple distillation steps. The new still dramatically increased production; the whisky was less intense and smoother making it more popular.

Second, there was a shortage of wine, brandy and cognac in France, significant by 1880, due to the "phylloxera" bug, a parasitic insect, destroying many of the wine vines; that shortage increased the demand for whisky. By the 1890s, almost forty new distilleries had opened in Scotland. The boom years continued until the industry was significantly affected by World War I and later, by the Great Depression; many of the companies closed and never re-opened.

The Scotch Whisky Association estimated that Scotland's whisky industry supported 40,000 jobs and accounted for £4.37 billion in exports in 2017. Of that total, single malt Scotch accounted for £1.17 billion in exports, a 14% increase over 2016. 

The industry's contribution to the economy of the UK was estimated as £5.5 billion in 2018; the industry provided £3.8 billion in direct GVA (gross value added) to Scotland. Whisky tourism has also become significant and accounts for £68.3 million per year. One factor negatively affected sales, an extra 3.9% duty on spirits imposed by the UK in 2017. (The effect of the 25% increase in tariffs imposed by the U.S. in October 2019 would not be apparent until 2020.) Nonetheless, by year-end 2017, exports had reached a record-breaking amount.

In November 2019, the Association announced that the government of the UK had agreed to consider revising the alcohol taxation system, hopefully producing a new plan that was simplified and "fairer". 

Exports in 2018 again increased 7.8% by value, and 3.6% in number of bottles, in spite of the duty imposed in 2017; exports grew to a record level, £4.7 billion. The US imported Scotch whisky with a value of just over £1 billion while the European Union was the second largest importer, taking 30% of global value. This was a boom year with a record high in exports, but the Scotch Whisky Association expressed concern for the future, particularly "the challenges posed by Brexit and by tensions in the global trading system."

Scotch whisky tourism has developed around the industry, with distilleries being the third most visited attractions in Scotland; roughly 2 million visits were recorded in 2018. Some 68 distilleries operate visitors' centres in Scotland and another eight accept visits by appointment. Hotels, restaurants and other facilities are also impacted by the tourism phenomenon. The tourism has had an especially visible impact on the economy in some remote rural areas, according to Fiona Hyslop MSP, Cabinet Secretary for Culture, Tourism and External Affairs. "The Scottish Government is committed to working with partners like the Scotch Whisky Association to increase our tourism offer and encourage more people to visit our distilleries," the Secretary said.

A 2016 report stated that only 20% of the whisky was made by companies owned in Scotland. Distilleries owned by Diageo, a London-based company, produce 40% of all Scotch whisky, with over 24 brands, such as Johnnie Walker, Oban, and Talisker. Another 20% of the product is made by distillers owned by Pernod Ricard of France, including brands such as Glenlivet, Chivas Regal, and Ballantine's. There are also smaller distillers that are owned by foreign companies, such as BenRiach whose parent is the Brown–Forman Corporation based in Kentucky. Nonetheless, Scotch whisky is produced according to the current regulations, as to ageing, production and so on, ensuring that it remains Scottish.

Independents owned by Scots companies make a substantial amount of whisky too, particularly William Grant & Sons, the largest of these. Grant produces 8% of all Scotch whisky, or about 7.6 million cases per year, with brands including Glenfiddich and Balvenie. Glenfiddich is the best-selling single malt Scotch in the world. Roughly 14 million bottles of Glenfiddich are sold annually.

Most malt distilleries sell a significant amount of whisky by the cask for blending, and sometimes to private buyers as well. Whisky from such casks is sometimes bottled as a single malt by "independent bottling" firms such as Duncan Taylor, Master of Malt, Gordon & MacPhail, Cadenhead's, The Scotch Malt Whisky Society, Murray McDavid, Berry Bros. & Rudd, Douglas Laing, and others. These are usually labelled with the distillery's name, but not using the distillery's trademarked logos or typefaces. An "official bottling" (or "proprietary bottling"), by comparison, is from the distillery (or its owner). Many independent bottlings are from single casks, and they may sometimes be very different from an official bottling.

For a variety of reasons, some independent brands do not identify which facility distilled the whisky in the bottle. They may instead identify only the general geographical area of the source, or they simply market the product using their own brand name without identifying their source. This may, in some cases, be simply to give the independent bottling company the flexibility to purchase from multiple distillers without changing their labels.

There are two basic types of Scotch whisky, from which all blends are made:

Excluded from the definition of "single grain Scotch whisky" is any spirit that qualifies as a single malt Scotch whisky or as a blended Scotch whisky. The latter exclusion is to ensure that a blended Scotch whisky produced from single malt(s) and single grain(s) distilled at the same distillery does not also qualify as single grain Scotch whisky.

Nearly 90% of the bottles of Scotch sold per year are blended whiskies. Three types of blends are defined for Scotch whisky:

The five Scotch whisky definitions are structured in such a way that the categories are mutually exclusive. The 2009 regulations changed the formal definition of blended Scotch whisky to achieve this result, but in a way that reflected traditional and current practice: before the 2009 SWR, any combination of Scotch whiskies qualified as a blended Scotch whisky, including for example a blend of single malt Scotch whiskies.

As was the case under the Scotch Whisky Act 1988, regulation 5 of the SWR 2009 stipulates that the only whisky that may be manufactured in Scotland is Scotch whisky. The definition of "manufacture" is "keeping for the purpose of maturation; and keeping, or using, for the purpose of blending, except for domestic blending for domestic consumption". This provision prevents the existence of two "grades" of whisky originating from Scotland, one "Scotch whisky" and the other, a "whisky – product of Scotland" that complies with the generic EU standard for whisky. According to the Scotch Whisky Association, allowing non-Scotch whisky production in Scotland would make it difficult to protect Scotch whisky as a distinctive product.

The SWR regulation also states that no additives may be used except for plain (E150A) caramel colouring.

To qualify for this category the Scotch whisky must be made in one distillery, in a pot still by batch distillation, using only water and malted barley. As with any other Scotch whisky, the Scotch Whisky Regulations of 2009 also require single malt Scotch to be made completely and bottled in Scotland and aged for at least three years. Most are aged for longer than the minimum three years.

Another term is sometimes seen, called "double wood" or "triple wood", sometimes incorrectly referred to as "double malt" or "triple malt". These indicate that the whisky was aged in two or three types of casks. Hence, if the whisky otherwise meets the criteria of a single malt, it still falls into the single malt category even if more than one type of cask was used for ageing. Examples include The Balvenie 12 Years Old DoubleWood and Laphroaig Triple Wood.

Single grain whisky is made with water and a malted barley but the distillery then adds other grains or cereals, wheat, corn or rye, for example. From that moment on, it can no longer be called single malt. This type of product must be from a single distillery and is often used in making blended Scotch. Single grain whiskies are not distilled in column stills but with pot stills.

Blended malt whisky—formerly called "vatted malt" or "pure malt" (terms that are now prohibited in the SWR 2009)—is one of the least common types of Scotch: a blend of single malts from more than one distillery (possibly with differing ages).

Blended malts contain only single malt whiskies from two or more distilleries. This type must contain no grain whiskies and is distinguished by the absence of the word "single" on the bottle. The age of the vat is that of the youngest of the original ingredients. For example, a blended malt marked "8 years old" may include older whiskies, with the youngest constituent being eight years old. Johnnie Walker Green Label and Monkey Shoulder are examples of blended malt whisky. Starting from November 2011, no Scotch whisky could be labelled as a vatted malt or pure malt, the SWR requiring them to be labelled blended malt instead.

Blended Scotch whisky constitutes about 90% of the whisky produced in Scotland. Blended Scotch whiskies contain both malt whisky and grain whisky. Producers combine the various malts and grain whiskies to produce a consistent brand style. Notable blended Scotch whisky brands include Ballantine's, Bell's, Chivas Regal, Cutty Sark, Dewar's, J&B, Johnnie Walker, Teacher's Highland Cream, The Famous Grouse, and Whyte and Mackay.

The term blended grain Scotch refers to whisky that contains at least two single grain Scotch whiskies from at least two distilleries, combined to create one batch of the product.

Dozens of compounds contribute to Scotch whisky flavour and aroma characteristics, including volatile alcohol congeners (also called "higher oils") formed during fermentation, such as acetaldehyde, methanol, ethyl acetate, n-propanol, and isobutanol. Other flavour and aroma compounds include vanillic acid, syringic acid, vanillin, syringaldehyde, furfural, phenyl ethanol, and acetic acid. One analysis established 13 distinct flavour characteristics dependent on individual compounds, including sour, sweet, grainy, and floral as major flavour perceptions.

Some distilleries use a peat fire to dry the barley for some of their products before grinding it and making the mash. Peat smoke contributes phenolic compounds, such as guaiacol, that give aromas similar to smoke. The Maillard browning process of the residual sugars in the mashing process, particularly through formation of 2-furanmethanol and pyrazines imparting nutty or cereal characteristics, contributes to the baked bread notes in the flavour and aroma profile. Maturation during multi-year casking from reusing sherry oak barrels originally used for bourbon whiskey production contributes a vanilla aroma to some premium Scotch whiskies.

Refilling and fabrication or tampering of branded Scotch whiskies are types of Scotch whisky adulteration that diminishes brand integrity, consumer confidence, and profitability in the Scotch industry. Deviation from normal concentrations of major constituents, such as alcohol congeners, provides a precise, quantitative method for determining authenticity of Scotch whiskies. Over 100 compounds can be detected during counterfeit analysis, including phenolics and terpenes which may vary in concentration by different geographic origins, the barley used in the fermentation mash, or the oak cask used during ageing. Typical high-throughput instruments used in counterfeit detection are liquid chromatography and mass spectrometry.

Scotland was traditionally divided into four regions: The Highlands, The Lowlands, The Isle of Islay, and Campbeltown. Due to the large number of distilleries found there, the Speyside area became the fifth, recognised by the Scotch Whisky Association (SWA) as a distinct region in 2014. The whisky-producing islands other than Islay are not recognised as a distinct region by the SWA, which groups them into the Highlands region.



Although only five regions are specified, any Scottish locale may be used to describe a whisky if it is distilled entirely within that place; for example a single malt whisky distilled on Orkney could be described as "Orkney Single Malt Scotch Whisky" instead of as an Island whisky.






</doc>
<doc id="28897" url="https://en.wikipedia.org/wiki?curid=28897" title="Special drawing rights">
Special drawing rights

Special drawing rights (abbreviated SDR, ISO 4217 currency code XDR (numeric: 960)) are supplementary foreign exchange reserve assets defined and maintained by the International Monetary Fund (IMF). SDRs are units of account for the IMF, and not a currency "per se". They instead represent a claim to currency held by IMF member countries for which they may be exchanged. SDRs were created in 1969 to supplement a shortfall of preferred foreign exchange reserve assets, namely gold and U.S. dollars. 

SDRs are allocated by the IMF to countries, and cannot be held or used by private parties. The number of SDRs in existence was around XDR 21.4 billion in August 2009. During the global financial crisis of 2009, an additional XDR 182.6 billion was allocated to "provide liquidity to the global economic system and supplement member countries’ official reserves". By October 2014, the number of SDRs in existence was XDR 204 billion.

The value of a SDR is based on a basket of key international currencies reviewed by IMF every five years. The weights assigned to each currency in the XDR basket are adjusted to take into account their current prominence in terms of international trade and national foreign exchange reserves. In the review conducted in November 2015, the IMF decided that the Renminbi (Chinese yuan) would be added to the basket effective 1 October 2016. From that date, the XDR basket now consists of the following five currencies: U.S. dollar 41.73%, euro 30.93%, renminbi (Chinese yuan) 10.92%, Japanese yen 8.33%, British pound 8.09%.

While the ISO 4217 currency code for special drawing rights is XDR, they are often referred to by their acronym SDR. Both refer to the name "special drawing rights".

Intentionally innocuous and free of connotations because of disagreements over the nature of this new reserve asset during its creation, the name derives from a debate about its primary function—money or credit. While the name would offend neither side, it can be argued that prior to 1981 the XDR was a debt security and so a form of credit. Member countries receiving XDR allocations were required by the reconstitution provision of the XDR articles to hold a prescribed number of XDRs. If a state used any of its allotment, it was expected to rebuild its XDR holdings. As the reconstitution provisions were abrogated in 1981, the XDR now functions less like credit than previously. Countries are still expected to maintain their XDR holdings at a certain level, but penalties for holding fewer than the allocated amount are now less onerous.

The name may actually derive from an early proposal for IMF "reserve drawing rights". The word "reserve" was later replaced with "special" because the idea that the IMF was creating a foreign exchange reserve asset was contentious.

Special drawing rights were created by the IMF in 1969 and were intended to be an asset held in foreign exchange reserves under the Bretton Woods system of fixed exchange rates. 1 XDR was initially defined as US$1, equal to 0.888671 g of gold. After the collapse of that system in the early 1970s the SDR has taken on a less important role. Acting as the unit of account for the IMF has been its primary purpose since 1972.

The IMF itself calls the current role of the XDR "insignificant". Developed countries, who hold the greatest number of XDRs, are unlikely to use them for any purpose. The only actual users of XDRs may be those developing countries that see them as "a rather cheap line of credit".

One reason XDRs may not see much use as foreign exchange reserve assets is that they must be exchanged into a currency before use. This is due in part to the fact private parties do not hold XDRs: they are only used and held by IMF member countries, the IMF itself, and a select few organizations licensed to do so by the IMF. Basic functions of foreign exchange reserves, such as market intervention and liquidity provision, as well as some less prosaic ones, such as maintaining export competitiveness via favorable exchange rates, cannot be accomplished directly using XDRs. This fact has led the IMF to label the XDR as an "imperfect reserve asset".

Another reason they may see little use is that the number of XDRs in existence is relatively few. As of January 2011, XDRs represented less than 4% of global foreign exchange reserve assets. To function well a foreign exchange reserve asset must have sufficient liquidity, but XDRs, because of their small number, may be perceived to be an illiquid asset. The IMF says, "expanding the volume of official XDRs is a prerequisite for them to play a more meaningful role as a substitute reserve asset."

The XDR comes to prominence when the U.S. dollar is weak or otherwise unsuitable to be a foreign exchange reserve asset. This usually manifests itself as an allocation of XDRs to IMF member countries. Distrust of the U.S. dollar is not the only stated reason allocations have been made, however.
One of its first roles was to alleviate an expected shortfall of U.S. dollars c. 1970. At this time, the United States had a conservative monetary policy and did not want to increase the total amount of U.S. dollars in existence. If the United States had continued down this path, the dollar would have become a less attractive foreign exchange reserve asset: it would not have had the necessary liquidity to serve this function. Soon after XDR allocations began, the United States reversed its former policy and provided sufficient liquidity. In the process a potential role for the XDR was removed. During this first round of allocations, 9.3 billion XDRs were distributed to IMF member countries.

The XDR resurfaced in 1978 when many countries were wary of taking on more foreign exchange reserve assets denominated in U.S. dollars. This suspicion of the dollar precipitated an allocation of 12 billion XDRs over a period of four years.

Concomitant with the financial crisis of 2007–08, the third round of XDR allocations occurred in the years 2009 and 2011. The IMF recognized the financial crisis as the cause for distributing the large majority of these third-round allotments, but some allocations were couched as distributing XDRs to countries that had never received any and others as a re-balancing of IMF quotas, which determine how many XDRs a country is allotted, to better represent the economic strength of emerging markets.

During this time China, a country with large holdings of U.S. dollar foreign exchange reserves, voiced its displeasure at the current international monetary system, and promoted measures that would allow the XDR to "fully satisfy the member countries' demand for a reserve currency." These comments, made by a chairman of the People's Bank of China, Zhou Xiaochuan, drew media attention, and the IMF showed some support for China's stance. It produced a paper exploring ways the substance and function of the XDR could be increased. China has also suggested the creation of a substitution account to allow exchange of U.S. dollars into XDRs. When substitution was proposed before, in 1978, the United States appeared reluctant to allow such a mechanism to become operational.

In 2001, the UN suggested allocating XDRs to developing countries for use by them as cost-free alternatives to building foreign exchange reserves through borrowing or running current account surpluses. In 2009, an XDR allocation was made to countries that had joined the IMF after the 1979–1981 round of allocations was complete (and so had never been allocated any). First proposed in 1997, many of the beneficiaries of this 2009 allocation were developing countries.

The IMF takes into account the value of several currencies important to the world’s trading and financial systems. Firstly, it is widely used in international transactions, including export quotas in the IMF members and the number of official reserve assets which were in their own currencies. Secondly, it is widely traded on the main foreign exchange market, including foreign exchange trading volume, whether there are forward exchange markets and so on. Also it requires no less than 70% of the votes among the IMF members. Initially its value was fixed at 1 XDR = 1 U.S. dollar (as equivalent to 0.888671 grams of fine gold), but this was abandoned in favor of a currency basket after the 1973 collapse of the Bretton Woods system of fixed exchange rates.

From July 1974 to December 1980, the XDR basket consisted of 16 currencies. From January 1981 until the birth of the euro, the basket consisted of only five currencies: the U.S. dollar, the Deutsche mark, the French franc, the British pound, and the Japanese yen. When the euro was introduced in January 1999, it replaced the German mark and French franc; the basket consisted of the U.S. dollar, the euro, the British pound and the Japanese yen. Since 1 October 2016, the XDR basket has included the Chinese renminbi.

This basket is re-evaluated every five years, and the currencies included as well as the weights given to them can then change. A currency's importance is currently measured by the degree to which it is used as a foreign exchange reserve asset and the amount of exports sold in that currency. 

Because of fluctuating exchange rates, the relative value of each currency varies continuously, as does the value of the XDR. The IMF sets the value of the XDR in terms of U.S. dollars every day. The latest U.S. dollar valuation of the XDR is published on the IMF website.

SDRs are allocated to member countries by the IMF. A country's IMF quota, the maximum amount of financial resources that it is obligated to contribute to the fund, determines its allotment of XDRs. Any new allocations must be voted on in the XDR Department of the IMF and pass with an 85% majority. All IMF member countries are represented in the XDR Department, but this is not a one country, one vote system; voting power is determined by a member country's IMF quota. For example, the United States has 16.7% of the vote as of March 2, 2011.

Allocations are not made on a regular basis and have only occurred on rare occasions. The first round took place because of a situation that was soon reversed, the possibility of an insufficient amount of U.S. dollars because of U.S. reluctance to run the deficit necessary to supply future demand. Extraordinary circumstances have, likewise, led to the other XDR allocation events. For example, during the global financial crisis of 2009, XDR 182.6 billion was allocated to "provide liquidity to the global economic system and supplement member countries’ official reserves". The 2011 allocations were to low-income member countries.

An IMF member country that requires actual foreign currency may sell its SDRs to another member country in exchange for the currency. To sell a part or all its SDRs, the country must find a willing party to buy them. The IMF acts as an intermediary in this voluntary exchange. 

The IMF also has the authority under the designation mechanism to ask member countries with strong foreign exchange reserves to purchase XDRs from those with weak reserves. The maximum obligation any country has under this mechanism is currently equal to twice the amount of its SDR allocation. As of 2015, XDRs may only be exchanged for euros, Japanese yen, UK pounds, or US dollars. The IMF says exchanging XDRs can take "several days."

It is not, however, the IMF that pays out foreign currency in exchange for XDRs: the claim to currency that XDRs represent is not a claim on the IMF.

The IMF calculates a weekly interest rate, which is based on "a weighted average of representative interest rates on short-term debt in the money markets of the XDR basket currencies". No interest is payable on the SDRs allocated to a country by the IMF. However, interest is payable by an IMF member country that has exchanged (sold) some or all of the SDRs it was allocated, and interest is paid to a member country that holds more SDRs than it was allocated (ie., the country that bought SDRs from another member).

Some international organizations use the XDR as a unit of account. The IMF says using the XDR in this way "help[s] cope with exchange rate volatility." As of 2001, organizations that use the XDR as a unit of account, besides the IMF itself, include: Universal Postal Union, African Development Bank, Arab Monetary Fund, Asian Development Bank, Bank for International Settlements, Common Fund for Commodities, East African Development Bank, Economic Community of West African States, International Center for Settlement of Investment Disputes, International Fund for Agricultural Development, and Islamic Development Bank. It is not only international organizations that use the XDR in this way. JETRO uses XDRs to price foreign aid. In addition, charges, liabilities, and fees prescribed by some international treaties are denominated in XDRs. In 2003, the Bank for International Settlements ceased to use the gold franc as their currency, in favour of XDR.

Some bonds are also denominated in SDR, like the IBRD 2016 SDR denominated bonds.

In some international treaties and agreements, XDRs are used to value penalties, charges or prices. For example, the Convention on Limitation of Liability for Maritime Claims caps personal liability for damages to ships at XDR 330,000. The Montreal Convention and other treaties also use XDRs in this way.

According to the IMF, "the SDR may not be any country’s optimal basket", but a few countries do peg their currencies to the XDR. One possible benefit to nations with XDR pegs is that they may be perceived to be more transparent. As of 2000, the number of countries that did so was four. This is a substantial decrease from 1983, when 14 countries had XDR pegs. As of 2010, Syria pegs its pound to the XDR.





</doc>
<doc id="28898" url="https://en.wikipedia.org/wiki?curid=28898" title="Special Operations Executive">
Special Operations Executive

The Special Operations Executive (SOE) was a British World War II organisation. It was officially formed on 22 July 1940 under Minister of Economic Warfare Hugh Dalton, from the amalgamation of three existing secret organisations. Its purpose was to conduct espionage, sabotage and reconnaissance in occupied Europe (and later, also in occupied Southeast Asia) against the Axis powers, and to aid local resistance movements.

Few people were aware of SOE's existence. Those who were part of it or liaised with it were sometimes referred to as the "Baker Street Irregulars", after the location of its London headquarters. It was also known as "Churchill's Secret Army" or the "Ministry of Ungentlemanly Warfare". Its various branches, and sometimes the organisation as a whole, were concealed for security purposes behind names such as the "Joint Technical Board" or the "Inter-Service Research Bureau", or fictitious branches of the Air Ministry, Admiralty or War Office.

SOE operated in all territories occupied or attacked by the Axis forces, except where demarcation lines were agreed with Britain's principal Allies (the United States and the Soviet Union). It also made use of neutral territory on occasion, or made plans and preparations in case neutral countries were attacked by the Axis. The organisation directly employed or controlled more than 13,000 people, about 3,200 of whom were women.

After the war, the organisation was officially dissolved on 15 January 1946. A memorial to SOE's agents was unveiled in October 2009 on the Albert Embankment by Lambeth Palace in London.
The Valençay SOE Memorial honors 104 SOE agents who lost their lives while working in France.

The organisation was formed from the merger of three existing secret departments, which had been formed shortly before the outbreak of the Second World War. Immediately after Germany annexed Austria (the "Anschluss") in March 1938, the Foreign Office created a propaganda organisation known as Department EH (after Electra House, its headquarters), run by Canadian newspaper magnate Sir Campbell Stuart. Later that month, the Secret Intelligence Service (SIS, also known as MI6) formed a section known as Section D, under Major Lawrence Grand RE, to investigate the use of sabotage, propaganda, and other irregular means to weaken an enemy. In the autumn of the same year, the War Office expanded an existing research department known as GS (R) and appointed Major J. C. Holland RE as its head to conduct research into guerrilla warfare. GS (R) was renamed MI(R) in early 1939.

These three departments worked with few resources until the outbreak of war. There was much overlap between their activities. Section D and EH duplicated much of each other's work. On the other hand, the heads of Section D and MI(R) knew each other and shared information. They agreed to a rough division of their activities; MI(R) researched irregular operations that could be undertaken by regular uniformed troops, while Section D dealt with truly undercover work.

During the early months of the war, Section D was based first at St Ermin's Hotel in Westminster and then the Metropole Hotel near Trafalgar Square. The Section attempted unsuccessfully to sabotage deliveries of vital strategic materials to Germany from neutral countries by mining the Iron Gate on the River Danube. MI(R) meanwhile produced pamphlets and technical handbooks for guerrilla leaders. MI(R) was also involved in the formation of the Independent Companies, autonomous units intended to carry out sabotage and guerrilla operations behind enemy lines in the Norwegian Campaign, and the Auxiliary Units, stay-behind commando units based on the Home Guard which would act in the event of an Axis invasion of Britain, as seemed possible in the early years of the war.

On 13 June 1940, at the instigation of newly appointed Prime Minister Winston Churchill, Lord Hankey (who held the Cabinet post of Chancellor of the Duchy of Lancaster) persuaded Section D and MI(R) that their operations should be coordinated. On 1 July, a Cabinet level meeting arranged the formation of a single sabotage organisation. On 16 July, Hugh Dalton, the Minister of Economic Warfare, was appointed to take political responsibility for the new organisation, which was formally created on 22 July 1940. Dalton recorded in his diary that on that day the War Cabinet agreed to his new duties and that Churchill had told him, "And now go and set Europe ablaze." Dalton used the Irish Republican Army (IRA) during the Irish War of Independence as a model for the organisation.

Sir Frank Nelson was nominated by SIS to be director of the new organisation, and a senior civil servant, Gladwyn Jebb, transferred from the Foreign Office to it, with the title of Chief Executive Officer. Campbell Stuart left the organisation, and the flamboyant Major Grand was returned to the regular army. At his own request, Major Holland also left to take up a regular appointment in the Royal Engineers. (Both Grand and Holland eventually attained the rank of Major-general.) However, Holland's former deputy at MI(R), Brigadier Colin Gubbins, returned from command of the Auxiliary Units to be Director of Operations of SOE.

One department of MI(R), MI R(C), which was involved in the development of weapons for irregular warfare, was not formally integrated into SOE but became an independent body codenamed MD1. Directed by Major (later Lieutenant Colonel) Millis Jefferis, it was located at The Firs in Whitchurch and nicknamed "Churchill's Toyshop" from the Prime Minister's close interest in it and his enthusiastic support.

The director of SOE was usually referred to by the initials "CD". Nelson, the first director to be appointed, was a former head of a trading firm in India, a back bench Conservative Member of Parliament and Consul in Basel, Switzerland. There he had also been engaged in undercover intelligence work.

In February 1942 Dalton became President of the Board of Trade and was replaced as Minister of Economic Warfare by Lord Selborne. Selborne in turn retired Nelson, who had suffered ill health as a result of his hard work, and appointed Sir Charles Hambro, head of Hambros Bank, to replace him. He also transferred Jebb back to the Foreign Office.

Hambro had been a close friend of Churchill before the war and had won the Military Cross in the First World War. He retained several other interests, for example remaining chairman of Hambros and a director of the Great Western Railway. Some of his subordinates and associates expressed reservations that these interests distracted him from his duties as director. Selborne and Hambro nevertheless cooperated closely until August 1943, when they fell out over the question of whether SOE should remain a separate body or co-ordinate its operations with those of the British Army in several theatres of war. Hambro felt that any loss of autonomy would cause a number of problems for SOE in the future. At the same time, Hambro was found to have failed to pass on vital information to Selborne. He was dismissed as director, and became head of a raw materials purchasing commission in Washington, D.C., which was involved in the exchange of nuclear information.
As part of the subsequent closer ties between the Imperial General Staff and SOE (although SOE had no representation on the Chiefs of Staff Committee), Hambro's replacement as director from September 1943 was Gubbins, now a Major-general. Gubbins had wide experience of commando and clandestine operations and had played a major part in MI(R)'s and SOE's early operations. He also put into practice many of the lessons he learned from the IRA during the Irish War of Independence.

The organisation of SOE continually evolved and changed during the war. Initially, it consisted of three broad departments: SO1, which dealt with propaganda; SO2 (operations); and SO3 (research). SO3 was quickly overloaded with paperwork and was merged into SO2. In August 1941, following quarrels between the Ministry of Economic Warfare and the Ministry of Information over their relative responsibilities, SO1 was removed from SOE and became an independent organisation, the Political Warfare Executive.

Thereafter, a single, broad "Operations" department controlled the Sections operating into enemy and sometimes neutral territory, and the selection and training of agents. Sections, usually referred to by code letters or groups of letters, were assigned to a single country. Some enemy-occupied countries had two or more sections assigned to deal with politically disparate resistance movements. (France had no less than six). For security purposes, each section had its own headquarters and training establishments. This strict compartmentalisation was so effective that in mid-1942 five governments in exile jointly suggested that a single sabotage organisation be created, and were startled to learn that SOE had been in existence for two years.

Four departments and some smaller groups were controlled by the director of scientific research, Professor Dudley Maurice Newitt, and were concerned with the development or acquisition and production of special equipment. A few other sections were involved with finance, security, economic research and administration, although SOE had no central registry or filing system. When Gubbins was appointed director, he formalised some of the administrative practices which had grown in an "ad hoc" fashion and appointed an establishment officer to oversee the manpower and other requirements of the various departments.

The main controlling body of SOE was its council, consisting of around fifteen heads of departments or sections. About half of the council were from the armed forces (although some were specialists who were only commissioned after the outbreak of war), the rest were various civil servants, lawyers, or business or industrial experts. Most of the members of the council, and the senior officers and functionaries of SOE generally, were recruited by word of mouth among public school alumni and Oxbridge graduates, although this did not notably affect SOE's political complexion.

Several subsidiary SOE headquarters and stations were set up to manage operations which were too distant for London to control directly. SOE's operations in the Middle East and Balkans were controlled from a headquarters in Cairo, which was notorious for poor security, infighting and conflicts with other agencies. It finally became known in April 1944 as Special Operations (Mediterranean), or SO(M). Shortly after the Allied landings in North Africa, a station codenamed ""Massingham"" was established near Algiers in late 1942, which operated into Southern France. Following the Allied invasion of Italy, personnel from "Massingham" established forward stations in Brindisi and near Naples. A subsidiary headquarters initially known as "Force 133" was later set up in Bari in Southern Italy, under the Cairo headquarters, to control operations in the Balkans and Northern Italy.

An SOE station, which was first called the "India Mission", and was subsequently known as "GS I(k)" was set up in India late in 1940. It subsequently moved to Ceylon so as to be closer to the headquarters of the Allied South East Asia Command and became known as Force 136. A "Singapore Mission" was set up at the same time as the India Mission but was unable to overcome official opposition to its attempts to form resistance movements in Malaya before the Japanese overran Singapore. Force 136 took over its surviving staff and operations.

New York City also had a branch office, formally titled British Security Coordination, and headed by the Canadian businessman Sir William Stephenson. This office, located at Room 3603, 630 Fifth Avenue, Rockefeller Center, coordinated the work of SOE, SIS and MI5 with the American FBI and Office of Strategic Services.

As with its leadership and organisation, the aims and objectives of SOE changed throughout the war, although they revolved around sabotaging and subverting the Axis war machines through indirect methods. SOE occasionally carried out operations with direct military objectives, such as Operation Harling, originally designed to cut one of the Axis supply lines to their troops fighting in North Africa. They also carried out some high-profile operations aimed mainly at the morale both of the Axis and occupied nations, such as Operation Anthropoid, the assassination in Prague of Reinhard Heydrich. In general also, SOE's objectives were to foment mutual hatred between the population of Axis-occupied countries and the occupiers, and to force the Axis to expend manpower and resources on maintaining their control of subjugated populations.

Dalton's early enthusiasm for fomenting widespread strikes, civil disobedience and nuisance sabotage in Axis-occupied areas had to be curbed. Thereafter, there were two main aims, often mutually incompatible; sabotage of the Axis war effort, and the creation of secret armies which would rise up to assist the liberation of their countries when Allied troops arrived or were about to do so. It was recognised that acts of sabotage would bring about reprisals and increased Axis security measures which would hamper the creation of underground armies. As the tide of war turned in the Allies' favour, these underground armies became more important.

At the government level, SOE's relationships with the Foreign Office were often difficult. On several occasions, various governments in exile protested at operations taking place without their knowledge or approval, provoking Axis reprisals against civilian populations, or complained about SOE's support for movements opposed to the exiled governments. SOE's activities also threatened relationships with neutral countries. SOE nevertheless generally adhered to the rule, ""No bangs without Foreign Office approval.""

Early attempts at bureaucratic control of Jefferis's MIR(c) by the Ministry of Supply were eventually foiled by Churchill's intervention. Thereafter, they co-operated, though at arm's length, with Dudley Newitt's various supply and development departments. The Treasury were accommodating from the start and were often prepared to turn a blind eye to some of SOE's questionable activities.

With other military headquarters and commands, SOE cooperated fairly well with Combined Operations Headquarters during the middle years of the war, usually on technical matters as SOE's equipment was readily adopted by commandos and other raiders. This support was lost when Vice Admiral Louis Mountbatten left Combined Operations, though by this time SOE had its own transport and had no need to rely on Combined Operations for resources. On the other hand, the Admiralty objected to SOE developing its own underwater vessels, and the duplication of effort this involved. The Royal Air Force, and in particular RAF Bomber Command under "Bomber" Harris were usually reluctant to allocate aircraft to SOE.

Towards the end of the war, as Allied forces began to liberate territories occupied by the Axis and in which SOE had established resistance forces, SOE also liaised with and to some extent came under the control of the Allied theatre commands. Relationships with Supreme Headquarters Allied Expeditionary Force in north-west Europe (whose commander was General Dwight D. Eisenhower) and South East Asia Command (whose commander was Admiral Louis Mountbatten, already well known to SOE) were generally excellent. However, there were difficulties with the Commanders in Chief in the Mediterranean, partly because of the complaints over impropriety at SOE's Cairo headquarters during 1941 and partly because both the supreme command in the Mediterranean and SOE's establishments were split in 1942 and 1943, leading to divisions of responsibility and authority.

There was tension between SOE and SIS, which the Foreign Office controlled. Where SIS preferred placid conditions in which it could gather intelligence and work through influential persons or authorities, SOE was intended to create unrest and turbulence, and often backed anti-establishment organisations, such as the Communists, in several countries. At one stage, SIS actively hindered SOE's attempts to infiltrate agents into enemy-occupied France.

Even before the United States joined the war, the head of the newly formed Office of the Coordinator of Information (COI), William J. Donovan, had received technical information from SOE and had arranged for some members of his organisation to undergo training at a camp run by SOE in Oshawa in Canada. In early 1942, Donovan's organisation became the Office of Strategic Services. SOE and OSS worked out respective areas of operation: OSS's exclusive sphere included China (including Manchuria), Korea and Australia, the Atlantic islands and Finland. SOE retained India, the Middle East and East Africa, and the Balkans. While the two services both worked in Western Europe, it was expected that SOE would be the leading partner.

In the middle of the war, the relations between SOE and OSS were not often smooth. They established a joint headquarters in Algiers but the officers of the two organisations working there refused to share information with each other. In the Balkans, and Yugoslavia especially, SOE and OSS several times worked at cross-purposes, reflecting their governments' differing (and changing) attitudes to the Partisans and Chetniks. However, in 1944 SOE and OSS successfully pooled their personnel and resources to mount Operation Jedburgh, providing large scale support to the French Resistance following the Normandy landings.

SOE had some nominal contact with the Soviet NKVD, but this was limited to a single liaison officer at each other's headquarters.

After working from temporary offices in Central London, the headquarters of SOE was moved on 31 October 1940 into 64 Baker Street (hence the nickname ""the Baker Street Irregulars""). Ultimately, SOE occupied much of the western side of Baker Street. "Baker Street" became the euphemistic way of referring to SOE. The precise nature of the buildings remained concealed; it had no entry in the telephone directories, and correspondence to external bodies bore service addresses; MO1 (SP) (a War Office branch), NID(Q) (Admiralty), AI10 (Air Ministry), or other fictitious bodies or civilian companies.

SOE maintained a large number of training, research and development or administrative centres. It was a joke that ""SOE"" stood for ""Stately 'omes of England"", after the large number of country houses and estates it requisitioned and used.

The establishments connected with experimentation and production of equipment were mainly concentrated in and around Hertfordshire and were designated by roman numbers. The main weapons and devices research establishments were The Firs, the home of MD1 near Aylesbury in Buckinghamshire (although this was not formally part of SOE), and Station IX at The Frythe, a country house (and former private hotel) outside Welwyn Garden City where, under the cover name of ISRB (Inter Services Research Bureau), SOE developed radios, weapons, explosive devices and booby traps.

Section D originally had a research station at Bletchley Park, which also held the Government Code and Cipher School, until in November 1940 it was decided that it was unwise to conduct codebreaking and explosives experiments on the same site. The establishment moved to Aston House near Stevenage in Hertfordshire and was renamed Station XII. It originally conducted research and development but from 1941 it became a production, storage and distribution centre for devices already developed.

Station XV, at the Thatched Barn near Borehamwood, was devoted to camouflage, which usually meant equipping agents with authentic local clothing and personal effects. Various sub-stations in London were also involved in this task. Station XV and other camouflage sections also devised methods of hiding weapons, explosives or radios in innocuous-seeming items.

Agents also needed identity papers, ration cards, currency and so on. Station XIV, at Briggens House near Roydon in Essex, was originally the home of STS38, a training facility for Polish saboteurs, ) who set up their own forgery section. As the work expanded, it became the central forgery department for SOE and the Poles eventually moved out on 1 April 1942. The technicians at Station XIV included a number of ex-convicts.

The training establishments, and properties used by country sections, were designated by Arabic numbers and were widely distributed. The initial training centres of the SOE were at country houses such as Wanborough Manor, Guildford. Agents destined to serve in the field underwent commando training at Arisaig in Scotland, where they were taught armed and unarmed combat skills by William E. Fairbairn and Eric A. Sykes, former Inspectors in the Shanghai Municipal Police. Those who passed this course received parachute training by STS 51 and 51a situated near Altrincham, Cheshire with the assistance of No.1 Parachute Training School RAF, at RAF Ringway (which later became Manchester Airport). They then attended courses in security and Tradecraft at Group B schools around Beaulieu in Hampshire. Finally, depending on their intended role, they received specialist training in skills such as demolition techniques or Morse code telegraphy at various country houses in England.

SOE's Cairo branch established a commando and parachute training school numbered STS 102 at Ramat David near Haifa. This school trained agents who joined SOE from among the armed forces stationed in the Middle East, and also members of the Special Air Service and Greek Sacred Squadron.

A commando training centre similar to Arisaig and run by Fairbairn was later set up at Oshawa, for Canadian members of SOE and members of the newly created American organisation, the Office of Strategic Services.

A variety of people from all classes and pre-war occupations served SOE in the field. The backgrounds of agents in F Section, for example, ranged from aristocrats such as Polish-born Countess Krystyna Skarbek, and Noor Inayat Khan, the daughter of an Indian Sufi leader, to working class people such as Violette Szabo, with some even reputedly from the criminal underworld.

In most cases, the primary quality required of an agent was a deep knowledge of the country in which he or she was to operate, and especially its language, if the agent was to pass as a native of the country. Dual nationality was often a prized attribute. This was particularly so of France. In other cases, especially in the Balkans, a lesser degree of fluency was required as the resistance groups concerned were already in open rebellion and a clandestine existence was unnecessary. A flair for diplomacy combined with a taste for rough soldiering was more necessary. Some regular army officers proved adept as envoys, although others (such as the former diplomat Fitzroy Maclean or the classicist Christopher Woodhouse) were commissioned only during wartime.

Several of SOE's agents were from the Jewish Parachutists of Mandate Palestine, many of whom were already Émigrés from Nazi or other oppressive or anti-semitic regimes in Europe. Thirty-two of them served as agents in the field, seven of whom were captured and executed.

Exiled or escaped members of the armed forces of some occupied countries were obvious sources of agents. This was particularly true of Norway and the Netherlands. In other cases (such as Frenchmen owing loyalty to Charles de Gaulle and especially the Poles), the agents' first loyalty was to their leaders or governments in exile, and they treated SOE only as a means to an end. This could occasionally lead to mistrust and strained relations in Britain.

The organisation was prepared to ignore almost any contemporary social convention in its fight against the Axis. It employed known homosexuals, people with criminal records (some of whom taught skills such as picking locks) or bad conduct records in the armed forces, Communists and anti-British nationalists. Some of these might have been considered a security risk, but no known case exists of an SOE agent wholeheartedly going over to the enemy. The case of Henri Déricourt is an example in which the conduct of agents was questionable, but it was impossible to establish whether they were acting under secret orders from SOE or MI6.

SOE was also far ahead of contemporary attitudes in its use of women in armed combat. Although women were first considered only as couriers in the field or as wireless operators or administrative staff in Britain, those sent into the field were trained to use weapons and in unarmed combat. Most were commissioned into either the First Aid Nursing Yeomanry (FANY) or the Women's Auxiliary Air Force. Women often assumed leadership roles in the field. Pearl Witherington became the organiser (leader) of a "highly successful" resistance network in France. Others such as Odette Hallowes or Violette Szabo were decorated for bravery, posthumously in Szabo's case. Of SOE's 42 female agents serving in Section F (France) sixteen did not survive with thirteen killed or executed in Nazi concentration camps.

Most of the resistance networks which SOE formed or liaised with were controlled by radio directly from Britain or one of SOE's subsidiary headquarters. All resistance circuits contained at least one wireless operator, and all drops or landings were arranged by radio, except for some early exploratory missions sent "blind" into enemy-occupied territory.

At first, SOE's radio traffic went through the SIS-controlled radio station at Bletchley Park. From 1 June 1942 SOE used its own transmitting and receiving stations at Grendon Underwood in Buckinghamshire and Poundon nearby, as the location and topography were suitable. Teleprinters linked the radio stations with SOE's HQ in Baker Street. Operators in the Balkans worked to radio stations in Cairo.

SOE was highly dependent upon the security of radio transmissions, involving three factors: the physical qualities and capabilities of the radio sets, the security of the transmission procedures and the provision of proper ciphers.

SOE's first radios were supplied by SIS. They were large, clumsy and required large amounts of power. SOE acquired a few, much more suitable, sets from the Poles in exile, but eventually designed and manufactured their own, such as the Paraset, under the direction of Lieutenant Colonel F. W. Nicholls R. Sigs who had served with Gubbins between the wars. The A Mk III, with its batteries and accessories, weighed only , and could fit into a small attache case, although the B Mk II, otherwise known as the B2, which weighed , was required to work over ranges greater than about .

Operating procedures were insecure at first. Operators were forced to transmit verbose messages on fixed frequencies and at fixed times and intervals. This allowed German direction finding teams time to triangulate their positions. After several operators were captured or killed, procedures were made more flexible and secure. The SOE wireless operators were also known as "The Pianists".

As with their first radio sets, SOE's first ciphers were inherited from SIS. Leo Marks, SOE's chief cryptographer, was responsible for the development of better codes to replace the insecure poem codes. Eventually, SOE settled on single use ciphers, printed on silk. Unlike paper, which would be given away by rustling, silk would not be detected by a casual search if it was concealed in the lining of clothing.

The BBC also played its part in communications with agents or groups in the field. During the war, it broadcast to almost all Axis-occupied countries, and was avidly listened to, even at risk of arrest. The BBC included various "personal messages" in its broadcasts, which could include lines of poetry or apparently nonsensical items. They could be used to announce the safe arrival of an agent or message in London for example, or could be instructions to carry out operations on a given date. These were used for example to mobilise the resistance groups in the hours before Operation Overlord.

In the field, agents could sometimes make use of the postal services, though these were slow, not always reliable and letters were almost certain to be opened and read by the Axis security services. In training, agents were taught to use a variety of easily available substances to make invisible ink, though most of these could be detected by a cursory examination, or to hide coded messages in apparently innocent letters. The telephone services were even more certain to be intercepted and listened to by the enemy, and could be used only with great care.

The most secure method of communication in the field was by courier. In the earlier part of the war, most women sent as agents in the field were employed as couriers, on the assumption that they would be less likely to be suspected of illicit activities.

Although SOE used some suppressed assassination weapons such as the De Lisle carbine and the Welrod (specifically developed for SOE at Station IX), it took the view that weapons issued to resisters should not require extensive training in their use, or need careful maintenance. The crude and cheap Sten was a favourite. For issue to large forces such as the Yugoslav Partisans, SOE used captured German or Italian weapons. These were available in large quantities after the Tunisian and Sicilian campaigns and the surrender of Italy, and the partisans could acquire ammunition for these weapons (and the Sten) from enemy sources.

SOE also adhered to the principle that resistance fighters would be handicapped rather than helped by heavy equipment such as mortars or anti-tank guns. These were awkward to transport, almost impossible to conceal and required skilled and highly trained operators. Later in the war however, when resistance groups staged open rebellions against enemy occupation, some heavy weapons were dispatched, for example to the Maquis du Vercors. Weapons such as the British Army's standard Bren light machine gun were also supplied in such cases.

Most SOE agents received training on captured enemy weapons before being sent into enemy-occupied territory. Ordinary SOE agents were also armed with handguns acquired abroad, such as, from 1941, a variety of US pistols, and a large quantity of the Spanish Llama .38 ACP in 1944. Such was SOE's demand for weapons, a consignment of 8,000 Ballester–Molina .45 calibre weapons was purchased from Argentina, apparently with the mediation of the US.

SOE agents were issued with the Fairbairn–Sykes fighting knife also issued to Commandos. For specialised operations or use in extreme circumstances, SOE issued small fighting knives which could be concealed in the heel of a hard leather shoe or behind a coat lapel. Given the likely fate of agents captured by the Gestapo, SOE also disguised suicide pills as coat buttons.

SOE developed a wide range of explosive devices for sabotage, such as limpet mines, shaped charges and time fuses, which were also widely used by commando units. Most of these devices were designed and produced at The Firs. The Time Pencil, invented by Commander A.J.G. Langley, the first commandant of Station XII at Aston was used to give a saboteur time to escape after setting a charge and was far simpler to carry and use than lighted fuses or electrical detonators. It relied on crushing an internal vial of acid which then corroded a retaining wire, which sometimes made it inaccurate in cold or hot conditions. Later the L-Delay, which instead allowed a lead retaining wire to "creep" until it broke and was less affected by the temperature, was introduced.

SOE pioneered the use of plastic explosive. (The term "plastique" comes from plastic explosive packaged by SOE and originally destined for France but taken to the United States instead.) Plastic explosive could be shaped and cut to perform almost any demolition task. It was also inert and required a powerful detonator to cause it to explode, and was therefore safe to transport and store. It was used in everything from car bombs, to exploding rats designed to destroy coal-fired boilers.

Other, more subtle sabotage methods included lubricants laced with grinding materials, intended for introduction into vehicle oil systems, railway wagon axle boxes, etc., incendiaries disguised as innocuous objects, explosive material concealed in coal piles to destroy locomotives, and land mines disguised as cow or elephant dung. On the other hand, some sabotage methods were extremely simple but effective, such as using sledgehammers to crack cast-iron mountings for machinery.

Station IX developed several miniature submersible craft. The Welman submarine and "Sleeping Beauty" were offensive weapons, intended to place explosive charges on or adjacent to enemy vessels at anchor. The Welman was used once or twice in action, but without success. The Welfreighter was intended to deliver stores to beaches or inlets, but it too was unsuccessful.

A sea trials unit was set up in West Wales at Goodwick, by Fishguard (station IXa) where these craft were tested. In late 1944 craft were dispatched to Australia to the Allied Intelligence Bureau (SRD), for tropical testing.

SOE also revived some medieval devices, such as the caltrop, which could be used to burst the tyres of vehicles or injure foot soldiers and crossbows powered by multiple rubber bands to shoot incendiary bolts. There were two types, known as ""Big Joe"" and ""Lil Joe"" respectively. They had tubular alloy skeleton stocks and were designed to be collapsible for ease of concealment.

An important section of SOE was the Operation Research and Trials Section, which was formally established in August 1943. The section had the responsibility both for issuing formal requirements and specifications to the relevant development and production sections, and for testing prototypes of the devices produced under field conditions. Over the period from 1 November 1943 to 1 November 1944, the section tested 78 devices. Some of these were weapons such as the Sleeve gun or fuses or adhesion devices to be used in sabotage, others were utility objects such as waterproof containers for stores to be dropped by parachute, or night glasses (lightweight binoculars with plastic lenses). Of the devices tested, 47% were accepted for use with little or no modification, 31% were accepted only after considerable modification and the remaining 22% were rejected.

Before SOE's research and development procedures were formalised in 1943, a variety of more or less useful devices were developed. Some of the more imaginative devices invented by SOE included exploding pens with enough explosive power to blast a hole in the bearer's body, or guns concealed in tobacco pipes, though there is no record of any of these being used in action. Station IX developed a miniature folding motorbike (the "Welbike") for use by parachutists, though this was noisy and conspicuous, used scarce petrol and was of little use on rough ground.

The continent of Europe was largely closed to normal travel. Although it was possible in some cases to cross frontiers from neutral countries such as Spain or Sweden, it was slow and there were problems over violating these countries' neutrality. SOE had to rely largely on its own air or sea transport for movement of people, arms and equipment.

SOE never had its own air force, but had to rely on the RAF for its planes. It was engaged in disputes with the RAF from its early days. In January 1941, an intended ambush (Operation Savanna) against the aircrew of a German "pathfinder" air group near Vannes in Brittany was thwarted when Air Vice Marshal Charles Portal, the Chief of the Air Staff, objected on moral grounds to parachuting what he regarded as assassins, although Portal's objections were later overcome and "Savanna" was mounted, unsuccessfully. From 1942, when Air Marshal Arthur Harris (""Bomber Harris"") became the Commander-in-Chief of RAF Bomber Command, he consistently resisted the diversion of the most capable types of bombers to SOE purposes.

SOE's first aircraft were two Armstrong Whitworth Whitleys belonging to 419 Flight RAF, which was formed in September 1940. In 1941, the flight was expanded to become No. 138 Squadron RAF. In February 1942, they were joined by No. 161 Squadron RAF. 161 Squadron flew agent insertions and pick-ups, while 138 Squadron delivered arms and stores by parachute. "C" flight from No. 138 Squadron later became No. 1368 Flight of the Polish Air Force, which joined No. 624 Squadron flying Halifaxes in the Mediterranean. By the later stages of the war several United States Army Air Forces squadrons were operating Douglas C-47 Skytrains in the Mediterranean, although by this time their operations had passed from SOE proper to the "Balkan Air Terminal Service", and three Special Duties squadrons operating in the Far East

Nos. 161 and 138 Squadrons were based at RAF Tempsford in Bedfordshire though No. 161 Squadron often moved forward to RAF Tangmere, close to the coast in West Sussex, to shorten their flights. The airfield at Tempsford became the RAF's most secret base. (Tempsford had been rejected for Bomber Command's purposes by Harris in March 1942, as it frequently became waterlogged.) RAF Tempsford was designed to look like an ordinary working farm. The SOE used Tangmere Cottage, opposite the main entrance to the base. SOE agents were lodged in a local hotel before being ferried to farm buildings, the "Gibraltar Farm" within the airfield's perimeter track. After final briefings and checks at the farm, the agents were issued firearms in the barn, and then boarded a waiting aircraft.

The squadrons' first task was to take agents to France who could select suitable fields for their aircraft. Most of these agents were French expatriates, some of whom had been pilots in the French Armée de l'Air. Once the agent was in place and had selected a number of potential fields, 161 Squadron delivered SOE agents, wireless equipment and operators and weapons, and flew French political leaders, resistance leaders or their family members, and downed allied airmen to Britain. Between them, the two squadrons transported 101 agents to, and recovered 128 agents, diplomats and airmen from occupied France.

161 Squadron's principal aircraft was the Westland Lysander. It handled very well at low speed and could use landing grounds only long. It had an effective range of , and could carry one to three passengers in the rear cockpit and stores in a pannier underneath the fuselage. It was flown by a single pilot, who also had to navigate, so missions had to be flown on clear nights with a full or near full moon. Bad weather often thwarted missions, German night fighters were also a hazard, and pilots could never know when landing whether they would be greeted by the resistance or the Gestapo.

The procedure once a Lysander reached its destination in France was described by Squadron Leader Hugh Verity. Once the aircraft reached the airfield the agent on the ground would signal the aircraft by flashing a prearranged code letter in Morse. The aircraft would respond by blinking back the appropriate code response letter. The agent and his men would then mark the field by lighting the three landing lights, which were flashlights attached to poles. The "A" lamp was at the base of the landing ground. 150 metres beyond it and into the wind was the "B" light, and 50 metres to the right of "B" was the "C" light. The three lights formed an inverted "L", with the "B" and "C" lights upwind from "A". With the code passed the pilot would land the aircraft. He then would taxi back to the "A" lamp, where the passengers would clamber down the fixed ladder to the ground, often while the pilot was making a slow U-turn. Before leaving the last passenger would hand off the luggage and then take aboard the outgoing luggage before climbing down the ladder as well. Then the outgoing passengers would climb aboard and the aircraft would take off. The whole exchange might take as little as 3 minutes.

The Lockheed Hudson had a range greater and could carry more passengers (ten or more), but required landing strips three times as long as those needed for the Lysander (450 yards vs. 150 yards). It carried a navigator, to ease the load on the pilot, and could also be fitted with navigational equipment such as the "Rebecca" homing receiver. The Hudson's use with 161 Squadron was developed by Charles Pickard and Hugh Verity. Pickard determined that the Hudson's stall speed was actually some 20 mph slower than its manual stated. Before it was first used on 13 January 1943, 161 Squadron had to send two Lysander aircraft in what they termed "a double" if larger parties needed to be picked up.

No. 138 Squadron's primary mission was the delivery of equipment, and occasionally agents, by parachute. It flew a variety of bomber-type aircraft, often modified with extra fuel tanks and flame-suppressing exhaust shrouds: the Armstrong Whitworth Whitley until November 1942, the Handley Page Halifax and later the Short Stirling. The Stirling could carry a very large load, but the aircraft with the longest range was the Halifax, which when based in Italy could reach drop zones as far away as eastern Poland. Later in the war, some RAF Special Duties units used the very long-range Consolidated B-24 Liberator.

Stores were usually parachuted in cylindrical containers. The "C" type was long, and when fully loaded could weigh up to . The "H" type was the same size overall but could be broken down into five smaller sections. This made it easier to carry and conceal but it could not be loaded with longer loads such as rifles. Some inert stores such as boots and blankets were "free-dropped" i.e. simply thrown out of the aircraft bundled together without a parachute, often to the hazard of any receiving committee on the ground.

Some devices used by SOE were designed specifically to guide aircraft to landing strips and dropping zones. Such sites could be marked by an agent on the ground with bonfires or bicycle lamps, but this required good visibility, as the pilot or navigator of a plane had not only to spot the ground signals, but also to navigate by visible landmarks to correct dead reckoning. Many landings or drops were thwarted by bad weather. To overcome these problems, SOE and Allied airborne forces used the Rebecca/Eureka transponding radar, which enabled a Hudson or larger aircraft to home in on a point on the ground even in thick weather. It was however difficult for agents or resistance fighters to carry or conceal the ground-based "Eureka" equipment.

SOE also developed the S-Phone, which allowed a pilot or radio operator aboard an aircraft to communicate by voice with the "reception committee". Sound quality was good enough for voices to be recognisable, so that a mission could be aborted in case of any doubts of an agent's identity.

SOE also experienced difficulties with the Royal Navy, who were usually unwilling to allow SOE to use its submarines or motor torpedo boats to deliver agents or equipment. Submarines were regarded as too valuable to risk within range of enemy coastal defences. They could also carry only small numbers of agents, in great discomfort, and could disembark stores only in small dinghies or canoes, which made it difficult to land large quantities of equipment. SOE nevertheless used them in the Indian Ocean where the distances made it impracticable to use any smaller craft.

The vessels used by SOE during the early part of the war were clandestine craft such as fishing boats or caiques. They could pass muster as innocent local craft and carry large quantities of stores. They also had the advantage of being largely outside Admiralty control. However, SOE's first small craft organisation, which was set up in the Helford estuary, suffered from obstruction from SIS, which had a similar private navy nearby. Eventually, in spring 1943, the Admiralty created a Deputy Director of Operations (Irregular), to superintend all such private navies. This officer turned out to be the former commander of SIS's craft in the Helford estuary, but his successor in charge of SIS's Helford base cooperated much better with SOE's flotilla. Even so, while SIS and SOE (and MI9) landed and embarked several dozen agents, refugees and allied aircrew, it was impossible to transport large quantities of arms and equipment inland from beaches in heavily patrolled coastal areas, until France was almost liberated.

After the German occupation of Norway, many Norwegian merchant seamen and fishermen made their way to Britain. SOE recruited several to maintain communications to Norway, using fishing boats from a base in the Shetland Islands. The service became so reliable that it became known as the Shetland Bus. One of its boats and crews launched a daring but unsuccessful attack ("Operation Title") against the German battleship Tirpitz. A similar organisation ran missions to occupied Denmark (and neutral Sweden) from the East Coast of Britain.

The "Shetland Bus" was unable to operate only during the very long hours of daylight in the arctic summer, because of the risk that the slow fishing boats would be attacked by patrolling German aircraft. Late in the war, the unit acquired three fast Submarine chasers for such missions. About the same time, SOE also acquired MTBs and Motor Gun Boats for the Helford flotilla.

SOE also ran feluccas from Algiers into southern France and Corsica, and some caiques in the Aegean.

In France, most agents were directed by two London-based country sections. F Section was under SOE control, while RF Section was linked to Charles de Gaulle's Free French Government in exile. Most native French agents served in RF. Two smaller sections also existed: EU/P Section, which dealt with the Polish community in France, and the DF Section which was responsible for establishing escape routes. During the latter part of 1942 another section known as AMF was established in Algiers, to operate into Southern France.

On 5 May 1941 Georges Bégué (1911–1993) became the first SOE agent dropped into German occupied France. Between Bégué's first drop in May 1941 and August 1944, more than four hundred F Section agents were sent into occupied France, with Andrée Borrel (1919–1944) being the first woman parachuted into France on 24 September 1942. They served in a variety of functions including arms and sabotage instructors, couriers, circuit organisers, liaison officers and radio operators. RF sent about the same number; AMF sent 600 (although not all of these belonged to SOE). EU/P and DF sent a few dozen agents each.

Some networks were compromised, with the loss of many agents. In particular agents continued to be sent to the "Prosper" network for some time after it had been controlled by Germans. The head of F Section, Maurice Buckmaster was blamed by many for the loss, and overwork may have played a part.

To support the Allied invasion of France on D Day in June 1944 three-man parties were dropped into various parts of France as part of Operation Jedburgh, to co-ordinate widespread overt (as opposed to clandestine) acts of resistance. A total of 100 men were eventually dropped, together with 6,000 tons of military stores (4,000 tons had been dropped during the years before D-Day). At the same time, all the various sections operating in France (except EU/P) were nominally placed under a London-based HQ titled État-major des Forces Françaises de l'Intérieur (EMFFI).

It was to take many weeks for a full assessment of the contributions of the Jedburgh teams to the Allied landings in Normandy, but when it came it vindicated Gubbins’ belief that careful planned sabotage could cripple a modern army. General Eisenhower’s staff at the Supreme Headquarters of the Allied Expeditionary Force said that the Jedburghs had "succeeded in imposing more or less serious delays on all the division moved to Normandy". This had prevented Hitler from striking back in the crucial opening hours of Operation Overlord. Eisenhower’s staff singled out the work of Tommy Macpherson and his two comrades-in-arms for particular praise. The most “outstanding example was the delay to the 2nd SS Panzer Division”, they said, and added a very personal endorsement, agreeing that the work carried out under Gubbins’ leadership played a “very considerable part in our complete and final victory”.

SOE did not need to instigate Polish resistance, because unlike the Vichy French the Poles overwhelmingly refused to collaborate with the Nazis. Early in the war the Poles established the Polish Home Army, led by a clandestine resistance government known as the Polish Secret State. Nevertheless, many members of SOE were Polish and the SOE and the Polish resistance cooperated extensively.

SOE assisted the Polish government in exile with training facilities and logistical support for its 605 special forces operatives known as the Cichociemni, or ""The Dark and Silent"". Members of the unit, which was based in Audley End House, Essex, were rigorously trained before being parachuted into occupied Poland. Because of the distance involved in air travel to Poland, customised aircraft with extra fuel capacity were used in Polish operations such as Operation Wildhorn III. Sue Ryder chose the title Baroness Ryder of Warsaw in honour of these operations.

Secret Intelligence Service member Krystyna Skarbek ("nom de guerre" Christine Granville) was a founder member of SOE and helped establish a cell of Polish spies in Central Europe. She ran several operations in Poland, Egypt, Hungary (with Andrzej Kowerski) and France, often using the staunchly anti-Nazi Polish expatriate community as a secure international network. Non-official cover agents Elzbieta Zawacka and Jan Nowak-Jezioranski perfected the Gibraltar courier route out of occupied Europe. Maciej Kalenkiewicz was parachuted into occupied Poland, only to be killed by the Soviets. A Polish agent was integral to SOE's Operation Foxley, the plan to assassinate Hitler.

Thanks to co-operation between SOE and the Polish Home Army, the Poles were able to deliver the first Allied intelligence on the Holocaust to London in June 1942. Witold Pilecki of the Polish Home Army designed a joint operation with SOE to liberate Auschwitz, but the British rejected it as infeasible. Joint Anglo-Polish operations provided London with vital intelligence on the V-2 rocket, German troops movements on the Eastern Front, and the Soviet repressions of Polish citizens.

RAF 'Special Duties Flights' were sent to Poland to assist the Warsaw uprising against the Nazis. The rebellion was defeated with a loss of 200,000 casualties (mostly German executions of Polish civilians) after the nearby Red Army refused military assistance to the Polish Home Army. RAF Special Duties Flights were refused landing rights at Soviet-held airfields near Warsaw, even when requiring emergency landings after battle damage. These flights were also attacked by Soviet fighters, despite the USSR's officially Allied status.

Due to the dangers and lack of friendly population few operations were conducted in Germany itself. The German and Austrian section of SOE was run by Lieutenant Colonel Ronald Thornley for most of the war, and was mainly involved with black propaganda and administrative sabotage in collaboration with the German section of the Political Warfare Executive. After D-Day, the section was re-organised and enlarged with Major General Gerald Templer heading the Directorate, with Thornley as his deputy.

Several major operations were planned, including Operation Foxley, a plan to assassinate Hitler, and Operation Periwig, an ingenious plan to simulate the existence of a large-scale anti-Nazi resistance movement within Germany. "Foxley" was never carried out but "Periwig" went ahead despite restrictions placed on it by SIS and SHAEF. Several German prisoners of war were trained as agents, briefed to make contact with the anti-Nazi resistance and to conduct sabotage. They were then parachuted into Germany in the hope that they would either hand themselves in to the "Gestapo" or be captured by them, and reveal their supposed mission. Fake coded wireless transmissions were broadcast to Germany and various pieces of agent paraphernalia such as code books and wireless receivers were allowed to fall into the hands of the German authorities.

Section N of SOE ran operations in the Netherlands. They committed some of SOE's worst blunders in security, which allowed the Germans to capture many agents and much sabotage material, in what the Germans called the 'Englandspiel'. SOE apparently ignored the absence of security checks in radio transmissions, and other warnings from their chief cryptographer, Leo Marks, that the Germans were running the supposed resistance networks. A total of 50 agents were caught and brought to Camp Haaren in the South of the Netherlands.

Five captured men managed to escape from the camp. Two of them, Pieter Dourlein and Ben Ubbink, escaped on 29 August 1943 and found their way to Switzerland. There, the Netherlands Embassy sent messages over their controlled sets to England that SOE Netherlands was compromised. SOE set up new elaborate networks, which continued to operate until the Netherlands were liberated at the end of the war.

From September 1944 to April 1945, eight Jedburgh teams were also active in the Netherlands. The first team, code named "Dudley" was parachuted into the east of the Netherlands one week before Operation Market Garden. The next four teams were attached to the Airborne forces that carried out Market Garden. After the failure of Market Garden, one Jedburgh team trained (former) resistance men in the liberated South of the Netherlands. In April 1945 the last two Dutch Jedburgh teams became operational. One team code named "Gambling", was a combined Jedburgh/Special Air Service (SAS) group that was dropped into the centre of the Netherlands to assist the Allied advance. The last team was parachuted into the Northern Netherlands as part of SAS operation "Amherst". Despite the fact that operating in the flat and densely populated Netherlands was very difficult for the Jedburghs, the teams were quite successful.

Section T established some effective networks in Belgium, in part orchestrated by fashion designer Hardy Amies, who rose to the rank of lieutenant colonel. Amies adapted names of fashion accessories for use as code words, while managing some of the most murderous and ruthless agents in the field. The rapid liberation of the country by Allied forces in September 1944 provided the resistance with little time to stage an uprising. They did assist the Allies to bypass German rearguards, and enabled the Allies to capture the vital Port of Antwerp intact. 

After Brussels was liberated, Amies outraged his superiors by setting up a "Vogue" photo-shoot in Belgium. In 1946, he was knighted in Belgium for his service with SOE, being named an officer of the Order of the Crown.

As both an enemy country, and supposedly a monolithic fascist state with no organised opposition which SOE could use, SOE made little effort in Italy before mid-1943, when Mussolini's government collapsed and Allied forces already occupied Sicily. In April 1941, in a mission codenamed "Yak", Peter Fleming attempted to recruit agents from among the many thousands of Italian prisoners of war captured in the Western Desert Campaign. He met with no response. Attempts to search among Italian immigrants in the United States, Britain and Canada for agents to be sent to Italy had similarly poor results.

During the first three years of war, the most important "episode" of the collaboration between SOE and Italian anti-fascism was a project of an anti-fascist uprising in Sardinia, which the SOE supported at some stage but did not receive approval from the Foreign Office.

In the aftermath of the Italian collapse, SOE (in Italy renamed No. 1 Special Force) helped build a large resistance organisation in the cities of Northern Italy, and in the Alps. Italian partisans harassed German forces in Italy throughout the autumn and winter of 1944, and in the Spring 1945 offensive in Italy they captured Genoa and other cities unaided by Allied forces. SOE helped the Italian Resistance send British missions to the partisan formations and supply war material to the bands of patriots, a supply made without political prejudices, and which also helped the Communist formations (Brigate Garibaldi).

Late in 1943, SOE established a base at Bari in Southern Italy, from which they operated their networks and agents in the Balkans. This organisation had the codename ""Force 133"". This later became ""Force 266"", reserving 133 for operations run from Cairo rather than the heel of Italy. Flights from Brindisi were run to the Balkans and Poland, particularly once control had been wrested from SOE's Cairo headquarters and was exercised directly by Gubbins. SOE established a new packing station for the parachute containers close to Brindisi Air base, along the lines of those created at Saffron Walden. This was ME 54, a factory employing hundreds, the American (OSS) side of which was known as "Paradise Camp".

In the aftermath of the German invasion in 1941, the Kingdom of Yugoslavia fragmented. Croatia, had a substantial pro-Axis movement, the Ustaše. In Croatia as well as the remainder of Yugoslavia, two resistance movements formed; the royalist Chetniks under Draža Mihailović, and the Communist Partisans under Josip Broz Tito.

Mihailović was the first to attempt to contact the Allies, and SOE despatched a party on 20 September 1941 under Major "Marko" Hudson. Hudson also encountered Tito's forces. Through the royalist government in exile, SOE at first supported the Chetniks. Eventually, however, due to reports that the Chetniks were less effective and even collaborating with German and Italian forces on occasion, British support was redirected to the Partisans, even before the Tehran Conference in 1943.

Although relations were often touchy throughout the war, it can be argued that SOE's unstinting support was a factor in Yugoslavia's maintaining a neutral stance during the Cold War. However, accounts vary dramatically between all historical works on the ""Chetnik controversy"".

SOE was unable to establish links or contacts in Hungary before the regime of Miklós Horthy aligned itself with the Axis Powers. Distance and lack of such contacts prevented any effort being made by SOE until the Hungarians themselves dispatched a diplomat (László Veress) in a clandestine attempt to contact the Western Allies. SOE facilitated his return, with some radio sets. Before the Allied governments could agree terms, Hungary was placed under German military occupation and Veress was forced to flee the country.

Two missions subsequently dropped "blind" i.e. without prior arrangement for a reception party, failed. So too did an attempt by Basil Davidson to incite a partisan movement in Hungary, after he made his way there from northeastern Yugoslavia.

Greece was overrun by the Axis after a desperate defence lasting several months. In the aftermath, SIS and another intelligence organisation, SIME, discouraged attempts at sabotage or resistance as this might imperil relations with Turkey, although SOE maintained contacts with resistance groups in Crete. When an agent, "Odysseus", a former tobacco-smuggler, attempted to contact potential resistance groups in Greece, he reported that no group was prepared to co-operate with the monarchist government in exile in Cairo.

In late 1942, at the army's instigation, SOE mounted its first operation, codenamed Operation Harling, into Greece in an attempt to disrupt the railway which was being used to move materials to the German Panzer Army Africa. A party under Colonel (later Brigadier) Eddie Myers, assisted by Christopher Woodhouse, was parachuted into Greece and discovered two guerrilla groups operating in the mountains: the pro-Communist ELAS and the republican EDES. On 25 November 1942, Myers's party blew up one of the spans of the railway viaduct at Gorgopotamos, supported by 150 Greek partisans from these two organisations who engaged Italians guarding the viaduct. This cut the railway linking Thessaloniki with Athens and Piraeus.

Relations between the resistance groups and the British soured. When the British needed once again to disrupt the railway across Greece as part of the deception operations preceding Operation Husky, the Allied invasion of Sicily, the resistance groups refused to take part, rightly fearing German reprisals against civilians. Instead, a six-man commando party from the British and New Zealand armies, led by New Zealander Lieutenant Colonel Cecil Edward Barnes a civil engineer, carried out the destruction of the Asopos viaduct on 21 June 1943. Two attempts by Mike Cumberlege to make the Corinth Canal unnavigable ended in failure.

EDES received most aid from SOE, but ELAS secured many weapons when Italy collapsed and Italian military forces in Greece dissolved. ELAS and EDES fought a vicious civil war in 1943 until SOE brokered an uneasy armistice (the Plaka agreement).

A lesser known, but important function of the SOE in Greece was to inform the Cairo headquarters of the movement of the German military aircraft that were serviced and repaired at the two former Greek military aircraft facilities in and around Athens.

Eventually, the British Army occupied Athens and Piraeus in the aftermath of the German withdrawal, and fought a street-by-street battle to drive ELAS from these cities and impose an interim government under Archbishop Damaskinos. SOE's last act was to evacuate several hundred disarmed EDES fighters to Corfu, preventing their massacre by ELAS.

Several resistance groups and Allied stay-behind parties operated in Crete after the Germans occupied the island in the Battle of Crete. SOE's operations involved figures such as Patrick Leigh Fermor, John Lewis, Harry Rudolph Fox Burr, Tom Dunbabin, Sandy Rendel, John Houseman, Xan Fielding and Bill Stanley Moss. Some of the most famous moments included the abduction of General Heinrich Kreipe led by Leigh Fermor and Moss – subsequently portrayed in the film "Ill Met by Moonlight", and the sabotage of Damasta led by Moss.

Albania had been under Italian influence since 1923, and was occupied by the Italian Army in 1939. In 1943, a small liaison party entered Albania from northwestern Greece. SOE agents who entered Albania then or later included Julian Amery, Anthony Quayle, David Smiley and Neil "Billy" McLean. They discovered another internecine war between the Communist partisans under Enver Hoxha, and the republican Balli Kombëtar. As the latter had collaborated with the Italian occupiers, Hoxha gained Allied support.

SOE's envoy to Albania, Brigadier Edmund "Trotsky" Davies, was captured by the Germans early in 1944. Some SOE officers warned that Hoxha's aim was primacy after the war, rather than fighting Germans. They were ignored, but Albania was never a major factor in the effort against the Germans.

SOE sent many missions into the Czech areas of the so-called Protectorate of Bohemia and Moravia, and later into Slovakia. The most famous mission was Operation Anthropoid, the assassination of SS-Obergruppenführer Reinhard Heydrich in Prague. From 1942 to 1943 the Czechoslovaks had their own Special Training School (STS) at Chicheley Hall in Buckinghamshire. In 1944, SOE sent men to support the Slovak National uprising.

In March 1941 a group performing commando raids in Norway, Norwegian Independent Company 1 (NOR.I.C.1) was organised under leadership of Captain Martin Linge. Their initial raid in 1941 was Operation Archery, the best known raid was probably the Norwegian heavy water sabotage. Communication lines with London were gradually improved so that by 1945, 64 radio operators were spread throughout Norway.

Most of the actions by the Danish resistance were railway sabotage to hinder German troop and material movements from and to Norway; however, sabotage on a much larger scale also occurred, especially by BOPA. In all the Danish resistance conducted 1,000 operations from 1942 and onwards.

In October 1943 the Danish resistance also saved nearly all of the Danish Jews from certain death in German concentration camps. This was a massive overnight operation and is to this day recognised among Jews as one of the most significant displays of public defiance against the Germans.

The Danish resistance assisted SOE in its activities in neutral Sweden. For example, SOE was able to obtain several shiploads of vital ball-bearings which had been interned in Swedish ports. The Danes also pioneered several secure communications methods; for example, a burst transmitter/receiver which transcribed Morse code onto a paper tape faster than a human operator could handle.

In 1943 an SOE delegation was parachuted into Romania to instigate resistance against the Nazi occupation at "any cost" (Operation Autonomous). The delegation, including Colonel Gardyne de Chastelain, Captain Silviu Meţianu and Ivor Porter, was captured by the Romanian Gendarmerie and held until the night of King Michael's Coup on 23 August 1944.

Abyssinia was the scene of some of SOE's earliest and most successful efforts. SOE organised a force of Ethiopian irregulars under Orde Charles Wingate in support of the exiled Emperor Haile Selassie. This force (named Gideon Force by Wingate) caused heavy casualties to the Italian occupation forces, and contributed to the successful British campaign there. Wingate was to use his experience to create the Chindits in Burma.

The neutral Spanish island of Fernando Po was the scene of Operation Postmaster, one of SOE's most successful exploits. The large Italian merchant vessel "Duchessa d'Aosta" and the German tug "Likomba" had taken refuge in the harbour of Santa Isabel. On 14 January 1942, while the ships' officers were attending a party ashore thrown by an SOE agent, commandos and SOE personnel led by Gus March-Phillipps boarded the two vessels, cut the anchor cables and towed them out to sea, where they later rendezvoused with Royal Navy ships. Several neutral authorities and observers were impressed by the British display of ruthlessness.

As early as 1940, SOE was preparing plans for operations in Southeast Asia. As in Europe, after initial Allied military disasters, SOE built up indigenous resistance organisations and guerrilla armies in enemy (Japanese) occupied territory. SOE also launched ""Operation Remorse"" (1944–45), which was ultimately aimed at protecting the economic and political status of Hong Kong. Force 136 engaged in covert trading of goods and currencies in China. Its agents proved remarkably successful, raising £77m through their activities, which were used to provide assistance for Allied prisoners of war and, more controversially, to buy influence locally to facilitate a smooth return to pre-war conditions.

In late 1944, as it became clear that the war would soon be over, Lord Selborne advocated keeping SOE or a similar body in being, and that it would report to the Ministry of Defence. Anthony Eden, the Foreign Secretary, insisted that his ministry, already responsible for the SIS, should control SOE or its successors. The Joint Intelligence Committee, which had a broad co-ordinating role over Britain's intelligence services and operations, took the view that SOE was a more effective organisation than the SIS but that it was unwise to split the responsibility for espionage and more direct action between separate ministries, or to perform special operations outside the ultimate control of the Chiefs of Staff. The debate continued for several months until on 22 May 1945, Selborne wrote:

Churchill took no immediate decision, and after he lost the general election on 5 July 1945, the matter was dealt with by the Labour Prime Minister, Clement Attlee. Selborne told Attlee that SOE still possessed a worldwide network of clandestine radio networks and sympathisers. Attlee replied that he had no wish to own a British Comintern, and closed Selborne's network down at 48 hours' notice.

SOE was dissolved officially on 15 January 1946. Some of its senior staff moved easily into financial services in the City of London, although some of them had not lost their undercover mentality and did little for the City's name. Most of SOE's other personnel reverted to their peacetime occupations or regular service in the armed forces, but 280 of them were taken into the ""Special Operations Branch"" of MI6. Some of these had served as agents in the field, but MI6 was most interested in SOE's training and research staff. Sir Stewart Menzies, the head of MI6 (who was generally known simply as "C") soon decided that a separate Special Operations branch was unsound, and merged it into the general body of MI6.

Gubbins, the last director, was not given further employment by the Army, but he later founded the Special Forces Club for former members of SOE and similar organisations.

Although the wartime British government considered the activities of the SOE to be lawful, the German invaders, as in World War I and the War of 1870, argued that those engaging in resistance (local resistance fighters and the agents of foreign governments who supported them) were “bandits” and “terrorists”, maintaining that all "Francs-tireurs" (and said agents) were engaging in an illegal form of warfare, and, as such, had no legal rights. A view expressed by Fritz Sauckel, the General Plenipotentiary for Labour Deployment, making him the man in charge of bringing workers to the factories in Germany for forced labour, who demanded the flight of young French men to the countryside be stopped and called the "maquis" “terrorists”, “bandits” and “criminals” for their opposition to lawful authority.

The mode of warfare encouraged and promoted by SOE is considered by several modern commentators to have established the modern model that many alleged terrorist organisations emulate.

Two opposed views were quoted by Tony Geraghty in "The Irish War: The Hidden Conflict Between the IRA and British Intelligence". M. R. D. Foot, who wrote several official histories of SOE wrote:
However the British military historian John Keegan wrote:
Another, later view, on the moral contribution of SOE, was expressed by Max Hastings:

Since the end of the war, the SOE has appeared in many films, comics, books, and television.









</doc>
<doc id="28899" url="https://en.wikipedia.org/wiki?curid=28899" title="System request">
System request

System request (often abbreviated SysRq or Sys Req) is a key on personal computer keyboards that has no standard use. Introduced by IBM with the PC/AT, it was intended to be available as a special key to directly invoke low-level operating system functions with no possibility of conflicting with any existing software. A special BIOS routine – software interrupt 0x15, subfunction 0x85 – was added to signal the OS when SysRq was pushed or released. Unlike most keys, when it is pressed nothing is stored in the keyboard buffer.

The specific low level function that the SysRq key was meant for was to switch between operating systems. When the original IBM-PC was created in 1980, there were three leading competing operating systems: PC DOS, CP/M-86, and UCSD p-System, while Xenix was added in 1983–1984. The SysRq key was added so that multiple operating systems could be run on the same computer, making use of the capabilities of the 286 chip in the PC/AT.

A special key was needed because most software of the day operated at a low level, often bypassing the OS entirely, and typically made use of many hotkey combinations. The use of Terminate and Stay Resident (TSR) programs further complicated matters. To implement a task switching or multitasking environment, it was thought that a special, separate key was needed. This is similar to the way "Control-Alt-Delete" is used under Windows NT.

On 84-key keyboards (except the 84-key IBM Model M space saver keyboard), SysRq was a key of its own. On the later 101-key keyboard, it shares a physical key with the Print screen key function. The Alt key must be held down while pressing this dual-function key to invoke SysRq.

The default BIOS keyboard routines simply ignore SysRq and return without taking action. So did the MS-DOS input routines. The keyboard routines in libraries supplied with many high-level languages followed suit. Although it is still included on most PC keyboards manufactured, and though it is used by some debugging software, the key is of no use for the vast majority of users.

On the Hyundai/Hynix Super-16 computer, pressing will hard boot the system (it will reboot when is unresponsive, and it will invoke startup memory tests that are bypassed on soft-boot).

In Linux, the kernel can be configured to provide functions for system debugging and crash recovery. This use is known as the "magic SysRq key".

Microsoft has also used SysRq for various OS- and application-level debuggers. In the CodeView debugger, it was sometimes used to break into the debugging during program execution. For the Windows NT remote kernel debugger, it can be used to force the system into the debugger.

IBM 3270-type console keyboards of the IBM System/370 mainframe computer, created in 1970, had an operator interrupt key that was used to cause the operating system such as VM/370 or MVS to allow the console to give input to the operating system.



</doc>
<doc id="28900" url="https://en.wikipedia.org/wiki?curid=28900" title="Split infinitive">
Split infinitive

In the English language, a split infinitive or cleft infinitive is a grammatical construction in which a word or phrase comes between the "to" and the bare infinitive of the "to" form of the infinitive verb. Usually, an adverb or an adverbial phrase comes between them. The opening sequence of the "Star Trek" television series contains a well-known example, where William Shatner says "to "boldly" go where no man has gone before"; the adverb "boldly" is said to split the infinitive "to go". There are occasions where more than one word splits the infinitive, such as: "The population is expected to more than double in the next ten years".

In the 19th century, some linguistic prescriptivists sought to introduce a prescriptive rule against the split infinitive. The construction is to some extent still the subject of disagreement, but modern English usage guides have dropped the objection to it.

In Old English, infinitives were single words ending in "-n" or "-an" (comparative to modern Dutch and German "-n", "-en"). Gerunds were formed using "to" followed by a verbal noun in the dative case, which ended in "-anne" or "-enne" (e.g. "tō cumenne" = "coming, to come"). In Middle English, the bare infinitive and the gerund coalesced into the same form ending in "-(e)n" (e.g. "comen" "come"; "to comen" "to come"). The "to" infinitive was not split in Old or Early Middle English.

The first known example of a split infinitive in English, in which a pronoun rather than an adverb splits the infinitive, is in Layamon's "Brut" (early 13th century):

This may be a poetic inversion for the sake of meter, and therefore says little about whether Layamon would have felt the construction to be syntactically natural. However, no such reservation applies to the following prose example from John Wycliffe (14th century), who often split infinitives:

After its rise in Middle English, the construction became rare in the 15th and 16th centuries. William Shakespeare used it once, or perhaps twice. The uncontroversial example appears to be a syntactical inversion for the sake of meter:

Edmund Spenser, John Dryden, Alexander Pope, and the King James Version of the Bible used none, and they are very rare in the writing of Samuel Johnson. John Donne used them several times, though, and Samuel Pepys also used at least one. No reason for the near disappearance of the split infinitive is known; in particular, no prohibition is recorded.

Split infinitives reappeared in the 18th century and became more common in the 19th.
Daniel Defoe, Benjamin Franklin, William Wordsworth, Abraham Lincoln, George Eliot, Henry James, and Willa Cather are among the writers who used them. Examples in the poems of Robert Burns attest its presence also in 18th-century Scots:

In colloquial speech the construction came to enjoy widespread use. Today, according to the "American Heritage Book of English Usage", "people split infinitives all the time without giving it a thought". In corpora of contemporary spoken English, some adverbs such as "always" and "completely" appear more often in the split position than the unsplit.

Although it is difficult to say why the construction developed in Middle English, or why it revived so powerfully in Modern English, a number of theories have been postulated.

Traditional grammarians have suggested that the construction appeared because people frequently place adverbs before finite verbs. George Curme writes: "If the adverb should immediately precede the finite verb, we feel that it should immediately precede also the infinitive…" Thus, if one says:
one may, by analogy, wish to say:
This is supported by the fact that split infinitives are often used as echoes, as in the following exchange, in which the riposte parodies the slightly odd collocation in the original sentence:
Here is an example of an adverb being transferred into split infinitive position from a parallel position in a different construction.

Transformational grammarians have attributed the construction to a re-analysis of the role of "to".

In the modern language, splitting usually involves a single adverb coming between the verb and its marker. Very frequently, this is an emphatic adverb, for example:

Sometimes it is a negation, as in the self-referential joke:

However, in modern colloquial English almost any adverb may be found in this syntactic position, especially when the adverb and the verb form a close syntactic unit (really-pull, not-split).

Compound split infinitives, i.e., infinitives split by more than one word, usually involve a pair of adverbs or a multi-word adverbial:

Examples of non-adverbial elements participating in the split-infinitive construction seem rarer in Modern English than in Middle English. The pronoun "all" commonly appears in this position:
and may even be combined with an adverb:
However an object pronoun, as in the Layamon example above, would be unusual in modern English, perhaps because this might cause a listener to misunderstand the "to" as a preposition:

While, structurally, acceptable as poetic formulation, this would result in a garden path sentence  particularly evident if the indirect object is omitted:

Other parts of speech would be very unusual in this position. However, in verse, poetic inversion for the sake of meter or of bringing a rhyme word to the end of a line often results in abnormal syntax, as with Shakespeare's split infinitive ("to pitied be", cited above), in fact an inverted passive construction in which the infinitive is split by a past participle. Presumably, this would not have occurred in a prose text by the same author.

Finally, there is a construction with a word or words between "to" and an infinitive that nevertheless is not considered a split infinitive, namely, infinitives joined by a conjunction. This is not objected to even when an adverb precedes the second infinitive. Examples include "We pray you "to" proceed/ And "justly and religiously unfold"..." (Shakespeare, "Henry V", Act II, scene 9) and "...she is determined "to" be independent, and "not live" with aunt Pullet" (George Eliot, "The Mill on the Floss", volume VI, chapter I).

It was not until the very end of the 19th century that terminology emerged to describe the construction. The earliest use of the term "split infinitive" on record dates from 1890. The now rare "cleft infinitive" is almost as old, attested from 1893. "Splitting the infinitive" is slightly older, back to 1887. According to the main etymological dictionaries, "infinitive-splitting" and "infinitive-splitter" followed in 1926 and 1927, respectively. The term "compound split infinitive" is not found in these dictionaries and appears to be very recent.

This terminology implies analysing the full infinitive as a two-word infinitive, which not all grammarians accept. As one who used "infinitive" to mean the single-word verb, Otto Jespersen challenged the epithet: "'To' is no more an essential part of an infinitive than the definite article is an essential part of a nominative, and no one would think of calling 'the good man' a split nominative." However, no alternative terminology has been proposed.

Although it is sometimes reported that a prohibition on split infinitives goes back to Renaissance times, and frequently the 18th century scholar Robert Lowth is cited as the originator of the prescriptive rule, such a rule is not to be found in Lowth's writing, and is not known to appear in any text before the 19th century.

Possibly the earliest comment against split infinitives was by the American John Comly in 1803.

An adverb should not be placed between the verb of the infinitive mood and the preposition "to", which governs it; as "Patiently" to wait—not To "patiently" wait.

Another early prohibition came from an anonymous American in 1834:

The practice of separating the prefix of the infinitive mode from the verb, by the intervention of an adverb, is not unfrequent among uneducated persons … I am not conscious, that any rule has been heretofore given in relation to this point … The practice, however, of not separating the particle from its verb, is so general and uniform among good authors, and the exceptions are so rare, that the rule which I am about to propose will, I believe, prove to be as accurate as most rules, and may be found beneficial to inexperienced writers. It is this :—"The particle, "TO", which comes before the verb in the infinitive mode, must not be separated from it by the intervention of an adverb or any other word or phrase; but the adverb should immediately precede the particle, or immediately follow the verb."

In 1840, Richard Taylor also condemned split infinitives as a "disagreeable affectation", and in 1859, Solomon Barrett, Jr., called them "a common fault". However, the issue seems not to have attracted wider public attention until Henry Alford addressed it in his "Plea for the Queen's English" in 1864:
A correspondent states as his own usage, and defends, the insertion of an adverb between the sign of the infinitive mood and the verb. He gives as an instance, ""to scientifically illustrate"". But surely this is a practice entirely unknown to English speakers and writers. It seems to me, that we ever regard the "to" of the infinitive as inseparable from its verb. And, when we have already a choice between two forms of expression, "scientifically to illustrate" and "to illustrate scientifically", there seems no good reason for flying in the face of common usage.

Others followed, among them Bache, 1869 ("The "to" of the infinitive mood is inseparable from the verb"); William B. Hodgson, 1889; and Raub, 1897 ("The sign "to" must not be separated from the remaining part of the infinitive by an intervening word").

Even as these authorities were condemning the split infinitive, others were endorsing it: Brown, 1851 (saying some grammarians had criticized it and it was less elegant than other adverb placements but sometimes clearer); Hall, 1882; Onions, 1904; Jespersen, 1905; and Fowler and Fowler, 1906. Despite the defence by some grammarians, by the beginning of the 20th century the prohibition was firmly established in the press. In the 1907 edition of "The King's English", the Fowler brothers wrote:

The 'split' infinitive has taken such hold upon the consciences of journalists that, instead of warning the novice against splitting his infinitives, we must warn him against the curious superstition that the splitting or not splitting makes the difference between a good and a bad writer.

In large parts of the school system, the construction was opposed with ruthless vigour. A correspondent to the BBC on a programme about English grammar in 1983 remarked:

One reason why the older generation feel so strongly about English grammar is that we were severely punished if we didn't obey the rules! One split infinitive, one whack; two split infinitives, two whacks; and so on.

As a result, the debate took on a degree of passion which the bare facts of the matter never warranted. There was frequent skirmishing between the splitters and anti-splitters until the 1960s. George Bernard Shaw wrote letters to newspapers supporting writers who used the split infinitive and Raymond Chandler complained to the editor of "The Atlantic" about a proofreader who interfered with Chandler's split infinitives:

By the way, would you convey my compliments to the purist who reads your proofs and tell him or her that I write in a sort of broken-down patois which is something like the way a Swiss-waiter talks, and that when I split an infinitive, God damn it, I split it so it will remain split, and when I interrupt the velvety smoothness of my more or less literate syntax with a few sudden words of barroom vernacular, this is done with the eyes wide open and the mind relaxed and attentive. The method may not be perfect, but it is all I have.

Post-1960 authorities show a strong tendency to accept the split infinitive. Follett, in "Modern American Usage" (1966) writes: "The split infinitive has its place in good composition. It should be used when it is expressive and well led up to." Fowler (Gowers' revised second edition, 1965) offers the following example of the consequences of refusal to split infinitives: "The greatest difficulty about assessing the economic achievements of the Soviet Union is that its spokesmen try "absurdly to exaggerate" them; in consequence the visitor may tend "badly to underrate" them" (italics added). This question results: "Has dread of the split infinitive led the writer to attach the adverbs ['absurdly' and 'badly'] to the wrong verbs, and would he not have done better "to boldly split" both infinitives, since he cannot put the adverbs after them without spoiling his rhythm" (italics added)? Bernstein (1985) argues that, although infinitives should not always be split, they should be split where doing so improves the sentence: "The natural position for a modifier is before the word it modifies. Thus the natural position for an adverb modifying an infinitive should be just … "after" the to" (italics added). Bernstein continues: "Curme's contention that the split infinitive is often an improvement … cannot be disputed." Heffernan and Lincoln, in their modern English composition textbook, agree with the above authors. Some sentences, they write, "are weakened by … cumbersome splitting", but in other sentences "an infinitive may be split by a one-word modifier that would be awkward in any other position".

Objections to the split infinitive fall into three categories, of which only the first is accorded any credence by linguists.

An early proposed rule proscribing the split infinitive, which was expressed by an anonymous author in the "New-England Magazine" in 1834, was based on the purported observation that it was a feature of a form of English commonly used by uneducated persons but not by "good authors".
Henry Alford, in his "Plea for the Queen's English" in 1864 went further, stating that use of the "split infinitive" was "a practice entirely unknown to English speakers and writers".

A second argument is summed up by Alford's statement "It seems to me that we ever regard the "to" of the infinitive as inseparable from its verb."

The "to" in the infinitive construction, which is found throughout the Germanic languages, is originally a preposition before the dative of a verbal noun, but in the modern languages it is widely regarded as a particle which serves as a marker of the infinitive. In German and Dutch, this marker ("zu" and "te" respectively) sometimes precedes the infinitive, but is not regarded as part of it. In English, on the other hand, it is traditional to speak of the "bare infinitive" without "to" and the "full infinitive" with it, and to conceive of "to" as part of the full infinitive. (In the sentence "I had my daughter clean her room", "clean" is a bare infinitive; in "I told my daughter to clean her room", "to clean" is a full infinitive.) Possibly this is because the absence of an "inflected" infinitive form made it useful to include the particle in the citation form of the verb, and in some nominal constructions in which other Germanic languages would omit it (e.g. "to know her is to love her"). The concept of a two-word infinitive can reinforce an intuitive sense that the two words belong together. For instance, the rhetorician John Duncan Quackenbos said, ""To have" is as much one thing, and as inseparable by modifiers, as the original form "habban", or the Latin "habere"." The usage writer John Opdycke based a similar argument on the closest French, German, and Latin translations.

That there are two parts to the infinitive is disputed, and some linguists say that the infinitive in English is a single-word verb form, which may or may not be preceded by the particle "to". Some modern generative analysts classify "to" as a "peculiar" auxiliary verb; other analysts, as the infinitival subordinator. Moreover, even when the concept of the full infinitive is accepted, it does not necessarily follow that any two words that belong together grammatically need be adjacent to each other. They usually are, but counter-examples are easily found, such as an adverb splitting a two-word finite verb ("will not do", "has not done").

A frequently discussed argument states that the split-infinitive prohibition is based on Latin. An infinitive in Latin is never used with a marker equivalent to English "to", and thus there is no parallel there for the construction. The claim that those who dislike split infinitives are applying rules of Latin grammar to English is asserted in many references that accept the split infinitive. One example is in the "American Heritage Book of English Usage": "The only rationale for condemning the construction is based on a false analogy with Latin." In more detail, the usage author Marilyn Moriarty states:
The rule forbidding a split infinitive comes from the time when Latin was the universal language of the world. All scholarly, respectable writing was done in Latin. Scientists and scholars even took Latin names to show that they were learned. In Latin, infinitives appear as a single word. The rule which prohibits splitting an infinite shows deference to Latin and to the time when the rules which governed Latin grammar were applied to other languages.

The assertion is also made in the "Oxford Guide to Plain English", "Compact Oxford English Dictionary", and Steven Pinker's "The Language Instinct", among other sources.
The argument implies an adherence to the humanist idea of the greater purity of the classics, which, particularly in Renaissance times, led people to regard as inferior aspects of English that differed from Latin. However, by the 19th century, such views were no longer widespread; Moriarty is in error about the age of the prohibition. 
It has also been stated that an argument from Latin would be fallacious because "there is no precedent in these languages for condemning the split infinitive because in Greek and Latin (and all the other romance languages) the infinitive is a single word that is impossible to sever".

Although many sources suggest that the argument from classical languages motivated the early opponents of the split infinitive, there is little primary source evidence; indeed, Richard Bailey has noted that despite the lack of evidence, this theory has simply become “part of the folklore of linguistics.”

Present style and usage manuals deem simple split infinitives unobjectionable. For example, Curme's "Grammar of the English Language" (1931) says that not only is the split infinitive correct, but it "should be furthered rather than censured, for it makes for clearer expression". "The Columbia Guide to Standard American English" notes that the split infinitive "eliminates all possibility of ambiguity", in contrast to the "potential for confusion" in an unsplit construction. "Merriam–Webster's Dictionary of English Usage" says: "the objection to the split infinitive has never had a rational basis". According to Mignon Fogarty, "today almost everyone agrees that it is OK to split infinitives".

Nevertheless, many teachers of English still admonish students against using split infinitives in writing. Because the prohibition has become so widely known, the "Columbia Guide" recommends that writers "follow the conservative path [of avoiding split infinitives when they are not necessary], especially when you're uncertain of your readers' expectations and sensitivities in this matter". Likewise, the Oxford Dictionaries do not regard the split infinitive as ungrammatical, but on balance consider it likely to produce a weak style and advise against its use for formal correspondence. R. W. Burchfield's revision of Fowler's "Modern English Usage" goes farther (quoting Burchfield's own 1981 book "The Spoken Word"): "Avoid splitting infinitives whenever possible, but do not suffer undue remorse if a split infinitive is unavoidable for the completion of a sentence already begun." Still more strongly, older editions of "The Economist" Style Guide said, "Happy the man who has never been told that it is wrong to split an infinitive: the ban is pointless. Unfortunately, to see it broken is so annoying to so many people that you should observe it" (but added "To never split an infinitive is quite easy."). This recommendation, however, is weakened in the 12th edition. After stating that the ban is pointless, "The Economist Style Guide" now says "To see a split infinitive nevertheless annoys some readers, so try to avoid placing a modifier between "to" and the verb in an infinitive. But if moving the modifier would ruin the rhythm, change the meaning or even just put the emphasis in the wrong place, splitting the infinitive is the best option."

As well as varying according to register, tolerance of split infinitives varies according to type. While most authorities accept split infinitives in general, it is not hard to construct an example which any native speaker would reject. Wycliff's Middle English compound split would, if transferred to modern English, be regarded by most people as un-English:
Attempts to define the boundaries of normality are controversial. In 1996, the usage panel of "The American Heritage Book" was evenly divided for and against such sentences as,
but more than three-quarters of the panel rejected
Here the problem appears to be the breaking up of the verbal phrase "to be seeking a plan to relieve": a segment of the head verbal phrase is so far removed from the remainder that the listener or reader must expend greater effort to understand the sentence. By contrast, 87 percent of the panel deemed acceptable the multi-word adverbial in
not surprisingly perhaps, because here there is no other place to put the words "more than" without substantially recasting the sentence.

A special case is the splitting of an infinitive by the negation in sentences like
Here traditional idiom, placing the negation before the marker ("I soon learned not to provoke her") or with verbs of desire, negating the finite verb ("I don't want to see you anymore") remains easy and natural, and is still overwhelmingly the more common construction. Some argue that the two forms have different meanings, while others see a grammatical difference, but most speakers do not make such a distinction. 
In an example drawn from the British National Corpus the use of "to not be" against "not to be" is only 0.35% (from a total of 3121 sampled usages).

Writers who avoid splitting infinitives either place the splitting element elsewhere in the sentence or reformulate the sentence, perhaps rephrasing it without an infinitive and thus avoiding the issue. However, a sentence such as "to more than double" must be completely rewritten to avoid the split infinitive; it is ungrammatical to put the words "more than" anywhere else in the sentence. While split infinitives can be avoided, a writer must be careful not to produce an awkward or ambiguous sentence. Fowler (1926) stressed that, if a sentence is to be rewritten to remove a split infinitive, this must be done without compromising the language:

It is of no avail merely to fling oneself desperately out of temptation; one must so do it that no traces of the struggle remain; that is, sentences must be thoroughly remodeled instead of having a word lifted from its original place & dumped elsewhere …
In some cases, moving the adverbial creates an ungrammatical sentence or changes the meaning. R. L. Trask uses this example:

The sentence can be rewritten to maintain its meaning, however, by using a noun or a different grammatical aspect of the verb, or by eschewing the informal "get rid":

Fowler notes that the option of rewriting is always available but questions whether it is always worth the trouble.





</doc>
<doc id="28901" url="https://en.wikipedia.org/wiki?curid=28901" title="Symmetric group">
Symmetric group

In abstract algebra, the symmetric group defined over any set is the group whose elements are all the bijections from the set to itself, and whose group operation is the composition of functions. In particular, the finite symmetric group S defined over a finite set of "n" symbols consists of the permutation operations that can be performed on the "n" symbols. Since there are "n"! ("n" factorial) such permutation operations, the order (number of elements) of the symmetric group S is "n"!.

Although symmetric groups can be defined on infinite sets, this article focuses on the finite symmetric groups: their applications, their elements, their conjugacy classes, a finite presentation, their subgroups, their automorphism groups, and their representation theory. For the remainder of this article, "symmetric group" will mean a symmetric group on a finite set.

The symmetric group is important to diverse areas of mathematics such as Galois theory, invariant theory, the representation theory of Lie groups, and combinatorics. Cayley's theorem states that every group "G" is isomorphic to a subgroup of the symmetric group on "G".

The symmetric group on a finite set "X" is the group whose elements are all bijective functions from "X" to "X" and whose group operation is that of function composition. For finite sets, "permutations" and "bijective functions" refer to the same operation, namely rearrangement. The symmetric group of degree "n" is the symmetric group on the set 

The symmetric group on a set "X" is denoted in various ways, including S, 𝔖, Σ, Σ(X), "X"! and Sym("X"). If "X" is the set } then the name may be abbreviated to S, 𝔖, Σ, or Sym("n").

Symmetric groups on infinite sets behave quite differently from symmetric groups on finite sets, and are discussed in , , and .

The symmetric group on a set of "n" elements has order "n"! (the factorial of "n"). It is abelian if and only if "n" is less than or equal to 2. For and (the empty set and the singleton set), the symmetric group is trivial (it has order ). The group S is solvable if and only if . This is an essential part of the proof of the Abel–Ruffini theorem that shows that for every there are polynomials of degree "n" which are not solvable by radicals, that is, the solutions cannot be expressed by performing a finite number of operations of addition, subtraction, multiplication, division and root extraction on the polynomial's coefficients.

The symmetric group on a set of size "n" is the Galois group of the general polynomial of degree "n" and plays an important role in Galois theory. In invariant theory, the symmetric group acts on the variables of a multi-variate function, and the functions left invariant are the so-called symmetric functions. In the representation theory of Lie groups, the representation theory of the symmetric group plays a fundamental role through the ideas of Schur functors. In the theory of Coxeter groups, the symmetric group is the Coxeter group of type A and occurs as the Weyl group of the general linear group. In combinatorics, the symmetric groups, their elements (permutations), and their representations provide a rich source of problems involving Young tableaux, plactic monoids, and the Bruhat order. Subgroups of symmetric groups are called permutation groups and are widely studied because of their importance in understanding group actions, homogeneous spaces, and automorphism groups of graphs, such as the Higman–Sims group and the Higman–Sims graph.

The elements of the symmetric group on a set "X" are the permutations of "X".

The group operation in a symmetric group is function composition, denoted by the symbol ∘ or simply by juxtaposition of the permutations. The composition of permutations "f" and "g", pronounced ""f" of "g"", maps any element "x" of "X" to "f"("g"("x")). Concretely, let (see permutation for an explanation of notation):

Applying "f" after "g" maps 1 first to 2 and then 2 to itself; 2 to 5 and then to 4; 3 to 4 and then to 5, and so on. So composing "f" and "g" gives

A cycle of length , taken to the "k"-th power, will decompose into "k" cycles of length "m": For example, (, ),

To check that the symmetric group on a set "X" is indeed a group, it is necessary to verify the group axioms of closure, associativity, identity, and inverses.

A transposition is a permutation which exchanges two elements and keeps all others fixed; for example (1 3) is a transposition. Every permutation can be written as a product of transpositions; for instance, the permutation "g" from above can be written as "g" = (1 2)(2 5)(3 4). Since "g" can be written as a product of an odd number of transpositions, it is then called an odd permutation, whereas "f" is an even permutation.

The representation of a permutation as a product of transpositions is not unique; however, the number of transpositions needed to represent a given permutation is either always even or always odd. There are several short proofs of the invariance of this parity of a permutation.

The product of two even permutations is even, the product of two odd permutations is even, and all other products are odd. Thus we can define the sign of a permutation:

With this definition,
is a group homomorphism ({+1, –1} is a group under multiplication, where +1 is e, the neutral element). The kernel of this homomorphism, that is, the set of all even permutations, is called the alternating group A. It is a normal subgroup of S, and for it has elements. The group S is the semidirect product of A and any subgroup generated by a single transposition.

Furthermore, every permutation can be written as a product of "adjacent transpositions", that is, transpositions of the form . For instance, the permutation "g" from above can also be written as . The sorting algorithm bubble sort is an application of this fact. The representation of a permutation as a product of adjacent transpositions is also not unique.

A cycle of "length" "k" is a permutation "f" for which there exists an element "x" in {1...,"n"} such that "x", "f"("x"), "f"("x"), ..., "f"("x") = "x" are the only elements moved by "f"; it is required that since with the element "x" itself would not be moved either. The permutation "h" defined by

is a cycle of length three, since , and , leaving 2 and 5 untouched. We denote such a cycle by , but it could equally well be written or by starting at a different point. The order of a cycle is equal to its length. Cycles of length two are transpositions. Two cycles are "disjoint" if they move disjoint subsets of elements. Disjoint cycles commute: for example, in S there is the equality . Every element of S can be written as a product of disjoint cycles; this representation is unique up to the order of the factors, and the freedom present in representing each individual cycle by choosing its starting point.

Cycles admit the following conjugation property with any permutation formula_8, this property is often used to obtain its generators and relations.

Certain elements of the symmetric group of {1, 2, ..., "n"} are of particular interest (these can be generalized to the symmetric group of any finite totally ordered set, but not to that of an unordered set).

The is the one given by:
This is the unique maximal element with respect to the Bruhat order and the
longest element in the symmetric group with respect to generating set consisting of the adjacent transpositions , .

This is an involution, and consists of formula_11 (non-adjacent) transpositions

so it thus has sign:

which is 4-periodic in "n".

In S, the "perfect shuffle" is the permutation that splits the set into 2 piles and interleaves them. Its sign is also formula_15

Note that the reverse on "n" elements and perfect shuffle on 2"n" elements have the same sign; these are important to the classification of Clifford algebras, which are 8-periodic.

The conjugacy classes of S correspond to the cycle structures of permutations; that is, two elements of S are conjugate in S if and only if they consist of the same number of disjoint cycles of the same lengths. For instance, in S, (1 2 3)(4 5) and (1 4 3)(2 5) are conjugate; (1 2 3)(4 5) and (1 2)(4 5) are not. A conjugating element of S can be constructed in "two line notation" by placing the "cycle notations" of the two conjugate permutations on top of one another. Continuing the previous example:

which can be written as the product of cycles, namely: (2 4).

This permutation then relates (1 2 3)(4 5) and (1 4 3)(2 5) via conjugation, that is,

It is clear that such a permutation is not unique.

The low-degree symmetric groups have simpler and exceptional structure, and often must be treated separately.







Other than the trivial map and the sign map , the most notable homomorphisms between symmetric groups, in order of relative dimension, are:
There are also a host of other homomorphisms where .

Symmetric groups are Coxeter groups and reflection groups. They can be realized as a group of reflections with respect to hyperplanes . Braid groups B admit symmetric groups S as quotient groups.

Cayley's theorem states that every group "G" is isomorphic to a subgroup of the symmetric group on the elements of "G", as a group acts on itself faithfully by (left or right) multiplication.

For , the alternating group A is simple, and the induced quotient is the sign map: which is split by taking a transposition of two elements. Thus S is the semidirect product , and has no other proper normal subgroups, as they would intersect A in either the identity (and thus themselves be the identity or a 2-element group, which is not normal), or in A (and thus themselves be A or S).

S acts on its subgroup A by conjugation, and for , S is the full automorphism group of A: Aut(A) ≅ S. Conjugation by even elements are inner automorphisms of A while the outer automorphism of A of order 2 corresponds to conjugation by an odd element. For , there is an exceptional outer automorphism of A so S is not the full automorphism group of A.

Conversely, for , S has no outer automorphisms, and for it has no center, so for it is a complete group, as discussed in automorphism group, below.

For , S is an almost simple group, as it lies between the simple group A and its group of automorphisms.

S can be embedded into A by appending the transposition to all odd permutations, while embedding into A is impossible for .

The symmetric group on letters, , may be described as follows. It has generators formula_18 and relations:

The generator can be thought of as swapping positions and .

Other possible generating sets include the set of transpositions that swap and for , and a set containing any -cycle and a -cycle of adjacent elements in the -cycle.

A subgroup of a symmetric group is called a permutation group.

The normal subgroups of the finite symmetric groups are well understood. If , S has at most 2 elements, and so has no nontrivial proper subgroups. The alternating group of degree "n" is always a normal subgroup, a proper one for and nontrivial for ; for it is in fact the only non-identity proper normal subgroup of S, except when where there is one additional such normal subgroup, which is isomorphic to the Klein four group.

The symmetric group on an infinite set does not have an associated alternating group: not all elements can be written as a (finite) product of transpositions. However it does contain a normal subgroup "S" of permutations that fix all but finitely many elements, and such permutations can be classified as either even or odd. The even elements of "S" form the alternating subgroup "A" of "S", and since "A" is even a characteristic subgroup of "S", it is also a normal subgroup of the full symmetric group of the infinite set. The groups "A" and "S" are the only non-identity proper normal subgroups of the symmetric group on a countably infinite set. For more details see or .

The maximal subgroups of the finite symmetric groups fall into three classes: the intransitive, the imprimitive, and the primitive. The intransitive maximal subgroups are exactly those of the form for . The imprimitive maximal subgroups are exactly those of the form Sym("k") wr Sym("n"/"k") where is a proper divisor of "n" and "wr" denotes the wreath product acting imprimitively. The primitive maximal subgroups are more difficult to identify, but with the assistance of the O'Nan–Scott theorem and the classification of finite simple groups, gave a fairly satisfactory description of the maximal subgroups of this type according to .

The Sylow subgroups of the symmetric groups are important examples of "p"-groups. They are more easily described in special cases first:

The Sylow "p"-subgroups of the symmetric group of degree "p" are just the cyclic subgroups generated by "p"-cycles. There are such subgroups simply by counting generators. The normalizer therefore has order and is known as a Frobenius group (especially for ), and is the affine general linear group, .

The Sylow "p"-subgroups of the symmetric group of degree "p" are the wreath product of two cyclic groups of order "p". For instance, when , a Sylow 3-subgroup of Sym(9) is generated by and the elements "x" = (1 2 3), "y" = (4 5 6), "z" = (7 8 9), and every element of the Sylow 3-subgroup has the form "a""x""y""z" for 0 ≤ "i","j","k","l" ≤ 2.

The Sylow "p"-subgroups of the symmetric group of degree "p" are sometimes denoted W("n"), and using this notation one has that is the wreath product of W("n") and W(1).

In general, the Sylow "p"-subgroups of the symmetric group of degree "n" are a direct product of "a" copies of W("i"), where 0 ≤ "a" ≤ "p" − 1 and "n" = "a" + "p"·"a" + ... + "p"·"a" (the base "p" expansion of "n").

For instance, W(1) = C and W(2) = D, the dihedral group of order 8, and so a Sylow 2-subgroup of the symmetric group of degree 7 is generated by and is isomorphic to .

These calculations are attributed to and described in more detail in . Note however that attributes the result to an 1844 work of Cauchy, and mentions that it is even covered in textbook form in .

A transitive subgroup of S is a subgroup whose action on {1, 2, ..., "n"} is transitive. For example, the Galois group of a (finite) Galois extension is a transitive subgroup of S, for some "n".

For , S is a complete group: its center and outer automorphism group are both trivial.

For , the automorphism group is trivial, but S is not trivial: it is isomorphic to C, which is abelian, and hence the center is the whole group.

For , it has an outer automorphism of order 2: , and the automorphism group is a semidirect product .

In fact, for any set "X" of cardinality other than 6, every automorphism of the symmetric group on "X" is inner, a result first due to according to .

The group homology of S is quite regular and stabilizes: the first homology (concretely, the abelianization) is:

The first homology group is the abelianization, and corresponds to the sign map S → S which is the abelianization for "n" ≥ 2; for "n" < 2 the symmetric group is trivial. This homology is easily computed as follows: S is generated by involutions (2-cycles, which have order 2), so the only non-trivial maps are to S and all involutions are conjugate, hence map to the same element in the abelianization (since conjugation is trivial in abelian groups). Thus the only possible maps send an involution to 1 (the trivial map) or to −1 (the sign map). One must also show that the sign map is well-defined, but assuming that, this gives the first homology of S.

The second homology (concretely, the Schur multiplier) is:
This was computed in , and corresponds to the double cover of the symmetric group, 2 · S.

Note that the exceptional low-dimensional homology of the alternating group (formula_24 corresponding to non-trivial abelianization, and formula_25 due to the exceptional 3-fold cover) does not change the homology of the symmetric group; the alternating group phenomena do yield symmetric group phenomena – the map formula_26 extends to formula_27 and the triple covers of A and A extend to triple covers of S and S – but these are not "homological" – the map formula_28 does not change the abelianization of S, and the triple covers do not correspond to homology either.

The homology "stabilizes" in the sense of stable homotopy theory: there is an inclusion map , and for fixed "k", the induced map on homology is an isomorphism for sufficiently high "n". This is analogous to the homology of families Lie groups stabilizing.

The homology of the infinite symmetric group is computed in , with the cohomology algebra forming a Hopf algebra.

The representation theory of the symmetric group is a particular case of the representation theory of finite groups, for which a concrete and detailed theory can be obtained. This has a large area of potential applications, from symmetric function theory to problems of quantum mechanics for a number of identical particles.

The symmetric group S has order "n"<nowiki>!</nowiki>. Its conjugacy classes are labeled by partitions of "n". Therefore, according to the representation theory of a finite group, the number of inequivalent irreducible representations, over the complex numbers, is equal to the number of partitions of "n". Unlike the general situation for finite groups, there is in fact a natural way to parametrize irreducible representation by the same set that parametrizes conjugacy classes, namely by partitions of "n" or equivalently Young diagrams of size "n".

Each such irreducible representation can be realized over the integers (every permutation acting by a matrix with integer coefficients); it can be explicitly constructed by computing the Young symmetrizers acting on a space generated by the Young tableaux of shape given by the Young diagram.

Over other fields the situation can become much more complicated. If the field "K" has characteristic equal to zero or greater than "n" then by Maschke's theorem the group algebra "K"S is semisimple. In these cases the irreducible representations defined over the integers give the complete set of irreducible representations (after reduction modulo the characteristic if necessary).

However, the irreducible representations of the symmetric group are not known in arbitrary characteristic. In this context it is more usual to use the language of modules rather than representations. The representation obtained from an irreducible representation defined over the integers by reducing modulo the characteristic will not in general be irreducible. The modules so constructed are called "Specht modules", and every irreducible does arise inside some such module. There are now fewer irreducibles, and although they can be classified they are very poorly understood. For example, even their dimensions are not known in general.

The determination of the irreducible modules for the symmetric group over an arbitrary field is widely regarded as one of the most important open problems in representation theory.




</doc>
<doc id="28902" url="https://en.wikipedia.org/wiki?curid=28902" title="SMS (disambiguation)">
SMS (disambiguation)

SMS is short message service, a form of text-messaging communication based on phones.

SMS may also refer to:









</doc>
<doc id="28904" url="https://en.wikipedia.org/wiki?curid=28904" title="Short Message Peer-to-Peer">
Short Message Peer-to-Peer

Short Message Peer-to-Peer (SMPP) in the telecommunications industry is an open, industry standard protocol designed to provide a flexible data communication interface for the transfer of short message data between External Short Messaging Entities (ESMEs), Routing Entities (REs) and Message Centres.

SMPP is often used to allow third parties (e.g. value-added service providers like news organizations) to submit messages, often in bulk, but it may be used for SMS peering as well. SMPP is able to carry short messages including EMS, voicemail notifications, Cell Broadcasts, WAP messages including WAP Push messages (used to deliver MMS notifications), USSD messages and others. Because of its versatility and support for non-GSM SMS protocols, like UMTS, IS-95 (CDMA), CDMA2000, ANSI-136 (TDMA) and iDEN, SMPP is the most commonly used protocol for short message exchange outside SS7 networks.

SMPP (Short Message Peer-to-Peer) was originally designed by Aldiscon, a small Irish company that was later acquired by Logica (since 2016, after a number of changes Mavenir). The protocol was originally created by a developer, Ian J Chambers, to test the functionality of the SMSC without using SS7 test equipment to submit messages. In 1999, Logica formally handed over SMPP to the SMPP Developers Forum, later renamed as The SMS Forum and now disbanded. The SMPP protocol specifications are still available through the website which also carries a notice stating that it will be taken down at the end of 2007. As part of the original handover terms, SMPP ownership has now returned to Mavenir due to the disbanding of the SMS Forum.

To date, SMPP development is suspended and SMS Forum is disbanded. From the SMS Forum website:

July 31, 2007 - The SMS Forum, a non-profit organization with a mission to develop, foster and promote SMS (short message service) to the benefit of the global wireless industry will disband by July 27, 2007

A press release, attached to the news, also warns that site will be suspended soon. In spite of this, the site is still mostly functioning and specifications can still be downloaded (as of 31 January 2012).

The site has ceased operation according to Cormac Long, former technical moderator and webmaster for the SMS Forum. Please contact Mavenir for the SMPP specification. The specifications are also available from the former site of the SMPP Developers Forum (predecessor to SMS Forum) at SMPP Protocol - SMS API.

Contrary to its name, the SMPP uses the client-server model of operation. The Short Message Service Center (SMSC) usually acts as a server, awaiting connections from ESMEs. When SMPP is used for SMS peering, the sending MC usually acts as a client.

The protocol is based on pairs of request/response PDUs (protocol data units, or packets) exchanged over OSI layer 4 (TCP session or X.25 SVC3) connections. The well-known port assigned by the IANA for SMPP when operating over TCP is 2775, but multiple arbitrary port numbers are often used in messaging environments.

Before exchanging any messages, a bind command must be sent and acknowledged. The bind command determines in which direction will be possible to send messages; bind_transmitter only allows client to submit messages to the server, bind_receiver means that the client will only receive the messages, and bind_transceiver (introduced in SMPP 3.4) allows message transfer in both directions. In the bind command the ESME identifies itself using system_id, system_type and password; the address_range field designed to contain ESME address is usually left empty. The bind command contains interface_version parameter to specify which version of SMPP protocol will be used.

Message exchange may be synchronous, where each peer waits for a response for each PDU being sent, or asynchronous, where multiple requests can be issued without waiting and acknowledged in a skew order by the other peer; the number of unacknowledged requests is called a "window"; for the best performance both communicating sides must be configured with the same window size.

The SMPP standard has evolved during the time. The most commonly used versions of SMPP are:


The applicable version is passed in the interface_version parameter of a bind command.

The SMPP PDUs are binary encoded for efficiency. They start with a header which may be followed by a body:

Each PDU starts with a header. The header consists of 4 fields, each of length of 4 octets:


All numeric fields in SMPP use the big endian order, which means that the first octet is the Most Significant Byte (MSB).

This is an example of the binary encoding of a 60-octet "submit_sm" PDU. The data is shown in Hex octet values as a single dump and followed by a header and body break-down of that PDU.

This is best compared with the definition of the submit_sm PDU from the SMPP specification in order to understand how the encoding matches the field by field definition.

The value break-downs are shown with decimal in parentheses and Hex values after that. Where you see one or several hex octets appended, this is because the given field size uses 1 or more octets encoding.

Again, reading the definition of the submit_sm PDU from the spec will make all this clearer.

 'command_length', (60) ... 00 00 00 3C

 'service_type', () ... 00

Note that the text in the short_message field must match the data_coding. When the data_coding is 8 (UCS2), the text must be in UCS-2BE (or its extension, UTF-16BE). When the data_coding indicates a 7-bit encoding, each septet is stored in a separate octet in the short_message field (with the most significant bit set to 0). SMPP 3.3 data_coding exactly copied TP-DCS values of GSM 03.38, which make it suitable only for GSM 7-bit default alphabet, UCS2 or binary messages; SMPP 3.4 introduced a new list of data_coding values:

The meaning of the data_coding=4 or 8 is the same as in SMPP 3.3. Other values in the range 1-15 are reserved in SMPP 3.3. Unfortunately, unlike SMPP 3.3, where data_coding=0 was unambiguously GSM 7-bit default alphabet, for SMPP 3.4 and higher the GSM 7-bit default alphabet is missing in this list, and data_coding=0 may differ for various Short message service centers—it may be ISO-8859-1, ASCII, GSM 7-bit default alphabet, UTF-8 or even configurable per ESME. When using data_coding=0, both sides (ESME and SMSC) must be sure they consider it the same encoding. Otherwise it is better not to use data_coding=0. It may be tricky to use the GSM 7-bit default alphabet, some Short message service centers requires data_coding=0, others e.g. data_coding=241.

Despite its wide acceptance, the SMPP has a number of problematic features:


Although data_coding value in SMPP 3.3 are based on the GSM 03.38, since SMPP 3.4 there is no data_coding value for GSM 7-bit alphabet (GSM 03.38). However, it is common for DCS=0 to indicate the GSM 7-bit alphabet, particularly for SMPP connections to SMSCs on GSM mobile networks.

According to SMPP 3.4 and 5.0 the data_coding=0 means ″SMSC Default Alphabet″. Which encoding it really is, depends on the type of the SMSC and its configuration.

One of the encodings in CDMA standard C.R1001 is Shift-JIS used for Japanese. SMPP 3.4 and 5.0 specifies three encodings for Japanese (JIS, ISO-2022-JP and Extended Kanji JIS), but none of them is identical with CDMA MSG_ENCODING 00101. It seems that the Pictogram encoding (data_coding=9) is used to carry the messages in Shift-JIS in SMPP.

When a submit_sm fails, the SMSC returns a submit_sm_resp with non-zero value of command_status and ″empty″ message_id.


For the best compatibility, any SMPP implementation should accept both variants of negative submit_sm_resp regardless of the version of SMPP standard used for the communication.

The only way to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose. While SMPP 3.3 states that Message ID is a C-Octet String (Hex) of up to 8 characters (plus terminating '\0'), the SMPP 3.4 states that the id field in the Delivery Receipt Format is a C-Octet String (Decimal) of up to 10 characters. This splits SMPP implementations to 2 groups:


Since introduction of Tag-Length-Value (TLV) parameters in version 3.4, the SMPP may be regarded an extensible protocol. In order to achieve the highest possible degree of compatibility and interoperability any implementation should apply the Internet robustness principle: ″Be conservative in what you send, be liberal in what you accept″. It should use a minimal set of features which are necessary to accomplish a task. And if the goal is communication and not quibbling, each implementation should overcome minor nonconformities with standard:


Information applicable to one version of SMPP can often be found in another version of SMPP, for example with the case of SMPP 3.4 describing the only mechanism of delivery receipts in SMPP 3.3 described above.

The SMPP protocol is designed on a clear-text binary protocol which needs to be considered if using for potentially sensitive information such as one-time passwords via SMS. There are, however, implementations of SMPP over secure SSL/TLS if required.




</doc>
<doc id="28906" url="https://en.wikipedia.org/wiki?curid=28906" title="Strike from the record">
Strike from the record

To strike from the record is for a judge to forbid a decision maker (such as a juror) to consider a particular piece of testimony or other evidence when deciding the case even though he or she has already learned what that evidence or testimony concerned. The commonly heard request is "move to strike", with the intent to erase previous testimony or court proceeding from record.


</doc>
<doc id="28908" url="https://en.wikipedia.org/wiki?curid=28908" title="Suburb">
Suburb

A suburb is a mixed-use or residential area, existing either as part of a city or urban area or as a separate residential community within commuting distance of a city. Suburbs might have their political jurisdiction, especially in the United States, but this is not always the case, especially in the United Kingdom where suburbs are located within the administrative boundaries of cities. In most English-speaking countries, suburban areas are defined in contrast to central or inner-city areas, but in Australian English and South African English, "suburb" has become largely synonymous with what is called a "neighborhood" in other countries and the term extends to inner-city areas. In some areas, such as Australia, India, China, New Zealand, the United Kingdom, and parts of the United States and Canada, new suburbs are routinely annexed by adjacent cities. In others, such as Morocco, France, and much of the United States and Canada, many suburbs remain separate municipalities or are governed as part of a larger local government area such as a county. In the United States, beyond the suburbs are exurbs, or "exurban areas", with less density but linked to the metropolitan area economically and by commuters.

Suburbs first emerged on a large scale in the 19th and 20th centuries as a result of improved rail and road transport, which led to an increase in commuting. In general, they have lower population densities than inner city neighborhoods within a metropolitan area, and most residents commute to central cities or other business districts; however, there are many exceptions, including industrial suburbs, planned communities, and satellite cities. Suburbs tend to proliferate around cities that have an abundance of adjacent flat land.

The English word is derived from the Old French "subburbe", which is in turn derived from the Latin "suburbium", formed from "sub" (meaning "under" or "below") and "urbs" ("city"). The first recorded usage of the term in English, was made by John Wycliffe in 1380, where the form "subarbis" was used, according to the "Oxford English Dictionary".

In Australia and New Zealand, suburbs (in the wider sense noted in the lead paragraph) have become formalised as geographic subdivisions of a city and are used by postal services in addressing. In rural areas in both countries, their equivalents are called localities (see suburbs and localities). The terms "inner suburb" and "outer suburb" are used to differentiate between the higher-density areas in proximity to the city centre (which would not be referred to as 'suburbs' in most other countries), and the lower-density suburbs on the outskirts of the urban area. The term 'middle suburbs' is also used. Inner suburbs, such as Te Aro in Wellington, Eden Terrace in Auckland, Prahran in Melbourne and Ultimo in Sydney, are usually characterised by higher density apartment housing and greater integration between commercial and residential areas.

In New Zealand, most suburbs are not legally defined which can lead to confusion as to where they may begin and end. Although there is a geospatial file defining suburbs for use by emergency services developed and maintained by Fire and Emergency New Zealand (formerly the New Zealand Fire Service), in collaboration with other government agencies, to date this file has not been released publicly. New Zealand company Koordinates Limited requested access to the geospatial file under the Official Information Act 1982 but this request was rejected by the New Zealand Fire Service on the basis that it would prejudice the health & safety of, or cause material loss, to the public. In September 2014 a decision was made by the Ombudsman of New Zealand ruling that the New Zealand Fire Service refusal to release the geospatial file without agreeing to terms which included, among other restrictions, a prohibition on redistribution of the geospatial file, was reasonable.

In the United Kingdom and in Ireland, "suburb" merely refers to a residential area outside the city centre, regardless of administrative boundaries. Suburbs, in this sense, can range from areas that seem more like residential areas of a city proper to areas separated by open countryside from the city centre. In large cities such as London and Leeds, suburbs include formerly separate towns and villages that have been gradually absorbed during a city's growth and expansion, such as Ealing, Bromley, and Guiseley.

In the United States and Canada, "suburb" can refer either to an outlying residential area of a city or town or to a separate municipality or unincorporated area outside a town or city.

The earliest appearance of suburbs coincided with the spread of the first urban settlements. Large walled towns tended to be the focus around which smaller villages grew up in a symbiotic relationship with the market town. The word 'suburbani' was first employed by the Roman statesman Cicero in reference to the large villas and estates built by the wealthy patricians of Rome on the city's outskirts.

Towards the end of the Eastern Han Dynasty (up until 190 AD, when Dong Zhuo razed the city), the capital, Luoyang, was mainly occupied by the emperor and important officials; the city's people mostly lived in small cities right outside Luoyang, which were suburbs in all but name.

As populations grew during the Early Modern Period in Europe, urban towns swelled with a steady influx of people from the countryside. In some places, nearby settlements were swallowed up as the main city expanded. The peripheral areas on the outskirts of the city were generally inhabited by the very poorest.

Due to the rapid migration of the rural poor to the industrialising cities of England in the late 18th century, a trend in the opposite direction began to develop; that is, newly rich members of the middle classes began to purchase estates and villas on the outskirts of London. This trend accelerated through the 19th century, especially in cities like London and Manchester that were growing rapidly, and the first suburban districts sprung up around the city centres to accommodate those who wanted to escape the squalid conditions of the industrial towns. Toward the end of the century, with the development of public transit systems such as the underground railways, trams and buses, it became possible for the majority of the city's population to reside outside the city and to commute into the center for work.

By the mid-19th century, the first major suburban areas were springing up around London as the city (then the largest in the world) became more overcrowded and unsanitary. A major catalyst for suburban growth was the opening of the Metropolitan Railway in the 1860s. The line joined the capital's financial heart in the City to what were to become the suburbs of Middlesex. Harrow was reached in 1880.

Unlike other railway companies, which were required to dispose of surplus land, the Met was allowed to retain such land that it believed was necessary for future railway use. Initially, the surplus land was managed by the Land Committee, and, from the 1880s, the land was developed and sold to domestic buyers in places like Willesden Park Estate, Cecil Park, near Pinner and at Wembley Park.

In 1912, it was suggested that a specially formed company should take over from the Surplus Lands Committee and develop suburban estates near the railway. However, World War I delayed these plans and it was only in 1919, with the expectation of a postwar housing boom, that Metropolitan Railway Country Estates Limited (MRCE) was formed. MRCE went on to develop estates at Kingsbury Garden Village near Neasden, Wembley Park, Cecil Park and Grange Estate at Pinner and the Cedars Estate at Rickmansworth and create places such as Harrow Garden Village.

The term "Metro-land" was coined by the Met's marketing department in 1915 when the "Guide to the Extension Line" became the "Metro-land" guide, priced at 1d. This promoted the land served by the Met for the walker, visitor and later the house-hunter. Published annually until 1932, the last full year of independence for the Met, the guide extolled the benefits of "The good air of the Chilterns", using language such as "Each lover of Metroland may well have his own favourite wood beech and coppice — all tremulous green loveliness in Spring and russet and gold in October". The dream promoted was of a modern home in beautiful countryside with a fast railway service to central London. By 1915, people from across London had flocked to live the new suburban dream in large newly built areas across North West London.

Suburbanisation in the interwar period was heavily influenced by the garden city movement of Ebenezer Howard and the creation of the first garden suburbs at the turn of the 20th century. The first garden suburb was developed through the efforts of social reformer Henrietta Barnett and her husband; inspired by Ebenezer Howard and the model housing development movement (then exemplified by Letchworth garden city), as well as the desire to protect part of Hampstead Heath from development, they established trusts in 1904 which bought 243 acres of land along the newly opened Northern line extension to Golders Green and created the Hampstead Garden Suburb. The suburb attracted the talents of architects including Raymond Unwin and Sir Edwin Lutyens, and it ultimately grew to encompass over 800 acres.

During the First World War the Tudor Walters Committee was commissioned to make recommendations for the post war reconstruction and housebuilding. In part, this was a response to the shocking lack of fitness amongst many recruits during World War One, attributed to poor living conditions; a belief summed up in a housing poster of the period "you cannot expect to get an A1 population out of C3 homes" - referring to military fitness classifications of the period.

The Committee's report of 1917 was taken up by the government, which passed the Housing, Town Planning, &c. Act 1919, also known as the Addison Act after Dr. Christopher Addison, the then Minister for Housing. The Act allowed for the building of large new housing estates in the suburbs after the First World War, and marked the start of a long 20th century tradition of state-owned housing, which would later evolve into council estates.

The Report also legislated on the required, minimum standards necessary for further suburban construction; this included regulation on the maximum housing density and their arrangement and it even made recommendations on the ideal number of bedrooms and other rooms per house. Although the semi-detached house was first designed by the Shaws (a father and son architectural partnership) in the 19th century, it was during the suburban housing boom of the interwar period that the design first proliferated as a suburban icon, being preferred by middle class home owners to the smaller terraced houses. The design of many of these houses, highly characteristic of the era, was heavily influenced by the Art Deco movement, taking influence from Tudor Revival, chalet style, and even ship design.

Within just a decade suburbs dramatically increased in size. Harrow Weald went from just 1,500 to over 10,000 while Pinner jumped from 3,000 to over 20,000. During the 1930s, over 4 million new suburban houses were built, the 'suburban revolution' had made England the most heavily suburbanized country in the world, by a considerable margin.

Boston and New York spawned the first major suburbs. The streetcar lines in Boston and the rail lines in Manhattan made daily commutes possible. No metropolitan area in the world was as well served by railroad commuter lines at the turn of the twentieth century as New York, and it was the rail lines to Westchester from the Grand Central Terminal commuter hub that enabled its development. Westchester's true importance in the history of American suburbanization derives from the upper-middle class development of villages including Scarsdale, New Rochelle and Rye serving thousands of businessmen and executives from Manhattan.

The suburban population in North America exploded during the post-World War II economic expansion. Returning veterans wishing to start a settled life moved in masses to the suburbs. Levittown developed as a major prototype of mass-produced housing.

Very little housing had been built during the Great Depression and World War II, except for emergency quarters near war industries. Overcrowded and inadequate apartments was the common condition. Some suburbs had developed around large cities where there was rail transportation to the jobs downtown. However, the real growth in suburbia depended on the availability of automobiles, highways, and inexpensive housing. The population had grown, and the stock of family savings had accumulated the money for down payments, automobiles and appliances. The product was a great housing boom. Whereas, an average of 316,000 new housing non-farm units should have been constructed 1930s through 1945, there were 1,450,000 annually from 1946 through 1955. The G.I. Bill guaranteed low cost loans for veterans, with very low down payments, and low interest rates. With 16 million eligible veterans, the opportunity to buy a house was suddenly at hand. In 1947 alone, 540,000 veterans bought one; their average price was $7300. The construction industry kept prices low by standardization – for example standardizing sizes for kitchen cabinets, refrigerators and stoves, allowed for mass production of kitchen furnishings. Developers purchased empty land just outside the city, installed tract houses based on a handful of designs, and provided streets and utilities, or local public officials race to build schools. The most famous development was Levittown, in Long Island just east of New York City. It offered a new house for $1000 down, and $70 a month; it featured three bedrooms, fireplace, gas range and gas furnace, and a landscaped lot of 75 by 100 feet, all for a total price of $10,000. Veterans could get one with a much lower down payment.

At the same time, African Americans were rapidly moving north and west for better jobs and educational opportunities than were available to them in the segregated South. Their arrival in Northern and Western cities en masse, in addition to being followed by race riots in several large cities such as Philadelphia, Los Angeles, Detroit, Chicago, and Washington, D.C., further stimulated white suburban migration. The growth of the suburbs was facilitated by the development of zoning laws, redlining and numerous innovations in transport. After World War II, availability of FHA loans stimulated a housing boom in American suburbs. In the older cities of the northeast U.S., streetcar suburbs originally developed along train or trolley lines that could shuttle workers into and out of city centers where the jobs were located. This practice gave rise to the term "bedroom community", meaning that most daytime business activity took place in the city, with the working population leaving the city at night for the purpose of going home to sleep.

Economic growth in the United States encouraged the suburbanization of American cities that required massive investments for the new infrastructure and homes. Consumer patterns were also shifting at this time, as purchasing power was becoming stronger and more accessible to a wider range of families. Suburban houses also brought about needs for products that were not needed in urban neighborhoods, such as lawnmowers and automobiles. During this time commercial shopping malls were being developed near suburbs to satisfy consumers' needs and their car–dependent lifestyle.

Zoning laws also contributed to the location of residential areas outside of the city center by creating wide areas or "zones" where only residential buildings were permitted. These suburban residences are built on larger lots of land than in the central city. For example, the lot size for a residence in Chicago is usually deep, while the width can vary from wide for a row house to wide for a large stand–alone house. In the suburbs, where stand–alone houses are the rule, lots may be wide by deep, as in the Chicago suburb of Naperville. Manufacturing and commercial buildings were segregated in other areas of the city.

Alongside suburbanization, many companies began locating their offices and other facilities in the outer areas of the cities, which resulted in the increased density of older suburbs and the growth of lower density suburbs even further from city centers. An alternative strategy is the deliberate design of "new towns" and the protection of green belts around cities. Some social reformers attempted to combine the best of both concepts in the garden city movement.

In the U.S., 1950 was the first year that more people lived in suburbs than elsewhere. In the U.S, the development of the skyscraper and the sharp inflation of downtown real estate prices also led to downtowns being more fully dedicated to businesses, thus pushing residents outside the city center.

In the 20th century, many suburban areas, especially those not within the political boundaries of the city containing the central business area, began to see independence from the central city as an asset. In some cases, suburbanites saw self-government as a means to keep out people who could not afford the added suburban property maintenance costs not needed in city living. Federal subsidies for suburban development accelerated this process as did the practice of redlining by banks and other lending institutions. In some cities such as Miami and San Francisco, the main city is much smaller than the surrounding suburban areas, leaving the city proper with a small portion of the metro area's population and land area.

Mesa, Arizona and Virginia Beach, the two most populous suburbs in the United States, are actually more populous than many of America's largest cities, including Miami, Minneapolis, New Orleans, Cleveland, Tampa, St. Louis, Pittsburgh, Cincinnati, and others. Virginia Beach is now the largest city in its metropolitan area of Hampton Roads, having long since exceeded the population of its neighboring primary city, Norfolk. While Virginia Beach has slowly been taking on the characteristics of an urban city, it will not likely achieve the population density and urban characteristics of Norfolk. It is generally assumed that the population of Chesapeake, another Hampton Roads city, will also exceed that of Norfolk in 2018 if its current growth rate continues at its same pace.

Cleveland, Ohio is typical of many American central cities; its municipal borders have changed little since 1922, even though the Cleveland urbanized area has grown many times over. Several layers of suburban municipalities now surround cities like Boston, Cleveland, Chicago, Detroit, Los Angeles, Dallas, Denver, Houston, New York City, San Francisco, Sacramento, Atlanta, Miami, Baltimore, Milwaukee, Pittsburgh, Philadelphia, Phoenix, Roanoke, St. Louis, Salt Lake City, Las Vegas, Minneapolis, and Washington, D.C..

Suburbs in the United States have a prevalence of usually detached single-family homes.

They are characterized by:


By 2010, suburbs increasingly gained people in racial minority groups, as many members of minority groups gained better access to education and sought more favorable living conditions compared to inner city areas.

Conversely, many white Americans also moved back to city centers. Nearly all major city downtowns (such as Downtown Miami, Downtown Detroit, Downtown Philadelphia, Downtown Roanoke, or Downtown Los Angeles) are experiencing a renewal, with large population growth, residential apartment construction, and increased social, cultural, and infrastructural investments, as have suburban neighborhoods close to city centers. Better public transit, proximity to work and cultural attractions, and frustration with suburban life and gridlock have attracted young Americans to the city centers.

Canada is an urbanized nation where over 80% of the population live in urban areas (loosely defined), and roughly two-thirds live in one of Canada's 33 census metropolitan areas (CMAs) with a population of over 100,000. However, of this metropolitan population, in 2001 nearly half lived in low-density neighborhoods, with only one in five living in a typical "urban" neighborhood. The percentage living in low-density neighborhoods varied from a high of nearly two-thirds of Calgary CMA residents (67%), to a low of about one-third of Montréal CMA residents (34%).

Often, Canadian suburbs are less automobile-centred and public transit use is encouraged but can be notably unused. Throughout Canada, there are comprehensive plans in place to curb sprawl.

Population and income growth in Canadian suburbs had tended to outpace growth in core urban or rural areas, but in many areas this trend has now reversed. The suburban population increased 87% between 1981 and 2001, well ahead of urban growth. The majority of recent population growth in Canada's three largest metropolitan areas (Greater Toronto, Greater Montréal, and Greater Vancouver) has occurred in non-core municipalities. This trend is also beginning to take effect in Vancouver, and to a lesser extent, Montréal. In certain cities, particularly Edmonton and Calgary, suburban growth takes place within the city boundaries as opposed to in bedroom communities. This is due to annexation and large geographic footprint within the city borders.
Calgary is unusual among Canadian cities because it has developed as a unicity - it has annexed most of its surrounding towns and large amounts of undeveloped land around the city. As a result, most of the communities that Calgarians refer to as "suburbs" are actually inside the city limits. In the 2016 census, the City of Calgary had a population of 1,239,220, whereas the Calgary Metropolitan Area had a population of 1,392,609, indicating the vast majority of people in the Calgary CMA lived within the city limits. The perceived low population density of Calgary largely results from its many internal suburbs and the large amount of undeveloped land within the city. The city actually has a policy of densifying its new developments.

In many parts of the developed world, suburbs can be economically distressed areas, inhabited by higher proportions of recent immigrants, with higher delinquency rates and social problems. Sometimes the notion of suburb may even refer to people in real misery, who are kept at the limit of the city borders for economic, social, and sometimes ethnic reasons. An example in the developed world would be the "banlieues" of France, or the concrete suburbs of Sweden, even if the suburbs of these countries also include middle-class and upper-class neighborhoods that often consist of single-family houses. Thus some of the suburbs of most of the developed world are comparable to several inner cities of the U.S. and Canada.

The growth in the use of trains, and later automobiles and highways, increased the ease with which workers could have a job in the city while commuting in from the suburbs. In the United Kingdom, as mentioned above, railways stimulated the first mass exodus to the suburbs. The Metropolitan Railway, for example, was active in building and promoting its own housing estates in the north-west of London, consisting mostly of detached houses on large plots, which it then marketed as "Metro-land". The Australian and New Zealand usage came about as outer areas were quickly surrounded in fast-growing cities, but retained the appellation "suburb"; the term was eventually applied to the original core as well. In Australia, Sydney's urban sprawl has occurred predominantly in the Western Suburbs. The locality of Olympic Park was designated an official suburb in 2009.

In the UK, the government is seeking to impose minimum densities on newly approved housing schemes in parts of South East England. The goal is to "build sustainable communities" rather than housing estates. However, commercial concerns tend to delay the opening of services until a large number of residents have occupied the new neighbourhood.

In Mexico, suburbs are generally similar to their United States counterparts. Houses are made in many different architectural styles which may be of European, American and International architecture and which vary in size. Suburbs can be found in Guadalajara, Mexico City, Monterrey, and most major cities. Lomas de Chapultepec is an example of an affluent suburb, although it is located inside the city and by no means is today a suburb in the strict sense of the word. In other countries, the situation is similar to that of Mexico, with many suburbs being built, most notably in Peru and Chile, which have experienced a boom in the construction of suburbs since the late 1970s and early 80s. As the growth of middle-class and upper-class suburbs increased, low-class squatter areas have increased, most notably "lost cities" in Mexico, campamentos in Chile, barriadas in Peru, villa miserias in Argentina, asentamientos in Guatemala and favelas of Brazil.

Brazilian affluent suburbs are generally denser, more vertical and mixed in use inner suburbs. They concentrate infrastructure, investment and attention from the municipal seat and the best offer of mass transit. True sprawling towards neighboring municipalities is typically empoverished – ("the periphery", in the sense of it dealing with spatial marginalization) –, with a very noticeable example being the rail suburbs of Rio de Janeiro – the North Zone, the Baixada Fluminense, the part of the West Zone associated with SuperVia's Ramal de Santa Cruz. These, in comparison with the inner suburbs, often prove to be remote, violent food deserts with inadequate sewer structure coverage, saturated mass transit, more precarious running water, electricity and communication services, and lack of urban planning and landscaping, while also not necessarily qualifying as actual or slums. They often are former agricultural land or wild areas settled through squatting, and grew in amount particularly due to mass rural exodus during the years of the military dictatorship. This is particularly true to São Paulo, Rio de Janeiro and Brasília, which grew with migration from more distant and empoverished parts of the country and dealt with overpopulation as a result.

In Africa, since the beginning of the 1990s, the development of middle-class suburbs boomed. Due to the industrialization of many African countries, particularly in cities such as Cairo, Johannesburg and Lagos, the middle class has grown. In an illustrative case of South Africa, RDP housing has been built. In much of Soweto, many houses are American in appearance, but are smaller, and often consist of a kitchen and living room, two or three bedrooms, and a bathroom. However, there are more affluent neighborhoods, more comparable to American suburbs, particularly east of the FNB Stadium. In Cape Town there is a distinct European style which is due to the European influence during the mid-1600s when the Dutch conquered the area. Houses like these are called Cape Dutch Houses and can be found in the affluent suburbs of Constantia and Bishopscourt.

In the illustrative case of Rome, Italy, in the 1920s and 1930s, suburbs were intentionally created "ex novo" in order to give lower classes a destination, in consideration of the actual and foreseen massive arrival of poor people from other areas of the country. Many critics have seen in this development pattern (which was circularly distributed in every direction) also a quick solution to a problem of public order (keeping the unwelcome poorest classes together with the criminals, in this way better controlled, comfortably remote from the elegant "official" town). On the other hand, the expected huge expansion of the town soon effectively covered the distance from the central town, and now those suburbs are completely engulfed by the main territory of the town. Other newer suburbs (called exurbs) were created at a further distance from them.

In Russia, the term suburb refers to high-rise residential apartments which usually consist of two bedrooms, one bathroom, a kitchen and a living room. These suburbs, however are usually not in poor neighborhoods, unlike the banlieuees.
In China, the term suburb is new, although suburbs are already being constructed rapidly. Chinese suburbs mostly consist of rows upon rows of apartment blocks and condos that end abruptly into the countryside. Also new town developments are extremely common. Single family suburban homes tend to be similar to their Western equivalents; although primarily outside Beijing and Shanghai, also mimic Spanish and Italian architecture. In Hong Kong, however, suburbs are mostly government-planned new towns containing numerous public housing estates. New Towns such as Tin Shui Wai may gain notoriety as a slum. However, other new towns also contain private housing estates and low density developments for the upper classes.

In Japan, the construction of suburbs has boomed since the end of World War II and many cities are experiencing the urban sprawl effect.
In Malaysia, suburbs are common, especially in areas surrounding the Klang Valley, which is the largest conurbation in the country. These suburbs also serve as major housing areas and commuter towns. Terraced houses, semi-detached houses and shophouses are common concepts in suburbs. In certain areas such as Klang, Subang Jaya and Petaling Jaya, suburbs form the core of these places. The latter one has been turned into a satellite city of Kuala Lumpur. Suburbs are also evident in other major conurbations in the country including Penang (e.g. Pulau Tikus), Ipoh (e.g. Bercham), Johor Bahru (e.g. Tebrau), Kota Kinabalu (e.g. Likas), Kuching (e.g. Stampin), Melaka City (e.g. Batu Berendam) and Alor Setar (e.g. Anak Bukit).

Suburbs typically have longer travel times to work than traditional neighborhoods. Only the traffic "within" the short streets themselves is less. This is due to three factors: almost-mandatory automobile ownership due to poor suburban bus systems, longer travel distances and the hierarchy system, which is less efficient at distributing traffic than the traditional grid of streets.

In the suburban system, most trips from one component to another component requires that cars enter a collector road, no matter how short or long the distance is. This is compounded by the hierarchy of streets, where entire neighborhoods and subdivisions are dependent on one or two collector roads. Because all traffic is forced onto these roads, they are often heavy with traffic all day. If a traffic crash occurs on a collector road, or if road construction inhibits the flow, then the entire road system may be rendered useless until the blockage is cleared. The traditional "grown" grid, in turn, allows for a larger number of choices and alternate routes.

Suburban systems of the sprawl type are also quite inefficient for cyclists or pedestrians, as the direct route is usually not available for them either. This encourages car trips even for distances as low as several hundreds of yards or meters (which may have become up to several miles or kilometers due to the road network). Improved sprawl systems, though retaining the car detours, possess cycle paths and footpaths connecting across the arms of the sprawl system, allowing a more direct route while still keeping the cars out of the residential and side streets.

More commonly, central cities seek ways to tax nonresidents working downtown – known as commuter taxes – as property tax bases dwindle. Taken together, these two groups of taxpayers represent a largely untapped source of potential revenue that cities may begin to target more aggressively, particularly if they're struggling. According to struggling cities, this will help bring in a substantial revenue for the city which is a great way to tax the people who make the most use of the highways and repairs.

Today more companies settle down in suburbs because of low property costs.

The history of suburbia is part of the study of urban history, which focuses on the origins, growth, diverse typologies, culture, and politics of suburbs, as well as on the gendered and family-oriented nature of suburban space. Many people have assumed that early-20th-century suburbs were enclaves for middle-class whites, a concept that carries tremendous cultural influence yet is actually stereotypical. Many suburbs are based on a heterogeneous society of working-class and minority residents, many of whom want to own their own house. Mary Corbin Sies argues that it is necessary to examine how "suburb" is defined as well as the distinction made between cities and suburbs, geography, economic circumstances, and the interaction of numerous factors that move research beyond acceptance of stereotyping and its influence on scholarly assumptions.

Suburbs and suburban living have been the subject for a wide variety of films, books, television shows and songs.

French songs like "La Zone" by Fréhel (1933), "Aux quatre coins de la banlieue" by Damia (1936), "Ma banlieue" by Reda Caire (1937), or "Banlieue" by Robert Lamoureux (1953), evoke the suburbs of Paris explicitly since the 1930s. Those singers give a sunny festive, almost bucolic, image of the suburbs, yet still few urbanized. During the fifties and the sixties, French singer-songwriter Léo Ferré evokes in his songs popular and proletarian suburbs of Paris, to oppose them to the city, considered by comparison as a bourgeois and conservative place.

French cinema was although soon interested in urban changes in the suburbs, with such movies as "Mon oncle" by Jacques Tati (1958), "L'Amour existe" by Maurice Pialat (1961) or "Two or Three Things I Know About Her" by Jean-Luc Godard (1967).

In his one-act opera "Trouble in Tahiti" (1952), Leonard Bernstein skewers American suburbia, which produces misery instead of happiness.

The American photojournalist Bill Owens documented the culture of suburbia in the 1970s, most notably in his book "Suburbia". The 1962 song "Little Boxes" by Malvina Reynolds lampoons the development of suburbia and its perceived bourgeois and conformist values, while the 1982 song "Subdivisions" by the Canadian band Rush also discusses suburbia, as does Rockin' the Suburbs by Ben Folds. The 2010 album "The Suburbs" by the Canadian-based alternative band Arcade Fire dealt with aspects of growing up in suburbia, suggesting aimlessness, apathy and endless rushing are ingrained into the suburban culture and mentality. "Suburb The Musical," was written by Robert S. Cohen and David Javerbaum. Over the Hedge is a syndicated comic strip written and drawn by Michael Fry and T. Lewis. It tells the story of a raccoon, turtle, a squirrel, and their friends who come to terms with their woodlands being taken over by suburbia, trying to survive the increasing flow of humanity and technology while becoming enticed by it at the same time. A film adaptation of Over the Hedge was produced in 2006.

British television series such as "The Good Life", "Butterflies" and "The Fall and Rise of Reginald Perrin" have depicted suburbia as well-manicured but relentlessly boring, and its residents as either overly conforming or prone to going stir crazy. In contrast, U.S. shows – such as "Knots Landing", "Desperate Housewives" and "Weeds" – portray the suburbs as concealing darker secrets behind a façade of perfectly manicured lawns, friendly people, and beautifully kept houses. Films such as "The 'Burbs", "Disturbia" and "Hot Fuzz", have brought this theme to the cinema. This trope was also used in the episode of "The X-Files" "Arcadia" and on one level of the video game "Psychonauts".





</doc>
<doc id="28910" url="https://en.wikipedia.org/wiki?curid=28910" title="Shōnen manga">
Shōnen manga

The kanji characters (少年) literally mean "boy" (or "youth"), and the characters (漫画) means "comic". Thus, the complete phrase means "young person's comic", or simply "boys' comic"; its female equivalent is shōjo manga. Shōnen manga is the most popular and best-selling form of manga.

Shōnen manga is typically characterized by high-action, often humorous plots featuring male protagonists. Commonly-found themes in shōnen manga include martial arts, robots, science fiction, sports, horror or mythological creatures. The camaraderie between boys or men on sports teams, fighting squads, and the like are often emphasized. Protagonists of such manga often feature an ongoing desire to better themselves., and often face challenges to their abilities, skills and maturity, where self-perfection, austere self-discipline, sacrifice in the cause of duty and honorable service to society, community, family and friends are stressed.

None of these listed characteristics are a requirement, as seen in shōnen manga like "Yotsuba&!", which features a female lead and almost no fan service or action; what defines whether or not a series is shōnen is the official classification of the magazine it is serialized in.

The art style of shōnen is generally less "flowery" than that of shōjo manga, although this varies greatly from artist to artist, and some artists draw both shōnen and shōjo manga.

Akira Toriyama's "Dragon Ball" (1984–1995) is credited with setting the trend of popular shōnen manga from the 1980s onward, with manga critic Jason Thompson in 2011 calling it "by far the most influential shōnen manga of the last 30 years." Many currently successful shōnen authors such as Eiichiro Oda, Masashi Kishimoto, Tite Kubo, Hiro Mashima and Kentaro Yabuki cite him and "Dragon Ball" as an influence on their own now popular works.

After the arrest and trial of serial killer Tsutomu Miyazaki, depictions of violence and sexual matters became more highly regulated in manga in general, but especially in shōnen manga.

Manga has been said to have existed since the eighteenth century, but originally did not target a specific gender or age group. By 1905, however, a boom in publishing manga magazines occurred, and began targeting genders as evidenced by their names, such as "Shōnen Sekai", "Shōjo Sekai", and "Shōnen Pakku" (a kodomo manga magazine). "Shōnen Sekai" was one of the first shōnen manga magazines, and was published from 1895 to 1914.

The post-World War II occupation of Japan had a profound impact on its culture during the 1950s and beyond (see culture of Post-occupation Japan), including on manga. Modern manga developed during this period, including the modern format of shōnen manga we experience today, of which boys and young men were among the earliest readers. During this time, Shōnen manga focused on topics thought to interest the archetypical boy: sci-tech subjects like robots and space travel, and heroic action-adventure. Osamu Tezuka, creator of "Astro Boy" is said to have played an influential role in manga during this period. Between 1950 and 1969, an increasingly large readership for manga emerged in Japan with the solidification of its two main marketing genres, shōnen manga aimed at boys and shōjo manga aimed at girls.

The magazine "Weekly Shōnen Jump" began production in 1968, and continues to be produced today as the best-selling manga magazine in Japan. Many of the most popular shōnen manga titles have been serialized in "Jump", including "Dragon Ball", "Captain Tsubasa", "Slam Dunk", "One Piece", "Naruto", "Bleach", and others.

With the relaxation of censorship in Japan in the 1990s, a wide variety of explicit sexual themes appeared in manga intended for male readers, and correspondingly occur in English translations. However, in 2010 the Tokyo Metropolitan Government passed the controversial Bill 156 to restrict harmful content despite opposition by many authors and publishers in the manga industry.

In early shōnen manga, men and boys played all the major roles. Of the nine cyborgs in Shotaro Ishinomori's 1964 "Cyborg 009", only one is female, and she soon vanishes from the action. Even some more modern instances of shōnen manga virtually omit women, e.g. the martial arts story "Baki the Grappler" by Itagaki Keisuke, and the supernatural fantasy "Sand Land" by Akira Toriyama. By the 1980s, however, girls and women began to play increasingly important roles in shōnen manga. For example, in Toriyama's 1980 "Dr. Slump", the main character is the mischievous and powerful girl robot Arale Norimaki. Discussing his character Lisa Lisa from "Battle Tendency", the second story arc of the manga series "Jojo’s Bizarre Adventure", author Hirohiko Araki stated that at the time female characters in shōnen manga were typically cute and designed to be "a man's ideal woman." He said readers were not interested in realistic portrayals of women, but rather the type of girl "that giggles during a conversation" with heart marks next to her. He believes this made the warrior-type Lisa Lisa feel fresh and "unheard of" in both manga and society in general and said it was exciting to challenge people's expectations with her. Araki also said that the supernatural basis of the fights in his series evened the battlefield for women and children to match up against strong men.

The role of girls and women in manga for male readers has evolved considerably since Arale. One class is the "bishōjo" or "beautiful young girl." Sometimes the woman is unattainable, and she is always an object of the hero's emotional and/or sexual interest, like Shao-lin from "Guardian Angel Getten" by Minene Sakurano or Belldandy from the seinen manga "Oh My Goddess!" by Kōsuke Fujishima. In other stories, the hero is surrounded by such girls and women, as in "Negima! Magister Negi Magi" by Ken Akamatsu and "Hanaukyo Maid Team" by Morishige. The male protagonist does not always succeed in forming a relationship with the woman, for example when Bright Honda and Aimi Komori fail to bond in "Shadow Lady" by Masakazu Katsura. In other cases, a successful couple's sexual activities are depicted or implied, like in "Outlanders" by Johji Manabe. In still other cases, the initially naive and immature hero grows up to become a man by learning how to deal and live with women emotionally and sexually; examples of heroes who follow this path include Yota in "Video Girl Ai" by Masakazu Katsura and Train Man in the seinen manga "" by Hidenori Hara.
However, since the 80s, there have been increase in female protagonists in shōnen manga, albeit lesser in number. They are often portrayed as central characters or characters with important roles in manga. Some examples include "Fullmetal Alchemist", "Urusei Yatsura", "Inuyasha", "Attack on Titan", "Ranma ½", "Fairy Tail", "Gunslinger Girl", "WataMote", "Nisekoi", "Strawberry Marshmallow", "School Rumble" and "Soul Eater".




</doc>
<doc id="28912" url="https://en.wikipedia.org/wiki?curid=28912" title="Srebrenica">
Srebrenica

Srebrenica (, ) is a town and municipality located in the easternmost part of Republika Srpska, an entity of Bosnia and Herzegovina. It is a small mountain town, with its main industry being salt mining and a nearby spa. As of 2013, town has a population of 2,604 inhabitants, while the municipality has 13,409 inhabitants.

During the Bosnian War, Srebrenica was the site of a massacre of more than 8,000 Bosniak men and boys, which was subsequently designated as an act of genocide by the International Criminal Tribunal for the former Yugoslavia and the International Court of Justice.

During the Roman times, there was a settlement of Domavia, known to have been near a mine. Silver ore from there was moved to the mints in Salona in the southwest and Sirmium in the northeast using the Via Argentaria.

In the 13th and 14th century the region was part of the Banate of Bosnia, and, subsequently, the Bosnian Kingdom. The earliest reference to the name Srebrenica was in 1376, by which time it was already an important centre for trade in the western Balkans, based especially on the silver mines of the region. (Compare modern srebro "silver".) By that time, a large number of merchants of the Republic of Ragusa were established there, and they controlled the domestic silver trade and the export by sea, almost entirely via the port of Ragusa (Dubrovnik). During the 14th century, many German miners moved into the area. There were often armed conflicts about Srebrenica because of its mines. According to Czech historian Konstantin Josef Jireček, from 1410 to 1460, Srebrenica switched hands several times, being Serbian five times, Bosnian four times, and Ottoman three times. The mines of Bosnian Podrinje and Usora were part of the Serbian Despotate prior to the Ottoman conquest.

With the town coming under Ottoman rule, becoming less influenced by the Republic of Ragusa, the economic importance of Srebrenica went into decline, as did the proportion of Christians in the population. The Franciscan monastery was converted into a mosque, but the large number of Catholics, Ragusan and Saxon, caused the transformation of the town to Islam to be slower than in most of the other towns in the area.

The area of Osat was liberated for a short time during the First Serbian Uprising (1804–13), under the leadership of Kara-Marko Vasić from Crvica. Upon the breakout of the uprising, Metropolitan Hadži Melentije Stevanović contacted Vasić, who met with the rebel leadership. After participated in battles on the Drina (1804), Vasić asked Karađorđe for an army to liberate Osat; Lazar Mutap was dispatched and the region came under rebel rule. In 1808, the Ottomans cleared out Osat, and by 1813, the rebels left the region.

During the Second World War there were many atrocities committed by the Chetniks and Ustashas. Partisans fought Chetniks and Ustashe during the war and the people of Srebrenica built a partisan memorial cemetery monument for the fallen victims.

The town of Srebrenica came to international prominence as a result of events during the Bosnian War (1992–1995). The strategic objectives proclaimed by the secessionist Bosnian Serb presidency included the creation of a border separating the Serb people from Bosnia's other ethnic communities and the abolition of the border along the River Drina separating Serbia and the Bosnian Serbs' Republika Srpska. The Bosnian Muslim/Bosniak majority population of the Drina Valley posed a major obstacle to the achievement of these objectives. In the early days of the campaign of forcible transfer (ethnic cleansing) that followed the outbreak of war in April 1992 the town of Srebrenica was occupied by Serb/Serbian forces. It was subsequently retaken by Bosniak resistance groups. Refugees expelled from towns and villages across the central Drina valley sought shelter in Srebrenica, swelling the town's population.

The town and its surrounding area was surrounded and besieged by Serb forces. On 16 April 1993, the United Nations declared the Bosnian Muslim/Bosniak enclave a UN safe area, to be "free from any armed attack or any other hostile act", and guarded by a small Dutch unit operating under the mandate of United Nations Protection Force (UNPROFOR), which did nothing to defend the local population. 

Srebrenica and the other UN safe areas of Žepa and Goražde were isolated pockets of Bosnian government-held territory in eastern Bosnia. In July 1995, despite the town's UN-protected status, it was attacked and captured by the Army of Republika Srpska. Following the town's capture, all men of fighting age who fell into Bosnian Serb hands were massacred in a systematically organised series of summary executions. The women of the town and men below 12 years of age and above 65 were transferred by bus to Tuzla.

The Srebrenica massacre is considered the worst genocide in post-Second World War European history to this day.

In 2001, the Srebrenica massacre was determined by judgement of the International Criminal Tribunal for the former Yugoslavia (ICTY) to have been a crime of genocide (confirmed on appeal in 2004). This finding was upheld in 2007 by the International Court of Justice. The decision of the ICTY was followed by an admission to and an apology for the massacre by the Republika Srpska government.

Under the 1995 Dayton Agreement which ended the Bosnian War, Srebrenica was included in the territory assigned to Bosnian Serb control as the Republika Srpska entity of Bosnia and Herzegovina. Although guaranteed under the provisions of the Dayton Agreement, the return of survivors was repeatedly obstructed. In 2007, verbal and physical attacks on returning refugees continued to be reported in the region around Srebrenica.

In 1992, Bosniak villages around Srebrenica were under constant attacks by Serb forces. The Bosnian Institute in the United Kingdom has published a list of 296 villages destroyed by Serb forces around Srebrenica three years before the genocide and in the first three months of war (April–June 1992):

According to the Naser Orić trial judgement:

The British National Archives in Kew released the documents dating back to July 1995 which deal with communication between British military and political actors during the Bosnian war. 

Several of the reports appear to blame the Bosniak Army (BiH) for provoking the Srebrenica attack. British intelligence doubted that Pale (Bosnian Serb headquarters) had any plans to overrun Srebrenica. Instead, the manoeuvre came as a response due to repeated Bosniak Army (BiH) attacks on BSA (Bosnian Serb Army) supply lines.

When Serbs entered the town, General Mladić threatened to shell the Dutch camp if UN troops do not disarm Bosniak troops. However, the report confirms no Bosniak army soldiers remained at the camp, all 2,000 armed Muslims "had simply left during the night" in the direction of Tuzla.<ref name="PREM 19/5487"></ref> 

The significance of these declassified documents is yet to be evaluated.

In 2007, Srebrenica's municipal assembly adopted a resolution demanding independence from the Republika Srpska entity (although not from Bosnia's sovereignty); the Serb members of the assembly did not vote on the resolution. In the 2016 elections Mladen Grujičić, a Bosnian Serb and native of the town of Srebrenica, was elected as the Mayor of Srebrenica.

The municipality (општина or "opština") is further subdivided into the following local communities (мјесне заједнице or "mjesne zajednice"):

According to the 2013 census results, the municipality of Srebrenica has a population of 13,409 inhabitants.

The borders of the municipality in the 1953 and 1961 census were different. In 1953, a distinctive Muslim nationality had been yet to emerge as an ethnicity, leading Slavic Muslims to identify as Yugoslavs. As "Yugoslav" was itself not adopted in 1948, they were classified as "other" and while many self-identified as “Serbs” or “Croats”. Until 1961 census, the municipality of Srebrenica included today's territory of Bratunac municipality. The ethnic composition of the municipality:

Before 1992, there was a metal factory in the town, and lead, zinc, and gold mines nearby. The town's name (Srebrenica) means "silver mine", the same meaning of its old Latin name "Argentaria".

Before the war, Srebrenica also had a big spa and the town prospered from tourism. Nowadays, Srebrenica has some tourism but a lot less developed than before the war. Currently, a pension, motel and a hostel are operating in the town.

The following table gives a preview of total number of registered people employed in legal entities per their core activity (as of 2018):



</doc>
<doc id="28913" url="https://en.wikipedia.org/wiki?curid=28913" title="Steve Bracks">
Steve Bracks

Stephen Phillip Bracks AC (born 15 October 1954) is a former Australian politician and the 44th Premier of Victoria. He first won the electoral district of Williamstown in 1994 for the Labor Party and was party leader and premier from 1999 to 2007.

Bracks led Labor in Victoria to minority government at the 1999 election, defeating the incumbent Jeff Kennett Liberal and National coalition government. Labor was returned with a majority government after a landslide win at the 2002 election. Labor was elected for a third term at the 2006 election with a substantial but reduced majority. Bracks is the second-longest-serving Labor premier in Victorian history, only John Cain Jr. served for a longer period. The treasurer, John Brumby, became Labor leader and premier in 2007 when Bracks retired from politics.

Bracks will serve as the 6th Chancellor of Victoria University from 2021.

Steve Bracks was born in Ballarat, where his family owns a fashion business. He is a Lebanese Australian; his paternal grandfather came to Australia as a child from Zahlé in the Beqaa Valley of Lebanon in the 1890s. His family were Melkite Catholic before migrating and became Roman Catholic.

Bracks was educated in Ballarat at St Patrick's College and the Ballarat College of Advanced Education (now the Federation University), where he graduated in business studies and education. He became a keen follower of Australian rules football, supporting the Geelong Football Club.

From 1976 to 1981 Bracks was a school commerce teacher at Sacred Heart College, Ballarat. During the 1980s he worked in local government in Ballarat and then as Executive Director of the Ballarat Education Centre. While in these positions he twice (1985 and 1988) contested the seat of Ballarat North in the Victorian Legislative Assembly for the Labor Party.

In 1989 Bracks was appointed statewide manager of Victorian state government employment programs, under the Labor government of John Cain Jr. He then became an adviser to both Cain and Cain's successor as Premier, Joan Kirner. Here he was able to witness from the inside the collapse of the Labor government following the economic and budgetary crisis which began in 1988. This experience gave Bracks a very conservative and cautious view of economic management in government.

Following the defeat of the Kirner government by the Liberal leader Jeff Kennett in late 1992, Bracks became Executive Director of the Victorian Printing Industry Training Board. He quit this post in 1994 when Kirner resigned from Parliament and Bracks was elected for Kirner's seat of Williamstown in the western suburbs of Melbourne, where he lived with his wife Terry and their three children. One of his children is Nick Bracks, Australian model.

Bracks was immediately elected to Labor's front bench, as Shadow Minister for Employment, Industrial Relations and Tourism. In 1996, after Labor under John Brumby was again defeated, he became Shadow Treasurer. In March 1999, when it became apparent that Labor was headed for another defeat under Brumby's leadership, Brumby resigned and Bracks was elected Opposition Leader.

Political observers were almost unanimous that Bracks had no chance of defeating Liberal premier Jeff Kennett at the September 1999 election: polls gave Kennett a 60% popularity rating. Bracks and his senior colleagues (particularly Brumby, who comes from Bendigo) campaigned heavily in regional areas, accusing Kennett of ignoring regional communities. In response, voters in regional areas deserted the Kennett government. On election night, much to its own surprise, Labor increased its seat count from 29 to 41, with the Liberals and their National Party allies retaining 43, and three falling to rural independents. With the Coalition one seat short of government, the election was to be decided in Frankston East, when the death of incumbent Peter McLellan forced a supplementary election. That supplementary election was won by Labor on a large swing, resulting in a hung parliament. The independents then threw their support to Labor, allowing Bracks to form government by one seat.

The Coalition briefly considered forcing Bracks to demonstrate that he had support on the floor of the Assembly. However, two of the independents, Russell Savage and Susan Davies, felt Kennett had given them short shrift in the previous legislature, and would not have even considered supporting him. In any event, this gambit was brought undone when Kennett announced his retirement from politics on 20 October. Bracks then advised the Governor, Sir James Gobbo, that he could form a government, which was duly sworn in later that day. Bracks became the first Catholic Labor Premier of Victoria since 1932.

Former leader Brumby, appointed Treasurer, was regarded as a major part of the government's success. He and the Deputy Premier and Minister for Health, John Thwaites, and the Attorney-General, Rob Hulls, were regarded as the key ministers in the Bracks government.

Following a pre-1999 election commitment to consider the feasibility of introducing fast rail services to regional centres, in 2000 the government approved funding for the Regional Fast Rail project, upgrading rail lines between Melbourne and Ballarat, Bendigo, Geelong and Traralgon. However, in 2006 the Victorian Auditor General noted that in spite of $750 million spent, "We found that the delivery of more frequent fast rail services in the Geelong, Ballarat, and Bendigo corridors by the agreed dates was not achieved. In total, the journey time outcomes will be more modest than we would have expected with only a minority of travellers likely to benefit from significant journey time improvements. These outcomes occur because giving some passengers full express services means bypassing often large numbers of passengers at intermediate stations along the corridors."

On 14 December 2000, Steve Bracks released a document outlining his government's intent to introduce the Racial and Religious Tolerance Act 2001.

The major criticism of Bracks's first government was that their insistence on consultation stood in the way of effective, proactive government. Bracks, according to critics, achieved little, and lost the excitement of constant change that was characteristic of the Kennett years. The talents of some of the more junior ministers in the government were also questioned. Nevertheless, Bracks got through his first term without major mishaps, and his popularity undiminished.

Labor won the 2002 election in a landslide, taking 62 seats out of 88 in the Legislative Assembly—only the third time in Victoria's history that a Labor government had been reelected. In another first, Labor won a slim but clear majority in the Legislative Council as well. While this was the greatest victory Labor had ever had in a Victorian state election, it brought with it considerable risks. With majorities in both houses Bracks could no longer cite his weak parliamentary position as an excuse for inaction.

On 28 August 2002, Bracks, in conjunction with his then New South Wales counterpart, Bob Carr, opened the Mowamba aqueduct between Jindabyne and Dalgety, to divert 38 gigalitres of water a year from Lake Eucumbene to the Snowy and Murray rivers. The ten-year plan cost A$300 million with Victoria and NSW splitting the costs. Melbourne Water has stated that within 50 years there will be 20 per cent less water going into Victorian reservoirs.

In May 2003 Bracks broke an election promise and announced that the proposed Scoresby Freeway in Melbourne's eastern suburbs would be a tollway rather than a freeway, as promised at the 2002 elections. As well as risking a loss of support in marginal seats in eastern Melbourne, this decision brought about a strong response from the Howard Federal government, which cut off federal funding for the project on the grounds that the Bracks government had reneged on the terms of the federal-state funding agreement. The decision seems to have been on the recommendation of Brumby, who was concerned with the state's budgetary position. Also opposing the decision was the Federal Labor Opposition, which feared anti-Labor reaction at the 2004 Federal election. The then Opposition Leader Mark Latham described a meeting with Bracks and federal shadow ministers, writing:
This backflip, while seen by many as an opportunity for the Liberals to make ground, saw the then leader of the Liberals, Robert Doyle, adopt a much-criticised policy of half tolls, which was later overturned by his successor, Ted Baillieu.

In 2005, following extensive independent studies it was found that cattle had created extensive damage to the high country National Park and their continued presence in the Park was incompatible with the values of National Parks. Bracks backed the environment and his environment minister, John Thwaites and announced that Victoria would follow the NSW example and cattlemen would be banned from using the "High Plains" in Victoria's National Parks to graze cattle. Some said this ended a 170-year tradition, the reality was the ban was only in the National Parks. Stockmen had been fearing this decision since 1984, when a Labor government excised land to create the Alpine National Park. Some estimated three hundred cattlemen rode horses down Bourke street in protest while police said it was closer to 100. Colourful Victorian National Party leader Peter Ryan was quoted as saying that Bracks had "killed the man from Snowy River", a reference to the Banjo Paterson poem "The Man from Snowy River"... which was a bit strange because the Poem was about mustering horses not cattle - a practice which was stopped in the high country just after World War 2.

Bracks' second government achieved one of Victorian Labor's longest-held goals with a complete reform of the state's system for electing its upper house. It saw the introduction of proportional representation, with eight five-member regions replacing the current single-member constituencies. This system increases the opportunity for minor parties such as the Greens and DLP to win seats in the Legislative Council, giving them a greater chance of holding the balance of power. Illustrating the historic importance Labor assigns to the changes, in a speech to a conference celebrating the 150th anniversary of the Eureka Stockade, Bracks said it was "another victory for the aspirations of Eureka", and has described the changes as "his proudest achievement".

The staging of the 2006 Commonwealth Games, generally viewed as a success (albeit an expensive one), was viewed as a plus for Bracks and the government. With times reasonably good, a perception arguably reinforced by an extensive government advertising campaign selling the virtues of Victoria to Victorians, polls indicated little interest in change, although towards the end of the election campaign polling indicated that the Liberals under Baillieu were closing the gap.

The election campaign was a relatively low-key affair, with the Government and Bracks largely running on their record, as well as their plans to tackle infrastructure issues in their third term. Bracks' image loomed large in Labor's election advertising. Liberal attacks concentrated on the slow process of infrastructure development under Bracks (notably on water supply issues relating to the severe drought affecting Victoria in the election leadup), and new Liberal leader Ted Baillieu promised to start construction on a range of new infrastructure initiatives, including a new dam on the Maribyrnong River and a desalination plant. Labor's broken election promise on Eastlink was also expected to be a factor in some seats in the eastern suburbs of Melbourne.

On 25 November 2006, Steve Bracks won his third election, comfortably defeating Baillieu to secure a third term, with a slightly reduced majority in the Lower House. This marked only the second time that the Victorian Labor Party had won a third term in office. His third term Cabinet was sworn in on 1 December 2006 with Bracks also holding the portfolio of Veterans' Affairs and Multicultural Affairs.

Bracks announced his resignation as Premier on 27 July 2007, saying this was in order to spend more time with his family. He stepped down on 30 July 2007. According to the ABC, Bracks had been under political and personal pressure in the weeks before his resignation. Alone among State Premiers, he had refused to agree to the Federal Government's $10 billion Murray-Darling Basin water conservation plan, and his son had been involved in an accident involving a charge of drink driving. Bracks told a media conference he could no longer give a 100 per cent commitment to politics:

Bracks' deputy John Thwaites announced his resignation on the same day. News of the resignations caused surprise to the general community as well as to politicians. It was revealed that then Federal Labor Leader Kevin Rudd was informed only minutes before the announcement, and tried to talk Bracks out of his decision. Bracks' Treasurer John Brumby was elected unopposed by the Victorian Labor Caucus as Premier, while Attorney-General Rob Hulls was elected Deputy Premier.

One consequence of Bracks leaving politics may have been the introduction of abortion law reform in Victoria. It has been suggested that the resignation of Premier Bracks sowed the seeds for abortion law reform by legislation that parliamentarians previously had refused to support, fearing a backlash from anti-abortion groups led by veteran campaigner Margaret Tighe. Bracks, as a Catholic of Lebanese descent, almost certainly would not have allowed abortion legislation into the parliament, but his successor John Brumby did not share this view, and the Abortion Law Reform Bill introduced by upper house member Candy Broad was passed by the Parliament in 2008.

In August 2007, following his resignation as Premier, Bracks announced he would provide a short-term pro bono advising role in East Timor working alongside the newly elected Prime Minister Xanana Gusmão. Bracks was to spend a year travelling between Melbourne and Dili helping with the establishment of Gusmão's administration, the key departments that would need to be involved, and advising on how they would be accountable and reportable to the legislature.

During 2008 Bracks indicated his support for Victorian abortion law reform in Victoria.

In addition to his role advising Gusmão, Bracks also joined several company advisory boards: KPMG, insurance firm Jardine Lloyd Thompson Group, the AIMS Financial Group and the NAB. The KPMG appointment was controversial, as the Victorian government had awarded the firm over 100 contracts during Bracks' time as Premier. On 14 February 2008, the Federal Labor Government appointed Bracks to head an inquiry into the ongoing viability of the Australian car industry.

In 2010, Bracks was appointed a Companion of the Order of Australia for services to the community and the Parliament of Victoria. In recognition of his distinguished services to the Victorian community, he was awarded the degree of Doctor of Laws (honoris causa) – LL.D "(h.c.)" by Deakin University on 27 April 2010. He was also appointed to the Honorary Chair of the Deakin University Foundation.

In February 2013 after the announcement that Nicola Roxon would retire from federal politics, Bracks was cited as a possible candidate for her safe Labor seat of Gellibrand, but he ruled out running for the seat.

Bracks was appointed to the role of Australian Consul-General in New York in May 2013, by the Federal ALP Government of Julia Gillard. At the time, the shadow Foreign Minister, the Coalition's Julie Bishop, described the appointment as "inappropriate" because of the proximity to the upcoming election and "arrogant" because of a lack of consultation with the then-opposition. Following the defeat of the ALP at the 7 September election, incoming foreign minister Julie Bishop reversed the appointment in a decision described as 'petty and vindictive' by acting ALP foreign affairs spokeswoman Tanya Plibersek.

In March 2019, it was announced that Bracks will serve as the 6th Chancellor of Victoria University from 2021.

 


</doc>
<doc id="28915" url="https://en.wikipedia.org/wiki?curid=28915" title="Small Isles">
Small Isles

The Small Isles ("") are a small archipelago of islands in the Inner Hebrides, off the west coast of Scotland. They lie south of Skye and north of Mull and Ardnamurchan – the most westerly point of mainland Scotland. 

The islands form part of the Lochaber area of the Highland council area. Until 1891 Canna, Rùm and Muck were historically part of the shire of Argyll; Eigg was historically part of Inverness-shire. All of the Small Isles were in Inverness-shire between 1891 and 1975, and remain part of the registration county of Inverness for land registration and statistical purposes. A single community council covers the islands.

"Small Isles" is the name of the conterminous civil parish and former Church of Scotland parish, originally created in 1726 from part of Sleat parish, the balance of which lies on the much larger island of Skye. The original name of the new parish was Eigg or Short Isles. "In process of time the name was by an easy transition changed from 'Short' to 'Small' Isles." The islands are not especially small, with Rùm being the 15th largest in Scotland. The Gaelic name of "" translates as "cross isles" referring to the islands position between Morar and the Uists.

The four main islands are Canna, Rùm, Eigg and Muck. The largest is Rùm with an area of .

Smaller islands surrounding the main four include:

There are also a number of skerries:

According to the 2011 census, the total population of the Small Isles was 153. Five of the islands are inhabited: Eigg (83), Muck (27), Rùm (22), Canna (12) and Sanday (9).

The inhabited islands are in contrasting forms of ownership: Canna (along with the tidally linked Sanday) is owned by a national conservation charity, the National Trust for Scotland; Eigg has been owned by a local community trust since 1997; Muck remains in private ownership; and Rùm is largely in the hands of the state (via Scottish Natural Heritage), although some land in and around the only village (Kinloch) is owned by a community trust.

A Caledonian MacBrayne ferry, , links the Small Isles to each other and to the mainland port of Mallaig. The ferry runs a daily service, calling at different islands depending on the day of the week; there are two calls at certain islands on each day to allow for day visits to and from each island. The Lochnevis has a landing craft-style stern ramp allowing vehicles to be driven onto and off the vessel at a new slipway constructed in 2001, however visitors are not normally permitted to bring vehicles to the Small Isles. During the summer months the islands are also served by Arisaig Marine's passenger ferry "MV Sheerwater" from Arisaig, south of Mallaig. Timetables are also arranged to allow time onshore on different islands depending on the day of the week.

The Small Isles are all important for their wildlife, with Rùm being designated as both a national nature reserve and a Special Area of Conservation (SAC). Rùm is home to one of the world’s largest colony of Manx shearwater, and was the location for the first stage of the reintroduction of white-tailed sea-eagles into Scotland, with 82 birds being released between 1975 and 1985. Rùm, and Canna and Sanday (jointly), are designated as Special Protection Areas (SPA) due their birdlife, with all three islands hosting important breeding populations of guillemots and kittiwakes. The Canna and Sanday SPA is also designated due to its importance to breeding Atlantic puffins and shags, whilst the Rùm SPA designation notes the presence of golden eagles, Manx shearwaters, and red-throated divers.

Around of the waters around Rùm, Canna and the low-lying rocky islet of Oigh-sgeir have been designated as the Small Isles Nature Conservation Marine Protected Area (NCMPA). Of particular note is that this area holds the UK's only known colony of fan mussels. The seas surrounding all of the Small Isles have also been designated as a SAC due to their importance for harbour porpoises.

The islands and surrounding sea area together form the Small Isles national scenic area, one of the forty such areas in Scotland, which are defined so as to identify areas of exceptional scenery and to ensure its protection from inappropriate development. The designated area covers in total, of which is on land and the remaining is marine (i.e. below low tide level).



</doc>
