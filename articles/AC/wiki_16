<doc id="25740" url="https://en.wikipedia.org/wiki?curid=25740" title="Robert E. Lee">
Robert E. Lee

Robert Edward Lee (January 19, 1807 – October 12, 1870) was an American and Confederate soldier, best known as a commander of the Confederate States Army. He commanded the Army of Northern Virginia in the American Civil War from 1862 until its surrender in 1865.

A son of Revolutionary War officer Henry "Light Horse Harry" Lee III, Lee was a top graduate of the United States Military Academy and an exceptional officer and military engineer in the United States Army for 32 years. During this time, he served throughout the United States, distinguished himself during the Mexican–American War, and served as Superintendent of the United States Military Academy. He was also the husband of Mary Anna Custis Lee, adopted great granddaughter of George Washington.

When Virginia's 1861 Richmond Convention declared secession from the Union, Lee chose to follow his home state, despite his desire for the country to remain intact and an offer of a senior Union command. During the first year of the Civil War, he served as a senior military adviser to Confederate President Jefferson Davis. Once he took command of the Army of Northern Virginia in 1862, he soon emerged as an able tactician and battlefield commander, winning most of his battles, nearly all against far larger Union armies.

Lee's two major strategic offensives into Union territory ended in defeat. His aggressive tactics, especially at the Battle of Gettysburg, which resulted in high casualties at a time when the Confederacy had a shortage of manpower, have come under criticism in recent years.

Lee surrendered the remnant of his army to Ulysses S. Grant at Appomattox Court House on April 9, 1865. Since Confederate Congress had appointed him supreme commander of Confederate armies, the remaining Confederate forces capitulated after his surrender. Lee rejected the proposal of a sustained insurgency against the Union and called for national reconciliation.

In 1865, Lee became president of Washington College (later Washington and Lee University) in Lexington, Virginia; in that position, he supported reconciliation between North and South. He accepted "the extinction of slavery" provided for by the Thirteenth Amendment, but publicly opposed racial equality and granting African Americans the right to vote and other political rights. Lee died in 1870. In 1975, the U.S. Congress posthumously restored Lee's citizenship effective June 13, 1865.

Lee was born at Stratford Hall Plantation in Westmoreland County, Virginia, to Major General Henry Lee III (Light Horse Harry) (1756–1818), Governor of Virginia, and his second wife, Anne Hill Carter (1773–1829). His birth date has traditionally been recorded as January 19, 1807, but according to the historian Elizabeth Brown Pryor, "Lee's writings indicate he may have been born the previous year."

One of Lee's great grandparents, Henry Lee I, was a prominent Virginian colonist of English descent. Lee's family is one of Virginia's first families, descended from Richard Lee I, Esq., "the Immigrant" (1618–1664), from the county of Shropshire in England.

Lee's mother grew up at Shirley Plantation, one of the most elegant homes in Virginia. Lee's father, a tobacco planter, suffered severe financial reverses from failed investments.

Little is known of Lee as a child; he rarely spoke of his boyhood as an adult. Nothing is known of his relationship with his father who, after leaving his family, mentioned Robert only once in a letter. When given the opportunity to visit his father's Georgia grave, he remained there only briefly; yet, during his time as president of Washington College, he defended his father in a biographical sketch while editing Light Horse Harry's memoirs.

In 1809, Harry Lee was put in debtors' prison; soon after his release the following year, Harry and Anne Lee and their five children moved to a small house on Cameron Street in Alexandria, Virginia, both because there were then high quality local schools there, and because several members of her extended family lived nearby. In 1811, the family, including the newly born sixth child, Mildred, moved to a house on Oronoco Street, still close to the center of town and with the houses of a number of Lee relatives close by.

In 1812, Harry Lee was badly injured in a political riot in Baltimore and traveled to the West Indies. He would never return, dying when his son Robert was eleven years old. Left to raise six children alone in straitened circumstances, Anne Lee and her family often paid extended visits to relatives and family friends. Robert Lee attended school at Eastern View, a school for young gentlemen, in Fauquier County, and then at the Alexandria Academy, free for local boys, where he showed an aptitude for mathematics. Although brought up to be a practicing Christian, he was not confirmed in the Episcopal Church until age 46.

Anne Lee's family was often supported by a relative, William Henry Fitzhugh, who owned the Oronoco Street house and allowed the Lees to stay at his home in Fairfax County, Ravensworth. When Robert was 17 in 1824, Fitzhugh wrote to the Secretary of War, John C. Calhoun, urging that Robert be given an appointment to the United States Military Academy at West Point. Fitzhugh wrote little of Robert's academic prowess, dwelling much on the prominence of his family, and erroneously stated the boy was 18. Instead of mailing the letter, Fitzhugh had young Robert deliver it. In March 1824, Robert Lee received his appointment to West Point, but due to the large number of cadets admitted, Lee would have to wait a year to begin his studies there.

Lee entered West Point in the summer of 1825. At the time, the focus of the curriculum was engineering; the head of the Army Corps of Engineers supervised the school and the superintendent was an engineering officer. Cadets were not permitted leave until they had finished two years of study, and were rarely allowed off the Academy grounds. Lee graduated second in his class, behind only Charles Mason (who resigned from the Army a year after graduation). Lee did not incur any demerits during his four-year course of study, a distinction shared by five of his 45 classmates. In June 1829, Lee was commissioned a brevet second lieutenant in the Corps of Engineers. After graduation, while awaiting assignment, he returned to Virginia to find his mother on her deathbed; she died at Ravensworth on July 26, 1829.

On August 11, 1829, Brigadier General Charles Gratiot ordered Lee to Cockspur Island, Georgia. The plan was to build a fort on the marshy island which would command the outlet of the Savannah River. Lee was involved in the early stages of construction as the island was being drained and built up. In 1831, it became apparent that the existing plan to build what became known as Fort Pulaski would have to be revamped, and Lee was transferred to Fort Monroe at the tip of the Virginia Peninsula (today in Hampton, Virginia).

While home in the summer of 1829, Lee had apparently courted Mary Custis whom he had known as a child. Lee obtained permission to write to her before leaving for Georgia, though Mary Custis warned Lee to be "discreet" in his writing, as her mother read her letters, especially from men. Custis refused Lee the first time he asked to marry her; her father did not believe the son of the disgraced Light Horse Harry Lee was a suitable man for his daughter. She accepted him with her father's consent in September 1830, while he was on summer leave, and the two were wed on June 30, 1831.

Lee's duties at Fort Monroe were varied, typical for a junior officer, and ranged from budgeting to designing buildings. Although Mary Lee accompanied her husband to Hampton Roads, she spent about a third of her time at Arlington, though the couple's first son, Custis Lee was born at Fort Monroe. Although the two were by all accounts devoted to each other, they were different in character: Robert Lee was tidy and punctual, qualities his wife lacked. Mary Lee also had trouble transitioning from being a rich man's daughter to having to manage a household with only one or two slaves. Beginning in 1832, Robert Lee had a close but platonic relationship with Harriett Talcott, wife of his fellow officer Andrew Talcott.

Life at Fort Monroe was marked by conflicts between artillery and engineering officers. Eventually the War Department transferred all engineering officers away from Fort Monroe, except Lee, who was ordered to take up residence on the artificial island of Rip Raps across the river from Fort Monroe, where Fort Wool would eventually rise, and continue work to improve the island. Lee duly moved there, then discharged all workers and informed the War Department he could not maintain laborers without the facilities of the fort.

In 1834, Lee was transferred to Washington as General Gratiot's assistant. Lee had hoped to rent a house in Washington for his family, but was not able to find one; the family lived at Arlington, though Lieutenant Lee rented a room at a Washington boarding house for when the roads were impassable. In mid-1835, Lee was assigned to assist Andrew Talcott in surveying the southern border of Michigan. While on that expedition, he responded to a letter from an ill Mary Lee, which had requested he come to Arlington, "But why do you urge my "immediate" return, & tempt one in the "strongest" manner<nowiki>[?]</nowiki> ... I rather require to be strengthened & encouraged to the "full" performance of what I am called on to execute." Lee completed the assignment and returned to his post in Washington, finding his wife ill at Ravensworth. Mary Lee, who had recently given birth to their second child, remained bedridden for several months. In October 1836, Lee was promoted to first lieutenant.

Lee served as an assistant in the chief engineer's office in Washington, D.C. from 1834 to 1837, but spent the summer of 1835 helping to lay out the state line between Ohio and Michigan. As a first lieutenant of engineers in 1837, he supervised the engineering work for St. Louis harbor and for the upper Mississippi and Missouri rivers. Among his projects was the mapping of the Des Moines Rapids on the Mississippi above Keokuk, Iowa, where the Mississippi's mean depth of was the upper limit of steamboat traffic on the river. His work there earned him a promotion to captain. Around 1842, Captain Robert E. Lee arrived as Fort Hamilton's post engineer.

While Lee was stationed at Fort Monroe, he married Mary Anna Randolph Custis (1808–1873), great-granddaughter of Martha Washington by her first husband Daniel Parke Custis, and step-great-granddaughter of George Washington, the first president of the United States. Mary was the only surviving child of George Washington Parke Custis, George Washington's stepgrandson, and Mary Lee Fitzhugh Custis, daughter of William Fitzhugh and Ann Bolling Randolph. Robert and Mary married on June 30, 1831, at Arlington House, her parents' house just across from Washington, D.C. The 3rd U.S. Artillery served as honor guard at the marriage. They eventually had seven children, three boys and four girls:


All the children survived him except for Annie, who died in 1862. They are all buried with their parents in the crypt of the Lee Chapel at Washington and Lee University in Lexington, Virginia.

Lee was a great-great-great grandson of William Randolph and a great-great grandson of Richard Bland. He was also related to Helen Keller through Helen's mother, Kate, and was a distant relative of Admiral Willis Augustus Lee.

On May 1, 1864, General Lee was at the baptism of General A.P. Hill's daughter, Lucy Lee Hill, to serve as her godfather. This is referenced in the painting "Tender is the Heart" by Mort Künstler. He is the godfather of actress and writer Odette Tyler, the daughter of brigadier general William Whedbee Kirkland.

Lee distinguished himself in the Mexican–American War (1846–1848). He was one of Winfield Scott's chief aides in the march from Veracruz to Mexico City. He was instrumental in several American victories through his personal reconnaissance as a staff officer; he found routes of attack that the Mexicans had not defended because they thought the terrain was impassable.

He was promoted to brevet major after the Battle of Cerro Gordo on April 18, 1847. He also fought at Contreras, Churubusco, and Chapultepec and was wounded at the last. By the end of the war, he had received additional brevet promotions to lieutenant colonel and colonel, but his permanent rank was still captain of engineers, and he would remain a captain until his transfer to the cavalry in 1855.

For the first time, Robert E. Lee and Ulysses S. Grant met and worked with each other during the Mexican–American War. Close observations of their commanders constituted a learning process for both Lee and Grant. The Mexican–American War concluded on February 2, 1848.

After the Mexican War, Lee spent three years at Fort Carroll in Baltimore harbor. During this time, his service was interrupted by other duties, among them surveying and updating maps in Florida. Cuban revolutionary Narciso López intended to forcibly liberate Cuba from Spanish rule. In 1849, searching for a leader for his filibuster expedition, he approached Jefferson Davis, then a United States senator. Davis declined and suggested Lee, who also declined. Both decided it was inconsistent with their duties.

The 1850s were a difficult time for Lee, with his long absences from home, the increasing disability of his wife, troubles in taking over the management of a large slave plantation, and his often morbid concern with his personal failures.

In 1852, Lee was appointed Superintendent of the Military Academy at West Point. He was reluctant to enter what he called a "snake pit", but the War Department insisted and he obeyed. His wife occasionally came to visit. During his three years at West Point, Brevet Colonel Robert E. Lee improved the buildings and courses and spent much time with the cadets. Lee's oldest son, George Washington Custis Lee, attended West Point during his tenure. Custis Lee graduated in 1854, first in his class.

Lee was enormously relieved to receive a long-awaited promotion as second-in-command of the 2nd Cavalry Regiment in Texas in 1855. It meant leaving the Engineering Corps and its sequence of staff jobs for the combat command he truly wanted. He served under Colonel Albert Sidney Johnston at Camp Cooper, Texas; their mission was to protect settlers from attacks by the Apache and the Comanche.

In 1857, his father-in-law George Washington Parke Custis died, creating a serious crisis when Lee took on the burden of executing the will. Custis's will encompassed vast landholdings and hundreds of slaves balanced against massive debts, and required Custis's former slaves "to be emancipated by my executors in such manner as to my executors may seem most expedient and proper, the said emancipation to be accomplished in not exceeding five years from the time of my decease." The estate was in disarray, and the plantations had been poorly managed and were losing money.
Lee tried to hire an overseer to handle the plantation in his absence, writing to his cousin, "I wish to get an energetic honest farmer, who while he will be considerate & kind to the negroes, will be firm & make them do their duty." But Lee failed to find a man for the job, and had to take a two-year leave of absence from the army in order to run the plantation himself.

Lee's cruelty on the Arlington plantation nearly led to a slave revolt, since many of the slaves had been given to understand that they were to be made free as soon as Custis died, and protested angrily at the delay. In May 1858, Lee wrote to his son Rooney, "I have had some trouble with some of the people. Reuben, Parks & Edward, in the beginning of the previous week, rebelled against my authority—refused to obey my orders, & said they were as free as I was, etc., etc.—I succeeded in capturing them & lodging them in jail. They resisted till overpowered & called upon the other people to rescue them." Less than two months after they were sent to the Alexandria jail, Lee decided to remove these three men and three female house slaves from Arlington, and sent them under lock and key to the slave-trader William Overton Winston in Richmond, who was instructed to keep them in jail until he could find "good & responsible" slaveholders to work them until the end of the five-year period.

Lee ruptured the Washington and Custis tradition of respecting slave families and by 1860 he had broken up every family but one on the estate, some of whom had been together since Mount Vernon days.

In 1859, three of the Arlington slaves—Wesley Norris, his sister Mary, and a cousin of theirs—fled for the North, but were captured a few miles from the Pennsylvania border and forced to return to Arlington. On June 24, 1859, the anti-slavery newspaper "New York Daily Tribune" published two anonymous letters (dated June 19, 1859 and June 21, 1859), each claiming to have heard that Lee had the Norrises whipped, and each going so far as to claim that the overseer refused to whip the woman but that Lee took the whip and flogged her personally. Lee privately wrote to his son Custis that "The N. Y. Tribune has attacked me for my treatment of your grandfather's slaves, but I shall not reply. He has left me an unpleasant legacy."

Wesley Norris himself spoke out about the incident after the war, in an 1866 interview printed in an abolitionist newspaper, the "National Anti-Slavery Standard". Norris stated that after they had been captured, and forced to return to Arlington, Lee told them that "he would teach us a lesson we would not soon forget." According to Norris, Lee then had the three of them firmly tied to posts by the overseer, and ordered them whipped with fifty lashes for the men and twenty for Mary Norris. Norris claimed that Lee encouraged the whipping, and that when the overseer refused to do it, called in the county constable to do it instead. Unlike the anonymous letter writers, he does not state that Lee himself whipped any of the slaves. According to Norris, Lee "frequently enjoined [Constable] Williams to 'lay it on well,' an injunction which he did not fail to heed; not satisfied with simply lacerating our naked flesh, Gen. Lee then ordered the overseer to thoroughly wash our backs with brine, which was done."

The Norris men were then sent by Lee's agent to work on the railroads in Virginia and Alabama. According to the interview, Norris was sent to Richmond in January 1863 "from which place I finally made my escape through the rebel lines to freedom." But Federal authorities reported that Norris came within their lines on September 5, 1863, and that he "left Richmond ... with a pass from General Custis Lee." Lee freed the Custis slaves, including Wesley Norris, after the end of the five-year period in the winter of 1862, filing the deed of manumission on December 29, 1862.

Biographers of Lee have differed over the credibility of the account of the punishment as described in the letters in the "Tribune" and in Norris's personal account. They broadly agree that Lee had a group of escaped slaves recaptured, and that after recapturing them he hired them out off of the Arlington plantation as a punishment; but they disagree over the likelihood that Lee flogged them, and over the charge that he personally whipped Mary Norris. In 1934, Douglas S. Freeman described them as "Lee's first experience with the extravagance of irresponsible antislavery agitators" and asserted that "There is no evidence, direct or indirect, that Lee ever had them or any other Negroes flogged. The usage at Arlington and elsewhere in Virginia among people of Lee's station forbade such a thing."

In 2000, Michael Fellman, in "The Making of Robert E. Lee", found the claims that Lee had "personally" whipped "Mary" Norris "extremely unlikely," but found it not at all unlikely that Lee had ordered the runaways whipped: "corporal punishment (for which Lee substituted the euphemism 'firmness') was (believed to be) an intrinsic and necessary part of slave discipline. Although it was supposed to be applied only in a calm and rational manner, overtly physical domination of slaves, unchecked by law, was always brutal and potentially savage."

In 2003, Bernice-Marie Yates's "The Perfect Gentleman", cited Freeman's denial and followed his account in holding that, because of Lee's family connections to George Washington, he "was a prime target for abolitionists who lacked all the facts of the situation."

Lee biographer Elizabeth Brown Pryor concluded in 2008 that "the facts are verifiable," based on "the consistency of the five extant descriptions of the episode (the only element that is not repeatedly corroborated is the allegation that Lee gave the beatings himself), as well as the existence of an account book that indicates the constable received compensation from Lee on the date that this event occurred."

In 2014, Michael Korda wrote that "Although these letters are dismissed by most of Lee's biographers as exaggerated, or simply as unfounded abolitionist propaganda, it is hard to ignore them. ... It seems incongruously out of character for Lee to have whipped a slave woman himself, particularly one stripped to the waist, and that charge may have been a flourish added by the two correspondents; it was not repeated by Wesley Norris when his account of the incident was published in 1866. ... [A]lthough it seems unlikely that he would have done any of the whipping himself, he may not have flinched from observing it to make sure his orders were carried out exactly."

Several historians have noted the paradoxical nature of Lee's beliefs and actions concerning race and slavery. While Lee protested he had sympathetic feelings for blacks, they were subordinate to his own racial identity. While Lee held slavery to be an evil institution, he also saw some benefit to blacks held in slavery. While Lee helped assist individual slaves to freedom in Liberia, and provided for their emancipation in his own will, he believed the enslaved should be eventually freed in a general way only at some unspecified future date as a part of God's purpose. Slavery for Lee was a moral and religious issue, and not one that would yield to political solutions. Emancipation would sooner come from Christian impulse among slave masters before "storms and tempests of fiery controversy" such as was occurring in "Bleeding Kansas". Countering southerners who argued for slavery as a positive good, Lee in his well-known analysis of slavery from an 1856 letter called it a moral and political evil. While both Robert and his wife Mary Lee were disgusted with slavery, they also defended it against Abolitionist demands for immediate emancipation for all enslaved.

Lee's father-in-law G. W. Parke Custis freed his slaves in his will. In the same tradition, before leaving to serve in Mexico, Lee had written a will providing for the manumission of the only slaves he owned. Parke Custis was a member of the American Colonization Society, which was formed to gradually end slavery by establishing a free republic in Liberia for African-Americans, and Lee assisted several ex-slaves to emigrate there. Also, according to historian Richard B. McCaslin, Lee was a gradual emancipationist, denouncing extremist proposals for immediate abolition of slavery. Lee rejected what he called evilly motivated political passion, fearing a civil and servile war from precipitous emancipation.

Historian Elizabeth Brown Pryor offered an alternative interpretation of Lee's voluntary manumission of slaves in his will, and assisting slaves to a life of freedom in Liberia, seeing Lee as conforming to a "primacy of slave law". She wrote that Lee's private views on race and slavery,

On taking on the role of administrator for the Parke Custis will, Lee used a provision to retain them in slavery to produce income for the estate to retire debt. Lee did not welcome the role of planter while administering the Custis properties at Romancoke, another nearby the Pamunkey River and Arlington; he rented the estate's mill. While all the estates prospered under his administration, Lee was unhappy at direct participation in slavery as a hated institution.

Even before what Michael Fellman called a "sorry involvement in actual slave management", Lee judged the experience of white mastery to be a greater moral evil to the white man than blacks suffering under the "painful discipline" of slavery which introduced Christianity, literacy and a work ethic to the "heathen African". Columbia University historian Eric Foner notes that:
By the time of Lee's career in the U.S. Army, the officers of West Point stood aloof from political-party and sectional strife on such issues as slavery, as a matter of principle, and Lee adhered to the principle. He considered it his patriotic duty to be apolitical while in active Army service, and Lee did not speak out publicly on the subject of slavery prior to the Civil War. Before the outbreak of the War, in 1860, Lee voted for John C. Breckinridge, who was the extreme pro-slavery candidate in the 1860 presidential election, not John Bell, the more moderate Southerner who won Virginia.

Lee himself owned a small number of slaves in his lifetime and considered himself a paternalistic master. There are various historical and newspaper hearsay accounts of Lee personally whipping a slave, but they are not direct eyewitness accounts. He was definitely involved in administering the day-to-day operations of a plantation and was involved in the recapture of runaway slaves. One historian noted that Lee separated slave families, something that prominent slave-holding families in Virginia such as Washington and Custis did not do. In 1862, Lee freed the slaves that his wife inherited, but that was in accordance with his father-in-law's will.

Lee claimed that he found slavery bothersome and time-consuming as an everyday institution to run. In an 1856 letter to his wife, he maintained that slavery was a great evil, but primarily due to adverse impact that it had on white people:In this enlightened age, there are few I believe, but what will acknowledge, that slavery as an institution, is a moral & political evil in any Country. It is useless to expatiate on its disadvantages. I think it however a greater evil to the white man than to the black race, & while my feelings are strongly enlisted in behalf of the latter, my sympathies are more strong for the former. The blacks are immeasurably better off here than in Africa, morally, socially & physically. The painful discipline they are undergoing, is necessary for their instruction as a race, & I hope will prepare & lead them to better things. How long their subjugation may be necessary is known & ordered by a wise Merciful Providence.

Foner writes that "Lee's code of gentlemanly conduct did not seem to apply to blacks" during the War, as he did not stop his soldiers from kidnapping free black farmers and selling them into slavery. Princeton University historian James M. McPherson noted that Lee initially rejected a prisoner exchange between the Confederacy and the Union when the Union demanded that black Union soldiers be included. Lee did not accept the swap until a few months before the Confederacy's surrender.

After the War, Lee told a congressional committee that blacks were "not disposed to work" and did not possess the intellectual capacity to vote and participate in politics. Lee also said to the committee that he hoped that Virginia could "get rid of them," referring to blacks. While not politically active, Lee defended Lincoln's successor, Andrew Johnson's approach to Reconstruction, which according to Foner, "abandoned the former slaves to the mercy of governments controlled by their former owners." According to Foner, "A word from Lee might have encouraged white Southerners to accord blacks equal rights and inhibited the violence against the freed people that swept the region during Reconstruction, but he chose to remain silent." Lee was also urged to condemn the white-supremacy organization Ku Klux Klan, but opted to remain silent.

In the generation following the war, Lee, though he died just a few years later, became a central figure in the Lost Cause interpretation of the war. The argument that Lee had always somehow opposed slavery, and freed his wife's slaves, helped maintain his stature as a symbol of Southern honor and national reconciliation. Douglas Southall Freeman's Pulitzer prize-winning four-volume "R. E. Lee: A Biography" (1936), which was for a long period considered the definitive work on Lee, downplayed his involvement in slavery and emphasized Lee as a virtuous person. Eric Foner, who describes Freeman's volume as a "hagiography", notes that on the whole, Freeman "displayed little interest in Lee's relationship to slavery. The index to his four volumes contained 22 entries for 'devotion to duty', 19 for 'kindness', 53 for Lee's celebrated horse, Traveller. But 'slavery', 'slave emancipation' and 'slave insurrection' together received five. Freeman observed, without offering details, that slavery in Virginia represented the system 'at its best'. He ignored the postwar testimony of Lee's former slave Wesley Norris about the brutal treatment to which he had been subjected."

Both Harpers Ferry and the secession of Texas were monumental events leading up to the Civil War. Robert E. Lee was at both events. Lee initially remained loyal to the Union after Texas seceded.

John Brown led a band of 21 abolitionists who seized the federal arsenal at Harpers Ferry, Virginia, in October 1859, hoping to incite a slave rebellion. President James Buchanan gave Lee command of detachments of militia, soldiers, and United States Marines, to suppress the uprising and arrest its leaders. By the time Lee arrived that night, the militia on the site had surrounded Brown and his hostages. At dawn, Brown refused the demand for surrender. Lee attacked, and Brown and his followers were captured after three minutes of fighting. Lee's summary report of the episode shows Lee believed it "was the attempt of a fanatic or madman". Lee said Brown achieved "temporary success" by creating panic and confusion and by "magnifying" the number of participants involved in the raid.

In 1860, Lt. Col. Robert E. Lee relieved Major Heintzelman at Fort Brown, and the Mexican authorities offered to restrain "their citizens from making predatory descents upon the territory and people of Texas ... this was the last active operation of the Cortina War". Rip Ford, a Texas Ranger at the time, described Lee as "dignified without hauteur, grand without pride ... he evinced an imperturbable self-possession, and a complete control of his passions ... possessing the capacity to accomplish great ends and the gift of controlling and leading men."

When Texas seceded from the Union in February 1861, General David E. Twiggs surrendered all the American forces (about 4,000 men, including Lee, and commander of the Department of Texas) to the Texans. Twiggs immediately resigned from the U.S. Army and was made a Confederate general. Lee went back to Washington and was appointed Colonel of the First Regiment of Cavalry in March 1861. Lee's colonelcy was signed by the new President, Abraham Lincoln. Three weeks after his promotion, Colonel Lee was offered a senior command (with the rank of Major General) in the expanding Army to fight the Southern States that had left the Union. Fort Mason, Texas was Lee's last command with the United States Army.

Unlike many Southerners who expected a glorious war, Lee correctly predicted it as protracted and devastating. He privately opposed the new Confederate States of America in letters in early 1861, denouncing secession as "nothing but revolution" and an unconstitutional betrayal of the efforts of the Founding Fathers. Writing to George Washington Custis in January, Lee stated:

Despite opposing secession, Lee said in January that "we can with a clear conscience separate" if all peaceful means failed. He agreed with secessionists in most areas, such as dislike of Northern anti-slavery criticisms and prevention of expanding slavery to new territories, and fear of its larger population. Lee supported the Crittenden Compromise, which would have constitutionally protected slavery.

Lee's objection to secession was ultimately outweighed by a sense of personal honor, reservations about the legitimacy of a strife-ridden "Union that can only be maintained by swords and bayonets", and duty to defend his native Virginia if attacked. He was asked while leaving Texas by a lieutenant if he intended to fight for the Confederacy or the Union, to which Lee replied, "I shall never bear arms against the Union, but it may be necessary for me to carry a musket in the defense of my native state, Virginia, in which case I shall not prove recreant to my duty".

Although Virginia had the most slaves of any state, it was more similar to Maryland, which stayed in the Union, than the Deep South; a convention voted against secession in early 1861. Scott, commanding general of the Union Army and Lee's mentor, told Lincoln he wanted him for a top command, telling Secretary of War Simon Cameron that he had "entire confidence" in Lee. He accepted a promotion to colonel of the 1st Cavalry Regiment on March 28, again swearing an oath to the United States. Meanwhile, Lee ignored an offer of command from the Confederacy. After Lincoln's call for troops to put down the rebellion, a second Virginia convention in Richmond voted to secede on April 17, and a May 23 referendum would likely ratify the decision. That night Lee dined with brother Smith and cousin Phillips, naval officers. Because of Lee's indecision, Phillips went to the War Department the next morning to warn that the Union might lose his cousin if the government did not act quickly.

In Washington that day, Lee was offered by presidential advisor Francis P. Blair a role as major general to command the defense of the national capital. He replied:

Lee agreed that to avoid dishonor he had to resign before receiving unwanted orders. While historians have usually called his decision inevitable ("the answer he was born to make", wrote Douglas Southall Freeman; another called it a "no-brainer") given the ties to family and state, an 1871 letter from his eldest daughter, Mary Custis Lee, to a biographer described Lee as "worn and harassed" yet calm as he deliberated alone in his office. People on the street noticed Lee's grim face as he tried to decide over the next two days, and he later said that he kept the resignation letter for a day before sending it on April 20. Two days later the Richmond convention invited Lee to the city. It elected him as commander of Virginia state forces before his arrival on April 23, and almost immediately gave him George Washington's sword as symbol of his appointment; whether he was told of a decision he did not want without time to decide, or did want the excitement and opportunity of command, is unclear.

A cousin on Scott's staff told the family that Lee's decision so upset Scott that he collapsed on a sofa and mourned as if he had lost a son, and asked to not hear Lee's name. When Lee told family his decision he said "I suppose you will all think I have done very wrong", as the others were mostly pro-Union; only Mary Custis was a secessionist, and her mother especially wanted to choose the Union but told her husband that she would support whatever he decided. Many younger men like nephew Fitzhugh wanted to support the Confederacy, but Lee's three sons joined the Confederate military only after their father's decision.

Most family members like brother Smith reluctantly also chose the South, but Smith's wife and Anne, Lee's sister, still supported the Union; Anne's son joined the Union Army, and no one in his family ever spoke to Lee again. Many cousins fought for the Confederacy, but Phillips and John Fitzgerald told Lee in person that they would uphold their oaths; John H. Upshur stayed with the Union military despite much family pressure; Roger Jones stayed in the Union army after Lee refused to advise him on what to do; and two of Philip Fendall's sons fought for the Union. Forty percent of Virginian officers stayed with the North.

At the outbreak of war, Lee was appointed to command all of Virginia's forces, but upon the formation of the Confederate States Army, he was named one of its first five full generals. Lee did not wear the insignia of a Confederate general, but only the three stars of a Confederate colonel, equivalent to his last U.S. Army rank. He did not intend to wear a general's insignia until the Civil War had been won and he could be promoted, in peacetime, to general in the Confederate Army.

Lee's first field assignment was commanding Confederate forces in western Virginia, where he was defeated at the Battle of Cheat Mountain and was widely blamed for Confederate setbacks. He was then sent to organize the coastal defenses along the Carolina and Georgia seaboard, appointed commander, "Department of South Carolina, Georgia and Florida" on November 5, 1861. Between then and the fall of Fort Pulaski, April 11, 1862, he put in place a defense of Savannah that proved successful in blocking Federal advance on Savannah. Confederate fort and naval gunnery dictated night time movement and construction by the besiegers. Federal preparations required four months. In those four months, Lee developed a defense in depth. Behind Fort Pulaski on the Savannah River, Fort Jackson was improved, and two additional batteries covered river approaches. In the face of the Union superiority in naval, artillery and infantry deployment, Lee was able to block any Federal advance on Savannah, and at the same time, well-trained Georgia troops were released in time to meet McClellan's Peninsula Campaign. The city of Savannah would not fall until Sherman's approach from the interior at the end of 1864.

At first, the press spoke to the disappointment of losing Fort Pulaski. Surprised by the effectiveness of large caliber Parrott Rifles in their first deployment, it was widely speculated that only betrayal could have brought overnight surrender to a Third System Fort. Lee was said to have failed to get effective support in the Savannah River from the three sidewheeler gunboats of the Georgia Navy. Although again blamed by the press for Confederate reverses, he was appointed military adviser to Confederate President Jefferson Davis, the former U.S. Secretary of War. While in Richmond, Lee was ridiculed as the 'King of Spades' for his excessive digging of trenches around the capitol. These trenches would later play a pivotal role in battles near the end of the war.

In the spring of 1862, in the Peninsula Campaign, the Union Army of the Potomac under General George B. McClellan advanced on Richmond from Fort Monroe to the east. McClellan forced Gen. Joseph E. Johnston and the Army of Virginia to retreat to just north and east of the Confederate capital.

Then Johnston was wounded at the Battle of Seven Pines, on June 1, 1862. Lee now got his first opportunity to lead an army in the field – the force he renamed the Army of "Northern" Virginia, signalling his confidence that the Union army would be driven away from Richmond. Early in the war, Lee had been called "Granny Lee" for his allegedly timid style of command. Confederate newspaper editorials objected to him replacing Johnston, opining that Lee would be passive, waiting for Union attack. And for the first three weeks of June, he did not attack, instead strengthening Richmond's defenses.

But then he launched a series of bold attacks against McClellan's forces, the Seven Days Battles. Despite superior Union numbers, and some clumsy tactical performances by his subordinates, Lee's attacks derailed McClellan's plans and drove back part of his forces. Confederate casualties were heavy, but McClellan was unnerved, retreated to the lower James River, and abandoned the Peninsula Campaign. This success completely changed Confederate morale, and the public's regard for Lee. After the Seven Days Battles, and until the end of the war, his men called him simply "Marse Robert", a term of respect and affection.

The setback, and the resulting drop in Union morale, impelled Lincoln to adopt a new policy of relentless, committed warfare. After the Seven Days, Lincoln decided he would move to emancipate most Confederate slaves by executive order, as a military act, using his authority as commander-in-chief. But he needed a Union victory first.

Meanwhile, Lee defeated another Union army under Gen. John Pope at the Second Battle of Bull Run. In less than 90 days after taking command, Lee had run McClellan off the Peninsula, defeated Pope, and moved the battle lines from outside Richmond, to outside Washington.

Lee now invaded Maryland and Pennsylvania, hoping to collect supplies in Union territory, and possibly win a victory that would sway the upcoming Union elections in favor of ending the war. But McClellan's men found a lost Confederate dispatch, Special Order 191, that revealed Lee's plans and movements. McClellan always exaggerated Lee's numerical strength, but now he knew the Confederate army was divided and could be destroyed in detail. However, McClellan moved slowly, not realizing a spy had informed Lee that McClellan had the plans. Lee quickly concentrated his forces west of Antietam Creek, near Sharpsburg, Maryland, where McClellan attacked on September 17. The Battle of Antietam was the single bloodiest day of the war, with both sides suffering enormous losses. Lee's army barely withstood the Union assaults, then retreated to Virginia the next day. This narrow Confederate defeat gave President Abraham Lincoln the opportunity to issue his Emancipation Proclamation, which put the Confederacy on the diplomatic and moral defensive.

Disappointed by McClellan's failure to destroy Lee's army, Lincoln named Ambrose Burnside as commander of the Army of the Potomac. Burnside ordered an attack across the Rappahannock River at Fredericksburg, Virginia. Delays in bridging the river allowed Lee's army ample time to organize strong defenses, and the Union frontal assault on December 13, 1862, was a disaster. There were 12,600 Union casualties to 5,000 Confederate; one of the most one-sided battles in the Civil War. After this victory, Lee reportedly said "It is well that war is so terrible, else we should grow too fond of it." At Fredericksburg, according to historian Michael Fellman, Lee had completely entered into the "spirit of war, where destructiveness took on its own beauty."

After the bitter Union defeat at Fredericksburg, President Lincoln named Joseph Hooker commander of the Army of the Potomac. In May 1863, Hooker maneuvered to attack Lee's army via Chancellorsville, Virginia. But Hooker was defeated by Lee's daring maneuver: dividing his army and sending Stonewall Jackson's corps to attack Hooker's flank. Lee won a decisive victory over a larger force, but with heavy casualties, including Jackson, his finest corps commander, who was accidentally killed by his own troops.

The critical decisions came in May–June 1863, after Lee's smashing victory at the Battle of Chancellorsville. The western front was crumbling, as multiple uncoordinated Confederate armies were unable to handle General Ulysses S. Grant's campaign against Vicksburg. The top military advisers wanted to save Vicksburg, but Lee persuaded Davis to overrule them and authorize yet another invasion of the North. The immediate goal was to acquire urgently needed supplies from the rich farming districts of Pennsylvania; a long-term goal was to stimulate peace activity in the North by demonstrating the power of the South to invade. Lee's decision proved a significant strategic blunder and cost the Confederacy control of its western regions, and nearly cost Lee his own army as Union forces cut him off from the South.

In the summer of 1863, Lee invaded the North again, marching through western Maryland and into south central Pennsylvania. He encountered Union forces under George G. Meade at the three-day Battle of Gettysburg in Pennsylvania in July; the battle would produce the largest number of casualties in the American Civil War. With some of his subordinates being new and inexperienced in their commands, J.E.B. Stuart's cavalry being out of the area, and Lee being slightly ill, he was less than comfortable with how events were unfolding. While the first day of battle was controlled by the Confederates, key terrain that should have been taken by General Ewell was not. The second day ended with the Confederates unable to break the Union position, and the Union being more solidified. Lee's decision on the third day, against the judgment of his best corps commander General Longstreet, to launch a massive frontal assault on the center of the Union line turned out to be disastrous. The assault known as Pickett's Charge was repulsed and resulted in heavy Confederate losses. The general rode out to meet his retreating army and proclaimed, "All this has been my fault." Lee was compelled to retreat. Despite flooded rivers that blocked his retreat, he escaped Meade's ineffective pursuit. Following his defeat at Gettysburg, Lee sent a letter of resignation to President Davis on August 8, 1863, but Davis refused Lee's request. That fall, Lee and Meade met again in two minor campaigns that did little to change the strategic standoff. The Confederate Army never fully recovered from the substantial losses incurred during the three-day battle in southern Pennsylvania. The historian Shelby Foote stated, "Gettysburg was the price the South paid for having Robert E. Lee as commander."

In 1864 the new Union general-in-chief, Lt. Gen. Ulysses S. Grant, sought to use his large advantages in manpower and material resources to destroy Lee's army by attrition, pinning Lee against his capital of Richmond. Lee successfully stopped each attack, but Grant with his superior numbers kept pushing each time a bit farther to the southeast. These battles in the Overland Campaign included the Wilderness, Spotsylvania Court House and Cold Harbor.

Grant eventually was able to stealthily move his army across the James River. After stopping a Union attempt to capture Petersburg, Virginia, a vital railroad link supplying Richmond, Lee's men built elaborate trenches and were besieged in Petersburg, a development which presaged the trench warfare of World War I. Lee attempted to break the stalemate by sending Jubal A. Early on a raid through the Shenandoah Valley to Washington, D.C., but Early was defeated early on by the superior forces of Philip Sheridan. The Siege of Petersburg lasted from June 1864 until March 1865, with Lee's outnumbered and poorly supplied army shrinking daily because of desertions by disheartened Confederates.

On February 6, 1865, Lee was appointed General in Chief of the Armies of the Confederate States.

As the South ran out of manpower the issue of arming the slaves became paramount. Lee explained, "We should employ them without delay ... [along with] gradual and general emancipation." The first units were in training as the war ended. As the Confederate army was devastated by casualties, disease and desertion, the Union attack on Petersburg succeeded on April 2, 1865. Lee abandoned Richmond and retreated west. Lee then made an attempt to escape to the southwest and join up with Joseph E. Johnston's Army of Tennessee in North Carolina. However, his forces were soon surrounded and he surrendered them to Grant on April 9, 1865, at the Battle of Appomattox Court House. Other Confederate armies followed suit and the war ended. The day after his surrender, Lee issued his Farewell Address to his army.

Lee resisted calls by some officers to reject surrender and allow small units to melt away into the mountains, setting up a lengthy guerrilla war. He insisted the war was over and energetically campaigned for inter-sectional reconciliation. "So far from engaging in a war to perpetuate slavery, I am rejoiced that slavery is abolished. I believe it will be greatly for the interests of the South."

The following are summaries of Civil War campaigns and major battles where Robert E. Lee was the commanding officer:

After the war, Lee was not arrested or punished (although he was indicted), but he did lose the right to vote as well as some property. Lee's prewar family home, the Custis-Lee Mansion, was seized by Union forces during the war and turned into Arlington National Cemetery, and his family was not compensated until more than a decade after his death.

Lee somewhat supported President Johnson's plan of Reconstruction. In 1866 Lee counseled southerners not to resume fighting, of which Grant said Lee was "setting an example of forced acquiescence so grudging and pernicious in its effects as to be hardly realized". Lee joined with Democrats in opposing the Radical Republicans who demanded punitive measures against the South, distrusted its commitment to the abolition of slavery and, indeed, distrusted the region's loyalty to the United States. Lee supported a system of free public schools for blacks, but forthrightly opposed allowing blacks to vote. "My own opinion is that, at this time, they [black Southerners] cannot vote intelligently, and that giving them the [vote] would lead to a great deal of demagogism, and lead to embarrassments in various ways," Lee stated. Emory Thomas says Lee had become a suffering Christ-like icon for ex-Confederates. President Grant invited him to the White House in 1869, and he went. Nationally he became an icon of reconciliation between the North and South, and the reintegration of former Confederates into the national fabric.

Lee hoped to retire to a farm of his own, but he was too much a regional symbol to live in obscurity. From April to June 1865, he and his family resided in Richmond at the Stewart-Lee House. He accepted an offer to serve as the president of Washington College (now Washington and Lee University) in Lexington, Virginia, and served from October 1865 until his death. The Trustees used his famous name in large-scale fund-raising appeals and Lee transformed Washington College into a leading Southern college, expanding its offerings significantly, adding programs in commerce and journalism, and incorporating the Lexington Law School. Lee was well liked by the students, which enabled him to announce an "honor system" like that of West Point, explaining that "we have but one rule here, and it is that every student be a gentleman." To speed up national reconciliation Lee recruited students from the North and made certain they were well treated on campus and in town.

Several glowing appraisals of Lee's tenure as college president have survived, depicting the dignity and respect he commanded among all. Previously, most students had been obliged to occupy the campus dormitories, while only the most mature were allowed to live off-campus. Lee quickly reversed this rule, requiring most students to board off-campus, and allowing only the most mature to live in the dorms as a mark of privilege; the results of this policy were considered a success. A typical account by a professor there states that "the students fairly worshipped him, and deeply dreaded his displeasure; yet so kind, affable, and gentle was he toward them that all loved to approach him. ... No student would have dared to violate General Lee's expressed wish or appeal; if he had done so, the students themselves would have driven him from the college."

While at Washington College, Lee told a colleague that the greatest mistake of his life was taking a military education.

On May 29, 1865, President Andrew Johnson issued a Proclamation of Amnesty and Pardon to persons who had participated in the rebellion against the United States. There were fourteen excepted classes, though, and members of those classes had to make special application to the President. Lee sent an application to Grant and wrote to President Johnson on June 13, 1865:

On October 2, 1865, the same day that Lee was inaugurated as president of Washington College in Lexington, Virginia, he signed his Amnesty Oath, thereby complying fully with the provision of Johnson's proclamation. Lee was not pardoned, nor was his citizenship restored.

Three years later, on December 25, 1868, Johnson proclaimed a second amnesty which removed previous exceptions, such as the one that affected Lee.

Lee, who had opposed secession and remained mostly indifferent to politics before the Civil War, supported President Andrew Johnson's plan of Presidential Reconstruction that took effect in 1865–66. However, he opposed the Congressional Republican program that took effect in 1867. In February 1866, he was called to testify before the Joint Congressional Committee on Reconstruction in Washington, where he expressed support for Johnson's plans for quick restoration of the former Confederate states, and argued that restoration should return, as far as possible, to the "status quo ante" in the Southern states' governments (with the exception of slavery).

Lee told the Committee, "...every one with whom I associate expresses kind feelings towards the freedmen. They wish to see them get on in the world, and particularly to take up some occupation for a living, and to turn their hands to some work." Lee also expressed his "willingness that blacks should be educated, and ... that it would be better for the blacks and for the whites." Lee forthrightly opposed allowing blacks to vote: "My own opinion is that, at this time, they [black Southerners] cannot vote intelligently, and that giving them the [vote] would lead to a great deal of demagogism, and lead to embarrassments in various ways."

In an interview in May 1866, Lee said: "The Radical party are likely to do a great deal of harm, for we wish now for good feeling to grow up between North and South, and the President, Mr. Johnson, has been doing much to strengthen the feeling in favor of the Union among us. The relations between the Negroes and the whites were friendly formerly, and would remain so if legislation be not passed in favor of the blacks, in a way that will only do them harm."

In 1868, Lee's ally Alexander H. H. Stuart drafted a public letter of endorsement for the Democratic Party's presidential campaign, in which Horatio Seymour ran against Lee's old foe Republican Ulysses S. Grant. Lee signed it along with thirty-one other ex-Confederates. The Democratic campaign, eager to publicize the endorsement, published the statement widely in newspapers. Their letter claimed paternalistic concern for the welfare of freed Southern blacks, stating that "The idea that the Southern people are hostile to the negroes and would oppress them, if it were in their power to do so, is entirely unfounded. They have grown up in our midst, and we have been accustomed from childhood to look upon them with kindness." However, it also called for the restoration of white political rule, arguing that "It is true that the people of the South, in common with a large majority of the people of the North and West, are, for obvious reasons, inflexibly opposed to any system of laws that would place the political power of the country in the hands of the negro race. But this opposition springs from no feeling of enmity, but from a deep-seated conviction that, at present, the negroes have neither the intelligence nor the other qualifications which are necessary to make them safe depositories of political power."

In his public statements and private correspondence, Lee argued that a tone of reconciliation and patience would further the interests of white Southerners better than hotheaded antagonism to federal authority or the use of violence. Lee repeatedly expelled white students from Washington College for violent attacks on local black men, and publicly urged obedience to the authorities and respect for law and order. He privately chastised fellow ex-Confederates such as Jefferson Davis and Jubal Early for their frequent, angry responses to perceived Northern insults, writing in private to them as he had written to a magazine editor in 1865, that "It should be the object of all to avoid controversy, to allay passion, give full scope to reason and to every kindly feeling. By doing this and encouraging our citizens to engage in the duties of life with all their heart and mind, with a determination not to be turned aside by thoughts of the past and fears of the future, our country will not only be restored in material prosperity, but will be advanced in science, in virtue and in religion."

On September 28, 1870, Lee suffered a stroke. He died two weeks later, shortly after 9 a.m. on October 12, 1870, in Lexington, Virginia, from the effects of pneumonia. According to one account, his last words on the day of his death, were "Tell Hill he "must" come up! Strike the tent", but this is debatable because of conflicting accounts and because Lee's stroke had resulted in aphasia, possibly rendering him unable to speak.

At first no suitable coffin for the body could be located. The muddy roads were too flooded for anyone to get in or out of the town of Lexington. An undertaker had ordered three from Richmond that had reached Lexington, but due to unprecedented flooding from long-continued heavy rains, the caskets were washed down the Maury River. Two neighborhood boys, C.G. Chittum and Robert E. Hillis, found one of the coffins that had been swept ashore. Undamaged, it was used for the General's body, though it was a bit short for him. As a result, Lee was buried without shoes. He was buried underneath Lee Chapel at Washington and Lee University, where his body remains.

Among the supporters of the Confederacy, Lee came to be even more revered after his surrender than he had been during the war, when Stonewall Jackson had been the great Confederate hero. In an address before the Southern Historical Society in Atlanta, Georgia in 1874, Benjamin Harvey Hill described Lee in this way:

By the end of the 19th century, Lee's popularity had spread to the North. Lee's admirers have pointed to his character and devotion to duty, and his brilliant tactical successes in battle after battle against a stronger foe.

Military historians continue to pay attention to his battlefield tactics and maneuvering, though many think he should have designed better strategic plans for the Confederacy. He was not given full direction of the Southern war effort until late in the conflict.

Historian Eric Foner writes that at the end of his life,

Robert E. Lee has been commemorated on U.S. postage stamps at least five times, the first one being a commemorative stamp that also honored Stonewall Jackson, issued in 1936. A second "regular-issue" stamp was issued in 1955. He was commemorated with a 32-cent stamp issued in the American Civil War Issue of June 29, 1995. His horse Traveller is pictured in the background.

Washington and Lee University in Lexington, Virginia was commemorated on its 200th anniversary on November 23, 1948, with a 3-cent postage stamp. The central design is a view of the university, flanked by portraits of generals George Washington and Robert E. Lee. Lee was again commemorated on a commemorative stamp in 1970, along with Jefferson Davis and Thomas J. "Stonewall" Jackson, depicted on horseback on the 6-cent Stone Mountain Memorial commemorative issue, modeled after the actual Stone Mountain Memorial carving in Georgia. The stamp was issued on September 19, 1970, in conjunction with the dedication of the Stone Mountain Confederate Memorial in Georgia on May 9, 1970. The design of the stamp replicates the memorial, the largest high relief sculpture in the world. It is carved on the side of Stone Mountain 400 feet above the ground.

Stone Mountain also led to Lee's appearance on a commemorative coin, the 1925 Stone Mountain Memorial half dollar. During the 1920s and '30s dozens of specially designed half dollars were struck to raise money for various events and causes. This issue had a particularly wide distribution, with 1,314,709 minted. Unlike some of the other issues it remains a very common coin.
On September 29, 2007, General Lee's three Civil War-era letters were sold for $61,000 at auction by Thomas Willcox, much less than the record of $630,000 for a Lee item in 2002. The auction included more than 400 documents of Lee's from the estate of the parents of Willcox that had been in the family for generations. South Carolina sued to stop the sale on the grounds that the letters were official documents and therefore property of the state, but the court ruled in favor of Willcox.

In 1865, after the war, Lee was paroled and signed an oath of allegiance, asking to have his citizenship of the United States restored. However, his application was misplaced and as a result he did not receive a pardon and his citizenship was not restored. On January 30, 1975, Senate Joint Resolution 23, "A joint resolution to restore posthumously full rights of citizenship to General R. E. Lee" was introduced into the Senate by Senator Harry F. Byrd Jr. (I-VA), the result of a five-year campaign to accomplish this. The resolution, which enacted Public Law 94-67, was passed, and the bill was signed by President Gerald Ford on September 5.

Lee opposed the construction of public memorials to Confederate rebellion on the grounds that they would prevent the healing of wounds inflicted during the war. Nevertheless, after his death, he became an icon used by promoters of "Lost Cause" mythology, who sought to romanticize the Confederate cause and strengthen white supremacy in the South. Later in the 20th century, particularly following the civil rights movement, historians reassessed Lee; his reputation fell based on his failure to support rights for freedmen after the war, and even his strategic choices as a military leader fell under scrutiny.

From its installation in 1884 until its removal in 2017, the most prominent monument in New Orleans was a -tall monument to General Lee. A statue of Lee stood tall upon a towering column of white marble in the middle of Lee Circle. The statue of Lee, which weighs more than faced the north. Lee Circle is situated along New Orleans's famous St. Charles Avenue. The New Orleans streetcars roll past Lee Circle and New Orleans's best Mardi Gras parades go around Lee Circle (the spot is so popular that bleachers are set up annually around the perimeter for Mardi Gras). Around the corner from Lee Circle is New Orleans's Confederate museum, which contains the second-largest collection of Confederate memorabilia in the world. The statue of General Lee was removed on May 19, 2017, the last of four Confederate monuments in New Orleans to be taken down.

In a tribute to Lee Circle (which had formerly been known as Tivoli Circle), former Confederate soldier George Washington Cable wrote:
In Tivoli Circle, New Orleans, from the centre and apex of its green flowery mound, an immense column of pure white marble rises in the ... majesty of Grecian proportions high up above the city's house-tops into the dazzling sunshine ... On its dizzy top stands the bronze figure of one of the world's greatest captains. He is alone. Not one of his mighty lieutenants stand behind, beside or below him. His arms are folded on that breast that never knew fear, and his calm, dauntless gaze meets the morning sun as it rises, like the new prosperity of the land he loved and served so masterly, above the far distant battle fields where so many thousands of his gray veterans lie in the sleep of fallen heroes. ("Silent South", 1885, The Century Illustrated Monthly Magazine)

Arlington House, The Robert E. Lee Memorial, also known as the Custis–Lee Mansion, is a Greek revival mansion in Arlington, Virginia, that was once Lee's home. It overlooks the Potomac River and the National Mall in Washington, D.C. During the Civil War, the grounds of the mansion were selected as the site of Arlington National Cemetery, in part to ensure that Lee would never again be able to return to his home. The United States designated the mansion as a National Memorial to Lee in 1955, a mark of widespread respect for him in both the North and South.

In Richmond, Virginia, a large equestrian statue of Lee by French sculptor Jean Antonin Mercié is the centerpiece of the city's famous Monument Avenue, which boasts four other statues to famous Confederates. This monument to Lee was unveiled on May 29, 1890; over 100,000 people attended this dedication. That has been described as "the day white Virginia stopped admiring Gen. Robert E. Lee and started worshiping him". Lee is also shown mounted on Traveller in Gettysburg National Military Park on top of the Virginia Monument; he is facing roughly in the direction of Pickett's Charge. Lee's portrayal on a mural on Richmond's Flood Wall on the James River, considered offensive by some, was removed in the late 1990s, but currently is back on the flood wall.

Also in Virginia, the Robert Edward Lee (sculpture) at Charlottesville was listed on the National Register of Historic Places in 1997. Since there is no historical link between Lee and the city of Charlottesville, the City Council of Charlottesville voted in February 2017 to remove it, along with a statue of Stonewall Jackson, but this was temporarily stayed by court action. They did rename Lee Park, Emancipation Park. The prospect of the statues being removed and the parks being renamed brought many out-of-towners, described as white supremacist and alt-right, to Charlottesville in the Unite the Right rally of August 2017, in which 3 people died. For several months the monuments were shrouded in black. As of October 2018, the fate of the statue of Lee is unresolved. The name of the park it is located in was changed again by the City Council, to Market Street Park, in July 2018.

In Baltimore's Wyman Park, a large double equestrian statue of Lee and Jackson is located directly across from the Baltimore Museum of Art. Designed by Laura Gardin Fraser and dedicated in 1948, Lee is depicted astride his horse Traveller next to Stonewall Jackson who is mounted on "Little Sorrel." Architect John Russell Pope created the base, which was dedicated on the anniversary of the eve of the Battle of Chancellorsville. The Baltimore area of Maryland is also home to a large nature park called Robert E. Lee Memorial Park.

In 1953, two stained-glass windows – one honoring Lee, the other Stonewall Jackson – were installed in the Washington National Cathedral. The stained glass of Lee shows him on horseback at Chancellorsville; it was sponsored by the United Daughters of the Confederacy. In 2017, these windows were removed by a vote of the Cathedral's governing board. The cathedral plans to keep the windows and eventually display them in historical context.

An equestrian statue of Lee was installed in Robert E. Lee Park, in Dallas, until 2017; and in Austin, a statue of Lee is on display at the main mall of the University of Texas at Austin. A statue of Robert E. Lee is one of two statues (the other is Washington) representing Virginia in Statuary Hall in the Capitol in Washington, D.C. Lee is one of the figures depicted in bas-relief carved into Stone Mountain near Atlanta. Accompanying him on horseback in the relief are Stonewall Jackson and Jefferson Davis.

The birthday of Robert E. Lee is celebrated or commemorated in several states. In Virginia, Lee–Jackson Day is celebrated on the Friday preceding Martin Luther King, Jr. Day which is the third Monday in January. In Texas, he is celebrated as part of Confederate Heroes Day on January 19, Lee's birthday. In Alabama and Mississippi, his birthday is celebrated on the same day as Martin Luther King, Jr. Day, while in Georgia, this occurred on the day after Thanksgiving before 2016, when the state stopped officially recognizing the holiday.

One United States college and one junior college are named for Lee: Washington and Lee University in Lexington, Virginia; and Lee College in Baytown, Texas, respectively. Lee Chapel at Washington and Lee University marks Lee's final resting place. Throughout the South, many primary and secondary schools were also named for him as well as private schools such as Robert E. Lee Academy in Bishopville, South Carolina.

In 1900, Lee was one of the first 29 individuals selected for the Hall of Fame for Great Americans (the first Hall of Fame in the United States), designed by Stanford White, on the Bronx, New York, campus of New York University, now a part of Bronx Community College. However, his bust was removed in August 2017 by order of New York Governor Andrew Cuomo.

Lee is featured on the 1925 Stone Mountain Memorial half dollar.

In 1862, the newly formed Confederate Navy purchased a 642-ton iron-hulled side-wheel gunboat, built in at Glasgow, Scotland, and gave her the name of CSS "Robert E. Lee" in honor of this Confederate General. During the next year, she became one of the South's most famous Confederate blockade runners, successfully making more than twenty runs through the Union blockade.

The Mississippi River steamboat "Robert E. Lee" was named for Lee after the Civil War. It was the participant in an 1870 St. Louis – New Orleans race with the "Natchez VI", which was featured in a Currier and Ives lithograph. The "Robert E. Lee" won the race. The steamboat inspired the 1912 song "Waiting for the Robert E. Lee" by Lewis F. Muir and L. Wolfe Gilbert. In more modern times, the , a built in 1958, was named for Lee, as was the M3 Lee tank, produced in 1941 and 1942.

The Commonwealth of Virginia issues an optional license plate honoring Lee, making reference to him as 'The Virginia Gentleman'. In February 2014, a road on Fort Bliss previously named for Lee was renamed to honor Buffalo Soldiers.

A recent biographer, Jonathan Horn, outlines the unsuccessful efforts in Washington to memorialize Lee in the naming of the Arlington Memorial Bridge after both Grant and Lee.


Lee is a main character in the Shaara Family novels "The Killer Angels" (1974, "Gettysburg"), "Gods and Generals" (1988), and "The Last Full Measure" (2000), as well as the film adaptations of "Gettysburg" (1993) and "Gods and Generals" (2003). He is played by Martin Sheen in the former and by Lee's descendant Robert Duvall in the latter. Lee is portrayed as a hero in the historical children's novel "Lee and Grant at Appomattox" (1950) by MacKinlay Kantor. His part in the Civil War is told from the perspective of his horse in Richard Adams's book "Traveller" (1988).

Lee is an obvious subject for American Civil War alternate histories. Ward Moore's "Bring the Jubilee" (1953), MacKinlay Kantor's "If the South Had Won the Civil War" (1960), and Harry Turtledove's "The Guns of the South" (1992), all have Lee ending up as President of a victorious Confederacy and freeing the slaves (or laying the groundwork for the slaves to be freed in a later decade). Although Moore and Kantor's novels relegate him to a set of passing references, Lee is more of a main character in Turtledove's "Guns". He is also the prime character of Turtledove's "Lee at the Alamo," which can be read on-line, and sees the opening of the Civil War drastically altered so as to affect Lee's personal priorities considerably. Turtledove's "War Between the Provinces" series is an allegory of the Civil War told in the language of fairy tales, with Lee appearing as a knight named "Duke Edward of Arlington." Lee is also a knight in "The Charge of Lee's Brigade" in "Alternate Generals" volume 1, written by Turtledove's friend S.M. Stirling and featuring Lee, whose Virginia is still a loyal British colony, fighting for the Crown against the Russians in Crimea. In Lee Allred's "East of Appomattox" in "Alternate Generals" volume 3, Lee is the Confederate Minister to London circa 1868, desperately seeking help for a CSA which has turned out poorly suited to independence. Robert Skimin's "Grey Victory" features Lee as a supporting character preparing to run for the presidency in 1867.

In Connie Willis' 1987 novel, Lincoln's Dreams, a research assistant meets a young woman who dreams about the Civil War from Robert E. Lee's point of view.

The Dodge Charger featured in the CBS television series "The Dukes of Hazzard" (1979–1985) was named The General Lee. In the 2005 film based on this series, the car is driven past a statue of the General, while the car's occupants salute him.







</doc>
<doc id="25742" url="https://en.wikipedia.org/wiki?curid=25742" title="Raster graphics">
Raster graphics

In computer graphics, a raster graphics or bitmap image is a dot matrix data structure that represents a generally rectangular grid of pixels (points of color), viewable via a monitor, paper, or other display medium. Raster images are stored in image files with varying formats.

A bitmap is a rectangular grid of pixels, with each pixel's color being specified by a number of bits. A bitmap might be created for storage in the display's video memory or as a device-independent bitmap file. A raster is technically characterized by the width and height of the image in pixels and by the number of bits per pixel (or color depth, which determines the number of colors it can represent).

The printing and prepress industries know raster graphics as contones (from "continuous tones"). The opposite to contones is "line work", usually implemented as vector graphics in digital systems. Vector images can be rasterized (converted into pixels), and raster images vectorized (raster images converted into vector graphics), by software. In both cases some information is lost, although vectorizing can also restore some information back to machine readability, as happens in optical character recognition.

The word "raster" has its origins in the Latin "rastrum" (a rake), which is derived from "radere" (to scrape). It originates from the raster scan of cathode ray tube (CRT) video monitors, which paint the image line by line by magnetically steering a focused electron beam. By association, it can also refer to a rectangular grid of pixels. The word rastrum is now used to refer to a device for drawing musical staff lines.

Most modern computers have bitmapped displays, where each on-screen pixel directly corresponds to a small number of bits in memory. The screen is refreshed simply by scanning through pixels and coloring them according to each set of bits. The refresh procedure, being speed critical, is often implemented by dedicated circuitry, often as a part of a graphics processing unit. An early scanned display with raster computer graphics was invented in the late 1960s by A. Michael Noll at Bell Labs, but its patent application filed February 5, 1970 was abandoned at the Supreme Court in 1977 over the issue of the patentability of computer software.

Most computer images are stored in raster graphics formats or compressed variations, including GIF, JPEG, and PNG, which are popular on the World Wide Web.

Three-dimensional voxel raster graphics are employed in video games and are also used in medical imaging such as MRI scanners.

GIS data is commonly stored in a raster format to encode geographic data as the pixel values. Georeferencing information can also be associated with pixels.

Raster graphics are resolution dependent, meaning they cannot scale up to an arbitrary resolution without loss of apparent quality. This property contrasts with the capabilities of vector graphics, which easily scale up to the quality of the device rendering them. Raster graphics deal more practically than vector graphics with photographs and photo-realistic images, while vector graphics often serve better for typesetting or for graphic design. Modern computer-monitors typically display about 72 to 130 pixels per inch (PPI), and some modern consumer printers can resolve 2400 dots per inch (DPI) or more; determining the most appropriate image resolution for a given printer-resolution can pose difficulties, since printed output may have a greater level of detail than a viewer can discern on a monitor. Typically, a resolution of 150 to 300 PPI works well for 4-color process (CMYK) printing.

However, for printing technologies that perform color mixing through dithering (halftone) rather than through overprinting (virtually all home/office inkjet and laser printers), printer DPI and image PPI have a very different meaning, and this can be misleading. Because, through the dithering process, the printer builds a single image pixel out of several printer dots to increase color depth, the printer's DPI setting must be set far higher than the desired PPI to ensure sufficient color depth without sacrificing image resolution. Thus, for instance, printing an image at 250 PPI may actually require a printer setting of 1200 DPI.

Raster-based image editors, such as PaintShop Pro, Painter, Photoshop, Paint.NET, MS Paint, and GIMP, revolve around editing pixels, unlike vector-based image editors, such as Xfig, CorelDRAW, Adobe Illustrator, or Inkscape, which revolve around editing lines and shapes (vectors). When an image is rendered in a raster-based image editor, the image is composed of millions of pixels. At its core, a raster image editor works by manipulating each individual pixel. Most pixel-based image editors work using the RGB color model, but some also allow the use of other color models such as the CMYK color model.



</doc>
<doc id="25745" url="https://en.wikipedia.org/wiki?curid=25745" title="Rerun">
Rerun

A rerun or repeat is a rebroadcast of an episode of a radio or television program. There are two types of reruns – those that occur during a hiatus, and those that occur when a program is syndicated. Reruns can also be, as the case with more popular shows, when a show is aired outside its timeslot (for example, in the afternoon).

In the United Kingdom, the word "repeat" refers only to a single episode; "rerun" or "rerunning" is the preferred term for an entire series/season. A "repeat" is a single episode of a series that is broadcast outside its original timeslot on the same channel/network. The episode is usually the "repeat" of the scheduled episode that was broadcast in the original timeslot earlier the previous week. It allows viewers who weren't able to watch the show in its timeslot to catch up before the next episode is broadcast. The term "rerun" can also be used in some respects as a synonym for "reprint", the equivalent term for print items; this is especially true for print items that are part of ongoing series (such as comic strips; "Peanuts", for instance, has been in reruns since the retirement and death of creator Charles M. Schulz). In South Africa, reruns of the daily soap opera 7de Laan, and others, are called an Omnibus. The Omnibus is a weekly rerun that is broadcast on a Sunday afternoon on the original channel/network. It only broadcasts the past week's episodes back to back.

When used to refer to the rebroadcast of a single episode, Lucille Ball and Desi Arnaz are generally credited as the inventors of the rerun; it was first utilized for the American television series "I Love Lucy" (1951–57) during Ball's pregnancy. Prior to "I Love Lucy" rerunning its episodes during the summer, shows typically went on a summer hiatus and were replaced with "summer replacements", generally lower-priority programs; this strategy has seen increased use in the 21st century as fewer episodes have been produced each season and in-season reruns have increased. Rod Serling's 1955 teleplay "Patterns" was credited with proving reruns' viability; buoyed by strong word of mouth, the rerun of "Patterns" drew more viewers than the first run as people who had missed the first airing a month prior tuned in to catch the re-airing.

In the United States, most television shows from the late 1940s and early 1950s were performed live, and in many cases they were never recorded. However, television networks in the United States began making kinescope recordings of shows broadcast live from the East Coast. This allowed the show to be broadcast later for the West Coast. These kinescopes, along with pre-filmed shows, and later, videotape, paved the way for extensive reruns of syndicated television series.

In the United States, currently running shows will rerun older episodes from the same season to fill the time slot with the same program during the "off-season" period when no new episodes are being made. Shows will tend to start rerunning episodes after the November sweeps period (the ratings during which will determine the cost of a commercial run during that time slot). and usually show only reruns from mid-December until mid-January or even February sweeps This winter (or "mid-season") phase is also used to try out new shows that did not make it onto the fall schedule to see how they fare with the public. These series usually run six to 13 episodes. If they do well with the public, they may get a renewal for a half (13 weeks) or full season in the new schedule. Shows that are already popular will return from February sweeps until the end of the season (which sometimes ends before May sweeps) with only limited reruns used.

The number of episodes per season, originally well over 30 episodes during the 1950s and 1960s, dropped below 26 (the number of episodes required to fill a time slot for a year without rerunning any episode more than once) in the 1970s. Specials typically pad out the remainder of the schedule.

Often, if a television special such as "Peter Pan" or a network television broadcast of a classic film like "The Wizard of Oz" is especially well-received, it will be rerun from time to time. Before the VCR era, this would be the only opportunity audiences had of seeing a program more than once.

Seasonal programming such as "How the Grinch Stole Christmas", "The Ten Commandments", "It's A Wonderful Life" or the Charlie Brown television specials are normally re-shown each year, in the appropriate timeframe.

A television program goes into syndication when many episodes of the program are sold as a package. Generally the buyer is either a cable channel or an owner of local television stations. Often, programs are not particularly profitable until they are sold for syndication. Since local television stations often need to sell more commercial airtime than network affiliates, syndicated shows are usually edited to make room for extra commercials. Often about 100 episodes (four to five seasons' worth) are required for a weekly series to be rerun in daily syndication (at least four times a week). Very popular series running more than four seasons may start daily reruns of the first seasons, while production and airings continue of the current season's episodes; until approximately the early 1980s, shows that aired in syndication while still in production had the reruns aired under an alternate name (or multiple alternate names, as was the case with "Death Valley Days") to differentiate the reruns from the first-run episodes.

Few people anticipated the long life that a popular television series would eventually see in syndication, so most performers signed contracts that limited residual payments to about six repeats. After that, the actors received nothing and the production company would keep 100% of any income until the copyright expired; many shows did not even have their copyrights renewed and others were systematically destroyed to recycle valuable film, such was the lack of awareness of the potential for revenue from them. This situation went unchanged until the mid-1970s, when contracts for new shows extended residual payments for the performers, regardless of the number of reruns, while tape recycling effectively came to an end (rapid advancements in digital video in the 1990s made preservation far more economical) and the Copyright Act of 1976 extended copyright terms to much longer lengths, eliminating the need for renewal.

Once a series is no longer performing well enough to be sold in syndication, it may still remain in "barter" syndication, in which television stations are offered the program for free in exchange for a requirement to air additional advertisements (without compensation) bundled with the free program during other shows (barter syndication is far more common, if not the norm, in radio, where only the most popular programs charge rights fees). The Program Exchange was once the most prominent barter syndicator in United States television, offering mostly older series from numerous network libraries. Barter syndicated series may be seen on smaller, independent stations with small budgets or as short-term filler on larger stations; they tend not to be as widely syndicated as programs syndicated with a rights fee.

With the growing availability of cable and satellite television channels as well as over-the-air digital subchannels, combined with a growing body of available post-syndication programming, a handful of specialty channels have been built solely or primarily to run former network programming which otherwise would no longer be in syndication. Branded as "classic television", these often carry reruns of programming dating back to the monochrome television era and are promoted as nostalgia. The corresponding radio format would be that of an oldies, classic rock, classic hits or adult standards station. Depending on the programs chosen for a classic network, running the format can be very inexpensive, due to many shows beginning to fall into the public domain.

On cable and satellite, channels that devote at least some of their program schedule to post-syndication reruns include Nick at Nite, TV Land, TBS, USA Network, WGN America, Pop, Discovery Family, Game Show Network, Boomerang, Nicktoons, INSP, RFD-TV, and the Hallmark Channel. Equity Media Holdings had been using low-power television stations to carry its own Retro Television Network in various markets; those stations were, as a result of Equity going bankrupt, sold to religious broadcaster Daystar Television Network. Since the early 2010s, the growth of digital subchannel networks has allowed for increasing specialization of these classic networks: in addition to general-interest program networks such as MeTV, Logo TV, Retro TV and Antenna TV, there exist networks solely for sitcoms (Laff), game shows (Buzzr), black-oriented programs (Bounce TV), children's programming (PBJ, Qubo), true crime and court programming (Justice Network), and feature films (Movies!, getTV and This TV).

Traditionally, shows most likely to be rerun in this manner are scripted comedies and dramas. Such shows are more likely to be considered evergreen content that can be rerun for a long period of time without losing its cultural relevance. Game shows, variety shows, Saturday morning cartoons and, to a lesser extent, newsmagazines, tabloid talk shows and late-night talk shows (often in edited form) have been seen less commonly in reruns; game shows can quickly become dated because of inflation, while talk shows often draw humor from contemporary events. Most variants of reality television have proven to be a comparative failure in reruns, due to a number of factors (high cast turnover, loss of the element of surprise, overall hostility toward the format, and lack of media cross-promotion among them); some self-contained and personality-driven reality shows have been successfully rerun. Reruns of sports broadcasts, which face many of the same issues reality shows face, have found a niche, and networks such as MSG Network, ESPN Classic and NFL Network currently have a significant portion of programming time devoted to reruns of live sportscasts.

With the rise of the DVD video format, box sets featuring season or series runs of television series have become an increasingly important retail item. Some view this development as a rising new idea in the industry of reruns as an increasingly major revenue source in themselves instead of the standard business model as a draw for audiences for advertising. While there were videotape releases of television series before DVD, the format's limited content capacity, large size and reliance on mechanical winding made it impractical as a widespread retail item. Many series which continue to air first-run episodes (such as "Modern Family" and "Grey's Anatomy") may release DVD sets of the prior season between the end of that season and the beginning of the next.

Some television programs that are released on DVD (particularly those that have been out of production for several years) may not have all of the seasons released, either due to poor overall sales or prohibitive costs for obtaining rights to music used in the program; one such incidence is "Perfect Strangers", which has seldom been in wide syndication since the late 1990s primarily due to lack of demand, which had only a DVD set of the first and second seasons released due to the expensiveness of relicensing songs used in later seasons of the series that are performed by the show's two lead characters. In some cases, series whose later season releases have been held up for these reasons may have the remaining seasons made available on DVD, often after a distributor that does not hold syndication rights to the program (such as Shout! Factory) secures the rights for future DVD releases.

TV Guide originally used the term "rerun" to designate rebroadcast programs, but abruptly changed to "repeat" in the early 1970s.

Other TV listings services and publications, including local newspapers, would often indicate reruns as "(R)"; since the early 2000s, many listing services only provide a notation if an episode is new ("(N)"), with reruns getting no notation.

In the United Kingdom, most drama and comedy series run for shorter seasons – typically six, seven or thirteen episodes – and are then replaced by others. An exception is soap operas, which are either on all year round (for example, "EastEnders" and "Coronation Street"), or are on for a season similar to the American format.

As in the U.S., fewer new episodes are made during the summer. Until recently it was also common practice for the BBC, ITV and Channel 4 to repeat classic shows from their archives, but this has more or less dried up in favor of newer (and cheaper) formats like reality shows, except on the BBC where older BBC shows, especially sitcoms like "Dad's Army" and "Fawlty Towers", are frequently repeated.

Syndication did not exist as such in United Kingdom until the arrival of satellite, cable and later, from 1998 on, digital television, although it could be argued that many ITV programs up to the early 1990s, particularly imported programming was syndicated in the sense that each ITV region bought some programs independently of the ITV Network, and in particular many programs out of prime time made by smaller ITV stations were "part-networked" where some regions would show them and others would not. Nowadays there are many channels in the UK (for example, Gold) which repackage and rebroadcast "classic" programming from both sides of the Atlantic. Some of these channels, like their U.S. counterparts, make commercial timing cuts; others get around this by running shows in longer time slots, and critics of timing cuts see no reason why all channels should not do the same. 

Early on in the history of British television, agreements with the actors' union Equity and other trade bodies limited the number of times a single program could be broadcast, usually only twice, and these showings were limited to within a set time period such as five years. This was due to the unions' fear that the channels filling their schedules with repeats could put actors and other production staff out of work as fewer new shows would be made. It also had the unintentional side effect of causing many programs to be junked after their repeat rights had expired, as they were considered to be of no further use by the broadcasters. Although these agreements changed during the 1980s and beyond, it is still expensive to repeat archive television series on British terrestrial television, as new contracts have to be drawn up and payments made to the artists concerned. Repeats on multi-channel television are cheaper, as are re-showings of newer programs covered by less strict repeat clauses. However, programs are no longer destroyed, as the historical and cultural reasons for keeping them have now been seen and the cost to maintain archives is now far less, even if the programs have little or no repeat value.



</doc>
<doc id="25748" url="https://en.wikipedia.org/wiki?curid=25748" title="Router (computing)">
Router (computing)

A router is a networking device that forwards data packets between computer networks. Routers perform the traffic directing functions on the Internet. Data sent through the internet, such as a web page or email, is in the form of data packets. A packet is typically forwarded from one router to another router through the networks that constitute an internetwork (e.g. the Internet) until it reaches its destination node.

A router is connected to two or more data lines from different IP networks. When a data packet comes in on one of the lines, the router reads the network address information in the packet header to determine the ultimate destination. Then, using information in its routing table or routing policy, it directs the packet to the next network on its journey.

The most familiar type of IP routers are home and small office routers that simply forward IP packets between the home computers and the Internet. An example of a router would be the owner's cable or DSL router, which connects to the Internet through an Internet service provider (ISP). More sophisticated routers, such as enterprise routers, connect large business or ISP networks up to the powerful core routers that forward data at high speed along the optical fiber lines of the Internet backbone. 

When multiple routers are used in interconnected networks, the routers can exchange information about destination addresses using a routing protocol. Each router builds up a routing table listing the preferred routes between any two computer systems on the interconnected networks.

A router has two types of network element components organized onto separate processing "planes":

A router may have interfaces for different types of physical layer connections, such as copper cables, fiber optic, or wireless transmission. It can also support different network layer transmission standards. Each network interface is used to enable data packets to be forwarded from one transmission system to another. Routers may also be used to connect two or more logical groups of computer devices known as subnets, each with a different network prefix.

Routers may provide connectivity within enterprises, between enterprises and the Internet, or between internet service providers' (ISPs') networks. The largest routers (such as the Cisco CRS-1 or Juniper PTX) interconnect the various ISPs, or may be used in large enterprise networks. Smaller routers usually provide connectivity for typical home and office networks.

All sizes of routers may be found inside enterprises. The most powerful routers are usually found in ISPs, academic and research facilities. Large businesses may also need more powerful routers to cope with ever-increasing demands of intranet data traffic. A hierarchical internetworking model for interconnecting routers in large networks is in common use.

Access routers, including small office/home office (SOHO) models, are located at home and customer sites such as branch offices that do not need hierarchical routing of their own. Typically, they are optimized for low cost. Some SOHO routers are capable of running alternative free Linux-based firmware like Tomato, OpenWrt or DD-WRT.

Distribution routers aggregate traffic from multiple access routers. Distribution routers are often responsible for enforcing quality of service across a wide area network (WAN), so they may have considerable memory installed, multiple WAN interface connections, and substantial onboard data processing routines. They may also provide connectivity to groups of file servers or other external networks.

In enterprises, a core router may provide a collapsed backbone interconnecting the distribution tier routers from multiple buildings of a campus, or large enterprise locations. They tend to be optimized for high bandwidth, but lack some of the features of edge routers.

External networks must be carefully considered as part of the overall security strategy of the local network. A router may include a firewall, VPN handling, and other security functions, or these may be handled by separate devices. Routers also commonly perform network address translation which restricts connections initiated from external connections but is not recognised as a security feature by all experts. Some experts argue that open source routers are more secure and reliable than closed source routers because open source routers allow mistakes to be quickly found and corrected.

Routers are also often distinguished on the basis of the network in which they operate. A router in a local area network (LAN) of a single organisation is called an "interior router". A router that is operated in the Internet backbone is described as "exterior router". While a router that connects a LAN with the Internet or a wide area network (WAN) is called a "border router", or "gateway router".

Routers intended for ISP and major enterprise connectivity usually exchange routing information using the Border Gateway Protocol (BGP). defines the types of BGP routers according to their functions:

The concept of an "Interface computer" was first used by Donald Davies for the NPL network in 1966. The Interface Message Processor (IMP), conceived in 1967 for use in the ARPANET, had fundamentally the same functionality as a router does today. The idea for a router (called "gateways" at the time) initially came about through an international group of computer networking researchers called the International Network Working Group (INWG). Set up in 1972 as an informal group to consider the technical issues involved in connecting different networks, it became a subcommittee of the International Federation for Information Processing later that year. These gateway devices were different from most previous packet switching schemes in two ways. First, they connected dissimilar kinds of networks, such as serial lines and local area networks. Second, they were connectionless devices, which had no role in assuring that traffic was delivered reliably, leaving that entirely to the hosts.

The idea was explored in more detail, with the intention to produce a prototype system as part of two contemporaneous programs. One was the initial DARPA-initiated program, which created the TCP/IP architecture in use today. The other was a program at Xerox PARC to explore new networking technologies, which produced the PARC Universal Packet system; due to corporate intellectual property concerns it received little attention outside Xerox for years. Some time after early 1974, the first Xerox routers became operational. The first true IP router was developed by Ginny Strazisar at BBN, as part of that DARPA-initiated effort, during 1975-1976. By the end of 1976, three PDP-11-based routers were in service in the experimental prototype Internet.

The first multiprotocol routers were independently created by staff researchers at MIT and Stanford in 1981; the Stanford router was done by William Yeager, and the MIT one by Noel Chiappa; both were also based on PDP-11s. Virtually all networking now uses TCP/IP, but multiprotocol routers are still manufactured. They were important in the early stages of the growth of computer networking when protocols other than TCP/IP were in use. Modern Internet routers that handle both IPv4 and IPv6 are multiprotocol but are simpler devices than routers processing AppleTalk, DECnet, IP and Xerox protocols.

From the mid-1970s and in the 1980s, general-purpose minicomputers served as routers. Modern high-speed routers are highly specialized computers with extra hardware added to speed both common routing functions, such as packet forwarding, and specialised functions such as IPsec encryption. There is substantial use of Linux and Unix software based machines, running open source routing code, for research and other applications. The Cisco IOS operating system was independently designed. Major router operating systems, such as Junos and NX-OS, are extensively modified versions of Unix software.

The main purpose of a router is to connect multiple networks and forward packets destined either for its own networks or other networks. A router is considered a layer-3 device because its primary forwarding decision is based on the information in the layer-3 IP packet, specifically the destination IP address. When a router receives a packet, it searches its routing table to find the best match between the destination IP address of the packet and one of the addresses in the routing table. Once a match is found, the packet is encapsulated in the layer-2 data link frame for the outgoing interface indicated in the table entry. A router typically does not look into the packet payload, but only at the layer-3 addresses to make a forwarding decision, plus optionally other information in the header for hints on, for example, quality of service (QoS). For pure IP forwarding, a router is designed to minimize the state information associated with individual packets. Once a packet is forwarded, the router does not retain any historical information about the packet.

The routing table itself can contain information derived from a variety of sources, such as a default or static routes that are configured manually, or dynamic routing protocols where the router learns routes from other routers. A default route is one that is used to route all traffic whose destination does not otherwise appear in the routing table; this is common – even necessary – in small networks, such as a home or small business where the default route simply sends all non-local traffic to the Internet service provider. The default route can be manually configured (as a static route), or learned by dynamic routing protocols, or be obtained by DHCP.

A router can run more than one routing protocol at a time, particularly if it serves as an autonomous system border router between parts of a network that run different routing protocols; if it does so, then redistribution may be used (usually selectively) to share information between the different protocols running on the same router.

Besides making a decision as to which interface a packet is forwarded to, which is handled primarily via the routing table, a router also has to manage congestion when packets arrive at a rate higher than the router can process. Three policies commonly used in the Internet are tail drop, random early detection (RED), and weighted random early detection (WRED). Tail drop is the simplest and most easily implemented; the router simply drops new incoming packets once the length of the queue exceeds the size of the buffers in the router. RED probabilistically drops datagrams early when the queue exceeds a pre-configured portion of the buffer, until a pre-determined max, when it becomes tail drop. WRED requires a weight on the average queue size to act upon when the traffic is about to exceed the pre-configured size, so that short bursts will not trigger random drops.

Another function a router performs is to decide which packet should be processed first when multiple queues exist. This is managed through QoS, which is critical when Voice over IP is deployed, so as not to introduce excessive latency.

Yet another function a router performs is called policy-based routing where special rules are constructed to override the rules derived from the routing table when a packet forwarding decision is made.

Router functions may be performed through the same internal paths that the packets travel inside the router. Some of the functions may be performed through an application-specific integrated circuit (ASIC) to avoid overhead of scheduling CPU time to process the packets. Others may have to be performed through the CPU as these packets need special attention that cannot be handled by an ASIC.




</doc>
<doc id="25750" url="https://en.wikipedia.org/wiki?curid=25750" title="Routing">
Routing

Routing is the process of selecting a path for traffic in a network or between or across multiple networks. Broadly, routing is performed in many types of networks, including circuit-switched networks, such as the public switched telephone network (PSTN), and computer networks, such as the Internet.

In packet switching networks, routing is the higher-level decision making that directs network packets from their source toward their destination through intermediate network nodes by specific packet forwarding mechanisms. Packet forwarding is the transit of network packets from one network interface to another. Intermediate nodes are typically network hardware devices such as routers, gateways, firewalls, or switches. General-purpose computers also forward packets and perform routing, although they have no specially optimized hardware for the task.

The routing process usually directs forwarding on the basis of routing tables. Routing tables maintain a record of the routes to various network destinations. Routing tables may be specified by an administrator, learned by observing network traffic or built with the assistance of routing protocols.

Routing, in a narrower sense of the term, often refers to IP routing and is contrasted with bridging. IP routing assumes that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within local area networks.

Routing schemes differ in how they deliver messages:

Unicast is the dominant form of message delivery on the Internet. This article focuses on unicast routing algorithms.

With static routing, small networks may use manually configured routing tables. Larger networks have complex topologies that can change rapidly, making the manual construction of routing tables unfeasible. Nevertheless, most of the public switched telephone network (PSTN) uses pre-computed routing tables, with fallback routes if the most direct route becomes blocked (see routing in the PSTN).

Dynamic routing attempts to solve this problem by constructing routing tables automatically, based on information carried by routing protocols, allowing the network to act nearly autonomously in avoiding network failures and blockages. Dynamic routing dominates the Internet. Examples of dynamic-routing protocols and algorithms include Routing Information Protocol (RIP), Open Shortest Path First (OSPF) and Enhanced Interior Gateway Routing Protocol (EIGRP).

Distance vector algorithms use the Bellman–Ford algorithm. This approach assigns a "cost" number to each of the links between each node in the network. Nodes send information from point A to point B via the path that results in the lowest "total cost" (i.e. the sum of the costs of the links between the nodes used).

When a node first starts, it only knows of its immediate neighbors and the direct cost involved in reaching them. (This information — the list of destinations, the total cost to each, and the "next hop" to send data to get there — makes up the routing table, or "distance table".) Each node, on a regular basis, sends to each neighbor node its own current assessment of the total cost to get to all the destinations it knows of. The neighboring nodes examine this information and compare it to what they already know; anything that represents an improvement on what they already have, they insert in their own table. Over time, all the nodes in the network discover the best next hop and total cost for all destinations.

When a network node goes down, any nodes that used it as their next hop discard the entry and convey the updated routing information to all adjacent nodes, which in turn repeat the process. Eventually, all the nodes in the network receive the updates and discover new paths to all the destinations that don't involve the down node.

When applying link-state algorithms, a graphical map of the network is the fundamental data used for each node. To produce its map, each node floods the entire network with information about the other nodes it can connect to. Each node then independently assembles this information into a map. Using this map, each router independently determines the least-cost path from itself to every other node using a standard shortest paths algorithm such as Dijkstra's algorithm. The result is a tree graph rooted at the current node, such that the path through the tree from the root to any other node is the least-cost path to that node. This tree then serves to construct the routing table, which specifies the best next hop to get from the current node to any other node.

A link-state routing algorithm optimized for mobile ad hoc networks is the optimized Link State Routing Protocol (OLSR). OLSR is proactive; it uses Hello and Topology Control (TC) messages to discover and disseminate link-state information through the mobile ad hoc network. Using Hello messages, each node discovers 2-hop neighbor information and elects a set of "multipoint relays" (MPRs). MPRs distinguish OLSR from other link-state routing protocols.

Distance vector and link-state routing are both intra-domain routing protocols. They are used inside an autonomous system, but not between autonomous systems. Both of these routing protocols become intractable in large networks and cannot be used in inter-domain routing. Distance vector routing is subject to instability if there are more than a few hops in the domain. Link state routing needs significant resources to calculate routing tables. It also creates heavy traffic due to flooding.

Path-vector routing is used for inter-domain routing. It is similar to distance vector routing. Path-vector routing assumes that one node (there can be many) in each autonomous system acts on behalf of the entire autonomous system. This node is called the "speaker node." The speaker node creates a routing table and advertises it to neighboring speaker nodes in neighboring autonomous systems. The idea is the same as distance vector routing except that only speaker nodes in each autonomous system can communicate with each other. The speaker node advertises the path, not the metric, of the nodes in its autonomous system or other autonomous systems.

Path-vector routing is discussed in RFC 1322. The path-vector routing algorithm is similar to the distance vector algorithm in the sense that each border router advertises the destinations it can reach to its neighboring router. However, instead of advertising networks in terms of a destination and the distance to that destination, networks are advertised as destination addresses and path descriptions to reach those destinations. A route is defined as a pairing between a destination and the attributes of the path to that destination, thus the name, path-vector routing; The routers receive a vector that contains paths to a set of destinations.

The path, expressed in terms of the domains (or confederations) traversed so far, is carried in a special path attribute that records the sequence of routing domains through which the reachability information has passed.

Path selection involves applying a routing metric to multiple routes to select (or predict) the best route. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.

In computer networking, the metric is computed by a routing algorithm, and can cover information such as bandwidth, network delay, hop count, path cost, load, MTU (maximum transmission unit), reliability, and communication cost (see e.g. this survey for a list of proposed routing metrics). The routing table stores only the best possible routes, while link-state or topological databases may store all other information as well.

In case of overlapping or equal routes, algorithms consider the following elements to decide which routes to install into the routing table (sorted by priority):

Because a routing metric is specific to a given routing protocol, multi-protocol routers must use some external heuristic to select between routes learned from different routing protocols. Cisco routers, for example, attribute a value known as the administrative distance to each route, where smaller administrative distances indicate routes learned from a supposedly more reliable protocol.

A local network administrator, in special cases, can set up host-specific routes to a particular device that provides more control over network usage, permits testing, and better overall security. This is useful for debugging network connections or routing tables.

In some small systems, a single central device decides ahead of time the complete path of every packet.
In some other small systems, whichever edge device injects a packet into the network decides ahead of time the complete path of that particular packet.
In both of these systems, that route-planning device needs to know a lot of information about what devices are connected to the network and how they are connected to each other.
Once it has this information, it can use an algorithm such as A* search algorithm to find the best path.

In high-speed systems, there are so many packets transmitted every second that it is infeasible for a single device to calculate the complete path for each and every packet. Early high-speed systems dealt with this by setting up a circuit switching relay channel once for the first packet between some source and some destination; later packets between that same source and that same destination continue to follow the same path without recalculating until the channel teardown. Later high-speed systems inject packets into the network without any one device ever calculating a complete path for that packet—multiple agents.

In large systems, there are so many connections between devices, and those connections change so frequently, that it is infeasible for any one device to even know how all the devices are connected to each other, much less calculate a complete path through them.
Such systems generally use next-hop routing.

Most systems use a deterministic dynamic routing algorithm:
When a device chooses a path to a particular final destination, that device always chooses the same path to that destination until it receives information that makes it think some other path is better.
A few routing algorithms do not use a deterministic algorithm to find the "best" link for a packet to get from its original source to its final destination.
Instead, to avoid congestion in switched systems or network hot spots in packet systems, a few algorithms use a randomized algorithm—Valiant's paradigm—that routes a path to a randomly picked intermediate destination, and from there to its true final destination.
In many early telephone switches, a randomizer was often used to select the start of a path through a multistage switching fabric.

Depending on the application for which path selection is performed, different metrics can be used. For example, for web requests one can use minimum latency paths to minimize web page load time, or for bulk data transfers one can choose the least utilized path to balance load across the network and increase throughput. A popular path selection objective is to reduce the average completion times of traffic flows and the total network bandwidth consumption which basically leads to better use of network capacity. Recently, a path selection metric was proposed that computes the total number of bytes scheduled on the edges per path as selection metric. An empirical analysis of several path selection metrics, including this new proposal, has been made available.

In some networks, routing is complicated by the fact that no single entity is responsible for selecting paths; instead, multiple entities are involved in selecting paths or even parts of a single path. Complications or inefficiency can result if these entities choose paths to optimize their own objectives, which may conflict with the objectives of other participants.

A classic example involves traffic in a road system, in which each driver picks a path that minimizes their travel time. With such routing, the equilibrium routes can be longer than optimal for all drivers. In particular, Braess' paradox shows that adding a new road can "lengthen" travel times for all drivers.

In another model, for example, used for routing automated guided vehicles (AGVs) on a terminal, reservations are made for each vehicle to prevent simultaneous use of the same part of an infrastructure. This approach is also referred to as context-aware routing.

The Internet is partitioned into autonomous systems (ASs) such as internet service providers (ISPs), each of which controls routes involving its network, at multiple levels. First, AS-level paths are selected via the BGP protocol, which produces a sequence of ASs through which packets flow. Each AS may have multiple paths, offered by neighboring ASs, from which to choose. Its decision often involves business relationships with these neighboring ASs, which may be unrelated to path quality or latency. Second, once an AS-level path has been selected, there are often multiple corresponding router-level paths, in part because two ISPs may be connected in multiple locations. In choosing the single router-level path, it is common practice for each ISP to employ hot-potato routing: sending traffic along the path that minimizes the distance through the ISP's own network—even if that path lengthens the total distance to the destination.

Consider two ISPs, "A" and "B". Each has a presence in New York, connected by a fast link with latency 5 ms—and each has a presence in London connected by a 5 ms link. Suppose both ISPs have trans-Atlantic links that connect their two networks, but "A"'s link has latency 100 ms and B's has latency 120 ms. When routing a message from a source in "A" 's London network to a destination in "B" 's New York network, "A" may choose to immediately send the message to "B" in London. This saves "A" the work of sending it along an expensive trans-Atlantic link, but causes the message to experience latency 125 ms when the other route would have been 20 ms faster.

A 2003 measurement study of Internet routes found that, between pairs of neighboring ISPs, more than 30% of paths have inflated latency due to hot-potato routing, with 5% of paths being delayed by at least 12 ms. Inflation due to AS-level path selection, while substantial, was attributed primarily to BGP's lack of a mechanism to directly optimize for latency, rather than to selfish routing policies. It was also suggested that, were an appropriate mechanism in place, ISPs would be willing to cooperate to reduce latency rather than use hot-potato routing.

Such a mechanism was later published by the same authors, first for the case of two ISPs and then for the global case.

As the Internet and IP networks become mission critical business tools, there has been increased interest in techniques and methods to monitor the routing posture of networks. Incorrect routing or routing issues cause undesirable performance degradation, flapping and/or downtime. Monitoring routing in a network is achieved using route analytics tools and techniques.

In networks where a logically centralized control is available over the forwarding state, for example, using Software-defined networking, routing techniques can be used that aim to optimize global and network-wide performance metrics. This has been used by large internet companies that operate many data centers in different geographical locations attached using private optical links examples of which includes Microsoft's Global WAN, Facebook's Express Backbone, and Google's B4. Global performance metrics to optimize include maximizing network utilization, minimizing traffic flow completion times, and maximizing the traffic delivered prior to specific deadlines. Minimizing flow completion times over private WAN, particularly, has not received much attention from the research community. However, with the increasing number of businesses that operate globally distributed data centers connected using private inter-data center networks, it is likely to see increasing research effort in this realm. A very recent work on reducing the completion times of flows over private WAN discusses modeling routing as a graph optimization problem by pushing all the queuing to the end-points. Authors also propose a heuristic to solve the problem efficiently while sacrificing negligible performance.




</doc>
<doc id="25751" url="https://en.wikipedia.org/wiki?curid=25751" title="RIP (disambiguation)">
RIP (disambiguation)

RIP (abbreviating "rest in peace" or ) ) is a common element of Christian epitaphs.

RIP may also refer to:










</doc>
<doc id="25754" url="https://en.wikipedia.org/wiki?curid=25754" title="Resistor">
Resistor

A resistor is a passive two-terminal electrical component that implements electrical resistance as a circuit element. In electronic circuits, resistors are used to reduce current flow, adjust signal levels, to divide voltages, bias active elements, and terminate transmission lines, among other uses. High-power resistors that can dissipate many watts of electrical power as heat, may be used as part of motor controls, in power distribution systems, or as test loads for generators.
Fixed resistors have resistances that only change slightly with temperature, time or operating voltage. Variable resistors can be used to adjust circuit elements (such as a volume control or a lamp dimmer), or as sensing devices for heat, light, humidity, force, or chemical activity.

Resistors are common elements of electrical networks and electronic circuits and are ubiquitous in electronic equipment. Practical resistors as discrete components can be composed of various compounds and forms. Resistors are also implemented within integrated circuits.

The electrical function of a resistor is specified by its resistance: common commercial resistors are manufactured over a range of more than nine orders of magnitude. The nominal value of the resistance falls within the manufacturing tolerance, indicated on the component.

Two typical schematic diagram symbols are as follows:
The notation to state a resistor's value in a circuit diagram varies.

One common scheme is the RKM code following IEC 60062. It avoids using a decimal separator and replaces the decimal separator with a letter loosely associated with SI prefixes corresponding with the part's resistance. For example, "8K2" as part marking code, in a circuit diagram or in a bill of materials (BOM) indicates a resistor value of 8.2 kΩ. Additional zeros imply a tighter tolerance, for example "15M0" for three significant digits. When the value can be expressed without the need for a prefix (that is, multiplicator 1), an "R" is used instead of the decimal separator. For example, "1R2" indicates 1.2 Ω, and "18R" indicates 18 Ω.

The behaviour of an ideal resistor is dictated by the relationship specified by Ohm's law:

Ohm's law states that the voltage (V) across a resistor is proportional to the current (I), where the constant of proportionality is the resistance (R). For example, if a 300 ohm resistor is attached across the terminals of a 12 volt battery, then a current of 12 / 300 = 0.04 amperes flows through that resistor.

Practical resistors also have some inductance and capacitance which affect the relation between voltage and current in alternating current circuits.

The ohm (symbol: Ω) is the SI unit of electrical resistance, named after Georg Simon Ohm. An ohm is equivalent to a volt per ampere. Since resistors are specified and manufactured over a very large range of values, the derived units of milliohm (1 mΩ = 10 Ω), kilohm (1 kΩ = 10 Ω), and megohm (1 MΩ = 10 Ω) are also in common usage.

The total resistance of resistors connected in series is the sum of their individual resistance values.

The total resistance of resistors connected in parallel is the reciprocal of the sum of the reciprocals of the individual resistors.

For example, a 10 ohm resistor connected in parallel with a 5 ohm resistor and a 15 ohm resistor produces ohms of resistance, or = 2.727 ohms.

A resistor network that is a combination of parallel and series connections can be broken up into smaller parts that are either one or the other. Some complex networks of resistors cannot be resolved in this manner, requiring more sophisticated circuit analysis. Generally, the Y-Δ transform, or matrix methods can be used to solve such problems.

At any instant, the power "P" (watts) consumed by a resistor of resistance "R" (ohms) is calculated as:
formula_4
where "V" (volts) is the voltage across the resistor and "I" (amps) is the current flowing through it. Using Ohm's law, the two other forms can be derived. This power is converted into heat which must be dissipated by the resistor's package before its temperature rises excessively.

Resistors are rated according to their maximum power dissipation. Discrete resistors in solid-state electronic systems are typically rated as 1/10, 1/8, or 1/4 watt. They usually absorb much less than a watt of electrical power and require little attention to their power rating.
Resistors required to dissipate substantial amounts of power, particularly used in power supplies, power conversion circuits, and power amplifiers, are generally referred to as "power resistors"; this designation is loosely applied to resistors with power ratings of 1 watt or greater. Power resistors are physically larger and may not use the preferred values, color codes, and external packages described below.

If the average power dissipated by a resistor is more than its power rating, damage to the resistor may occur, permanently altering its resistance; this is distinct from the reversible change in resistance due to its temperature coefficient when it warms. Excessive power dissipation may raise the temperature of the resistor to a point where it can burn the circuit board or adjacent components, or even cause a fire. There are flameproof resistors that fail (open circuit) before they overheat dangerously.

Since poor air circulation, high altitude, or high operating temperatures may occur, resistors may be specified with higher rated dissipation than is experienced in service.

All resistors have a maximum voltage rating; this may limit the power dissipation for higher resistance values.

Practical resistors have a series inductance and a small parallel capacitance; these specifications can be important in high-frequency applications. In a low-noise amplifier or pre-amp, the noise characteristics of a resistor may be an issue.

The temperature coefficient of the resistance may also be of concern in some precision applications.

The unwanted inductance, excess noise, and temperature coefficient are mainly dependent on the technology used in manufacturing the resistor. They are not normally specified individually for a particular family of resistors manufactured using a particular technology. A family of discrete resistors is also characterized according to its form factor, that is, the size of the device and the position of its leads (or terminals) which is relevant in the practical manufacturing of circuits using them.

Practical resistors are also specified as having a maximum power rating which must exceed the anticipated power dissipation of that resistor in a particular circuit: this is mainly of concern in power electronics applications.
Resistors with higher power ratings are physically larger and may require heat sinks. In a high-voltage circuit, attention must sometimes be paid to the rated maximum working voltage of the resistor. While there is no minimum working voltage for a given resistor, failure to account for a resistor's maximum rating may cause the resistor to incinerate when current is run through it.

Through-hole components typically have "leads" (pronounced ) leaving the body "axially," that is, on a line parallel with the part's longest axis. Others have leads coming off their body "radially" instead. Other components may be SMT (surface mount technology), while high power resistors may have one of their leads designed into the heat sink.

Carbon composition resistors (CCR) consist of a solid cylindrical resistive element with embedded wire leads or metal end caps to which the lead wires are attached. The body of the resistor is protected with paint or plastic. Early 20th-century carbon composition resistors had uninsulated bodies; the lead wires were wrapped around the ends of the resistance element rod and soldered. The completed resistor was painted for color-coding of its value.

The resistive element is made from a mixture of finely powdered carbon and an insulating material, usually ceramic. A resin holds the mixture together. The resistance is determined by the ratio of the fill material (the powdered ceramic) to the carbon. Higher concentrations of carbon, which is a good conductor, result in lower resistance. Carbon composition resistors were commonly used in the 1960s and earlier, but are not popular for general use now as other types have better specifications, such as tolerance, voltage dependence, and stress. Carbon composition resistors change value when stressed with over-voltages. Moreover, if internal moisture content, from exposure for some length of time to a humid environment, is significant, soldering heat creates a non-reversible change in resistance value. Carbon composition resistors have poor stability with time and were consequently factory sorted to, at best, only 5% tolerance.
These resistors are non-inductive, which provides benefits when used in voltage pulse reduction and surge protection applications.
Carbon composition resistors have higher capability to withstand overload relative to the component's size.

Carbon composition resistors are still available, but relatively expensive. Values ranged from fractions of an ohm to 22 megohms. Due to their high price, these resistors are no longer used in most applications. However, they are used in power supplies and welding controls. They are also in demand for repair of vintage electronic equipment where authenticity is a factor.

A carbon pile resistor is made of a stack of carbon disks compressed between two metal contact plates. Adjusting the clamping pressure changes the resistance between the plates. These resistors are used when an adjustable load is required, for example in testing automotive batteries or radio transmitters. A carbon pile resistor can also be used as a speed control for small motors in household appliances (sewing machines, hand-held mixers) with ratings up to a few hundred watts. A carbon pile resistor can be incorporated in automatic voltage regulators for generators, where the carbon pile controls the field current to maintain relatively constant voltage. The principle is also applied in the carbon microphone.

A carbon film is deposited on an insulating substrate, and a helix is cut in it to create a long, narrow resistive path. Varying shapes, coupled with the resistivity of amorphous carbon (ranging from 500 to 800 μΩ m), can provide a wide range of resistance values. Compared to carbon composition they feature low noise, because of the precise distribution of the pure graphite without binding. Carbon film resistors feature a power rating range of 0.125 W to 5 W at 70 °C. Resistances available range from 1 ohm to 10 megohm. The carbon film resistor has an operating temperature range of −55 °C to 155 °C. It has 200 to 600 volts maximum working voltage range. Special carbon film resistors are used in applications requiring high pulse stability.

Carbon composition resistors can be printed directly onto printed circuit board (PCB) substrates as part of the PCB manufacturing process. Although this technique is more common on hybrid PCB modules, it can also be used on standard fibreglass PCBs. Tolerances are typically quite large, and can be in the order of 30%. A typical application would be non-critical pull-up resistors.

Thick film resistors became popular during the 1970s, and most SMD (surface mount device) resistors today are of this type. The resistive element of thick films is 1000 times thicker than thin films, but the principal difference is how the film is applied to the cylinder (axial resistors) or the surface (SMD resistors).

Thin film resistors are made by sputtering (a method of vacuum deposition) the resistive material onto an insulating substrate. The film is then etched in a similar manner to the old (subtractive) process for making printed circuit boards; that is, the surface is coated with a photo-sensitive material, then covered by a pattern film, irradiated with ultraviolet light, and then the exposed photo-sensitive coating is developed, and underlying thin film is etched away.

Thick film resistors are manufactured using screen and stencil printing processes.

Because the time during which the sputtering is performed can be controlled, the thickness of the thin film can be accurately controlled. The type of material is also usually different consisting of one or more ceramic (cermet) conductors such as tantalum nitride (TaN), ruthenium oxide (), lead oxide (PbO), bismuth ruthenate (), nickel chromium (NiCr), or bismuth iridate ().

The resistance of both thin and thick film resistors after manufacture is not highly accurate; they are usually trimmed to an accurate value by abrasive or laser trimming. Thin film resistors are usually specified with tolerances of 1% and 5%, and with temperature coefficients of 5 to 50 ppm/K. They also have much lower noise levels, on the level of 10–100 times less than thick film resistors.
Thick film resistors may use the same conductive ceramics, but they are mixed with sintered (powdered) glass and a carrier liquid so that the composite can be screen-printed. This composite of glass and conductive ceramic (cermet) material is then fused (baked) in an oven at about 850 °C.

Thick film resistors, when first manufactured, had tolerances of 5%, but standard tolerances have improved to 2% or 1% in the last few decades. Temperature coefficients of thick film resistors are high, typically ±200 or ±250 ppm/K; a 40-kelvin (70 °F) temperature change can change the resistance by 1%.

Thin film resistors are usually far more expensive than thick film resistors. For example, SMD thin film resistors, with 0.5% tolerances, and with 25 ppm/K temperature coefficients, when bought in full size reel quantities, are about twice the cost of 1%, 250 ppm/K thick film resistors.

A common type of axial-leaded resistor today is the metal-film resistor. Metal Electrode Leadless Face (MELF) resistors often use the same technology.

Metal film resistors are usually coated with nickel chromium (NiCr), but might be coated with any of the cermet materials listed above for thin film resistors. Unlike thin film resistors, the material may be applied using different techniques than sputtering (though this is one of the techniques). Also, unlike thin-film resistors, the resistance value is determined by cutting a helix through the coating rather than by etching. (This is similar to the way carbon resistors are made.) The result is a reasonable tolerance (0.5%, 1%, or 2%) and a temperature coefficient that is generally between 50 and 100 ppm/K. Metal film resistors possess good noise characteristics and low non-linearity due to a low voltage coefficient. Also beneficial are their tight tolerance, low temperature coefficient and long-term stability.

Metal-oxide film resistors are made of metal oxides which results in a higher operating temperature and greater stability/reliability than Metal film. They are used in applications with high endurance demands.

Wirewound resistors are commonly made by winding a metal wire, usually nichrome, around a ceramic, plastic, or fiberglass core. The ends of the wire are soldered or welded to two caps or rings, attached to the ends of the core. The assembly is protected with a layer of paint, molded plastic, or an enamel coating baked at high temperature. These resistors are designed to withstand unusually high temperatures of up to 450 °C. Wire leads in low power wirewound resistors are usually between 0.6 and 0.8 mm in diameter and tinned for ease of soldering. For higher power wirewound resistors, either a ceramic outer case or an aluminum outer case on top of an insulating layer is used – if the outer case is ceramic, such resistors are sometimes described as "cement" resistors, though they do not actually contain any traditional cement. The aluminum-cased types are designed to be attached to a heat sink to dissipate the heat; the rated power is dependent on being used with a suitable heat sink, e.g., a 50 W power rated resistor overheats at a fraction of the power dissipation if not used with a heat sink. Large wirewound resistors may be rated for 1,000 watts or more.

Because wirewound resistors are coils they have more undesirable inductance than other types of resistor, although winding the wire in sections with alternately reversed direction can minimize inductance. Other techniques employ bifilar winding, or a flat thin former (to reduce cross-section area of the coil). For the most demanding circuits, resistors with Ayrton–Perry winding are used.

Applications of wirewound resistors are similar to those of composition resistors with the exception of the high frequency. The high frequency response of wirewound resistors is substantially worse than that of a composition resistor.

In 1960 Felix Zandman and Sidney J. Stein presented a development of resistor film of very high stability.

The primary resistance element of a foil resistor is a chromium nickel alloy foil several micrometers thick. Chromium nickel alloys are characterized by having a large electrical resistance (about 58 times that of copper), a small temperature coefficient and high resistance to oxidation. Examples are Chromel A and Nichrome V, whose typical composition is 80 Ni and 20 Cr, with a melting point of 1420° C. When iron is added, the chromium nickel alloy becomes more ductile. The Nichrome and Chromel C are examples of an alloy containing iron. The composition typical of Nichrome is 60 Ni, 12 Cr, 26 Fe, 2 Mn and Chromel C, 64 Ni, 11 Cr, Fe 25. The melting temperature of these alloys are 1350° and 1390 ° C, respectively. 

Since their introduction in the 1960s, foil resistors have had the best precision and stability of any resistor available. One of the important parameters of stability is the temperature coefficient of resistance (TCR). The TCR of foil resistors is extremely low, and has been further improved over the years. One range of ultra-precision foil resistors offers a TCR of 0.14 ppm/°C, tolerance ±0.005%, long-term stability (1 year) 25 ppm, (3 years) 50 ppm (further improved 5-fold by hermetic sealing), stability under load (2000 hours) 0.03%, thermal EMF 0.1 μV/°C, noise −42 dB, voltage coefficient 0.1 ppm/V, inductance 0.08 μH, capacitance 0.5 pF.

The thermal stability of this type of resistor also has to do with the opposing effects of the metal's electrical resistance increasing with temperature, and being reduced by thermal expansion leading to an increase in thickness of the foil, whose other dimensions are constrained by a ceramic substrate.

An ammeter shunt is a special type of current-sensing resistor, having four terminals and a value in milliohms or even micro-ohms. Current-measuring instruments, by themselves, can usually accept only limited currents. To measure high currents, the current passes through the shunt across which the voltage drop is measured and interpreted as current. A typical shunt consists of two solid metal blocks, sometimes brass, mounted on an insulating base. Between the blocks, and soldered or brazed to them, are one or more strips of low temperature coefficient of resistance (TCR) manganin alloy. Large bolts threaded into the blocks make the current connections, while much smaller screws provide volt meter connections. Shunts are rated by full-scale current, and often have a voltage drop of 50 mV at rated current. Such meters are adapted to the shunt full current rating by using an appropriately marked dial face; no change need to be made to the other parts of the meter.

In heavy-duty industrial high-current applications, a grid resistor is a large convection-cooled lattice of stamped metal alloy strips connected in rows between two electrodes. Such industrial grade resistors can be as large as a refrigerator; some designs can handle over 500 amperes of current, with a range of resistances extending lower than 0.04 ohms. They are used in applications such as dynamic braking and load banking for locomotives and trams, neutral grounding for industrial AC distribution, control loads for cranes and heavy equipment, load testing of generators and harmonic filtering for electric substations.

The term "grid resistor" is sometimes used to describe a resistor of any type connected to the control grid of a vacuum tube. This is not a resistor technology; it is an electronic circuit topology.


A resistor may have one or more fixed tapping points so that the resistance can be changed by moving the connecting wires to different terminals. Some wirewound power resistors have a tapping point that can slide along the resistance element, allowing a larger or smaller part of the resistance to be used.

Where continuous adjustment of the resistance value during operation of equipment is required, the sliding resistance tap can be connected to a knob accessible to an operator. Such a device is called a rheostat and has two terminals.

A potentiometer (colloquially, "pot") is a three-terminal resistor with a continuously adjustable tapping point controlled by rotation of a shaft or knob or by a linear slider. The name "potentiometer" comes from its function as an adjustable voltage divider to provide a variable potential at the terminal connected to the tapping point. Volume control in an audio device is a common application of a potentiometer. A typical low power potentiometer "(see drawing)" is constructed of a flat resistance element "(B)" of carbon composition, metal film, or conductive plastic, with a springy phosphor bronze wiper contact "(C)" which moves along the surface. An alternate construction is resistance wire wound on a form, with the wiper sliding axially along the coil. These have lower resolution, since as the wiper moves the resistance changes in steps equal to the resistance of a single turn.

High-resolution multiturn potentiometers are used in precision applications. These have wire-wound resistance elements typically wound on a helical mandrel, with the wiper moving on a helical track as the control is turned, making continuous contact with the wire. Some include a conductive-plastic resistance coating over the wire to improve resolution. These typically offer ten turns of their shafts to cover their full range. They are usually set with dials that include a simple turns counter and a graduated dial, and can typically achieve three digit resolution. Electronic analog computers used them in quantity for setting coefficients, and delayed-sweep oscilloscopes of recent decades included one on their panels.

A resistance decade box or resistor substitution box is a unit containing resistors of many values, with one or more mechanical switches which allow any one of various discrete resistances offered by the box to be dialed in. Usually the resistance is accurate to high precision, ranging from laboratory/calibration grade accuracy of 20 parts per million, to field grade at 1%. Inexpensive boxes with lesser accuracy are also available. All types offer a convenient way of selecting and quickly changing a resistance in laboratory, experimental and development work without needing to attach resistors one by one, or even stock each value. The range of resistance provided, the maximum resolution, and the accuracy characterize the box. For example, one box offers resistances from 0 to 100 megohms, maximum resolution 0.1 ohm, accuracy 0.1%.

There are various devices whose resistance changes with various quantities. The resistance of NTC thermistors exhibit a strong negative temperature coefficient, making them useful for measuring temperatures. Since their resistance can be large until they are allowed to heat up due to the passage of current, they are also commonly used to prevent excessive current surges when equipment is powered on. Similarly, the resistance of a humistor varies with humidity. One sort of photodetector, the photoresistor, has a resistance which varies with illumination.

The strain gauge, invented by Edward E. Simmons and Arthur C. Ruge in 1938, is a type of resistor that changes value with applied strain. A single resistor may be used, or a pair (half bridge), or four resistors connected in a Wheatstone bridge configuration. The strain resistor is bonded with adhesive to an object that is subjected to mechanical strain. With the strain gauge and a filter, amplifier, and analog/digital converter, the strain on an object can be measured.

A related but more recent invention uses a Quantum Tunnelling Composite to sense mechanical stress. It passes a current whose magnitude can vary by a factor of 10 in response to changes in applied pressure.

The value of a resistor can be measured with an ohmmeter, which may be one function of a multimeter. Usually, probes on the ends of test leads connect to the resistor. A simple ohmmeter may apply a voltage from a battery across the unknown resistor (with an internal resistor of a known value in series) producing a current which drives a meter movement. The current, in accordance with Ohm's law, is inversely proportional to the sum of the internal resistance and the resistor being tested, resulting in an analog meter scale which is very non-linear, calibrated from infinity to 0 ohms. A digital multimeter, using active electronics, may instead pass a specified current through the test resistance. The voltage generated across the test resistance in that case is linearly proportional to its resistance, which is measured and displayed. In either case the low-resistance ranges of the meter pass much more current through the test leads than do high-resistance ranges, in order for the voltages present to be at reasonable levels (generally below 10 volts) but still measurable.

Measuring low-value resistors, such as fractional-ohm resistors, with acceptable accuracy requires four-terminal connections. One pair of terminals applies a known, calibrated current to the resistor, while the other pair senses the voltage drop across the resistor. Some laboratory quality ohmmeters, especially milliohmmeters, and even some of the better digital multimeters sense using four input terminals for this purpose, which may be used with special test leads. Each of the two so-called Kelvin clips has a pair of jaws insulated from each other. One side of each clip applies the measuring current, while the other connections are only to sense the voltage drop. The resistance is again calculated using Ohm's Law as the measured voltage divided by the applied current.

Resistor characteristics are quantified and reported using various national standards. In the US, MIL-STD-202 contains the relevant test methods to which other standards refer.

There are various standards specifying properties of resistors for use in equipment:

There are other United States military procurement MIL-R- standards.

The primary standard for resistance, the "mercury ohm" was initially defined in 1884 in as a column of mercury 106.3 cm long and in cross-section, at . Difficulties in precisely measuring the physical constants to replicate this standard result in variations of as much as 30 ppm. From 1900 the mercury ohm was replaced with a precision machined plate of manganin. Since 1990 the international resistance standard has been based on the quantized Hall effect discovered by Klaus von Klitzing, for which he won the Nobel Prize in Physics in 1985.

Resistors of extremely high precision are manufactured for calibration and laboratory use. They may have four terminals, using one pair to carry an operating current and the other pair to measure the voltage drop; this eliminates errors caused by voltage drops across the lead resistances, because no charge flows through voltage sensing leads. It is important in small value resistors (100–0.0001 ohm) where lead resistance is significant or even comparable with respect to resistance standard value.

Axial resistors' cases are usually tan, brown, blue, or green (though other colors are occasionally found as well, such as dark red or dark gray), and display 3–6 colored stripes that indicate resistance (and by extension tolerance), and may be extended to indicate the temperature coefficient and reliability class. The first two stripes represent the first two digits of the resistance in ohms, the third represents a multiplier, and the fourth the tolerance (which if absent, denotes ±20%). For five- and six- striped resistors the third is the third digit, the fourth the multiplier and the fifth is the tolerance; a sixth stripe represents the temperature coefficient. The power rating of the resistor is usually not marked and is deduced from the size.

Surface-mount resistors are marked numerically.

Early 20th century resistors, essentially uninsulated, were dipped in paint to cover their entire body for color-coding. A second color of paint was applied to one end of the element, and a color dot (or band) in the middle provided the third digit. The rule was "body, tip, dot", providing two significant digits for value and the decimal multiplier, in that sequence. Default tolerance was ±20%. Closer-tolerance resistors had silver (±10%) or gold-colored (±5%) paint on the other end.

Early resistors were made in more or less arbitrary round numbers; a series might have 100, 125, 150, 200, 300, etc. Resistors as manufactured are subject to a certain percentage tolerance, and it makes sense to manufacture values that correlate with the tolerance, so that the actual value of a resistor overlaps slightly with its neighbors. Wider spacing leaves gaps; narrower spacing increases manufacturing and inventory costs to provide resistors that are more or less interchangeable.

A logical scheme is to produce resistors in a range of values which increase in a geometric progression, so that each value is greater than its predecessor by a fixed multiplier or percentage, chosen to match the tolerance of the range. For example, for a tolerance of ±20% it makes sense to have each resistor about 1.5 times its predecessor, covering a decade in 6 values. In practice the factor used is 1.4678, giving values of 1.47, 2.15, 3.16, 4.64, 6.81, 10 for the 1–10-decade (a decade is a range increasing by a factor of 10; 0.1–1 and 10–100 are other examples); these are rounded in practice to 1.5, 2.2, 3.3, 4.7, 6.8, 10; followed by 15, 22, 33, … and preceded by … 0.47, 0.68, 1. This scheme has been adopted as the E48 series of the IEC 60063 preferred number values. There are also E12, E24, E48, E96 and E192 series for components of progressively finer resolution, with 12, 24, 96, and 192 different values within each decade. The actual values used are in the IEC 60063 lists of preferred numbers.

A resistor of 100 ohms ±20% would be expected to have a value between 80 and 120 ohms; its E6 neighbors are 68 (54–82) and 150 (120–180) ohms. A sensible spacing, E6 is used for ±20% components; E12 for ±10%; E24 for ±5%; E48 for ±2%, E96 for ±1%; E192 for ±0.5% or better. Resistors are manufactured in values from a few milliohms to about a gigaohm in IEC60063 ranges appropriate for their tolerance. Manufacturers may sort resistors into tolerance-classes based on measurement. Accordingly, a selection of 100 ohms resistors with a tolerance of ±10%, might not lie just around 100 ohm (but no more than 10% off) as one would expect (a bell-curve), but rather be in two groups – either between 5 and 10% too high or 5 to 10% too low (but not closer to 100 ohm than that) because any resistors the factory had measured as being less than 5% off would have been marked and sold as resistors with only ±5% tolerance or better. When designing a circuit, this may become a consideration. This process of sorting parts based on post-production measurement is known as "binning", and can be applied to other components than resistors (such as speed grades for CPUs).

Earlier power wirewound resistors, such as brown vitreous-enameled types, however, were made with a different system of preferred values, such as some of those mentioned in the first sentence of this section.

Surface mounted resistors of larger sizes (metric 1608 and above) are printed with numerical values in a code related to that used on axial resistors. Standard-tolerance surface-mount technology (SMT) resistors are marked with a three-digit code, in which the first two digits are the first two significant digits of the value and the third digit is the power of ten (the number of zeroes). For example:

Resistances less than 100 Ω are written: 100, 220, 470. The final zero represents ten to the power zero, which is 1. For example:

Sometimes these values are marked as 10 or 22 to prevent a mistake.

Resistances less than 10 Ω have 'R' to indicate the position of the decimal point (radix point). For example:

Precision resistors are marked with a four-digit code, in which the first three digits are the significant figures and the fourth is the power of ten. For example:

000 and 0000 sometimes appear as values on surface-mount zero-ohm links, since these have (approximately) zero resistance.

More recent surface-mount resistors are too small, physically, to permit practical markings to be applied.

Format:"<nowiki> [two letters]<space>[resistance value (three digit)]<nospace>[tolerance code(numerical – one digit)]
</nowiki>"

Steps to find out the resistance or capacitance values:


If a resistor is coded:

In amplifying faint signals, it is often necessary to minimize electronic noise, particularly in the first stage of amplification. As a dissipative element, even an ideal resistor naturally produces a randomly fluctuating voltage, or noise, across its terminals. This Johnson–Nyquist noise is a fundamental noise source which depends only upon the temperature and resistance of the resistor, and is predicted by the fluctuation–dissipation theorem. Using a larger value of resistance produces a larger voltage noise, whereas a smaller value of resistance generates more current noise, at a given temperature.

The thermal noise of a practical resistor may also be larger than the theoretical prediction and that increase is typically frequency-dependent. Excess noise of a practical resistor is observed only when current flows through it. This is specified in unit of μV/V/decade – μV of noise per volt applied across the resistor per decade of frequency. The μV/V/decade value is frequently given in dB so that a resistor with a noise index of 0 dB exhibits 1 μV (rms) of excess noise for each volt across the resistor in each frequency decade. Excess noise is thus an example of 1/"f" noise. Thick-film and carbon composition resistors generate more excess noise than other types at low frequencies. Wire-wound and thin-film resistors are often used for their better noise characteristics. Carbon composition resistors can exhibit a noise index of 0 dB while bulk metal foil resistors may have a noise index of −40 dB, usually making the excess noise of metal foil resistors insignificant. Thin film surface mount resistors typically have lower noise and better thermal stability than thick film surface mount resistors. Excess noise is also size-dependent: in general excess noise is reduced as the physical size of a resistor is increased (or multiple resistors are used in parallel), as the independently fluctuating resistances of smaller components tend to average out.

While not an example of "noise" per se, a resistor may act as a thermocouple, producing a small DC voltage differential across it due to the thermoelectric effect if its ends are at different temperatures. This induced DC voltage can degrade the precision of instrumentation amplifiers in particular. Such voltages appear in the junctions of the resistor leads with the circuit board and with the resistor body. Common metal film resistors show such an effect at a magnitude of about 20 µV/°C. Some carbon composition resistors can exhibit thermoelectric offsets as high as 400 µV/°C, whereas specially constructed resistors can reduce this number to 0.05 µV/°C. In applications where the thermoelectric effect may become important, care has to be taken to mount the resistors horizontally to avoid temperature gradients and to mind the air flow over the board.

The failure rate of resistors in a properly designed circuit is low compared to other electronic components such as semiconductors and electrolytic capacitors. Damage to resistors most often occurs due to overheating when the average power delivered to it greatly exceeds its ability to dissipate heat (specified by the resistor's "power rating"). This may be due to a fault external to the circuit, but is frequently caused by the failure of another component (such as a transistor that shorts out) in the circuit connected to the resistor. Operating a resistor too close to its power rating can limit the resistor's lifespan or cause a significant change in its resistance. A safe design generally uses overrated resistors in power applications to avoid this danger.

Low-power thin-film resistors can be damaged by long-term high-voltage stress, even below maximum specified voltage and below maximum power rating. This is often the case for the startup resistors feeding the SMPS integrated circuit.

When overheated, carbon-film resistors may decrease or increase in resistance.
Carbon film and composition resistors can fail (open circuit) if running close to their maximum dissipation. This is also possible but less likely with metal film and wirewound resistors.

There can also be failure of resistors due to mechanical stress and adverse environmental factors including humidity. If not enclosed, wirewound resistors can corrode.

Surface mount resistors have been known to fail due to the ingress of sulfur into the internal makeup of the resistor. This sulfur chemically reacts with the silver layer to produce non-conductive silver sulfide. The resistor's impedance goes to infinity. Sulfur resistant and anti-corrosive resistors are sold into automotive, industrial, and military applications. ASTM B809 is an industry standard that tests a part's susceptibility to sulfur.

An alternative failure mode can be encountered where large value resistors are used (hundreds of kilohms and higher). Resistors are not only specified with a maximum power dissipation, but also for a maximum voltage drop. Exceeding this voltage causes the resistor to degrade slowly reducing in resistance. The voltage dropped across large value resistors can be exceeded before the power dissipation reaches its limiting value. Since the maximum voltage specified for commonly encountered resistors is a few hundred volts, this is a problem only in applications where these voltages are encountered.

Variable resistors can also degrade in a different manner, typically involving poor contact between the wiper and the body of the resistance. This may be due to dirt or corrosion and is typically perceived as "crackling" as the contact resistance fluctuates; this is especially noticed as the device is adjusted. This is similar to crackling caused by poor contact in switches, and like switches, potentiometers are to some extent self-cleaning: running the wiper across the resistance may improve the contact. Potentiometers which are seldom adjusted, especially in dirty or harsh environments, are most likely to develop this problem. When self-cleaning of the contact is insufficient, improvement can usually be obtained through the use of contact cleaner (also known as "tuner cleaner") spray. The crackling noise associated with turning the shaft of a dirty potentiometer in an audio circuit (such as the volume control) is greatly accentuated when an undesired DC voltage is present, often indicating the failure of a DC blocking capacitor in the circuit.




</doc>
<doc id="25755" url="https://en.wikipedia.org/wiki?curid=25755" title="Republicanism">
Republicanism

Republicanism is a representative form of government organization. It is a political ideology centered on citizenship in a state organized as a republic. Historically, it ranges from the rule of a representative minority or oligarchy to popular sovereignty. It has had different definitions and interpretations which vary significantly based on historical context and methodological approach.

Republicanism may also refer to the non-ideological scientific approach to politics and governance. As the republican thinker and second president of the United States John Adams stated in the introduction to his famous "Defense of the Constitution," the "science of politics is the science of social happiness" and a republic is the form of government arrived at when the science of politics is appropriately applied to the creation of a rationally designed government. Rather than being ideological, this approach focuses on applying a scientific methodology to the problems of governance through the rigorous study and application of past experience and experimentation in governance. This is the approach that may best be described to apply to republican thinkers such as Niccolò Machiavelli (as evident in his "Discourses on Livy"), John Adams, and James Madison. 
The word "republic" derives from the Latin noun-phrase "res publica" (thing of the people), which referred to the system of government that emerged in the 6th century BCE following the expulsion of the kings from Rome by Lucius Junius Brutus and Collatinus.
This form of government in the Roman state collapsed in the latter part of the 1st century B.C., giving way to what was a monarchy in form, if not in name. Republics recurred subsequently, with, for example, Renaissance Florence or early modern Britain. The concept of a republic became a powerful force in Britain's North American colonies, where it contributed to the American Revolution. In Europe, it gained enormous influence through the French Revolution and through the First French Republic of 1792–1804.

In Ancient Greece, several philosophers and historians analysed and described elements we now recognize as classical republicanism. Traditionally, the Greek concept of "politeia" was rendered into Latin as res publica. Consequently, political theory until relatively recently often used republic in the general sense of "regime". There is no single written expression or definition from this era that exactly corresponds with a modern understanding of the term "republic" but most of the essential features of the modern definition are present in the works of Plato, Aristotle, and Polybius. These include theories of mixed government and of civic virtue. For example, in "The Republic", Plato places great emphasis on the importance of civic virtue (aiming for the good) together with personal virtue ('just man') on the part of the ideal rulers. Indeed, in Book V, Plato asserts that until rulers have the nature of philosophers (Socrates) or philosophers become the rulers, there can be no civic peace or happiness.

A number of Ancient Greek city-states such as Athens and Sparta have been classified as "classical republics", because they featured extensive participation by the citizens in legislation and political decision-making. Aristotle considered Carthage to have been a republic as it had a political system similar to that of some of the Greek cities, notably Sparta, but avoided some of the defects that affected them.

Both Livy, a Roman historian, and Plutarch, who is noted for his biographies and moral essays, described how Rome had developed its legislation, notably the transition from a "kingdom" to a "republic", by following the example of the Greeks. Some of this history, composed more than 500 years after the events, with scant written sources to rely on, may be fictitious reconstruction.

The Greek historian Polybius, writing in the mid-2nd century BCE, emphasized (in Book 6) the role played by the Roman Republic as an institutional form in the dramatic rise of Rome's hegemony over the Mediterranean. In his writing on the constitution of the Roman Republic, Polybius described the system as being a "mixed" form of government. Specifically, Polybius described the Roman system as a mixture of monarchy, aristocracy, and democracy with the Roman Republic constituted in such a manner that it applied the strengths of each system to offset the weaknesses of the others. In his view, the mixed system of the Roman Republic provided the Romans with a much greater level of domestic tranquility than would have been experienced under another form of government. Furthermore, Polybius argued, the comparative level of domestic tranquility the Romans enjoyed allowed them to conquer the Mediterranean. Polybius exerted a great influence on Cicero as he wrote his politico-philosophical works in the 1st century BCE. In one of these works, "De re publica", Cicero linked the Roman concept of "res publica" to the Greek "politeia".

The modern term "republic", despite its derivation, is not synonymous with the Roman "res publica". Among the several meanings of the term "res publica", it is most often translated "republic" where the Latin expression refers to the Roman state, and its form of government, between the era of the Kings and the era of the Emperors. This Roman Republic would, by a modern understanding of the word, still be defined as a true republic, even if not coinciding entirely. Thus, Enlightenment philosophers saw the Roman Republic as an ideal system because it included features like a systematic separation of powers.

Romans still called their state "Res Publica" in the era of the early emperors because, on the surface, the organization of the state had been preserved by the first emperors without significant alteration. Several offices from the Republican era, held by individuals, were combined under the control of a single person. These changes became permanent, and gradually conferred sovereignty on the Emperor.

Cicero's description of the ideal state, in "De re Publica", does not equate to a modern-day "republic"; it is more like enlightened absolutism. His philosophical works were influential when Enlightenment philosophers such as Voltaire developed their political concepts.

In its classical meaning, a republic was any stable well-governed political community. Both Plato and Aristotle identified three forms of government: democracy, aristocracy, and monarchy. First Plato and Aristotle, and then Polybius and Cicero, held that the ideal republic is a mixture of these three forms of government. The writers of the Renaissance embraced this notion.

Cicero expressed reservations concerning the republican form of government. While in his "theoretical" works he defended monarchy, or at least a mixed monarchy/oligarchy, in his own political life, he generally opposed men, like Julius Caesar, Mark Antony, and Octavian, who were trying to realise such ideals. Eventually, that opposition led to his death and Cicero can be seen as a victim of his own Republican ideals.

Tacitus, a contemporary of Plutarch, was not concerned with whether a form of government could be analyzed as a "republic" or a "monarchy". He analyzed how the powers accumulated by the early Julio-Claudian dynasty were all given by a State that was still notionally a republic. Nor was the Roman Republic "forced" to give away these powers: it did so freely and reasonably, certainly in Augustus' case, because of his many services to the state, freeing it from civil wars and disorder.

Tacitus was one of the first to ask whether such powers were given to the head of state because the citizens wanted to give them, or whether they were given for other reasons (for example, because one had a deified ancestor). The latter case led more easily to abuses of power. In Tacitus' opinion, the trend away from a true republic was "irreversible" only when Tiberius established power, shortly after Augustus' death in 14 CE (much later than most historians place the start of the Imperial form of government in Rome). By this time, too many principles defining some powers as "untouchable" had been implemented.

In Europe, republicanism was revived in the late Middle Ages when a number of states, which arose from medieval communes, embraced a republican system of government. These were generally small but wealthy trading states in which the merchant class had risen to prominence. Haakonssen notes that by the Renaissance, Europe was divided, such that those states controlled by a landed elite were monarchies, and those controlled by a commercial elite were republics. The latter included the Italian city-states of Florence, Genoa, and Venice and members of the Hanseatic League. One notable exception was Dithmarschen, a group of largely autonomous villages, who confederated in a peasants' republic. Building upon concepts of medieval feudalism, Renaissance scholars used the ideas of the ancient world to advance their view of an ideal government. Thus the republicanism developed during the Renaissance is known as 'classical republicanism' because it relied on classical models. This terminology was developed by Zera Fink in the 1960s, but some modern scholars, such as Brugger, consider it confuses the "classical republic" with the system of government used in the ancient world. 'Early modern republicanism' has been proposed as an alternative term. It is also sometimes called civic humanism. Beyond simply a non-monarchy, early modern thinkers conceived of an "ideal" republic, in which mixed government was an important element, and the notion that virtue and the common good were central to good government. Republicanism also developed its own distinct view of liberty.
Renaissance authors who spoke highly of republics were rarely critical of monarchies. While Niccolò Machiavelli's "Discourses on Livy" is the period's key work on republics, he also wrote the treatise "The Prince," which is better remembered and more widely read, on how best to run a monarchy. The early modern writers did not see the republican model as universally applicable; most thought that it could be successful only in very small and highly urbanized city-states. Jean Bodin in "Six Books of the Commonwealth" (1576) identified monarchy with republic.

Classical writers like Tacitus, and Renaissance writers like Machiavelli tried to avoid an outspoken preference for one government system or another. Enlightenment philosophers, on the other hand, expressed a clear opinion. Thomas More, writing before the Age of Enlightenment, was too outspoken for the reigning king's taste, even though he coded his political preferences in a utopian allegory.

In England a type of republicanism evolved that was not wholly opposed to monarchy; thinkers such as Thomas More and Sir Thomas Smith saw a monarchy, firmly constrained by law, as compatible with republicanism.

Anti-monarchism became more strident in the Dutch Republic during and after the Eighty Years' War, which began in 1568. This anti-monarchism was more propaganda than a political philosophy; most of the anti-monarchist works appeared in the form of widely distributed pamphlets. This evolved into a systematic critique of monarchy, written by men such as the brothers Johan and Peter de la Court. They saw all monarchies as illegitimate tyrannies that were inherently corrupt. These authors were more concerned with preventing the position of Stadholder from evolving into a monarchy, than with attacking their former rulers. Dutch republicanism also influenced on French Huguenots during the Wars of Religion. In the other states of early modern Europe republicanism was more moderate.

In the Polish–Lithuanian Commonwealth, republicanism was the influential ideology. After the establishment of the Commonwealth of Two Nations, republicans supported the status quo, of having a very weak monarch, and opposed those who thought a stronger monarchy was needed. These mostly Polish republicans, such as Łukasz Górnicki, Andrzej Wolan, and Stanisław Konarski, were well read in classical and Renaissance texts and firmly believed that their state was a republic on the Roman model, and started to call their state the Rzeczpospolita. Atypically, Polish–Lithuanian republicanism was not the ideology of the commercial class, but rather of the landed nobility, which would lose power if the monarchy were expanded. This resulted in an oligarchy of the great landed magnates.

The first of the Enlightenment republics established in Europe during the eighteenth century occurred in the small Mediterranean island of Corsica. Although perhaps an unlikely place to act as a laboratory for such political experiments, Corsica combined a number of factors that made it unique: a tradition of village democracy; varied cultural influences from the Italian city-states, Spanish empire and Kingdom of France which left it open to the ideas of the Italian Renaissance, Spanish humanism and French Enlightenment; and a geo-political position between these three competing powers which led to frequent power vacuums in which new regimes could be set up, testing out the fashionable new ideas of the age.

From the 1720s the island had been experiencing a series of short-lived but ongoing rebellions against its current sovereign, the Italian city-state of Genoa. During the initial period (1729–36) these merely sought to restore the control of the Spanish Empire; when this proved impossible, an independent Kingdom of Corsica (1736–40) was proclaimed, following the Enlightenment ideal of a written constitutional monarchy. But the perception grew that the monarchy had colluded with the invading power, a more radical group of reformers led by the Pascal Paoli pushed for political overhaul, in the form of a constitutional and parliamentary republic inspired by the popular ideas of the Enlightenment.

Its governing philosophy was both inspired by the prominent thinkers of the day, notably the French philosophers Montesquieu and Voltaire and the Swiss theorist Jean-Jacques Rousseau. Not only did it include a permanent national parliament with fixed-term legislatures and regular elections, but, more radically for the time, it introduced universal male suffrage, and it is thought to be the first constitution in the world to grant women the right to vote female suffrage may also have existed. It also extended Enlightened principles to other spheres, including administrative reform, the foundation of a national university at Corte, and the establishment of a popular standing army.

The Corsican Republic lasted for fifteen years, from 1755 to 1769, eventually falling to a combination of Genoese and French forces and was incorporated as a province of the Kingdom of France. But the episode resonated across Europe as an early example of Enlightened constitutional republicanism, with many of the most prominent political commentators of the day recognising it to be an experiment in a new type of popular and democratic government. Its influence was particularly notable among the French Enlightenment philosophers: Rousseau's famous work On the Social Contract (1762: chapter 10, book II) declared, in its discussion on the conditions necessary for a functional popular sovereignty, that ""There is still one European country capable of making its own laws: the island of Corsica. valour and persistency with which that brave people has regained and defended its liberty well deserves that some wise man should teach it how to preserve what it has won. I have a feeling that some day that little island will astonish Europe"."; indeed Rousseau volunteered to do precisely that, offering a draft constitution for Paoli'se use. Similarly, Voltaire affirmed in his "Précis du siècle de Louis XV" (1769: chapter LX) that ""Bravery may be found in many places, but such bravery only among free peoples"". But the influence of the Corsican Republic as an example of a sovereign people fighting for liberty and enshrining this constitutionally in the form of an Enlightened republic was even greater among the Radicals of Great Britain and North America, where it was popularised via An Account of Corsica, by the Scottish essayist James Boswell. The Corsican Republic went on to influence the American revolutionaries ten years later: the Sons of Liberty, initiators of the American Revolution, would declare Pascal Paoli to be a direct inspiration for their own struggle against despotism; the son of Ebenezer Mackintosh was named Pascal Paoli Mackintosh in his honour, and no fewer than five American counties are named Paoli for the same reason.

Oliver Cromwell set up a republic called the Commonwealth of England (1649–1660) which he ruled after the overthrow of King Charles I. James Harrington was then a leading philosopher of republicanism. John Milton was another important Republican thinker at this time, expressing his views in political tracts as well as through poetry and prose. In his epic poem "Paradise Lost", for instance, Milton uses Satan's fall to suggest that unfit monarchs should be brought to justice, and that such issues extend beyond the constraints of one nation. As Christopher N. Warren argues, Milton offers “a language to critique imperialism, to question the legitimacy of dictators, to defend free international discourse, to fight unjust property relations, and to forge new political bonds across national lines.” This form of international Miltonic republicanism has been influential on later thinkers including 19th-century radicals Karl Marx and Friedrich Engels, according to Warren and other historians.

The collapse of the Commonwealth of England in 1660 and the restoration of the monarchy under Charles II discredited republicanism among England's ruling circles. Nevertheless, they welcomed the liberalism, and emphasis on rights, of John Locke, which played a major role in the Glorious Revolution of 1688. Even so, republicanism flourished in the "country" party of the early 18th century (commonwealthmen), which denounced the corruption of the "court" party, producing a political theory that heavily influenced the American colonists. In general, the English ruling classes of the 18th century vehemently opposed republicanism, typified by the attacks on John Wilkes, and especially on the American Revolution and the French Revolution.

French and Swiss Enlightenment thinkers, such as Baron Charles de Montesquieu and later Jean-Jacques Rousseau, expanded upon and altered the ideas of what an ideal republic should be: some of their new ideas were scarcely traceable to antiquity or the Renaissance thinkers. Concepts they contributed, or heavily elaborated, were social contract, positive law, and mixed government. They also borrowed from, and distinguished republicanism from, the ideas of liberalism that were developing at the same time.

Liberalism and republicanism were frequently conflated during this period, because they both opposed absolute monarchy. Modern scholars see them as two distinct streams that both contributed to the democratic ideals of the modern world. An important distinction is that, while republicanism stressed the importance of civic virtue and the common good, liberalism was based on economics and individualism. It is clearest in the matter of private property, which, according to some, can be maintained only under the protection of established positive law.

Jules Ferry, Prime Minister of France from 1880 to 1885, followed both these schools of thought. He eventually enacted the Ferry Laws, which he intended to overturn the Falloux Laws by embracing the anti-clerical thinking of the "Philosophs". These laws ended the Catholic Church's involvement in many government institutions in late 19th-century France, including schools.

In recent years a debate has developed over the role of republicanism in the American Revolution and in the British radicalism of the 18th century. For many decades the consensus was that liberalism, especially that of John Locke, was paramount and that republicanism had a distinctly secondary role.

The new interpretations were pioneered by J.G.A. Pocock, who argued in "The Machiavellian Moment" (1975) that, at least in the early 18th century, republican ideas were just as important as liberal ones. Pocock's view is now widely accepted. Bernard Bailyn and Gordon Wood pioneered the argument that the American founding fathers were more influenced by republicanism than they were by liberalism. Cornell University professor Isaac Kramnick, on the other hand, argues that Americans have always been highly individualistic and therefore Lockean. Joyce Appleby has argued similarly for the Lockean influence on America.

In the decades before the American Revolution (1776), the intellectual and political leaders of the colonies studied history intently, looking for models of good government. They especially followed the development of republican ideas in England. Pocock explained the intellectual sources in America:

The Whig canon and the neo-Harringtonians, John Milton, James Harrington and Sidney, Trenchard, Gordon and Bolingbroke, together with the Greek, Roman, and Renaissance masters of the tradition as far as Montesquieu, formed the authoritative literature of this culture; and its values and concepts were those with which we have grown familiar: a civic and patriot ideal in which the personality was founded in property, perfected in citizenship but perpetually threatened by corruption; government figuring paradoxically as the principal source of corruption and operating through such means as patronage, faction, standing armies (opposed to the ideal of the militia), established churches (opposed to the Puritan and deist modes of American religion) and the promotion of a monied interest – though the formulation of this last concept was somewhat hindered by the keen desire for readily available paper credit common in colonies of settlement. A neoclassical politics provided both the ethos of the elites and the rhetoric of the upwardly mobile, and accounts for the singular cultural and intellectual homogeneity of the Founding Fathers and their generation.

The commitment of most Americans to these republican values made the American Revolution inevitable. Britain was increasingly seen as corrupt and hostile to republicanism, and as a threat to the established liberties the Americans enjoyed.

Leopold von Ranke in 1848 claimed that American republicanism played a crucial role in the development of European liberalism:

By abandoning English constitutionalism and creating a new republic based on the rights of the individual, the North Americans introduced a new force in the world. Ideas spread most rapidly when they have found adequate concrete expression. Thus republicanism entered our Romanic/Germanic world... Up to this point, the conviction had prevailed in Europe that monarchy best served the interests of the nation. Now the idea spread that the nation should govern itself. But only after a state had actually been formed on the basis of the theory of representation did the full significance of this idea become clear. All later revolutionary movements have this same goal... This was the complete reversal of a principle. Until then, a king who ruled by the grace of God had been the center around which everything turned. Now the idea emerged that power should come from below... These two principles are like two opposite poles, and it is the conflict between them that determines the course of the modern world. In Europe the conflict between them had not yet taken on concrete form; with the French Revolution it did.

Republicanism, especially that of Rousseau, played a central role in the French Revolution and foreshadowed modern republicanism. The revolutionaries, after overthrowing the French monarchy in the 1790s, began by setting up a republic; Napoleon converted it into an Empire with a new aristocracy. In the 1830s Belgium adopted some of the innovations of the progressive political philosophers of the Enlightenment.

"Républicanisme" is a French version of modern republicanism. It is a form of social contract, deduced from Jean-Jacques Rousseau's idea of a general will. Ideally, each citizen is engaged in a direct relationship with the state, removing the need for identity politics based on local, religious, or racial identification.

"Républicanisme", in theory, makes anti-discrimination laws unnecessary, but some critics argue that colour-blind laws serve to perpetuate discrimination.

Inspired by the American and French Revolutions, the Society of United Irishmen was founded in 1791 in Belfast and Dublin. The inaugural meeting of the United Irishmen in Belfast on 18 October 1791 approved a declaration of the society's objectives. It identified the central grievance that Ireland had no national government: "...we are ruled by Englishmen, and the servants of Englishmen, whose object is the interest of another country, whose instrument is corruption, and whose strength is the weakness of Ireland..." They adopted three central positions: (i) to seek out a cordial union among all the people of Ireland, to maintain that balance essential to preserve liberties and extend commerce; (ii) that the sole constitutional mode by which English influence can be opposed, is by a complete and radical reform of the representation of the people in Parliament; (iii) that no reform is practicable or efficacious, or just which shall not include Irishmen of every religious persuasion. The declaration, then, urged constitutional reform, union among Irish people and the removal of all religious disqualifications.

The event that above all influenced men's thoughts at that time was the French Revolution. Public interest, already strongly aroused, was brought to a pitch by the publication in 1790 of Edmund Burke's "Reflections on the Revolution in France", and Thomas Paine's response, "Rights of Man", in February 1791. Theobald Wolfe Tone wrote later that, "This controversy, and the gigantic event which gave rise to it, changed in an instant the politics of Ireland." Paine himself was aware of this commenting on sales of Part I of "Rights of Man" in November 1791, only eight months after publication of the first edition, he informed a friend that in England "almost sixteen thousand has gone off – and in Ireland above forty thousand". Paine my have been inclined to talk up sales of his works but what is striking in this context is that Paine believed that Irish sales were so far ahead of English ones before Part II had appeared. On 5 June 1792, Thomas Paine, author of the "Rights of Man" was proposed for honorary membership of the Dublin Society of the United Irishmen.

The fall of the Bastille was to be celebrated in Belfast on 14 July 1791 by a Volunteer meeting. At the request of Thomas Russell, Tone drafted suitable resolutions for the occasion, including one favouring the inclusion of Catholics in any reforms. In a covering letter to Russell, Tone wrote, "I have not said one word that looks like a wish for separation, though I give it to you and your friends as my most decided opinion that such an event would be a regeneration of their country". By 1795, Tone's Republicanism and that of the society had openly crystallized when he tells us: "I remember particularly two days thae we passed on Cave Hill. On the first Russell, Neilson, Simms, McCracken and one or two more of us, on the summit of McArt's fort, took a solemn obligation...never to desist in our efforts until we had subverted the authority of England over our country and asserted her independence."

The culmination was an uprising against British rule in Ireland lasting from May to September 1798 – the Irish Rebellion of 1798 – with military support from revolutionary France in August and again October 1798. After the failure of the rising of 1798 the United Irishman, John Daly Burk, an émigré in the United States in his " The History of the Late War in Ireland" written in 1799, was most emphatic in its identification of the Irish, French and American causes.

During the Enlightenment, anti-monarchism extended beyond the civic humanism of the Renaissance. Classical republicanism, still supported by philosophers such as Rousseau and Montesquieu, was only one of several theories seeking to limit the power of monarchies rather than directly opposing them. New forms of anti-monarchism, such as liberalism and later socialism, quickly overtook classical republicanism as the leading ideologies. Republicanism gained support, and monarchies were challenged throughout Europe.

The French version of Republicanism after 1870 was called "Radicalism"; it became the Radical Party a major political party. In Western Europe, there were similar smaller "radical" parties. They all supported a constitutional republic and universal suffrage, while European "liberals" were at the time in favor of constitutional monarchy and census suffrage. Most radical parties later favored economic liberalism and capitalism. This distinction between radicalism and liberalism had not totally disappeared in the 20th century, although many radicals simply joined liberal parties. For example, the Radical Party of the Left in France or the (originally Italian) Transnational Radical Party, which still exist, focus more on republicanism than on simple liberalism.

Liberalism, was represented in France by the Orleanists who rallied to the Third Republic only in the late 19th century, after the comte de Chambord's 1883 death and the 1891 papal encyclical "Rerum novarum".

But the early Republican, Radical and Radical-Socialist Party in France, and Chartism in Britain, were closer to republicanism. Radicalism remained close to republicanism in the 20th century, at least in France, where they governed several times with other parties (participating in both the Cartel des Gauches coalitions as well as the Popular Front).

Discredited after the Second World War, French radicals split into a left-wing party – the Radical Party of the Left, an associate of the Socialist Party – and the Radical Party "valoisien", an associate party of the conservative Union for a Popular Movement (UMP) and its Gaullist predecessors. Italian radicals also maintained close links with republicanism, as well as with socialism, with the "Partito radicale" founded in 1955, which became the Transnational Radical Party in 1989.

Increasingly, after the fall of communism in 1989 and the collapse of the Marxist interpretation of the French Revolution, France increasingly turned to Republicanism to define its national identity. Charles de Gaulle, presenting himself as the military savior of France in the 1940s, and the political savior in the 1950s, refashioned the meaning of Republicanism. Both left and right enshrined him in the Republican pantheon.

Republicanism became the dominant political value of Americans during and after the American Revolution. The "Founding Fathers" were strong advocates of republican values, especially Thomas Jefferson, Samuel Adams, Patrick Henry, Thomas Paine, Benjamin Franklin, John Adams, James Madison and Alexander Hamilton. However, in 1854, social movements started to harness values of abolitionism and free labor. These burgeoning radical traditions in America became epitomized in the early formation of the Republican Party, known as "red republicanism." The efforts were primarily led by political leaders such as Alvan E. Bovay, Thaddeus Stevens, and Abraham Lincoln.

In some countries of the British Empire, later the Commonwealth of Nations, republicanism has taken a variety of forms.

In Barbados, the government gave the promise of a referendum on becoming a republic in August 2008, but it was postponed due to the change of government in the 2008 election.

In South Africa, republicanism in the 1960s was identified with the supporters of apartheid, who resented British interference in their treatment of the country's black population.

In Australia, the debate between republicans and monarchists is still active, and republicanism draws support from across the political spectrum. Former Prime Minister Malcolm Turnbull was a leading proponent of an Australian republic prior to joining the centre-right Liberal Party, and led the pro-republic campaign during the failed 1999 Australian republic referendum. After becoming Prime Minister in 2015, he confirmed he still supports a republic, but stated that the issue should wait until after the reign of Queen Elizabeth II. The centre-left Labor Party officially supports the abolition of the monarchy and another referendum on the issue.

On 22 March 2015, Prime Minister Freundel Stuart announced that Barbados will move towards a republican form of government "in the very near future".

Andrew Holness, the current Prime Minister of Jamaica, has announced that his government intends to begin the process of transitioning to a republic.

In New Zealand, there is also a republican movement.

Republican groups are also active in the United Kingdom. The major organisation campaigning for a republic in the United Kingdom is 'Republic'.

The Netherlands have known two republican periods: the Dutch Republic (1581–1795) that gained independence from the Spanish Empire during the Eighty Years' War, followed by the Batavian Republic (1795–1806) that after conquest by the French First Republic had been established as a Sister Republic. After Napoleon crowned himself Emperor of the French, he made his brother Louis Bonaparte King of Holland (1806–1810), then annexed the Netherlands into the French First Empire (1810–1813) until he was defeated at the Battle of Leipzig. Thereafter the Sovereign Principality of the United Netherlands (1813–1815) was established, granting the Orange-Nassau family, who during the Dutch Republic had only been stadtholders, a princely title over the Netherlands, and soon William Frederick even crowned himself King of the Netherlands. His rather autocratic tendencies in spite of the principles of constitutional monarchy met increasing resistance from Parliament and the population, which eventually limited the monarchy's power and democratised the government, most notably through the Constitutional Reform of 1848. Since the late 19th century, republicanism has had various degrees of support in society, which the royal house generally dealt with by gradually letting go of its formal influence in politics and taking on a more ceremonial and symbolic role. Nowadays, popularity of the monarchy is high, but there is a significant republican minority that strives to abolish the monarchy altogether.

In the period around and after the dissolution of the union between Norway and Sweden in 1905, an opposition to the monarchy grew in Norway, and republican movements and thoughts continues to exist to this day.

In Sweden, a major promoter of republicanism is the Swedish Republican Association, which advocates for a democratic ending to the Monarchy of Sweden.

There is a renewed interest in republicanism in Spain after two earlier attempts: the First Spanish Republic (1873–1874) and the Second Spanish Republic (1931–1939). Movements such as "", Citizens for the Republic in Spanish, have emerged, and parties like United Left (Spain) and the Republican Left of Catalonia increasingly refer to republicanism. In a survey conducted in 2007 reported that 69% of the population prefer the monarchy to continue, compared with 22% opting for a Republic. In a 2008 survey, 58% of Spanish citizens were indifferent, 16% favored a republic, 16% were monarchists, and 7% claimed they were "Juancarlistas" (supporters of continued monarchy under King Juan Carlos I, without a common position for the fate of the monarchy after his death). In the last years republicanism has been rising, especially among the young people.

Neorepublicanism is the effort by current scholars to draw on a classical republican tradition in the development of an attractive public philosophy intended for contemporary purposes. With traditional socialism virtually defunct, it emerges as an alternative postsocialist critique of market society from the left.

Prominent theorists in this movement are Philip Pettit and Cass Sunstein, who have each written several works defining republicanism and how it differs from liberalism. Michael Sandel, a late convert to republicanism from communitarianism, advocates replacing or supplementing liberalism with republicanism, as outlined in his "Democracy's Discontent: America in Search of a Public Philosophy."

Contemporary work from a neorepublican include jurist K. Sabeel Rahman's book "Democracy Against Domination", which seeks to create a neorepublican framework for economic regulation grounded in the thought of Louis Brandeis and John Dewey and popular control, in contrast to both New Deal-style managerialism and neoliberal deregulation. Philosopher Elizabeth Anderson's "Private Government" traces the history of republican critiques of private power, arguing that the classical free market policies of the 18th and 19th centuries intended to help workers only lead to their domination by employers. In "From Slavery to the Cooperative Commonwealth," political scientist Alex Gourevitch examines a strain of late 19th century American republicanism known as labor republicanism that was the producerist labor union The Knights of Labor, and how republican concepts were used in service of workers rights, but also with a strong critique of the role of that union in supporting the Chinese Exclusion Act.

In the late 18th century there was convergence of democracy and republicanism. Republicanism is a system that replaces or accompanies inherited rule. There is an emphasis on liberty, and a rejection of corruption. It strongly influenced the American Revolution and the French Revolution in the 1770s and 1790s, respectively. Republicans, in these two examples, tended to reject inherited elites and aristocracies, but left open two questions: whether a republic, to restrain unchecked majority rule, should have an unelected upper chamber—perhaps with members appointed as meritorious experts—and whether it should have a constitutional monarch.

Though conceptually separate from democracy, republicanism included the key principles of rule by consent of the governed and sovereignty of the people. In effect, republicanism held that kings and aristocracies were not the real rulers, but rather the whole people were. Exactly "how" the people were to rule was an issue of democracy: republicanism itself did not specify a means. In the United States, the solution was the creation of political parties that reflected the votes of the people and controlled the government (see Republicanism in the United States). Many exponents of republicanism, such as Benjamin Franklin, Thomas Paine, and Thomas Jefferson were strong promoters of representative democracy. Other supporters of republicanism, such as John Adams and Alexander Hamilton, were more distrustful of majority rule and sought a government with more power for elites. There were similar debates in many other democratizing nations.

In contemporary usage, the term "democracy" refers to a government chosen by the people, whether it is direct or representative. Today the term "republic" usually refers to representative democracy with an elected head of state, such as a president, who serves for a limited term; in contrast to states with a hereditary monarch as a head of state, even if these states also are representative democracies, with an elected or appointed head of government such as a prime minister.

The Founding Fathers of the United States rarely praised and often criticized democracy, which in their time tended to specifically mean direct democracy and which they equated with mob rule; James Madison argued that what distinguished a "democracy" from a "republic" was that the former became weaker as it got larger and suffered more violently from the effects of faction, whereas a republic could get stronger as it got larger and combatted faction by its very structure. What was critical to American values, John Adams insisted, was that the government should be "bound by fixed laws, which the people have a voice in making, and a right to defend."

Some countries (such as the United Kingdom, the Netherlands, Belgium, Luxembourg, the Scandinavian countries, and Japan) turned powerful monarchs into constitutional ones with limited, or eventually merely symbolic, powers. Often the monarchy was abolished along with the aristocratic system, whether or not they were replaced with democratic institutions (such as in France, China, Iran, Russia, Germany, Austria, Hungary, Italy, Greece, Turkey and Egypt). In Australia, New Zealand, Canada, Papua New Guinea, and some other countries the monarch, or its representative, is given supreme executive power, but by convention acts only on the advice of his or her ministers. Many nations had elite upper houses of legislatures, the members of which often had lifetime tenure, but eventually these houses lost much power (as the UK House of Lords), or else became elective and remained powerful.







</doc>
<doc id="25756" url="https://en.wikipedia.org/wiki?curid=25756" title="Repetitive strain injury">
Repetitive strain injury

A repetitive strain injury (RSI) is an injury to part of the musculoskeletal or nervous system which is caused by repetitive use, vibrations, compression or long periods in a fixed position. Other common names include repetitive stress disorders, cumulative trauma disorders (CTDs), and overuse syndrome.

Some examples of symptoms experienced by patients with RSI are aching, pulsing pain, tingling and extremity weakness, initially presenting with intermittent discomfort and then with a higher degree of frequency.

Repetitive strain injury (RSI) and associative trauma orders are umbrella terms used to refer to several discrete conditions that can be associated with repetitive tasks, forceful exertions, vibrations, mechanical compression, sustained or awkward positions, or repetitive eccentric contractions. The exact terminology is controversial, but the terms now used by the United States Department of Labor and the National Institute of Occupational Safety and Health (NIOSH) are musculoskeletal disorders (MSDs) and work-related muscular skeletal disorders (WMDs).

Examples of conditions that may sometimes be attributed to such causes include tendinosis (or less often tendinitis), carpal tunnel syndrome, cubital tunnel syndrome, De Quervain syndrome, thoracic outlet syndrome, intersection syndrome, golfer's elbow (medial epicondylitis), tennis elbow (lateral epicondylitis), trigger finger (so-called stenosing tenosynovitis), radial tunnel syndrome, ulnar tunnel syndrome, and focal dystonia.

A general worldwide increase since the 1970s in RSIs of the arms, hands, neck, and shoulder has been attributed to the widespread use in the workplace of keyboard entry devices, such as typewriters and computers, which require long periods of repetitive motions in a fixed posture. Extreme temperatures have also been reported as risk factor for RSI.

Workers in certain fields are at risk of repetitive strains. Most occupational injuries are musculoskeletal disorders, and many of these are caused by cumulative trauma rather than a single event. Miners and poultry workers, for example, must make repeated motions which can cause tendon, muscular, and skeletal injuries. Jobs that involve repeated motion patterns or prolonged posture within a work cycle, or both, may be repetitive. Young athletes are predisposed to RSIs due to an underdeveloped musculoskeletal system. 

Factors such as personality differences to work-place organization problems. Certain workers may negatively perceive their work organization due to excessive work rate, long work hours, limited job control, and low social support. Previous studies shown elevated urinary catecholamines (stress-related chemicals) in workers with RSI. Pain related to RSI may evolve into chronic pain syndrome particularly for workers who do not have supports from co-workers and supervisors.

Age and gender are important risk factors for RSIs. The risk of RSI increases with age. Women are more likely affected than men because of their smaller frame, lower muscle mass and strength, and due to endocrine influences. In addition, lifestyle choices such as smoking and alcohol consumption are recognizable risk factors for RSI. Recent scientific findings indicate that obesity and diabetes may predispose an individual to RSIs by creating a chronic low grade inflammatory response that prevents the body from effectively healing damaged tissues. 

RSIs are assessed using a number of objective clinical measures. These include effort-based tests such as grip and pinch strength, diagnostic tests such as Finkelstein's test for De Quervain's tendinitis, Phalen's Contortion, Tinel's Percussion for carpal tunnel syndrome, and nerve conduction velocity tests that show nerve compression in the wrist. Various imaging techniques can also be used to show nerve compression such as x-ray for the wrist, and MRI for the thoracic outlet and cervico-brachial areas. Utilization of routine imaging  is useful in early detection and treatment of overuse injuries in at risk populations, which is important in preventing long term adverse effects. 

There are no quick fixes for RSI. Early diagnosis is critical to limiting damage. RICE is used as the first treatment for many muscle strains, ligament sprains, or other bruises and injuries. RICE stands for Rest, Ice, Compression, and Elevation. RICE is used immediately after an injury happens and for the first 24 to 48 hours after the injury. These modalities can help reduce the swelling and pain. Commonly prescribed treatments for early-stage RSIs include analgesics, myofeedback, biofeedback, physical therapy, relaxation, and ultrasound therapy. Low-grade RSIs can sometimes resolve themselves if treatments begin shortly after the onset of symptoms. However, some RSIs may require more aggressive intervention including surgery and can persist for years.

Although there are no "quick fixes" for RSI, there are effective approaches to its treatment and prevention. One is that of ergonomics, the changing of one's environment (especially workplace equipment) to minimize repetitive strain. Another is specific massage techniques such as trigger point therapy and related techniques such as the Alexander Technique. Licensed massage therapists specializing in RSI, as well as physical therapists and chiropractors, generally provide hands-on therapy, but also expect that the patient supplement and reinforce the office-visit therapy sessions with daily (or several times daily) exercises, self-massage, and stretching as prescribed by the practitioner.

General exercise has been shown to decrease the risk of developing RSI. Doctors sometimes recommend that RSI sufferers engage in specific strengthening exercises, for example to improve sitting posture, reduce excessive kyphosis, and potentially thoracic outlet syndrome. Modifications of posture and arm use (human factors and ergonomics) are often recommended.

Although seemingly a modern phenomenon, RSIs have long been documented in the medical literature. In 1700, the Italian physician Bernardino Ramazzini first described RSI in more than 20 categories of industrial workers in Italy, including musicians and clerks. Carpal tunnel syndrome was first identified by the British surgeon James Paget in 1854. The April 1875 issue of "The Graphic" describes "telegraphic paralysis."

The Swiss surgeon Fritz de Quervain first identified De Quervain’s tendinitis in Swiss factory workers in 1895. The French neurologist Jules Tinel (1879–1952) developed his percussion test for compression of the median nerve in 1900. The American surgeon George Phalen improved the understanding of the aetiology of carpal tunnel syndrome with his clinical experience of several hundred patients during the 1950s and 1960s.

Specific sources of discomfort have been popularly referred to by terms such as Blackberry thumb, iPod finger, Wii elbow, mouse arm disease, PlayStation thumb, Rubik's wrist or "cuber's thumb", stylus finger, raver's wrist, and Emacs pinky, among others.




</doc>
<doc id="25757" url="https://en.wikipedia.org/wiki?curid=25757" title="Robertson Davies">
Robertson Davies

William Robertson Davies, (28 August 1913 – 2 December 1995) was a Canadian novelist, playwright, critic, journalist, and professor. He was one of Canada's best known and most popular authors and one of its most distinguished "men of letters", an unfashionable term Davies gladly accepted for himself. Davies was the founding Master of Massey College, a graduate residential college associated with the University of Toronto.

Davies was born in Thamesville, Ontario, the third son of William Rupert Davies and Florence Sheppard McKay. Growing up, Davies was surrounded by books and lively language. His father, senator of Kingston, Ontario, from 1942 to his death in 1967, was a newspaperman from Welshpool, Wales, and both parents were voracious readers. He followed in their footsteps and read everything he could. He also participated in theatrical productions as a child, where he developed a lifelong interest in drama.

He spent his formative years in Renfrew, Ontario (Blairlogie in his novel "What's Bred in the Bone"); many of the novel's characters are named after families he knew there. He attended Upper Canada College in Toronto from 1926 to 1932 and while there attended services at the Church of St. Mary Magdalene. He would later leave the Presbyterian Church and join Anglicanism over objections to Calvinist theology. Davies later used his experience of the ceremonial of High Mass at St. Mary Magdalene's in his novel "The Cunning Man".

After Upper Canada College, he studied at Queen's University at Kingston, Ontario, from 1932 until 1935. At Queen's, he was enrolled as a special student not working towards a degree, and wrote for the student paper, "The Queen's Journal". He left Canada to study at Balliol College, Oxford, where he received a BLitt degree in 1938. The next year he published his thesis, "Shakespeare's Boy Actors", and embarked on an acting career outside London. In 1940, he played small roles and did literary work for the director at the Old Vic Repertory Company in London. Also that year, Davies married Australian Brenda Mathews, whom he had met at Oxford, and who was then working as stage manager for the theatre. They spent their honeymoon in the Welsh countryside at Fronfraith Hall, Abermule, Montgomery, the family house owned by Rupert Davies.

Davies's early life provided him with themes and material to which he would often return in his later work, including the theme of Canadians returning to England to finish their education, and the theatre.

Davies and his new bride returned to Canada in 1940, where he took the position of literary editor at "Saturday Night" magazine. Two years later, he became editor of the "Peterborough Examiner" in the small city of Peterborough, Ontario, northeast of Toronto. Again he was able to mine his experiences here for many of the characters and situations which later appeared in his novels and plays.

Davies, along with family members William Rupert Davies and Arthur Davies, purchased several media outlets. Along with the "Examiner" newspaper, they owned the "Kingston Whig-Standard" newspaper, CHEX-AM, CKWS-AM, CHEX-TV, and CKWS-TV.

During his tenure as editor of the "Examiner", which lasted from 1942 to 1955 (he subsequently served as publisher from 1955 to 1965), Davies published a total of 18 books, produced several of his own plays, and wrote articles for various journals. Davies set out his theory of acting in his "Shakespeare for Young Players" (1947), and then put theory into practice when he wrote "Eros at Breakfast", a one-act play which was named best Canadian play of the year by the 1948 Dominion Drama Festival.

"Eros at Breakfast" was followed by "Fortune, My Foe" in 1949 and "At My Heart's Core", a three-act play, in 1950. Meanwhile, Davies was writing humorous essays in the "Examiner" under the pseudonym Samuel Marchbanks. Some of these were collected and published in "The Diary of Samuel Marchbanks" (1947), "The Table Talk of Samuel Marchbanks" (1949), and later in "Samuel Marchbanks' Almanack" (1967). An omnibus edition of the three Marchbanks books, with new notes by the author, was published under the title "The Papers of Samuel Marchbanks" in 1985.

During the 1950s, Davies played a major role in launching the Stratford Shakespearean Festival of Canada. He served on the Festival's board of governors, and collaborated with the Festival's director, Sir Tyrone Guthrie, in publishing three books about the Festival's early years.

Although his first love was drama and he had achieved some success with his occasional humorous essays, Davies found his greatest success in fiction. His first three novels, which later became known as The Salterton Trilogy, were "Tempest-Tost" (1951), "Leaven of Malice" (1954, also the basis of the unsuccessful play "Love and Libel") which won the Stephen Leacock Award for Humour, and "A Mixture of Frailties" (1958). These novels explored the difficulty of sustaining a cultural life in Canada, and life on a small-town newspaper, subjects of which Davies had first-hand knowledge.

In 1960, Davies joined Trinity College at the University of Toronto, where he would teach literature until 1981. The following year he published a collection of essays on literature, "A Voice From the Attic", and was awarded the Lorne Pierce Medal for his literary achievements.

In 1963, he became the Master of Massey College, the University of Toronto's new graduate college. During his stint as Master, he initiated a tradition of writing and telling ghost stories at the yearly Christmas celebrations. His stories were later collected in the book, "High Spirits" (1982).

Davies drew on his interest in Jungian psychology to create "Fifth Business" (1970), a novel that relies heavily on Davies' own experiences, his love of myth and magic and his knowledge of small-town mores. The narrator, like Davies, is of immigrant Canadian background, with a father who runs the town paper. The book's characters act in roles that roughly correspond to Jungian archetypes according to Davies' belief in the predominance of spirit over the things of the world.

Davies built on the success of "Fifth Business" with two more novels: "The Manticore" (1972), a novel cast largely in the form of a Jungian analysis (for which he received that year's Governor General's Literary Award), and "World of Wonders" (1975). Together these three books came to be known as "The Deptford Trilogy".

When Davies retired from his position at the university, his seventh novel, a satire of academic life, "The Rebel Angels" (1981), was published, followed by "What's Bred in the Bone" (1985) which was short-listed for the Booker Prize for fiction in 1986. "The Lyre of Orpheus" (1988) follows these two books in what became known as "The Cornish Trilogy".

During his retirement from academe he continued to write novels which further established him as a major figure in the literary world: "Murther and Walking Spirits" (1991) and "The Cunning Man" (1994). A third novel in what would have been a further trilogy – the Toronto Trilogy – was in progress at the time of Davies' death. He also realized a long-held dream when he penned the libretto to an opera: "The Golden Ass", based on "The Metamorphoses" of Lucius Apuleius, just like that written by one of the characters in Davies' 1958 "A Mixture of Frailties". The opera was performed by the Canadian Opera Company at the Hummingbird Centre in Toronto, in April 1999, several years after Davies' death.

In its obituary, "The Times" wrote: "Davies encompassed all the great elements of life ... His novels combined deep seriousness and psychological inquiry with fantasy and exuberant mirth." He remained close friends with John Kenneth Galbraith, attending Galbraith's eighty-fifth birthday party in Boston in 1993, and became so close a friend and colleague of the American novelist John Irving that Irving gave one of the scripture readings at Davies' funeral in the chapel of Trinity College, Toronto. He also wrote in support of Salman Rushdie when the latter was threatened by a "fatwā" from Ayatollah Ruhollah Khomeini of Iran in reaction to supposed anti-Islam expression in his novel "The Satanic Verses".

Davies was married to Brenda Ethel Davies (1917–2013) in 1940 and survived by four grandchildren and three great-grandchildren from his three daughters Miranda Davies, Rosamond Bailey and author Jennifer Surridge.



Fictional essays
edited by the author into:

Criticism









</doc>
<doc id="25758" url="https://en.wikipedia.org/wiki?curid=25758" title="RNA">
RNA

Ribonucleic acid (RNA) is a polymeric molecule essential in various biological roles in coding, decoding, regulation and expression of genes. RNA and DNA are nucleic acids, and, along with lipids, proteins and carbohydrates, constitute the four major macromolecules essential for all known forms of life. Like DNA, RNA is assembled as a chain of nucleotides, but unlike DNA it is more often found in nature as a single strand folded onto itself, rather than a paired double strand. Cellular organisms use messenger RNA (mRNA) to convey genetic information (using the nitrogenous bases of guanine, uracil, adenine, and cytosine, denoted by the letters G, U, A, and C) that directs synthesis of specific proteins. Many viruses encode their genetic information using an RNA genome.

Some RNA molecules play an active role within cells by catalyzing biological reactions, controlling gene expression, or sensing and communicating responses to cellular signals. One of these active processes is protein synthesis, a universal function in which RNA molecules direct the synthesis of proteins on ribosomes. This process uses transfer RNA (tRNA) molecules to deliver amino acids to the ribosome, where ribosomal RNA (rRNA) then links amino acids together to form coded proteins.

Like DNA, most biologically active RNAs, including mRNA, tRNA, rRNA, snRNAs, and other non-coding RNAs, contain self-complementary sequences that allow parts of the RNA to fold and pair with itself to form double helices. Analysis of these RNAs has revealed that they are highly structured. Unlike DNA, their structures do not consist of long double helices, but rather collections of short helices packed together into structures akin to proteins.
In this fashion, RNAs can achieve chemical catalysis (like enzymes). For instance, determination of the structure of the ribosome—an RNA-protein complex that catalyzes peptide bond formation—revealed that its active site is composed entirely of RNA.

Each nucleotide in RNA contains a ribose sugar, with carbons numbered 1' through 5'. A base is attached to the 1' position, in general, adenine (A), cytosine (C), guanine (G), or uracil (U). Adenine and guanine are purines, cytosine and uracil are pyrimidines. A phosphate group is attached to the 3' position of one ribose and the 5' position of the next. The phosphate groups have a negative charge each, making RNA a charged molecule (polyanion). The bases form hydrogen bonds between cytosine and guanine, between adenine and uracil and between guanine and uracil. However, other interactions are possible, such as a group of adenine bases binding to each other in a bulge,
or the GNRA tetraloop that has a guanine–adenine base-pair.
An important structural component of RNA that distinguishes it from DNA is the presence of a hydroxyl group at the 2' position of the ribose sugar. The presence of this functional group causes the helix to mostly take the A-form geometry, although in single strand dinucleotide contexts, RNA can rarely also adopt the B-form most commonly observed in DNA. The A-form geometry results in a very deep and narrow major groove and a shallow and wide minor groove. A second consequence of the presence of the 2'-hydroxyl group is that in conformationally flexible regions of an RNA molecule (that is, not involved in formation of a double helix), it can chemically attack the adjacent phosphodiester bond to cleave the backbone.
RNA is transcribed with only four bases (adenine, cytosine, guanine and uracil), but these bases and attached sugars can be modified in numerous ways as the RNAs mature. Pseudouridine (Ψ), in which the linkage between uracil and ribose is changed from a C–N bond to a C–C bond, and ribothymidine (T) are found in various places (the most notable ones being in the TΨC loop of tRNA). Another notable modified base is hypoxanthine, a deaminated adenine base whose nucleoside is called inosine (I). Inosine plays a key role in the wobble hypothesis of the genetic code.

There are more than 100 other naturally occurring modified nucleosides. The greatest structural diversity of modifications can be found in tRNA, while pseudouridine and nucleosides with 2'-O-methylribose often present in rRNA are the most common. The specific roles of many of these modifications in RNA are not fully understood. However, it is notable that, in ribosomal RNA, many of the post-transcriptional modifications occur in highly functional regions, such as the peptidyl transferase center and the subunit interface, implying that they are important for normal function.

The functional form of single-stranded RNA molecules, just like proteins, frequently requires a specific tertiary structure. The scaffold for this structure is provided by secondary structural elements that are hydrogen bonds within the molecule. This leads to several recognizable "domains" of secondary structure like hairpin loops, bulges, and internal loops. Since RNA is charged, metal ions such as Mg are needed to stabilise many secondary and tertiary structures.

The naturally occurring enantiomer of RNA is -RNA composed of -ribonucleotides. All chirality centers are located in the -ribose. By the use of -ribose or rather -ribonucleotides, -RNA can be synthesized. -RNA is much more stable against degradation by RNase.

Like other structured biopolymers such as proteins, one can define topology of a folded RNA molecule. This is often done based on arrangement of intra-chain contacts within a folded RNA, termed as circuit topology.

Synthesis of RNA is usually catalyzed by an enzyme—RNA polymerase—using DNA as a template, a process known as transcription. Initiation of transcription begins with the binding of the enzyme to a promoter sequence in the DNA (usually found "upstream" of a gene). The DNA double helix is unwound by the helicase activity of the enzyme. The enzyme then progresses along the template strand in the 3’ to 5’ direction, synthesizing a complementary RNA molecule with elongation occurring in the 5’ to 3’ direction. The DNA sequence also dictates where termination of RNA synthesis will occur.

Primary transcript RNAs are often modified by enzymes after transcription. For example, a poly(A) tail and a 5' cap are added to eukaryotic pre-mRNA and introns are removed by the spliceosome.

There are also a number of RNA-dependent RNA polymerases that use RNA as their template for synthesis of a new strand of RNA. For instance, a number of RNA viruses (such as poliovirus) use this type of enzyme to replicate their genetic material. Also, RNA-dependent RNA polymerase is part of the RNA interference pathway in many organisms.

Messenger RNA (mRNA) is the RNA that carries information from DNA to the ribosome, the sites of protein synthesis (translation) in the cell. The coding sequence of the mRNA determines the amino acid sequence in the protein that is produced. However, many RNAs do not code for protein (about 97% of the transcriptional output is non-protein-coding in eukaryotes).

These so-called non-coding RNAs ("ncRNA") can be encoded by their own genes (RNA genes), but can also derive from mRNA introns. The most prominent examples of non-coding RNAs are transfer RNA (tRNA) and ribosomal RNA (rRNA), both of which are involved in the process of translation. There are also non-coding RNAs involved in gene regulation, RNA processing and other roles. Certain RNAs are able to catalyse chemical reactions such as cutting and ligating other RNA molecules, and the catalysis of peptide bond formation in the ribosome; these are known as ribozymes.

According to the length of RNA chain, RNA includes small RNA and long RNA. Usually, small RNAs are shorter than 200 nt in length, and long RNAs are greater than 200 nt long. Long RNAs, also called large RNAs, mainly include long non-coding RNA (lncRNA) and mRNA. Small RNAs mainly include 5.8S ribosomal RNA (rRNA), 5S rRNA, transfer RNA (tRNA), microRNA (miRNA), small interfering RNA (siRNA), small nucleolar RNA (snoRNAs), Piwi-interacting RNA (piRNA), tRNA-derived small RNA (tsRNA) and small rDNA-derived RNA (srRNA).

Messenger RNA (mRNA) carries information about a protein sequence to the ribosomes, the protein synthesis factories in the cell. It is coded so that every three nucleotides (a codon) corresponds to one amino acid. In eukaryotic cells, once precursor mRNA (pre-mRNA) has been transcribed from DNA, it is processed to mature mRNA. This removes its introns—non-coding sections of the pre-mRNA. The mRNA is then exported from the nucleus to the cytoplasm, where it is bound to ribosomes and translated into its corresponding protein form with the help of tRNA. In prokaryotic cells, which do not have nucleus and cytoplasm compartments, mRNA can bind to ribosomes while it is being transcribed from DNA. After a certain amount of time, the message degrades into its component nucleotides with the assistance of ribonucleases.

Transfer RNA (tRNA) is a small RNA chain of about 80 nucleotides that transfers a specific amino acid to a growing polypeptide chain at the ribosomal site of protein synthesis during translation. It has sites for amino acid attachment and an anticodon region for codon recognition that binds to a specific sequence on the messenger RNA chain through hydrogen bonding.

Ribosomal RNA (rRNA) is the catalytic component of the ribosomes. Eukaryotic ribosomes contain four different rRNA molecules: 18S, 5.8S, 28S and 5S rRNA. Three of the rRNA molecules are synthesized in the nucleolus, and one is synthesized elsewhere. In the cytoplasm, ribosomal RNA and protein combine to form a nucleoprotein called a ribosome. The ribosome binds mRNA and carries out protein synthesis. Several ribosomes may be attached to a single mRNA at any time. Nearly all the RNA found in a typical eukaryotic cell is rRNA.

Transfer-messenger RNA (tmRNA) is found in many bacteria and plastids. It tags proteins encoded by mRNAs that lack stop codons for degradation and prevents the ribosome from stalling.

The earliest known regulators of gene expression were proteins known as repressors and activators, regulators with specific short binding sites within enhancer regions near the genes to be regulated.  More recently, RNAs have been found to regulate genes as well.  There are several kinds of RNA-dependent processes in eukaryotes regulating the expression of genes at various points, such as RNAi repressing genes post-transcriptionally, long non-coding RNAs shutting down blocks of chromatin epigenetically, and enhancer RNAs inducing increased gene expression. In addition to these mechanisms in eukaryotes, both bacteria and archaea have been found to use regulatory RNAs extensively. Bacterial small RNA and the CRISPR system are examples of such prokaryotic regulatory RNA systems. Fire and Mello were awarded the 2006 Nobel Prize in Physiology or Medicine for discovering microRNAs (miRNAs), specific short RNA molecules that can base-pair with mRNAs.

Post-transcriptional expression levels of many genes can be controlled by RNA interference, in which miRNAs, specific short RNA molecules, pair with meRNA regions and target them for degradation. This antisense-based process involves steps that first process the RNA so that it can base-pair with a region of its target mRNAs. Once the base pairing occurs, other proteins direct the mRNA to be destroyed by nucleases. Fire and Mello were awarded the 2006 Nobel Prize in Physiology or Medicine for this discovery.

Next to be linked to regulation were Xist and other long noncoding RNAs associated with X chromosome inactivation.  Their roles, at first mysterious, were shown by Jeannie T. Lee and others to be the silencing of blocks of chromatin via recruitment of Polycomb complex so that messenger RNA could not be transcribed from them.  Additional lncRNAs, currently defined as RNAs of more than 200 base pairs that do not appear to have coding potential, have been found associated with regulation of stem cell pluripotency and cell division.

The third major group of regulatory RNAs is called enhancer RNAs.  It is not clear at present whether they are a unique category of RNAs of various lengths or constitute a distinct subset of lncRNAs.  In any case, they are transcribed from enhancers, which are known regulatory sites in the DNA near genes they regulate.  They up-regulate the transcription of the gene(s) under control of the enhancer from which they are transcribed.

At first, regulatory RNA was thought to be a eukaryotic phenomenon, a part of the explanation for why so much more transcription in higher organisms was seen than had been predicted. But as soon as researchers began to look for possible RNA regulators in bacteria, they turned up there as well, termed as small RNA (sRNA). Currently, the ubiquitous nature of systems of RNA regulation of genes has been discussed as support for the RNA World theory. Bacterial small RNAs generally act via antisense pairing with mRNA to down-regulate its translation, either by affecting stability or affecting cis-binding ability. Riboswitches have also been discovered. They are cis-acting regulatory RNA sequences acting allosterically. They change shape when they bind metabolites so that they gain or lose the ability to bind chromatin to regulate expression of genes.

Archaea also have systems of regulatory RNA. The CRISPR system, recently being used to edit DNA "in situ", acts via regulatory RNAs in archaea and bacteria to provide protection against virus invaders.

Many RNAs are involved in modifying other RNAs.
Introns are spliced out of pre-mRNA by spliceosomes, which contain several small nuclear RNAs (snRNA), or the introns can be ribozymes that are spliced by themselves.
RNA can also be altered by having its nucleotides modified to nucleotides other than A, C, G and U.
In eukaryotes, modifications of RNA nucleotides are in general directed by small nucleolar RNAs (snoRNA; 60–300 nt), found in the nucleolus and cajal bodies. snoRNAs associate with enzymes and guide them to a spot on an RNA by basepairing to that RNA. These enzymes then perform the nucleotide modification. rRNAs and tRNAs are extensively modified, but snRNAs and mRNAs can also be the target of base modification. RNA can also be methylated.

Like DNA, RNA can carry genetic information. RNA viruses have genomes composed of RNA that encodes a number of proteins. The viral genome is replicated by some of those proteins, while other proteins protect the genome as the virus particle moves to a new host cell. Viroids are another group of pathogens, but they consist only of RNA, do not encode any protein and are replicated by a host plant cell's polymerase.

Reverse transcribing viruses replicate their genomes by reverse transcribing DNA copies from their RNA; these DNA copies are then transcribed to new RNA. Retrotransposons also spread by copying DNA and RNA from one another, and telomerase contains an RNA that is used as template for building the ends of eukaryotic chromosomes.

Double-stranded RNA (dsRNA) is RNA with two complementary strands, similar to the DNA found in all cells, but with the replacement of thymine by uracil. dsRNA forms the genetic material of some viruses (double-stranded RNA viruses). Double-stranded RNA, such as viral RNA or siRNA, can trigger RNA interference in eukaryotes, as well as interferon response in vertebrates.

In the late 1970s, it was shown that there is a single stranded covalently closed, i.e. circular form of RNA expressed throughout the animal and plant kingdom (see circRNA). circRNAs are thought to arise via a "back-splice" reaction where the spliceosome joins a downstream donor to an upstream acceptor splice site. So far the function of circRNAs is largely unknown, although for few examples a microRNA sponging activity has been demonstrated.

Research on RNA has led to many important biological discoveries and numerous Nobel Prizes. Nucleic acids were discovered in 1868 by Friedrich Miescher, who called the material 'nuclein' since it was found in the nucleus. It was later discovered that prokaryotic cells, which do not have a nucleus, also contain nucleic acids. The role of RNA in protein synthesis was suspected already in 1939. Severo Ochoa won the 1959 Nobel Prize in Medicine (shared with Arthur Kornberg) after he discovered an enzyme that can synthesize RNA in the laboratory. However, the enzyme discovered by Ochoa (polynucleotide phosphorylase) was later shown to be responsible for RNA degradation, not RNA synthesis. In 1956 Alex Rich and David Davies hybridized two separate strands of RNA to form the first crystal of RNA whose structure could be determined by X-ray crystallography.

The sequence of the 77 nucleotides of a yeast tRNA was found by Robert W. Holley in 1965, winning Holley the 1968 Nobel Prize in Medicine (shared with Har Gobind Khorana and Marshall Nirenberg).

During the early 1970s, retroviruses and reverse transcriptase were discovered, showing for the first time that enzymes could copy RNA into DNA (the opposite of the usual route for transmission of genetic information). For this work, David Baltimore, Renato Dulbecco and Howard Temin were awarded a Nobel Prize in 1975.
In 1976, Walter Fiers and his team determined the first complete nucleotide sequence of an RNA virus genome, that of bacteriophage MS2.

In 1977, introns and RNA splicing were discovered in both mammalian viruses and in cellular genes, resulting in a 1993 Nobel to Philip Sharp and Richard Roberts.
Catalytic RNA molecules (ribozymes) were discovered in the early 1980s, leading to a 1989 Nobel award to Thomas Cech and Sidney Altman. In 1990, it was found in "Petunia" that introduced genes can silence similar genes of the plant's own, now known to be a result of RNA interference.

At about the same time, 22 nt long RNAs, now called microRNAs, were found to have a role in the development of "C. elegans".
Studies on RNA interference gleaned a Nobel Prize for Andrew Fire and Craig Mello in 2006, and another Nobel was awarded for studies on the transcription of RNA to Roger Kornberg in the same year. The discovery of gene regulatory RNAs has led to attempts to develop drugs made of RNA, such as siRNA, to silence genes. Adding to the Nobel prizes awarded for research on RNA in 2009 it was awarded for the elucidation of the atomic structure of the ribosome to Venki Ramakrishnan, Tom Steitz, and Ada Yonath.

In 1967, Carl Woese hypothesized that RNA might be catalytic and suggested that the earliest forms of life (self-replicating molecules) could have relied on RNA both to carry genetic information and to catalyze biochemical reactions—an RNA world.

In March 2015, complex DNA and RNA nucleotides, including uracil, cytosine and thymine, were reportedly formed in the laboratory under outer space conditions, using starter chemicals, such as pyrimidine, an organic compound commonly found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), is one of the most carbon-rich compounds found in the Universe and may have been formed in red giants or in interstellar dust and gas clouds.




</doc>
<doc id="25762" url="https://en.wikipedia.org/wiki?curid=25762" title="Russian Revolution">
Russian Revolution

The Russian Revolution was a period of political and social revolution across the territory of the Russian Empire, commencing with the abolition of the monarchy in 1917, and concluding in 1923 after the Bolshevik establishment of the Soviet Union, including national states of Ukraine, Azebaijan and others, and end of the Civil War.

It began during the First World War, with the February Revolution that was focused in and around Petrograd (now Saint Petersburg), the capital of Russia at that time. The revolution erupted in the context of Russia's major military losses during the War, which resulted in much of the Russian Army being ready to mutiny. In the chaos, members of the Duma, Russia's parliament, assumed control of the country, forming the Russian Provisional Government. This was dominated by the interests of large capitalists and the noble aristocracy. The army leadership felt they did not have the means to suppress the revolution, and Emperor Nicholas II abdicated his throne. Grassroots community assemblies called 'Soviets', which were dominated by soldiers and the urban industrial working class, initially permitted the Provisional Government to rule, but insisted on a prerogative to influence the government and control various militias.

A period of dual power ensued, during which the Provisional Government held state power while the national network of Soviets, led by socialists, had the allegiance of the lower classes and, increasingly, the left-leaning urban middle class. During this chaotic period, there were frequent mutinies, protests and strikes. Many socialist political organizations were engaged in daily struggle and vied for influence within the Duma and the Soviets, central among which were the Bolsheviks ("Ones of the Majority") led by Vladimir Lenin. He campaigned for an immediate end of Russia's participation in the War, granting land to the peasants, and providing bread to the urban workers. When the Provisional Government chose to continue fighting the war with Germany, the Bolsheviks and other socialist factions exploited the virtually universal disdain towards the war effort as justification to advance the revolution further. The Bolsheviks turned workers' militias under their control into the Red Guards (later the Red Army), over which they exerted substantial control.

The situation climaxed with the October Revolution in 1917, a Bolshevik-led armed insurrection by workers and soldiers in Petrograd that successfully overthrew the Provisional Government, transferring all its authority to the Soviets. They soon relocated the national capital to Moscow. The Bolsheviks had secured a strong base of support within the Soviets and, as the supreme governing party, established a federal government dedicated to reorganizing the former empire into the world's first socialist state, to practice Soviet democracy on a national and international scale. Their promise to end Russia's participation in the First World War was fulfilled when the Bolshevik leaders signed the Treaty of Brest-Litovsk with Germany in March 1918. To further secure the new state, the Bolsheviks established the Cheka, a secret police that functioned as a revolutionary security service to weed out, execute, or punish those considered to be "enemies of the people" in campaigns consciously modeled on those of the French Revolution.

Soon after, civil war erupted among the "Reds" (Bolsheviks), the "Whites" (counter-revolutionaries), the independence movements, and other socialist factions opposed to the Bolsheviks. It continued for several years, during which the Bolsheviks defeated both the Whites and all rival socialists. Victorious, they reconstituted themselves as the Communist Party. They also established Soviet power in the newly independent republics of Armenia, Azerbaijan, Belarus, Georgia and Ukraine. They brought these jurisdictions into unification under the Union of Soviet Socialist Republics (USSR) in 1922. While many notable historical events occurred in Moscow and Petrograd, there were also major changes in cities throughout the state, and among national minorities throughout the empire and in the rural areas, where peasants took over and redistributed land.

The Russian Revolution of 1905 was said to be a major factor contributing to the cause of the Revolutions of 1917. The events of Bloody Sunday triggered nationwide protests and soldier mutinies. A council of workers called the St. Petersburg Soviet was created in this chaos. While the 1905 Revolution was ultimately crushed, and the leaders of the St. Petersburg Soviet were arrested, this laid the groundwork for the later Petrograd Soviet and other revolutionary movements during the lead up to 1917. The 1905 Revolution also led to the creation of a Duma (parliament), that would later form the Provisional Government following February 1917.

The outbreak of World War I prompted general outcry directed at Tsar Nicholas II and the Romanov family. While the nation was initially engaged in a wave of nationalism, increasing numbers of defeats and poor conditions soon flipped the nation's opinion. The Tsar attempted to remedy the situation by taking personal control of the army in 1915. This proved to be extremely disadvantageous for the Tsar, as he was now held personally responsible for Russia's continuing defeats and losses. In addition, Tsarina Alexandra, left to rule in while the Tsar commanded at the front, was German born, leading to suspicion of collusion, only to be exacerbated by rumors relating to her relationship with the controversial mystic Grigori Rasputin. Rasputin's influence led to disastrous ministerial appointments and corruption, resulting in a worsening of conditions within Russia. This led to general dissatisfaction with the Romanov family, and was a major factor contributing to the retaliation of the Russian Communists against the royal family.

After the entry of the Ottoman Empire on the side of the Central Powers in October 1914, Russia was deprived of a major trade route through the Dardanelles, which further contributed to the economic crisis, in which Russia became incapable of providing munitions to their army in the years leading to 1917. However, the problems were primarily administrative, not industrial, as Germany was able to produce great amounts of munitions whilst constantly fighting on two major battlefronts.

The conditions during the war resulted in a devastating loss of morale within the Russian army and the general population of Russia itself. This was particularly apparent in the cities, owing to a lack of food in response to the disruption of agriculture. Food scarcity had become a considerable problem in Russia, but the cause of this did not lie in any failure of the harvests, which had not been significantly altered during wartime. The indirect reason was that the government, in order to finance the war, printed millions of ruble notes, and by 1917, inflation had made prices increase up to four times what they had been in 1914. Farmers were consequently faced with a higher cost of living, but with little increase in income. As a result, they tended to hoard their grain and to revert to subsistence farming. Thus the cities were constantly short of food. At the same time, rising prices led to demands for higher wages in the factories, and in January and February 1916, revolutionary propaganda, in part aided by German funds, led to widespread strikes. This resulted in a growing criticism of the government, including an increased participation of workers in revolutionary parties.

Liberal parties too had an increased platform to voice their complaints, as the initial fervor of the war resulted in the Tsarist government creating a variety of political organizations. In July 1915, a Central War Industries Committee was established under the chairmanship of a prominent Octobrist, Alexander Guchkov (1862–1936), including ten workers' representatives. The Petrograd Mensheviks agreed to join despite the objections of their leaders abroad. All this activity gave renewed encouragement to political ambitions, and in September 1915, a combination of Octobrists and Kadets in the Duma demanded the forming of a responsible government. which the Tsar rejected.

All these factors had given rise to a sharp loss of confidence in the regime, even within the ruling class, growing throughout the war. Early in 1916, Guchkov discussed with senior army officers and members of the Central War Industries Committee about a possible coup to force the abdication of the Tsar. In December, a small group of nobles assassinated Rasputin, and in January 1917 the Tsar's cousin, Grand Duke Nicholas, was asked indirectly by Prince Lvov whether he would be prepared to take over the throne from his nephew, Tsar Nicholas II. None of these incidents were in themselves the immediate cause of the February Revolution, but they do help to explain why the monarchy survived only a few days after it had broken out.

Meanwhile, Socialist Revolutionary leaders in exile, many of them living in Switzerland, had been the glum spectators of the collapse of international socialist solidarity. French and German Social Democrats had voted in favour of their respective governments' war efforts. Georgi Plekhanov in Paris had adopted a violently anti-German stand, while Alexander Parvus supported the German war effort as the best means of ensuring a revolution in Russia. The Mensheviks largely maintained that Russia had the right to defend herself against Germany, although Julius Martov (a prominent Menshevik), now on the left of his group, demanded an end to the war and a settlement on the basis of national self-determination, with no annexations or indemnities.

It was these views of Martov that predominated in a manifesto drawn up by Leon Trotsky (at the time a Menshevik) at a conference in Zimmerwald, attended by 35 Socialist leaders in September 1915. Inevitably Vladimir Lenin, supported by Zinoviev and Radek, strongly contested them. Their attitudes became known as the Zimmerwald Left. Lenin rejected both the defence of Russia and the cry for peace. Since the autumn of 1914, he had insisted that "from the standpoint of the working class and of the labouring masses the lesser evil would be the defeat of the Tsarist Monarchy"; the war must be turned into a civil war of the proletarian soldiers against their own governments, and if a proletarian victory should emerge from this in Russia, then their duty would be to wage a revolutionary war for the liberation of the masses throughout Europe.

An elementary theory of property, believed by many peasants, was that land should belong to those who work on it. At the same time, peasant life and culture was changing constantly. Change was facilitated by the physical movement of growing numbers of peasant villagers who migrated to and from industrial and urban environments, but also by the introduction of city culture into the village through material goods, the press, and word of mouth.

Workers also had good reasons for discontent: overcrowded housing with often deplorable sanitary conditions, long hours at work (on the eve of the war, a 10-hour workday six days a week was the average and many were working 11–12 hours a day by 1916), constant risk of injury and death from poor safety and sanitary conditions, harsh discipline (not only rules and fines, but foremen's fists), and inadequate wages (made worse after 1914 by steep wartime increases in the cost of living). At the same time, urban industrial life had its benefits, though these could be just as dangerous (in terms of social and political stability) as the hardships. There were many encouragements to expect more from life. Acquiring new skills gave many workers a sense of self-respect and confidence, heightening expectations and desires. Living in cities, workers encountered material goods they had never seen in villages. Most importantly, workers living in cities were exposed to new ideas about the social and political order.

The social causes of the Russian Revolution can be derived from centuries of oppression of the lower classes by the Tsarist regime and Nicholas's failures in World War I. While rural agrarian peasants had been emancipated from serfdom in 1861, they still resented paying redemption payments to the state, and demanded communal tender of the land they worked. The problem was further compounded by the failure of Sergei Witte's land reforms of the early 20th century. Increasing peasant disturbances and sometimes actual revolts occurred, with the goal of securing ownership of the land they worked. Russia consisted mainly of poor farming peasants and substantial inequality of land ownership, with 1.5% of the population owning 25% of the land.

The rapid industrialization of Russia also resulted in urban overcrowding and poor conditions for urban industrial workers (as mentioned above). Between 1890 and 1910, the population of the capital, Saint Petersburg, swelled from 1,033,600 to 1,905,600, with Moscow experiencing similar growth. This created a new 'proletariat' which, due to being crowded together in the cities, was much more likely to protest and go on strike than the peasantry had been in previous times. In one 1904 survey, it was found that an average of 16 people shared each apartment in Saint Petersburg, with six people per room. There was also no running water, and piles of human waste were a threat to the health of the workers. The poor conditions only aggravated the situation, with the number of strikes and incidents of public disorder rapidly increasing in the years shortly before World War I. Because of late industrialization, Russia's workers were highly concentrated. By 1914, 40% of Russian workers were employed in factories of 1,000+ workers (32% in 1901). 42% worked in 100–1,000 worker enterprises, 18% in 1–100 worker businesses (in the US, 1914, the figures were 18, 47 and 35 respectively).
World War I added to the chaos. Conscription across Russia resulted in unwilling citizens being sent off to war. The vast demand for factory production of war supplies and workers resulted in many more labor riots and strikes. Conscription stripped skilled workers from the cities, who had to be replaced with unskilled peasants. When famine began to hit due to the poor railway system, workers abandoned the cities in droves seeking food. Finally, the soldiers themselves, who suffered from a lack of equipment and protection from the elements, began to turn against the Tsar. This was mainly because, as the war progressed, many of the officers who were loyal to the Tsar were killed, being replaced by discontented conscripts from the major cities who had little loyalty to the Tsar.

Many sections of the country had reason to be dissatisfied with the existing autocracy. Nicholas II was a deeply conservative ruler and maintained a strict authoritarian system. Individuals and society in general were expected to show self-restraint, devotion to community, deference to the social hierarchy and a sense of duty to the country. Religious faith helped bind all of these tenets together as a source of comfort and reassurance in the face of difficult conditions and as a means of political authority exercised through the clergy. Perhaps more than any other modern monarch, Nicholas II attached his fate and the future of his dynasty to the notion of the ruler as a saintly and infallible father to his people.

This vision of the Romanov monarchy left him unaware of the state of his country. With a firm belief that his power to rule was granted by Divine Right, Nicholas assumed that the Russian people were devoted to him with unquestioning loyalty. This ironclad belief rendered Nicholas unwilling to allow the progressive reforms that might have alleviated the suffering of the Russian people. Even after the 1905 Revolution spurred the Tsar to decree limited civil rights and democratic representation, he worked to limit even these liberties in order to preserve the ultimate authority of the crown.

Despite constant oppression, the desire of the people for democratic participation in government decisions was strong. Since the Age of Enlightenment, Russian intellectuals had promoted Enlightenment ideals such as the dignity of the individual and the rectitude of democratic representation. These ideals were championed most vociferously by Russia's liberals, although populists, Marxists, and anarchists also claimed to support democratic reforms. A growing opposition movement had begun to challenge the Romanov monarchy openly well before the turmoil of World War I.

Dissatisfaction with Russian autocracy culminated in the huge national upheaval that followed the Bloody Sunday massacre of January 1905, in which hundreds of unarmed protesters were shot by the Tsar's troops. Workers responded to the massacre with a crippling general strike, forcing Nicholas to put forth the October Manifesto, which established a democratically elected parliament (the State Duma). Although the Tsar accepted the 1906 Fundamental State Laws one year later, he subsequently dismissed the first two Dumas when they proved uncooperative. Unfulfilled hopes of democracy fueled revolutionary ideas and violent outbursts targeted at the monarchy.

One of the Tsar's principal rationales for risking war in 1914 was his desire to restore the prestige that Russia had lost amid the debacles of the Russo-Japanese War (1904-1905). Nicholas also sought to foster a greater sense of national unity with a war against a common and old enemy. The Russian Empire was an agglomeration of diverse ethnicities that had demonstrated significant signs of disunity in the years before the First World War. Nicholas believed in part that the shared peril and tribulation of a foreign war would mitigate the social unrest over the persistent issues of poverty, inequality, and inhumane working conditions. Instead of restoring Russia's political and military standing, World War I led to the slaughter of Russian troops and military defeats that undermined both the monarchy and Russian society to the point of collapse.

The outbreak of war in August 1914 initially served to quiet the prevalent social and political protests, focusing hostilities against a common external enemy, but this patriotic unity did not last long. As the war dragged on inconclusively, war-weariness gradually took its toll. Although many ordinary Russians joined anti-German demonstrations in the first few weeks of the war, hostility toward the Kaiser and the desire to defend their land and their lives did not necessarily translate into enthusiasm for the Tsar or the government.

Russia's first major battle of the war was a disaster; in the 1914 Battle of Tannenberg, over 30,000 Russian troops were killed or wounded and 90,000 captured, while Germany suffered just 12,000 casualties. However, Austro-Hungarian forces allied to Germany were driven back deep into the Galicia region by the end of the year. In the autumn of 1915, Nicholas had taken direct command of the army, personally overseeing Russia's main theatre of war and leaving his ambitious but incapable wife Alexandra in charge of the government. Reports of corruption and incompetence in the Imperial government began to emerge, and the growing influence of Grigori Rasputin in the Imperial family was widely resented.

In 1915, things took a critical turn for the worse when Germany shifted its focus of attack to the Eastern front. The superior German army – better led, better trained, and better supplied – was quite effective against the ill-equipped Russian forces, driving the Russians out of Galicia, as well as Russian Poland during the Gorlice–Tarnów Offensive campaign. By the end of October 1916, Russia had lost between 1,600,000 and 1,800,000 soldiers, with an additional 2,000,000 prisoners of war and 1,000,000 missing, all making up a total of nearly 5,000,000 men.

These staggering losses played a definite role in the mutinies and revolts that began to occur. In 1916, reports of fraternizing with the enemy began to circulate. Soldiers went hungry, lacked shoes, munitions, and even weapons. Rampant discontent lowered morale, which was further undermined by a series of military defeats.

Casualty rates were the most vivid sign of this disaster. By the end of 1914, only five months into the war, around 390,000 Russian men had lost their lives and nearly 1,000,000 were injured. Far sooner than expected, inadequately trained recruits were called for active duty, a process repeated throughout the war as staggering losses continued to mount. The officer class also saw remarkable changes, especially within the lower echelons, which were quickly filled with soldiers rising up through the ranks. These men, usually of peasant or working-class backgrounds, were to play a large role in the politicization of the troops in 1917.

The army quickly ran short of rifles and ammunition (as well as uniforms and food), and by mid-1915, men were being sent to the front bearing no arms. It was hoped that they could equip themselves with arms recovered from fallen soldiers, of both sides, on the battlefields. The soldiers did not feel as if they were valuable, rather they felt as if they were expendable.

By the spring of 1915, the army was in steady retreat, which was not always orderly; desertion, plundering, and chaotic flight were not uncommon. By 1916, however, the situation had improved in many respects. Russian troops stopped retreating, and there were even some modest successes in the offensives that were staged that year, albeit at great loss of life. Also, the problem of shortages was largely solved by a major effort to increase domestic production. Nevertheless, by the end of 1916, morale among soldiers was even worse than it had been during the great retreat of 1915. The fortunes of war may have improved, but the fact of war remained which continually took Russian lives. The crisis in morale (as was argued by Allan Wildman, a leading historian of the Russian army in war and revolution) "was rooted fundamentally in the feeling of utter despair that the slaughter would ever end and that anything resembling victory could be achieved."

The war did not only devastate soldiers. By the end of 1915, there were manifold signs that the economy was breaking down under the heightened strain of wartime demand. The main problems were food shortages and rising prices. Inflation dragged incomes down at an alarmingly rapid rate, and shortages made it difficult for an individual to sustain oneself. These shortages were a problem especially in the capital, St. Petersburg, where distance from supplies and poor transportation networks made matters particularly worse. Shops closed early or entirely for lack of bread, sugar, meat, and other provisions, and lines lengthened massively for what remained. Conditions became increasingly difficult to afford food and physically obtain it.

Strikes increased steadily from the middle of 1915, and so did crime, but, for the most part, people suffered and endured, scouring the city for food. Working-class women in St. Petersburg reportedly spent about forty hours a week in food lines, begging, turning to prostitution or crime, tearing down wooden fences to keep stoves heated for warmth, and continued to resent the rich.

Government officials responsible for public order worried about how long people's patience would last. A report by the St. Petersburg branch of the security police, the Okhrana, in October 1916, warned bluntly of "the possibility in the near future of riots by the lower classes of the empire enraged by the burdens of daily existence."

Tsar Nicholas was blamed for all of these crises, and what little support he had left began to crumble. As discontent grew, the State Duma issued a warning to Nicholas in November 1916, stating that, inevitably, a terrible disaster would grip the country unless a constitutional form of government was put in place. Nicholas ignored these warnings and Russia's Tsarist regime collapsed a few months later during the February Revolution of 1917. One year later, the Tsar and his entire family were executed.

At the beginning of February, Petrograd workers began several strikes and demonstrations. On , workers at Putilov, Petrograd's largest industrial plant, announced a strike.

The next day, a series of meetings and rallies were held for International Women's Day, which gradually turned into economic and political gatherings. Demonstrations were organised to demand bread, and these were supported by the industrial working force who considered them a reason for continuing the strikes. The women workers marched to nearby factories bringing out over 50,000 workers on strike. By , virtually every industrial enterprise in Petrograd had been shut down, together with many commercial and service enterprises. Students, white-collar workers, and teachers joined the workers in the streets and at public meetings.

To quell the riots, the Tsar looked to the army. At least 180,000 troops were available in the capital, but most were either untrained or injured. Historian Ian Beckett suggests around 12,000 could be regarded as reliable, but even these proved reluctant to move in on the crowd, since it included so many women. It was for this reason that on , when the Tsar ordered the army to suppress the rioting by force, troops began to revolt. Although few actively joined the rioting, many officers were either shot or went into hiding; the ability of the garrison to hold back the protests was all but nullified, symbols of the Tsarist regime were rapidly torn down around the city, and governmental authority in the capital collapsed – not helped by the fact that Nicholas had prorogued the Duma that morning, leaving it with no legal authority to act. The response of the Duma, urged on by the liberal bloc, was to establish a Temporary Committee to restore law and order; meanwhile, the socialist parties established the Petrograd Soviet to represent workers and soldiers. The remaining loyal units switched allegiance the next day.

The Tsar directed the royal train back towards Petrograd, which was stopped on , by a group of revolutionaries at Malaya Vishera. When the Tsar finally arrived at in Pskov, the Army Chief Nikolai Ruzsky, and the Duma deputies Alexander Guchkov and Vasily Shulgin suggested in unison that he abdicate the throne. He did so on , on behalf of himself, and then, having taken advice on behalf of his son, the Tsarevich. Nicholas nominated his brother, the Grand Duke Michael Alexandrovich, to succeed him. But the Grand Duke realised that he would have little support as ruler, so he declined the crown on , stating that he would take it only if that was the consensus of democratic action. Six days later, Nicholas, no longer Tsar and addressed with contempt by the sentries as "Nicholas Romanov", was reunited with his family at the Alexander Palace at Tsarskoye Selo. He was placed under house arrest with his family by the Provisional Government.

The immediate effect of the February Revolution was a widespread atmosphere of elation and excitement in Petrograd. On , a provisional government was announced. The center-left was well represented, and the government was initially chaired by a liberal aristocrat, Prince Georgy Yevgenievich Lvov, a member of the Constitutional Democratic Party (KD). The socialists had formed their rival body, the Petrograd Soviet (or workers' council) four days earlier. The Petrograd Soviet and the Provisional Government competed for power over Russia.

The effective power of the Provisional Government was challenged by the authority of an institution that claimed to represent the will of workers and soldiers and could, in fact, mobilize and control these groups during the early months of the revolution – the Petrograd Soviet Council of Workers' Deputies. The model for the Soviets were workers' councils that had been established in scores of Russian cities during the 1905 Revolution. In February 1917, striking workers elected deputies to represent them and socialist activists began organizing a citywide council to unite these deputies with representatives of the socialist parties. On 27 February, socialist Duma deputies, mainly Mensheviks and Socialist Revolutionaries, took the lead in organizing a citywide council. The Petrograd Soviet met in the Tauride Palace, the same building where the new government was taking shape.

The leaders of the Petrograd Soviet believed that they represented particular classes of the population, not the whole nation. They also believed Russia was not ready for socialism. They viewed their role as limited to pressuring hesitant "bourgeoisie" to rule and to introduce extensive democratic reforms in Russia (the replacement of the monarchy by a republic, guaranteed civil rights, a democratic police and army, abolition of religious and ethnic discrimination, preparation of elections to a constituent assembly, and so on). They met in the same building as the emerging Provisional Government not to compete with the Duma Committee for state power, but to best exert pressure on the new government, to act, in other words, as a popular democratic lobby.

The relationship between these two major powers was complex from the beginning and would shape the politics of 1917. The representatives of the Provisional Government agreed to "take into account the opinions of the Soviet of Workers' Deputies", though they were also determined to prevent "interference in the actions of the government", which would create "an unacceptable situation of dual power". In fact, this was precisely what was being created, though this "dual power" (dvoevlastie) was the result less of the actions or attitudes of the leaders of these two institutions than of actions outside their control, especially the ongoing social movement taking place on the streets of Russia's cities, factories, shops, barracks, villages, and in the trenches.

A series of political crises – see the chronology below – in the relationship between population and government and between the Provisional Government and the Soviets (which developed into a nationwide movement with a national leadership). The All-Russian Central Executive Committee of Soviets (VTsIK) undermined the authority of the Provisional Government but also of the moderate socialist leaders of the Soviets. Although the Soviet leadership initially refused to participate in the "bourgeois" Provisional Government, Alexander Kerensky, a young, popular lawyer and a member of the Socialist Revolutionary Party (SRP), agreed to join the new cabinet, and became an increasingly central figure in the government, eventually taking leadership of the Provisional Government. As minister of war and later Prime Minister, Kerensky promoted freedom of speech, released thousands of political prisoners, continued the war effort, even organizing another offensive (which, however, was no more successful than its predecessors). Nevertheless, Kerensky still faced several great challenges, highlighted by the soldiers, urban workers, and peasants, who claimed that they had gained nothing by the revolution:

The political group that proved most troublesome for Kerensky, and would eventually overthrow him, was the Bolshevik Party, led by Vladimir Lenin. Lenin had been living in exile in neutral Switzerland and, due to democratization of politics after the February Revolution, which legalized formerly banned political parties, he perceived the opportunity for his Marxist revolution. Although return to Russia had become a possibility, the war made it logistically difficult. Eventually, German officials arranged for Lenin to pass through their territory, hoping that his activities would weaken Russia or even – if the Bolsheviks came to power – lead to Russia's withdrawal from the war. Lenin and his associates, however, had to agree to travel to Russia in a sealed train: Germany would not take the chance that he would foment revolution in Germany. After passing through the front, he arrived in Petrograd in April 1917.

On the way to Russia, Lenin prepared the April Theses, which outlined central Bolshevik policies. These included that the Soviets take power (as seen in the slogan "all power to the Soviets") and denouncing the liberals and social revolutionaries in the Provisional Government, forbidding co-operation with it. Many Bolsheviks, however, had supported the Provisional Government, including Lev Kamenev.

With Lenin's arrival, the popularity of the Bolsheviks increased steadily. Over the course of the spring, public dissatisfaction with the Provisional Government and the war, in particular among workers, soldiers and peasants, pushed these groups to radical parties. Despite growing support for the Bolsheviks, buoyed by maxims that called most famously for "all power to the Soviets", the party held very little real power in the moderate-dominated Petrograd Soviet. In fact, historians such as Sheila Fitzpatrick have asserted that Lenin's exhortations for the Soviet Council to take power were intended to arouse indignation both with the Provisional Government, whose policies were viewed as conservative, and the Soviets themselves, which were viewed as subservients to the conservative government. By some other historians' accounts, Lenin and his followers were unprepared for how their groundswell of support, especially among influential worker and soldier groups, would translate into real power in the summer of 1917.

On 18 June, the Provisional Government launched an attack against Germany that failed miserably. Soon after, the government ordered soldiers to go to the front, reneging on a promise. The soldiers refused to follow the new orders. The arrival of radical Kronstadt sailors – who had tried and executed many officers, including one admiral – further fueled the growing revolutionary atmosphere. Sailors and soldiers, along with Petrograd workers, took to the streets in violent protest, calling for "all power to the Soviets". The revolt, however, was disowned by Lenin and the Bolshevik leaders and dissipated within a few days. In the aftermath, Lenin fled to Finland under threat of arrest while Trotsky, among other prominent Bolsheviks, was arrested. The July Days confirmed the popularity of the anti-war, radical Bolsheviks, but their unpreparedness at the moment of revolt was an embarrassing gaffe that lost them support among their main constituent groups: soldiers and workers.

The Bolshevik failure in the July Days proved temporary. The Bolsheviks had undergone a spectacular growth in membership. Whereas, in February 1917, the Bolsheviks were limited to only 24,000 members, by September 1917 there were 200,000 members of the Bolshevik faction. Previously, the Bolsheviks had been in the minority in the two leading cities of Russia—St. Petersburg and Moscow behind the Mensheviks and the Socialist Revolutionaries, by September the Bolsheviks were in the majority in both cities. Furthermore, the Bolshevik-controlled Moscow Regional Bureau of the Party also controlled the Party organizations of the 13 provinces around Moscow. These 13 provinces held 37% of Russia's population and 20% of the membership of the Bolshevik faction.

In August, poor and misleading communication led General Lavr Kornilov, the recently appointed Supreme Commander of Russian military forces, to believe that the Petrograd government had already been captured by radicals, or was in serious danger thereof. In response, he ordered troops to Petrograd to pacify the city. To secure his position, Kerensky had to ask for Bolshevik assistance. He also sought help from the Petrograd Soviet, which called upon armed Red Guards to "defend the revolution". The Kornilov Affair failed largely due to the efforts of the Bolsheviks, whose influence over railroad and telegraph workers proved vital in stopping the movement of troops. With his coup failing, Kornilov surrendered and was relieved of his position. The Bolsheviks' role in stopping the attempted coup further strengthened their position.

In early September, the Petrograd Soviet freed all jailed Bolsheviks and Trotsky became chairman of the Petrograd Soviet. Growing numbers of socialists and lower-class Russians viewed the government less as a force in support of their needs and interests. The Bolsheviks benefited as the only major organized opposition party that had refused to compromise with the Provisional Government, and they benefited from growing frustration and even disgust with other parties, such as the Mensheviks and Socialist Revolutionaries, who stubbornly refused to break with the idea of national unity across all classes.

In Finland, Lenin had worked on his book "State and Revolution" and continued to lead his party, writing newspaper articles and policy decrees. By October, he returned to Petrograd (present-day St. Petersburg), aware that the increasingly radical city presented him no legal danger and a second opportunity for revolution. Recognising the strength of the Bolsheviks, Lenin began pressing for the immediate overthrow of the Kerensky government by the Bolsheviks. Lenin was of the opinion that taking power should occur in both St. Petersburg and Moscow simultaneously, parenthetically stating that it made no difference which city rose up first, but expressing his opinion that Moscow may well rise up first. The Bolshevik Central Committee drafted a resolution, calling for the dissolution of the Provisional Government in favor of the Petrograd Soviet. The resolution was passed 10–2 (Lev Kamenev and Grigory Zinoviev prominently dissenting) promoting the October Revolution.

The October Revolution, night to Wednesday 7 November 1917 according to the modern Gregorian calendar and night to Wednesday 25 October according to the Julian calendar at the time in tsarist Russia, was organized by the Bolshevik party. Lenin did not have any direct role in the revolution and due to his personal security he was hiding. The Revolutionary Military Committee established by the Bolshevik party was organizing the insurrection and Leon Trotsky was the chairman. However, Lenin played a crucial role in the debate in the leadership of the Bolshevik party for a revolutionary insurrection as the party in the autumn of 1917 received a majority in the soviets. An ally in the left fraction of the Revolutionary-Socialist Party, with huge support among the peasants who opposed Russia's participation in the war, supported the slogan 'All power to the Soviets'.

Liberal and monarchist forces, loosely organized into the White Army, immediately went to war against the Bolsheviks' Red Army, in a series of battles that would become known as the Russian Civil War. This did not happen in 1917. The Civil War began in early 1918 with domestic anti-Bolshevik forces confronting the nascent Red Army. In autumn of 1918 allied countries chose to send troops to support the "Whites" with supplies of weapons, ammunition and logistic equipment being sent from the main Western countries but this was not at all coordinated. Germany did not participate in the civil war as it surrendered to the Allied.

Of more interests is the anarchist movement of Nestor Makhno in Ukraine who fought against the White generals, saved Moscow in 1919 from an attack by the general Denikin and in November 1920 helped the Bolshevik to defeat general Wrangel. However, 26 November 1920 the Bolshevik government invited headquarters staff and many of Makhno's subordinate commanders to a Red Army planning conference in Moscow only to have them imprisoned and executed. At that time was there already a decision to eliminate the Makhno movement. Nestor Makno escaped the hunt by the Red Army and in August 1921 he and 77 of his followers managed to escape into Romania and further to Poland, Germany to reach France where Makno died in 25 July 1934.
The provisional government with its second and third coalition was led by a right wing fraction of the Socialist-Revolutionary party, SR. This non-elected provisional government faced the revolutionary situation and the growing mood against the war by avoiding elections to the state Duma. However, the October revolution forced the political parties behind the newly dissolved provisional government to move and move fast for immediate elections. All happened so fast that the left SR fraction did not have time to reach out and be represented in ballots of the SR party which was part of the coalition in the provisional government. This non-elected government supported continuation of the war on the side of the allied forces. The elections to the State Duma 25 November 1917 therefore did not mirror the true political situation among peasants even if we don't know how the outcome would be if the anti-war left SR fraction had a fair chance to challenge the party leaders. In the elections the Bolshevik party received 25% of the votes and the Socialist-Revolutionaries as much as 58%. It is possible the left SR had a good chance to reach more than 25% of the votes and thereby legitimate the October revolution but we can only guess.

Lenin did not believe as Karl Marx that a socialist revolution presupposed a developed capitalist economy and not in a semi-capitalist country as Russia. Russia was backward but not that backward with a working class population of more than some 4-5% of the population.

Though Lenin was the leader of the Bolshevik Party, it has been argued that since Lenin was not present during the actual takeover of the Winter Palace, it was really Trotsky's organization and direction that led the revolution, merely spurred by the motivation Lenin instigated within his party. Critics on the Right have long argued that the financial and logistical assistance of German intelligence via their key agent, Alexander Parvus was a key component as well, though historians are divided, since there is little evidence supporting that claim.

Soviet membership was initially freely elected, but many members of the Socialist Revolutionary Party, anarchists, and other leftists created opposition to the Bolsheviks through the Soviets themselves. The elections to the Russian Constituent Assembly took place 25 November 1917. The Bolsheviks gained 25% of the vote. When it became clear that the Bolsheviks had little support outside of the industrialized areas of Saint Petersburg and Moscow, they simply barred non-Bolsheviks from membership in the Soviets. The Bolsheviks dissolved the Constituent Assembly in January 1918. Not surprisingly, this caused mass domestic tension with many individuals who called for another series of political reform, revolting, and calling for "a third Russian revolution," a movement that received a significant amount of support. The most notable instances of this anti-Bolshevik mentality were expressed in the Tambov rebellion, 1919–1921, and the Kronstadt rebellion in March 1921. These movements, which made a wide range of demands and lacked effective coordination, were eventually defeated along with the White Army during the Civil War.

The Russian Civil War, which broke out in 1918 shortly after the October Revolution, resulted in the deaths and suffering of millions of people regardless of their political orientation. The war was fought mainly between the Red Army ("Reds"), consisting of the uprising majority led by the Bolshevik minority, and the "Whites" – army officers and cossacks, the "bourgeoisie", and political groups ranging from the far Right, to the Socialist Revolutionaries who opposed the drastic restructuring championed by the Bolsheviks following the collapse of the Provisional Government, to the Soviets (under clear Bolshevik dominance). The Whites had backing from other countries such as Great Britain, France, the United States, and Japan, while the Reds possessed internal support, proving to be much more effective. Though the Allied nations, using external interference, provided substantial military aid to the loosely knit anti-Bolshevik forces, they were ultimately defeated.

The Bolsheviks firstly assumed power in Petrograd, expanding their rule outwards. They eventually reached the Easterly Siberian Russian coast in Vladivostok, four years after the war began, an occupation that is believed to have ended all significant military campaigns in the nation. Less than one year later, the last area controlled by the White Army, the Ayano-Maysky District, directly to the north of the Krai containing Vladivostok, was given up when General Anatoly Pepelyayev capitulated in 1923.

Several revolts were initiated against the Bolsheviks and their army near the end of the war, notably the Kronstadt Rebellion. This was a naval mutiny engineered by Soviet Baltic sailors, former Red Army soldiers, and the people of Kronstadt. This armed uprising was fought against the antagonizing Bolshevik economic policies that farmers were subjected to, including seizures of grain crops by the Communists. This all amounted to large-scale discontent. When delegates representing the Kronstadt sailors arrived at Petrograd for negotiations, they raised 15 demands primarily pertaining to the Russian right to freedom. The Government firmly denounced the rebellions and labelled the requests as a reminder of the Social Revolutionaries, a political party that was popular among Soviets before Lenin, but refused to cooperate with the Bolshevik Army. The Government then responded with an armed suppression of these revolts and suffered ten thousand casualties before entering the city of Kronstadt. This ended the rebellions fairly quickly, causing many of the rebels to flee seeking political exile.

During the Civil War, Nestor Makhno led a Ukrainian anarchist movement, the Black Army allied to the Bolsheviks thrice, one of the powers ending the alliance each time. However, a Bolshevik force under Mikhail Frunze destroyed the Makhnovist movement, when the Makhnovists refused to merge into the Red Army. In addition, the so-called "Green Army" (peasants defending their property against the opposing forces) played a secondary role in the war, mainly in the Ukraine.

Revolutionary tribunals were present during both the Revolution and the Civil War, intended for the purpose of combatting forces of counter-revolution. At the Civil War's zenith, it is reported that upwards of 200,000 cases were investigated by approximately 200 tribunals. These tribunals established themselves more so from the Cheka as a more moderate force that acted under the banner of revolutionary justice, rather than a utilizer of strict brute force as the former did. However, these tribunals did come with their own set of inefficiencies, such as responding to cases in a matter of months and not having a concrete definition of "counter-revolution" that was determined on a case-by-case basis. The "Decree on Revolutionary Tribunals" used by the People's Commissar of Justice, states in article 2 that "In fixing the penalty, the Revolutionary Tribunal shall be guided by the circumstances of the case and the dictates of the revolutionary conscience." Revolutionary tribunals ultimately demonstrated that a form of justice was still prevalent in Russian society where the Russian Provisional Government failed. This, in part, triggered the political transition of the October Revolution and the Civil War that followed in its aftermath.

The Bolsheviks executed the tsar and his family on 16 July 1918. In early March, the Provisional Government placed Nicholas and his family under house arrest in the Alexander Palace at Tsarskoye Selo, south of Petrograd. In August 1917 the Kerensky government evacuated the Romanovs to Tobolsk in the Urals, to protect them from the rising tide of revolution. However, Kerensky lost control after the Bolsheviks came to power in October 1917, and the conditions of their imprisonment grew stricter and talk of putting Nicholas on trial increased. As the counter revolutionary White movement gathered force, leading to full-scale civil war by the summer, the Romanovs were moved during April and May 1918 to Yekaterinburg, a militant Bolshevik stronghold.

During the early morning of 16 July, Nicholas, Alexandra, their children, their physician, and several servants were taken into the basement and shot. According to Edvard Radzinsky and Dmitrii Volkogonov, the order came directly from Lenin and Sverdlov in Moscow. That the order came from the top has long been believed, although there is a lack of hard evidence. The execution may have been carried out on the initiative of local Bolshevik officials, or it may have been an option pre-approved in Moscow should White troops approach Yekaterinburg. Radzinsky noted that Lenin's bodyguard personally delivered the telegram ordering the execution and that he was ordered to destroy the evidence.

The Russian Revolution became the site for many instances of symbolism, both physical and non-physical. Communist symbolism is perhaps the most notable of this time period, such as the debut of the iconic hammer and sickle as a representation of the October Revolution in 1917, eventually becoming the official symbol of the USSR in 1924. Although the Bolsheviks did not have extensive political experience, their portrayal of the revolution itself as both a political and symbolic order resulted in Communism's portrayal as a messianic faith, formally known as communist messianism. Portrayals of notable revolutionary figures such as Lenin were done in iconographic methods, equating them similarly to religious figures, though religion itself was banned in the USSR and groups such as the Russian Orthodox Church were persecuted.

The revolution ultimately led to the establishment of the future Soviet Union as an ideocracy; however, the establishment of such a state came as an ideological paradox, as Marx's ideals of how a socialist state ought to be created were based on the formation being natural and not artificially incited (i.e. by means of revolution). Leon Trotsky said that the goal of socialism in Russia would not be realized without the success of the world revolution. A revolutionary wave caused by the Russian Revolution lasted until 1923, but despite initial hopes for success in the German Revolution of 1918–19, the short-lived Hungarian Soviet Republic, and others like it, no other Marxist movement at the time succeeded in keeping power in its hands.

This issue is subject to conflicting views on communist history by various Marxist groups and parties. Joseph Stalin later rejected this idea, stating that socialism was possible in one country.

The confusion regarding Stalin's position on the issue stems from the fact that, after Lenin's death in 1924, he successfully used Lenin's argument – the argument that socialism's success needs the support of workers of other countries in order to happen – to defeat his competitors within the party by accusing them of betraying Lenin and, therefore, the ideals of the October Revolution.

The Russian Revolution inspired other communist movements around the world in regions such as South Asia, Southeast Asia, and Latin America.

The Chinese Communist Revolution began in 1946 and was part of the ongoing Chinese Civil War. Marx had envisioned European revolutions to be intertwined with Asian revolutions in the mid-19th century with his 1853 "New York Tribune" article, "Revolution in China and Europe," in which he references the Chinese as people in "revolutionary convulsion," brought about by British economic control. The May Fourth Movement is considered a turning point where Communism took root in Chinese society, especially among intellectuals. China was officially made a communist country on 1 October 1949, resulting in the establishment of the People's Republic of China (which still remains to this day) with Chairman Mao Zedong at its head. China's current leaders retain that Mao "developed the theory of revolutionary socialism" whilst reformer Deng Xiopeng "developed the theory of building socialism with Chinese characteristics."

Cuba experienced its own communist revolution as well, known as the Cuban Revolution, which began in July 1953 under the leadership of revolutionary Fidel Castro. Castro's 26th of July Movement and Cuban Revolution followed in the footsteps of the Sergeant's Revolt in Cuba in 1933, similarly to how the 1905 Revolution in Russia preceded the October Revolution. Castro's movement sought "political democracy, political and economic nationalism, agrarian reform, industrialization, social security, and education." Similarly to the October Revolution, the Cuban Revolution removed a more traditional, hierarchical regime with the aim of establishing greater overall equality, specifically in the removal of former authoritarian president Fulgencio Batista. Cuba's revolution contributed to escalating tensions between the United States and USSR during Cold War, such as the CIA's failed Bay of Pigs Invasion by Cuban exiles in April 1961, and the Cuban Missile Crisis in October 1962. Today, Cuba is moving more towards Capitalism and a free-market economy, as the Center for Democracy in the Americas (CDA) believes Castro's policies during his rule fostered "an acceptance that market forces can play a role in economic policy and that economic growth must be the central criterion to judge economic success."

The August Revolution took place on 14 August 1945, led by revolutionary leader Ho Chi Minh with the aid of his Viet Minh. During the Second World War, the French and Japanese fascists in Indochina (now known as Southeast Asia) began to experience significant resistance to their colonial rule. Due to the fact that both France and Japan were engaged in World War II, the Vietnamese people realized an opportunity to engage in an uprising, resulting in the bloody August Insurrection, ending colonial rule in Vietnam. Marxism was manifested in Vietnam as early as the Spring of 1925 when the Vietnamese Revolutionary Youth League was established, with the league being described as "first truly Marxist organization in Indochina" The domino effect caused more concern among Western countries in regards to Communism in Southeast Asia. One interpretation of the United States' involvement in the Vietnam War is "America had lost a guerrilla war in Asia, a loss of caused by failure to appreciate the nuances of counterinsurgency war." Since the Fall of Saigon on 30 April 1975, Vietnam has remained a communist country.

Few events in historical research have been as conditioned by political influences as the October Revolution. The historiography of the Revolution generally divides into three camps: the Soviet-Marxist view, the Western-Totalitarian view, and the Revisionist view. Since the fall of Communism (and the USSR) in Russia in 1991, the Western-Totalitarian view has again become dominant and the Soviet-Marxist view has practically vanished. While the Soviet-Marxist view has been largely discredited, an "anti-Stalinist" version of it attempts to draw a distinction between the "Lenin period" (1917-1923) and the "Stalin period" (1923-1953) 

A Lenin biographer, Robert Service, states he "laid the foundations of dictatorship and lawlessness. Lenin had consolidated the principle of state penetration of the whole society, its economy and its culture. Lenin had practised terror and advocated revolutionary amoralism."

"Dates are correct for the Julian calendar, which was used in Russia until 1918. It was 12 days behind the Gregorian calendar during the 19th century and thirteen days behind it during the 20th century."
George Orwell's classic novella "Animal Farm" is an allegory of the Russian Revolution and its aftermath. It describes the dictator Stalin as a big Berkshire boar named, "Napoleon." Trotsky is represented by a pig called Snowball who is a brilliant talker and makes magnificent speeches. However, Napoleon overthrows Snowball as Stalin overthrew Trotsky and Napoleon takes over the farm the animals live on. Napoleon becomes a tyrant and uses force and propaganda to oppress the animals, while culturally teaching them that they are free.

The Russian Revolution has been portrayed in or served as backdrop for many films. Among them, in order of release date:


The Russian Revolution has been used as a direct backdrop for select video games. Among them, in order of release date:









</doc>
<doc id="25764" url="https://en.wikipedia.org/wiki?curid=25764" title="Raven Software">
Raven Software

Raven Software is an American video game developer based in Wisconsin and founded in 1988. In 1997, Raven made an exclusive publishing deal with Activision and was subsequently acquired by them. After the acquisition, many of the studio's original developers, largely responsible for creating the "Heretic" and "" games, left to form Human Head Studios.

Raven Software was founded in 1988 by brothers Brian and Steve Raffel. The company was independent until 1997 when it was acquired by Activision.

Raven has a history of working with id Software, who were briefly located on the same street. They used id's engines for many of their games, such as "Heretic" in 1994. They took over development of id's "Quake" franchise for "Quake 4" and the 2009 iteration of id's "Wolfenstein" series.

The company started with three development teams. In August 2009 following poor performance and possible over-budget of "Wolfenstein," the company made a major layoff of 30-35 staff, leaving two development teams. This was reduced to one after more layoffs in October 2010, after delays with "Singularity;" as many as 40 staff were released. Following the layoffs, Raven has been focused on assisting with the "Call of Duty" series ever since.

In 2012, Raven began hiring employees for a game, and were announced as collaborating with Infinity Ward on "" in May 2013.

On April 3, 2013 following the closure of LucasArts, Raven Software released the source code for "" and "" on Sourceforge.

As of April 2014, the company is the lead developer of the free-to-play Chinese "Call of Duty" title, "". The company also remastered "," titled "".


</doc>
<doc id="25765" url="https://en.wikipedia.org/wiki?curid=25765" title="RNA world">
RNA world

The RNA world is a hypothetical stage in the evolutionary history of life on Earth, in which self-replicating RNA molecules proliferated before the evolution of DNA and proteins. The term also refers to the hypothesis that posits the existence of this stage.

Alexander Rich first proposed the concept of the RNA world in 1962, and Walter Gilbert coined the term in 1986. Alternative chemical paths to life have been proposed, and RNA-based life may not have been the first life to exist. Even so, the evidence for an RNA world is strong enough that the hypothesis has gained wide acceptance. The concurrent formation of all four RNA building blocks further strengthened the hypothesis.

Like DNA, RNA can store and replicate genetic information; like protein enzymes, RNA enzymes (ribozymes) can catalyze (start or accelerate) chemical reactions that are critical for life. One of the most critical components of cells, the ribosome, is composed primarily of RNA. Ribonucleotide moieties in many coenzymes, such as Acetyl-CoA, NADH, FADH and F420, may be surviving remnants of covalently bound coenzymes in an RNA world.

Although RNA is fragile, some ancient RNAs may have evolved the ability to methylate other RNAs to protect them.

If the RNA world existed, it was probably followed by an age characterized by the evolution of ribonucleoproteins (RNP world), which in turn ushered in the era of DNA and longer proteins. DNA has better stability and durability than RNA; this may explain why it became the predominant storage molecule.
Protein enzymes may have come to replace RNA-based ribozymes as biocatalysts because their greater abundance and diversity of monomers makes them more versatile. As some co-factors contain both nucleotide and amino-acid characteristics, it may be that amino acids, peptides and finally proteins initially were co-factors for ribozymes.

One of the challenges in studying abiogenesis is that the system of reproduction and metabolism utilized by all extant life involves three distinct types of interdependent macromolecules (DNA, RNA, and protein). This suggests that life could not have arisen in its current form, which has led researchers to hypothesize mechanisms whereby the current system might have arisen from a simpler precursor system. The concept of RNA as a primordial molecule can be found in papers by Francis Crick and Leslie Orgel, as well as in Carl Woese's 1967 book "The Genetic Code". In 1962, the molecular biologist Alexander Rich posited much the same idea in an article he contributed to a volume issued in honor of Nobel-laureate physiologist Albert Szent-Györgyi. Hans Kuhn in 1972 laid out a possible process by which the modern genetic system might have arisen from a nucleotide-based precursor, and this led Harold White in 1976 to observe that many of the cofactors essential for enzymatic function are either nucleotides or could have been derived from nucleotides. He proposed that these nucleotide cofactors represent "fossils of nucleic acid enzymes". The phrase "RNA World" was first used by Nobel laureate Walter Gilbert in 1986, in a commentary on how recent observations of the catalytic properties of various forms of RNA fit with this hypothesis.

In November 2019, scientists reported detecting, for the first time, sugar molecules, including ribose, in meteorites, suggesting that chemical processes on asteroids can produce some fundamentally essential bio-ingredients important to life, and supporting the notion of an RNA world prior to a DNA-based origin of life on Earth, and possibly, as well, the notion of panspermia.

The properties of RNA make the idea of the RNA world hypothesis conceptually plausible, though its general acceptance as an explanation for the origin of life requires further evidence. RNA is known to form efficient catalysts and its similarity to DNA makes clear its ability to store information. Opinions differ, however, as to whether RNA constituted the first autonomous self-replicating system or was a derivative of a still-earlier system. One version of the hypothesis is that a different type of nucleic acid, termed "pre-RNA", was the first one to emerge as a self-reproducing molecule, to be replaced by RNA only later. On the other hand, the discovery in 2009 that activated pyrimidine ribonucleotides can be synthesized under plausible prebiotic conditions suggests that it is premature to dismiss the RNA-first scenarios. Suggestions for 'simple' "pre-RNA" nucleic acids have included peptide nucleic acid (PNA), threose nucleic acid (TNA) or glycol nucleic acid (GNA). Despite their structural simplicity and possession of properties comparable with RNA, the chemically plausible generation of "simpler" nucleic acids under prebiotic conditions has yet to be demonstrated.

RNA enzymes, or ribozymes, are found in today's DNA-based life and could be examples of living fossils. Ribozymes play vital roles, such as that of the ribosome, an RNA-protein complex responsible for protein synthesis. Many other ribozyme functions exist; for example, the hammerhead ribozyme performs self-cleavage and an RNA polymerase ribozyme can synthesize a short RNA strand from a primed RNA template.

Among the enzymatic properties important for the beginning of life are:

RNA is a very similar molecule to DNA, with only two major chemical differences (the backbone of RNA uses ribose instead of deoxyribose and its nucleobases include uracil instead of thymine). The overall structure of RNA and DNA are immensely similar—one strand of DNA and one of RNA can bind to form a double helical structure. This makes the storage of information in RNA possible in a very similar way to the storage of information in DNA. However, RNA is less stable, being more prone to hydrolysis due to the presence of a hydroxyl group at the ribose 2' position.

The major difference between RNA and DNA is the presence of a hydroxyl group at the 2'-position of the ribose sugar in RNA (illustration, right). This group makes the molecule less stable because, when not constrained in a double helix, the 2' hydroxyl can chemically attack the adjacent phosphodiester bond to cleave the phosphodiester backbone. The hydroxyl group also forces the ribose into the C3'-"endo" sugar conformation unlike the C2'-"endo" conformation of the deoxyribose sugar in DNA. This forces an RNA double helix to change from a B-DNA structure to one more closely resembling A-DNA.

RNA also uses a different set of bases than DNA—adenine, guanine, cytosine and uracil, instead of adenine, guanine, cytosine and thymine. Chemically, uracil is similar to thymine, differing only by a methyl group, and its production requires less energy. In terms of base pairing, this has no effect. Adenine readily binds uracil or thymine. Uracil is, however, one product of damage to cytosine that makes RNA particularly susceptible to mutations that can replace a GC base pair with a GU (wobble) or AU base pair.

RNA is thought to have preceded DNA, because of their ordering in the biosynthetic pathways. The deoxyribonucleotides used to make DNA are made from ribonucleotides, the building blocks of RNA, by removing the 2'-hydroxyl group. As a consequence a cell must have the ability to make RNA before it can make DNA.

The chemical properties of RNA make large RNA molecules inherently fragile, and they can easily be broken down into their constituent nucleotides through hydrolysis. These limitations do not make use of RNA as an information storage system impossible, simply energy intensive (to repair or replace damaged RNA molecules) and prone to mutation. While this makes it unsuitable for current 'DNA optimised' life, it may have been acceptable for more primitive life.

Riboswitches have been found to act as regulators of gene expression, particularly in bacteria, but also in plants and archaea. Riboswitches alter their secondary structure in response to the binding of a metabolite. This change in structure can result in the formation or disruption of a terminator, truncating or permitting transcription respectively. Alternatively, riboswitches may bind or occlude the Shine-Dalgarno sequence, affecting translation. It has been suggested that these originated in an RNA-based world. In addition, RNA thermometers regulate gene expression in response to temperature changes.

The RNA world hypothesis is supported by RNA's ability to store, transmit, and duplicate genetic information, as DNA does. RNA can act as a ribozyme, a special type of enzyme. Because it can perform the tasks of both DNA and enzymes, RNA is believed to have once been capable of supporting independent life forms. Some viruses use RNA as their genetic material, rather than DNA. Further, while nucleotides were not found in experiments based on Miller-Urey experiment, their formation in prebiotically plausible conditions was reported in 2009; the purine base known as adenine is merely a pentamer of hydrogen cyanide. Experiments with basic ribozymes, like Bacteriophage Qβ RNA, have shown that simple self-replicating RNA structures can withstand even strong selective pressures (e.g., opposite-chirality chain terminators).

Since there were no known chemical pathways for the abiogenic synthesis of nucleotides from pyrimidine nucleobases cytosine and uracil under prebiotic conditions, it is thought by some that nucleic acids did not contain these nucleobases seen in life's nucleic acids. The nucleoside cytosine has a half-life in isolation of 19 days at and 17,000 years in freezing water, which some argue is too short on the geologic time scale for accumulation. Others have questioned whether ribose and other backbone sugars could be stable enough to find in the original genetic material, and have raised the issue that all ribose molecules would have had to be the same enantiomer, as any nucleotide of the wrong chirality acts as a chain terminator.

Pyrimidine ribonucleosides and their respective nucleotides have been prebiotically synthesised by a sequence of reactions that by-pass free sugars and assemble in a stepwise fashion by including nitrogenous and oxygenous chemistries. In a series of publications, John Sutherland and his team at the School of Chemistry, University of Manchester, have demonstrated high yielding routes to cytidine and uridine ribonucleotides built from small 2 and 3 carbon fragments such as glycolaldehyde, glyceraldehyde or glyceraldehyde-3-phosphate, cyanamide and cyanoacetylene. One of the steps in this sequence allows the isolation of enantiopure ribose aminooxazoline if the enantiomeric excess of glyceraldehyde is 60% or greater, of possible interest towards biological homochirality. This can be viewed as a prebiotic purification step, where the said compound spontaneously crystallised out from a mixture of the other pentose aminooxazolines. Aminooxazolines can react with cyanoacetylene in a mild and highly efficient manner, controlled by inorganic phosphate, to give the cytidine ribonucleotides. Photoanomerization with UV light allows for inversion about the 1' anomeric centre to give the correct beta stereochemistry; one problem with this chemistry is the selective phosphorylation of alpha-cytidine at the 2' position. However, in 2009, they showed that the same simple building blocks allow access, via phosphate controlled nucleobase elaboration, to 2',3'-cyclic pyrimidine nucleotides directly, which are known to be able to polymerise into RNA. Organic chemist Donna Blackmond described this finding as "strong evidence" in favour of the RNA world. However, John Sutherland said that while his team's work suggests that nucleic acids played an early and central role in the origin of life, it did not necessarily support the RNA world hypothesis in the strict sense, which he described as a "restrictive, hypothetical arrangement".

The Sutherland group's 2009 paper also highlighted the possibility for the photo-sanitization of the pyrimidine-2',3'-cyclic phosphates. A potential weakness of these routes is the generation of enantioenriched glyceraldehyde, or its 3-phosphate derivative (glyceraldehyde prefers to exist as its keto tautomer dihydroxyacetone).

On August 8, 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of RNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. In 2017, a numerical model suggests that the RNA world may have emerged in warm ponds on the early Earth, and that meteorites were a plausible and probable source of the RNA building blocks (ribose and nucleic acids) to these environments. On August 29, 2012, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Because glycolaldehyde is needed to form RNA, this finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.

Nucleotides are the fundamental molecules that combine in series to form RNA. They consist of a nitrogenous base attached to a sugar-phosphate backbone. RNA is made of long stretches of specific nucleotides arranged so that their sequence of bases carries information. The RNA world hypothesis holds that in the primordial soup (or sandwich), there existed free-floating nucleotides. These nucleotides regularly formed bonds with one another, which often broke because the change in energy was so low. However, certain sequences of base pairs have catalytic properties that lower the energy of their chain being created, enabling them to stay together for longer periods of time. As each chain grew longer, it attracted more matching nucleotides faster, causing chains to now form faster than they were breaking down. 

These chains have been proposed by some as the first, primitive forms of life. In an RNA world, different sets of RNA strands would have had different replication outputs, which would have increased or decreased their frequency in the population, i.e. natural selection. As the fittest sets of RNA molecules expanded their numbers, novel catalytic properties added by mutation, which benefitted their persistence and expansion, could accumulate in the population. Such an autocatalytic set of ribozymes, capable of self replication in about an hour, has been identified. It was produced by molecular competition ("in vitro" evolution) of candidate enzyme mixtures.

Competition between RNA may have favored the emergence of cooperation between different RNA chains, opening the way for the formation of the first protocell. Eventually, RNA chains developed with catalytic properties that help amino acids bind together (a process called peptide-bonding). These amino acids could then assist with RNA synthesis, giving those RNA chains that could serve as ribozymes the selective advantage. The ability to catalyze one step in protein synthesis, aminoacylation of RNA, has been demonstrated in a short (five-nucleotide) segment of RNA.

In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under conditions found only in outer space, using starting chemicals, like pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), may have been formed in giant red stars or in interstellar dust and gas clouds, according to the scientists.

In 2018, researchers at Georgia Institute of Technology identified three molecular candidates for the bases that might have formed an earliest version of proto-RNA: barbituric acid, melamine, and 2,4,6-triaminopyrimidine (TAP). These three molecules are simpler versions of the four bases in current RNA, which could have been present in larger amounts and could still be forwards compatibile with them, but may have been discarded by evolution in exchange for more optimal base pairs. Specifically, TAP can form nucleotides with a large range of sugars. Both TAP and melamine base pair with batbituric acid. All three spontaneously form nucleotides with ribose.

One of the problems with the RNA world hypothesis is to discover the pathway by which RNA became upgraded to the DNA system. Geoffrey Diemer and Ken Stedman, at Portland State University in Oregon, may have found a solution. While conducting a survey of viruses in a hot acidic lake in Lassen Volcanic National Park, California, they uncovered evidence that a simple DNA virus had acquired a gene from a completely unrelated RNA-based virus. Virologist Luis Villareal of the University of California Irvine also suggests that viruses capable of converting an RNA-based gene into DNA and then incorporating it into a more complex DNA-based genome might have been common in the Virus world during the RNA to DNA transition some 4 billion years ago. This finding bolsters the argument for the transfer of information from the RNA world to the emerging DNA world before the emergence of the last universal common ancestor. From the research, the diversity of this virus world is still with us.

Additional evidence supporting the concept of an RNA world has resulted from research on viroids, the first representatives of a novel domain of "subviral pathogens".
Viroids are mostly plant pathogens, which consist of short stretches (a few hundred nucleobases) of highly complementary, circular, single-stranded, and non-coding RNA without a protein coat. Compared with other infectious plant pathogens, viroids are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection are about 2,000 nucleobases long.

In 1989, Diener proposed that, based on their characteristic properties, viroids are more plausible "living relics" of the RNA world than are introns or other RNAs then so considered. If so, viroids have attained potential significance beyond plant pathology to evolutionary biology, by representing the most plausible macromolecules known capable of explaining crucial intermediate steps in the evolution of life from inanimate matter (see: abiogenesis).

Apparently, Diener's hypothesis lay dormant until 2014, when Flores et al. published a review paper, in which Diener's evidence supporting his hypothesis was summarized. In the same year, a New York Times science writer published a popularized version of Diener's proposal, in which, however, he mistakenly credited Flores et al. with the hypothesis' original conception.

Pertinent viroid properties listed in 1989 are:


The existence, in extant cells, of RNAs with molecular properties predicted for RNAs of the RNA World constitutes an additional argument supporting the RNA World hypothesis.

Eigen "et al". and Woese proposed that the genomes of early protocells were composed of single-stranded RNA, and that individual genes corresponded to separate RNA segments, rather than being linked end-to-end as in present-day DNA genomes. A protocell that was haploid (one copy of each RNA gene) would be vulnerable to damage, since a single lesion in any RNA segment would be potentially lethal to the protocell (e.g. by blocking replication or inhibiting the function of an essential gene).

Vulnerability to damage could be reduced by maintaining two or more copies of each RNA segment in each protocell, i.e. by maintaining diploidy or polyploidy. Genome redundancy would allow a damaged RNA segment to be replaced by an additional replication of its homolog. However, for such a simple organism, the proportion of available resources tied up in the genetic material would be a large fraction of the total resource budget. Under limited resource conditions, the protocell reproductive rate would likely be inversely related to ploidy number. The protocell's fitness would be reduced by the costs of redundancy. Consequently, coping with damaged RNA genes while minimizing the costs of redundancy would likely have been a fundamental problem for early protocells.

A cost-benefit analysis was carried out in which the costs of maintaining redundancy were balanced against the costs of genome damage. This analysis led to the conclusion that, under a wide range of circumstances, the selected strategy would be for each protocell to be haploid, but to periodically fuse with another haploid protocell to form a transient diploid. The retention of the haploid state maximizes the growth rate. The periodic fusions permit mutual reactivation of otherwise lethally damaged protocells. If at least one damage-free copy of each RNA gene is present in the transient diploid, viable progeny can be formed. For two, rather than one, viable daughter cells to be produced would require an extra replication of the intact RNA gene homologous to any RNA gene that had been damaged prior to the division of the fused protocell. The cycle of haploid reproduction, with occasional fusion to a transient diploid state, followed by splitting to the haploid state, can be considered to be the sexual cycle in its most primitive form. In the absence of this sexual cycle, haploid protocells with damage in an essential RNA gene would simply die.

This model for the early sexual cycle is hypothetical, but it is very similar to the known sexual behavior of the segmented RNA viruses, which are among the simplest organisms known. Influenza virus, whose genome consists of 8 physically separated single-stranded RNA segments, is an example of this type of virus. In segmented RNA viruses, "mating" can occur when a host cell is infected by at least two virus particles. If these viruses each contain an RNA segment with a lethal damage, multiple infection can lead to reactivation providing that at least one undamaged copy of each virus gene is present in the infected cell. This phenomenon is known as "multiplicity reactivation". Multiplicity reactivation has been reported to occur in influenza virus infections after induction of RNA damage by UV-irradiation, and ionizing radiation.

Patrick Forterre has been working on a novel hypothesis, called "three viruses, three domains": that viruses were instrumental in the transition from RNA to DNA and the evolution of Bacteria, Archaea, and Eukaryota. He believes the last universal common ancestor was RNA-based and evolved RNA viruses. Some of the viruses evolved into DNA viruses to protect their genes from attack. Through the process of viral infection into hosts the three domains of life evolved. Another interesting proposal is the idea that RNA synthesis might have been driven by temperature gradients, in the process of thermosynthesis.
Single nucleotides have been shown to catalyze organic reactions.

Steven Benner has argued that chemical conditions on the planet Mars, such as the presence of boron, molybdenum and oxygen, may have been better for initially producing RNA molecules than those on Earth. If so, life-suitable molecules, originating on Mars, may have later migrated to Earth via panspermia or similar process.

The hypothesized existence of an RNA world does not exclude a "Pre-RNA world", where a metabolic system based on a different nucleic acid is proposed to pre-date RNA. A candidate nucleic acid is peptide nucleic acid (PNA), which uses simple peptide bonds to link nucleobases. PNA is more stable than RNA, but its ability to be generated under prebiological conditions has yet to be demonstrated experimentally.

Threose nucleic acid (TNA) has also been proposed as a starting point, as has glycol nucleic acid (GNA), and like PNA, also lack experimental evidence for their respective abiogenesis.

An alternative — or complementary — theory of RNA origin is proposed in the PAH world hypothesis, whereby polycyclic aromatic hydrocarbons (PAHs) mediate the synthesis of RNA molecules. PAHs are the most common and abundant of the known polyatomic molecules in the visible Universe, and are a likely constituent of the primordial sea. PAHs and fullerenes (also implicated in the origin of life) have been detected in nebulae.

The iron-sulfur world theory proposes that simple metabolic processes developed before genetic materials did, and these energy-producing cycles catalyzed the production of genes.

Some of the difficulties of producing the precursors on earth are bypassed by another alternative or complementary theory for their origin, panspermia. It discusses the possibility that the earliest life on this planet was carried here from somewhere else in the galaxy, possibly on meteorites similar to the Murchison meteorite. This does not invalidate the concept of an RNA world, but posits that this world or its precursors originated not on Earth but rather another, probably older, planet.

There are hypotheses that are in direct conflict to the RNA world hypothesis. The relative chemical complexity of the nucleotide and the unlikelihood of it spontaneously arising, along with the limited number of combinations possible among four base forms, as well as the need for RNA polymers of some length before seeing enzymatic activity, have led some to reject the RNA world hypothesis in favor of a metabolism-first hypothesis, where the chemistry underlying cellular function arose first, along with the ability to replicate and facilitate this metabolism.

Another proposal is that the dual-molecule system we see today, where a nucleotide-based molecule is needed to synthesize protein, and a peptide-based (protein) molecule is needed to make nucleic acid polymers, represents the original form of life. This theory is called RNA-peptide coevolution, or the Peptide-RNA world, and offers a possible explanation for the rapid evolution of high-quality replication in RNA (since proteins are catalysts), with the disadvantage of having to postulate the coincident formation of two complex molecules, an enzyme (from peptides) and a RNA (from nucleotides). In this Peptide-RNA World scenario, RNA would have contained the instructions for life, while peptides (simple protein enzymes) would have accelerated key chemical reactions to carry out those instructions. The study leaves open the question of exactly how those primitive systems managed to replicate themselves — something neither the RNA World hypothesis nor the Peptide-RNA World theory can yet explain, unless polymerases (enzymes that rapidly assemble the RNA molecule) played a role.

A research project completed in March 2015 by the Sutherland group found that a network of reactions beginning with hydrogen cyanide and hydrogen sulfide, in streams of water irradiated by UV light, could produce the chemical components of proteins and lipids, alongside those of RNA. The researchers used the term "cyanosulfidic" to describe this network of reactions. In November 2017, a team at the Scripps Research Institute identified reactions involving the compound diamidophosphate which could have linked the chemical components into short peptide and lipid chains as well as short RNA-like chains of nucleotides.

The RNA world hypothesis, if true, has important implications for the definition of life. For most of the time that followed Watson and Crick's elucidation of DNA structure in 1953, life was largely defined in terms of DNA and proteins: DNA and proteins seemed the dominant macromolecules in the living cell, with RNA only aiding in creating proteins from the DNA blueprint.

The RNA world hypothesis places RNA at center-stage when life originated. The RNA world hypothesis is supported by the observations that ribosomes are ribozymes: the catalytic site is composed of RNA, and proteins hold no major structural role and are of peripheral functional importance. This was confirmed with the deciphering of the 3-dimensional structure of the ribosome in 2001. Specifically, peptide bond formation, the reaction that binds amino acids together into proteins, is now known to be catalyzed by an adenine residue in the rRNA.

RNAs are known to play roles in other cellular catalytic processes, specifically in the targeting of enzymes to specific RNA sequences. In eukaryotes, the processing of pre-mRNA and RNA editing take place at sites determined by the base pairing between the target RNA and RNA constituents of small nuclear ribonucleoproteins (snRNPs). Such enzyme targeting is also responsible for gene down regulation though RNA interference (RNAi), where an enzyme-associated guide RNA targets specific mRNA for selective destruction. Likewise, in eukaryotes the maintenance of telomeres involves copying of an RNA template that is a constituent part of the telomerase ribonucleoprotein enzyme. Another cellular organelle, the vault, includes a ribonucleoprotein component, although the function of this organelle remains to be elucidated.





</doc>
<doc id="25766" url="https://en.wikipedia.org/wiki?curid=25766" title="Ribosome">
Ribosome

Ribosomes () (named also Palade's corpuscles) comprise a complex macromolecular machine, found within all living cells, that serves as the site of biological protein synthesis (translation). Ribosomes link amino acids together in the order specified by messenger RNA (mRNA) molecules. Ribosomes consist of two major components: the small ribosomal subunits, which read the mRNA, and the large subunits, which join amino acids to form a polypeptide chain. Each subunit consists of one or more ribosomal RNA (rRNA) molecules and a variety of ribosomal proteins (r-protein or rProtein). The ribosomes and associated molecules are also known as the "translational apparatus".

The sequence of DNA, which encodes the sequence of the amino acids in a protein, is copied into a messenger RNA chain. It may be copied many times into RNA chains. Ribosomes can bind to a messenger RNA chain and use its sequence for determining the correct sequence of amino acids for generating a given protein. Amino acids are selected and collected and carried to the ribosome by transfer RNA (tRNA) molecules, which enter one part of the ribosome and bind to the messenger RNA chain. It is during this binding that the correct translation of nucleic acid sequence to amino acid sequence occurs. For each coding triplet in the messenger RNA there is a distinct transfer RNA that matches and which carries the correct amino acid for that coding triplet. The attached amino acids are then linked together by another part of the ribosome. Once the protein is produced, it can then fold to produce a specific functional three-dimensional structure although during synthesis some proteins start folding into their correct form.

A ribosome is made from complexes of RNAs and proteins and is therefore a ribonucleoprotein. Each ribosome is divided into two subunits:
When a ribosome finishes reading an mRNA molecule, these two subunits split apart. Ribosomes are ribozymes, because the catalytic peptidyl transferase activity that links amino acids together is performed by the ribosomal RNA. Ribosomes are often associated with the intracellular membranes that make up the rough endoplasmic reticulum.

Ribosomes from bacteria, archaea and eukaryotes in the three-domain system, resemble each other to a remarkable degree, evidence of a common origin. They differ in their size, sequence, structure, and the ratio of protein to RNA. The differences in structure allow some antibiotics to kill bacteria by inhibiting their ribosomes, while leaving human ribosomes unaffected. In bacteria and archaea, more than one ribosome may move along a single mRNA chain at one time, each "reading" its sequence and producing a corresponding protein molecule.

The mitochondrial ribosomes of eukaryotic cells, are produced from mitochondrial genes, and functionally resemble many features of those in bacteria, reflecting the likely evolutionary origin of mitochondria.

Ribosomes were first observed in the mid-1950s by Romanian-American cell biologist George Emil Palade, using an electron microscope, as dense particles or granules. The term "ribosome" was proposed by scientist Richard B. Roberts in the end of 1950s:
Albert Claude, Christian de Duve, and George Emil Palade were jointly awarded the Nobel Prize in Physiology or Medicine, in 1974, for the discovery of the ribosome. The Nobel Prize in Chemistry 2009 was awarded to Venkatraman Ramakrishnan, Thomas A. Steitz and Ada E. Yonath for determining the detailed structure and mechanism of the ribosome.

The ribosome is a highly complex cellular machine. It is largely made up of specialized RNA known as ribosomal RNA (rRNA) as well as dozens of distinct proteins (the exact number varies slightly between species). The ribosomal proteins and rRNAs are arranged into two distinct ribosomal pieces of different size, known generally as the large and small subunit of the ribosome. Ribosomes consist of two subunits that fit together (Figure 2) and work as one to translate the mRNA into a polypeptide chain during protein synthesis (Figure 1). Because they are formed from two subunits of non-equal size, they are slightly longer in the axis than in diameter.

Prokaryotic ribosomes are around 20 nm (200 Å) in diameter and are composed of 65% rRNA and 35% ribosomal proteins. Eukaryotic ribosomes are between 25 and 30 nm (250–300 Å) in diameter with an rRNA-to-protein ratio that is close to 1. Crystallographic work has shown that there are no ribosomal proteins close to the reaction site for polypeptide synthesis. This suggests that the protein components of ribosomes do not directly participate in peptide bond formation catalysis, but rather that these proteins act as a scaffold that may enhance the ability of rRNA to synthesize protein (See: Ribozyme).

The ribosomal subunits of bacteria and eukaryotes are quite similar.

The unit of measurement used to describe the ribosomal subunits and the rRNA fragments is the Svedberg unit, a measure of the rate of sedimentation in centrifugation rather than size. This accounts for why fragment names do not add up: for example, bacterial 70S ribosomes are made of 50S and 30S subunits.

Bacteria have 70S ribosomes, each consisting of a small (30S) and a large (50S) subunit. "E. coli", for example, has a 16S RNA subunit (consisting of 1540 nucleotides) that is bound to 21 proteins. The large subunit is composed of a 5S RNA subunit (120 nucleotides), a 23S RNA subunit (2900 nucleotides) and 31 proteins.
Affinity label for the tRNA binding sites on the "E. coli" ribosome allowed the identification of A and P site proteins most likely associated with the peptidyltransferase activity; labelled proteins are L27, L14, L15, L16, L2; at least L27 is located at the donor site, as shown by E. Collatz and A.P. Czernilofsky. Additional research has demonstrated that the S1 and S21 proteins, in association with the 3′-end of 16S ribosomal RNA, are involved in the initiation of translation.

Eukaryotes have 80S ribosomes located in their cytosol, each consisting of a small (40S) and large (60S) subunit. Their 40S subunit has an 18S RNA (1900 nucleotides) and 33 proteins. The large subunit is composed of a 5S RNA (120 nucleotides), 28S RNA (4700 nucleotides), a 5.8S RNA (160 nucleotides) subunits and 46 proteins.
During 1977, Czernilofsky published research that used affinity labeling to identify tRNA-binding sites on rat liver ribosomes. Several proteins, including L32/33, L36, L21, L23, L28/29 and L13 were implicated as being at or near the peptidyl transferase center.

In eukaryotes, ribosomes are present in mitochondria (sometimes called mitoribosomes) and in plastids such as chloroplasts (also called plastoribosomes). They also consist of large and small subunits bound together with proteins into one 70S particle. These ribosomes are similar to those of bacteria and these organelles are thought to have originated as symbiotic bacteria Of the two, chloroplastic ribosomes are closer to bacterial ones than mitochrondrial ones are. Many pieces of ribosomal RNA in the mitochrondria are shortened, and in the case of 5S rRNA, replaced by other structures in animals and fungi. In particular, "Leishmania tarentolae" has a minimalized set of mitochondrial rRNA.

The cryptomonad and chlorarachniophyte algae may contain a nucleomorph that resembles a vestigial eukaryotic nucleus. Eukaryotic 80S ribosomes may be present in the compartment containing the nucleomorph.

The differences between the bacterial and eukaryotic ribosomes are exploited by pharmaceutical chemists to create antibiotics that can destroy a bacterial infection without harming the cells of the infected person. Due to the differences in their structures, the bacterial 70S ribosomes are vulnerable to these antibiotics while the eukaryotic 80S ribosomes are not. Even though mitochondria possess ribosomes similar to the bacterial ones, mitochondria are not affected by these antibiotics because they are surrounded by a double membrane that does not easily admit these antibiotics into the organelle. The same cannot be said of chloroplasts, where antibiotic resistance in ribosomal proteins is a trait to be introduced as a marker in genetic engineering.

The various ribosomes share a core structure, which is quite similar despite the large differences in size. Much of the RNA is highly organized into various tertiary structural motifs, for example pseudoknots that exhibit coaxial stacking. The extra RNA in the larger ribosomes is in several long continuous insertions, such that they form loops out of the core structure without disrupting or changing it. All of the catalytic activity of the ribosome is carried out by the RNA; the proteins reside on the surface and seem to stabilize the structure.

The general molecular structure of the ribosome has been known since the early 1970s. In the early 2000s, the structure has been achieved at high resolutions, of the order of a few ångströms.

The first papers giving the structure of the ribosome at atomic resolution were published almost simultaneously in late 2000. The 50S (large prokaryotic) subunit was determined from the archaeon "Haloarcula marismortui" and the bacterium "Deinococcus radiodurans", and the structure of the 30S subunit was determined from "Thermus thermophilus". These structural studies were awarded the Nobel Prize in Chemistry in 2009. In May 2001 these coordinates were used to reconstruct the entire "T. thermophilus" 70S particle at 5.5 Å resolution.

Two papers were published in November 2005 with structures of the "Escherichia coli" 70S ribosome. The structures of a vacant ribosome were determined at 3.5 Å resolution using X-ray crystallography. Then, two weeks later, a structure based on cryo-electron microscopy was published, which depicts the ribosome at 11–15 Å resolution in the act of passing a newly synthesized protein strand into the protein-conducting channel.

The first atomic structures of the ribosome complexed with tRNA and mRNA molecules were solved by using X-ray crystallography by two groups independently, at 2.8 Å and at 3.7 Å. These structures allow one to see the details of interactions of the "Thermus thermophilus" ribosome with mRNA and with tRNAs bound at classical ribosomal sites. Interactions of the ribosome with long mRNAs containing Shine-Dalgarno sequences were visualized soon after that at 4.5–5.5 Å resolution.

In 2011, the first complete atomic structure of the eukaryotic 80S ribosome from the yeast "Saccharomyces cerevisiae" was obtained by crystallography. The model reveals the architecture of eukaryote-specific elements and their interaction with the universally conserved core. At the same time, the complete model of a eukaryotic 40S ribosomal structure in "Tetrahymena thermophila" was published and described the structure of the 40S subunit, as well as much about the 40S subunit's interaction with eIF1 during translation initiation. Similarly, the eukaryotic 60S subunit structure was also determined from "Tetrahymena thermophila" in complex with eIF6.

Ribosomes are minute particles consisting of RNA and associated proteins that function to synthesize proteins. Proteins are needed for many cellular functions such as repairing damage or directing chemical processes. Ribosomes can be found floating within the cytoplasm or attached to the endoplasmic reticulum. Basically, their main function is to convert genetic code into an amino acid sequence and to build protein polymers from amino acid monomers. 

Ribosomes act as catalysts in two extremely important biological processes called peptidyl transfer and peptidyl hydrolysis. The "PT center is responsible for producing protein bonds during protein elongation".

Ribosomes are the workplaces of protein biosynthesis, the process of translating mRNA into protein. The mRNA comprises a series of codons which are decoded by the ribosome so as to make the protein. Using the mRNA as a template, the ribosome traverses each codon (3 nucleotides) of the mRNA, pairing it with the appropriate amino acid provided by an aminoacyl-tRNA. Aminoacyl-tRNA contains a complementary anticodon on one end and the appropriate amino acid on the other. For fast and accurate recognition of the appropriate tRNA, the ribosome utilizes large conformational changes (conformational proofreading)
The small ribosomal subunit, typically bound to an aminoacyl-tRNA containing the first amino acid methionine, binds to an AUG codon on the mRNA and recruits the large ribosomal subunit. The ribosome contains three RNA binding sites, designated A, P and E. The A-site binds an aminoacyl-tRNA or termination release factors; the P-site binds a peptidyl-tRNA (a tRNA bound to the poly-peptide chain); and the E-site (exit) binds a free tRNA. Protein synthesis begins at a start codon AUG near the 5' end of the mRNA. mRNA binds to the P site of the ribosome first. The ribosome recognizes the start codon by using the Shine-Dalgarno sequence of the mRNA in prokaryotes and Kozak box in eukaryotes.

Although catalysis of the peptide bond involves the C2 hydroxyl of RNA's P-site adenosine in a proton shuttle mechanism, other steps in protein synthesis (such as translocation) are caused by changes in protein conformations. Since their catalytic core is made of RNA, ribosomes are classified as "ribozymes," and it is thought that they might be remnants of the RNA world.

In Figure 5, both ribosomal subunits (small and large) assemble at the start codon (towards the 5' end of the mRNA). The ribosome uses tRNA that matches the current codon (triplet) on the mRNA to append an amino acid to the polypeptide chain. This is done for each triplet on the mRNA, while the ribosome moves towards the 3' end of the mRNA. Usually in bacterial cells, several ribosomes are working parallel on a single mRNA, forming what is called a "polyribosome" or "polysome".

The ribosome is known to actively participate in the protein folding. The structures obtained in this way are usually identical to the ones obtained during protein chemical refolding, however, the pathways leading to the final product may be different. In some cases, the ribosome is crucial in obtaining the functional protein form. For example, one of the possible mechanisms of folding of the deeply knotted proteins relies on the ribosome pushing the chain through the attached loop.

Presence of a ribosome quality control protein Rqc2 is associated with mRNA-independent protein elongation. This elongation is a result of ribosomal addition (via tRNAs brought by Rqc2) of CAT tails": ribosomes extend the C-terminus of a stalled protein with random, translation-independent sequences of alanines and t"hreonines.

Ribosomes are classified as being either "free" or "membrane-bound".
Free and membrane-bound ribosomes differ only in their spatial distribution; they are identical in structure. Whether the ribosome exists in a free or membrane-bound state depends on the presence of an ER-targeting signal sequence on the protein being synthesized, so an individual ribosome might be membrane-bound when it is making one protein, but free in the cytosol when it makes another protein.

Ribosomes are sometimes referred to as organelles, but the use of the term "organelle" is often restricted to describing sub-cellular components that include a phospholipid membrane, which ribosomes, being entirely particulate, do not. For this reason, ribosomes may sometimes be described as "non-membranous organelles".

Free ribosomes can move about anywhere in the cytosol, but are excluded from the cell nucleus and other organelles. Proteins that are formed from free ribosomes are released into the cytosol and used within the cell. Since the cytosol contains high concentrations of glutathione and is, therefore, a reducing environment, proteins containing disulfide bonds, which are formed from oxidized cysteine residues, cannot be produced within it.

When a ribosome begins to synthesize proteins that are needed in some organelles, the ribosome making this protein can become "membrane-bound". In eukaryotic cells this happens in a region of the endoplasmic reticulum (ER) called the "rough ER". The newly produced polypeptide chains are inserted directly into the ER by the ribosome undertaking vectorial synthesis and are then transported to their destinations, through the secretory pathway. Bound ribosomes usually produce proteins that are used within the plasma membrane or are expelled from the cell via "exocytosis".

In bacterial cells, ribosomes are synthesized in the cytoplasm through the transcription of multiple ribosome gene operons. In eukaryotes, the process takes place both in the cell cytoplasm and in the nucleolus, which is a region within the cell nucleus. The assembly process involves the coordinated function of over 200 proteins in the synthesis and processing of the four rRNAs, as well as assembly of those rRNAs with the ribosomal proteins.

The ribosome may have first originated in an RNA world, appearing as a self-replicating complex that only later evolved the ability to synthesize proteins when amino acids began to appear. Studies suggest that ancient ribosomes constructed solely of rRNA could have developed the ability to synthesize peptide bonds. In addition, evidence strongly points to ancient ribosomes as self-replicating complexes, where the rRNA in the ribosomes had informational, structural, and catalytic purposes because it could have coded for tRNAs and proteins needed for ribosomal self-replication. Hypothetical cellular organisms with self-replicating RNA but without DNA are called ribocytes (or ribocells).

As amino acids gradually appeared in the RNA world under prebiotic conditions, their interactions with catalytic RNA would increase both the range and efficiency of function of catalytic RNA molecules. Thus, the driving force for the evolution of the ribosome from an ancient self-replicating machine into its current form as a translational machine may have been the selective pressure to incorporate proteins into the ribosome’s self-replicating mechanisms, so as to increase its capacity for self-replication.

Many textbooks suggest that there are only two kinds of ribosomes, namely prokaryotic and eukaryotic ribosomes. However, ribosomes are surprisingly heterogeneous with different compositions in different species. Heterogeneous ribosomes have different structures and thus different activities compared to typical ribosomes in major model organisms.

Heterogeneity in ribosome composition has been proposed to be involved in translational control of protein synthesis. Vincent Mauro and Gerald Edelman proposed the ribosome filter hypothesis to explain the regulatory functions of ribosomes. Emerging evidence has shown that specialized ribosomes specific to different cell populations can affect how genes are translated. Some ribosomal proteins exchange from the assembled complex with cytosolic copies suggesting that the structure of the "in vivo" ribosome can be modified without synthesizing an entire new ribosome.

A group of highly acidic ribosomal proteins (RPs), also known as P proteins, are known to be present on the 60S subunit in multiple copies in the ribosomes stalk and P proteins mediate selective translation. These P proteins can be found in yeast and mammalian cells. If P proteins are not present in yeast this can cause the yeast to have a cold-sensitive phenotype. If P proteins are not present in human cells, this could cause autophagy induction.

Certain ribosomal proteins are absolutely critical while others are not. For instance, Rpl28 and Rpl5 mutant flies are alive but have abnormally large wings. Rpl38 also appears to be critical in mammals only under very specific conditions: in mice Rpl38 is required for the translation of a subset of Hox mRNA and a mutation of Rpl38 lead to a homeotic transformation with a short tail.

Other ways heterogeneity can arise from post-translational modifications to RPs include acetylation and methylation in species like yeast, "Arabidopsis", and human cells. But there is no overall change in protein synthesis.

Modifications to core ribosomal proteins (RPs) can also give rise to the formation of heterogeneous ribosomes. For instance, in yeast, Rpl28 ubiquitination levels vary with the cell cycle. Ribosomes with the polyubiquitinated Rpl28 carry out protein synthesis at a higher rate "in vitro" compared with ribosomes with monoubiquitinated Rpl28.

Heterogeneous ribosomes can also arise from other factors binding to the ribosome surface. For example, ribosome-associated factor (RACK1) is so tightly associated with the ribosome that its binding is resistant to high-salt washes "in vitro". RACK1 is required for efficient translation of mRNA with short open reading frames.

Viral IRES (internal ribosome entry site) translations can also be mediated by specialized ribosomes. Specifically, 40S ribosomal units without RPS25 in yeast and mammalian cells are unable to begin translation from two viral IRESes, namely the hepatitis C virus IRES and the cricket paralysis virus (CrPV) intergenic region. The CrPV only needs RPS25 to begin translation and mediate ribosome recruitment. Usually if RPS25 is not present in a certain IRES, initiation from these IRESes can be defective.

Heterogeneity of ribosomal RNA modifications plays an important role in structural maintenance and/or function, such as modulating translation and most mRNA modifications are found in highly conserved regions. The most common rRNA modifications are pseudouridylation and 2’-O methylation of ribose.



</doc>
<doc id="25767" url="https://en.wikipedia.org/wiki?curid=25767" title="Real-time computing">
Real-time computing

In computer science, real-time computing (RTC), or reactive computing describes hardware and software systems subject to a "real-time constraint", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as "deadlines". The correctness of these types of systems depends on their temporal aspects as well as their functional aspects. Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually "guarantee" a response within any timeframe, although "typical" or "expected" response times may be given.

A real-time system has been described as one which "controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time". The term "real-time" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock, and in process control and enterprise systems to mean "without significant delay".

Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems, and real-time networks, each of which provide essential frameworks on which to build a real-time software application.

Systems used for many mission critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes on a vehicle, which must produce maximum deceleration but intermittently stop braking to prevent skidding. Real-time processing "fails" if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load.

The term "real-time" derives from its use in early simulation, in which a real-world process is simulated at a rate that matched that of the real process (now called real-time simulation to avoid ambiguity). Analog computers, most often, were capable of simulating at a much faster pace than real-time, a situation that could be just as dangerous as a slow simulation if it were not also recognized and accounted for. 

Minicomputers, particularly in the 1970s onwards, when built into dedicated embedded systems such as DOG scanners, increased the need for low-latency priority-driven responses to important interactions with incoming data and so operating systems such as Data General's RDOS (Real-Time Disk Operatings System) and RTOS with background and foreground scheduling as well as Digital Equipment Corporation's RT-11 date from this era. Background-foreground scheduling allowed low priority tasks CPU time when no foreground task needed to execute, and gave absolute priority within the foreground to threads/tasks with the highest priority. Real-time operating systems would also be used for time-sharing multiuser duties. For example, Data General Business Basic could run in the foreground or background of RDOG (and would introduce additional elements to the scheduling algorithm to make it more appropriate for people interacting via dumb terminals.

Once when the MOS Technology 6502 (used in the Commodore 64 and Apple II), and later when the Motorola 68000 (used in the Macintosh, Atari ST, and Commodore Amiga) were popular, anybody could use their home computer as a real-time system. The possibility to deactivate other interrupts allowed for hard-coded loops with defined timing, and the low interrupt latency allowed the implementation of a real-time operating system, giving the user interface and the disk drives lower priority than the real-time thread. Compared to these the programmable interrupt controller of the Intel CPUs (8086..80586) generates a very large latency and the Windows operating system is neither a real-time operating system nor does it allow a program to take over the CPU completely and use its own scheduler, without using native machine language and thus surpassing all interrupting Windows code. However, several coding libraries exist which offer real time capabilities in a high level language on a variety of operating systems, for example Java Real Time. The Motorola 68000 and subsequent family members (68010, 68020 etc.) also became popular with manufacturers of industrial control systems. This application area is one in which real-time control offers genuine advantages in terms of process performance and safety.

A system is said to be "real-time" if the total correctness of an operation depends not only upon its logical correctness, but also upon the time in which it is performed. Real-time systems, as well as their deadlines, are classified by the consequence of missing a deadline:

Thus, the goal of a "hard real-time system" is to ensure that all deadlines are met, but for "soft real-time systems" the goal becomes meeting a certain subset of deadlines in order to optimize some application-specific criteria. The particular criteria optimized depend on the application, but some typical examples include maximizing the number of deadlines met, minimizing the lateness of tasks and maximizing the number of high priority tasks meeting their deadlines.

Hard real-time systems are used when it is imperative that an event be reacted to within a strict deadline. Such strong guarantees are required of systems for which not reacting in a certain interval of time would cause great loss in some manner, especially damaging the surroundings physically or threatening human lives (although the strict definition is simply that missing the deadline constitutes failure of the system). For example, a car engine control system is a hard real-time system because a delayed signal may cause engine failure or damage. Other examples of hard real-time embedded systems include medical systems such as heart pacemakers and industrial process controllers. Hard real-time systems are typically found interacting at a low level with physical hardware, in embedded systems. Early video game systems such as the Atari 2600 and Cinematronics vector graphics had hard real-time requirements because of the nature of the graphics and timing hardware.

In the context of multitasking systems the scheduling policy is normally priority driven (pre-emptive schedulers). Other scheduling algorithms include earliest deadline first, which, ignoring the overhead of context switching, is sufficient for system loads of less than 100%. New overlay scheduling systems, such as an adaptive partition scheduler assist in managing large systems with a mixture of hard real-time and non real-time applications.

Soft real-time systems are typically used to solve issues of concurrent access and the need to keep a number of connected systems up-to-date through changing situations. An example can be software that maintains and updates the flight plans for commercial airliners: the flight plans must be kept reasonably current, but they can operate with the latency of a few seconds. Live audio-video systems are also usually soft real-time; violation of constraints results in degraded quality, but the system can continue to operate and also recover in the future using workload prediction and reconfiguration methodologies.

In a real-time digital signal processing (DSP) process, the analyzed (input) and generated (output) samples can be processed (or generated) continuously in the time it takes to input and output the same set of samples "independent" of the processing delay. It means that the processing delay must be bounded even if the processing continues for an unlimited time. That means that the mean processing time per sample, including overhead, is no greater than the sampling period, which is the reciprocal of the sampling rate. This is the criterion whether the samples are grouped together in large segments and processed as blocks or are processed individually and whether there are long, short, or non-existent input and output buffers.

Consider an audio DSP example; if a process requires 2.01 seconds to analyze, synthesize, or process 2.00 seconds of sound, it is not real-time. However, if it takes 1.99 seconds, it is or can be made into a real-time DSP process.

A common life analog is standing in a line or queue waiting for the checkout in a grocery store. If the line asymptotically grows longer and longer without bound, the checkout process is not real-time. If the length of the line is bounded, customers are being "processed" and output as rapidly, on average, as they are being inputted and that process "is" real-time. The grocer might go out of business or must at least lose business if they cannot make their checkout process real-time; thus, it is fundamentally important that this process is real-time.

A signal processing algorithm that cannot keep up with the flow of input data with output falling farther and farther behind the input is not real-time. But if the delay of the output (relative to the input) is bounded regarding a process that operates over an unlimited time, then that signal processing algorithm is real-time, even if the throughput delay may be very long.

Real-time signal processing is necessary, but not sufficient in and of itself, for live signal processing such as what is required in live event support. Live audio digital signal processing requires both real-time operation and a sufficient limit to throughput delay so as to be tolerable to performers using stage monitors or in-ear monitors and not noticeable as lip sync error by the audience also directly watching the performers. Tolerable limits to latency for live, real-time processing is a subject of investigation and debate but is estimated to be between 6 and 20 milliseconds.

Real-time bidirectional telecommunications delays of less than 300 ms ("round trip" or twice the unidirectional delay) are considered "acceptable" to avoid undesired "talk-over" in conversation.

Real-time computing is sometimes misunderstood to be high-performance computing, but this is not an accurate classification. For example, a massive supercomputer executing a scientific simulation may offer impressive performance, yet it is not executing a real-time computation. Conversely, once the hardware and software for an anti-lock braking system have been designed to meet its required deadlines, no further performance gains are obligatory or even useful. Furthermore, if a network server is highly loaded with network traffic, its response time may be slower but will (in most cases) still succeed before it times out (hits its deadline). Hence, such a network server would not be considered a real-time system: temporal failures (delays, time-outs, etc.) are typically small and compartmentalized (limited in effect) but are not catastrophic failures. In a real-time system, such as the FTSE 100 Index, a slow-down beyond limits would often be considered catastrophic in its application context. The most important requirement of a real-time system is consistent output, not high throughput.

Some kinds of software, such as many chess-playing programs, can fall into either category. For instance, a chess program designed to play in a tournament with a clock will need to decide on a move before a certain deadline or lose the game, and is therefore a real-time computation, but a chess program that is allowed to run indefinitely before moving is not. In both of these cases, however, high performance is desirable: the more work a tournament chess program can do in the allotted time, the better its moves will be, and the faster an unconstrained chess program runs, the sooner it will be able to move. This example also illustrates the essential difference between real-time computations and other computations: if the tournament chess program does not make a decision about its next move in its allotted time it loses the game—i.e., it fails as a real-time computation—while in the other scenario, meeting the deadline is assumed not to be necessary. High-performance is indicative of the amount of processing that is performed in a given amount of time, whereas real-time is the ability to get done with the processing to yield a useful output in the available time.

The term "near real-time" or "nearly real-time" (NRT), in telecommunications and computing, refers to the time delay introduced, by automated data processing or network transmission, between the occurrence of an event and the use of the processed data, such as for display or feedback and control purposes. For example, a near-real-time display depicts an event or situation as it existed at the current time minus the processing time, as nearly the time of the live event.

The distinction between the terms "near real time" and "real time" is somewhat nebulous and must be defined for the situation at hand. The term implies that there are no significant delays. In many cases, processing described as "real-time" would be more accurately described as "near real-time".

Near real-time also refers to delayed real-time transmission of voice and video. It allows playing video images, in approximately real-time, without having to wait for an entire large video file to download. Incompatible databases can export/import to common flat files that the other database can import/export on a scheduled basis so that they can sync/share common data in "near real-time" with each other.

The distinction between "near real-time" and "real-time" varies, and the delay is dependent on the type and speed of the transmission. The delay in near real-time is typically of the order of several seconds to several minutes.

Several methods exist to aid the design of real-time systems, an example of which is MASCOT, an old but very successful method which represents the concurrent structure of the system. Other examples are HOOD, Real-Time UML, AADL, the Ravenscar profile, and Real-Time Java.




</doc>
<doc id="25768" url="https://en.wikipedia.org/wiki?curid=25768" title="Ruby (programming language)">
Ruby (programming language)

Ruby is an interpreted, high-level, general-purpose programming language. It was designed and developed in the mid-1990s by Yukihiro "Matz" Matsumoto in Japan.

Ruby is dynamically typed and uses garbage collection. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. According to the creator, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, Basic, and Lisp.

Matsumoto has said that Ruby was conceived in 1993. In a 1999 post to the "ruby-talk" mailing list, he describes some of his early ideas about the language:

Matsumoto describes the design of Ruby as being like a simple Lisp language at its core, with an object system like that of Smalltalk, blocks inspired by higher-order functions, and practical utility like that of Perl.

The name "Ruby" originated during an online chat session between Matsumoto and Keiju Ishitsuka on February 24, 1993, before any code had been written for the language. Initially two names were proposed: "Coral" and "Ruby". Matsumoto chose the latter in a later e-mail to Ishitsuka. Matsumoto later noted a factor in choosing the name "Ruby" – it was the birthstone of one of his colleagues.

The first public release of Ruby 0.95 was announced on Japanese domestic newsgroups on December 21, 1995. Subsequently, three more versions of Ruby were released in two days. The release coincided with the launch of the Japanese-language "ruby-list" mailing list, which was the first mailing list for the new language.

Already present at this stage of development were many of the features familiar in later releases of Ruby, including object-oriented design, classes with inheritance, mixins, iterators, closures, exception handling and garbage collection.

Following the release of Ruby 0.95 in 1995, several stable versions of Ruby were released in the following years:

In 1997, the first article about Ruby was published on the Web. In the same year, Matsumoto was hired by netlab.jp to work on Ruby as a full-time developer.

In 1998, the Ruby Application Archive was launched by Matsumoto, along with a simple English-language homepage for Ruby.

In 1999, the first English language mailing list "ruby-talk" began, which signaled a growing interest in the language outside Japan. In this same year, Matsumoto and Keiju Ishitsuka wrote the first book on Ruby, "The Object-oriented Scripting Language Ruby" (オブジェクト指向スクリプト言語 Ruby), which was published in Japan in October 1999. It would be followed in the early 2000s by around 20 books on Ruby published in Japanese.

By 2000, Ruby was more popular than Python in Japan. In September 2000, the first English language book "Programming Ruby" was printed, which was later freely released to the public, further widening the adoption of Ruby amongst English speakers. In early 2002, the English-language "ruby-talk" mailing list was receiving more messages than the Japanese-language "ruby-list", demonstrating Ruby's increasing popularity in the non-Japanese speaking world.

Ruby 1.8 was initially released August 2003, was stable for a long time, and was retired June 2013. Although deprecated, there is still code based on it. Ruby 1.8 is only partially compatible with Ruby 1.9.

Ruby 1.8 has been the subject of several industry standards. The language specifications for Ruby were developed by the Open Standards Promotion Center of the Information-Technology Promotion Agency (a Japanese government agency) for submission to the Japanese Industrial Standards Committee (JISC) and then to the International Organization for Standardization (ISO). It was accepted as a Japanese Industrial Standard (JIS X 3017) in 2011 and an international standard (ISO/IEC 30170) in 2012.

Around 2005, interest in the Ruby language surged in tandem with Ruby on Rails, a web framework written in Ruby. Rails is frequently credited with increasing awareness of Ruby.

Ruby 1.9 was released on Christmas Day in 2007. Effective with Ruby 1.9.3, released October 31, 2011, Ruby switched from being dual-licensed under the Ruby License and the GPL to being dual-licensed under the Ruby License and the two-clause BSD license. Adoption of 1.9 was slowed by changes from 1.8 that required many popular third party gems to be rewritten.

Ruby 1.9 introduces many significant changes over the 1.8 series. Examples:

Ruby 1.9 has been obsolete since February 23, 2015, and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.

Ruby 2.0 added several new features, including:

Ruby 2.0 is intended to be fully backward compatible with Ruby 1.9.3. As of the official 2.0.0 release on February 24, 2013, there were only five known (minor) incompatibilities.

It has been obsolete since February 22, 2016, and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.

Ruby 2.1.0 was released on Christmas Day in 2013. The release includes speed-ups, bugfixes, and library updates.

Starting with 2.1.0, Ruby's versioning policy is more like semantic versioning. Although similar, Ruby's versioning policy is not compatible with semantic versioning:

Semantic versioning also provides additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format, not available at Ruby.

Ruby 2.1 has been obsolete since April 1, 2017, and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.

Ruby 2.2.0 was released on Christmas Day in 2014. The release includes speed-ups, bugfixes, and library updates and removes some deprecated APIs. Most notably, Ruby 2.2.0 introduces changes to memory handling an incremental garbage collector, support for garbage collection of symbols and the option to compile directly against jemalloc. It also contains experimental support for using vfork(2) with system() and spawn(), and added support for the Unicode 7.0 specification.

Features that were made obsolete or removed include callcc, the DL library, Digest::HMAC, lib/rational.rb, lib/complex.rb, GServer, Logger::Application as well as various C API functions.

Ruby 2.2 has been obsolete since April 1, 2018, and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.


Ruby 2.3.0 was released on Christmas Day in 2015. A few notable changes include:

The 2.3 branch also includes many performance improvements, updates, and bugfixes including changes to Proc#call, Socket and IO use of exception keywords, Thread#name handling, default passive Net::FTP connections, and Rake being removed from stdlib.

Ruby 2.4.0 was released on Christmas Day in 2016. A few notable changes include:

The 2.4 branch also includes performance improvements to hash table, Array#max, Array#min, and instance variable access.

Ruby 2.5.0 was released on Christmas Day in 2017. A few notable changes include:

On top of that come a lot of performance improvements like faster block passing (3 times faster), faster Mutexes, faster ERB templates and improvements on some concatenation methods.

Ruby 2.6.0 was released on Christmas Day in 2018. A few notable changes include:

Ruby 2.7.0 was released on Christmas Day in 2019. A few notable changes include:

Matsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design. At a Google Tech Talk in 2008 Matsumoto further stated, "I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language." He stresses that systems design needs to emphasize human, rather than computer, needs:

Ruby is said to follow the principle of least astonishment (POLA), meaning that the language should behave in such a way as to minimize confusion for experienced users. Matsumoto has said his primary design goal was to make a language that he himself enjoyed using, by minimizing programmer work and possible confusion. He has said that he had not applied the principle of least astonishment to the design of Ruby, but nevertheless the phrase has come to be closely associated with the Ruby programming language. The phrase has itself been a source of surprise, as novice users may take it to mean that Ruby's behaviors try to closely match behaviors familiar from other languages. In a May 2005 discussion on the newsgroup comp.lang.ruby, Matsumoto attempted to distance Ruby from POLA, explaining that because any design choice will be surprising to someone, he uses a personal standard in evaluating surprise. If that personal standard remains consistent, there would be few surprises for those familiar with the standard.

Matsumoto defined it this way in an interview:


Ruby is object-oriented: every value is an object, including classes and instances of types that many other languages designate as primitives (such as integers, booleans, and "null"). Variables always hold references to objects. Every function is a method and methods are always called on an object. Methods defined at the top level scope become methods of the Object class. Since this class is an ancestor of every other class, such methods can be called on any object. They are also visible in all scopes, effectively serving as "global" procedures. Ruby supports inheritance with dynamic dispatch, mixins and singleton methods (belonging to, and defined for, a single instance rather than being defined on the class). Though Ruby does not support multiple inheritance, classes can import modules as mixins.

Ruby has been described as a multi-paradigm programming language: it allows procedural programming (defining functions/variables outside classes makes them part of the root, 'self' Object), with object orientation (everything is an object) or functional programming (it has anonymous functions, closures, and continuations; statements all have values, and functions return the last evaluation). It has support for introspection, reflection and metaprogramming, as well as support for interpreter-based threads. Ruby features dynamic typing, and supports parametric polymorphism.

According to the Ruby FAQ, the syntax is similar to Perl and the semantics are similar to Smalltalk, but it differs greatly from Python.

The syntax of Ruby is broadly similar to that of Perl and Python. Class and method definitions are signaled by keywords, whereas code blocks can be both defined by keywords or braces. In contrast to Perl, variables are not obligatorily prefixed with a sigil. When used, the sigil changes the semantics of scope of the variable. For practical purposes there is no distinction between expressions and statements. Line breaks are significant and taken as the end of a statement; a semicolon may be equivalently used. Unlike Python, indentation is not significant.

One of the differences from Python and Perl is that Ruby keeps all of its instance variables completely private to the class and only exposes them through accessor methods (codice_11, codice_12, etc.). Unlike the "getter" and "setter" methods of other languages like C++ or Java, accessor methods in Ruby can be created with a single line of code via metaprogramming; however, accessor methods can also be created in the traditional fashion of C++ and Java. As invocation of these methods does not require the use of parentheses, it is trivial to change an instance variable into a full function, without modifying a single line of calling code or having to do any refactoring achieving similar functionality to C# and VB.NET property members.

Python's property descriptors are similar, but come with a tradeoff in the development process. If one begins in Python by using a publicly exposed instance variable, and later changes the implementation to use a private instance variable exposed through a property descriptor, code internal to the class may need to be adjusted to use the private variable rather than the public property. Ruby’s design forces all instance variables to be private, but also provides a simple way to declare codice_13 and codice_14 methods. This is in keeping with the idea that in Ruby, one never directly accesses the internal members of a class from outside the class; rather, one passes a message to the class and receives a response.

See the Examples section below for samples of code demonstrating Ruby syntax.

The Ruby official distribution also includes codice_15, an interactive command-line interpreter that can be used to test code quickly. The following code fragment represents a sample session using codice_15:

The following examples can be run in a Ruby shell such as Interactive Ruby Shell, or saved in a file and run from the command line by typing codice_17.

Classic Hello world example:

Some basic Ruby code:

Input:

Conversions:

There are a variety of ways to define strings in Ruby.

The following assignments are equivalent:

Strings support variable interpolation:

The following assignments are equivalent and produce raw strings:

Constructing and using an array:

Constructing and using an [[associative array]] (in Ruby, called a "hash"):

If statement:

The two syntaxes for creating a code block:

A code block can be passed to a method as an optional block argument. Many built-in methods have such arguments:

Parameter-passing a block to be a [[Closure (computer science)|closure]]:

Creating an [[anonymous function]]:

Returning [[Closure (computer science)|closures]] from a method:

Yielding the flow of program control to a block that was provided at calling time:

Iterating over enumerations and arrays using blocks:

A method such as codice_18 can accept both a parameter and a block. The codice_18 method iterates over each member of a list, performing some function on it while retaining an aggregate. This is analogous to the codice_20 function in [[functional programming languages]]. For example:

On the first pass, the block receives 10 (the argument to inject) as codice_21, and 1 (the first element of the array) as codice_22. This returns 11, which then becomes codice_21 on the next pass. It is added to 3 to get 14, which is then added to 5 on the third pass, to finally return 19.

Using an enumeration and a block to square the numbers 1 to 10 (using a "range"):

Or invoke a method on each item (codice_24 is a synonym for codice_25):

The following code defines a class named codice_26. In addition to codice_27, the usual constructor to create new objects, it has two methods: one to override the codice_28 comparison operator (so codice_29 can sort by age) and the other to override the codice_30 method (so codice_31 can format its output). Here, codice_12 is an example of metaprogramming in Ruby: codice_33 defines getter and setter methods of instance variables, but codice_12 only getter methods. The last evaluated statement in a method is its return value, allowing the omission of an explicit codice_35 statement.

The preceding code prints three names in reverse age order:

codice_26 is a constant and is a reference to a codice_37 object.

In Ruby, classes are never closed: methods can always be added to an existing class. This applies to "all" classes, including the standard, built-in classes. All that is needed to do is open up a class definition for an existing class, and the new contents specified will be added to the existing contents. A simple example of adding a new method to the standard library's codice_38 class:

Adding methods to previously defined classes is often called [[monkey patch|monkey-patching]]. If performed recklessly, the practice can lead to both behavior collisions with subsequent unexpected results and code scalability problems.

Since Ruby 2.0 it has been possible to use refinements to reduce the potentially negative consequences of monkey-patching, by limiting the scope of the patch to particular areas of the code base.

An exception is raised with a codice_39 call:

An optional message can be added to the exception:

Exceptions can also be specified by the programmer:

Alternatively, an exception instance can be passed to the codice_39 method:

This last construct is useful when raising an instance of a custom exception class featuring a constructor that takes more than one argument:

Exceptions are handled by the codice_41 clause. Such a clause can catch exceptions that inherit from codice_42. Other flow control keywords that can be used when handling exceptions are codice_43 and codice_44:

It is a common mistake to attempt to catch all exceptions with a simple rescue clause. To catch all exceptions one must write:

Or catch particular exceptions:

It is also possible to specify that the exception object be made available to the handler clause:

Alternatively, the most recent exception is stored in the magic global codice_45.

Several exceptions can also be caught:

Ruby code can programmatically modify, at [[Run time (program lifecycle phase)|runtime]], aspects of its own structure that would be fixed in more rigid languages, such as class and method definitions. This sort of [[metaprogramming]] can be used to write more concise code and effectively extend the language.

For example, the following Ruby code generates new methods for the built-in codice_46 class, based on a list of colors. The methods wrap the contents of the string with an HTML tag styled with the respective color.

The generated methods could then be used like this:

To implement the equivalent in many other languages, the programmer would have to write each method (codice_47, codice_48, codice_49, etc.) separately.

Some other possible uses for Ruby metaprogramming include:

The original Ruby [[interpreter (computer software)|interpreter]] is often referred to as [[Ruby MRI|Matz's Ruby Interpreter]] or MRI. This implementation is written in C and uses its own Ruby-specific [[virtual machine]].

The standardized and retired Ruby 1.8 [[Ruby MRI|implementation]] was written in [[C (programming language)|C]], as a single-pass [[interpreted language]].

Starting with Ruby 1.9, and continuing with Ruby 2.x and above, the official Ruby interpreter has been [[YARV]] ("Yet Another Ruby VM"), and this implementation has superseded the slower virtual machine used in previous releases of MRI.

, there are a number of alternative implementations of Ruby, including [[JRuby]], [[Rubinius]], and [[mruby]]. Each takes a different approach, with JRuby and Rubinius providing [[just-in-time compilation]] and mruby also providing [[ahead-of-time compilation]].

Ruby has three major alternate implementations:

Other Ruby implementations include:

Other now defunct Ruby implementations were:

The maturity of Ruby implementations tends to be measured by their ability to run the [[Ruby on Rails]] (Rails) framework, because it is complex to implement and uses many Ruby-specific features. The point when a particular implementation achieves this goal is called "the Rails singularity". The reference implementation, JRuby, and Rubinius are all able to run Rails unmodified in a production environment.

Matsumoto originally did Ruby development on the [[BSD|4.3BSD]]-based [[Sony NEWS|Sony NEWS-OS]] 3.x, but later migrated his work to [[SunOS]] 4.x, and finally to [[Linux]].

By 1999, Ruby was known to work across many different [[operating system]]s, including NEWS-OS, SunOS, [[AIX]], [[SVR4]], [[Solaris (operating system)|Solaris]], [[NEC]] [[UP-UX]], [[NeXTSTEP]], BSD, Linux, [[Classic Mac OS|Mac OS]], [[DOS]], [[Windows (operating system)|Windows]], and [[BeOS]].

Modern Ruby versions and implementations are available on many operating systems, such as Linux, BSD, Solaris, AIX, [[macOS]], Windows, [[Windows Phone]], [[Windows CE]], [[Symbian OS]], BeOS, and [[IBM i]].

[[RubyGems]] is Ruby's package manager. A Ruby package is called a "gem" and can easily be installed via the command line. Most gems are libraries, though a few exist that are applications, such as [[integrated development environment|IDEs]]. There are over 10,000 Ruby gems hosted on RubyGems.org.

Many new and existing Ruby libraries are hosted on [[GitHub]], a service that offers [[Revision control|version control]] repository hosting for [[Git (software)|Git]].

The Ruby Application Archive, which hosted applications, documentation, and libraries for Ruby programming, was maintained until 2013, when its function was transferred to RubyGems.


[[Category:Ruby (programming language)| ]]
[[Category:Articles with example Ruby code]]
[[Category:Class-based programming languages]]
[[Category:Dynamic programming languages]]
[[Category:Dynamically typed programming languages]]
[[Category:Free software programmed in C]]
[[Category:ISO standards]]
[[Category:Object-oriented programming languages]]
[[Category:Programming languages created in 1995]]
[[Category:Programming languages with an ISO standard]]
[[Category:Scripting languages]]
[[Category:Software using the BSD license]]
[[Category:Text-oriented programming languages]]
[[Category:Free compilers and interpreters]]

</doc>
<doc id="25774" url="https://en.wikipedia.org/wiki?curid=25774" title="Render farm">
Render farm

A render farm is a high-performance computer system, e.g. a computer cluster, built to render computer-generated imagery (CGI), typically for film and television visual effects.

The term "render farm" was born during the production of the Autodesk 3D Studio animated short "The Bored Room" in July 1990 when, to meet an unrealistic deadline, a room filled with Compaq 386 computers was configured to do the rendering. At the time the system wasn't networked so each computer had to be set up by hand to render a specific animation sequence. The rendered images would then be 'harvested' via a rolling platform to a large-format optical storage drive, then loaded frame by frame to a Sony CRV disc.

The Autodesk technician assigned to manage this early render farm (Jamie Clay) had a regular habit of wearing farmer's overalls and the product manager for the software (Bob Bennett) joked that what Clay was doing was farming the frames and at that moment he named the collection of computers a "render farm". In the second release of the software, Autodesk introduced network rendering, making the task of running a render farm significantly easier. A BTS of The Bored Room doesn't show Clay in the overalls but does give a glimpse of the production environment.

A render farm is different from a render wall, which is a networked, tiled display used for real-time rendering. The rendering of images is a highly parallelizable activity, as frames and sometimes tiles can be calculated independently of the others, with the main communication between processors being the upload of the initial source material, such as models and textures, and the download of the finished images.

Over the decades, advances in computer capability have allowed an image to take less time to render. However, the increased computation is appropriated to meet demands to achieve state-of-the-art image quality. While simple images can be produced rapidly, more realistic and complicated higher-resolution images can now be produced in more reasonable amounts of time. The time spent producing images can be limited by production time-lines and deadlines, and the desire to create high-quality work drives the need for increased computing power, rather than simply wanting the same images created faster. Project such as the Big and Ugly Rendering Project have been available for rendering images using Blender across both widely distributed networks and local networks.

To manage large farms, one must introduce a "queue manager" that automatically distributes processes to the many processors. Each "process" could be the rendering of one full image, a few images, or even a sub-section (or "tile") of an image. The software is typically a client–server package that facilitates communication between the processors and the queue manager, although some queues have no central manager. Some common features of queue managers are: re-prioritization of the queue, management of software licenses, and algorithms to best optimize throughput based on various types of hardware in the farm. Software licensing handled by a queue manager might involve dynamic allocation of licenses to available CPUs or even cores within CPUs.
A tongue-in-cheek job title for systems engineers who work primarily in the maintenance and monitoring of a render farm is a "render wrangler" to further the "farm" theme. This job title can be seen in film credits.

Beyond on-site render farms, cloud-based render farm options have been facilitated by the rise of high-speed Internet access. Many cloud computing services, including some dedicated to rendering, offer render farm services which bill only for processor time used. Understanding the cost or processing time required to complete rendering is unpredictable so render farms bill using GHz per hour. Those considering outsourcing their renders to a farm or to the cloud can do a number of things to improve their predictions and reduce their costs. These services eliminate the need for a customer to build and maintain their own rendering solution. Another phenomenon is collaborative rendering, in which users join a network of animators who contribute their processing power to the group. However, this has technological and security limitations.




</doc>
<doc id="25775" url="https://en.wikipedia.org/wiki?curid=25775" title="Render">
Render

Render, rendered, or rendering may refer to:






</doc>
<doc id="25776" url="https://en.wikipedia.org/wiki?curid=25776" title="Robert Borden">
Robert Borden

Sir Robert Laird Borden, (June 26, 1854 – June 10, 1937) was a Canadian lawyer and politician who served as the eighth prime minister of Canada, in office from 1911 to 1920. He is best known for his leadership of Canada during World War I.

Borden was born in Grand-Pré, Nova Scotia. He worked as a schoolteacher for a period and then served his articles of clerkship at a Halifax law firm. He was called to the bar in 1878, and soon became one of Nova Scotia's most prominent barristers. Borden was elected to the House of Commons of Canada in 1896, representing the Conservative Party. He replaced Charles Tupper as party leader in 1901, and became prime minister after the party's victory at the 1911 federal election.

As prime minister, Borden led Canada through World War I and its immediate aftermath. His government passed the "War Measures Act", created the Canadian Expeditionary Force, and eventually introduced compulsory military service, which sparked the 1917 conscription crisis. On the home front, it dealt with the consequences of the Halifax Explosion, introduced women's suffrage for federal elections, and used the North-West Mounted Police to break up the 1919 Winnipeg general strike. For the 1917 federal election (the first in six years), Borden created the Unionist Party, an amalgam of Conservatives and pro-conscription Liberals; his government was re-elected with an overwhelming majority.

Borden retired from politics in 1920, having accepted a knighthood in 1915 – the last Canadian prime minister to be knighted. He was also the last prime minister born before Confederation, and is the most recent Nova Scotian to hold the office. His portrait has appeared on Canadian one hundred-dollar notes produced since 1976, but in late 2016 the government announced Borden's image would be removed during the next redesign.

Robert Laird Borden was born and educated in Grand-Pré, Nova Scotia, a farming community at the eastern end of the Annapolis Valley, where his great-grandfather Perry Borden, Sr. of Tiverton, Rhode Island, had taken up Acadian land in 1760 as one of the New England Planters. The Borden family had immigrated from Headcorn, Kent, England, to New England in the 1600s. Also arriving in this group was a great-great-grandfather, Robert Denison, who had come from Connecticut at about the same time. Perry had accompanied his father, Samuel Borden, the chief surveyor chosen by the government of Massachusetts to survey the former Acadian land and draw up new lots for the Planters in Nova Scotia. Through the marriage of his patrilineal ancestor Richard Borden to Innocent Cornell, Borden is descendant from Thomas Cornell of Portsmouth, Rhode Island.

Borden's father Andrew Borden was judged by his son to be "a man of good ability and excellent judgement", of a "calm, contemplative and philosophical" turn of mind, but "he lacked energy and had no great aptitude for affairs". His mother Eunice Jane Laird was more driven, possessing "very strong character, remarkable energy, high ambition and unusual ability". Her ambition was transmitted to her first-born child, who applied himself to his studies while assisting his parents with the farm work he found so disagreeable. His cousin Sir Frederick Borden was a prominent Liberal politician.

Robert Borden was the last Canadian Prime Minister born before Confederation.

From 1868 to 1874, he worked as a teacher in Grand-Pré and Matawan, New Jersey. Seeing no future in teaching, he returned to Nova Scotia in 1874. Despite having no formal university education, he went to article for four years at a Halifax law firm. In August 1878, he was called to the Nova Scotia Bar, placing first in the bar examinations. Borden went to Kentville, Nova Scotia, as the junior partner of the Conservative lawyer John P. Chipman. In 1880, he was inducted into the Freemasons – St Andrew's lodge #1.

In 1882, he was asked by Wallace Graham to move to Halifax and join the Conservative law firm headed by Graham and Charles Hibbert Tupper. In the Autumn of 1889, when he was only 35, Borden became the senior partner following the departure of Graham and Tupper for the bench and politics, respectively. His financial future guaranteed, on September 25, 1889, he married Laura Bond (1863–1940), the daughter of a Halifax hardware merchant. They would have no children. In 1894, he bought a large property and home on the south side of Quinpool Road, which the couple called "Pinehurst". In 1893, Borden successfully argued the first of two cases which he took to the Judicial Committee of the Privy Council. He represented many of the important Halifax businesses, and sat on the boards of Nova Scotian companies including the Bank of Nova Scotia and the Crown Life Insurance Company. In 1896, he became President of the Nova Scotia Barristers' Society, and took the initiative in organizing the founding meetings of the Canadian Bar Association in Montreal within the same year. By the time he was prevailed upon to enter politics, Borden had what some judged to be the largest legal practice in the Maritime Provinces, and had become a wealthy man.

Borden was a Liberal until he broke with the party in 1891 over the issue of Reciprocity.

He was elected to Parliament in the 1896 federal election as a Conservative and in 1901 was selected by the Conservative caucus to succeed Sir Charles Tupper as leader of the Conservative Party. He was defeated in his Halifax seat in the 1904 federal election and re-entered the House of Commons the next year via a by-election in Carleton. Over the next decade he worked to rebuild the party and establish a reform policy, the Halifax Platform of 1907 which he described as "the most advanced and progressive policy ever put forward in Federal affairs". It called for reform of the Senate and the civil service, a more selective immigration policy, free rural mail delivery, and government regulation of telegraphs, telephones, and railways and eventually national ownership of telegraphs and telephones. Despite his efforts, his party lost the 1908 federal election to Wilfrid Laurier's Liberals. Borden was however elected again for Halifax. His party's fortunes turned around in the 1911 federal election, however, when the Conservatives successfully campaigned against Laurier's proposals for a Reciprocity (free trade) agreement with the United States. Borden countered with a revised version of John A. Macdonald's National Policy and appeals of loyalty to the British Empire and ran on the slogan "Canadianism or Continentalism". In British Columbia, the party ran on the slogan "A White Canada", playing to the fears of British Columbians that resented the increasing presence of cheap Asian labour and the resulting depression in wages. In Quebec, concurrently, Henri Bourassa led a campaign against what he saw as Laurier's capitulation to British imperialism, playing a part in the defeat of Laurier's government and the election of Borden's Tories.

Borden served as Prime Minister for the duration of the 12th Parliament of Canada, and for most of the 13th Parliament of Canada, before his retirement from active political life in July 1920.

As Prime Minister of Canada during the First World War, he transformed his government to a wartime administration, passing the "War Measures Act" in 1914. Borden committed Canada to provide half a million soldiers for the war effort. However, volunteers had quickly dried up when Canadians realized there would be no quick end to the war. Borden's determination to meet that huge commitment led to the "Military Service Act" and the Conscription Crisis of 1917, which split the country on linguistic lines. In 1917 Borden recruited members of the Liberals (with the notable exception of leader Wilfrid Laurier) to create a Unionist government. The 1917 election saw the "Government" candidates (including a number of Liberal-Unionists) crush the Opposition "Laurier Liberals" in English Canada resulting in a large parliamentary majority for Borden.
Sir Robert Borden pledged himself during the campaign to equal suffrage for women. With his return to power, he introduced a bill in 1918 for extending the franchise to women. This passed without division.

The war effort also enabled Canada to assert itself as an independent power. Borden wanted to create a single Canadian army, rather than have Canadian soldiers split up and assigned to British divisions as had happened during the Boer War. Sam Hughes, the Minister of Militia, generally ensured that Canadians were well-trained and prepared to fight in their own divisions, although with mixed results such as the Ross Rifle. Arthur Currie provided sensible leadership for the Canadian divisions in Europe, although they were still under overall British command. Nevertheless, Canadian troops proved themselves to be among the best in the world, fighting at the Somme, Ypres, Passchendaele, and especially at the Battle of Vimy Ridge.

During Borden's first term as Prime Minister, the National Research Council of Canada was established in 1916.

In world affairs, Borden played a crucial role (according to McMillan) in transforming the British Empire into a partnership of equal states, the Commonwealth of Nations, a term that was first discussed at an Imperial Conference in London during the war. Borden also introduced the first Canadian income tax under Income War Tax Act of 1917, which was then meant to be temporary but later became permanent.

Convinced that Canada had become a nation on the battlefields of Europe, Borden demanded that it have a separate seat at the Paris Peace Conference. This was initially opposed not only by Britain but also by the United States, which perceived such a delegation as an extra British vote. Borden responded by pointing out that since Canada had lost a far larger proportion of its men compared to the US in the war (although not more in absolute numbers), Canada at least had the right to the representation of a "minor" power. British Prime Minister David Lloyd George eventually relented, and convinced the reluctant Americans to accept the presence of separate Canadian, Indian, Australian, Newfoundland, New Zealand and South African delegations. Despite this, Borden boycotted the opening ceremony, protesting at the precedence given to the prime minister of the much smaller Newfoundland over him.

Not only did Borden's persistence allow him to represent Canada in Paris as a nation, it also ensured that each of the dominions could sign the Treaty of Versailles in its own right and receive a separate membership in the League of Nations. During the conference, Borden tried to act as an intermediary between the United States and other members of the British Empire delegation, particularly Australia and New Zealand over the issue of the League of Nations Mandate. Borden also discussed with Lloyd George the possibility of Canada taking over the administration of Belize and the West Indies, but no agreement was reached.

At Borden's insistence, the treaty was ratified by the Canadian Parliament. Borden was the last Prime Minister to be knighted after the House of Commons indicated its desire for the discontinuation of the granting of any future titles to Canadians in 1919 with the adoption of the Nickle Resolution.

In 1919 Borden approved the use of troops to put down the Winnipeg general strike, which was feared to be the result of Bolshevik agitation from the Soviet Union.

Sir Robert Borden retired from office in 1920. He was the Chancellor of Queen's University from 1924 to 1930 and also was Chancellor of McGill University from 1918 to 1920 while still Prime Minister. Borden also served as Vice-President of The Champlain Society between 1923 and 1925. He was the Society's first Honorary President between 1925 and 1938. Borden's successor Arthur Meighen was defeated by the new Liberal leader William Lyon Mackenzie King in the 1921 election. Nevertheless, Borden would go on to represent Canada once more on the international stage when he attended the Washington Naval Conference in 1922 and signed the resulting arms reduction treaty on Canada's behalf.

At the time of his death, Borden stood as president of two financial institutions: Barclays Bank of Canada and the Crown Life Insurance Company. Borden died on June 10, 1937, in Ottawa and is buried in the Beechwood Cemetery marked by a simple stone cross.

Robert Laird Borden married Laura Bond, youngest daughter of the late T. H. Bond, September 1889. She served as president of the Local Council of Women of Halifax, until her resignation in 1901. She served as President of the Aberdeen Association, Vice-President of the Women's Work Exchange in Halifax, and Corresponding Secretary of the Associated Charities of the United States.


Borden chose the following jurists to sit as justices of the Supreme Court of Canada:


By Sir Robert

By others




</doc>
<doc id="25781" url="https://en.wikipedia.org/wiki?curid=25781" title="Robot">
Robot

A robot is a machine—especially one programmable by a computer— capable of carrying out a complex series of actions automatically. Robots can be guided by an external control device or the control may be embedded within. Robots may be constructed on the lines of human form, but most robots are machines designed to perform a task with no regard to their aesthetics.

Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's "Advanced Step in Innovative Mobility" (ASIMO) and TOSY's "TOSY Ping Pong Playing Robot" (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed "swarm" robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own. Autonomous things are expected to proliferate in the coming decade, with home robotics and the autonomous car as some of the main drivers.

The branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.

From the time of ancient civilization there have been many accounts of user-configurable automated devices and even automata resembling animals and humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.

The term comes from a Czech word, "robota", meaning "forced labor"; the word 'robot' was first used to denote a fictional humanoid in a 1920 play "R.U.R." "(Rossumovi Univerzální Roboti - Rossum's Universal Robots)" by the Czech writer, Karel Čapek but it was Karel's brother Josef Čapek who was the word's true inventor. Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948, as well as Computer Numerical Control (CNC) machine tools in the late 1940s by John T. Parsons and Frank L. Stulen. The first commercial, digital and programmable robot was built by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.

Robots have replaced humans in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea. There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising technological unemployment as they replace workers in increasing numbers of functions. The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.

The word "robot" can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots. There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to possess some or all of the following abilities and functions: accept electronic programming, process data or physical perceptions electronically, operate autonomously to some degree, move around, operate physical parts of itself or physical processes, sense and manipulate their environment, and exhibit intelligent behavior, especially behavior which mimics humans or other animals. Closely related to the concept of a "robot" is the field of Synthetic Biology, which studies entities whose nature is more comparable to beings than to machines.

The idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China, Ancient Greece, and Ptolemaic Egypt, attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas, the artificial birds of Mozi and Lu Ban, a "speaking" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the "Lie Zi".

Many ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the island from pirates.

In ancient Greece, the Greek engineer Ctesibius (c. 270 BC) "applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures." In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called "The Pigeon". Hero of Alexandria , a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.
The 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka. 

In ancient China, the 3rd-century text of the "Lie Zi" describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs. There are also accounts of flying automata in the "Han Fei Zi" and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds ("ma yuan") that could successfully fly.

"Samarangana Sutradhara", a Sanskrit treatise by Bhoja (11th century), includes a chapter about the construction of mechanical contrivances (automata), including mechanical bees and birds, fountains shaped like humans and animals, and male and female dolls that refilled oil lamps, danced, played instruments, and re-enacted scenes from Hindu mythology.

13th century Muslim Scientist Ismail al-Jazari created several automated devices. He built automated moving peacocks driven by hydropower. He also invented the earliest known automatic gates, which were driven by hydropower, created automatic doors as part of one of his elaborate water clocks. One of al-Jazari's humanoid automata was a waitress that could serve water, tea or drinks. The drink was stored in a tank with a reservoir from where the drink drips into a bucket and, after seven minutes, into a cup, after which the waitress appears out of an automatic door serving the drink. Al-Jazari invented a hand washing automaton incorporating a flush mechanism now used in modern flush toilets. It features a female humanoid automaton standing by a basin filled with water. When the user pulls the lever, the water drains and the female automaton refills the basin.

Mark E. Rosheim summarizes the advances in robotics made by Muslim engineers, especially al-Jazari, as follows:Unlike the Greek designs, these Arab examples reveal an interest, not only in dramatic illusion, but in manipulating the environment for human comfort. Thus, the greatest contribution the Arabs made, besides preserving, disseminating and building on the work of the Greeks, was the concept of practical application. This was the key element that was missing in Greek robotic science.

In Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw. The design was probably based on anatomical research recorded in his "Vitruvian Man". It is not known whether he attempted to build it. According to "Encyclopædia Britannica", Leonardo da Vinci may have been influenced by the classic automata of al-Jazari.

In Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century "Karakuri zui" ("Illustrated Machinery", 1796). One such automaton was the karakuri ningyō, a mechanized puppet. Different variations of the karakuri existed: the "Butai karakuri", which were used in theatre, the "Zashiki karakuri", which were small and used in homes, and the "Dashi karakuri" which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.

In France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.

Remotely operated vehicles were demonstrated in the late 19th century in the form of several types of remotely controlled torpedoes. The early 1870s saw remotely controlled torpedoes by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).

The Brennan torpedo, invented by Louis Brennan in 1877, was powered by two contra-rotating propellors that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it "the world's first "practical" guided missile". In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by "Hertzian" (radio) waves and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.

Archibald Low, known as the "father of radio guidance systems" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.

'Robot' was first applied as a term for artificial automata in the 1920 play "R.U.R." by the Czech writer, Karel Čapek. However, Josef Čapek was named by his brother Karel as the true inventor of the term robot. The word 'robot' itself was not new, having been in the Slavic language as "robota" (forced laborer), a term which classified those peasants obligated to compulsory service under the feudal system (see: Robot Patent).
Čapek's fictional story postulated the technological creation of artificial human bodies without souls, and the old theme of the feudal "robota" class eloquently fit the imagination of a new class of manufactured, artificial workers.

English pronunciation of the word has evolved relatively quickly since its introduction. In the U.S. during the late '30s to early '40s the second sylable was pronounced with a long "O" like "row-boat." By the late '50s to early '60s, some were pronouncing it with a short "U" like "row-but" while others used a softer "O" like "row-bought." By the '70s, its current pronunciation "row-bot" had become predominant.

In 1928, one of the first humanoid robots, Eric, was exhibited at the annual exhibition of the Model Engineers Society in London, where it delivered a speech. Invented by W. H. Richards, the robot's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control. Both Eric and his "brother" George toured the world.

Westinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair. Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.

The first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors – essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named "Elmer" and "Elsie", were constructed between 1948 and 1949 and were often described as "tortoises" due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.

Walter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's "turtles" may be found in the form of BEAM robotics.
The first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry. Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them. Devol's patent for the first digitally operated programmable robotic arm represents the foundation of the modern robotics industry.

The first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. In 1973, a robot with six electromechanically driven axes was patented by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.

Commercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.

Various techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent "generation" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions. Another new type of robot is just recently introduced which acts both as a smartphone and robot and is named RoboHon.

As robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System is an open-source set of programs being developed at Stanford University, the Massachusetts Institute of Technology and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a "Windows for robots" system with its Robotics Developer Studio, which has been available since 2007.

Japan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.

Many future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction.
As early as 1982 people were confident that someday robots would:
1. Clean parts by removing molding flash
2. Spray paint automobiles with absolutely no human presence
3. Pack things in boxes—for example, orient and nest chocolate candies in candy boxes
4. Make electrical cable harness
5. Load trucks with boxes—a packing problem
6. Handle soft goods, such as garments and shoes
7. Shear sheep
8. prosthesis
9. Cook fast food and work in other service industries
10. Household robot.

Generally such predictions are overly optimistic in timescale.

In 2008, Caterpillar Inc. developed a dump truck which can drive itself without any human operator. Many analysts believe that self-driving trucks may eventually revolutionize logistics. By 2014, Caterpillar had a self-driving dump truck which is expected to greatly change the process of mining. In 2015, these Caterpillar trucks were actively used in mining operations in Australia by the mining company Rio Tinto Coal Australia. Some analysts believe that within the next few decades, most trucks will be self-driving.

A literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.

Baxter is a new robot introduced in 2012 which learns by guidance. A worker could teach Baxter how to perform a task by moving its hands in the desired motion and having Baxter memorize them. Extra dials, buttons, and controls are available on Baxter's arm for more precision and features. Any regular worker could program Baxter and it only takes a matter of minutes, unlike usual industrial robots that take extensive programs and coding in order to be used. This means Baxter needs no programming in order to operate. No software engineers are needed. This also means Baxter can be taught to perform multiple, more complicated tasks. Sawyer was added in 2015 for smaller, more precise tasks.

The word "robot" was introduced to the public by the Czech interwar writer Karel Čapek in his play "R.U.R. (Rossum's Universal Robots)", published in 1920. The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called "robots." The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieve self-awareness and incite robots all around the world to rise up against the humans).

Karel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the "Oxford English Dictionary" in which he named his brother, the painter and writer Josef Čapek, as its actual originator.

In an article in the Czech journal "Lidové noviny" in 1933, he explained that he had originally wanted to call the creatures "laboři" ("workers", from Latin "labor"). However, he did not like the word, and sought advice from his brother Josef, who suggested "roboti". The word "robota" means literally "corvée", "serf labor", and figuratively "drudgery" or "hard work" in Czech and also (more general) "work", "labor" in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as "robot" in Hungarian). Traditionally the "robota" (Hungarian "robot") was the work period a serf (corvée) had to give for his lord, typically 6 months of the year. The origin of the word is the Old Church Slavonic (Old Bulgarian) "rabota" "servitude" ("work" in contemporary Bulgarian and Russian), which in turn comes from the Proto-Indo-European root "*orbh-". "Robot" is cognate with the German root "Arbeit" (work).

The word robotics, used to describe this field of study, was coined by the science fiction writer Isaac Asimov. Asimov created the ""Three Laws of Robotics"" which are a recurring theme in his books. These have since been used by many others to define laws used in fiction. (The three laws are pure fiction, and no technology yet created has the ability to understand or follow them, and in fact most robots serve military purposes, which run quite contrary to the first law and often the third law. "People think about Asimov's laws, but they were set up to point out how a simple ethical system doesn't work. If you read the short stories, every single one is about a failure, and they are totally impractical," said Dr. Joanna Bryson of the University of Bath.)

Mobile robots have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the "automated guided vehicle" or "automatic guided vehicle" (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers. AGVs are discussed later in this article.

Mobile robots are also found in industry, military and security environments. They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.

Mobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.

Industrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.

The International Organization for Standardization gives a definition of a manipulating industrial robot in ISO 8373:

"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications."

This definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees.

Most commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term "service robot" is less well-defined. The International Federation of Robotics has proposed a tentative definition, "A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations."

Robots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.

There are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST LEGO League, Junior FIRST LEGO League, and FIRST Tech Challenge competitions.

There have also been robots such as the teaching computer, Leachim (1974). Leachim was an early example of speech synthesis using the using the Diphone synthesis method. 2-XL (1976) was a robot shaped game / teaching toy based on branching between audible tracks on an 8-track tape player, both invented by Michael J. Freeman. Later, the 8-track was upgraded to tape cassettes and then to digital.

Modular robots are a new breed of robots that are designed to increase the utilization of robots by modularizing their architecture. The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots. Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U and H shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These "ANAT robots" can be designed with "n" DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases. Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.

Modular robotic technology is currently being applied in hybrid transportation, industrial automation, duct cleaning and handling. Many research centres and universities have also studied this technology, and have developed prototypes.

A "collaborative robot" or "cobot" is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.

The collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.

Rethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks. Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer. , 190 companies in the US have bought Baxters and they are being used commercially in the UK.

Roughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa. 40% of all the robots in the world are in Japan, making Japan the country with the highest number of robots.

As robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior, and whether robots might be able to claim any kind of social, cultural, ethical or legal rights. One scientific team has said that it is possible that a robot brain will exist by 2019. Others predict robot intelligence breakthroughs by 2050. Recent advances have made robotic behavior more sophisticated. The social impact of intelligent robots is subject of a 2010 documentary film called "Plug & Pray".

Vernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this "the Singularity". He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism.

In 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls. Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns. In 2015, the Nao alderen robots were shown to have a capability for a degree of self-awareness. Researchers at the Rensselaer Polytechnic Institute AI and Reasoning Lab in New York conducted an experiment where a robot became aware of itself, and corrected its answer to a question once it had realised this.

Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots. The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.

One robot in particular, the EATR, has generated public concerns over its fuel source, as it can continually refuel itself using organic substances. Although the engine for the EATR is designed to run on biomass and vegetation specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.

Manuel De Landa has noted that "smart missiles" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.

For centuries, people have predicted that machines would make workers obsolete and increase unemployment, although the causes of unemployment are usually thought to be due to social policy.

A recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.

Lawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to improve redundancy laws.

Kevin J. Delaney said "Robots are taking human jobs. But Bill Gates believes that governments should tax companies’ use of them, as a way to at least temporarily slow the spread of automation and to fund other types of employment." The robot tax would also help pay a guaranteed living wage to the displaced workers.

The World Bank's World Development Report 2019 puts forth evidence showing that while automation displaces workers, technological innovation creates more new industries and jobs on balance.

At present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.

Robots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. All robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.

General-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in. Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.

Over the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.

Industrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.

Mass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy. Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.

Mobile robots, following markers or wires in the floor, or using vision or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.

Limited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.

Developed to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.

Such as SmartLoader, SpeciMinder, ADAM, Tug Eskorta, and MT 400 with Motivity are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.

There are many jobs which humans would rather leave to robots. The job may be boring, such as domestic cleaning or sports field line marking, or dangerous, such as exploring inside a volcano. Other jobs are physically inaccessible, such as exploring another planet, cleaning the inside of a long pipe, or performing laparoscopic surgery.

Almost every unmanned space probe ever launched was a robot. Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, among others.

Teleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time. They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely. Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets. Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).

Robots are used to automate picking fruit on orchards at a cost lower than that of human pickers.

Domestic robots are simple robots dedicated to a single task work in home use. They are used in simple but often disliked jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.

Military robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.

Unmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own. The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection. Flight trials are expected to begin in 2011.

The AAAI has studied this topic in depth and its president has commissioned a study to look at this issue.

Some have suggested a need to build "Friendly AI", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane. Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics. An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee. Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as "Robot Legal Studies." Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.

Mining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous truck fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia. Similarly, BHP has announced the expansion of its autonomous drill fleet to the world's largest, 21 autonomous Atlas Copco drills.

Drilling, longwall and rockbreaking machines are now also available as autonomous robots. The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths. Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination. These systems greatly enhance the safety and efficiency of mining operations.

Robots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.

Robots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1, through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.

The population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them. Humans make the best carers, but where they are unavailable, robots are gradually being introduced.

FRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.

Script Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form. The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient's medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it's the correct drug for the correct patient and then seals the vials and sends it out front to be picked up. 

McKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors. The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to its either stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify its pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.

While most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.

One approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.

Nanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10 meters). Also known as "nanobots" or "nanites", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest. Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog, manufacturing, weaponry and cleaning. Some people have suggested that if there were nanobots which could reproduce, the earth would turn into "grey goo", while others argue that this hypothetical outcome is nonsense.

A few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task, like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.

Robots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids) look and feel different from robots with rigid skeletons, and can have different behaviors. Soft, flexible (and sometimes even squishy) robots are often designed to mimic the biomechanics of animals and other things found in nature, which is leading to new applications in medicine, care giving, search and rescue, food handling and manufacturing, and scientific exploration.

Inspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors. Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.

Robotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called "haptic interfaces", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of "virtual" objects, which users can experience through their sense of touch.

Robotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also "bionic men/women", or humans with significant mechanical enhancements) have become a staple of science fiction.

The first reference in Western literature to mechanical servants appears in Homer's "Iliad". In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots. According to the Rieu translation, "Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods." The words "robot" or "android" are not used to describe them, but they are nevertheless mechanical devices human in appearance. "The first use of the word Robot was in Karel Čapek's play R.U.R. (Rossum's Universal Robots) (written in 1920)". Writer Karel Čapek was born in Czechoslovakia (Czech Republic).

Possibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992) who published over five-hundred books. Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works. Asimov carefully considered the problem of the ideal set of instructions robots might be given in order to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law. These were introduced in his 1942 short story "Runaround", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: "A robot may not harm humanity, or, by inaction, allow humanity to come to harm"; the rest of the laws are modified sequentially to acknowledge this.

According to the "Oxford English Dictionary," the first passage in Asimov's short story "Liar!" (1941) that mentions the First Law is the earliest recorded use of the word "robotics". Asimov was not initially aware of this; he assumed the word already existed by analogy with "mechanics," "hydraulics," and other similar terms denoting branches of applied knowledge.

Robots appear in many films. Most of the robots in cinema are fictional. Two of the most famous are R2-D2 and C-3PO from the "Star Wars" franchise.

The concept of humanoid sex robots has elicited both public attention and concern. Opponents of the concept have stated that the development of sex robots would be morally wrong. They argue that the introduction of such devices would be socially harmful, and demeaning to women and children.

Fears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race.
"Frankenstein" (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or android advancing beyond its creator.

Other works with similar themes include "The Mechanical Man", "The Terminator, Runaway, RoboCop", the Replicators in "Stargate", the Cylons in "Battlestar Galactica", the Cybermen and Daleks in "Doctor Who", "The Matrix", "Enthiran" and "I, Robot". Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are "", "Red Planet" and "Enthiran".

The 2017 game Horizon Zero Dawn explores themes of robotics in warfare, robot ethics, and the AI control problem, as well as the positive or negative impact such technologies could have on the environment.

Another common theme is the reaction, sometimes called the "uncanny valley", of unease and even revulsion at the sight of robots that mimic humans too closely.

More recently, fictional representations of artificially intelligent robots in films such as "A.I. Artificial Intelligence" and "Ex Machina" and the 2016 TV adaptation of "Westworld" have engaged audience sympathy for the robots themselves.








</doc>
<doc id="25783" url="https://en.wikipedia.org/wiki?curid=25783" title="R. B. Bennett">
R. B. Bennett

Richard Bedford Bennett, 1st Viscount Bennett, (July 3, 1870 – June 26, 1947) was a Canadian lawyer, businessman and politician. He served as the 11th prime minister of Canada, in office from 1930 to 1935. He led the Conservative Party from 1927 to 1938.

Bennett's prime ministership is widely regarded as a failure by historians, although he left lasting legacies in the form of the Canadian Broadcasting Corporation (established 1932) and the Bank of Canada (established 1934).

Bennett was born in Hopewell Hill, New Brunswick, and grew up in nearby Hopewell Cape. He studied law at Dalhousie University, graduating in 1893, and in 1897 moved to Calgary to establish a law firm in partnership with James Lougheed.

Bennett served in the Legislative Assembly of the Northwest Territories from 1898 to 1905, and later in the Alberta Legislature from 1909 to 1911. He was the inaugural leader of the Alberta Conservative Party from 1905, resigning upon his election to the House of Commons in 1911. From 1920 to 1921, Bennett was Minister of Justice under Arthur Meighen. He also served briefly as Minister of Finance in Meighen's second government in 1926, which lasted just a month. Meighen resigned the Conservative Party's leadership after its defeat at the 1926 election, with Bennett elected as his replacement (and thus Leader of the Opposition).

Bennett became prime minister after the 1930 election, where the Conservatives won a landslide victory over Mackenzie King's Liberal Party. He was the first prime minister to represent a constituency in Alberta. The main difficulty during Bennett's prime ministership was the Great Depression. He and his party initially tried to combat the crisis with "laissez-faire" policies, but these were largely ineffective. However, over time Bennett's government became increasingly interventionist, attempting to replicate the popular "New Deal" enacted by Franklin Roosevelt to the south. This about-face prompted a split within Conservative ranks, and was regarded by the general public as evidence of incompetence.

Bennett suffered a landslide defeat at the 1935 election, with Mackenzie King returning for a third term. Bennett remained leader of the Conservative Party until 1938, when he retired to England.

He was created Viscount Bennett, the only Canadian prime minister to be honoured with elevation to the peerage.

Bennett was born on 3 July 1870, when his mother, Henrietta Stiles, was visiting at her parents' home in Hopewell Hill, New Brunswick, Canada. He was the eldest of six children, and grew up nearby at the Bay of Fundy home of his father, Henry John Bennett, in Hopewell Cape, the shire town of Albert County, then a town of 1,800 people.

His father descended from English ancestors who had emigrated to Connecticut in the 17th century. His great-great-grandfather, Zadock Bennett, migrated from New London, Connecticut, to Nova Scotia c. 1760, before the American Revolution, as one of the New England Planters who took the lands forcibly removed from the deported Acadians during the Great Upheaval.

R. B. Bennett's family was poor, subsisting mainly on the produce of a small farm. His early days inculcated a lifelong habit of thrift. The driving force in his family was his mother. She was a Wesleyan Methodist and passed this faith and the Protestant ethic on to her son. Bennett's father does not appear to have been a good provider for his family, though the reason is unclear. He operated a general store for a while and tried to develop some gypsum deposits.

The Bennetts had previously been a relatively prosperous family, operating a shipyard in Hopewell Cape, but the change to steam-powered vessels in the mid-19th century meant the gradual winding down of their business. However, the household was a literate one, subscribing to three newspapers. They were strong Conservatives; indeed one of the largest and last ships launched by the Bennett shipyard (in 1869) was the "Sir John A. Macdonald".

Educated in the local school, Bennett was a very good student, but something of a loner. In addition to his Protestant faith, Bennett grew up with an abiding love of the British Empire, then at its apogee. A small legacy his mother received opened the doors for him to attend the Normal school in Fredericton, where he trained to be a teacher; he then taught for several years at Irishtown, north of Moncton, saving his money for law school.

One day, while Bennett was crossing the Miramichi River on the ferry boat, a well-dressed lad about nine years younger came over to him and struck up a conversation. This was the beginning of an improbable but important friendship with Max Aitken, later the industrialist and British press baron, Lord Beaverbrook. The agnostic Aitken liked to tease the Methodist Bennett, whose fiery temper contrasted with Aitken's ability to turn away wrath with a joke. This friendship would become important to his success later in life, as would his friendship with the Chatham lawyer, Lemuel J. Tweedie, a prominent Conservative politician. He began to study law with Tweedie on weekends and during summer holidays. Another important friendship was with the prominent Shirreff family of Chatham, the father being High Sheriff of Northumberland County for 25 years. The son, Harry, joined the E. B. Eddy Company, a large pulp and paper industrial concern, and was transferred to Halifax. His sister moved there to study nursing, and soon Bennett joined them to study law at Dalhousie University. Their friendship was renewed there, and became crucial to his later life when Jennie Shirreff married the head of the Eddy Company. She later made Bennett the lawyer for her extensive interests.

Bennett started at Dalhousie University in 1890, graduating in 1893 with a law degree and very high standing. He worked his way through with a job as assistant in the library, being recommended by the Dean, Dr. Richard Chapman Weldon, MP, and participated in debating and moot court activities.

He was then a partner in the Chatham law firm of Tweedie and Bennett. Max Aitken (later to become Lord Beaverbrook) was his office boy, while articling as a lawyer, acting as a stringer for the Montreal Gazette, and selling life insurance. Aitken persuaded him to run for alderman in the first Town Council of Chatham, and managed his campaign. Bennett was elected by one vote, and was later furious with Aitken when he heard all the promises he had made on Bennett's behalf.

Despite his election to the Chatham town council, Bennett's days in the town were numbered. He was ambitious and saw that the small community was too narrow a field for him. He was already negotiating with Sir James Lougheed to move to the North-West Territories and become his law partner in Calgary, on Weldon's recommendation. Lougheed was Calgary's richest man and most successful lawyer.
Bennett moved to Calgary in 1897. A lifelong bachelor and teetotaler (although Bennett was known by select associates to occasionally drink alcohol when the press was not around to observe this), he led a rather lonely life in a hotel and later, in a boarding house. For a while a younger brother roomed with him. He ate his noon meal on workdays at the Alberta Hotel. Social life, such as it was, centred on church. There was, however, no scandal attached to his personal life. Bennett worked hard and gradually built up his legal practice. In 1908 he was one of five people appointed to the first Library Board for the city of Calgary and was instrumental in establishing the Calgary Public Library.

In 1910, Bennett became a director of Calgary Power Ltd. (now formally TransAlta Corporation) and just a year later he became President. During his leadership projects completed included the first storage reservoir at Lake Minnewanka, a second transmission line to Calgary and the construction of the Kananaskis Falls hydro station. At that time, he was also director of Rocky Mountains Cement Company and Security Trust.

Bennett developed an extensive legal practice in Calgary. In 1922, he started the partnership Bennett, Hannah & Sanford, which would eventually become Bennett Jones LLP. In 1929-30, he served as national President of the Canadian Bar Association. His successor in that office was Louis St. Laurent, another future Prime Minister.

He was elected to the Legislative Assembly of the North-West Territories in the 1898 general election, representing the riding of West Calgary. He was re-elected to a second term in office in 1902 as an Independent in the North-West Territories legislature.

In 1905, when Alberta was carved out of the Territories and made a province, Bennett became the first leader of the Alberta Conservative Party. In 1909, he won a seat in the provincial legislature, before resigning and switching to federal politics. He was elected to the House of Commons of Canada in 1911.

At age 44, he tried to enlist in the Canadian military once World War I broke out, but was turned down as being medically unfit. In 1916, Bennett was appointed director general of the National service Board, which was in charge of identifying the number of potential recruits in the country.

While Bennett supported the Conservatives, he opposed Prime Minister Robert Borden's proposal for a Union Government that would include both Conservatives and Liberals, fearing that this would ultimately hurt the Conservative Party; he was proven to be correct in this analysis. While he campaigned for Conservative candidates in the 1917 federal election he did not stand for re-election himself.

Nevertheless, Borden's successor, Arthur Meighen appointed Bennett Minister of Justice in his government, as it headed into the 1921 federal election in which both the government and Bennett were defeated. Bennett won the seat of Calgary West in the 1925 federal election and was returned to government as Minister of Finance in Meighen's short-lived government in 1926. The government was defeated in the 1926 federal election. Meighen stepped down as Tory leader, and Bennett became the party's leader in 1927 at the first Conservative leadership convention.

As Opposition leader, Bennett faced off against the more experienced Liberal Prime Minister William Lyon Mackenzie King in Commons debates, and took some time to acquire enough experience to hold his own with King. In 1930, King blundered badly when he made overly partisan statements in response to criticism over his handling of the economic downturn, which was hitting Canada very hard. King's worst error was in stating that he "would not give Tory provincial governments a five-cent piece!" This serious mistake, which drew wide press coverage, gave Bennett his needed opening to attack King, which he did successfully in the election campaign which followed.

As the leader of the Conservative party, Bennett adapted its program, organization image to promote more rapid modernization of Canada. The "New Deal" was largely a mirror of the American program. The party was torn between reaction and reform, with deep internal factionalism that led to its defeat in 1935. Bennett's critics on the left had the last word, and textbooks typically portray him as a hard-driving capitalist, pushing for American-style high tariffs and British-style imperialism, while ignoring his reform efforts.

By defeating the William Lyon Mackenzie King in the 1930 federal election, he had the misfortune of taking office during the Great Depression. Bennett tried to combat the depression by increasing trade within the British Empire and imposing tariffs for imports from outside the Empire, promising that his measures would "blast" Canadian exports into world markets. His success was limited however, and his own wealth (often openly displayed) and impersonal style alienated many struggling Canadians.

While he was the first Prime Minister representing a constituency in Alberta, his party only won four of the province's 16 seats. His speeches to the Empire Clubs in Toronto and Montreal, when chairman of the House of Commons Committee on Representation under Borden, that while settlers from the United States were suitable to be included with those entitled to vote, they however lacked the 'noble element' normally found in the British, caused controversy. At the time, the federal government was required, under a Statute of British Parliament, to re-adjust representation to Alberta and Saskatchewan based on the 1911 census. The re-adjustment made to the four western provinces at the time can only be correlated if only those having British and French origins are considered.

When his "Imperial Preference" policy failed to generate the desired result, Bennett's government had no real contingency plan. The party's pro-business and pro-banking inclinations provided little relief to the millions of increasingly desperate and agitated unemployed. Despite the economic crisis, "laissez-faire" persisted as the guiding economic principle of Conservative Party ideology; similar attitudes dominated worldwide as well during this era. Government relief to the unemployed was considered a disincentive to individual initiative, and was therefore only granted in the most minimal amounts and attached to work programs. An additional concern of the federal government was that large numbers of disaffected unemployed men concentrating in urban centres created a volatile situation. As an "alternative to bloodshed on the streets", the stop-gap solution for unemployment chosen by the Bennett government was to establish military-run and -styled relief camps in remote areas throughout the country, where single unemployed men toiled for twenty cents a day. Any relief beyond this was left to provincial and municipal governments, many of which were either insolvent or on the brink of bankruptcy, and which railed against the inaction of other levels of government. Partisan differences began to sharpen on the question of government intervention in the economy, since lower levels of government were largely in Liberal hands, and protest movements were beginning to send their own parties into the political mainstream, notably the Cooperative Commonwealth Federation and William Aberhart's Social Credit Party in Alberta.

In July 1931, Bennett's government passed the Unemployment and Farm Relief Act in an effort to stanch the depression, but events were rapidly falling out of their control.

Bennett hosted the 1932 Imperial Economic Conference in Ottawa; this was the first time Canada had hosted the meetings. It was attended by the leaders of the independent dominions of the British Empire (which later became the Commonwealth of Nations). Bennett dominated the meetings, which were ultimately unproductive, due to the inability of leaders to agree on policies, mainly to combat the economic woes dominating the world at the time.

A nickname that would stick with Bennett for the remainder of his political career, "Iron Heel Bennett", came from a 1932 speech he gave in Toronto that ironically, if unintentionally, alluded to Jack London's socialist novel:

What do they offer you in exchange for the present order? Socialism, Communism, dictatorship. They are sowing the seeds of unrest everywhere. Right in this city such propaganda is being carried on and in the little out of the way places as well. And we know that throughout Canada this propaganda is being put forward by organizations from foreign lands that seek to destroy our institutions. And we ask that every man and woman put the iron heel of ruthlessness against a thing of that kind. 

Reacting to fears of communist subversion, Bennett invoked the controversial Section 98 of the Criminal Code. Enacted in the aftermath of the Winnipeg general strike, section 98 dispensed with the presumption of innocence in outlawing potential threats to the state: specifically, anyone belonging to an organization that officially advocated the violent overthrow of the government. Even if the accused had never committed an act of violence or personally supported such an action, they could be incarcerated merely for attending meetings of such an organization, publicly speaking in its defense, or distributing its literature. Despite the broad power authorized under section 98, it targeted specifically the Communist Party of Canada. Eight of the top party leaders, including Tim Buck, were arrested on 11 August 1931 and convicted under section 98. This plan to stamp out communism backfired, however, and proved to be a damaging embarrassment for the government, especially after Buck was the target of an apparent assassination attempt. While confined to his cell during a prison riot, despite not participating in the riot, shots were fired into his cell. When an agit-prop play depicting these events, "Eight Men Speak", was suppressed on 4 December 1933 by the Toronto police, a protest meeting was held where Communist politician A. E. Smith repeated the play's allegations, and he was consequently arrested for sedition. This created a storm of public protest, compounded when Buck was called as a witness to the trial and repeated the allegations in open court. Although the remarks were stricken from the record, they still discredited the prosecution's case and Smith was acquitted. As a result, the government's case against Buck lost any credibility, and Buck and his comrades were released early and fêted as heroic champions of civil liberties.

Having survived section 98, and benefiting from the public sympathy wrought by persecution, Communist Party members set out to organize workers in the relief camps set up by the Unemployment and Farm Relief Act. Camp workers laboured on a variety of infrastructure projects, including such things as municipal airports, roads, and park facilities, along with a number of other make-work schemes. Conditions in the camps were poor, not only because of the low pay, but also the lack of recreational facilities, isolation from family and friends, poor quality food, and the use of military discipline. Communists thus had ample grounds on which to organize camp workers, although the workers were there of their own volition. The Relief Camp Workers' Union was formed and affiliated with the Workers' Unity League, the trade union umbrella of the Communist Party. Camp workers in BC struck on 4 April 1935, and, after two months of protesting in Vancouver, began the On-to-Ottawa Trek to bring their grievances to Bennett's doorstep. The Prime Minister and his Minister of Justice, Hugh Guthrie, treated the trek as an attempted insurrection, and ordered it to be stopped. The Royal Canadian Mounted Police (RCMP) read the Riot Act to a crowd of 3,000 strikers and their supporters in Regina on 1 July 1935, resulting in two deaths and dozens of injured. All told, Bennett's anti-Communist policy would not bode well for his political career.

In January 1934, Bennett told the provinces that they were "wasteful and extravagant", and even told Quebec and Ontario that they were wealthy enough to manage their own problems. One year later, he had changed his tune. Following the lead of President Roosevelt's New Deal in the United States, Bennett, under the advice of William Duncan Herridge, who was Canada's Envoy to the United States, the government eventually began to follow the Americans' lead. In a series of five radio speeches to the nation in January 1935, Bennett introduced a Canadian version of the "New Deal", involving unprecedented public spending and federal intervention in the economy. Progressive income taxation, a minimum wage, a maximum number of working hours per week, unemployment insurance, health insurance, an expanded pension program, and grants to farmers were all included in the plan.

In one of his addresses to the nation, Bennett said:
Bennett's conversion, however, was seen as too little too late, and he faced criticism that his reforms either went too far, or did not go far enough, including from one of his cabinet ministers H. H. Stevens, who bolted the government to form the Reconstruction Party of Canada. Some of the measures were alleged to have encroached on provincial jurisdictions laid out in section 92 of the British North America Act, 1867. The courts, including the Judicial Committee of the Privy Council, agreed and eventually struck down virtually all of Bennett's reforms. However, some of Bennett's initiatives, such as the Bank of Canada, which he founded in 1934, remain in place to this day, and the Canadian Wheat Board remained in place until 2011 when the government of Stephen Harper abolished it.

Although there was no unity among the motley political groups that constituted Bennett's opposition, a consensus emerged that his handling of the economic crisis was insufficient and inappropriate, even from Conservative quarters. Bennett personally became a symbol of the political failings underscoring the depression. Car owners, for example, who could no longer afford gasoline, had horses pull their vehicles, which they named "Bennett buggies". Unity in his own administration suffered, notably by the defection of his Minister of Trade, Henry Herbert Stevens. Stevens left the Conservatives and formed the Reconstruction Party of Canada, after Bennett refused to implement Stevens' plan for drastic economic reform to deal with the economic crisis.

The beneficiary of the overwhelming opposition during Bennett's tenure was the Liberal Party. The Tories were decimated in the October 1935 general election, winning only 40 seats to 173 for Mackenzie King's Liberals. The Tories would not form a majority government again in Canada until 1958. King's government soon implemented its own moderate reforms, including the replacement of relief camps with a scaled down provincial relief project scheme, and the repeal of section 98. Ultimately, Canada pulled out of the depression as a result of government-funded jobs associated with the preparation for and onset of the Second World War.

Bennett was something of a far sighted man. For example, during his tenure,

Bennett retired to Britain in 1938, and, on 12 June 1941, became the first and only former Canadian Prime Minister to be elevated to the peerage as Viscount Bennett, of Mickleham in the County of Surrey and of Calgary and Hopewell in the Dominion of Canada. The honour, conferred by British PM Winston Churchill, was in recognition for Bennett's valuable unsalaried work in the Ministry of Aircraft Production, managed by his lifelong friend Lord Beaverbrook. Bennett took an active role in the House of Lords, and attended frequently until his death.

Bennett's interest in increasing public awareness and accessibility to Canada's historical records, led him to serve as Vice-President of The Champlain Society from 1933 until his death.

He died after suffering a heart attack while taking a bath on 26 June 1947 at Mickleham. He was exactly one week shy of his 77th birthday. He is buried there in St. Michael's Churchyard, Mickleham. The tomb, and Government of Canada marker outside, are steps from the front doors of the church. He is the only deceased former Canadian Prime Minister not buried in Canada. Unmarried, Bennett was survived by nephews William Herridge, Jr., and Robert Coats, and by brother Ronald V. Bennett. The viscountcy became extinct on his death.

While Bennett was, and is still, often criticized for lack of compassion for the impoverished masses, he stayed up through many nights reading and responding to personal letters from ordinary citizens asking for his help, and often dipped into his personal fortune to send a five-dollar bill to a starving family. The total amount he gave personally is uncertain, although he personally estimated that in 1927–37 he spent well over 2.3 million dollars. Bennett was a controlling owner of the E. B. Eddy match company, which was the largest safety match manufacturer in Canada, and he was one of the richest Canadians at that time. Bennett helped put many poor, struggling young men through university. Relative to the times he lived in, he was likely the wealthiest Canadian to become prime minister.

Bennett worked an exhausting schedule throughout his years as prime minister, often more than 14 hours per day, and dominated his government, usually holding several cabinet posts. He lived in a suite in the Château Laurier hotel, a short walk from Parliament Hill. The respected author Bruce Hutchison wrote that had the economic times been more normal, Bennett would likely have been regarded as a good, perhaps great, Canadian prime minister.

Bennett was also a noted talent spotter. He took note of and encouraged the young Lester Pearson in the early 1930s, and appointed Pearson to significant roles on two major government inquiries: the 1931 Royal Commission on Grain Futures, and the 1934 Royal Commission on Price Spreads. Bennett saw that Pearson was recognized with an OBE after he shone in that work, arranged a bonus of $1,800, and invited him to a London conference. Former Prime Minister John Turner, who as a child, knew Bennett while he was prime minister, praised Bennett's promotion of Turner's economist mother to the highest civil service post held by a Canadian woman to that time.

Most historians consider his premiership to have been a failure at a time of severe economic crisis. H. Blair Neatby says categorically that "as a politician he was a failure". Jack Granatstein and Norman Hillmer, comparing him to all other Canadian prime ministers concluded, "Bennett utterly failed as a leader. Everyone was alienated by the end—Cabinet, caucus, party, voter and foreigner."

Bennett was ranked #12 by a survey of Canadian historians out of the then 20 Prime Ministers of Canada through Jean Chrétien. The results of the survey were included in the book "Prime Ministers: Ranking Canada's Leaders" by J. L. Granatstein and Norman Hillmer.

A 2001 book by Quebec nationalist writer Normand Lester, "Le Livre noir du Canada anglais" (later translated as "The Black Book of English Canada") accused Bennett of having a political affiliation with, and of having provided financial support to, fascist Quebec writer Adrien Arcand. This is based on a series of letters sent to Bennett following his election as Prime Minister by Arcand, his colleague Ménard and two Conservative caucus members asking for financial support for Arcand's antisemitic newspaper "Le Goglu". The book also claims that in a 1936 letter to Bennett, A. W. Reid, a Conservative organizer, estimated that Conservative Party members gave Arcand a total of $27,000 (the modern equivalent $359,284).

Bennett chose the following jurists to be appointed as justices of the Supreme Court of Canada by the Governor General:

Bennett was the Honorary Colonel of The Calgary Highlanders from the year of their designation as such in 1921 to his death in 1947. He visited the Regiment in England during the Second World War, and always ensured the 1st Battalion had a turkey dinner at Christmas every year they were overseas, including the Christmas of 1944 when the battalion was holding front line positions in the Nijmegen Salient.

Bennett served as the Rector of Queen's University in Kingston, Ontario, from 1935 to 1937, even while he was still prime minister. At the time, this role covered mediation for significant disputes between Queen's students and the university administration.

Bennett's Coat of Arms was designed by Alan Beddoe "Argent within two bendlets Gules three maple leaves proper all between two demi-lions rampant couped gules. Crest, a demi-lion Gules grapsing in the dexter paw a battle axe in bend sinister Or and resting the sinister paw on an escallop also Gules. Supporters, Dexter a buffalo, sinister a moose, both proper. Motto, To be Pressed not Oppressed."

The by-election was caused by the resignation of Richard Bennett, who resigned his seat to run for the House of Commons of Canada in the 1900 Canadian federal election.






</doc>
<doc id="25784" url="https://en.wikipedia.org/wiki?curid=25784" title="Renewable energy">
Renewable energy

Renewable energy is energy that is collected from renewable resources, which are naturally replenished on a human timescale, such as sunlight, wind, rain, tides, waves, and geothermal heat. Renewable energy often provides energy in four important areas: electricity generation, air and water heating/cooling, transportation, and rural (off-grid) energy services.

Based on REN21's 2017 report, renewables contributed 19.3% to humans' global energy consumption and 24.5% to their generation of electricity in 2015 and 2016, respectively. This energy consumption is divided as 8.9% coming from traditional biomass, 4.2% as heat energy (modern biomass, geothermal and solar heat), 3.9% from hydroelectricity and the remaining 2.2% is electricity from wind, solar, geothermal, and other forms of biomass. Worldwide investments in renewable technologies amounted to more than US$286 billion in 2015. In 2017, worldwide investments in renewable energy amounted to US$279.8 billion with China accounting for US$126.6 billion or 45% of the global investments, the United States for US$40.5 billion and Europe for US$40.9 billion. Globally there are an estimated 7.7 million jobs associated with the renewable energy industries, with solar photovoltaics being the largest renewable employer. Renewable energy systems are rapidly becoming more efficient and cheaper and their share of total energy consumption is increasing. As of 2019, more than two-thirds of worldwide newly installed electricity capacity was renewable. Growth in consumption of coal and oil could end by 2020 due to increased uptake of renewables and natural gas.

At the national level, at least 30 nations around the world already have renewable energy contributing more than 20 percent of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond.
Some places and at least two countries, Iceland and Norway, generate all their electricity using renewable energy already, and many other countries have the set a goal to reach 100% renewable energy in the future.
At least 47 nations around the world already have over 50 percent of electricity from renewable resources. Renewable energy resources exist over wide geographical areas, in contrast to fossil fuels, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency technologies is resulting in significant energy security, climate change mitigation, and economic benefits. In international public opinion surveys there is strong support for promoting renewable sources such as solar power and wind power.

While many renewable energy projects are large-scale, renewable technologies are also suited to rural and remote areas and developing countries, where energy is often crucial in human development. As most of renewable energy technologies provide electricity, renewable energy deployment is often applied in conjunction with further electrification, which has several benefits: electricity can be converted to heat (where necessary generating higher temperatures than fossil fuels), can be converted into mechanical energy with high efficiency, and is clean at the point of consumption. In addition, electrification with renewable energy is more efficient and therefore leads to significant reductions in primary energy requirements.

Renewable energy flows involve natural phenomena such as sunlight, wind, tides, plant growth, and geothermal heat, as the International Energy Agency explains:
Renewable energy resources and significant opportunities for energy efficiency exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency, and technological diversification of energy sources, would result in significant energy security and economic benefits. It would also reduce environmental pollution such as air pollution caused by burning of fossil fuels and improve public health, reduce premature mortalities due to pollution and save associated health costs that amount to several hundred billion dollars annually only in the United States. Renewable energy sources, that derive their energy from the sun, either directly or indirectly, such as hydro and wind, are expected to be capable of supplying humanity energy for almost another 1 billion years, at which point the predicted increase in heat from the Sun is expected to make the surface of the earth too hot for liquid water to exist.

Climate change and global warming concerns, coupled with the continuing fall in the costs of some renewable energy equipment, such as wind turbines and solar panels, are driving increased use of renewables. New government spending, regulation and policies helped the industry weather the global financial crisis better than many other sectors. , however, according to the International Renewable Energy Agency, renewables overall share in the energy mix (including power, heat and transport) needs to grow six times faster, in order to keep the rise in average global temperatures "well below" during the present century, compared to pre-industrial levels.

As of 2011, small solar PV systems provide electricity to a few million households, and micro-hydro configured into mini-grids serves many more. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves. United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond, and some 120 countries have various policy targets for longer-term shares of renewable energy, including a 20% target of all electricity generated for the European Union by 2020. Some countries have much higher long-term policy targets of up to 100% renewables. Outside Europe, a diverse group of 20 or more other countries target renewable energy shares in the 2020–2030 time frame that range from 10% to 50%.

Renewable energy often displaces conventional fuels in four areas: electricity generation, hot water/space heating, transportation, and rural (off-grid) energy services:

Prior to the development of coal in the mid 19th century, nearly all energy used was renewable. Almost without a doubt the oldest known use of renewable energy, in the form of traditional biomass to fuel fires, dates from more than a million years ago. Use of biomass for fire did not become commonplace until many hundreds of thousands of years later. Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships in the Persian Gulf and on the Nile. From hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times. Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor, animal power, water power, wind, in grain crushing windmills, and firewood, a traditional biomass.

In the 1860s and 1870s there were already fears that civilization would run out of fossil fuels and the need was felt for a better source. In 1873 Professor Augustin Mouchot wrote:

In 1885, Werner von Siemens, commenting on the discovery of the photovoltaic effect in the solid state, wrote:

Max Weber mentioned the end of fossil fuel in the concluding paragraphs of his Die protestantische Ethik und der Geist des Kapitalismus (The Protestant Ethic and the Spirit of Capitalism), published in 1905. Development of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 "Scientific American" article: "in the far distant future, natural fuels having been exhausted [solar power] will remain as the only means of existence of the human race".

The theory of peak oil was published in 1956. In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil, as well as for an escape from dependence on oil, and the first electricity-generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980.

In 2018, worldwide installed capacity of wind power was 564 GW.

Air flow can be used to run wind turbines. Modern utility-scale wind turbines range from around 600 kW to 9 MW of rated power. The power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine. Areas where winds are stronger and more constant, such as offshore and high-altitude sites, are preferred locations for wind farms. Typically, full load hours of wind turbines vary between 16 and 57 percent annually, but might be higher in particularly favorable offshore sites.

Wind-generated electricity met nearly 4% of global electricity demand in 2015, with nearly 63 GW of new wind power capacity installed. Wind energy was the leading source of new capacity in Europe, the US and Canada, and the second largest in China. In Denmark, wind energy met more than 40% of its electricity demand while Ireland, Portugal and Spain each met nearly 20%.

Globally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore. As offshore wind speeds average ~90% greater than that of land, so offshore resources can contribute substantially more energy than land-stationed turbines.

In 2017, worldwide renewable hydropower capacity was 1,154 GW.

Since water is about 800 times denser than air, even a slow flowing stream of water, or moderate sea swell, can yield considerable amounts of energy. There are many forms of water energy:

Hydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. For countries having the largest percentage of electricity from renewables, the top 50 are primarily hydroelectric. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. There are now three hydroelectricity stations larger than 10 GW: the Three Gorges Dam in China, Itaipu Dam across the Brazil/Paraguay border, and Guri Dam in Venezuela.

Wave power, which captures the energy of ocean surface waves, and tidal power, converting the energy of tides, are two forms of hydropower with future potential; however, they are not yet widely employed commercially. A demonstration project operated by the Ocean Renewable Power Company on the coast of Maine, and connected to the grid, harnesses tidal power from the Bay of Fundy, location of world's highest tidal flow. Ocean thermal energy conversion, which uses the temperature difference between cooler deep and warmer surface waters, currently has no economic feasibility.

In 2017, global installed solar capacity was 390 GW.

Solar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, concentrated solar power (CSP), concentrator photovoltaics (CPV), solar architecture and artificial photosynthesis. Solar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert, and distribute solar energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air. Active solar technologies encompass solar thermal energy, using solar collectors for heating, and solar power, converting sunlight into electricity either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP).

A photovoltaic system converts light into electrical direct current (DC) by taking advantage of the photoelectric effect. Solar PV has turned into a multi-billion, fast-growing industry, continues to improve its cost-effectiveness, and has the most potential of any renewable technologies together with CSP. Concentrated solar power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Commercial concentrated solar power plants were first developed in the 1980s. CSP-Stirling has by far the highest efficiency among all solar energy technologies.

In 2011, the International Energy Agency said that "the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared". Italy has the largest proportion of solar electricity in the world; in 2015, solar supplied 7.7% of electricity demand in Italy. In 2017, after another year of rapid growth, solar generated approximately 2% of global power, or 460 TWh.

Global geothermal capacity in 2017 was 12.9 GW.

High Temperature Geothermal energy is from thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. Earth's geothermal energy originates from the original formation of the planet and from radioactive decay of minerals (in currently uncertain but possibly roughly equal proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective "geothermal" originates from the Greek roots "geo", meaning earth, and "thermos", meaning heat.

The heat that is used for geothermal energy can be from deep within the Earth, all the way down to Earth's core – down. At the core, temperatures may reach over 9,000 °F (5,000 °C). Heat conducts from the core to surrounding rock. Extremely high temperature and pressure cause some rock to melt, which is commonly known as magma. Magma convects upward since it is lighter than the solid rock. This magma then heats rock and water in the crust, sometimes up to .

Low Temperature Geothermal refers to the use of the outer crust of the earth as a Thermal Battery to facilitate Renewable thermal energy for heating and cooling buildings, and other refrigeration and industrial uses. In this form of Geothermal, a Geothermal Heat Pump and Ground-coupled heat exchanger are used together to move heat energy into the earth (for cooling) and out of the earth (for heating) on a varying seasonal basis. Low temperature Geothermal (generally referred to as "GHP") is an increasingly important renewable technology because it both reduces total annual energy loads associated with heating and cooling, and it also flattens the electric demand curve eliminating the extreme summer and winter peak electric supply requirements. Thus Low Temperature Geothermal/GHP is becoming an increasing national priority with multiple tax credit support and focus as part of the ongoing movement toward Net Zero Energy.

Bioenergy global capacity in 2017 was 109 GW.

Biomass is biological material derived from living, or recently living organisms. It most often refers to plants or plant-derived materials which are specifically called lignocellulosic biomass. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: "thermal", "chemical", and "biochemical" methods. Wood remains the largest biomass energy source today; examples include forest residues – such as dead trees, branches and tree stumps –, yard clippings, wood chips and even municipal solid waste. In the second sense, biomass includes plant or animal matter that can be converted into fibers or other industrial chemicals, including biofuels. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo, and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).

Plant energy is produced by crops specifically grown for use as fuel that offer high biomass output per hectare with low input energy. The grain can be used for liquid transportation fuels while the straw can be burned to produce heat or electricity. Plant biomass can also be degraded from cellulose to glucose through a series of chemical treatments, and the resulting sugar can then be used as a first generation biofuel.

Biomass can be converted to other usable forms of energy such as methane gas or transportation fuels such as ethanol and biodiesel. Rotting garbage, and agricultural and human waste, all release methane gasalso called landfill gas or biogas. Crops, such as corn and sugarcane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products such as vegetable oils and animal fats. Also, biomass to liquids (BTLs) and cellulosic ethanol are still under research. There is a great deal of research involving algal fuel or algae-derived biomass due to the fact that it is a non-food resource and can be produced at rates 5 to 10 times those of other types of land-based agriculture, such as corn and soy. Once harvested, it can be fermented to produce biofuels such as ethanol, butanol, and methane, as well as biodiesel and hydrogen. The biomass used for electricity generation varies by region. Forest by-products, such as wood residues, are common in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks). Animal husbandry residues, such as poultry litter, are common in the United Kingdom.

Biofuels include a wide range of fuels which are derived from biomass. The term covers solid, liquid, and gaseous fuels. Liquid biofuels include bioalcohols, such as bioethanol, and oils, such as biodiesel. Gaseous biofuels include biogas, landfill gas and synthetic gas. Bioethanol is an alcohol made by fermenting the sugar components of plant materials and it is made mostly from sugar and starch crops. These include maize, sugarcane and, more recently, sweet sorghum. The latter crop is particularly suitable for growing in dryland conditions, and is being investigated by International Crops Research Institute for the Semi-Arid Tropics for its potential to provide fuel, along with food and animal feed, in arid parts of Asia and Africa.

With advanced technology being developed, cellulosic biomass, such as trees and grasses, are also used as feedstocks for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the United States and in Brazil. The energy costs for producing bio-ethanol are almost equal to, the energy yields from bio-ethanol. However, according to the European Environment Agency, biofuels do not address global warming concerns. Biodiesel is made from vegetable oils, animal fats or recycled greases. It can be used as a fuel for vehicles in its pure form, or more commonly as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. Biofuels provided 2.7% of the world's transport fuel in 2010.

Biomass, biogas and biofuels are burned to produce heat/power and in doing so harm the environment. Pollutants such as sulphurous oxides (SO), nitrous oxides (NO), and particulate matter (PM) are produced from the combustion of biomass; the World Health Organisation estimates that 7 million premature deaths are caused each year by air pollution. Biomass combustion is a major contributor.

Renewable energy production from some sources such as wind and solar is more variable and more geographically spread than technology based on fossil fuels and nuclear. While integrating it into the wider energy system is feasible, it does lead to some additional challenges. In order for the energy system to remain stable, a set of measurements can be taken. Implementation of energy storage, using a wide variety of renewable energy technologies, and implementing a smart grid in which energy is automatically used at the moment it is produced can reduce risks and costs of renewable energy implementation. In some locations, individual households can opt to purchase renewable energy through a consumer green energy program.

Electrical energy storage is a collection of methods used to store electrical energy. 
Electrical energy is stored during times when production (especially from intermittent sources such as wind power, tidal power, solar power) exceeds consumption, and returned to the grid when production falls below consumption. Pumped-storage hydroelectricity accounts for more than 90% of all grid power storage. Costs of lithium-ion batteries are dropping rapidly, and are increasingly being deployed grid ancillary services and for domestic storage.

Renewable power has been more effective in creating jobs than coal or oil in the United States. In 2016, employment in the sector increased 6 percent in the United States, causing employment in the non-renewable energy sector to decrease 18 percent. Worldwide, renewables employ about 8.1 million as of 2016.

From the end of 2004, worldwide renewable energy capacity grew at rates of 10–60% annually for many technologies. In 2015 global investment in renewables rose 5% to $285.9 billion, breaking the previous record of $278.5 billion in 2011. 2015 was also the first year that saw renewables, excluding large hydro, account for the majority of all new power capacity (134 GW, making up 53.6% of the total). Of the renewables total, wind accounted for 72 GW and solar photovoltaics 56 GW; both record-breaking numbers and sharply up from 2014 figures (49 GW and 45 GW respectively). In financial terms, solar made up 56% of total new investment and wind accounted for 38%.

In 2014 global wind power capacity expanded 16% to 369,553 MW. Yearly wind energy production is also growing rapidly and has reached around 4% of worldwide electricity usage, 11.4% in the EU, and it is widely used in Asia, and the United States. In 2015, worldwide installed photovoltaics capacity increased to 227 gigawatts (GW), sufficient to supply 1 percent of global electricity demands. Solar thermal energy stations operate in the United States and Spain, and as of 2016, the largest of these is the 392 MW Ivanpah Solar Electric Generating System in California. The world's largest geothermal power installation is The Geysers in California, with a rated capacity of 750 MW. Brazil has one of the largest renewable energy programs in the world, involving production of ethanol fuel from sugar cane, and ethanol now provides 18% of the country's automotive fuel. Ethanol fuel is also widely available in the United States.

In 2017, investments in renewable energy amounted to US$279.8 billion worldwide, with China accounting for US$126.6 billion or 45% of the global investments, the US for US$40.5 billion, and Europe for US$40.9 billion. The results of a recent review of the literature concluded that as greenhouse gas (GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies.

Renewable energy technologies are getting cheaper, through technological change and through the benefits of mass production and market competition. A 2018 report from the International Renewable Energy Agency (IRENA), found that the cost of renewable energy is quickly falling, and will likely be equal to or less than the cost non-renewables such as fossil fuels by 2020. The report found that solar power costs have dropped 73% since 2010 and onshore wind costs have dropped by 23% in that same timeframe.

Current projections concerning the future cost of renewables vary however. The EIA has predicted that almost two thirds of net additions to power capacity will come from renewables by 2020 due to the combined policy benefits of local pollution, decarbonisation and energy diversification.

According to a 2018 report by Bloomberg New Energy Finance, wind and solar power are expected to generate roughly 50% of the world's energy needs by 2050, while coal powered electricity plants are expected to drop to just 11%.
Hydro-electricity and geothermal electricity produced at favourable sites are now the cheapest way to generate electricity. Renewable energy costs continue to drop, and the levelised cost of electricity (LCOE) is declining for wind power, solar photovoltaic (PV), concentrated solar power (CSP) and some biomass technologies. Renewable energy is also the most economic solution for new grid-connected capacity in areas with good resources. As the cost of renewable power falls, the scope of economically viable applications increases. Renewable technologies are now often the most economic solution for new generating capacity. Where "oil-fired generation is the predominant power generation source (e.g. on islands, off-grid and in some countries) a lower-cost renewable solution almost always exists today". A series of studies by the US National Renewable Energy Laboratory modeled the "grid in the Western US under a number of different scenarios where intermittent renewables accounted for 33 percent of the total power." In the models, inefficiencies in cycling the fossil fuel plants to compensate for the variation in solar and wind energy resulted in an additional cost of "between $0.47 and $1.28 to each MegaWatt hour generated"; however, the savings in the cost of the fuels saved "adds up to $7 billion, meaning the added costs are, at most, two percent of the savings."

In 2017 the world renewable hydropower capacity was 1,154 GW. Only a quarter of the worlds estimated hydroelectric potential of 14,000 TWh/year has been developed, the regional potentials for the growth of hydropower around the world are, 71% Europe, 75% North America, 79% South America, 95% Africa, 95% Middle East, 82% Asia Pacific. However, the political realities of new reservoirs in western countries, economic limitations in the third world and the lack of a transmission system in undeveloped areas, result in the possibility of developing 25% of the remaining potential before 2050, with the bulk of that being in the Asia Pacific area. There is slow growth taking place in Western counties, but not in the conventional dam and reservoir style of the past. New projects take the form of run-of-the-river and small hydro, neither using large reservoirs. It is popular to repower old dams thereby increasing their efficiency and capacity as well as quicker responsiveness on the grid. Where circumstances permit existing dams such as the Russell Dam built in 1985 may be updated with "pump back" facilities for pumped-storage which is useful for peak loads or to support intermittent wind and solar power. Countries with large hydroelectric developments such as Canada and Norway are spending billions to expand their grids to trade with neighboring countries having limited hydro.

Wind power is widely used in Europe, China, and the United States. From 2004 to 2017, worldwide installed capacity of wind power has been growing from 47 GW to 514 GW—a more than tenfold increase within 13 years As of the end of 2014, China, the United States and Germany combined accounted for half of total global capacity. Several other countries have achieved relatively high levels of wind power penetration, such as 21% of stationary electricity production in Denmark, 18% in Portugal, 16% in Spain, and 14% in Ireland in 2010 and have since continued to expand their installed capacity. More than 80 countries around the world are using wind power on a commercial basis.

Wind turbines are increasing in power with some commercially deployed models generating over 8MW per turbine. More powerful models are in development, see list of most powerful wind turbines.


Solar thermal energy capacity has increased from 1.3 GW in 2012 to 5.0 GW in 2017.

Spain is the world leader in solar thermal power deployment with 2.3 GW deployed. The United States has 1.8 GW, most of it in California where 1.4 GW of solar thermal power projects are operational. Several power plants have been constructed in the Mojave Desert, Southwestern United States. As of 2017 only 4 other countries have deployments above 100 MW: South Africa (300 MW) India (229 MW) Morocco (180 MW) and United Arab Emirates (100 MW).

The United States conducted much early research in photovoltaics and concentrated solar power. The U.S. is among the top countries in the world in electricity generated by the Sun and several of the world's largest utility-scale installations are located in the desert Southwest.

The oldest solar thermal power plant in the world is the 354 megawatt (MW) SEGS thermal power plant, in California. The Ivanpah Solar Electric Generating System is a solar thermal power project in the California Mojave Desert, 40 miles (64 km) southwest of Las Vegas, with a gross capacity of 377 MW. The 280 MW Solana Generating Station is a solar power plant near Gila Bend, Arizona, about southwest of Phoenix, completed in 2013. When commissioned it was the largest parabolic trough plant in the world and the first U.S. solar plant with molten salt thermal energy storage.

In developing countries, three World Bank projects for integrated solar thermal/combined-cycle gas-turbine power plants in Egypt, Mexico, and Morocco have been approved.

Photovoltaics (PV) is rapidly-growing with global capacity increasing from 177 GW at the end of 2014 to 385 GW in 2017.

PV uses solar cells assembled into solar panels to convert sunlight into electricity. PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale photovoltaic power station. The predominant PV technology is crystalline silicon, while thin-film solar cell technology accounts for about 10 percent of global photovoltaic deployment. In recent years, PV technology has improved its electricity generating efficiency, reduced the installation cost per watt as well as its energy payback time, and reached grid parity in at least 30 different markets by 2014.
Building-integrated photovoltaics or "onsite" PV systems use existing land and structures and generate power close to where it is consumed.

Photovoltaics grew fastest in China, followed by Japan and the United States. Italy meets 7.9 percent of its electricity demands with photovoltaic power—the highest share worldwide. Solar power is forecasted to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16% and 11%, respectively. This requires an increase of installed PV capacity to 4,600 GW, of which more than half is expected to be deployed in China and India.
Commercial concentrated solar power plants were first developed in the 1980s. As the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and utility-scale solar power stations with hundreds of megawatts are being built. Many solar photovoltaic power stations have been built, mainly in Europe, China and the United States. The 1.5 GW Tengger Desert Solar Park, in China is the world's largest PV power station. Many of these plants are integrated with agriculture and some use tracking systems that follow the sun's daily path across the sky to generate more electricity than fixed-mounted systems.

Bioenergy global capacity in 2017 was 109 GW.
Biofuels provided 3% of the world's transport fuel in 2017.

Mandates for blending biofuels exist in 31 countries at the national level and in 29 states/provinces. According to the International Energy Agency, biofuels have the potential to meet more than a quarter of world demand for transportation fuels by 2050.

Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter. Brazil's ethanol fuel program uses modern equipment and cheap sugarcane as feedstock, and the residual cane-waste (bagasse) is used to produce heat and power. There are no longer light vehicles in Brazil running on pure gasoline. By the end of 2008 there were 35,000 filling stations throughout Brazil with at least one ethanol pump. Unfortunately, Operation Car Wash has seriously eroded public trust in oil companies and has implicated several high ranking Brazilian officials.

Nearly all the gasoline sold in the United States today is mixed with 10% ethanol, and motor vehicle manufacturers already produce vehicles designed to run on much higher ethanol blends. Ford, Daimler AG, and GM are among the automobile companies that sell "flexible-fuel" cars, trucks, and minivans that can use gasoline and ethanol blends ranging from pure gasoline up to 85% ethanol. By mid-2006, there were approximately 6 million ethanol compatible vehicles on U.S. roads.

Global geothermal capacity in 2017 was 12.9 GW.

Geothermal power is cost effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are usually much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.

In 2017, the United States led the world in geothermal electricity production with 12.9 GW of installed capacity. The largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California. The Philippines follows the US as the second highest producer of geothermal power in the world, with 1.9 GW of capacity online.

Renewable energy technology has sometimes been seen as a costly luxury item by critics, and affordable only in the affluent developed world. This erroneous view has persisted for many years, however between 2016 and 2017, investments in renewable energy were higher in developing countries than in developed countries, with China leading global investment with a record 126.6 billion dollars. Many Latin American and African countries increased their investments significantly as well. 
Renewable energy can be particularly suitable for developing countries. In rural and remote areas, transmission and distribution of energy generated from fossil fuels can be difficult and expensive. Producing renewable energy locally can offer a viable alternative.
Technology advances are opening up a huge new market for solar power: the approximately 1.3 billion people around the world who don't have access to grid electricity. Even though they are typically very poor, these people have to pay far more for lighting than people in rich countries because they use inefficient kerosene lamps. Solar power costs half as much as lighting with kerosene. As of 2010, an estimated 3 million households get power from small solar PV systems. Kenya is the world leader in the number of solar power systems installed per capita. More than 30,000 very small solar panels, each producing 1 2 to 30 watts, are sold in Kenya annually. Some Small Island Developing States (SIDS) are also turning to solar power to reduce their costs and increase their sustainability.

Micro-hydro configured into mini-grids also provide power. Over 44 million households use biogas made in household-scale digesters for lighting and/or cooking, and more than 166 million households rely on a new generation of more-efficient biomass cookstoves. Clean liquid fuel sourced from renewable feedstocks are used for cooking and lighting in energy-poor areas of the developing world. Alcohol fuels (ethanol and methanol) can be produced sustainably from non-food sugary, starchy, and cellulostic feedstocks. Project Gaia, Inc. and CleanStar Mozambique are implementing clean cooking programs with liquid ethanol stoves in Ethiopia, Kenya, Nigeria and Mozambique.

Renewable energy projects in many developing countries have demonstrated that renewable energy can directly contribute to poverty reduction by providing the energy needed for creating businesses and employment. Renewable energy technologies can also make indirect contributions to alleviating poverty by providing energy for cooking, space heating, and lighting. Renewable energy can also contribute to education, by providing electricity to schools.

Policies to support renewable energy have been vital in their expansion. Where Europe dominated in establishing energy policy in early 2000s, most countries around the world now have some form of energy policy.

The International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed in 2009, by 75 countries signing the charter of IRENA. As of April 2019, IRENA has 160 member states. The then United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity, and in September 2011 he launched the UN Sustainable Energy for All initiative to improve energy access, efficiency and the deployment of renewable energy.

The 2015 Paris agreement on climate change motivated many countries to develop or improve renewable energy policies. In 2017, a total of 121 countries have adapted some form of renewable energy policy. National targets that year existed in at 176 countries. In addition, there is also a wide range of policies at state/provincial and local levels. Some public utilities help plan or install residential energy upgrades. Under president Barack Obama, the United States policy encouraged the uptake of renewable energy in line with commitments to the Paris agreement. Even though Trump has abandoned these goals, renewable investment is still on the rise.

Many national, state, and local governments have created green banks. A green bank is a quasi-public financial institution that uses public capital to leverage private investment in clean energy technologies. Green banks use a variety of financial tools to bridge market gaps that hinder the deployment of clean energy. The US military has also focused on the use of renewable fuels for military vehicles. Unlike fossil fuels, renewable fuels can be produced in any country, creating a strategic advantage. The US military has already committed itself to have 50% of its energy consumption come from alternative sources.

The incentive to use 100% renewable energy, for electricity, transport, or even total primary energy supply globally, has been motivated by global warming and other ecological as well as economic concerns. The Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. Renewable energy use has grown much faster than even advocates anticipated. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. Also, Professors S. Pacala and Robert H. Socolow have developed a series of "stabilization wedges" that can allow us to maintain our quality of life while avoiding catastrophic climate change, and "renewable energy sources," in aggregate, constitute the largest number of their "wedges".

Using 100% renewable energy was first suggested in a Science paper published in 1975 by Danish physicist Bent Sørensen. It was followed by several other proposals, until in 1998 the first detailed analysis of scenarios with very high shares of renewables were published. These were followed by the first detailed 100% scenarios. In 2006 a PhD thesis was published by Czisch in which it was shown that in a 100% renewable scenario energy supply could match demand in every hour of the year in Europe and North Africa. In the same year Danish Energy professor Henrik Lund published a first paper in which he addresses the optimal combination of renewables, which was followed by several other papers on the transition to 100% renewable energy in Denmark. Since then Lund has been publishing several papers on 100% renewable energy. After 2009 publications began to rise steeply, covering 100% scenarios for countries in Europe, America, Australia and other parts of the world.

In 2011 Mark Z. Jacobson, professor of civil and environmental engineering at Stanford University, and Mark Delucchi published a study on 100% renewable global energy supply in the journal Energy Policy. They found producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be "primarily social and political, not technological or economic". They also found that energy costs with a wind, solar, water system should be similar to today's energy costs.

Similarly, in the United States, the independent National Research Council has noted that "sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand."

The most significant barriers to the widespread implementation of large-scale renewable energy and low carbon energy strategies are primarily political and not technological. According to the 2013 "Post Carbon Pathways" report, which reviewed many international studies, the key roadblocks are: climate change denial, the fossil fuels lobby, political inaction, unsustainable energy consumption, outdated energy infrastructure, and financial constraints.

Other renewable energy technologies are still under development, and include cellulosic ethanol, hot-dry-rock geothermal power, and marine energy. These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding.

There are numerous organizations within the academic, federal, and commercial sectors conducting large scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.
Multiple federally supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners. Sandia has a total budget of $2.4 billion while NREL has a budget of $375 million.




Renewable electricity production, from sources such as wind power and solar power, is sometimes criticized for being variable or intermittent, but is not true for concentrated solar, geothermal and biofuels, that have continuity. In any case, the International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks.

There have been "not in my back yard" (NIMBY) concerns relating to the visual and other impacts of some wind farms, with local residents sometimes fighting or blocking construction. In the United States, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.

A recent UK Government document states that "projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake". In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment.

The market for renewable energy technologies has continued to grow. Climate change concerns and increasing in green jobs, coupled with high oil prices, peak oil, oil wars, oil spills, promotion of electric vehicles and renewable electricity, nuclear disasters and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization. New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors.

While renewables have been very successful in their ever-growing contribution to electrical power there are no countries dominated by fossil fuels who have a plan to stop and get that power from renwables. Only Scotland and Ontario have stopped burning coal, largely due to good natural gas supplies. In the area of transportation, fossil fuels are even more entrenched and solutions harder to find. It's unclear if there are failures with policy or renewable energy, but twenty years after the Kyoto Protocol fossil fuels are still our primary energy source and consumption continues to grow.

From around 2010 onwards, there was increasing discussion about the geopolitical impact of the growing use of renewable energy. It was argued that former fossil fuels exporters would experience a weakening of their position in international affairs, while countries with abundant sunshine, wind, hydropower, or geothermal resources would be strengthened. Also countries rich in critical materials for renewable energy technologies were expected to rise in importance in international affairs.

The ability of biomass and biofuels to contribute to a reduction in emissions is limited because both biomass and biofuels emit large amounts of air pollution when burned and in some cases compete with food supply. Furthermore, biomass and biofuels consume large amounts of water. Other renewable sources such as wind power, photovoltaics, and hydroelectricity have the advantage of being able to conserve water, lower pollution and reduce emissions.





</doc>
<doc id="25789" url="https://en.wikipedia.org/wiki?curid=25789" title="Romulus Augustulus">
Romulus Augustulus

Flavius Romulus Augustus (c. AD 460 – after AD 476; possibly still alive as late as AD 507), known derisively and historiographically as Romulus Augustulus, was the Roman emperor who ruled the Western Roman Empire from 31 October 475 until 4 September 476. He is often described as the "last Western Roman emperor", though some historians consider this to be Julius Nepos. His deposition by Odoacer traditionally marks the end of the Roman Empire in the West, the end of Ancient Rome, and the beginning of the Middle Ages in Western Europe.

Although he, as all other emperors, adopted the name Augustus upon his accession, he is better remembered by his derisive nickname "Augustulus". The Latin suffix "-ulus" is a diminutive; hence "Augustulus" effectively means "Little Augustus". The name "Romulus" was also changed derisively to "Momyllus" meaning "little disgrace".

The historical record contains few details of Romulus' life. He was the son of Orestes, a Roman who once served as a secretary in the court of Attila the Hun before coming into the service of Julius Nepos in AD 475. In the same year he was promoted to the rank of "magister militum", but then led a military revolt that forced Nepos to flee into exile. With the capital of Ravenna under his control, Orestes appointed his son Romulus to the throne despite the lack of support from the eastern court in Constantinople. Romulus, however, was little more than a child and figurehead for his father's rule. After ten months in power, during which time his authority and legitimacy were disputed beyond Italy, Romulus was forced to abdicate by Odoacer, a Germanic foederatus officer who defeated and executed Orestes. After seizing control of Ravenna, Odoacer sent the former emperor to live in the Castellum Lucullanum in Campania, after which he disappears from the historical record.

Romulus' father Orestes was a Roman citizen, originally from Pannonia, who had served as a secretary and diplomat for Attila the Hun and later rose through the ranks of the Roman army. The future emperor was named "Romulus" after his maternal grandfather, a nobleman from Poetovio in Noricum. Many historians have noted the coincidence that the last western emperor bore the names of both Romulus, the legendary founder and first king of Rome, and Augustus, the first emperor. Though this is not as coincidental as it may seem; all emperors bore the name "Augustus" as it was part of the imperial title.

Orestes was appointed Magister militum by Julius Nepos in 475. Shortly after his appointment, Orestes launched a rebellion and captured Ravenna, the capital of the Western Roman Empire since 402, on 28 August 475. Nepos fled to Dalmatia, where his uncle had ruled a semi-autonomous state in the 460s. Orestes, however, refused to become emperor, "from some secret motive", said historian Edward Gibbon. Instead, he installed his son on the throne on 31 October 475.

The empire Augustus ruled was a shadow of its former self and had shrunk significantly over the previous 80 years. Imperial authority had retreated to the Italian borders and parts of southern Gaul: Italia and Gallia Narbonensis, respectively. The Eastern Roman Empire treated its western counterpart as a client state. The Eastern Emperor Leo, who died in 474, had appointed the western emperors Anthemius and Julius Nepos, and Constantinople never recognized the new government. Neither Zeno nor Basiliscus, the two generals fighting for the eastern throne at the time of Romulus' accession, accepted him as ruler.

As a proxy for his father, Romulus made no decisions and left no monuments, although coins bearing his name were minted in Rome, Milan, Ravenna, and Gaul. Several months after Orestes took power, a coalition of Heruli, Scirian, and Turcilingi mercenaries demanded that he give them a third of the land in Italy. When Orestes refused, the tribes revolted under the leadership of the Scirian chieftain Odoacer. Orestes was captured near Piacenza on 28 August 476 and swiftly executed.

Odoacer advanced on Ravenna, capturing the city and the young emperor after the short and decisive Battle of Ravenna. Romulus was compelled to abdicate the throne on 4 September 476. This act has been cited as the end of the Western Roman Empire, although Romulus' deposition did not cause any significant disruption at the time. Rome had already lost its hegemony over the provinces, Germanic peoples dominated the Roman army, and Germanic generals like Odoacer had long been the real powers behind the throne. Italy would suffer far greater devastation in the next century when Emperor Justinian I reconquered it in the Gothic War.

After the abdication of Romulus, the Roman Senate, on behalf of Odoacer, sent representatives to the Eastern Roman Emperor Zeno, whom it asked to formally reunite the two halves of the Empire, with Odoacer as the "protector of the state": "The West, they declared, no longer required an Emperor of its own: one monarch sufficed for the world..." Zeno was asked to make Odoacer a patrician, and administrator of Italy in Zeno's name. Zeno pointed out that the Senate should rightfully have first requested that Julius Nepos take the throne once more, but he nonetheless agreed to their requests as a "fait accompli". Odoacer, already the "de facto" ruler of Italy, now ostensibly ruled "de jure" in Zeno's name.

The ultimate fate of Romulus is a mystery. The "Anonymus Valesianus" wrote that Odoacer, "taking pity on his youth" (he was about 16), spared Romulus' life and granted him an annual pension of 6,000 solidi before sending him to live with relatives in Campania. Jordanes and Marcellinus Comes say Odoacer exiled Romulus to Campania but do not mention any financial support from the Germanic king.

The sources do agree that Romulus took up residence in the Castel dell'Ovo (Lucullan Villa) in Naples, now a castle but originally built as a grand sea-side house by Lucullus in the 1st century BC, fortified by Valentinian III in the mid-5th century. From here, contemporary histories fall silent. In the "History of the Decline and Fall of the Roman Empire", Edward Gibbon notes that the disciples of Saint Severinus of Noricum were invited by a "Neapolitan lady" to bring his body to the villa in 488; Gibbon conjectures from this that Augustulus "was probably no more." The villa was converted into a monastery before 500 to hold the saint's remains.

Cassiodorus, then a secretary to Theodoric the Great, wrote a letter in 507 to a "Romulus" confirming a pension. Thomas Hodgkin, a translator of Cassiodorus' works, wrote in 1886 that it was "surely possible" the Romulus in the letter was the same person as the last western emperor. The letter would match the description of Odoacer's coup in the "Anonymus Valesianus", and Romulus could have been alive in the early sixth century. But Cassiodorus does not supply any details about his correspondent or the size and nature of his pension, and Jordanes, whose history of the period abridges an earlier work by Cassiodorus, makes no mention of a pension.

Some sources suggest that Julius Nepos claimed to hold legally the title of emperor when Odoacer took power. However, few of Nepos' contemporaries were willing to support his cause after he fled to Dalmatia. Some historians regard Julius Nepos, who ruled in Dalmatia until being murdered in 480, as the last lawful Western Roman Emperor.

Following Odoacer's coup, the Roman Senate sent a letter to Zeno stating that "the majesty of a sole monarch is sufficient to pervade and protect, at the same time, both the East and the West". While Zeno told the Senate that Nepos was their lawful sovereign, he did not press the point, and he accepted the imperial insignia brought to him by the senate.

As the last Western Roman emperor before the traditionally agreed-upon end of the Western Roman Empire, Romulus has been portrayed several times in film and literature; the play "Romulus the Great" (1950), by Friedrich Dürrenmatt, focuses on the reign of "Romulus Augustus" and the end of the Roman Empire in the West. The 2007 film "The Last Legion", and the novel on which it is based, includes a heavily fictionalized account of the reign and subsequent life of "Romulus Augustus"; escaping captivity with the aid of a small band of loyal Romans, he reaches Britain, where he eventually becomes Uther Pendragon.




</doc>
<doc id="25791" url="https://en.wikipedia.org/wiki?curid=25791" title="List of Roman emperors">
List of Roman emperors

The Roman emperors were the rulers of the Roman Empire dating from the granting of the title of to Gaius Julius Caesar Octavianus by the Roman Senate in 27 BC, after major roles played by the populist dictator and military leader Julius Caesar. Augustus maintained a facade of Republican rule, rejecting monarchical titles but calling himself (first man of the council) and (first citizen of the state). The title of Augustus was conferred on his successors to the imperial position. The style of government instituted by Augustus is called the Principate and continued until reforms by Diocletian. The modern word 'emperor' derives from the title , which was granted by an army to a successful general; during the initial phase of the empire, the title was generally used only by the . For example, Augustus' official name was "Imperator Caesar Divi Filius Augustus".

The territory under command of the emperor had developed under the period of the Roman Republic as it invaded and occupied most of Europe and portions of northern Africa and western Asia. Under the republic, regions of the empire were ruled by provincial governors answerable to and authorised by the Senate and People of Rome. During the republic, the chief magistrates of Rome were two consuls elected each year; consuls continued to be elected in the imperial period, but their authority was subservient to that of the emperor, and the election was controlled by the emperor. 

In the mid 1st century, Tiberius led major northern military conquests and under his reign, Jesus of Nazareth's preaching commences. Nero began the Persecution of Christians in the Roman Empire.

In the late 3rd century, after the Crisis of the Third Century, Diocletian formalised and embellished the recent manner of imperial rule, establishing the so-called Dominate period of the Roman Empire. This was characterised by the explicit increase of authority in the person of the Emperor, and the use of the style ("Our Lord"). The rise of powerful Barbarian tribes along the borders of the empire and the challenge they posed to defense of far-flung borders and unstable imperial succession led Diocletian to divide the administration geographically of the Empire in 286 with a co-Augustus. 

In 330, Constantine the Great, the first Christian emperor introduced freedom of religion and established a second capital in Byzantium, which he renamed Constantinople. For most of the period from 286 to 480, there was more than one recognised senior emperor, with the division usually based in geographic terms. This division was consistently in place after the death of Theodosius I in 395, which historians have dated as the division between the Western Roman Empire and the Eastern Roman Empire. However, formally the Empire remained a single polity, with separate co-emperors in the separate courts. The fall of the Western Roman Empire, and so the end of a separate list of emperors below, is dated either from the date of 476 when Romulus Augustulus was deposed by the Germanic Herulians led by Odoacer or the date of 480, on the death of Julius Nepos, when Eastern Emperor Zeno ended recognition of a separate Western court. In the period that followed, the Empire is usually treated by historians as the Byzantine Empire governed by the Byzantine Emperors, although this designation is not used universally, and continues to be a subject of specialist debate today. 

In the 7th century, Heraclius meets the Companions of the Prophet Muhammad, thus establishing the first meeting with Islam and later facing the Arab Muslim conquests.

The line of emperors continued until the death of Constantine XI Palaiologos during the Fall of Constantinople in 1453, when the remaining territories were captured by the Ottoman Empire under Mehmed II. The Ottoman dynasty carried on using the tile of Caesar of Rome.

Counting all individuals to have possessed the full imperial title, including those who did not technically rule in their own right (e.g. co-emperors or minors during regencies), this list contains 196 emperors and 6 ruling empresses, for a total of 202 monarchs.

The emperors listed in this article are those generally agreed to have been 'legitimate' emperors, and who appear in published regnal lists. The word 'legitimate' is used by most authors, but usually without clear definition, perhaps not surprisingly, since the emperorship was itself rather vaguely defined legally. In Augustus' original formulation, the "princeps" was selected by either the Senate or "the people" of Rome, but quite quickly the legions became an acknowledged stand-in for "the people." A person could be proclaimed as emperor by their troops or by "the mob" in the street, but in theory needed to be confirmed by the Senate. The coercion that frequently resulted was implied in this formulation. Furthermore, a sitting emperor was empowered to name a successor and take him on as apprentice in government and in that case the Senate had no role to play, although it sometimes did when a successor lacked the power to inhibit bids by rival claimants. By the medieval (or Byzantine) period, the very definition of the Senate became vague as well, adding to the complication.

Lists of legitimate emperors are therefore partly influenced by the subjective views of those compiling them, and also partly by historical convention. Many of the 'legitimate' emperors listed here acceded to the position by usurpation, and many 'illegitimate' claimants had a legitimate claim to the position. Historically, the following criteria have been used to derive emperor lists:

So for instance, Aurelian, though acceding to the throne by usurpation, was the sole and undisputed monarch between 270 and 275, and thus was a legitimate emperor. Gallienus, though not in control of the whole Empire, and plagued by other claimants, was the legitimate heir of (the legitimate emperor) Valerian. Claudius Gothicus, though acceding illegally, and not in control of the whole Empire, was the only claimant accepted by the Senate, and thus, for his reign, was the legitimate emperor. Equally, during the Year of the Four Emperors, all claimants, though not undisputed, were at some point accepted by the Senate and are thus included; conversely, during the Year of the Five Emperors neither Pescennius Niger nor Clodius Albinus were accepted by the Senate, and are thus not included. There are a few examples where individuals were made co-emperor, but never wielded power in their own right (typically the child of an emperor); these emperors are legitimate, but are not included in regnal lists, and in this article are listed together with the senior emperor.

After 395, the list of emperors in the East is based on the same general criteria, with the exception that the emperor only had to be in undisputed control of the Eastern part of the empire, or be the legitimate heir of the Eastern emperor.

The situation in the West is more complex. Throughout the final years of the Western Empire (395–480) the Eastern emperor was considered the senior emperor, and a Western emperor was only legitimate if recognized as such by the Eastern emperor. Furthermore, after 455 the Western emperor ceased to be a relevant figure and there was sometimes no claimant at all. For the sake of historical completeness, all Western Emperors after 455 are included in this list, even if they were not recognized by the Eastern Empire; some of these technically illegitimate emperors are included in regnal lists, while others are not. For instance, Romulus Augustulus was technically a usurper who ruled only the Italian peninsula and was never legally recognized. However, he was traditionally considered the "last Roman Emperor" by 18th and 19th century western scholars and his overthrow by Odoacer used as the marking point between historical epochs, and as such he is usually included in regnal lists. However, modern scholarship has confirmed that Romulus Augustulus' predecessor, Julius Nepos continued to rule as emperor in the other Western holdings and as a figurehead for Odoacer's rule in Italy until Nepos' death in 480. Since the question of what constitutes an emperor can be ambiguous, and dating the "fall of the Western Empire" arbitrary, this list includes details of both figures.

"Note: all dates hereafter."

Note: To maintain control and improve administration, various schemes to divide the work of the Roman Emperor by sharing it between individuals were tried after 285. The "Tetrarchy" proclaimed by Diocletian in 293 split the empire into two halves each to be ruled separately by two emperors, a senior "Augustus", and a junior "Caesar".

Note: Theodosius I was the last person to rule both halves of the Roman Empire, dividing the administration between his sons Arcadius and Honorius on his death.

Note: The classical Roman Empire is usually said to have ended with the deposition of Romulus Augustulus, with its continuation in the East referred to by modern scholars as the Byzantine Empire.

Note: Theodosius I was the last person to rule both halves of the Roman Empire, dividing the administration between his sons Arcadius and Honorius on his death.

Note: Between 1204 and 1261 there was an interregnum when Constantinople was occupied by the crusaders of the Fourth Crusade and the Empire was divided into the Empire of Nicaea, the Empire of Trebizond and the Despotate of Epirus, which were all contenders for rule of the Empire. The Laskarid dynasty of the Empire of Nicaea is considered the legitimate continuation of the Roman Empire because they had the support of the (Orthodox) Patriarch of Constantinople and managed to re-take Constantinople.

Although there were no formal succession laws in place within the empire which would designate a legitimate successor of Constantine XI as emperor, some of his surviving relatives claimed the title after his death:






</doc>
<doc id="25792" url="https://en.wikipedia.org/wiki?curid=25792" title="Roman calendar">
Roman calendar

The Roman calendar was the calendar used by the Roman kingdom and republic. The term often includes the Julian calendar established by the reforms of the dictator Julius Caesar and emperor Augustus in the late 1stcentury and sometimes includes any system dated by inclusive counting towards months' kalends, nones, and ides in the Roman manner. The term usually excludes the Alexandrian calendar of Roman Egypt, which continued the unique months of that land's former calendar; the Byzantine calendar of the later Roman Empire, which usually dated the Roman months in the simple count of the ancient Greek calendars; and the Gregorian calendar, which refined the Julian system to bring it into still closer alignment with the tropical year.

Roman dates were counted inclusively forward to the next of three principal days: the first of the month (the kalends), a day shortly before the middle of the month (the ides), and eight days—nine, counting inclusively—before this (the nones). The original calendar consisted of ten months beginning in spring with March; winter was left as an unassigned span of days. These months ran for 38 nundinal cycles, each forming an eight-day week (nine days counted inclusively, hence the name) ended by religious rituals and a public market. The winter period was later divided into two months, January and February. The legendary early kings Romulus and Numa Pompilius were traditionally credited with establishing this early fixed calendar, which bears traces of its origin as an observational lunar one. In particular, the kalends, nones, and ides seem to have derived from the first sighting of the crescent moon, the first-quarter moon, and the full moon respectively. The system ran well short of the solar year, and it needed constant intercalation to keep religious festivals and other activities in their proper seasons. For superstitious reasons, such intercalation occurred within the month of February even after it was no longer considered the last month.

After the establishment of the Roman Republic, years began to be dated by consulships and control over intercalation was granted to the pontifices, who eventually abused their power by lengthening years controlled by their political allies and shortening the years in their rivals' terms of office. Having won his war with Pompey, Caesar used his position as Rome's chief pontiff to enact a calendar reform in 46, coincidentally making the year of his third consulship last for 446 days. In order to avoid interfering with Rome's religious ceremonies, the reform added all its days towards the ends of months and did not adjust any nones or ides, even in months which came to have 31 days. The Julian calendar was supposed to have a single leap day on 24 February (a doubled ") every fourth year but following Caesar's assassination the priests figured this using inclusive counting and mistakenly added the bissextile day every three years. In order to bring the calendar back to its proper place, Augustus was obliged to suspend intercalation for one or two decades. The revised calendar remained slightly longer than the solar year; by the 16th century the date of Easter had shifted so far away from the vernal equinox that Pope Gregory XIII ordered the calendar’s adjustment, resulting in the Gregorian calendar.

The original Roman calendar is believed to have been an observational lunar calendar whose months began from the first signs of a new crescent moon. Because a lunar cycle is about days long, such months would have varied between 29 and 30 days. Twelve such months would have fallen 10 or 11 days short of the solar year; without adjustment, such a year would have quickly rotated out of alignment with the seasons in the manner of the Islamic calendar. Given the seasonal aspects of the later calendar and its associated religious festivals, this was presumably avoided through some form of intercalation or the suspension of the calendar during winter.

Rome's 8-day week, the nundinal cycle, was shared with the Etruscans, who used it as the schedule of royal audiences. It was presumably a part of the early calendar and was credited in Roman legend variously to Romulus and Servius Tullius.

The Romans themselves described their first organized year as one with ten fixed months, each of 30 or 31 days. Such a decimal division fitted general Roman practice. The four 31 day months were called "full" (') and the others "hollow" ('). Its 304 days made up exactly 38 nundinal cycles. The system is usually said to have left the remaining 50 odd days of the year as an unorganized "winter", although Licinius Macer's lost history apparently stated the earliest Roman calendar employed intercalation instead and Macrobius claims the 10 month calendar was allowed to shift until the summer and winter months were completely misplaced, at which time additional days belonging to no month were simply inserted into the calendar until it seemed things were restored to their proper place.

Later Roman writers credited this calendar to Romulus, their legendary first king and culture hero, although this was common with other practices and traditions whose origin had been lost to them. Some scholars doubt the existence of this calendar at all, as it is only attested in late Republican and Imperial sources and supported only by the misplaced names of the months from September to December. Rüpke also finds the coincidence of the length of the supposed "Romulan" year with the length of the first ten months of the Julian calendar to be suspicious.

Other traditions existed alongside this one, however. Plutarch's "Parallel Lives" recounts that Romulus's calendar had been solar but adhered to the general principle that the year should last for 360 days. Months were employed secondarily and haphazardly, with some counted as 20 days and others as 35 or more.

The attested calendar of the Roman Republic was quite different. It followed Greek calendars in assuming a lunar cycle of days and a solar year of synodic months ( days), which align every fourth year after the addition of two intercalary months. The additional two months of the year were January and February; the intercalary month was sometimes known as Mercedonius.

The Romans did not follow the usual Greek practice in alternating 29- and 30-day months and a 29- or 30-day intercalary month every other year. Instead, their 3rd, 5th, 7th, and 10th months had 31 days each; all the other months had 29 days except February, which had 28 days for three years and then 29 every fourth year. The total of these months over 4 years differed from the Greeks by 5 days, meaning the Roman intercalary month always had 27 days. Similarly, within each month, the weeks did not vary in the Greek fashion between 7 and 8 days; instead, the full months had two additional days in their first week and the other three weeks of every month ran for 8 days ("nine" by Roman reckoning). Still more unusually, the intercalary month was not placed at the end of the year but "within" the month of February after the Terminalia on the 23rd (""); the remaining days of February followed its completion. This seems to have arisen from Roman superstitions concerning the numbering and order of the months. The arrangement of the Roman calendar similarly seems to have arisen from Pythagorean superstitions concerning the luckiness of odd numbers.

These Pythagorean-based changes to the Roman calendar were generally credited by the Romans to Numa Pompilius, Romulus's successor and the second of Rome's seven kings, as were the two new months of the calendar. Most sources thought he had established intercalation with the rest of his calendar. Although Livy's Numa instituted a "lunar" calendar, the author claimed the king had instituted a 19-year system of intercalation equivalent to the Metonic cycle centuries before its development by Babylonian and Greek astronomers. Plutarch's account claims he ended the former chaos of the calendar by employing 12 months totaling 354 days—the length of the lunar and Greek years—and biennial intercalary months of 22 days.

According to Livy's Periochae, the beginning of the consular year changed from March to January 1 in 154 BC to respond to a rebellion in Hispania. Plutarch believed Numa was responsible for placing January and February first in the calendar; Ovid states January began as the first month and February the last, with its present order owing to the Decemvirs. W. Warde Fowler believed the Roman priests continued to treat January and February as the last months of the calendar throughout the Republican period.

The consuls' terms of office were not always a modern calendar year, but ordinary consuls were elected or appointed annually. The traditional list of Roman consuls used by the Romans to date their years began in 509.

Gnaeus Flavius, a secretary ("scriba") to censor App. Claudius Caecus introduced a series of reforms in 304. Their exact nature is uncertain, although he is thought to have begun the custom of publishing the calendar in advance of the month, depriving the priests of some of their power but allowing for a more consistent calendar for official business.

Julius Caesar, following his victory in his civil war and in his role as "pontifex maximus", ordered a reformation of the calendar in 46. This was undertaken by a group of scholars apparently including the Alexandrian Sosigenes and the Roman M. Flavius. Its main lines involved the insertion of ten additional days throughout the calendar and regular intercalation of a single leap day every fourth year to bring the Roman calendar into close agreement with the solar year. The year 46 was the last of the old system and included 3 intercalary months, the first inserted in February and two more—' and '—before the kalends of December.

After Caesar's assassination, Mark Antony had Caesar's birth month Quintilis renamed July (') in his honor. After Antony's defeat at Actium, Augustus assumed control of Rome and, finding the priests had (owing to their inclusive counting) been intercalating every third year instead of every fourth, suspended the addition of leap days to the calendar for one or two decades until its proper position had been restored. See Julian calendar: Leap year error. In 8, the plebiscite "Lex Pacuvia de Mense Augusto" renamed Sextilis August (') in his honor.

In large part, this calendar continued unchanged under the Roman Empire. (Egyptians used the related Alexandrian calendar, which Augustus had adapted from their wandering ancient calendar to maintain its alignment with Rome's.) A few emperors altered the names of the months after themselves or their family, but such changes were abandoned by their successors. Diocletian began the 15-year indiction cycles beginning from the 297 census; these became the required format for official dating under Justinian. Constantine formally established the 7-day week by making Sunday an official holiday in 321. Consular dating became obsolete following the abandonment of appointing nonimperial consuls in 541. The Roman method of numbering the days of the month never became widespread in the Hellenized eastern provinces and was eventually abandoned by the Byzantine Empire in its calendar.

Roman dates were counted inclusively forward to the next one of three principal days within each month:


These are thought to reflect a prehistoric lunar calendar, with the kalends proclaimed after the sighting of the first sliver of the new crescent moon a day or two after the new moon, the nones occurring on the day of the first-quarter moon, and the ides on the day of the full moon. The kalends of each month were sacred to Juno and the ides to Jupiter. The day before each was known as its eve ('); the day after each (') was considered particularly unlucky.

The days of the month were expressed in early Latin using the ablative of time, denoting points in time, in the contracted form "the 6th December Kalends" ('). In classical Latin, this use continued for the three principal days of the month but other days were idiomatically expressed in the accusative case, which usually expressed a duration of time, and took the form "6th day before the December Kalends" ('). This anomaly may have followed the treatment of days in Greek, reflecting the increasing use of such date phrases as an absolute phrase able to function as the object of another preposition, or simply originated in a mistaken agreement of ' with the preposition ' once it moved to the beginning of the expression. In late Latin, this idiom was sometimes abandoned in favor of again using the ablative of time.

The kalends were the day for payment of debts and the account books (") kept for them gave English its word "calendar". The public Roman calendars were the "fasti", which designated the religious and legal character of each month's days. The Romans marked each day of such calendars with the letters:


Each day was also marked by a letter from A to H to indicate its place within the nundinal cycle of market days.

The nundinae were the market days which formed a kind of weekend in Rome, Italy, and some other parts of Roman territory. By Roman inclusive counting, they were reckoned as "ninth days" although they actually occurred every eighth day. Because the republican and Julian years were not evenly divisible into eight-day periods, Roman calendars included a column giving every day of the year a nundinal letter from A to H marking its place in the cycle of market days. Each year, the letter used for the markets would shift 2–5 letters along the cycle. As a day when the city swelled with rural plebeians, they were overseen by the aediles and took on an important role in Roman legislation, which was supposed to be announced for three nundinal weeks (between 17 and 24 days) in advance of its coming to a vote. The patricians and their clients sometimes exploited this fact as a kind of filibuster, since the tribunes of the plebs were required to wait another three-week period if their proposals could not receive a vote before dusk on the day they were introduced. Superstitions arose concerning the bad luck that followed a nundinae on the nones of a month or, later, on the first day of January. Intercalation was supposedly used to avoid such coincidences, even after the Julian reform of the calendar.

The 7-day week began to be observed in Italy in the early imperial period, as practitioners and converts to eastern religions introduced Hellenistic and Babylonian astrology, the Jewish Saturday sabbath, and the Christian Lord's Day. The system was originally used for private worship and astrology but had replaced the nundinal week by the time Constantine made Sunday ("") an official day of rest in  321. The hebdomadal week was also reckoned as a cycle of letters from A to G; these were adapted for Christian use as the dominical letters.

The names of Roman months originally functioned as adjectives (e.g., the January kalends occur in the January month) before being treated as substantive nouns in their own right (e.g., the kalends of January occur in January). Some of their etymologies are well-established: January and March honor the gods Janus and Mars; July and August honor the dictator Julius Caesar and his successor, the emperor Augustus; and the months Quintilis, Sextilis, September, October, November, and December are archaic adjectives formed from the ordinal numbers from 5 to 10, their position in the calendar when it began around the spring equinox in March. Others are uncertain. February may derive from the Februa festival or its eponymous ' ("purifications, expiatory offerings"), whose name may be either Sabine or preserve an archaic word for sulphuric. April may relate to the Etruscan goddess Apru or the verb ' ("to open"). May and June may honor Maia and Juno or derive from archaic terms for "senior" and "junior". A few emperors attempted to add themselves to the calendar after Augustus, but without enduring success.

In classical Latin, the days of each month were usually reckoned as:

Dates after the ides count forward to the kalends of the next month and are expressed as such. For example, March 19 was expressed as "the 14th day before the April Kalends" ('), without a mention of March itself. The day after a kalends, nones, or ides was also often expressed as the "day after" (') owing to their special status as particularly unlucky "black days".

The anomalous status of the new 31-day months under the Julian calendar was an effect of Caesar's desire to avoid affecting the festivals tied to the nones and ides of various months. However, because the dates at the ends of the month all counted forward to the next kalends, they were all shifted by one or two days by the change. This created confusion with regard to certain anniversaries. For instance, Augustus's birthday on the 23rd day of September was ' in the old calendar but ' under the new system. The ambiguity caused honorary festivals to be held on either or both dates.

The Republican calendar only had 355 days, which meant that it would quickly unsynchronize from the solar year, causing, for example, agricultural festivals to occur out of season. The Roman solution to this problem was to periodically lengthen the calendar by adding extra days "within" February. February was broken into two parts, each with an odd number of days. The first part ended with the Terminalia on the 23rd ('), which was considered the end of the religious year; the five remaining days beginning with the Regifugium on the 24th (') formed the second part; and the intercalary month Mercedonius was inserted between them. In such years, the days between the ides and the Regifugium were counted down to either the Intercalary Kalends or to the Terminalia. The intercalary month counted down to nones and ides on its 5th and 13th day in the manner of the other short months. The remaining days of the month counted down towards the March Kalends, so that the end of Mercedonius and the second part of February were indistinguishable to the Romans, one ending on ' and the other picking up at ' and bearing the normal festivals of such dates.

Apparently because of the confusion of these changes or uncertainty as to whether an intercalary month would be ordered, dates after the February ides are attested as sometimes counting down towards the Quirinalia (Feb. 17), the Feralia (Feb. 21), or Terminalia (Feb. 23) rather than the intercalary or March kalends. 

The third-century writer Censorinus says:

When it was thought necessary to add (every two years) an intercalary month of 22 or 23 days, so that the civil year should correspond to the natural (solar) year, this intercalation was in preference made in February, between Terminalia [23rd] and Regifugium [24th].

The fifth-century writer Macrobius says that the Romans intercalated 22 and 23 days in alternate years ("Saturnalia", 1.13.12); the intercalation was placed after 23 February and the remaining five days of February followed ("Saturnalia", 1.13.15). To avoid the nones falling on a nundine, where necessary an intercalary day was inserted "in the middle of the Terminalia, where they placed the intercalary month".

This is historically correct. In 167 BC Intercalaris began on the day after 23 February and in 170 BC it began on the second day after 23 February. Varro, writing in the first century BC, says "the twelfth month was February, and when intercalations take place the five last days of this month are removed." Since all the days after the Ides of Intercalaris were counted down to the beginning of March Intercalaris had either 27 days (making 377 for the year) or 28 (making 378 for the year).

There is another theory which says that in intercalary years February had 23 or 24 days and Intercalaris had 27. No date is offered for the Regifugium in 378-day years. Macrobius describes a further refinement whereby, in one 8-year period within a 24-year cycle, there were only three intercalary years, each of 377 days. This refinement brings the calendar back in line with the seasons, and averages the length of the year to 365.25 days over 24 years.

The Pontifex Maximus determined when an intercalary month was to be inserted. On average, this happened in alternate years. The system of aligning the year through intercalary months broke down at least twice: the first time was during and after the Second Punic War. It led to the reform of the 191  Acilian Law on Intercalation, the details of which are unclear, but it appears to have successfully regulated intercalation for over a century. The second breakdown was in the middle of the first century  and may have been related to the increasingly chaotic and adversarial nature of Roman politics at the time. The position of Pontifex Maximus was not a full-time job; it was held by a member of the Roman elite, who would almost invariably be involved in the machinations of Roman politics. Because the term of office of elected Roman magistrates was defined in terms of a Roman calendar year, a Pontifex Maximus would have reason to lengthen a year in which he or his allies were in power or shorten a year in which his political opponents held office.

Although there are many stories to interpret the intercalation, a period of 22 or 23 days is always ¾ synodic month. Obviously, the month beginning shifts forward (from the new moon, to the third quarter, to the full moon, to the first quarter, back the new moon) after intercalation.

As mentioned above, Rome's legendary 10-month calendar notionally lasted for 304 days but was usually thought to make up the rest of the solar year during an unorganized winter period. The unattested but almost certain lunar year and the pre-Julian civil year were 354 or 355 days long, with the difference from the solar year more or less corrected by an irregular intercalary month. The Julian year was 365 days long, with a leap day doubled in length every fourth year, almost equivalent to the present Gregorian system.

The calendar era before and under the Roman kings is uncertain but dating by regnal years was common in antiquity. Under the Roman Republic, from 509, years were most commonly described in terms of their reigning ordinary consuls. (Temporary and honorary consuls were sometimes elected or appointed but were not used in dating.) Consular lists were displayed on the public calendars. After the institution of the Roman Empire, regnal dates based on the emperors' terms in office became more common. Some historians of the later republic and early imperial eras dated from the legendary founding of the city of Rome (" or ). Varro's date for this was 753 but other writers used different dates, varying by several decades. Such dating was, however, never widespread. After the consuls waned in importance, most Roman dating was regnal or followed Diocletian's 15-year Indiction tax cycle. These cycles were not distinguished, however, so that "year 2 of the indiction" may refer to any of 298, 313, 328, &c. The Orthodox subjects of the Byzantine Empire used various Christian eras, including those based on Diocletian's persecutions, Christ's incarnation, and the supposed age of the world.

The Romans did not have records of their early calendars but, like modern historians, assumed the year originally began in March on the basis of the names of the months following June. The consul M. Fulvius Nobilior (r.189) wrote a commentary on the calendar at the Temple of Hercules Musarum that claimed January had been named for Janus because the god faced both ways, suggesting it had been instituted as a first month. It was, however, usually said to have been instituted along with February, whose nature and festivals suggest it had originally been considered the last month of the year. The consuls' term of office—and thus the order of the years under the republic—seems to have changed several times. Their inaugurations were finally moved to 1January (') in 153 to allow Q. Fulvius Nobilior to attack Segeda in Spain during the Celtiberian Wars, before which they had occurred on 15March ('). There is reason to believe the inauguration date had been 1May during the until 222 and Livy mentions earlier inaugurations on 15May ('), 1July ('), 1August ('), 1October ('), and 15December ("). Under the Julian calendar, the year began on 1January but years of the Indiction cycle began on 1September.

In addition to Egypt's separate calendar, some provinces maintained their records using a local era. Africa dated its records sequentially from 39; Spain from 38. This dating system continued as the Spanish era used in medieval Spain.

The continuity of names from the Roman to the Gregorian calendar can lead to the mistaken belief that Roman dates correspond to Julian or Gregorian ones. In fact, the essentially complete list of Roman consuls allows general certainty of years back to the establishment of the republic but the uncertainty as to the end of lunar dating and the irregularity of Roman intercalation means that dates which can be independently verified are invariably weeks to months outside of their "proper" place. Two astronomical events dated by Livy show the calendar 4 months out of alignment with the Julian date in 190 and 2 months out of alignment in 168. Thus, "the year of the consulship of Publius Cornelius Sciopio Africanus and Publius Licinius Crassus" (usually given as "205") actually began on 15March 205 and ended on 14March 204 according to the Roman calendar but may have begun as early as November or December 206 owing to its misalignment. Even following the establishment of the Julian calendar, the leap years were not applied correctly by the Roman priests, meaning dates are a few days out of their "proper" place until a few decades into Augustus's reign.

Given the paucity of records regarding the state of the calendar and its intercalation, historians have reconstructed the correspondence of Roman dates to their Julian and Gregorian equivalents from disparate sources. There are detailed accounts of the decades leading up to the Julian reform, particularly the speeches and letters of Cicero, which permit an established chronology back to about 58. The nundinal cycle and a few known synchronisms—e.g., a Roman date in terms of the Attic calendar and Olympiad—are used to generate contested chronologies back to the start of the First Punic War in 264. Beyond that, dates are roughly known based on clues such as the dates of harvests and seasonal religious festivals.





</doc>
<doc id="25794" url="https://en.wikipedia.org/wiki?curid=25794" title="Revolver">
Revolver

A revolver (also called a wheel gun) is a repeating handgun that has a revolving cylinder containing multiple chambers and at least one barrel for firing. The revolver allows the user to fire multiple rounds without reloading after every shot, unlike older single-shot firearms. After a round is fired the hammer is cocked and the next chamber in the cylinder is aligned with the barrel by the shooter either manually pulling the hammer back (single action operation) or by rearward movement of the trigger (double action operation).

Revolvers still remain popular as back-up and off-duty handguns among American law enforcement officers and security guards and are still common in the American private sector as defensive and sporting/hunting firearms. Famous revolvers models include the Colt 1851 Navy Revolver, the Webley, the Colt Single Action Army, the Colt Official Police, Smith & Wesson Model 10, the Smith & Wesson Model 29 of "Dirty Harry" fame, the Nagant M1895, and the Colt Python.

Though the majority of weapons using a revolver mechanism are handguns, other firearms may also have a revolver action. These include some models of grenade launchers, shotguns, rifles and cannons. Revolver weapons differ from Gatling-style rotary weapons in that in a revolver only the chambers rotate, while in a rotary weapon there are multiple full firearm actions with their own barrels which rotate around a common ammunition feed.

In the development of firearms, an important limiting factor was the time required to reload the weapon after it was fired. While the user was reloading, the weapon was useless, effectively providing an adversary the opportunity to kill or seriously injure the user. Several approaches to the problem of increasing the rate of fire were developed, the earliest involving multi-barrelled weapons which allowed two or more shots without reloading. 
Later weapons featured multiple barrels revolving along a single axis.

During the late 16th century in China, Zhao Shi-zhen invented the Xun Lei Chong, a five-barreled musket revolver spear. Around the same time, the earliest examples of what today is called a revolver were made in Germany. These weapons featured a single barrel with a revolving cylinder holding the powder and ball. They would soon be made by many European gun-makers, in numerous designs and configurations. However, these weapons were difficult to use, complicated and prohibitively expensive to make, and as such they were not widely distributed. 

In 1836, an American, Samuel Colt, patented a popular revolver which led to the widespread use of the revolver. According to Samuel Colt, he came up with the idea for the revolver while at sea, inspired by the capstan, which had a ratchet and pawl mechanism on it, a version of which was used in his guns to rotate the cylinder by cocking the hammer. This provided a reliable and repeatable way to index each round and did away with the need to manually rotate the cylinder. Revolvers proliferated largely due to Colt's ability as a salesman. But his influence spread in other ways as well; the build quality of his company's guns became famous, and its armories in America and England trained several seminal generations of toolmakers and other machinists, who had great influence in other manufacturing efforts of the next half century.

Early revolvers were caplocks and loaded as a muzzle-loader: the user poured black powder into each chamber, rammed down a bullet on top of it, then placed percussion caps on the nipple at the rear of each chamber, where the hammer would fall on it. This was similar to loading a traditional single-shot muzzle-loading pistol, except that the powder and shot could be loaded directly into the front of the cylinder rather than having to be loaded down the whole length of the barrel. Importantly, this allowed the barrel itself to be rifled, since the user wasn't required to force the tight fitting bullet down the barrel in order to load it (a traditional muzzle-loading pistol had a smoothbore and relatively loose fitting shot, which allowed easy loading, but gave much less accuracy). When firing the next shot, the user would raise his pistol vertically as he cocked the hammer back so as to let the fragments of the burst percussion cap fall out so as to not jam the mechanism. Some of the most popular cap-and-ball revolvers were the Colt Model 1851 "Navy" Model, 1860 "Army" Model, and Colt Pocket Percussion revolvers, all of which saw extensive use in the American Civil War. Although American revolvers were the most common, European arms makers were making numerous revolvers by that time as well, many of which found their way into the hands of the American forces, including the single action Lefaucheux and LeMat revolver and the Beaumont–Adams and Tranter revolvers, which were early double-action weapons, in spite of being muzzle-loaders.
In 1854, Eugene Lefaucheux introduced the Lefaucheux Model 1854, the first revolver to use self-contained metallic cartridges rather than loose powder, pistol ball, and percussion caps. It is a single-action, pinfire revolver holding six rounds.

On November 17, 1856, Daniel B. Wesson and Horace Smith signed an agreement for the exclusive use of the Rollin White Patent at a rate of 25 cents for every revolver. Smith & Wesson began production late in 1857 and enjoyed years of exclusive production of rear-loading cartridge revolvers in America, due to their association with Rollin White, who held the patent and vigorously defended it against any perceived infringement by other manufacturers (much as Colt had done with his original patent on the revolver). Although White held the patent, other manufacturers were able to sell firearms using the design, provided they were willing to pay royalties.

After White's patent expired in April 1869, a 3rd extension was refused. Other gun-makers were then allowed to produce their own weapons using the rear-loading method, without having to pay a royalty on each gun sold. Early guns were often conversions of earlier cap-and-ball revolvers, modified to accept metallic cartridges loaded from the rear, but later models, such as the Colt Model 1872 "Open Top" and the Smith & Wesson Model 3, were designed from the start as cartridge revolvers.
In 1873, Colt introduced the famous Model 1873, also known as the Single Action Army, the "Colt .45" (not to be confused with Colt-made models of the M1911 semi-automatic) or simply, "the Peacemaker", one of the most famous handguns ever made. This popular design, which was a culmination of many of the advances introduced in earlier weapons, fired 6 metallic cartridges and was offered in over 30 different calibers and various barrel lengths. It is still in production, along with numerous clones and lookalikes, and its overall appearance has remained the same since 1873. Although originally made for the United States Army, the Model 1873 was widely distributed and popular with civilians, ranchers, lawmen, and outlaws alike. Its design has influenced countless other revolvers. Colt has discontinued its production twice, but brought it back due to popular demand and continues to make it to this day.

In the U.S. the traditional single-action revolver still reigned supreme until the late 19th century. In Europe, however, arms makers were quick to adopt the double-action trigger. While the US was producing weapons like the Model 1873, the Europeans were building double-action models like the French MAS Modèle 1873 and the somewhat later British Enfield Mk I and II revolvers (Britain relied on cartridge conversions of the earlier Beaumont–Adams double-action prior to this). Colt's first attempt at a double action revolver to compete with the European manufacturers was the Colt Model 1877, which earned lasting notoriety for its overly complex, expensive and fragile trigger mechanism, which in addition to failing frequently, also had a terrible trigger pull unless given the attentions of a competent gunsmith.
In 1889, Colt introduced the Model 1889, the first truly modern double action revolver, which differed from earlier double action revolvers by having a "swing-out" cylinder, as opposed to a "top-break" or "side-loading" cylinder. Swing out cylinders quickly caught on, because they combined the best features of earlier designs. Top-break actions gave the ability to eject all empty shells simultaneously, and exposed all chambers for easy reloading, but having the frame hinged into two halves weakened the gun and negatively affected accuracy, due to lack of rigidity. "Side-loaders", like the earlier Colt Model 1871 and 1873, gave a rigid frame, but required the user to eject and load one chamber at a time, as they rotated the cylinder to line each chamber up with the side-mounted loading gate. Smith & Wesson followed 7 years later with the <nowiki>"Hand Ejector, Model 1896"</nowiki> in .32 S&W Long caliber, followed by the very similar, yet improved, Model 1899 (later known as the Model 10), which introduced the new .38 Special cartridge. The Model 10 went on to become the best selling handgun of the 20th century, at 6,000,000 units, and the .38 Special is still the most popular chambering for revolvers in the world. These new guns were an improvement over the Colt 1889 design since they incorporated a combined center-pin and ejector rod to lock the cylinder in position. The 1889 did not use a center pin and the cylinder was prone to move out of alignment.

Revolvers have remained popular to the present day in many areas, although in the military and law enforcement, they have largely been supplanted by magazine-fed semi-automatic pistols such as the Beretta M9, especially in circumstances where reload time and higher cartridge capacity are deemed important.

Elisha Collier of Boston, Massachusetts patented a flintlock revolver in Britain in 1818, and significant numbers were being produced in London by 1822. The origination of this invention is in doubt, as similar designs were patented in the same year by Artemus Wheeler in the United States and by Cornelius Coolidge in France. Samuel Colt submitted a British patent for his revolver in 1835 and an American patent (number 138) on February 25, 1836 for a "Revolving gun", and made the first production model on March 5 of that year.

Another revolver patent was issued to Samuel Colt on August 29, 1839. The February 25, 1836 patent was then reissued as entitled "Revolving gun" on October 24, 1848. This was followed by on September 3, 1850 for a "Revolver", and by on September 10, 1850 for a "Revolver". was issued to Roger C. Field for an economical device for minimizing the flash gap of a revolver between the barrel and the cylinder. In 1855, Rollin White patented the bored-through cylinder entitled "Improvement in revolving fire-arms" . In 1856 Horace Smith & Daniel Wesson formed a partnership (S&W), developed and manufactured a revolver chambered for a self-contained metallic cartridge.

A revolver works by having several firing chambers arranged in a circle in a cylindrical block that are brought into alignment with the firing mechanism and barrel one at a time. In contrast, other repeating firearms, such as bolt-action, lever-action, pump-action, and semi-automatic, have a single firing chamber and a mechanism to load and extract cartridges into it.

A single-action revolver requires the hammer to be pulled back by hand before each shot, which also revolves the cylinder. This leaves the trigger with just one "single action" left to perform - releasing the hammer to fire the shot - so the force and distance required to pull the trigger can be minimal. In contrast, with a self-cocking revolver, one long squeeze of the trigger pulls back the hammer and revolves the cylinder, then finally fires the shot. They can generally be fired faster than a single-action, but with reduced accuracy in the hands of most shooters.

Most modern revolvers are "traditional double-action", which means they may operate either in single-action or self-cocking mode. The accepted meaning of "double-action" has, confusingly, come to be the same as "self-cocking", so modern revolvers that cannot be pre-cocked are called "double-action-only". These are intended for concealed carry, because the hammer of a traditional design is prone to snagging on clothes when drawn. Most revolvers do not come with accessory rails, which are used for mounting lights and lasers, except for the Smith & Wesson M&P R8 (.357 Magnum), Smith & Wesson Model 325 Thunder Ranch (.45 ACP), and all versions of the Chiappa Rhino (.357 Magnum, 9×19mm, .40 S&W, or 9×21mm) except for the 2" model, respectively. However, certain revolvers, such as the Taurus Judge and Charter Arms revolvers, can be fitted with accessory rails.

Most commonly, such revolvers have 5 or 6 chambers, hence the common names of "six-gun" or "six-shooter". However, some revolvers have 7, 8, 9, or 10 chambers, often depending on the caliber, and at least one revolver has 12 chambers (the US Fire Arms Model 12/22). Each chamber has to be reloaded manually, which makes reloading a revolver a much slower procedure than reloading a semi-automatic pistol.

Compared to autoloading handguns, a revolver is often much simpler to operate and may have greater reliability. For example, should a semiautomatic pistol fail to fire, clearing the chamber requires manually cycling the action to remove the errant round, as cycling the action normally depends on the energy of a cartridge firing. With a revolver, this is not necessary as none of the energy for cycling the revolver comes from the firing of the cartridge, but is supplied by the user either through cocking the hammer or, in a double-action design, by just squeezing the trigger. Another significant advantage of revolvers is superior ergonomics, particularly for users with small hands. A revolver's grip does not hold a magazine, and it can be designed or customized much more than the grip of a typical semi-automatic. Partially because of these reasons, revolvers still hold significant market share as concealed carry and home-defense weapons.

A revolver can be kept loaded and ready to fire without fatiguing any springs and is not very dependent on lubrication for proper firing. Additionally, in the case of double-action-only revolvers there is no risk of accidental discharge from dropping alone, as the hammer is cocked by the trigger pull. However, the revolver's clockwork-like internal parts are relatively delicate and can become misaligned after a severe impact, and its revolving cylinder can become jammed by excessive dirt or debris.

Over the long period of development of the revolver, many calibers have been used. Some of these have proved more durable during periods of standardization and some have entered general public awareness. Among these are the .22 rimfire, a caliber popular for target shooting and teaching novice shooters; .38 Special and .357 Magnum, known for police use; the .44 Magnum, famous from Clint Eastwood's "Dirty Harry" films; and the .45 Colt, used in the Colt revolver of the Wild West. Introduced in 2003, the Smith & Wesson Model 500 is one of the most powerful revolvers, utilizing the .500 S&W Magnum cartridge.

Because the rounds in a revolver are headspaced on the rim, some revolvers are capable of chambering more than one type of ammunition. The .44 Magnum round will chamber the shorter .44 Special and shorter .44 Colt, likewise the .357 Magnum will safely chamber .38 Special and .38 Short Colt. In 1996 a revolver known as the Medusa M47 was made that could chamber 25 different cartridges with bullet diameters between .355" and .357".

Revolver technology lives on in other weapons used by the military. Some autocannons and grenade launchers use mechanisms similar to revolvers, and some riot shotguns use spring-loaded cylinders holding up to 12 rounds. In addition to serving as backup guns, revolvers still fill the specialized niche role as a shield gun; law enforcement personnel using a "bulletproof" gun shield sometimes opt for a revolver instead of a self-loading pistol, because the slide of a pistol may strike the front of the shield when fired. Revolvers do not suffer from this disadvantage. A second revolver may be secured behind the shield to provide a quick means of continuity of fire. Many police also still use revolvers as their duty weapon due to their relative mechanical simplicity and user friendliness.

With the advancement of technology and design in 2010 major revolver manufacturers are coming out with polymer frame revolvers like the Ruger LCR, Smith & Wesson Bodyguard 38, and Taurus Protector Polymer. The new innovative design incorporates advanced polymer technology that lowers weight significantly, helps absorbs recoil, and strong enough to handle .38 Special +P and .357 Magnum loads. The polymer is only used on the lower frame and joined to a metal alloy upper frame, barrel, and cylinder. Polymer technology is considered one of the major advancements in revolver history because the frame has always been metal alloy and mostly one piece frame design.

Another recent development in revolver technology is the Rhino, a revolver introduced by Italian manufacturer Chiappa in 2009 and first sold in the U.S. in 2010. The Rhino, built with the U.S. concealed carry market in mind, is designed so that the bullet fires from the bottom chamber of the cylinder instead of the top chamber as in standard revolvers. This is intended to reduce muzzle flip, allowing for faster and more accurate repeat shots. In addition, the cylinder cross-section is hexagonal instead of circular, further reducing the weapon's profile.

The first revolvers were "front loading" (also referred to as muzzleloading), and were a bit like muskets in that the powder and bullet were loaded separately. These were caplocks or "cap and ball" revolvers, because the caplock method of priming was the first to be compact enough to make a practical revolver feasible. When loading, each chamber in the cylinder was rotated out of line with the barrel, and charged from the front with loose powder and an oversized bullet. Next, the chamber was aligned with the ramming lever underneath the barrel. Pulling the lever would drive a rammer into the chamber, pushing the ball securely in place. Finally, the user would place percussion caps on the nipples on the rear face of the cylinder.

After each shot, a user was advised to raise his revolver vertically while cocking back the hammer so as to allow the fragments of the spent percussion cap to fall out safely. Otherwise, the fragments could fall into the revolver's mechanism and jam it. Caplock revolvers were vulnerable to "chain fires", wherein hot gas from a shot ignited the powder in the other chambers. This could be prevented by sealing the chambers with cotton, wax, or grease.

Loading a cylinder in this manner was a slow and awkward process and generally could not be done in the midst of battle. Some soldiers solved this by carrying multiple revolvers in the field. Another solution was to use a revolver with a detachable cylinder design. These revolvers allowed the shooter to quickly remove a cylinder and replace it with a full one.

In many of the first generation of cartridge revolvers (especially those that were converted after manufacture), the base pin on which the cylinder revolved was removed, and the cylinder taken from the revolver for loading. Most revolvers using this method of loading are single-action revolvers, although Iver Johnson produced double-action models with removable cylinders. The removable-cylinder design is employed in some modern "micro-revolvers" (usually in .22 caliber), in order to simplify their design. These weapons are small enough to fit in the palm of the hand.

Later single-action revolver models with a fixed cylinder used a loading gate at the rear of the cylinder that allowed insertion of one cartridge at a time for loading, while a rod under the barrel could be pressed rearward to eject the fired case.

The loading gate on the original Colt designs (and on nearly all single-action revolvers since, such as the famous Colt Single Action Army) is on the right side, which was done to facilitate loading while on horseback; with the revolver held in the left hand with the reins of the horse, the cartridges can be ejected and loaded with the right hand.

Because the cylinders in these types of revolvers are firmly attached at the front and rear of the frame, and the frame is typically full thickness all the way around, fixed cylinder revolvers are inherently strong designs. Accordingly, many modern large caliber hunting revolvers tend to be based on the fixed cylinder design. Fixed cylinder revolvers can fire the strongest and most powerful cartridges, but at the price of being the slowest to load and reload and they cannot use speedloaders or moon clips for loading, as only one chamber is exposed at a time to the loading gate.

In a top-break revolver, the frame is hinged at the bottom front of the cylinder. Releasing the lock and pushing the barrel down exposes the rear face of the cylinder. In most top-break revolvers, this act also operates an extractor that pushes the cartridges in the chambers back far enough that they will fall free, or can be removed easily. Fresh rounds are then inserted into the cylinder. The barrel and cylinder are then rotated back and locked in place, and the revolver is ready to fire.

Top break revolvers can be loaded more rapidly than fixed-frame revolvers, especially with the aid of a speedloader or moon clip. However, this design is much weaker and cannot handle high pressure rounds. While this design is mostly obsolete today, supplanted by the stronger yet equally convenient swing-out design, manufacturers have begun making reproductions of late 19th century designs for use in cowboy action shooting.

The most commonly found top-break revolvers were manufactured by Smith & Wesson, Webley & Scott, Iver Johnson, Harrington & Richardson, Manhattan Fire Arms, Meriden Arms and Forehand & Wadsworth.

The tip-up was the first revolver design for use with metallic cartridges in the Smith & Wesson Model 1, on which the barrel pivoted upwards, hinged on the forward end of the topstrap. On the S & W tip-up revolvers, the barrel release catch is located on both sides of the frame in front of the trigger. Smith & Wesson discontinued it in the third series of the Smith & Wesson Model 1 1/2 but it was fairly widely used in Europe in the 19th century, after a patent by Spirlet in 1870, which also included an ejector.

The most modern method of loading and unloading a revolver is by means of the "swing out cylinder". The cylinder is mounted on a pivot that is parallel to the chambers, and the cylinder swings out and down (to the left in most cases). An extractor is fitted, operated by a rod projecting from the front of the cylinder assembly. When pressed, it will push all fired rounds free simultaneously (as in top break models, the travel is designed to not completely extract longer, unfired rounds). The cylinder may then be loaded, singly or again with a speedloader, closed, and latched in place.

The pivoting part that supports the cylinder is called the crane; it is the weak point of swing-out cylinder designs. Using the method often portrayed in movies and television of flipping the cylinder open and closed with a flick of the wrist can in fact cause the crane to bend over time, throwing the cylinder out of alignment with the barrel. Lack of alignment between chamber and barrel is a dangerous condition, as it can impede the bullet's transition from chamber to barrel. This gives rise to higher pressures in the chamber, bullet damage, and the potential for an explosion if the bullet becomes stuck.

The shock of firing can exert a great deal of stress on the crane, as in most designs the cylinder is only held closed at one point, the rear of the cylinder. Stronger designs, such as the Ruger Super Redhawk, use a lock in the crane as well as the lock at the rear of the cylinder. This latch provides a more secure bond between cylinder and frame, and allows the use of larger, more powerful cartridges. Swing out cylinders are rather strong, but not as strong as fixed cylinders, and great care must be taken with the cylinder when loading, so as not to damage the crane.

One unique design was designed by Merwin Hulbert in which the barrel and cylinder assembly were rotated 90° and pulled forward to eject shells from the cylinder.

In a single-action revolver, the hammer is manually cocked, usually with the thumb of the firing or supporting hand. This action advances the cylinder to the next round and locks the cylinder in place with the chamber aligned with the barrel. The trigger, when pulled, releases the hammer, which fires the round in the chamber. To fire again, the hammer must be manually cocked again. This is called "single-action" because the trigger only performs a single action, of releasing the hammer. Because only a single action is performed and trigger pull is lightened, firing a revolver in this way allows most shooters to achieve greater accuracy. Additionally, the need to cock the hammer manually acts as a safety. Unfortunately with some revolvers, since the hammer rests on the primer or nipple, accidental discharge from impact is more likely if all 6 chambers are loaded. The Colt Paterson Revolver, the Walker Colt, the Colt's Dragoon and the Colt Single Action Army pistol of the American Frontier era are all good examples of this system.

In double-action (DA), the stroke of the trigger pull generates two actions: 
Thus, DA means that a cocking action separate from the trigger pull is unnecessary; every trigger pull will result in a complete cycle. This allows uncocked carry, while also allowing draw-and-fire using only the trigger. A longer and harder trigger stroke is the trade-off. However, this drawback can also be viewed as a safety feature, as the gun is safer against accidental discharges from being dropped.

Most double-action revolvers may be fired in two ways.
Certain revolvers, called "double-action-only" (DAO) or, more correctly but less commonly, "self-cocking", lack the latch that enables the hammer to be locked to the rear, and thus can only be fired in the double-action mode. With no way to lock the hammer back, DAO designs tend to have "bobbed" or "spurless" hammers, and may even have the hammer completely covered by the revolver's frame (i.e., shrouded or hooded). These are generally intended for concealed carrying, where a hammer spur could snag when the revolver is drawn. The potential reduction in accuracy in aimed fire is offset by the increased capability for concealment.

DA and DAO revolvers were the standard-issue sidearm of countless police departments for many decades. Only in the 1980s and 1990s did the semiautomatic pistol begin to make serious inroads after the advent of safe actions. The reasons for these choices are the modes of carry and use. Double action is good for high-stress situations because it allows a mode of carry in which "draw and pull the trigger" is the only requirement—no safety catch release nor separate cocking stroke is required.

In the cap-and-ball days of the mid 19th century, two revolver models, the English Tranter and the American Savage "Figure Eight", used a method whereby the hammer was cocked by the shooter’s middle finger pulling on a second trigger below the main trigger.

Iver Johnson made an unusual model from 1940 to 1947 called the "Trigger Cocking Double Action". If the hammer was down, pulling the trigger would cock the hammer. If the trigger was pulled with the hammer cocked, it would then fire. This meant that to fire the revolver from a hammer down state, the trigger must be pulled twice.

The Zig zag revolver is a 3D printed .38 Revolver made public in May 2014. It was created by a $500 3D-printer using plastic filament, but the name of the printer was not revealed by the creator. It was created by a Japanese citizen from Kawasaki named Yoshitomo Imura. He was arrested in May 2014 after he had posted a video online of himself firing a 3D printed Zig Zag revolver. It is the first 3D printed Japanese gun in the world which can discharge live cartridges.

As a general rule, revolvers cannot be effective with a sound suppressor ("silencer"), as there is usually a small gap between the revolving cylinder and the barrel which a bullet must traverse or jump when fired. From this opening, a rather loud report is produced. A suppressor can only suppress noise coming from the muzzle.

A suppressible revolver design does exist in the Nagant M1895, a Belgian designed revolver used by Imperial Russia and later the Soviet Union from 1895 through World War II. This revolver uses a unique cartridge whose case extends beyond the tip of the bullet, and a cylinder that moves forward to place the end of the cartridge inside the barrel when ready to fire. This bridges the gap between the cylinder and the barrel, and expands to seal the gap when fired. While the tiny gap between cylinder and barrel on most revolvers is insignificant to the internal ballistics, the seal is especially effective when used with a suppressor, and a number of suppressed Nagant revolvers have been used since its invention.

There is a modern revolver of Russian design, the OTs-38, which uses ammunition that incorporates the silencing mechanism into the cartridge case, making the gap between cylinder and barrel irrelevant as far as the suppression issue is concerned. The OTs-38 does need an unusually close and precise fit between the cylinder and barrel due to the shape of bullet in the special ammunition (Soviet SP-4), which was originally designed for use in a semi-automatic.

Additionally, the US Military experimented with designing a special version of the Smith & Wesson Model 29 for Tunnel Rats, called the Quiet Special Purpose Revolver or QSPR. Using special .40 caliber ammunition, it never entered official service.

The term "automatic revolver" has two different meanings, the first being used in the late nineteenth and early twentieth centuries when "automatic" referred not to the operational mechanism of firing, but of extraction and ejection of spent casings. An "automatic revolver" in this context is one which extracts empty fired cases "automatically," i.e., upon breaking open the action, rather than requiring manual extraction of each case individually with a sliding rod or pin (as in the Colt Single Action Army design). This term was widely used in the advertising of the period as a way to distinguish such revolvers from the far more common rod-extraction types.

In the second sense, "automatic revolver" refers to the mechanism of firing rather than extraction. Double-action revolvers use a long trigger pull to cock the hammer, thus negating the need to manually cock the hammer between shots. The disadvantage of this is that the long, heavy pull cocking the hammer makes the double-action revolver much harder to shoot accurately than a single-action revolver (although cocking the hammer of a double-action reduces the length and weight of the trigger pull). A rare class of revolvers, called automatic for its firing design, attempts to overcome this restriction, giving the high speed of a double-action with the trigger effort of a single-action. The Webley-Fosbery Automatic Revolver is the most famous commercial example. It was recoil-operated, and the cylinder and barrel recoiled backwards to cock the hammer and revolve the cylinder. Cam grooves were milled on the outside of the cylinder to provide a means of advancing to the next chamber—half a turn as the cylinder moved back, and half a turn as it moved forward. .38 caliber versions held eight shots, .455 caliber versions six. At the time, the few available automatic pistols were larger, less reliable, and more expensive. The automatic revolver was popular when it first came out, but was quickly superseded by the creation of reliable, inexpensive semi-automatic pistols.

In 1997, the Mateba company developed a type of recoil-operated automatic revolver, commercially named the Mateba Autorevolver, which uses the recoil energy to auto-rotate a normal revolver cylinder holding six or seven cartridges, depending on the model. The company has made several versions of its Autorevolver, including longer-barrelled and carbine variations, chambered for .357 Magnum, .44 Magnum and .454 Casull.

The Pancor Jackhammer is a combat shotgun based on a similar mechanism to an automatic revolver. It uses a blow-forward action to move the barrel forward (which unlocks it from the cylinder) and then rotate the cylinder and cock the hammer.

Revolvers were not limited to handguns and as a longer barrelled arm is more useful in military applications than a sidearm, the idea was applied to both rifles and shotguns throughout the history of the revolver mechanism with mixed degrees of success.

Revolving rifles were an attempt to increase the rate of fire of rifles by combining them with the revolving firing mechanism that had been developed earlier for revolving pistols. Colt began experimenting with revolving rifles in the early 19th century, making them in a variety of calibers and barrel lengths. Colt revolving rifles were the first repeating rifles adopted by the U.S. Government, but they had their problems. They were officially given to soldiers because of their rate of fire. But after firing six shots, the shooter had to take an excessive amount of time to reload. Also, on occasion Colt rifles discharged all their rounds at once, endangering the shooter. Even so, an early model was used in the Seminole Wars in 1838. During the Civil War a LeMat Carbine was made based on the LeMat revolver.

Colt briefly manufactured several revolving shotguns that were met with mixed success. The Colt Model 1839 Shotgun was manufactured between 1839 and 1841. Later, the Colt Model 1855 Shotgun, based on the Model 1855 revolving rifle, was manufactured between 1860 and 1863. Because of their low production numbers and age they are among the rarest of all Colt firearms.

The Armsel Striker was a modern take on the revolving shotgun that held 10 rounds of 12 Gauge ammunition in its cylinder. It was copied by Cobray as the Streetsweeper.

Taurus manufactures a carbine variant of the Taurus Judge revolver along with its Australian partner company, Rossi known as the "Taurus/Rossi Circuit Judge". It comes in the original combination chambering of .410 bore and .45 Long Colt, as well as the .44 Remington Magnum chambering. The rifle has small blast shields attached to the cylinder to protect the shooter from hot gases escaping between the cylinder and barrel.

The MTs255 () is a shotgun fed by a 5-round internal revolving cylinder. It is produced by the TsKIB SOO, Central Design and Research Bureau of Sporting and Hunting Arms. They are available in 12, 20, 28 and 32 gauges, and .410 bore.

The Hawk MM-1, Milkor MGL, RG-6, and RGP-40 are grenade launchers that use a revolver action. Because the cylinders are much more massive, they use a spring-wound mechanism to index the cylinder.

Revolver cannons use a motor-driven revolver-like mechanism to fire.

A six gun is a revolver that holds six cartridges. The cylinder in a six gun is often called a "wheel", and the six gun is itself often called a "wheel gun". Although a "six gun" can refer to any six-chambered revolver, it is typically a reference to the Colt Single Action Army, or its modern look-alikes such as the Ruger Vaquero and Beretta Stampede.

Until the 1970s, when older-design revolvers such as the Colt Single Action Army and Ruger Blackhawk were re-engineered with drop safeties (such as firing pin blocks, hammer blocks, or transfer bars) that prevent the firing pin from contacting the cartridge's primer unless the trigger is pulled, safe carry required the hammer being positioned over an empty chamber, reducing the available cartridges from six to five, or, on some models, in between chambers on either a pin or in a groove for that purpose, thus keeping the full six rounds available. This kept the uncocked hammer from resting directly on the primer of a cartridge. If not used in this manner, the hammer rests directly on a primer and unintentional firing may occur if the gun is dropped or the hammer is struck. Some holster makers provided a thick leather thong to place underneath the hammer that both allowed the carry of a gun fully loaded with all six rounds and secured the gun in the holster to help prevent its accidental loss.

Six guns are used commonly by single-action shooting enthusiasts in shooting competitions, designed to mimic the gunfights of the Old West, and for general target shooting, hunting and personal defense.




</doc>
<doc id="25795" url="https://en.wikipedia.org/wiki?curid=25795" title="Robert Freitas">
Robert Freitas

Robert A. Freitas Jr. (born 1952) is a nanotechnology scientist.

Freitas holds a 1974 Bachelor's degree majoring in both physics and psychology from Harvey Mudd College, and a 1978 Juris Doctor (J.D.) degree from Santa Clara University School of Law. He has written more than 150 technical papers, book chapters, or popular articles on a diverse set of scientific, engineering, and legal topics.

Freitas began writing his Nanomedicine book series in 1994. Volume I, published in October 1999 by Landes Bioscience while Freitas was a Research Fellow at the Institute for Molecular Manufacturing. Volume IIA was published in October 2003 by Landes Bioscience.

In 2004, Freitas and Ralph Merkle coauthored and published Kinematic Self-Replicating Machines, a comprehensive survey of the field of physical and hypothetical self-replicating machines.

In 2009, Freitas was awarded the Feynman Prize in Nanotechnology.





</doc>
<doc id="25797" url="https://en.wikipedia.org/wiki?curid=25797" title="Robert Morris">
Robert Morris

Robert or Bob Morris may refer to:







</doc>
<doc id="25798" url="https://en.wikipedia.org/wiki?curid=25798" title="Reykjavík">
Reykjavík

Reykjavík ( ; ) is the capital and largest city of Iceland. It is located in southwestern Iceland, on the southern shore of Faxaflói bay. Its latitude is 64°08' N, making it the world's northernmost capital of a sovereign state. With a population of around 128,793 (and 228,231 in the Capital Region), it is the center of Iceland's cultural, economic and governmental activity, and is a popular tourist destination.

Reykjavík is believed to be the location of the first permanent settlement in Iceland, which, according to Landnámabók, was established by Ingólfr Arnarson in AD 874. Until the 19th century, there was no urban development in the city location. The city was founded in 1785 as an official trading town and grew steadily over the following decades, as it transformed into a regional and later national centre of commerce, population, and governmental activities. It is among the cleanest, greenest, and safest cities in the world.

The first permanent settlement in Iceland by Norsemen is believed to have been established at Reykjavík by Ingólfr Arnarson around AD 870; this is described in "Landnámabók", or the Book of Settlement. Ingólfur is said to have decided the location of his settlement using a traditional Norse method: he cast his high seat pillars (Öndvegissúlur) into the ocean when he saw the coastline, then settled where the pillars came to shore. This story is widely regarded as a legend; it appears likely that he settled near the hot springs to keep warm in the winter and would not have decided the location by happenstance. Furthermore, it seems unlikely that the pillars drifted to that location from where they were said to have been thrown from the boat. Nevertheless, that is what the "Landnamabok" says, and it says furthermore that Ingólfur's pillars are still to be found in a house in the town.

Steam from hot springs in the region is said to have inspired Reykjavík's name, which loosely translates to Smoke Cove (the city is sometimes referred to as "Bay of Smoke" or "Smoky Bay" in English language travel guides). In the modern language, as in English, the word for 'smoke' and the word for fog or steamy vapour are not commonly confused, but this is believed to have been the case in the old language.
The original name was Reykjarvík (with an additional "r" representing the usual genitive ending of strong nouns) but this had vanished around 1800.

The Reykjavík area was farmland until the 18th century. In 1752, King Frederik V of Denmark donated the estate of Reykjavík to the Innréttingar Corporation; the name comes from the Danish-language word "indretninger", meaning institution. The leader of this movement was . In the 1750s, several houses were built to house the wool industry, which was Reykjavík's most important employer for a few decades and the original reason for its existence. Other industries were undertaken by the Innréttingar, such as fisheries, sulphur mining, agriculture, and shipbuilding.

The Danish Crown abolished monopoly trading in 1786 and granted six communities around the country an exclusive trading charter. Reykjavík was one of them and the only one to hold on to the charter permanently. 1786 is thus regarded as the date of the city's founding. Trading rights were limited to subjects of the Danish Crown, and Danish traders continued to dominate trade in Iceland. Over the following decades, their business in Iceland expanded. After 1880, free trade was expanded to all nationalities, and the influence of Icelandic merchants started to grow.

Icelandic nationalist sentiment gained influence in the 19th century, and the idea of Icelandic independence became widespread. Reykjavík, as Iceland's only city, was central to such ideas. Advocates of an independent Iceland realized that a strong Reykjavík was fundamental to that objective. All the important events in the history of the independence struggle were important to Reykjavík as well. In 1845 Alþingi, the general assembly formed in 930 AD, was re-established in Reykjavík; it had been suspended a few decades earlier when it was located at Þingvellir. At the time it functioned only as an advisory assembly, advising the king about Icelandic affairs. The location of Alþingi in Reykjavík effectively established the city as the capital of Iceland.

In 1874, Iceland was given a constitution; with it, Alþingi gained some limited legislative powers and in essence became the institution that it is today. The next step was to move most of the executive power to Iceland: Home Rule was granted in 1904 when the office of Minister For Iceland was established in Reykjavík. The biggest step towards an independent Iceland was taken on 1 December 1918 when Iceland became a sovereign country under the Crown of Denmark, the Kingdom of Iceland.

By the 1920s and 1930s most of the growing Icelandic fishing trawler fleet sailed from Reykjavík; cod production was its main industry, but the Great Depression hit Reykjavík hard with unemployment, and labour union struggles sometimes became violent.

On the morning of 10 May 1940, following the German occupation of Denmark and Norway on 9 April 1940, four British warships approached Reykjavík and anchored in the harbour. In a few hours, the allied occupation of Reykjavík was complete. There was no armed resistance, and taxi and truck drivers even assisted the invasion force, which initially had no motor vehicles. The Icelandic government had received many requests from the British government to consent to the occupation, but it always declined on the basis of the Neutrality Policy. For the remaining years of World War II, British and later American soldiers occupied camps in Reykjavík, and the number of foreign soldiers in Reykjavík became about the same as the local population of the city. The Royal Regiment of Canada formed part of the garrison in Iceland during the early part of the war.

The economic effects of the occupation were positive for Reykjavík: the unemployment of the Depression years vanished, and construction work began. The British built Reykjavík Airport, which is still in service today, mostly for short haul flights (to domestic destinations and Greenland). The Americans, meanwhile, built Keflavík Airport, situated west of Reykjavík, which became Iceland's primary international airport. In 1944, the Republic of Iceland was founded and a president, elected by the people, replaced the king; the office of the president was placed in Reykjavík.

In the post-war years, the growth of Reykjavík accelerated. An exodus from the rural countryside began, largely because improved technology in agriculture reduced the need for manpower, and because of a population boom resulting from better living conditions in the country. A once-primitive village was rapidly transformed into a modern city. Private cars became common, and modern apartment complexes rose in the expanding suburbs.

In 1972, Reykjavík hosted the world chess championship between Bobby Fischer and Boris Spassky. The 1986 Reykjavík Summit between Ronald Reagan and Mikhail Gorbachev underlined Reykjavík's international status. Deregulation in the financial sector and the computer revolution of the 1990s again transformed Reykjavík. The financial and IT sectors are now significant employers in the city. The city has fostered some world-famous talents in recent decades, such as Björk, Ólafur Arnalds and bands Múm, Sigur Rós and Of Monsters and Men, poet Sjón and visual artist Ragnar Kjartansson.

Reykjavík is located in the southwest of Iceland. The Reykjavík area coastline is characterized by peninsulas, coves, straits, and islands.

During the Ice Age (up to 10,000 years ago) a large glacier covered parts of the city area, reaching as far out as Álftanes. Other parts of the city area were covered by sea water. In the warm periods and at the end of the Ice Age, some hills like Öskjuhlíð were islands. The former sea level is indicated by sediments (with clams) reaching (at Öskjuhlíð, for example) as far as above the current sea level. The hills of Öskjuhlíð and Skólavörðuholt appear to be the remains of former shield volcanoes which were active during the warm periods of the Ice Age. After the Ice Age, the land rose as the heavy load of the glaciers fell away, and began to look as it does today.

The capital city area continued to be shaped by earthquakes and volcanic eruptions, like the one 4,500 years ago in the mountain range Bláfjöll, when the lava coming down the Elliðaá valley reached the sea at the bay of Elliðavogur.

The largest river to run through Reykjavík is the Elliðaá River, which is non-navigable. It is one of the best salmon fishing rivers in the country. Mount Esja, at , is the highest mountain in the vicinity of Reykjavík.

The city of Reykjavík is mostly located on the Seltjarnarnes peninsula, but the suburbs reach far out to the south and east. Reykjavík is a spread-out city: most of its urban area consists of low-density suburbs, and houses are usually widely spaced. The outer residential neighbourhoods are also widely spaced from each other; in between them are the main traffic arteries and a lot of empty space. The city's latitude is 64°08' N, making it the world's northernmost capital of a sovereign state (Nuuk, the capital of Greenland, is slightly further north at 64°10', but Greenland is a constituent country, not an independent state).

Reykjavík has a subpolar oceanic climate (Köppen: "Cfc") closely bordering on a continental subarctic climate (Köppen: "Dfc") in the 0 °C isoterm. While not much different from a tundra climate, the city has had its present climate classification since the beginning of the twentieth century.

At 64° north, Reykjavik is characterized by extremes of day and night length over the course of the year. From May 20 to July 24, daylight is essentially permanent as the sun never gets more than 5° below the horizon. Day length drops to less than five hours between December 2 and January 10. The sun climbs just 3° above the horizon during this time. However, day length begins increasing rapidly during January and by month's end there are seven hours of daylight.

Despite its northern latitude, temperatures very rarely drop below in the winter. The proximity to the Arctic Circle and the strong moderation of the Atlantic Ocean in the Icelandic coast (influence of North Atlantic Current, an extension of the Gulf Stream) shape a relatively mild winter and cool summer. The city's coastal location does make it prone to wind, however, and gales are common in winter. Summers are cool, with temperatures fluctuating between , rarely exceeding . Rain in Reykjavík averages 147 days at the threshold of 1 mm per year. Droughts are uncommon, although they occur in some summers. July and August are the warmest months of the year on average and January and February the coldest. 

In the summer of 2007, no rain was measured for one month. Summer tends to be the sunniest season, although May receives the most sunshine of any individual month. Overall, the city receives around 1,300 annual hours of sunshine, which is comparable with other places in northern and north-western Europe such as Ireland and Scotland, but substantially less than equally northern regions with a more continental climate, including Finland. Nonetheless, Reykjavík is one of the cloudiest and coolest capitals of any nation in the world. The highest-ever recorded temperature in Reykjavík was , recorded on 30 July 2008, while the lowest-ever recorded temperature was , recorded on 30 January 1971.

The Reykjavík City Council governs the city of Reykjavík and is directly elected by those aged over 18 domiciled in the city. The council has 23 members who are elected using the open list method for four-year terms.

The council selects members of boards, and each board controls a different field under the city council's authority. The most important board is the City Board that wields the executive rights along with the City Mayor. The City Mayor is the senior public official and also the director of city operations. Other public officials control city institutions under the mayor's authority. Thus, the administration consists of two different parts:

The Independence Party was historically the city's ruling party; it had an overall majority from its establishment in 1929 until 1978, when it narrowly lost. From 1978 until 1982, there was a three-party coalition composed of the People's Alliance, the Social Democratic Party, and the Progressive Party. In 1982, the Independence Party regained an overall majority, which it held for three consecutive terms. The 1994 election was won by Reykjavíkurlistinn (the R-list), an alliance of Icelandic socialist parties, led by Ingibjörg Sólrún Gísladóttir. This alliance won a majority in three consecutive elections, but was dissolved for the 2006 election when five different parties were on the ballot. The Independence Party won seven seats, and together with the one Progressive Party they were able to form a new majority in the council which took over in June 2006.

In October 2007 a new majority was formed on the council, consisting of members of the Progressive Party, the Social Democratic Alliance, the Left-Greens and the F-list (liberals and independents), after controversy regarding REI, a subsidiary of OR, the city's energy company. However, three months later the F-list formed a new majority together with the Independence Party. Ólafur F. Magnússon, the leader of the F-list, was elected mayor on 24 January 2008, and in March 2009 the Independence Party was due to appoint a new mayor. This changed once again on 14 August 2008 when the fourth coalition of the term was formed, by the Independence Party and the Social Democratic Alliance, with Hanna Birna Kristjánsdóttir becoming mayor.

The City Council election in May 2010 saw a new political party, The Best Party, win six of 15 seats, and they formed a coalition with the Social Democratic Alliance; comedian Jón Gnarr became mayor. At the 2014 election, the Social Democratic Alliance had its best showing yet, gaining five seats in the council, while Bright Future (successor to the Best Party) received two seats and the two parties formed a coalition with the Left-Green movement and the Pirate Party, which won one seat each. The Independence Party had its worst election ever, with only four seats.

The mayor is appointed by the city council; usually one of the council members is chosen, but they may also appoint a mayor who is not a member of the council.

The post was created in 1907 and advertised in 1908. Two applications were received, from Páll Einarsson, sheriff and town mayor of Hafnarfjörður and from Knud Zimsen, town councillor in Reykjavík. Páll was appointed on 7 May and was mayor for six years. At that time the city mayor received a salary of 4,500 ISK per year and 1,500 ISK for office expenses. The current mayor is Dagur B. Eggertsson.

Reykjavík is by far the largest and most populous settlement in Iceland. The municipality of Reykjavík had a population of 128,793 on 1 January 2019; that is 36% of the country's population. The Capital Region, which includes the capital and six municipalities around it, was home to 228,231 people; that is over 63% of the country's population.

On 1 January 2018, of the city's population of 126,041, immigrants of the first and second generation numbered 20,910 (16.6%), increasing from 12,352 (10.4%) in 2008 and 3,106 (2.9%) in 1998.
The most common foreign citizens are Poles, Lithuanians, and Latvians. About 80% of the city's foreign residents originate in European Union and EFTA member states, and over 58% are from the new member states of the EU, mainly former Eastern Bloc countries, which joined in 2004, 2007 and 2013.

Children of foreign origin form a more considerable minority in the city's schools: as many as a third in places. The city is also visited by thousands of tourists, students, and other temporary residents, at times outnumbering natives in the city centre.

Reykjavík is divided into 10 districts:

In addition there are hinterland areas (lightly shaded on the map) which are not assigned to any district.
Borgartún is the financial centre of Reykjavík, hosting a large number of companies and three investment banks.
Reykjavík has been at the centre of Iceland's economic growth and subsequent economic contraction over the 2000s, a period referred to in foreign media as the "Nordic Tiger" years, or "Iceland's Boom Years". The economic boom led to a sharp increase in construction, with large redevelopment projects such as Harpa concert hall and conference centre and others. Many of these projects came to a screeching halt in the following economic crash of 2008.

Per capita car ownership in Iceland is among the highest in the world at roughly 522 vehicles per 1,000 residents, though Reykjavík is not severely affected by congestion. Several multi-lane highways (mainly dual carriageways) run between the most heavily populated areas and most frequently driven routes. Parking spaces are also plentiful in most areas. Public transportation consists of a bus system called Strætó bs. Route 1 (the Ring Road) runs through the city outskirts and connects the city to the rest of Iceland.

Reykjavík Airport, the second largest airport in the country (after Keflavík International Airport), is positioned inside the city, just south of the city centre. It is mainly used for domestic flights, as well as flights to Greenland and the Faroe Islands. Since 1962, there has been some controversy regarding the location of the airport, since it takes up a lot of valuable space in central Reykjavík.

Reykjavík has two seaports, the old harbour near the city centre which is mainly used by fishermen and cruise ships, and "Sundahöfn" in the east city which is the largest cargo port in the country.
There are no public railways in Iceland, because of its sparse population, but the locomotives used to build the docks are on display. Proposals have been made for a high-speed rail link between the city and Keflavík.

Volcanic activity provides Reykjavík with geothermal heating systems for both residential and industrial districts. In 2008, natural hot water was used to heat roughly 90% of all buildings in Iceland. Of total annual use of geothermal energy of 39 PJ, space heating accounted for 48%.

Most of the district heating in Iceland comes from three main geothermal power plants:


Safnahúsið (the Culture House) was opened in 1909 and has a number of important exhibits. Originally built to house the National Library and National Archives and also previously the location of the National Museum and Natural History Museum, in 2000 it was re-modeled to promote the Icelandic national heritage. Many of Iceland's national treasures are on display, such as the Poetic Edda, and the Sagas in their original manuscripts. There are also changing exhibitions of various topics.

Alcohol is expensive at bars. People tend to drink at home before going out. Beer was banned in Iceland until 1 March 1989 but has since become popular among many Icelanders as their alcoholic drink of choice.

The Iceland Airwaves music festival is staged annually in November. This festival takes place all over the city, and the concert venue Harpa is one of the main locations. Other venues that frequently organise live music events are Kex, Húrra, Gaukurinn (grunge, metal, punk), Mengi (centre for contemporary music, avant-garde music and experimental music), the Icelandic Opera and the National Theatre of Iceland for classical music.

The arrival of the new year is a particular cause for celebration to the people of Reykjavík. Icelandic law states that anyone may purchase and use fireworks during a certain period around New Year's Eve. As a result, every New Year's Eve the city is lit up with fireworks displays.


Reykjavik Golf Club was established in 1934. It is the oldest and largest golf club in Iceland. It consists of two 18-hole courses—one at Grafarholt and the other at Korpa. The Grafarholt golf course opened in 1963, which makes it the oldest 18-hole golf course in Iceland. The Korpa golf course opened in 1997.





Reykjavík is twinned with:
In July 2013, mayor Jón Gnarr filed a motion before the city council to terminate the city's relationship with Moscow, in response to a trend of anti-gay legislation in Russia.



</doc>
<doc id="25799" url="https://en.wikipedia.org/wiki?curid=25799" title="Retrovirus">
Retrovirus

A retrovirus is a type of RNA virus that inserts a copy of its genome into the DNA of a host cell that it invades, thus changing the genome of that cell.

Once inside the host cell's cytoplasm, the virus uses its own reverse transcriptase enzyme to produce DNA from its RNA genome, the reverse of the usual pattern, thus "retro" (backwards). The new DNA is then incorporated into the host cell genome by an integrase enzyme, at which point the retroviral DNA is referred to as a provirus. The host cell then treats the viral DNA as part of its own genome, transcribing and translating the viral genes along with the cell's own genes, producing the proteins required to assemble new copies of the virus. It is difficult to detect the virus until it has infected the host. At that point, the infection will persist indefinitely.

In most viruses, DNA is transcribed into RNA, and then RNA is translated into protein. However, retroviruses function differently, as their RNA is reverse-transcribed into DNA, which is integrated into the host cell's genome (when it becomes a provirus), and then undergoes the usual transcription and translational processes to express the genes carried by the virus. The information contained in a retroviral gene is thus used to generate the corresponding protein via the sequence: RNA → DNA → RNA → polypeptide. This extends the fundamental process identified by Francis Crick (one gene-one peptide) in which the sequence is DNA → RNA → peptide (proteins are made of one or more polypeptide chains; for example, haemoglobin is a four-chain peptide).

Retroviruses are valuable research tools in molecular biology, and they have been used successfully in gene delivery systems.

Virions of retroviruses consist of enveloped particles about 100 nm in diameter. The virions also contain two identical single-stranded RNA molecules 7–10 kilobases in length. Although virions of different retroviruses do not have the same morphology or biology, all the virion components are very similar.

The main virion components are:


Retroviruses (and orterviruses in general) follow a layout of 5'-"gag"-"pro"-"pol"-"env"-3' in the RNA genome. "gag" and "pol" encode polyproteins, each managing the capsid and replication. Depending on the virus, the genes may overlap or fuse into larger polyprotein chains. Some viruses contain additional genes.

The polyproteins are cleaved into smaller proteins each with their own function. The nucleotides encoding them are known as "subgenes".

When retroviruses have integrated their own genome into the germ line, their genome is passed on to a following generation. These endogenous retroviruses (ERVs), contrasted with exogenous ones, now make up 5-8% of the human genome. Most insertions have no known function and are often referred to as "junk DNA". However, many endogenous retroviruses play important roles in host biology, such as control of gene transcription, cell fusion during placental development in the course of the germination of an embryo, and resistance to exogenous retroviral infection. Endogenous retroviruses have also received special attention in the research of immunology-related pathologies, such as autoimmune diseases like multiple sclerosis, although endogenous retroviruses have not yet been proven to play any causal role in this class of disease.

While transcription was classically thought to occur only from DNA to RNA, reverse transcriptase transcribes RNA into DNA. The term "retro" in retrovirus refers to this reversal (making DNA from RNA) of the usual direction of transcription. It still obeys the central dogma of molecular biology, which states that information can be transferred from nucleic acid to nucleic acid but cannot be transferred back from protein to either protein or nucleic acid. Reverse transcriptase activity outside of retroviruses has been found in almost all eukaryotes, enabling the generation and insertion of new copies of retrotransposons into the host genome. These inserts are transcribed by enzymes of the host into new RNA molecules that enter the cytosol. Next, some of these RNA molecules are translated into viral proteins. For example, the "gag" gene is translated into molecules of the capsid protein, the "pol" gene is translated into molecules of reverse transcriptase, and the "env" gene is translated into molecules of the envelope protein. It is important to note that a retrovirus must "bring" its own reverse transcriptase in its capsid, otherwise it is unable to utilize the enzymes of the infected cell to carry out the task, due to the unusual nature of producing DNA from RNA.

Industrial drugs that are designed as protease and reverse-transcriptase inhibitors are made such that they target specific sites and sequences within their respective enzymes. However these drugs can quickly become ineffective due to the fact that the gene sequences that code for the protease and the reverse transcriptase quickly mutate. These changes in bases cause specific codons and sites with the enzymes to change and thereby avoid drug targeting by losing the sites that the drug actually targets.

Because reverse transcription lacks the usual proofreading of DNA replication, a retrovirus mutates very often. This enables the virus to grow resistant to antiviral pharmaceuticals quickly, and impedes the development of effective vaccines and inhibitors for the retrovirus.

One difficulty faced with some retroviruses, such as the Moloney retrovirus, involves the requirement for cells to be actively dividing for transduction. As a result, cells such as neurons are very resistant to infection and transduction by retroviruses. This gives rise to a concern that insertional mutagenesis due to integration into the host genome might lead to cancer or leukemia. This is unlike "Lentivirus", a genus of "Retroviridae", which are able to integrate their RNA into the genome of non-dividing host cells.


This DNA can be incorporated into host genome as a provirus that can be passed on to progeny cells. The retrovirus DNA is inserted at random into the host genome. Because of this, it can be inserted into oncogenes. In this way some retroviruses can convert normal cells into cancer cells. Some provirus remains latent in the cell for a long period of time before it is activated by the change in cell environment.

Studies of retroviruses led to the first demonstrated synthesis of DNA from RNA templates, a fundamental mode for transferring genetic material that occurs in both eukaryotes and prokaryotes. It has been speculated that the RNA to DNA transcription processes used by retroviruses may have first caused DNA to be used as genetic material. In this model, the RNA world hypothesis, cellular organisms adopted the more chemically stable DNA when retroviruses evolved to create DNA from the RNA templates.

An estimate of the date of evolution of the foamy-like endogenous retroviruses placed the time of the most recent common ancestor at > .

Gammaretroviral and lentiviral vectors for gene therapy have been developed that mediate stable genetic modification of treated cells by chromosomal integration of the transferred vector genomes. This technology is of use, not only for research purposes, but also for clinical gene therapy aiming at the long-term correction of genetic defects, e.g., in stem and progenitor cells. Retroviral vector particles with tropism for various target cells have been designed. Gammaretroviral and lentiviral vectors have so far been used in more than 300 clinical trials, addressing treatment options for various diseases. Retroviral mutations can be developed to make transgenic mouse models to study various cancers and their metastatic models.

Retroviruses that cause tumor growth include "Rous sarcoma virus" and "Mouse mammary tumor virus". Cancer can be triggered by proto-oncogenes that were mistakenly incorporated into proviral DNA or by the disruption of cellular proto-oncogenes. Rous sarcoma virus contains the src gene that triggers tumor formation. Later it was found that a similar gene in cells is involved in cell signaling, which was most likely excised with the proviral DNA. Nontransforming viruses can randomly insert their DNA into proto-oncogenes, disrupting the expression of proteins that regulate the cell cycle. The promoter of the provirus DNA can also cause over expression of regulatory genes.

These are infectious RNA- or DNA-containing viruses which are transmitted from individual to individual.

Reverse-transcribing viruses fall into 2 groups of the Baltimore classification.

All members of Group VI use virally encoded reverse transcriptase, an RNA-dependent DNA polymerase, to produce DNA from the initial virion RNA genome. This DNA is often integrated into the host genome, as in the case of retroviruses and pseudoviruses, where it is replicated and transcribed by the host.

Group VI includes:

The family "Retroviridae" was previously divided into three subfamilies ("Oncovirinae", "Lentivirinae", and "Spumavirinae"), but are now divided into two: "Orthoretrovirinae" and "Spumaretrovirinae". The term oncovirus is now commonly used to describe a cancer-causing virus. This family now includes the following genera:
Note that according to ICTV 2017, genus "Spumavirus" has been divided into five genera, and its former type species "Simian foamy virus" is now upgraded to genus "Simiispumavirus" with not less than 14 species, including new type species "Eastern chimpanzee simian foamy virus".

Both families in Group VII have DNA genomes contained within the invading virus particles. The DNA genome is transcribed into both mRNA, for use as a transcript in protein synthesis, and pre-genomic RNA, for use as the template during genome replication. Virally encoded reverse transcriptase uses the pre-genomic RNA as a template for the creation of genomic DNA.

Group VII includes:
The latter family is closely related to the newly proposed
whilst families "Belpaoviridae", "Metaviridae", "Pseudoviridae", "Retroviridae", and "Caulimoviridae" constitute the order "Ortervirales".

Endogenous retroviruses are not formally included in this classification system, and are broadly classified into three classes, on the basis of relatedness to exogenous genera:

Antiretroviral drugs are medications for the treatment of infection by retroviruses, primarily HIV. Different classes of antiretroviral drugs act on different stages of the HIV life cycle. Combination of several (typically three or four) antiretroviral drugs is known as highly active anti-retroviral therapy (HAART).

"Feline leukemia virus" and "Feline immunodeficiency virus" infections are treated with biologics, including the only immunomodulator currently licensed for sale in the United States, Lymphocyte T-Cell Immune Modulator (LTCI).



</doc>
<doc id="25801" url="https://en.wikipedia.org/wiki?curid=25801" title="Round (music)">
Round (music)

A round (also called a perpetual canon ["canon perpetuus"] or infinite canon) is a musical composition, a limited type of canon, in which a minimum of three voices sing exactly the same melody at the unison (and may continue repeating it indefinitely), but with each voice beginning at different times so that different parts of the melody coincide in the different voices, but nevertheless fit harmoniously together . It is one of the easiest forms of part singing, as only one line of melody need be learned by all parts, and is part of a popular musical tradition. They were particularly favoured in glee clubs, which combined amateur singing with regular drinking (, especially at 21: "Catch-singing is unthinkable without a supply of liquor to hand..."). The earliest known rounds date from 12th century Europe.

"Row, Row, Row Your Boat" is a well-known children's round for four voices. Other well-known examples are "Frère Jacques", "Three Blind Mice", and, more recently, "God Only Knows" by The Beach Boys (the first usage in contemporary pop music) .

A catch is a round in which a phrase that is not apparent in a single line of lyrics emerges when the lyrics are split between the different voices. "Perpetual canon" refers to the end of the melody leading back to the beginning, allowing easy and immediate repetition. Often, "the final cadence is the same as the first measure" .

The term "round" first appears in English in the early 16th century, though the form was found much earlier. In medieval England, they were called rota or rondellus. Later, an alternative term was "roundel" (e.g., David Melvill's manuscript "Ane Buik off Roundells", Aberdeen, 1612). Special types of rounds are the "catch" (a comic English form found from about 1580 to 1800), and a specialized use of the word "canon", in 17th- and 18th-century England designating rounds with religious texts . The oldest surviving round in English is "Sumer Is Icumen In" , which is for four voices, plus two bass voices singing a ground (that is, a never-changing repeating part), also in canon. However, the earliest known rounds are two works with Latin texts found in the eleventh fascicle of the Notre Dame manuscript Pluteo 29.1. They are "Leto leta concio" (a two-voice round) and "O quanto consilio" (a four-voice round). The former dates from before 1180 and may be of German origin . The first published rounds in English were printed by Thomas Ravenscroft in 1609... "Three Blind Mice" appears in this collection, although in a somewhat different form from today's children's round:

What makes a round work is that after the work is divided into equal-sized blocks of a few measures each, corresponding notes in each block either are the same, or are different notes in the same chord. This is easiest with one chord, as in "Row, Row, Row Your Boat":
A new part can join the singing by starting at the beginning whenever another part reaches any asterisk in the above music. If one ignores the sixteenth notes that pass between the main chords, every single note is in the tonic triad—in this case, a C, E, or G.

Many rounds involve more than one chord, as in "Frère Jacques" :

The texture is simpler, but it uses a few more notes; this can perhaps be more easily seen if all four parts are run together into the same two measures:

The second beat of each measure does not sketch out a tonic triad, it outlines a dominant seventh chord (or "V7 chord").

Serious composers who turned their hand to the round format include Thomas Arne, John Blow, William Byrd, Henry Purcell, Louis Hardin, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven, and Benjamin Britten (for example, "Old Joe Has Gone Fishing", sung by the villagers in the pub to keep the peace, at the end of act 1 of "Peter Grimes") . Examples by J. S. Bach include the regular canons, Var. 3 and Var. 24 of the Goldberg Variations, and the perpetual canons, Canon 7 of The Musical Offering and Canon a 2 Perpetuus (BWV 1075) . Several rounds are included amongst Arnold Schoenberg's thirty-plus canons, which "within their natural limitations ... are brilliant pieces, containing too much of the composer's characteristically unexpected blend of seriousness, humour, vigour and tenderness to remain unperformed" .




</doc>
<doc id="25806" url="https://en.wikipedia.org/wiki?curid=25806" title="Reincarnation">
Reincarnation

Reincarnation is the philosophical or religious concept that the non-physical essence of a living being starts a new life in a different physical form or body after biological death. It is also called rebirth or transmigration.

Reincarnation is a central tenet of Indian religions, namely Jainism, Buddhism, Sikhism and Hinduism, although there are Hindu groups that do not believe in reincarnation but believe in an afterlife. It is an esoteric belief in many streams of Orthodox Judaism and is found (in different forms) in some beliefs of North American Natives and some Native Australians (while most believe in an afterlife or spirit world). A belief in rebirth/metempsychosis was held by Greek historic figures, such as Pythagoras, Socrates, and Plato. It is also a belief in various modern religions. Although the majority of denominations within Christianity and Islam do not believe that individuals reincarnate, particular groups within these religions do refer to reincarnation; these groups include the mainstream historical and contemporary followers of Cathars, Alawites, the Druze, and the Rosicrucians. The historical relations between these sects and the beliefs about reincarnation that were characteristic of Neoplatonism, Orphism, Hermeticism, Manicheanism, and Gnosticism of the Roman era as well as the Indian religions have been the subject of recent scholarly research. In recent decades, many Europeans and North Americans have developed an interest in reincarnation, and many contemporary works mention it.

The word "reincarnation" derives from Latin, literally meaning, "entering the flesh again". The Greek equivalent "metempsychosis" (μετεμψύχωσις) derives from "meta" (change) and "empsykhoun" (to put a soul into), a term attributed to Pythagoras. An alternate term is transmigration implying migration from one life (body) to another. Reincarnation refers to the belief that an aspect of every human being (or all living beings in some cultures) continues to exist after death, this aspect may be the soul or mind or consciousness or something transcendent which is reborn in an interconnected cycle of existence; the transmigration belief varies by culture, and is envisioned to be in the form of a newly born human being, or animal, or plant, or spirit, or as a being in some other non-human realm of existence. The term has been used by modern philosophers such as Kurt Gödel and has entered the English language. Another Greek term sometimes used synonymously is "palingenesis", "being born again".

Rebirth is a key concept found in major Indian religions, and discussed with various terms. "Punarjanman" (Sanskrit: पुनर्जन्मन्) means "rebirth, transmigration". Reincarnation is discussed in the ancient Sanskrit texts of Hinduism, Buddhism, and Jainism, with many alternate terms such as "punarāvṛtti" (पुनरावृत्ति), "punarājāti" (पुनराजाति), "punarjīvātu" (पुनर्जीवातु), "punarbhava" (पुनर्भव), "āgati-gati" (आगति-गति, common in Buddhist Pali text), "nibbattin" (निब्बत्तिन्), "upapatti" (उपपत्ति), and "uppajjana" (उप्पज्जन). These religions believe that this reincarnation is cyclic and an endless Saṃsāra, unless one gains spiritual insights that ends this cycle leading to liberation. The reincarnation concept is considered in Indian religions as a step that starts each "cycle of aimless drifting, wandering or mundane existence", but one that is an opportunity to seek spiritual liberation through ethical living and a variety of meditative, yogic ("marga"), or other spiritual practices. They consider the release from the cycle of reincarnations as the ultimate spiritual goal, and call the liberation by terms such as moksha, nirvana, "mukti" and "kaivalya". However, the Buddhist, Hindu and Jain traditions have differed, since ancient times, in their assumptions and in their details on what reincarnates, how reincarnation occurs and what leads to liberation.

"Gilgul", "Gilgul neshamot" or "Gilgulei Ha Neshamot" (Heb. גלגול הנשמות) is the concept of reincarnation in Kabbalistic Judaism, found in much Yiddish literature among Ashkenazi Jews. "Gilgul" means "cycle" and "neshamot" is "souls". Kabbalistic reincarnation says that humans reincarnate only to humans unless YHWH/Ein Sof/God chooses.

The origins of the notion of reincarnation are obscure. Discussion of the subject appears in the philosophical traditions of India. The Greek Pre-Socratics discussed reincarnation, and the Celtic Druids are also reported to have taught a doctrine of reincarnation.

The idea of reincarnation, saṃsāra, did not exist in the early Vedic religions. The idea of reincarnation has roots in the Upanishads of the late Vedic period (c. 1100 – c. 500 BCE), predating the Buddha and the Mahavira. The concepts of the cycle of birth and death, samsara, and liberation partly derive from ascetic traditions that arose in India around the middle of the first millennium BCE. Though no direct evidence of this has been found, the tribes of the Ganges valley or the Dravidian traditions of South India have been proposed as another early source of reincarnation beliefs.

The early Vedas do not mention the doctrine of Karma and rebirth but mention the belief in an afterlife. It is in the early Upanishads, which are pre-Buddha and pre-Mahavira, where these ideas are developed and described in a general way. Detailed descriptions first appear around the mid 1st millennium BCE in diverse traditions, including Buddhism, Jainism and various schools of Hindu philosophy, each of which gave unique expression to the general principle.

The texts of ancient Jainism that have survived into the modern era are post-Mahavira, likely from the last centuries of the 1st millennium BCE, and extensively mention rebirth and karma doctrines. The Jaina philosophy assumes that the soul ("Jiva" in Jainism, "Atman" in Hinduism) exists and is eternal, passing through cycles of transmigration and rebirth. After death, reincarnation into a new body is asserted to be instantaneous in early Jaina texts. Depending upon the accumulated karma, rebirth occurs into a higher or lower bodily form, either in heaven or hell or earthly realm. No bodily form is permanent: everyone dies and reincarnates further. Liberation ("kevalya") from reincarnation is possible, however, through removing and ending karmic accumulations to one's soul. From the early stages of Jainism on, a human being was considered the highest mortal being, with the potential to achieve liberation, particularly through asceticism.

The early Buddhist texts discuss rebirth as part of the doctrine of "Saṃsāra". This asserts that the nature of existence is a "suffering-laden cycle of life, death, and rebirth, without beginning or end". Also referred to as the wheel of existence ("Bhavacakra"), it is often mentioned in Buddhist texts with the term "punarbhava" (rebirth, re-becoming). Liberation from this cycle of existence, "Nirvana", is the foundation and the most important purpose of Buddhism. Buddhist texts also assert that an enlightened person knows his previous births, a knowledge achieved through high levels of meditative concentration. Tibetan Buddhism discusses death, bardo (an intermediate state), and rebirth in texts such as the "Tibetan Book of the Dead". While Nirvana is taught as the ultimate goal in the Theravadin Buddhism, and is essential to Mahayana Buddhism, the vast majority of contemporary lay Buddhists focus on accumulating good karma and acquiring merit to achieve a better reincarnation in the next life.

In early Buddhist traditions, "Saṃsāra" cosmology consisted of five realms through which the wheel of existence cycled. This included hells ("niraya"), hungry ghosts ("pretas"), animals ("tiryak"), humans ("manushya"), and gods ("devas", heavenly). In latter Buddhist traditions, this list grew to a list of six realms of rebirth, adding demi-gods ("asuras").

The earliest layers of Vedic text incorporate the concept of life, followed by an afterlife in heaven and hell based on cumulative virtues (merit) or vices (demerit). However, the ancient Vedic Rishis challenged this idea of afterlife as simplistic, because people do not live an equally moral or immoral life. Between generally virtuous lives, some are more virtuous; while evil too has degrees, and the texts assert that it would be unfair for people, with varying degrees of virtue or vices, to end up in heaven or hell, in "either or" and disproportionate manner irrespective of how virtuous or vicious their lives were. They introduced the idea of an afterlife in heaven or hell in proportion to one's merit.

Early texts of Hinduism, Buddhism and Jainism share the concepts and terminology related to reincarnation. They also emphasize similar virtuous practices and karma as necessary for liberation and what influences future rebirths. For example, all three discuss various virtues – sometimes grouped as Yamas and Niyamas – such as non-violence, truthfulness, non-stealing, non-possessiveness, compassion for all living beings, charity and many others.

Hinduism, Buddhism and Jainism disagree in their assumptions and theories about rebirth. Hinduism relies on its foundational assumption that "soul, Self exists" (Atman, attā), in contrast to Buddhist assumption that there is "no soul, no Self" (Anatta, anatman). Hindu traditions consider soul to be the unchanging eternal essence of a living being, and what journeys across reincarnations until it attains self-knowledge. Buddhism, in contrast, asserts a rebirth theory without a Self, and considers realization of non-Self or Emptiness as Nirvana (nibbana). Thus Buddhism and Hinduism have a very different view on whether a self or soul exists, which impacts the details of their respective rebirth theories.

The reincarnation doctrine in Jainism differs from those in Buddhism, even though both are non-theistic Sramana traditions. Jainism, in contrast to Buddhism, accepts the foundational assumption that soul exists ("Jiva") and asserts this soul is involved in the rebirth mechanism. Further, Jainism considers asceticism as an important means to spiritual liberation that ends all reincarnation, while Buddhism does not.

Early Greek discussion of the concept dates to the 6th century BCE. An early Greek thinker known to have considered rebirth is Pherecydes of Syros (fl. 540 BCE). His younger contemporary Pythagoras (c. 570–c. 495 BCE), its first famous exponent, instituted societies for its diffusion. Some authorities believe that Pythagoras was Pherecydes' pupil, others that Pythagoras took up the idea of reincarnation from the doctrine of Orphism, a Thracian religion, or brought the teaching from India.

Plato (428/427–348/347 BCE) presented accounts of reincarnation in his works, particularly the "Myth of Er". In "Phaedo", Plato has his teacher Socrates, prior to his death, state: "I am confident that there truly is such a thing as living again, and that the living spring from the dead." However Xenophon does not mention Socrates as believing in reincarnation and Plato may have systematised Socrates' thought with concepts he took directly from Pythagoreanism or Orphism.

The Orphic religion, which taught reincarnation, about the 6th century BC, organized itself into mystery schools at Eleusis and elsewhere, and produced a copious literature. Orpheus, its legendary founder, is said to have taught that the immortal soul aspires to freedom while the body holds it prisoner. The wheel of birth revolves, the soul alternates between freedom and captivity round the wide circle of necessity. Orpheus proclaimed the need of the grace of the gods, Dionysus in particular, and of self-purification until the soul has completed the spiral ascent of destiny to live for ever.

An association between Pythagorean philosophy and reincarnation was routinely accepted throughout antiquity. In the "Republic" Plato makes Socrates tell how Er, the son of Armenius, miraculously returned to life on the twelfth day after death and recounted the secrets of the other world. There are myths and theories to the same effect in other dialogues, in the Chariot allegory of the Phaedrus, in the Meno, Timaeus and Laws. The soul, once separated from the body, spends an indeterminate amount of time in "formland" (see The Allegory of the Cave in "The Republic") and then assumes another body.

In later Greek literature the doctrine is mentioned in a fragment of Menander and satirized by Lucian. In Roman literature it is found as early as Ennius, who, in a lost passage of his "Annals", told how he had seen Homer in a dream, who had assured him that the same soul which had animated both the poets had once belonged to a peacock. Persius in his satires (vi. 9) laughs at this, it is referred to also by Lucretius and Horace.

Virgil works the idea into his account of the Underworld in the sixth book of the Aeneid. It persists down to the late classic thinkers, Plotinus and the other Neoplatonists. In the Hermetica, a Graeco-Egyptian series of writings on cosmology and spirituality attributed to Hermes Trismegistus/Thoth, the doctrine of reincarnation is central.

In Greco-Roman thought, the concept of metempsychosis disappeared with the rise of Early Christianity, reincarnation being incompatible with the Christian core doctrine of salvation of the faithful after death. It has been suggested that some of the early Church Fathers, especially Origen, still entertained a belief in the possibility of reincarnation, but evidence is tenuous, and the writings of Origen as they have come down to us speak explicitly against it.

Some early Christian Gnostic sects professed reincarnation. The Sethians and followers of Valentinus believed in it. The followers of Bardaisan of Mesopotamia, a sect of the 2nd century deemed heretical by the Catholic Church, drew upon Chaldean astrology, to which Bardaisan's son Harmonius, educated in Athens, added Greek ideas including a sort of metempsychosis. Another such teacher was Basilides (132–? CE/AD), known to us through the criticisms of Irenaeus and the work of Clement of Alexandria (see also Neoplatonism and Gnosticism and Buddhism and Gnosticism).

In the third Christian century Manichaeism spread both east and west from Babylonia, then within the Sassanid Empire, where its founder Mani lived about 216–276. Manichaean monasteries existed in Rome in 312 AD. Noting Mani's early travels to the Kushan Empire and other Buddhist influences in Manichaeism, Richard Foltz attributes Mani's teaching of reincarnation to Buddhist influence. However the inter-relation of Manicheanism, Orphism, Gnosticism and neo-Platonism is far from clear.

In the 1st century BCE Alexander Cornelius Polyhistor wrote:

Julius Caesar recorded that the druids of Gaul, Britain and Ireland had metempsychosis as one of their core doctrines:

Surviving texts indicate that there was a belief in rebirth in Germanic paganism. Examples include figures from eddic poetry and sagas, potentially by way of a process of naming and/or through the family line. Scholars have discussed the implications of these attestations and proposed theories regarding belief in reincarnation among the Germanic peoples prior to Christianization and potentially to some extent in folk belief thereafter.

The belief in reincarnation had first existed among Jewish mystics in the Ancient World, among whom differing explanations were given of the afterlife, although with a universal belief in an immortal soul. Today, reincarnation is an esoteric belief within many streams of modern Judaism. Kabbalah teaches a belief in "gilgul", transmigration of souls, and hence the belief in reincarnation is universal in Hasidic Judaism, which regards the Kabbalah as sacred and authoritative, and is also held as an esoteric belief within Modern Orthodox Judaism. In Judaism, the Zohar, first published in the 13th century, discusses reincarnation at length, especially in the Torah portion "Balak." The most comprehensive kabbalistic work on reincarnation, "Shaar HaGilgulim", was written by Chaim Vital, based on the teachings of his mentor, the 16th century kabbalist Isaac Luria, who was said to know the past lives of each person through his semi-prophetic abilities. The 18th century Lithuanian master scholar and kabbalist, Elijah of Vilna, known as the Vilna Gaon, authored a commentary on the biblical Book of Jonah as an allegory of reincarnation.

The practice of conversion to Judaism is sometimes understood within Orthodox Judaism in terms of reincarnation. According to this school of thought in Judaism, when non-Jews are drawn to Judaism, it is because they had been Jews in a former life. Such souls may "wander among nations" through multiple lives, until they find their way back to Judaism, including through finding themselves born in a gentile family with a "lost" Jewish ancestor.

There is an extensive literature of Jewish folk and traditional stories that refer to reincarnation.

Taoist documents from as early as the Han Dynasty claimed that Lao Tzu appeared on earth as different persons in different times beginning in the legendary era of Three Sovereigns and Five Emperors. The (ca. 3rd century BC) "Chuang Tzu" states: "Birth is not a beginning; death is not an end. There is existence without limitation; there is continuity without a starting-point. Existence without limitation is Space. Continuity without a starting point is Time. There is birth, there is death, there is issuing forth, there is entering in."

Around the 11–12th century in Europe, several reincarnationist movements were persecuted as heresies, through the establishment of the Inquisition in the Latin west. These included the Cathar, Paterene or Albigensian church of western Europe, the Paulician movement, which arose in Armenia, and the Bogomils in Bulgaria.

Christian sects such as the Bogomils and the Cathars, who professed reincarnation and other gnostic beliefs, were referred to as "Manichean", and are today sometimes described by scholars as "Neo-Manichean". As there is no known Manichaean mythology or terminology in the writings of these groups there has been some dispute among historians as to whether these groups truly were descendants of Manichaeism.

While reincarnation has been a matter of faith in some communities from an early date it has also frequently been argued for on principle, as Plato does when he argues that the number of souls must be finite because souls are indestructible, Benjamin Franklin held a similar view. Sometimes such convictions, as in Socrates' case, arise from a more general personal faith, at other times from anecdotal evidence such as Plato makes Socrates offer in the "Myth of Er".

During the Renaissance translations of Plato, the Hermetica and other works fostered new European interest in reincarnation. Marsilio Ficino argued that Plato's references to reincarnation were intended allegorically, Shakespeare alluded to the doctrine of reincarnation but Giordano Bruno was burned at the stake by authorities after being found guilty of heresy by the Roman Inquisition for his teachings. But the Greek philosophical works remained available and, particularly in north Europe, were discussed by groups such as the Cambridge Platonists.

By the 19th century the philosophers Schopenhauer and Nietzsche could access the Indian scriptures for discussion of the doctrine of reincarnation, which recommended itself to the American Transcendentalists Henry David Thoreau, Walt Whitman and Ralph Waldo Emerson and was adapted by Francis Bowen into "Christian Metempsychosis".

By the early 20th century, interest in reincarnation had been introduced into the nascent discipline of psychology, largely due to the influence of William James, who raised aspects of the philosophy of mind, comparative religion, the psychology of religious experience and the nature of empiricism. James was influential in the founding of the American Society for Psychical Research (ASPR) in New York City in 1885, three years after the British Society for Psychical Research (SPR) was inaugurated in London, leading to systematic, critical investigation of paranormal phenomena. Famous World War II American General George Patton was a strong believer in reincarnation, believing, among other things, he was a reincarnation of the Carthaginian General Hannibal.

At this time popular awareness of the idea of reincarnation was boosted by the Theosophical Society's dissemination of systematised and universalised Indian concepts and also by the influence of magical societies like The Golden Dawn. Notable personalities like Annie Besant, W. B. Yeats and Dion Fortune made the subject almost as familiar an element of the popular culture of the west as of the east. By 1924 the subject could be satirised in popular children's books. Humorist Don Marquis created a fictional cat named Mehitabel who claimed to be a reincarnation of Queen Cleopatra.

Théodore Flournoy was among the first to study a claim of past-life recall in the course of his investigation of the medium Hélène Smith, published in 1900, in which he defined the possibility of cryptomnesia in such accounts.
Carl Gustav Jung, like Flournoy based in Switzerland, also emulated him in his thesis based on a study of cryptomnesia in psychism. Later Jung would emphasise the importance of the persistence of memory and ego in psychological study of reincarnation: "This concept of rebirth necessarily implies the continuity of personality... (that) one is able, at least potentially, to remember that one has lived through previous existences, and that these existences were one's own..." Hypnosis, used in psychoanalysis for retrieving forgotten memories, was eventually tried as a means of studying the phenomenon of past life recall.

According to various Buddhist scriptures, Gautama Buddha believed in the existence of an afterlife in another world and in reincarnation,

The Buddha also asserted that karma influences rebirth, and that the cycles of repeated births and deaths are endless. Before the birth of Buddha, ancient Indian scholars had developed competing theories of afterlife, including the materialistic school such as Charvaka, which posited that death is the end, there is no afterlife, no soul, no rebirth, no karma, and they described death to be a state where a living being is completely annihilated, dissolved. Buddha rejected this theory, adopted the alternate existing theories on rebirth, criticizing the materialistic schools that denied rebirth and karma, states Damien Keown. Such beliefs are inappropriate and dangerous, stated Buddha, because such annihilationism views encourage moral irresponsibility and material hedonism; he tied moral responsibility to rebirth.

The Buddha introduced the concept that there is no permanent self (soul), and this central concept in Buddhism is called "anattā". Major contemporary Buddhist traditions such as Theravada, Mahayana and Vajrayana traditions accept the teachings of Buddha. These teachings assert there is rebirth, there is no permanent self and no irreducible ātman (soul) moving from life to another and tying these lives together, there is impermanence, that all compounded things such as living beings are aggregates dissolve at death, but every being reincarnates. The rebirth cycles continue endlessly, states Buddhism, and it is a source of "Dukkha" (suffering, pain), but this reincarnation and "Dukkha" cycle can be stopped through nirvana. The "anattā" doctrine of Buddhism is a contrast to Hinduism, the latter asserting that "soul exists, it is involved in rebirth, and it is through this soul that everything is connected".

Different traditions within Buddhism have offered different theories on what reincarnates and how reincarnation happens. One theory suggests that it occurs through consciousness (Pali: "samvattanika-viññana") or stream of consciousness (Pali: "viññana-sotam", Sanskrit: "vijñāna-srotām, vijñāna-santāna", or "citta-santāna") upon death, which reincarnates into a new aggregation. This process, states this theory, is similar to the flame of a dying candle lighting up another. The consciousness in the newly born being is neither identical to nor entirely different from that in the deceased but the two form a causal continuum or stream in this Buddhist theory. Transmigration is influenced by a being's past "karma" ("kamma"). The root cause of rebirth, states Buddhism, is the abiding of consciousness in ignorance (Pali: "avijja", Sanskrit: "avidya") about the nature of reality, and when this ignorance is uprooted, rebirth ceases.
Buddhist traditions also vary in their mechanistic details on rebirth. Theravada Buddhists assert that rebirth is immediate while the Tibetan schools hold to the notion of a "bardo" (intermediate state) that can last up to 49 days. The "bardo" rebirth concept of Tibetan Buddhism, along with "yidam", developed independently in Tibet without Indian influence, and involves 42 peaceful deities, and 58 wrathful deities. These ideas led to mechanistic maps on karma and what form of rebirth one takes after death, discussed in texts such as "The Tibetan Book of the Dead". The major Buddhist traditions accept that the reincarnation of a being depends on the past karma and merit (demerit) accumulated, and that there are six realms of existence in which the rebirth may occur after each death.

Within Japanese Zen, reincarnation is accepted by some, but rejected by others. A distinction can be drawn between "folk Zen", as in the Zen practiced by devotional lay people, and "philosophical Zen". Folk Zen generally accepts the various supernatural elements of Buddhism such as rebirth. Philosophical Zen, however, places more emphasis on the present moment.

Some schools conclude that karma continues to exist and adhere to the person until it works out its consequences. For the Sautrantika school, each act "perfumes" the individual or "plants a seed" that later germinates. Tibetan Buddhism stresses the state of mind at the time of death. To die with a peaceful mind will stimulate a virtuous seed and a fortunate rebirth; a disturbed mind will stimulate a non-virtuous seed and an unfortunate rebirth.

In the major Christian denominations, the concept of reincarnation is absent and it is nowhere explicitly referred to in the Bible. However, the impossibility of a second earthly death is stated by , where it affirms that Jesus Christ God died once forever (in Latin: "semel", a single time) for the sins of all the human kind. In , king Herod Antipas identified Jesus Christ God with a risen John the Baptist, before ordering his necking execution. 

In a survey by the Pew Forum in 2009, 24% of American Christians expressed a belief in reincarnation and in a 1981 survey 31% of regular churchgoing European Catholics expressed a belief in reincarnation.

Some Christian theologians interpret certain Biblical passages as referring to reincarnation. These passages include the questioning of Jesus as to whether he is Elijah, John the Baptist, Jeremiah, or another prophet (Matthew 16:13–15 and John 1:21–22) and, less clearly (while Elijah was said not to have died, but to have been taken up to heaven), John the Baptist being asked if he is not Elijah (John 1:25). Geddes MacGregor, an Episcopalian priest and professor of philosophy, has made a case for the compatibility of Christian doctrine and reincarnation.

There is evidence that Origen, a Church father in early Christian times, taught reincarnation in his lifetime but that when his works were translated into Latin these references were concealed. One of the epistles written by St. Jerome, "To Avitus" (Letter 124; Ad Avitum. Epistula CXXIV), which asserts that Origen's "On First Principles" (Latin: "De Principiis"; Greek: Περὶ Ἀρχῶν) was mistranscribed:

Under the impression that Origen was a heretic like Arius, St. Jerome criticizes ideas described in "On First Principles". Further in "To Avitus" (Letter 124), St. Jerome writes about "convincing proof" that Origen teaches reincarnation in the original version of the book:

The original text of "On First Principles" has almost completely disappeared. It remains extant as "De Principiis" in fragments faithfully translated into Latin by St. Jerome and in "the not very reliable Latin translation of Rufinus."

Belief in reincarnation was rejected by Augustine of Hippo in The City of God.

Reincarnation is a paramount tenet in the Druze faith. There is an eternal duality of the body and the soul and it is impossible for the soul to exist without the body. Therefore, reincarnations occur instantly at one's death. While in the Hindu and Buddhist belief system a soul can be transmitted to any living creature, in the Druze belief system this is not possible and a human soul will only transfer to a human body. Furthermore, souls cannot be divided into different or separate parts and the number of souls existing is finite.

Few Druzes are able to recall their past but, if they are able to they are called a "Nateq". Typically souls who have died violent deaths in their previous incarnation will be able to recall memories. Since death is seen as a quick transient state, mourning is discouraged. Unlike other Abrahamic faiths, heaven and hell are spiritual. Heaven is the ultimate happiness received when soul escapes the cycle of rebirths and reunites with the Creator, while hell is conceptualized as the bitterness of being unable to reunite with the Creator and escape from the cycle of rebirth.

The body dies, assert the Hindu traditions, but not the soul, which they assume to be the eternal reality, indestructible and bliss. Everything and all existence is believed to be connected and cyclical in many Hinduism-sects, all living beings composed of two things, the soul and the body or matter. Atman does not change and cannot change by its innate nature in the Hindu belief. Current Karma impacts the future circumstances in this life, as well as the future forms and realms of lives. Good intent and actions lead to good future, bad intent and actions lead to bad future, impacting how one reincarnates, in the Hindu view of existence.
There is no permanent heaven or hell in most Hinduism-sects. In the afterlife, based on one's karma, the soul is reborn as another being in heaven, hell, or a living being on earth (human, animal). Gods too die once their past karmic merit runs out, as do those in hell, and they return getting another chance on earth. This reincarnation continues, endlessly in cycles, until one embarks on a spiritual pursuit, realizes self-knowledge, and thereby gains "mokṣa", the final release out of the reincarnation cycles. This release is believed to be a state of utter bliss, which Hindu traditions believe is either related or identical to Brahman, the unchanging reality that existed before the creation of universe, continues to exist, and shall exist after the universe ends.

The Upanishads, part of the scriptures of the Hindu traditions, primarily focus on the liberation from reincarnation. The Bhagavad Gita discusses various paths to liberation. The Upanishads, states Harold Coward, offer a "very optimistic view regarding the perfectibility of human nature", and the goal of human effort in these texts is a continuous journey to self-perfection and self-knowledge so as to end "Saṃsāra" – the endless cycle of rebirth and redeath. The aim of spiritual quest in the Upanishadic traditions is find the true self within and to know one's soul, a state that they assert leads to blissful state of freedom, moksha.

The Bhagavad Gita states:
There are internal differences within Hindu traditions on reincarnation and the state of moksha. For example, the dualistic devotional traditions such as Madhvacharya's Dvaita Vedanta tradition of Hinduism champion a theistic premise, assert that human soul and Brahman are different, loving devotion to Brahman (god Vishnu in Madhvacharya's theology) is the means to release from Samsara, it is the grace of God which leads to moksha, and spiritual liberation is achievable only in after-life ("videhamukti"). The nondualistic traditions such as Adi Shankara's Advaita Vedanta tradition of Hinduism champion a monistic premise, asserting that the individual human soul and Brahman are identical, only ignorance, impulsiveness and inertia leads to suffering through Saṃsāra, in reality they are no dualities, meditation and self-knowledge is the path to liberation, the realization that one's soul is identical to Brahman is moksha, and spiritual liberation is achievable in this life ("jivanmukti").

Most Islamic schools of thought reject any idea of reincarnation of human beings or God. It teaches a linear concept of life, wherein a human being has only one life and upon death he or she is judged by God, then rewarded in heaven or punished in hell. Islam teaches final resurrection and Judgement Day, but there is no prospect for the reincarnation of a human being into a different body or being. During the early history of Islam, some of the Caliphs persecuted all reincarnation-believing people to the point of extinction (Manichaeism) in Mesopotamia and Persia (modern day Iraq and Iran). However, some Muslim minority sects such as those found among Sufis, and some Muslims in South Asia and Indonesia have retained their pre-Islamic Hindu and Buddhist beliefs in reincarnation. For instance, historically, South Asian Isma'ilis performed chantas yearly, one of which is for seeking forgiveness of sins committed in past lives.

The idea of reincarnation is accepted by a few Shia Muslim sects, particularly of the Ghulat. Alawis, belonging to the Shia denomination of Islam, hold that they were originally stars or divine lights that were cast out of heaven through disobedience and must undergo repeated reincarnation (or metempsychosis) before returning to heaven. They can be reincarnated as Christians or others through sin and as animals if they become infidels.

Reincarnation was also accepted by some streams of Sufism. Modern Sufis who embrace the idea include Bawa Muhaiyadeen. However Inayat Khan has criticized the idea as unhelpful to the spiritual seeker.

In Jainism, the reincarnation doctrine, along with its theories of "Saṃsāra" and Karma, are central to its theological foundations, as evidenced by the extensive literature on it in the major sects of Jainism, and their pioneering ideas on these topics from the earliest times of the Jaina tradition. Reincarnation in contemporary Jainism traditions is the belief that the worldly life is characterized by continuous rebirths and suffering in various realms of existence.

Karma forms a central and fundamental part of Jain faith, being intricately connected to other of its philosophical concepts like transmigration, reincarnation, liberation, non-violence ("ahiṃsā") and non-attachment, among others. Actions are seen to have consequences: some immediate, some delayed, even into future incarnations. So the doctrine of karma is not considered simply in relation to one life-time, but also in relation to both future incarnations and past lives. "Uttarādhyayana-sūtra" 3.3–4 states: "The "jīva" or the soul is sometimes born in the world of gods, sometimes in hell. Sometimes it acquires the body of a demon; all this happens on account of its karma. This "jīva" sometimes takes birth as a worm, as an insect or as an ant." The text further states (32.7): "Karma is the root of birth and death. The souls bound by karma go round and round in the cycle of existence."

Actions and emotions in the current lifetime affect future incarnations depending on the nature of the particular karma. For example, a good and virtuous life indicates a latent desire to experience good and virtuous themes of life. Therefore, such a person attracts karma that ensures that their future births will allow them to experience and manifest their virtues and good feelings unhindered. In this case, they may take birth in heaven or in a prosperous and virtuous human family. On the other hand, a person who has indulged in immoral deeds, or with a cruel disposition, indicates a latent desire to experience cruel themes of life. As a natural consequence, they will attract karma which will ensure that they are reincarnated in hell, or in lower life forms, to enable their soul to experience the cruel themes of life.

There is no retribution, judgment or reward involved but a natural consequences of the choices in life made either knowingly or unknowingly. Hence, whatever suffering or pleasure that a soul may be experiencing in its present life is on account of choices that it has made in the past. As a result of this doctrine, Jainism attributes supreme importance to pure thinking and moral behavior.

The Jain texts postulate four "gatis", that is states-of-existence or birth-categories, within which the soul transmigrates. The four "gatis" are: "deva" (demi-gods), "manuṣya" (humans), "nāraki" (hell beings) and "tiryañca" (animals, plants and micro-organisms). The four "gatis" have four corresponding realms or habitation levels in the vertically tiered Jain universe: demi-gods occupy the higher levels where the heavens are situated; humans, plants and animals occupy the middle levels; and hellish beings occupy the lower levels where seven hells are situated.

Single-sensed souls, however, called "nigoda", and element-bodied souls pervade all tiers of this universe. "Nigodas" are souls at the bottom end of the existential hierarchy. They are so tiny and undifferentiated, that they lack even individual bodies, living in colonies. According to Jain texts, this infinity of "nigodas" can also be found in plant tissues, root vegetables and animal bodies. Depending on its karma, a soul transmigrates and reincarnates within the scope of this cosmology of destinies. The four main destinies are further divided into sub-categories and still smaller sub-sub-categories. In all, Jain texts speak of a cycle of 8.4 million birth destinies in which souls find themselves again and again as they cycle within "samsara".

In Jainism, God has no role to play in an individual's destiny; one's personal destiny is not seen as a consequence of any system of reward or punishment, but rather as a result of its own personal karma. A text from a volume of the ancient Jain canon, "Bhagvati sūtra" 8.9.9, links specific states of existence to specific karmas. Violent deeds, killing of creatures having five sense organs, eating fish, and so on, lead to rebirth in hell. Deception, fraud and falsehood lead to rebirth in the animal and vegetable world. Kindness, compassion and humble character result in human birth; while austerities and the making and keeping of vows lead to rebirth in heaven.

Each soul is thus responsible for its own predicament, as well as its own salvation. Accumulated karma represent a sum total of all unfulfilled desires, attachments and aspirations of a soul. It enables the soul to experience the various themes of the lives that it desires to experience. Hence a soul may transmigrate from one life form to another for countless of years, taking with it the karma that it has earned, until it finds conditions that bring about the required fruits. In certain philosophies, heavens and hells are often viewed as places for eternal salvation or eternal damnation for good and bad deeds. But according to Jainism, such places, including the earth are simply the places which allow the soul to experience its unfulfilled karma.

Jewish mystical texts (the Kabbalah), from their classic Medieval canon onward, teach a belief in "Gilgul Neshamot" (Hebrew for metempsychosis of souls: literally "soul cycle", plural ""gilgulim""). The Zohar and the Sefer HaBahir specifically discuss reincarnation. It is a common belief in contemporary Hasidic Judaism, which regards the Kabbalah as sacred and authoritative, though understood in light of a more innate psychological mysticism. Kabbalah also teaches that "The soul of Moses is reincarnated in every generation." Other, Non-Hasidic, Orthodox Jewish groups while not placing a heavy emphasis on reincarnation, do acknowledge it as a valid teaching. Its popularization entered modern secular Yiddish literature and folk motif.

The 16th century mystical renaissance in communal Safed replaced scholastic Rationalism as mainstream traditional Jewish theology, both in scholarly circles and in the popular imagination. References to "gilgul" in former Kabbalah became systematized as part of the metaphysical purpose of creation. Isaac Luria (the Ari) brought the issue to the centre of his new mystical articulation, for the first time, and advocated identification of the reincarnations of historic Jewish figures that were compiled by Haim Vital in his Shaar HaGilgulim. "Gilgul" is contrasted with the other processes in Kabbalah of Ibbur ("pregnancy"), the attachment of a second soul to an individual for (or by) good means, and Dybuk ("possession"), the attachment of a spirit, demon, etc. to an individual for (or by) "bad" means.

In Lurianic Kabbalah, reincarnation is not retributive or fatalistic, but an expression of Divine compassion, the microcosm of the doctrine of cosmic rectification of creation. "Gilgul" is a heavenly agreement with the individual soul, conditional upon circumstances. Luria's radical system focused on rectification of the Divine soul, played out through Creation. The true essence of anything is the divine spark within that gives it existence. Even a stone or leaf possesses such a soul that "came into this world to receive a rectification". A human soul may occasionally be exiled into lower inanimate, vegetative or animal creations. The most basic component of the soul, the nefesh, must leave at the cessation of blood production. There are four other soul components and different nations of the world possess different forms of souls with different purposes. Each Jewish soul is reincarnated in order to fulfill each of the 613 Mosaic commandments that elevate a particular spark of holiness associated with each commandment. Once all the Sparks are redeemed to their spiritual source, the Messianic Era begins. Non-Jewish observance of the 7 Laws of Noah assists the Jewish people, though Biblical adversaries of Israel reincarnate to oppose.

Among the many rabbis who accepted reincarnation are Nahmanides (the Ramban) and Rabbenu Bahya ben Asher, Levi ibn Habib (the Ralbah), Shelomoh Alkabez, Moses Cordovero, Moses Chaim Luzzatto; early Hasidic masters such as the Baal Shem Tov, Schneur Zalman of Liadi and Nachman of Breslov, as well as virtually all later Hasidic masters; contemporary Hasidic teachers such as DovBer Pinson, Moshe Weinberger and Joel Landau; and key Mitnagdic leaders, such as the Vilna Gaon and Chaim Volozhin and their school, as well as Rabbi Shalom Sharabi (known at the RaShaSH), the Ben Ish Chai of Baghdad, and the Baba Sali. Rabbis who have rejected the idea include Saadia Gaon, David Kimhi, Hasdai Crescas, Joseph Albo, Abraham ibn Daud, Leon de Modena, Solomon ben Aderet, Maimonides and Asher ben Jehiel. Among the Geonim, Hai Gaon argued in favour of "gilgulim".

Reincarnation is an intrinsic part of some northern Native American and Inuit traditions. In the now heavily Christian Polar North (now mainly parts of Greenland and Nunavut), the concept of reincarnation is enshrined in the Inuit language.

The following is a story of human-to-human reincarnation as told by Thunder Cloud, a Winnebago (Ho-Chunk tribe) shaman referred to as T. C. in the narrative. Here T. C. talks about his two previous lives and how he died and came back again to this his third lifetime. He describes his time between lives, when he was “blessed” by Earth Maker and all the abiding spirits and given special powers, including the ability to heal the sick.

T. C.'s Account of His Two Reincarnations:
Founded in the 15th century, Sikhism's founder Guru Nanak had a choice between the cyclical reincarnation concept of ancient Indian religions and the linear concept of Islam and he chose the cyclical concept of time. Sikhism teaches reincarnation theory similar to those in Hinduism, but with some differences from its traditional doctrines. Sikh rebirth theories about the nature of existence are similar to ideas that developed during the devotional Bhakti movement particularly within some Vaishnava traditions, which define liberation as a state of union with God attained through the grace of God.

The doctrines of Sikhism teach that the soul exists, and is passed from one body to another in endless cycles of Saṃsāra, until liberation from the death and re-birth cycle. Each birth begins with karma ("karam"), and these actions leave a "karni" (karmic signature) on one's soul which influences future rebirths, but it is God whose grace that liberates from the death and re-birth cycle. The way out of the reincarnation cycle, asserts Sikhism, is to live an ethical life, devote oneself to God and constantly remember God's name. The precepts of Sikhism encourage the bhakti of One Lord for "mukti" (liberation from the death and re-birth cycle.).

Spiritism, a Christian philosophy codified in the 19th century by the French educator Allan Kardec, teaches reincarnation or rebirth into human life after death. According to this doctrine, free will and cause and effect are the corollaries of reincarnation, and reincarnation provides a mechanism for man's spiritual evolution in successive lives.

The Theosophical Society draws much of its inspiration from India. In the Theosophical world-view reincarnation is the vast rhythmic process by which the soul, the part of a person which belongs to the formless non-material and timeless worlds, unfolds its spiritual powers in the world and comes to know itself. It descends from sublime, free, spiritual realms and gathers experience through its effort to express itself in the world. Afterwards there is a withdrawal from the physical plane to successively higher levels of reality, in death, a purification and assimilation of the past life. Having cast off all instruments of personal experience it stands again in its spiritual and formless nature, ready to begin its next rhythmic manifestation, every lifetime bringing it closer to complete self-knowledge and self-expression. However it may attract old mental, emotional, and energetic "karma" patterns to form the new personality.

Anthroposophy describes reincarnation from the point of view of Western philosophy and culture. The ego is believed to transmute transient soul experiences into universals that form the basis for an individuality that can endure after death. These universals include ideas, which are intersubjective and thus transcend the purely personal (spiritual consciousness), intentionally formed human character (spiritual life), and becoming a fully conscious human being (spiritual humanity). Rudolf Steiner described both the general principles he believed to be operative in reincarnation, such as that one's will activity in one life forms the basis for the thinking of the next, and a number of successive lives of various individualities.

Inspired by Helena Blavatsky's major works, including "Isis Unveiled" and "The Secret Doctrine", astrologers in the early twentieth-century integrated the concepts of karma and reincarnation into the practice of Western astrology. Notable astrologers who advanced this development included Alan Leo, Charles E. O. Carter, Marc Edmund Jones, and Dane Rudhyar. A new synthesis of East and West resulted as Hindu and Buddhist concepts of reincarnation were fused with Western astrology's deep roots in Hermeticism and Neoplatonism. In the case of Rudhyar, this synthesis was enhanced with the addition of Jungian depth psychology. This dynamic integration of astrology, reincarnation and depth psychology has continued into the modern era with the work of astrologers Steven Forrest and Jeffrey Wolf Green. Their respective schools of Evolutionary Astrology are based on "an acceptance of the fact that human beings incarnate in a succession of lifetimes."

Past reincarnation, usually termed "past lives", is a key part of the principles and practices of the Church of Scientology. Scientologists believe that the human individual is actually a "thetan", an immortal spiritual entity, that has fallen into a degraded state as a result of past-life experiences. Scientology auditing is intended to free the person of these past-life traumas and recover past-life memory, leading to a higher state of spiritual awareness. This idea is echoed in their highest fraternal religious order, the Sea Organization, whose motto is "Revenimus" or "We Come Back", and whose members sign a "billion-year contract" as a sign of commitment to that ideal. L. Ron Hubbard, the founder of Scientology, does not use the word "reincarnation" to describe its beliefs, noting that: "The common definition of reincarnation has been altered from its original meaning. The word has come to mean 'to be born again in different life forms' whereas its actual definition is 'to be born again into the flesh of another body.' Scientology ascribes to this latter, original definition of reincarnation."

The first writings in Scientology regarding past lives date from around 1951 and slightly earlier. In 1960, Hubbard published a book on past lives entitled "Have You Lived Before This Life". In 1968 he wrote "Mission into Time", a report on a five-week sailing expedition to Sardinia, Sicily and Carthage to see if specific evidence could be found to substantiate L. Ron Hubbard's recall of incidents in his own past, centuries ago.

The Indian spiritual teacher Meher Baba stated that reincarnation occurs due to desires and once those desires are extinguished the ego-mind ceases to reincarnate.

Wicca is a neo-pagan religion focused on nature, guided by the philosophy of Wiccan Rede that advocates the tenets "Harm None, Do As Ye Will". Wiccans believe in a form of karmic return where one's deeds are returned, either in the current life or in another life, threefold or multiple times in order to teach one lessons (The Threefold Law). Reincarnation is therefore an accepted part of the Wiccan faith. Wiccans also believe that death and afterlife are important experiences for the soul to transform and prepare for future lifetimes.

Before the late nineteenth century, reincarnation was a relatively rare theme in the West. In ancient Greece, the Orphic Mysteries and Pythagoreans believed in various forms of reincarnation. Emanuel Swedenborg believed that we leave the physical world once, but then go through several lives in the spiritual world — a kind of hybrid of Christian tradition and the popular view of reincarnation.

More recently, many people in the West have developed an interest in and acceptance of reincarnation. Many new religious movements include reincarnation among their beliefs, e.g. modern Neopagans, Spiritism, Astara, Dianetics, and Scientology. Many esoteric philosophies also include reincarnation, e.g. Theosophy, Anthroposophy, Kabbalah, and Gnostic and Esoteric Christianity. 

Demographic survey data from 1999–2002 shows a significant minority of people from Europe (22%) and America (20%) believe in the existence of life before birth and after death, leading to a physical rebirth. The belief in reincarnation is particularly high in the Baltic countries, with Lithuania having the highest figure for the whole of Europe, 44%, while the lowest figure is in East Germany, 12%. A quarter of U.S. Christians, including 10% of all born again Christians, embrace the idea.
Skeptic Carl Sagan asked the Dalai Lama what he would do if a fundamental tenet of his religion (reincarnation) were definitively disproved by science. The Dalai Lama answered, "If science can disprove reincarnation, Tibetan Buddhism would abandon reincarnation… but it's going to be mighty hard to disprove reincarnation.". Sagan considers claims of memories of past lives to be worthy of research, although he considers reincarnation to be an unlikely explanation for these..

Ian Stevenson reported that belief in reincarnation is held (with variations in details) by adherents of almost all major religions except Christianity and Islam. In addition, between 20 and 30 percent of persons in western countries who may be nominal Christians also believe in reincarnation.

Edgar Cayce an American self-professed clairvoyant answered questions on subjects as varied as healing, reincarnation, wars, Atlantis, and future events while allegedly asleep.

According to Dr. Brian Weiss, in 1980 one of his patients, "Catherine", began discussing past-life experiences under hypnosis. Weiss did not believe in reincarnation at the time but, after confirming elements of Catherine's stories through public records, came to be convinced of the survival of an element of the human personality after death. Weiss claims he has regressed more than 4,000 patients since 1980.

Neale Donald Walsch, an American author of the series Conversations with God who says his books are not channelled, but rather that they are inspired by God and that they can help a person relate to God from a modern perspective claims that he has reincarnated more than 600 times.

Other influential contemporary figures that have written on reincarnation include Alice Ann Bailey, one of the first writers to use the terms New Age and age of Aquarius, Torkom Saraydarian, an Armenian-American musician and religious author, Dolores Cannon, Atul Gawande, Michael Newton, Bruce Greyson, Raymond Moody and Unity Church founder Charles Fillmore.

One 1999 study by Walter and Waterhouse reviewed the previous data on the level of reincarnation belief and performed a set of thirty in-depth interviews in Britain among people who did not belong to a religion advocating reincarnation. The authors reported that surveys have found about one fifth to one quarter of Europeans have some level of belief in reincarnation, with similar results found in the USA. In the interviewed group, the belief in the existence of this phenomenon appeared independent of their age, or the type of religion that these people belonged to, with most being Christians. The beliefs of this group also did not appear to contain any more than usual of "new age" ideas (broadly defined) and the authors interpreted their ideas on reincarnation as "one way of tackling issues of suffering", but noted that this seemed to have little effect on their private lives.

Waterhouse also published a detailed discussion of beliefs expressed in the interviews. She noted that although most people "hold their belief in reincarnation quite lightly" and were unclear on the details of their ideas, personal experiences such as past-life memories and near-death experiences had influenced most believers, although only a few had direct experience of these phenomena. Waterhouse analyzed the influences of second-hand accounts of reincarnation, writing that most of the people in the survey had heard other people's accounts of past-lives from regression hypnosis and dreams and found these fascinating, feeling that there "must be something in it" if other people were having such experiences.

Psychiatrist Ian Stevenson, from the University of Virginia, having grown up with a mother who was a theosophist, dedicated his latter career to investigating claims of reincarnation in hopes of providing evidence that reincarnation happens. Other people who have undertaken similar pursuits include Jim B. Tucker, Antonia Mills, Satwant Pasricha, Godwin Samararatne, and Erlendur Haraldsson, but Stevenson's publications remain the most well-known.

Stevenson conducted more than 2,500 case studies of young children who claimed to remember past lives over a period of 40 years and published twelve books, including "Twenty Cases Suggestive of Reincarnation", Reincarnation and Biology: A Contribution to the Etiology of Birthmarks and Birth Defects, a two-part monograph and "Where Reincarnation and Biology Intersect". He documented the family's and child's statements along with correlates to a deceased person he believed matched the child's memory. Stevenson also claimed that some birthmarks and birth defects matched wounds and scars on the deceased, sometimes providing medical records like autopsy photographs to make his case. Expecting controversy and skepticism, Stevenson also searched for disconfirming evidence and alternative explanations for the reports, but he argued (not without criticism) that his methods ruled out all possible "normal" explanations for the child’s memories. Stevenson's work in this regard was impressive enough to Carl Sagan that he referred to what was apparently Stevenson's investigations in his book "The Demon-Haunted World" as an example of carefully collected empirical data, though he rejected reincarnation as a parsimonious explanation for the stories, although he admitted that the phenomenon of alleged past life memories should be researched. Sam Harris cited Stevenson's works in his book "The End of Faith" as part of a body of data that seems to attest to the reality of psychic phenomena, but that only relies on subjective personal experience.

Critical reviews of these claims include work by Paul Edwards who criticized the accounts of reincarnation as being purely anecdotal and cherry-picked. Instead, Edwards says such stories are attributable to selective thinking, suggestion, and false memories that can result from the family's or researcher's belief systems, and thus cannot be counted as empirical evidence. The philosopher Keith Augustine wrote in critique that the fact that "the vast majority of Stevenson's cases come from countries where a religious belief in reincarnation is strong, and rarely elsewhere, seems to indicate that cultural conditioning (rather than reincarnation) generates claims of spontaneous past-life memories." Further, Ian Wilson pointed out that a large number of Stevenson’s cases consisted of poor children remembering wealthy lives or belonging to a higher caste. In these societies, claims of reincarnation are sometimes used as schemes to obtain money from the richer families of alleged former incarnations. Following these types of criticism, Stevenson published a book on "European Cases of the Reincarnation Type" in order to show the reports were cross-cultural. Even still, Robert Baker asserted that all the past-life experiences investigated by Stevenson and other parapsychologists are understandable in terms of known psychological factors including a mixture of cryptomnesia and confabulation. Edwards also objected that reincarnation invokes assumptions that are inconsistent with modern science. As the vast majority of people do not remember previous lives and there is no empirically documented mechanism known that allows personality to survive death and travel to another body, positing the existence of reincarnation is subject to the principle that "extraordinary claims require extraordinary evidence". Researchers such as Stevenson acknowledged these limitations.

Stevenson also claimed there were a handful of cases that suggested evidence of xenoglossy, including two where a subject under hypnosis allegedly conversed with people speaking the foreign language, instead of merely being able to recite foreign words. Sarah Thomason, a linguist (and skeptical researcher) at the University of Michigan, reanalyzed these cases, concluding that "the linguistic evidence is too weak to provide support for the claims of xenoglossy."




</doc>
<doc id="25808" url="https://en.wikipedia.org/wiki?curid=25808" title="Robert Noyce">
Robert Noyce

Robert Norton Noyce (December 12, 1927 – June 3, 1990), nicknamed "the Mayor of Silicon Valley," was an American physicist who co-founded Fairchild Semiconductor in 1957 and Intel Corporation in 1968. He is also credited with the realization of the first monolithic integrated circuit or microchip, which fueled the personal computer revolution and gave Silicon Valley its name.

Noyce was born on December 12, 1927, in Burlington, Iowa the third of four sons of the Rev. Ralph Brewster Noyce. His father graduated from Doane College, Oberlin College, and the Chicago Theological Seminary and was also nominated for a Rhodes Scholarship. In the 1930s and 1940s, the Reverend Noyce worked as a Congregational clergyman and as the associate superintendent of the Iowa Conference of Congregational Churches.

His mother, Harriet May Norton, was the daughter of the Rev. Milton J. Norton, a Congregational clergyman, and Louise Hill. She was a graduate of Oberlin College and prior to her marriage, she had dreams of becoming a missionary. She has been described as an intelligent woman with a commanding will.

Noyce had three siblings: Donald Sterling Noyce, Gaylord Brewster Noyce and Ralph Harold Noyce. His earliest childhood memory involved beating his father at ping pong and feeling absolutely shocked when his mother reacted to the thrilling news of his victory with a distracted "Wasn't that nice of Daddy to let you win?" Even at the age of five, Noyce felt offended by the notion of intentionally losing at anything. "That's not the game", he sulked to his mother. "If you're going to play, play to win!"

When Noyce was 12 years old in the summer of 1940, he and his brother built a boy-sized aircraft, which they used to fly from the roof of the Grinnell College stables. Later he built a radio from scratch and motorized his sled by welding a propeller and an engine from an old washing machine to the back of it. His parents were both religious but Noyce became an agnostic and irreligious in later life.

Noyce grew up in Grinnell, Iowa. While in high school, he exhibited a talent for mathematics and science and took the Grinnell College freshman physics course in his senior year. He graduated from Grinnell High School in 1945 and entered Grinnell College in the fall of that year. He was the star diver on the 1947 Midwest Conference Championship swim team. While at Grinnell College, Noyce sang, played the oboe and acted. In Noyce's junior year, he got in trouble for stealing a 25-pound pig from the Grinnell mayor's farm and roasting it at a school luau. The mayor sent a letter home to Noyce's parents stating that “In the agricultural state of Iowa, stealing a domestic animal is a felony which carries a minimum penalty of a year in prison and a fine of one dollar.” So essentially, Noyce would have to be expelled from school. Grant Gale, Noyce's physics professor and president of the college, did not want to lose a student with Robert's potential. They were able to compromise with the mayor so that Grinnell would compensate him for the pig, Noyce would only be suspended for one semester, and no further charges would be pressed. He returned in February 1949. He graduated Phi Beta Kappa with a BA in physics and mathematics in 1949. He also received a signal honor from his classmates: the Brown Derby Prize, which recognized "the senior man who earned the best grades with the least amount of work".

While Noyce was an undergraduate, he was fascinated by the field of physics and took a course in the subject that was taught by professor Grant Gale. Gale obtained two of the very first transistors ever to come out of Bell Labs and showed them off to his class. Noyce was hooked. Gale suggested that he apply to the doctoral program in physics at MIT, which he did.

Noyce had a mind so quick that his graduate school friends called him "Rapid Robert." He received his doctorate in physics from MIT in 1953.

After graduating from MIT in 1953, Noyce took a job as a research engineer at the Philco Corporation in Philadelphia. He left in 1956 to join William Shockley, a co-inventor of the transistor and eventual Nobel Prize winner, at the Shockley Semiconductor Laboratory in Mountain View, California.

Noyce left a year later with the "traitorous eight" upon having issues with Shockley's management style, and co-founded the influential Fairchild Semiconductor corporation. According to Sherman Fairchild, Noyce's impassioned presentation of his vision was the reason Fairchild had agreed to create the semiconductor division for the traitorous eight.

After Jack Kilby invented the first hybrid integrated circuit (hybrid IC) in 1958, Noyce in 1959 independently invented a new type of integrated circuit, the monolithic integrated circuit (monolithic IC). It was more practical than Kilby's implementation. Noyce's design was made of silicon, whereas Kilby's chip was made of germanium. Noyce's invention was the first monolithic integrated circuit chip. Unlike Kilby's IC which had external wire connections and could not be mass-produced, Noyce's monolithic IC chip put all components on a chip of silicon and connected them with copper lines. The basis for Noyce's monolithic IC was the planar process, developed in early 1959 by Jean Hoerni. In turn, the basis for Hoerni's planar process were the silicon surface passivation and thermal oxidation methods developed by Mohamed Atalla in 1957.

Noyce and Gordon Moore founded Intel in 1968 when they left Fairchild Semiconductor. Arthur Rock, the chairman of Intel's board and a major investor in the company, said that for Intel to succeed, the company needed Noyce, Moore and Andrew Grove. And it needed them in that order. Noyce: the visionary, born to inspire; Moore: the virtuoso of technology; and Grove: the technologist turned management scientist. The relaxed culture that Noyce brought to Intel was a carry-over from his style at Fairchild Semiconductor. He treated employees as family, rewarding and encouraging teamwork. Noyce's management style could be called "roll up your sleeves". He shunned fancy corporate cars, reserved parking spaces, private jets, offices, and furnishings in favor of a less-structured, relaxed working environment in which everyone contributed and no one received lavish benefits. By declining the usual executive perks he stood as a model for future generations of Intel CEOs.

At Intel, he oversaw Ted Hoff's invention of the microprocessor, which was his second revolution.

In 1953, Noyce married Elizabeth Bottomley, who was a 1951 graduate of Tufts University. While living in Los Altos, California they had four children: William B., Pendred, Priscilla, and Margaret. Elizabeth loved New England, so the family acquired a 50-acre coastal summer home in Bremen, Maine. Elizabeth and the children would summer there. Robert would visit during the summer, but he continued working at Intel during the summer. They divorced in 1974. 

On November 27, 1974, Noyce married Ann Schmeltz Bowers. Bowers, a graduate of Cornell University, also received an honorary Ph.D. from Santa Clara University, where she was a trustee for nearly 20 years. She was the first Director of Personnel for Intel Corporation and the first Vice President of Human Resources for Apple Inc. She currently serves as Chair of the Board and the founding trustee of the Noyce Foundation.

Noyce kept active his entire life. He enjoyed reading Hemingway, and he flew his own airplane and also participated in hang-gliding and scuba diving. Noyce believed that microelectronics would continue to advance in complexity and sophistication well beyond its current state; this led to the question of what use society would make of the technology. In his last interview, Noyce was asked what he would do if he were "emperor" of the United States. He said that he would, among other things, "…make sure we are preparing our next generation to flourish in a high-tech age. And that means education of the lowest and the poorest, as well as at the graduate school level."

Noyce suffered a heart attack at age 62 at home on June 3, 1990, and later died at the Seton Medical Center in Austin, Texas.

In July 1959, he filed for "Semiconductor Device and Lead Structure", a type of integrated circuit. This independent effort was recorded only a few months after the key findings of inventor Jack Kilby. For his co-invention of the integrated circuit and its world-transforming impact, three presidents of the United States honored him.

Noyce was a holder of many honors and awards. President Ronald Reagan awarded him the National Medal of Technology in 1987. Two years later, he was inducted into the U.S. Business Hall of Fame sponsored by Junior Achievement, during a black tie ceremony keynoted by President George H. W. Bush. In 1990 Noycealong with, among others, Jack Kilby and transistor inventor John Bardeenreceived a "Lifetime Achievement Medal" during the bicentennial celebration of the Patent Act.

Noyce received the Franklin Institute's Stuart Ballantine Medal in 1966. He was awarded the IEEE Medal of Honor in 1978 "for his contributions to the silicon integrated circuit, a cornerstone of modern electronics." In 1979, he was awarded the National Medal of Science. Noyce was elected a Fellow of the American Academy of Arts and Sciences in 1980. The National Academy of Engineering awarded him its 1989 Charles Stark Draper Prize.

The science building at his alma mater, Grinnell College, is named after him.

On December 12, 2011, Noyce was honored with a Google Doodle celebrating the 84th anniversary of his birth.

December 8, 2000 According to the book 'The Innovators' Noyce was mentioned/credited as the honorary co-recipient in the Nobel Prize acceptance speech given by Kilby http://www.nobelprize.org/nobel_prizes/physics/laureates/2000/kilby-lecture.html

The Noyce Foundation was founded in 1990 by his family. The foundation was dedicated to improving public education in mathematics and science in grades K-12. The foundation announced that it would end operations in 2015.

Noyce was granted 15 patents. Patents are listed in order issued, not filed.

Note: In 1960 Clevite Corporation acquired Shockley Semiconductor Laboratory, a subsidiary of Beckman Instruments, for whom Noyce worked. 



</doc>
<doc id="25809" url="https://en.wikipedia.org/wiki?curid=25809" title="Riemann zeta function">
Riemann zeta function

The Riemann zeta function or Euler–Riemann zeta function, , is a function of a complex variable "s" that analytically continues the sum of the Dirichlet series 

which converges when the real part of is greater than 1. More general representations of for all are given below. The Riemann zeta function plays a pivotal role in analytic number theory and has applications in physics, probability theory, and applied statistics.

As a function of a real variable, Leonhard Euler first introduced and studied it in the first half of the eighteenth century without using complex analysis, which was not available at the time. Bernhard Riemann's 1859 article "On the Number of Primes Less Than a Given Magnitude" extended the Euler definition to a complex variable, proved its meromorphic continuation and functional equation, and established a relation between its zeros and the distribution of prime numbers.

The values of the Riemann zeta function at even positive integers were computed by Euler. The first of them, , provides a solution to the Basel problem. In 1979 Roger Apéry proved the irrationality of . The values at negative integer points, also found by Euler, are rational numbers and play an important role in the theory of modular forms. Many generalizations of the Riemann zeta function, such as Dirichlet series, Dirichlet -functions and -functions, are known.

The Riemann zeta function is a function of a complex variable . (The notation , , and is used traditionally in the study of the zeta function, following Riemann.)

The zeta function can be expressed by the following integral:

where 

is the gamma function.

For the special case where the real part of is greater than 1, always converges, and can be simplified to the following infinite series:

The Riemann zeta function is defined as the analytic continuation of the function defined for by the sum of the preceding series.

Leonhard Euler considered the above series in 1740 for positive integer values of , and later Chebyshev extended the definition to .

The above series is a prototypical Dirichlet series that converges absolutely to an analytic function for such that and diverges for all other values of . Riemann showed that the function defined by the series on the half-plane of convergence can be continued analytically to all complex values . For the series is the harmonic series which diverges to , and

Thus the Riemann zeta function is a meromorphic function on the whole complex -plane, which is holomorphic everywhere except for a simple pole at with residue 1.

For any positive even integer :

where is the th Bernoulli number.

For odd positive integers, no such simple expression is known, although these values are thought to be related to the algebraic -theory of the integers; see Special values of -functions.

For nonpositive integers, one has

for (using the NIST convention that )

In particular, vanishes at the negative even integers because for all odd other than 1. These are the so-called "trivial zeros" of the zeta function.

Via analytic continuation, one can show that: 






Taking the limit formula_17, one obtains formula_18.

The connection between the zeta function and prime numbers was discovered by Euler, who proved the identity

where, by definition, the left hand side is and the infinite product on the right hand side extends over all prime numbers (such expressions are called Euler products):

Both sides of the Euler product formula converge for . The proof of Euler's identity uses only the formula for the geometric series and the fundamental theorem of arithmetic. Since the harmonic series, obtained when , diverges, Euler's formula (which becomes ) implies that there are infinitely many primes.

The Euler product formula can be used to calculate the asymptotic probability that randomly selected integers are set-wise coprime. Intuitively, the probability that any single number is divisible by a prime (or any integer) is . Hence the probability that numbers are all divisible by this prime is , and the probability that at least one of them is "not" is . Now, for distinct primes, these divisibility events are mutually independent because the candidate divisors are coprime (a number is divisible by coprime divisors and if and only if it is divisible by , an event which occurs with probability ). Thus the asymptotic probability that numbers are coprime is given by a product over all primes,

The zeta function satisfies the functional equation:

where is the gamma function. This is an equality of meromorphic functions valid on the whole complex plane. The equation relates values of the Riemann zeta function at the points and , in particular relating even positive integers with odd negative integers. Owing to the zeros of the sine function, the functional equation implies that has a simple zero at each even negative integer , known as the trivial zeros of . When is an even positive integer, the product on the right is non-zero because has a simple pole, which cancels the simple zero of the sine factor.
A proof of the functional equation proceeds as follows:
We observe that if formula_23, then

As a result, if formula_25 then

where the "η"-series is convergent (albeit non-absolutely) in the larger half-plane (for a more detailed survey on the history of the functional equation, see e.g. Blagouchine).

Riemann also found a symmetric version of the functional equation applying to the xi-function:

which satisfies:

The functional equation shows that the Riemann zeta function has zeros at . These are called the trivial zeros. They are trivial in the sense that their existence is relatively easy to prove, for example, from being 0 in the functional equation. The non-trivial zeros have captured far more attention because their distribution not only is far less understood but, more importantly, their study yields impressive results concerning prime numbers and related objects in number theory. It is known that any non-trivial zero lies in the open strip , which is called the critical strip. The Riemann hypothesis, considered one of the greatest unsolved problems in mathematics, asserts that any non-trivial zero has . In the theory of the Riemann zeta function, the set  is called the critical line. For the Riemann zeta function on the critical line, see -function.

In 1914, Godfrey Harold Hardy proved that has infinitely many real zeros.

Hardy and John Edensor Littlewood formulated two conjectures on the density and distance between the zeros of on intervals of large positive real numbers. In the following, is the total number of real zeros and the total number of zeros of odd order of the function lying in the interval .

These two conjectures opened up new directions in the investigation of the Riemann zeta function.

The location of the Riemann zeta function's zeros is of great importance in the theory of numbers. The prime number theorem is equivalent to the fact that there are no zeros of the zeta function on the line. A better result that follows from an effective form of Vinogradov's mean-value theorem is that whenever and

The strongest result of this kind one can hope for is the truth of the Riemann hypothesis, which would have many profound consequences in the theory of numbers.

It is known that there are infinitely many zeros on the critical line. Littlewood showed that if the sequence () contains the imaginary parts of all zeros in the upper half-plane in ascending order, then

The critical line theorem asserts that a positive proportion of the nontrivial zeros lies on the critical line. (The Riemann hypothesis would imply that this proportion is 1.)

In the critical strip, the zero with smallest non-negative imaginary part is (). The fact that
for all complex implies that the zeros of the Riemann zeta function are symmetric about the real axis. Combining this symmetry with the functional equation, furthermore, one sees that the non-trivial zeros are symmetric about the critical line .

For sums involving the zeta-function at integer and half-integer values, see rational zeta series.

The reciprocal of the zeta function may be expressed as a Dirichlet series over the Möbius function :
for every complex number with real part greater than 1. There are a number of similar relations involving various well-known multiplicative functions; these are given in the article on the Dirichlet series.
The Riemann hypothesis is equivalent to the claim that this expression is valid when the real part of is greater than .

The critical strip of the Riemann zeta function has the remarkable property of universality. This zeta-function universality states that there exists some location on the critical strip that approximates any holomorphic function arbitrarily well. Since holomorphic functions are very general, this property is quite remarkable. The first proof of universality was provided by Sergei Mikhailovitch Voronin in 1975. More recent work has included effective versions of Voronin's theorem and extending it to Dirichlet L-functions.

Let the functions and be defined by the equalities

Here is a sufficiently large positive number, , , , . Estimating the values and from below shows, how large (in modulus) values can take on short intervals of the critical line or in small neighborhoods of points lying in the critical strip .

The case was studied by Kanakanahalli Ramachandra; the case , where is a sufficiently large constant, is trivial.

Anatolii Karatsuba proved, in particular, that if the values and exceed certain sufficiently small constants, then the estimates

hold, where and are certain absolute constants.

The function
is called the argument of the Riemann zeta function. Here is the increment of an arbitrary continuous branch of along the broken line joining the points , and .

There are some theorems on properties of the function . Among those results are the mean value theorems for and its first integral
on intervals of the real line, and also the theorem claiming that every interval for
contains at least
points where the function changes sign. Earlier similar results were obtained by Atle Selberg for the case

An extension of the area of convergence can be obtained by rearranging the original series. The series
converges for , while 
converges even for . In this way, the area of convergence can be extended to for any negative integer .

The Mellin transform of a function is defined as

in the region where the integral is defined. There are various expressions for the zeta-function as Mellin transform-like integrals. If the real part of is greater than one, we have

where denotes the gamma function. By modifying the contour, Riemann showed that

for all (where denotes the Hankel contour).

Starting with the integral formula formula_45 one can show by substitution and iterated differentation for natural formula_46
using the notation of umbral calculus where each power formula_48 is to be replaced by formula_49, so e.g. for formula_50 we have formula_51 while for formula_52 this becomes 

We can also find expressions which relate to prime numbers and the prime number theorem. If is the prime-counting function, then

for values with .

A similar Mellin transform involves the Riemann prime-counting function , which counts prime powers with a weight of , so that

Now we have

These expressions can be used to prove the prime number theorem by means of the inverse Mellin transform. Riemann's prime-counting function is easier to work with, and can be recovered from it by Möbius inversion.

The Riemann zeta function can be given by a Mellin transform

in terms of Jacobi's theta function

However, this integral only converges if the real part of is greater than 1, but it can be regularized. This gives the following expression for the zeta function, which is well defined for all except 0 and 1:

The Riemann zeta function is meromorphic with a single pole of order one at . It can therefore be expanded as a Laurent series about ; the series development is then

The constants here are called the Stieltjes constants and can be defined by the limit

The constant term is the Euler–Mascheroni constant.

For all , , the integral relation (cf. Abel–Plana formula)
holds true, which may be used for a numerical evaluation of the zeta-function.

Another series development using the rising factorial valid for the entire complex plane is

This can be used recursively to extend the Dirichlet series definition to all complex numbers.

The Riemann zeta function also appears in a form similar to the Mellin transform in an integral over the Gauss–Kuzmin–Wirsing operator acting on ; that context gives rise to a series expansion in terms of the falling factorial.

On the basis of Weierstrass's factorization theorem, Hadamard gave the infinite product expansion

where the product is over the non-trivial zeros of and the letter again denotes the Euler–Mascheroni constant. A simpler infinite product expansion is

This form clearly displays the simple pole at , the trivial zeros at −2, −4, ... due to the gamma function term in the denominator, and the non-trivial zeros at . (To ensure convergence in the latter formula, the product should be taken over "matching pairs" of zeros, i.e. the factors for a pair of zeros of the form and should be combined.)

A globally convergent series for the zeta function, valid for all complex numbers except for some integer , was conjectured by Konrad Knopp and proven by Helmut Hasse in 1930 (cf. Euler summation):

The series only appeared in an appendix to Hasse's paper, and did not become generally known until it was discussed by Jonathan Sondow in 1994.

Hasse also proved the globally converging series
in the same publication, but research by Iaroslav Blagouchine
has found that this latter series was actually first published by Joseph Ser in 1926. New proofs for both of these results were offered by Demetrios Kanoussis in 2017. Other similar globally convergent series include

where are the harmonic numbers, formula_69 are the Stirling numbers of the first kind, formula_70 is the Pochhammer symbol, are the Gregory coefficients, are the Gregory coefficients of higher order, are the Cauchy numbers of the second kind (, , ...), and 
are the Bernoulli polynomials of the second kind, see Blagouchine's paper.

Peter Borwein has developed an algorithm that applies Chebyshev polynomials to the Dirichlet eta function to produce a very rapidly convergent series suitable for high precision numerical calculations.

Here is the primorial sequence and is Jordan's totient function.

The function can be represented, for , by the infinite series
where , is the th branch of the Lambert -function, and is an incomplete poly-Bernoulli number.

The function :formula_73 is iterated to find the coefficients appearing in Engel expansions.

The Mellin transform of the map formula_74 is related to the Riemann zeta function by the formula

For formula_76 , the Riemann zeta function has for fixed formula_77 and for all formula_78 the following representation in terms of three absolutely and uniformly converging series,formula_79where for positive integer formula_80 one has to take the limit value formula_81. The derivatives of formula_82 can be calculated by differentiating the above series termwise. From this follows an algorithm which allows to compute, to arbitrary precision, formula_82 and its derivatives using at most formula_84 summands for any formula_85, with explicit error bounds. For formula_82, these are as follows:

For a given argument formula_87 with formula_88 and formula_89 one can approximate formula_82 to any accuracy formula_91 by summing the first series to formula_92, formula_93 to formula_94 and neglecting formula_95, if one chooses formula_96 as the next higher integer of the unique solution of formula_97 in the unknown formula_98, and from this formula_99. For formula_100 one can neglect formula_93 altogether. Under the mild condition formula_102 one needs at most formula_103 summands. Hence this algorithm is essentially as fast as the Riemann-Siegel formula. Similar algorithms are possible for Dirichlet L-functions.

The zeta function occurs in applied statistics (see Zipf's law and Zipf–Mandelbrot law).

Zeta function regularization is used as one possible means of regularization of divergent series and divergent integrals in quantum field theory. In one notable example, the Riemann
zeta-function shows up explicitly in one method of calculating the Casimir effect. The zeta function is also useful for the analysis of dynamical systems.

The zeta function evaluated at equidistant positive integers appears in infinite series representations of a number of constants. 

In fact the even and odd terms give the two sums
and

Parametrized versions of the above sums are given by

and


with formula_109 and where formula_110 and formula_111 are the polygamma function and Euler's constant, as well as


all of which are continuous at formula_113. Other sums include


where denotes the imaginary part of a complex number.

There are yet more formulas in the article Harmonic number.

There are a number of related zeta functions that can be considered to be generalizations of the Riemann zeta function. These include the Hurwitz zeta function

(the convergent series representation was given by Helmut Hasse in 1930, cf. Hurwitz zeta function), which coincides with the Riemann zeta function when (the lower limit of summation in the Hurwitz zeta function is 0, not 1), the Dirichlet -functions and the Dedekind zeta-function. For other related functions see the articles zeta function and -function.

The polylogarithm is given by

which coincides with the Riemann zeta function when .

The Lerch transcendent is given by
which coincides with the Riemann zeta function when and (the lower limit of summation in the Lerch transcendent is 0, not 1).

The Clausen function that can be chosen as the real or imaginary part of .

The multiple zeta functions are defined by

One can analytically continue these functions to the -dimensional complex space. The special values taken by these functions at positive integer arguments are called multiple zeta values by number theorists and have been connected to many different branches in mathematics and physics.





</doc>
<doc id="25813" url="https://en.wikipedia.org/wiki?curid=25813" title="Rice University">
Rice University

William Marsh Rice University, commonly known as Rice University, is a private research university in Houston, Texas. The university is situated on a 300-acre (121 ha) campus near the Houston Museum District and is adjacent to the Texas Medical Center.

Opened in 1912 after the murder of its namesake William Marsh Rice, Rice is now a research university with an undergraduate focus. Its emphasis on education is demonstrated by a small student body and 6:1 student-faculty ratio, and it has been nationally recognized as a leading university for undergraduate teaching. The university has a very high level of research activity, with $140.2 million in sponsored research funding in 2016. Rice is noted for its applied science programs in the fields of artificial heart research, structural chemical analysis, signal processing, space science, and nanotechnology. It was ranked first in the world in materials science research by the Times Higher Education (THE) in 2010. Rice is a member of the Association of American Universities.

The university is organized into eleven residential colleges and eight schools of academic study, including the Wiess School of Natural Sciences, the George R. Brown School of Engineering, the School of Social Sciences, School of Architecture, Shepherd School of Music and the School of Humanities. Rice's undergraduate program offers more than fifty majors and two dozen minors, and allows a high level of flexibility in pursuing multiple degree programs. Additional graduate programs are offered through the Jesse H. Jones Graduate School of Business and the Susanne M. Glasscock School of Continuing Studies. Rice students are bound by the strict Honor Code, which is enforced by a student-run Honor Council.

Rice competes in 14 NCAA Division I varsity sports and is a part of Conference USA, often competing with its cross-town rival the University of Houston. Intramural and club sports are offered in a wide variety of activities such as jiu jitsu, water polo, and crew.

The university has produced numerous prominent alumni, including more than two dozen Marshall Scholars and a dozen Rhodes Scholars. Given the university's close links to NASA, it has produced a significant number of astronauts and space scientists. In business, Rice graduates have become CEOs and founders of Fortune 500 companies; in politics, alumni have won positions as congressmen, cabinet secretaries, judges, and mayors. Two alumni have won the Nobel Prize, and numerous others are leading researchers in science, technology, and engineering.

Rice University's history began with the untimely demise of Massachusetts businessman William Marsh Rice, who made his fortune in real estate, railroad development and cotton trading in the state of Texas. In 1891, Rice decided to charter a free-tuition educational institute in Houston, bearing his name, to be created upon his death, earmarking most of his estate towards funding the project. Rice's will specified the institution was to be "a competitive institution of the highest grade" and that only white students would be permitted to attend. On the morning of September 23, 1900, Rice, age 84, was found dead by his valet, Charles F. Jones, and presumed to have died in his sleep. Shortly thereafter, a suspiciously large check made out to Rice's New York City lawyer, signed by the late Rice, was noticed by a bank teller due to a misspelling in the recipient's name. The lawyer, Albert T. Patrick, then announced that Rice had changed his will to leave the bulk of his fortune to Patrick, rather than to the creation of Rice's educational institute. A subsequent investigation led by the District Attorney of New York resulted in the arrests of Patrick and of Rice's butler and valet Charles F. Jones, who had been persuaded to administer chloroform to Rice while he slept. Rice's friend and personal lawyer in Houston, Captain James A. Baker, aided in the discovery of what turned out to be a fake will with a forged signature. Jones was not prosecuted since he cooperated with the district attorney, and testified against Patrick. Patrick was found guilty of conspiring to steal Rice's fortune and convicted of murder in 1901, although he was pardoned in 1912 due to conflicting medical testimony. Baker helped Rice's estate direct the fortune, worth $4.6 million in 1904 ($ million today), towards the founding of what was to be called the Rice Institute, later to become Rice University. The board took control of the assets on April 29 of that year.
In 1907, the Board of Trustees selected the head of the Department of Mathematics and Astronomy at Princeton University, Edgar Odell Lovett, to head the Institute, which was still in the planning stages. He came recommended by Princeton's president, Woodrow Wilson. In 1908, Lovett accepted the challenge, and was formally inaugurated as the Institute's first president on October 12, 1912. Lovett undertook extensive research before formalizing plans for the new Institute, including visits to 78 institutions of higher learning across the world on a long tour between 1908 and 1909. Lovett was impressed by such things as the aesthetic beauty of the uniformity of the architecture at the University of Pennsylvania, a theme which was adopted by the Institute, as well as the residential college system at Cambridge University in England, which was added to the Institute several decades later. Lovett called for the establishment of a university "of the highest grade," "an institution of liberal and technical learning" devoted "quite as much to investigation as to instruction." [We must] "keep the standards up and the numbers down," declared Lovett. "The most distinguished teachers must take their part in undergraduate teaching, and their spirit should dominate it all." 

In 1911, the cornerstone was laid for the Institute's first building, the Administration Building, now known as Lovett Hall in honor of the founding president. On September 23, 1912, the 12th anniversary of William Marsh Rice's murder, the "William Marsh Rice Institute for the Advancement of Letters, Science, and Art" began course work with 59 enrolled students, who were known as the "59 immortals," and about a dozen faculty. After 18 additional students joined later, Rice's initial class numbered 77, 48 male and 29 female. Unusual for the time, Rice accepted coeducational admissions from its beginning, but on-campus housing would not become co-ed until 1957. Three weeks after opening, a spectacular international academic festival was held, bringing Rice to the attention of the entire academic world.

Per William Marsh Rice's will and Rice Institute's initial charter, the students paid no tuition. Classes were difficult, however, and about half of Rice's students had failed after the first 1912 term. At its first commencement ceremony, held on June 12, 1916, Rice awarded 35 bachelor's degrees and one master's degree. That year, the student body also voted to adopt the Honor System, which still exists today. Rice's first doctorate was conferred in 1918 on mathematician Hubert Evelyn Bray.

The Founder's Memorial Statue, a bronze statue of a seated William Marsh Rice, holding the original plans for the campus, was dedicated in 1930, and installed in the central academic quad, facing Lovett Hall. The statue was crafted by John Angel.

During World War II, Rice Institute was one of 131 colleges and universities nationally that took part in the V-12 Navy College Training Program, which offered students a path to a Navy commission.

The residential college system proposed by President Lovett was adopted in 1958, with the East Hall residence becoming Baker College, South Hall residence becoming Will Rice College, West Hall becoming Hanszen College, and the temporary Wiess Hall becoming Wiess College.

In 1959, the Rice Institute Computer went online. 1960 saw Rice Institute formally renamed William Marsh Rice University. Rice acted as a temporary intermediary in the transfer of land between Humble Oil and Refining Company and NASA, for the creation of NASA's Manned Spacecraft Center (now called Johnson Space Center) in 1962. President John F. Kennedy then made a speech at Rice Stadium reiterating that the United States intended to reach the moon before the end of the decade of the 1960s, and "to become the world's leading space-faring nation". The relationship of NASA with Rice University and the city of Houston has remained strong .

The original charter of Rice Institute dictated that the university admit and educate, tuition-free, "the white inhabitants of Houston, and the state of Texas". In 1963, the governing board of Rice University filed a lawsuit to allow the university to modify its charter to admit students of all races and to charge tuition. Ph.D. student Raymond Johnson became the first black Rice student when he was admitted that year. In 1964, Rice officially amended the university charter to desegregate its graduate and undergraduate divisions. The Trustees of Rice University prevailed in a lawsuit to void the racial language in the trust in 1966. Rice began charging tuition for the first time in 1965. In the same year, Rice launched a $33 million ($ million) development campaign. $43 million ($ million) was raised by its conclusion in 1970. In 1974, two new schools were founded at Rice, the Jesse H. Jones Graduate School of Management and the Shepherd School of Music. The Brown Foundation Challenge, a fund-raising program designed to encourage annual gifts, was launched in 1976 and ended in 1996 having raised $185 million ($ million). The Rice School of Social Sciences was founded in 1979.

On-campus housing was exclusively for men for the first forty years, until 1957. Jones College was the first women's residence on the Rice campus, followed by Brown College. According to legend, the women's colleges were purposefully situated at the opposite end of campus from the existing men's colleges as a way of preserving campus propriety, which was greatly valued by Edgar Odell Lovett, who did not even allow benches to be installed on campus, fearing that they "might lead to co-fraternization of the sexes". The path linking the north colleges to the center of campus was given the tongue-in-cheek name of "Virgin's Walk". Individual colleges became coeducational between 1973 and 1987, with the single-sex floors of colleges that had them becoming co-ed by 2006. By then, several new residential colleges had been built on campus to handle the university's growth, including Lovett College, Sid Richardson College, and Martel College.

The Economic Summit of Industrialized Nations was held at Rice in 1990. Three years later, in 1993, the James A. Baker III Institute for Public Policy was created. In 1997, the Edythe Bates Old Grand Organ and Recital Hall and the Center for Nanoscale Science and Technology, renamed in 2005 for the late Nobel Prize winner and Rice professor Richard E. Smalley, were dedicated at Rice. In 1999, the Center for Biological and Environmental Nanotechnology was created. The Rice Owls baseball team was ranked #1 in the nation for the first time in that year (1999), holding the top spot for eight weeks.

In 2003, the Owls won their first national championship in baseball, which was the first for the university in any team sport, beating Southwest Missouri State in the opening game and then the University of Texas and Stanford University twice each en route to the title. In 2008, President David Leebron issued a ten-point plan titled "Vision for the Second Century" outlining plans to increase research funding, strengthen existing programs, and increase collaboration. The plan has brought about another wave of campus constructions, including the erection the newly renamed BioScience Research Collaborative building (intended to foster collaboration with the adjacent Texas Medical Center), a new recreational center and the renovated Autry Court basketball stadium, and the addition of two new residential colleges, Duncan College and McMurtry College.

Beginning in late 2008, the university considered a merger with Baylor College of Medicine, though the merger was ultimately rejected in 2010. Select Rice undergraduates are currently guaranteed admission to Baylor College of Medicine upon graduation as part of the Rice/Baylor Medical Scholars program. According to History Professor John Boles' recent book "University Builder: Edgar Odell Lovett and the Founding of the Rice Institute," the first president's original vision for the university included hopes for future medical and law schools.

In 2018, the university added an online MBA program, MBA@Rice.

Rice's campus is a heavily wooded tract of land in the museum district of Houston, located close to the city of West University Place.

Five streets demarcate the campus: Greenbriar Street, Rice Boulevard, Sunset Boulevard, Main Street, and University Boulevard. For most of its history, all of Rice's buildings have been contained within this "outer loop". In recent years, new facilities have been built close to campus, but the bulk of administrative, academic, and residential buildings are still located within the original pentagonal plot of land. The new Collaborative Research Center, all graduate student housing, the Greenbriar building, and the Wiess President's House are located off-campus.

Rice prides itself on the amount of green space available on campus; there are only about 50 buildings spread between the main entrance at its easternmost corner, and the parking lots and Rice Stadium at the West end. The Lynn R. Lowrey Arboretum, consisting of more than 4000 trees and shrubs (giving birth to the legend that Rice has a tree for every student), is spread throughout the campus.

The university's first president, Edgar Odell Lovett, intended for the campus to have a uniform architecture style to improve its aesthetic appeal. To that end, nearly every building on campus is noticeably Byzantine in style, with sand and pink-colored bricks, large archways and columns being a common theme among many campus buildings. Noteworthy exceptions include the glass-walled Brochstein Pavilion, Lovett College with its Brutalist-style concrete gratings, and the eclectic-Mediterranean Duncan Hall. In September 2011, Travel+Leisure listed Rice's campus as one of the most beautiful in the United States.

Lovett Hall, named for Rice's first president, is the university's most iconic campus building. Through its Sallyport arch, new students symbolically enter the university during matriculation and depart as graduates at commencement. Duncan Hall, Rice's computational engineering building, was designed to encourage collaboration between the four different departments situated there. The building's foyer, drawn from many world cultures, was designed by the architect to symbolically express this collaborative purpose.

The campus is organized in a number of quadrangles. The Academic Quad, anchored by a statue of founder William Marsh Rice, includes Ralph Adams Cram's masterpiece, the asymmetrical Lovett Hall, the original administrative building; Fondren Library; Herzstein Hall, the original physics building and home to the largest amphitheater on campus; Sewall Hall for the social sciences and arts; Rayzor Hall for the languages; and Anderson Hall of the Architecture department. The Humanities Building, winner of several architectural awards, is immediately adjacent to the main quad. Further west lies a quad surrounded by McNair Hall of the Jones Business School, the Baker Institute, and Alice Pratt Brown Hall of the Shepherd School of Music. These two quads are surrounded by the university's main access road, a one-way loop referred to as the "inner loop". In the Engineering Quad, a trinity of sculptures by Michael Heizer, collectively entitled "45 Degrees, 90 Degrees, 180 Degrees", are flanked by Abercrombie Laboratory, the Cox Building, and the Mechanical Laboratory, housing the Electrical, Mechanical, and Earth Science/Civil Engineering departments, respectively. Duncan Hall is the latest addition to this quad, providing new offices for the Computer Science, Computational and Applied Math, Electrical and Computer Engineering, and Statistics departments.
Roughly three-quarters of Rice's undergraduate population lives on campus. Housing is divided among eleven residential colleges, which form an integral part of student life at the university (see Residential colleges of Rice University). The colleges are named for university historical figures and benefactors, and while there is wide variation in their appearance, facilities, and dates of founding, are an important source of identity for Rice students, functioning as dining halls, residence halls, sports teams, among other roles. Rice does not have or endorse a Greek system, with the residential college system taking its place. Five colleges, McMurtry, Duncan, Martel, Jones, and Brown are located on the north side of campus, across from the "South Colleges", Baker, Will Rice, Lovett, Hanszen, Sid Richardson, and Wiess, on the other side of the Academic Quadrangle. Of the eleven colleges, Baker is the oldest, originally built in 1912, and the twin Duncan and McMurtry colleges are the newest, and opened for the first time for the 2009-10 school year. Will Rice, Baker, and Lovett colleges are undergoing renovation to expand their dining facilities as well as the number of rooms available for students.

The on-campus football facility, Rice Stadium, opened in 1950 with a capacity of 70,000 seats. After improvements in 2006, the stadium is currently configured to seat 47,000 for football but can readily be reconfigured to its original capacity of 70,000, more than the total number of Rice alumni, living and deceased. The stadium was the site of Super Bowl VIII and a speech by John F. Kennedy on September 12, 1962 in which he challenged the nation to send a man to the moon by the end of the decade. The recently renovated Tudor Fieldhouse, formerly known as Autry Court, is home to the basketball and volleyball teams. Other stadia include the Rice Track/Soccer Stadium and the Jake Hess Tennis Stadium. A new Rec Center now houses the intramural sports offices and provide an outdoor pool, training and exercise facilities for all Rice students, while athletics training will solely be held at Tudor Fieldhouse and the Rice Football Stadium.

The university and Houston Independent School District jointly established The Rice School, a kindergarten through 8th grade public magnet school in Houston. The school opened in August 1994. Through Cy-Fair ISD Rice University offers a credit course based summer school for grades 8 through 12. They also have skills based classes during the summer in the Rice Summer School.

Rice University is chartered as a non-profit organization and is governed by a privately appointed board of trustees. The board consists of a maximum of 25 voting members who serve four-year terms. The trustees serve without compensation and a simple majority of trustees must reside in Texas, including at least four within the greater Houston area. The board of trustees delegates its power by appointing a President to serve as the chief executive of the university. David W. Leebron was appointed President in 2004 and succeeded Malcolm Gillis who served since 1993. The provost, six vice presidents, and other university officials report to the President. The President is advised by a University Council composed of the Provost, eight members of the Faculty Council, two staff members, one graduate student, and two undergraduate students. The President presides over a Faculty Council which has the authority to alter curricular requirements, establish new degree programs, and approve candidates for degrees.

Rice's undergraduate students benefit from a centralized admissions process, which admits new students to the university as a whole, rather than a specific school (the schools of Music and Architecture are decentralized). Students are encouraged to select the major path that best suits their desires; a student can later decide that they would rather pursue study in another field, or continue their current coursework and add a second or third major. These transitions are designed to be simple at Rice, with students not required to decide on a specific major until their sophomore year of study.

Rice's academics are organized into six schools which offer courses of study at the graduate and undergraduate level, with two more being primarily focused on graduate education, while offering select opportunities for undergraduate students. Rice offers 360 degrees in over 60 departments. There are 40 undergraduate degree programs, 51 masters programs, and 29 doctoral programs.

Undergraduate tuition for the 2011-2012 school year was $34,900. $651 was charged for fees, and Rice projected an $800 budget for books and $1550 for personal expenses. Rice students were charged $12,270 for room and board. Per year, the total cost of a Rice University education was $50,171.

Faculty members of each of the departments elect chairs to represent the department to each School's dean and the deans report to the Provost who serves as the chief officer for academic affairs.

Rice is a medium-sized, highly residential research university. The majority of enrollments are in the full-time, four-year undergraduate program emphasizing arts & sciences and professions. There is a high graduate coexistence with the comprehensive graduate program and a very high level of research activity. It is accredited by the Southern Association of Colleges and Schools as well as the professional accreditation agencies for engineering, management, and architecture.

Each of Rice's departments is organized into one of three distribution groups, and students whose major lies within the scope of one group must take at least 3 courses of at least 3 credit hours each of approved distribution classes in each of the other two groups, as well as completing one physical education course as part of the LPAP (Lifetime Physical Activity Program) requirement. All new students must take a Freshman Writing Intensive Seminar (FWIS) class, and for students who do not pass the university's writing composition examination (administered during the summer before matriculation), FWIS 100, a writing class, becomes an additional requirement.

The majority of Rice's undergraduate degree programs grant B.S. or B.A. degrees. Rice has recently begun to offer minors in areas such as business, energy and water sustainability, and global health.

As of fall 2014, men make up 52% of the undergraduate body and 64% of the professional and post-graduate student body. The student body consists of students from all 50 states, including the District of Columbia, two U.S. Territories, and 83 foreign countries. Forty percent of degree-seeking students are from Texas.

Students enrolled at Rice University in full-time Undergraduate programs are majority White Male (20.5%), followed by White Female (15.8%) and Asian Female (12.5%). Students enrolled in full-time Graduate programs are majority White Male (26.2%), followed by White Female (11.7%) and Asian Male (4.98%).

The Rice Honor Code plays an integral role in academic affairs. Almost all Rice exams are unproctored and professors give timed, closed-book exams that students take home and complete at their own convenience. Potential infractions are reported to the student Honor Council, elected by popular vote. The penalty structure is established every year by Council consensus; typically, penalties have ranged from a letter of reprimand to an 'F' in the course and a two semester suspension. During Orientation Week, students must take and pass a test demonstrating that they understand the Honor System's requirements and sign a Matriculation Pledge. On assignments, Rice students affirm their commitment to the Honor Code by writing "On my honor, I have neither given nor received any unauthorized aid on this (examination, quiz or paper)."

Rice is noted for its pioneer applied science programs in the fields of nanotechnology, artificial heart research, structural chemical analysis, and space science, being ranked 1st in the world in materials science research by the Times Higher Education (THE) in 2010.

Admission to Rice is rated as "most selective" by "U.S. News & World Report".

After the Rice Investment financial aid initiative was announced in September 2018, Rice received a record-breaking number of applications for the matriculating class of 2019. Rice received 27,068 applications - 30% more than in 2018 - of which 2,364 students were admitted. This puts Rice's admission rate at 8.7% and under 10% for the first time in the university's history. For fall 2019, Rice received 27,087 freshmen applications; 2,361 were admitted (8.7%) and 964 enrolled. The middle 50% range of SAT scores were 1470-1560; the middle 50% range of the ACT Composite score was 33-35.

Rice was ranked tied at 17th among national universities and 107th among global universities, tied at 8th for "best undergraduate teaching", 13th for "Best Value", and tied for 27th "Most Innovative" among national universities in the U.S. by "U.S. News & World Report" in its 2020 edition. "Forbes" magazine ranked Rice University 21st nationally among 650 liberal arts colleges, universities and service academies in 2019, 19th among research universities and 2nd in the South. "Kiplinger's Personal Finance" places Rice 7th in its 2019 ranking of best value private universities in the United States.

In 2017, Rice was ranked 86th in the world by the "Times Higher Education World University Rankings". In 2016, Rice was ranked tied for 72nd internationally (38th nationally) by the "Academic Ranking of World Universities". Rice University was also ranked 90th globally in 2016 by "QS World University Rankings". Rice is noted for its entrepreneurial activity, and has been recognized as the top ranked business incubator in the world by the Stockholm-based UBI Index for both 2013 and 2014.

The "Princeton Review" ranked Rice 1st for "Best Quality of Life" and "Happiest Students" in its 2012 edition, 20th among the most LGBT friendliest colleges in its 2014 edition, and one of the top 50 best value private colleges in its 2011 edition. Rice was ranked 41st among research universities by the Center for Measuring University Performance in 2007. "Consumer's Digest" ranked Rice 3rd on the list of top 5 values in private colleges in its June 2011 issue. "Fiske Guide to Colleges" ranked Rice as one of the top 25 private "best buy" schools in its 2012 edition.
In 2011 the Leiden Ranking, which measures the performance of 500 major research universities worldwide, using metrics designed to measure research impact ranked Rice 4th Globally, for effectiveness and contribution of research. In 2013 the university was again ranked first globally for quality of research in natural sciences and engineering, and 6th globally for all sciences. In 2014, "The Daily Beast" ranked Rice 14th out of nearly 2,000 schools it evaluated. In 2016, "Money Magazine" ranked Rice 4th in the nation.

Situated on nearly in the center of Houston's Museum District and across the street from the city's Hermann Park, Rice is a green and leafy refuge; an oasis of learning convenient to the amenities of the nation's fourth-largest city. Rice's campus adjoins Hermann Park, the Texas Medical Center, and a neighborhood commercial center called Rice Village. Hermann Park includes the Houston Museum of Natural Science, the Houston Zoo, Miller Outdoor Theatre and an 18-hole municipal golf course. NRG Park, home of NRG Stadium and the Astrodome, is two miles (3 km) south of the campus. Among the dozen or so museums in the Museum District is the Rice University Art Gallery, open during the school year. Easy access to downtown's theater and nightlife district and to Reliant Park is provided by the Houston METRORail system, with a station adjacent to the campus's main gate. The campus recently joined the Zipcar program with two vehicles to increase the transportation options for students and staff who need but currently don't utilize a vehicle.

In 1957, Rice University implemented a residential college system, which was proposed by the university's first president, Edgar Odell Lovett. The system was inspired by existing systems in place at Oxford and Cambridge in England and at several other universities in the United States, most notably Yale University. The existing residences known as East, South, West, and Wiess Halls became Baker, Will Rice, Hanszen, and Wiess Colleges, respectively.

Below is a list of residential colleges in order of founding:

Although each college is composed of a full cross-section of students at Rice, they have over time developed their own traditions and "personalities". When students matriculate they are randomly assigned to one of the eleven colleges, although "legacy" exceptions are made for students whose siblings or parents have attended Rice 
. Students generally remain members of the college that they are assigned to for the duration of their undergraduate careers, even if they move off-campus at any point. Students are guaranteed on-campus housing for freshman year and two of the next three years; each college has its own system for determining allocation of the remaining spaces, collectively known as "Room Jacking". Students develop strong loyalties to their college and maintain friendly rivalry with other colleges, especially during events such as Beer Bike Race and O-Week. Colleges keep their rivalries alive by performing "jacks," or pranks, on each other, especially during O-Week and Willy Week. During Matriculation, Commencement, and other formal academic ceremonies, the colleges process in the order in which they were established.

The Baker 13 is a tradition in which students run around campus wearing nothing but shoes and shaving cream at 10 p.m. on the 13th and the 31st of every month, as well as the 26th on months with fewer than 31 days. The event, long sponsored by Baker College, usually attracts a small number of students, but Halloween night and the first and last relevant days of the school year both attract large numbers of revelers.
According to the official website, "Beer Bike is a combination intramural bicycle race and drinking competition dating back to 1957. Ten riders and ten chuggers make up a team. Elaborate rules include details such as a prohibition of "bulky or wet clothing articles designed to absorb beer/water or prevent spilled beer/water from being seen" and regulations for chug can design. Each residential college as well as the Graduate Student Association participates with a men's team, a women's team, and alumni (co-ed) team. Each leg of the race is a relay in which a team's "chugger" must chug of beer or water for the men's division and for women before the team's "rider" may begin to ride. Participants who both ride and chug are referred to as "Ironmen". Willy Week is a term coined in the 1990s to refer to the week preceding Beer-Bike, a time of general energy and excitement on campus. Jacks (pranks) are especially common during Willy Week; some examples in the past include removing showerheads and encasing the Hanszen guardian."
The morning of the Beer Bike race itself begins with what is by some estimations the largest annual water balloon fight in the world. Beer-Bike is Rice's most prominent student event, and for younger alumni it serves as an unofficial reunion weekend on par with Homecoming. The 2009 Beer Bike race was dedicated to the memory of Dr. Bill Wilson, a popular professor and long-time resident associate of Wiess College who died earlier that year.

In the event of inclement weather, Beer Bike becomes a Beer Run. The rules are nearly identical, except that the Bikers must instead run the length of the track.

A number of on-campus institutions form an integral part of student life at Rice. Many of these organizations have been operating for several decades.

Rice Coffeehouse finds its beginnings in Hanszen College, where students would serve coffee in the Weenie Loft, a study room in the old section's fourth floor. Later, the coffee house moved to the Hanszen basement to accommodate more student patrons. That coffeehouse became known as Breadsticks and Pomegranates. Due to flooding, an unfortunate effect of 1) its location in the basement and 2) the Houston climate, this coffee house closed. Demand for an on-campus Coffeehouse grew and in 1990, the Rice Coffeehouse was founded.

The Rice Coffeehouse is a not-for-profit student-run organization serving Rice University and the greater Houston community. Over the past few years, it has introduced fair-trade and organic coffee and loose-leaf teas.

Coffeehouse baristas are referred to as K.O.C.'s, or Keepers of the Coffee. Rice Coffeehouse has also adopted an unofficial mascot, the squirrel, which can be found on T-shirts, mugs, and bumper stickers stuck on laptops across campus. The logo pays tribute to Rice's unusually plump and frighteningly tame squirrel population.

Willy's Pub is Rice's undergraduate pub run by students located in the basement of the Rice Memorial Center. It opened on April 11, 1975, with Rice President Norman Hackerman pouring the first beer. The name was chosen by students in tribute to the university's founder, William Marsh Rice. After the drinking age in Texas was raised in 1986, the pub entered a period of financial difficulties and in April 1995, was destroyed in a fire. The space was gutted but renovated and remains open.

Rice Bikes is a full-service on-campus bicycle sale, rental, and repair shop. It originated in the basement of Sid Richardson College in February 2011. In 2012, Rice Bikes officially became the university's third student-run business. Rice Bikes merged with a student-run bicycle rental business in 2013, and operations moved to the Rice Memorial Center in 2014. In 2017, the business moved to the garage of the Rice Housing and Dining department's headquarters.

Rice Bikes sells refurbished bicycles bought from students and functions as a full bicycle repair shop.

Rice has a weekly student newspaper ("The Rice Thresher"), a yearbook (The Campanile), college radio station (KTRU Rice Radio), and now defunct, campus-wide student television station (RTV5). They are based out of the RMC student center. In addition, Rice hosts several student magazines dedicated to a range of different topics; in fact, the spring semester of 2008 saw the birth of two such magazines, a literary sex journal called "Open" and an undergraduate science research magazine entitled "Catalyst".
"The Rice Thresher" is published every Wednesday and is ranked by Princeton Review as one of the top campus newspapers nationally for student readership. It is distributed around campus, and at a few other local businesses and has a website. The "Thresher" has a small, dedicated staff and is known for its coverage of campus news, open submission opinion page, and the satirical Backpage, which has often been the center of controversy. The newspaper has won several awards from the College Media Association, Associated Collegiate Press and Texas Intercollegiate Press Association.

The Rice Campanile was first published in 1916 celebrating Rice's first graduating class. It has published continuously since then, publishing two volumes in 1944 since the university had two graduating classes due to World War II. The website was created sometime in the early to mid 2000s. The 2015 won the first place Pinnacle for best yearbook from College Media Association.

KTRU Rice Radio is the student-run radio station. Though most DJs are Rice students, anyone is allowed to apply. It is known for playing genres and artists of music and sound unavailable on other radio stations in Houston, and often, the US. The station takes requests over the phone or online. In 2000 and 2006, KTRU won Houston Press' Best Radio Station in Houston. In 2003, Rice alum and active KTRU DJ DL's hip-hip show won Houston Press' Best Hip-hop Radio Show. On August 17, 2010, it was announced that Rice University had been in negotiations to sell the station's broadcast tower, FM frequency and license to the University of Houston System to become a full-time classical music and fine arts programming station. The new station, KUHA, would be operated as a not-for-profit outlet with listener supporters. The FCC approved the sale and granted the transfer of license to the University of Houston System on April 15, 2011, however, KUHA proved to be an even larger failure and so after four and a half years of operation, The University of Houston System announced that KUHA's broadcast tower, FM frequency and license were once again up for sale in August 2015. KTRU continued to operate much as it did previously, streaming live on the Internet, via apps, and on HD2 radio using the 90.1 signal. Under student leadership, KTRU explored the possibility of returning to FM radio for a number of years. In spring 2015, KTRU was granted permission by the FCC to begin development of a new broadcast signal via LPFM radio. On October 1, 2015, KTRU made its official return to FM radio on the 96.1 signal. While broadcasting on HD2 radio has been discontinued, KTRU continues to broadcast via internet in addition to its LPFM signal.

RTV5 is a student-run television network available as channel 5 on campus. RTV5 was created initially as Rice Broadcast Television in 1997; RBT began to broadcast the following year in 1998, and aired its first live show across campus in 1999. It experienced much growth and exposure over the years with successful programs like "Drinking with Phil," a weekly news show, and extensive live coverage in December 2000 of the shut down of KTRU by the administration. In spring 2001, the Rice undergraduate community voted in the general elections to support RBT as a blanket tax organization, effectively providing a yearly income of $10,000 to purchase new equipment and provide the campus with a variety of new programming. In the spring of 2005, RBT members decided the station needed a new image and a new name: Rice Television 5. One of RTV5's most popular shows was the 24-hour show, where a camera and couch placed in the RMC stayed on air for 24 hours. One such show is held in fall and another in spring, usually during a weekend allocated for visits by prospective students. RTV5 has a video on demand site at rtv5.rice.edu. The station went off the air in 2014 and changed its name to Rice Video Productions. In 2015 the group's funding was threatened, but ultimately maintained. In 2016 the small student staff requested to no longer be a blanket-tax organization. In the fall of 2017, the club did not register as a club.

"The Rice Review", also known as R2, is a yearly student-run literary journal at Rice University that publishes prose, poetry, and creative nonfiction written by undergraduate students, as well as interviews. The journal was founded in 2004 by creative writing professor and author Justin Cronin.

"The Rice Standard" was an independent, student-run variety magazine modeled after such publications as "The New Yorker" and "Harper's". Prior to fall 2009, it was regularly published three times a semester with a wide array of content, running from analyses of current events and philosophical pieces to personal essays, short fiction and poetry. In August 2009, the "Standard" transitioned to a completely online format with the launch of their redesigned website, ricestandard.org. The first website of its kind on Rice's campus, the "Standard" featured blog-style content written by and for Rice students. "The Rice Standard" had around 20 regular contributors, and the site features new content every day (including holidays). In 2017 no one registered The Rice Standard as a club within the university.

"Open", a magazine dedicated to "literary sex content," predictably caused a stir on campus with its initial publication in spring 2008. A mixture of essays, editorials, stories and artistic photography brought Open attention both on campus and in the Houston Chronicle. The third and last annual edition of "Open" was released in spring of 2010.

Vahalla is the Graduate Student Association on-campus bar under the steps of the chemistry building.

Rice plays in NCAA Division I athletics and is part of Conference USA. Rice was a member of the Western Athletic Conference before joining Conference USA in 2005. Rice is the second-smallest school, measured by undergraduate enrollment, competing in NCAA Division I FBS football, only ahead of Tulsa.

The Rice baseball team won the 2003 College World Series, defeating Stanford, giving Rice its only national championship in a team sport. The victory made Rice University the smallest school in 51 years to win a national championship at the highest collegiate level of the sport. The Rice baseball team has played on campus at Reckling Park since the 2000 season. As of 2010, the baseball team has won 14 consecutive conference championships in three different conferences: the final championship of the defunct Southwest Conference, all nine championships while a member of the Western Athletic Conference, and five more championships in its first five years as a member of Conference USA. Additionally, Rice's baseball team has finished third in both the 2006 and 2007 College World Series tournaments. Rice now has made six trips to Omaha for the CWS. In 2004, Rice became the first school ever to have three players selected in the first eight picks of the MLB draft when Philip Humber, Jeff Niemann, and Wade Townsend were selected third, fourth, and eighth, respectively. In 2007, Joe Savery was selected as the 19th overall pick.
Rice has been very successful in women's sports in recent years. In 2004-05, Rice sent its women's volleyball, soccer, and basketball teams to their respective NCAA tournaments. The women's swim team has consistently brought at least one member of their team to the NCAA championships since 2013. In 2005-06, the women's soccer, basketball, and tennis teams advanced, with five individuals competing in track and field. In 2006-07, the Rice women's basketball team made the NCAA tournament, while again five Rice track and field athletes received individual NCAA berths. In 2008, the women's volleyball team again made the NCAA tournament. 
In 2011 the Women's Swim team won their first conference championship in the history of the university. This was an impressive feat considering they won without having a diving team. The team repeated their C-USA success in 2013 and 2014. 
In 2017, the women's basketball team, led by second-year head coach Tina Langley, won the Women's Basketball Invitational, defeating UNC-Greensboro 74-62 in the championship game at Tudor Fieldhouse. 
Though not a varsity sport, Rice's ultimate frisbee women's team, named Torque, won consecutive Division III national championships in 2014 and 2015.

In 2006, the football team qualified for its first bowl game since 1961, ending the second-longest bowl drought in the country at the time. On December 22, 2006, Rice played in the New Orleans Bowl in New Orleans, Louisiana against the Sun Belt Conference champion, Troy. The Owls lost 41-17. The bowl appearance came after Rice had a 14-game losing streak from 2004–05 and went 1-10 in 2005. The streak followed an internally authorized 2003 McKinsey report that stated football alone was responsible for a $4 million deficit in 2002. Tensions remained high between the athletic department and faculty, as a few professors who chose to voice their opinion were in favor of abandoning the football program. The program success in 2006, the "Rice Renaissance," proved to be a revival of the Owl football program, quelling those tensions. David Bailiff took over the program in 2007 and has remained head coach. Jarett Dillard set an NCAA record in 2006 by catching a touchdown pass in 13 consecutive games and took a 15-game overall streak into the 2007 season.
In 2008, the football team posted a 9-3 regular season, capping off the year with a 38-14 victory over Western Michigan University in the Texas Bowl. The win over Western Michigan marked the Owls' first bowl win in 45 years.

Rice Stadium also serves as the performance venue for the university's Marching Owl Band, or "MOB." Despite its name, the MOB is a scatter band that focuses on performing humorous skits and routines rather than traditional formation marching.

Rice Owls men's basketball won 10 conference titles in the former Southwest Conference (1918, 1935*, 1940, 1942*, 1943*, 1944*, 1945, 1949*, 1954*, 1970; * denotes shared title). Most recently, guard Morris Almond was drafted in the first round of the 2007 NBA Draft by the Utah Jazz. Rice named former Cal Bears head coach Ben Braun as head basketball coach to succeed Willis Wilson, fired after Rice finished the 2007-2008 season with a winless (0-16) conference record and overall record of 3-27.

Rice's mascot is Sammy the Owl. In previous decades, the university kept several live owls on campus in front of Lovett College, but this practice has been discontinued, due to public pressure over the welfare of the owls.

Rice also has a 12-member coed cheerleading squad and a coed dance team, both of which perform at football and basketball games throughout the year.

As of 2011, Rice has graduated 98 classes of students consisting of 51,961 living alumni. Over 100 students at Rice have been Fulbright Scholars, 25 Marshall Scholars, 25 Mellon Fellows, 12 Rhodes Scholars, 6 Udall Scholars, and 65 Watson Fellows, among several other honors and awards.

Rice's distinguished faculty and alumni consists of three Nobel laureates, two Pulitzer Prize award winners, six Fulbright Scholars, 29 Alexander von Humboldt Foundation Recipients, eight members of the American Academy of Arts and Sciences, one member of the American Philosophical Society, 35 Guggenheim Fellowships, 17 members of the National Academy of Engineering, seven members of the National Academy of Sciences, five fellows of the National Humanities Center, and 86 fellows of the National Science Foundation.

Alumni of Rice have occupied top positions in business, including Thomas H. Cruikshank, the former CEO of Halliburton; John Doerr, billionaire and venture capitalist; Howard Hughes; and Fred C. Koch.

In government and politics, Rice alumni include Alberto Gonzales, former Attorney General; Charles Duncan, former Secretary of Energy; William P. Hobby, Jr.; John Kline; George P. Bush; Josh Earnest, White House Press Secretary for President Obama; Ben Rhodes, Deputy National Security Advisor for President Obama and Annise Parker, the 61st Mayor of Houston.

Rice alumni who became prominent writers include Larry McMurtry, Pulitzer Prize–winning author and Oscar-winning writer of the screenplay for "Brokeback Mountain"; Joyce Carol Oates, who was once a doctoral candidate in English; John Graves, author of "Goodbye to a River"; and Candace Bushnell, author of "Sex and the City", who attended for three semesters.

Notable entrepreneurs who graduated from Rice include Tim and Karrie League, founders of the Alamo Drafthouse Cinema and Drafthouse Films; and Brock Wagner and Kevin Bartol, founders of Saint Arnold Brewing Company.

In science and technology, Rice alumni include 14 NASA astronauts; Robert Curl, Nobel Prize–winning discoverer of fullerene; Robert Woodrow Wilson, winner of the Nobel Prize in Physics for the discovery of cosmic microwave background radiation; David Eagleman, celebrity neuroscientist and "NYT" bestselling author; and NASA former Apollo 11 and 13 warning systems engineer and motivational speaker Jerry Woodfill. In engineering, Dr Powtawche Valerino, works for the NASA Jet Propulsion Laboratory and worked on the Cassini mission.

Rice athletes include Lance Berkman, Brock Holt, Bubba Crosby, Harold Solomon, Frank Ryan, Tommy Kramer, Jose Cruz, Jr., O.J. Brigance, Larry Izzo, James Casey, Courtney Hall, Bert Emanuel, Luke Willson, Tony Cingrani, Anthony Rendon, and Leo Rucka, as well as three Olympians (Funmi Jimoh '06, Allison Beckford '04, and William Fred Hansen '63).



</doc>
<doc id="25814" url="https://en.wikipedia.org/wiki?curid=25814" title="Richard Smalley">
Richard Smalley

Richard Errett Smalley (June 6, 1943 – October 28, 2005) was the Gene and Norman Hackerman Professor of Chemistry and a Professor of Physics and Astronomy at Rice University, in Houston, Texas. In 1996, along with Robert Curl, also a professor of chemistry at Rice, and Harold Kroto, a professor at the University of Sussex, he was awarded the Nobel Prize in Chemistry for the discovery of a new form of carbon, buckminsterfullerene, also known as buckyballs. He was an advocate of nanotechnology and its applications.

Smalley, the youngest of 4 siblings, was born in Akron, Ohio on June 6, 1943 to Frank Dudley Smalley, Jr., and Esther Virginia Rhoads. He grew up in Kansas City, Missouri. Richard Smalley credits his father, mother and aunt as formative influences in industry, science and chemistry. His father, Frank Dudley Smalley, Jr. worked with mechanical and electrical equipment and eventually became CEO of a trade journal for farm implements called "Implement and Tractor". His mother, Esther Rhoads Smalley, completed her B.A. Degree while Richard was a teenager. She was particularly inspired by mathematician Norman N. Royall Jr., who taught Foundations of Physical Science, and communicated her love of science to her son through long conversations and joint activities. Smalley's mother's sister, pioneering woman chemist Sara Jane Rhoads, interested Smalley in the field of chemistry, letting him work in her organic chemistry laboratory, and suggesting that he attend Hope College, which had a strong chemistry program.

Smalley attended Hope College for two years before transferring to the University of Michigan where he received his Bachelor of Science in 1965. Between his studies, he worked in industry, where he developed his unique managerial style. He received his Doctor of Philosophy (Ph.D.) from Princeton University in 1973 with Prof. E. R. Bernstein. He did postdoctoral work at the University of Chicago from 1973 to 1976, with Donald Levy and Lennard Wharton where he was a pioneer in the development of supersonic beam laser spectroscopy.

In 1976, Smalley joined Rice University. In 1982, he was appointed to the Gene and Norman Hackerman Chair in Chemistry at Rice. He helped to found the Rice Quantum Institute in 1979, serving as Chairman from 1986 to 1996. In 1990, he became also a Professor in the Department of Physics. In 1990, he helped to found the Center for Nanoscale Science and Technology. In 1996, he was appointed its Director.

He became a member of the National Academy of Sciences in 1990, and the American Academy of Arts and Sciences in 1991.

Smalley's research in physical chemistry investigated formation of inorganic and semiconductor clusters using pulsed molecular beams and time-of-flight mass spectrometry. As a consequence of this expertise, Robert Curl introduced him to Harry Kroto in order to investigate a question about the constituents of astronomical dust. These are carbon-rich grains expelled by old stars such as R Coronae Borealis. The result of this collaboration was the discovery of C (Known as Buckyballs) and the fullerenes as the third allotropic form of carbon.

The research that earned Kroto, Smalley and Curl the Nobel Prize mostly comprised three articles. First was the discovery of C in the Nov. 14, 1985, issue of "Nature", "C: Buckminsterfullerene". The second article detailed the discovery of the endohedral fullerenes in "Lanthanum Complexes of Spheroidal Carbon Shells" in the "Journal of the American Chemical Society" (1985). The third announced the discovery of the fullerenes in "Reactivity of Large Carbon Clusters: Spheroidal Carbon Shells and Their Possible Relevance to the Formation and Morphology of Soot" in the "Journal of Physical Chemistry" (1986).

Although only three people can be cited for a Nobel Prize, graduate students James R. Heath, Yuan Liu, and Sean C. O'Brien participated in the work. Smalley mentioned Heath and O'Brien in his Nobel Lecture. Heath went on to become a professor at California Institute of Technology (Caltech) and O'Brien joined Texas Instruments and is now at MEMtronics. Yuan Liu is a Senior Staff Scientist at Oak Ridge National Laboratory.

This research is significant for the discovery of a new allotrope of carbon known as a fullerene. Other allotropes of carbon include graphite, diamond and graphene. Harry Kroto's 1985 paper entitled "C60: Buckminsterfullerine", published with colleagues J. R. Heath, S. C. O'Brien, R. F. Curl, and R. E. Smalley, was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society, presented to Rice University in 2015. The discovery of fullerenes was recognized in 2010 by the designation of a National Historic Chemical Landmark by the American Chemical Society at the Richard E. Smalley Institute for Nanoscale Science and Technology at Rice University in Houston, Texas.

Following nearly a decade's worth of research into the formation of alternate fullerene compounds (e.g. C, C), as well as the synthesis of endohedral metallofullerenes (M@C), reports of the identification of carbon nanotube structures led Smalley to begin investigating their iron-catalyzed synthesis.

As a consequence of these researches, Smalley was able to persuade the administration of Rice University under then-president Malcolm Gillis to create Rice's Center for Nanoscale Science and Technology (CNST) focusing on any aspect of molecular nanotechnology. It was renamed The Richard E. Smalley Institute for Nanoscale Science and Technology after Smalley's death in 2005, and has since merged with the Rice Quantum Institute, becoming the Smalley-Curl Institute (SCI) in 2015.

Smalley's latest research was focused on carbon nanotubes, specifically focusing on the chemical synthesis side of nanotube research. He is well known for his group's invention of the high-pressure carbon monoxide (HiPco) method of producing large batches of high-quality nanotubes. Smalley spun off his work into a company, Carbon Nanotechnologies Inc. and associated nanotechnologies.

He was an outspoken skeptic of the idea of molecular assemblers, as advocated by K. Eric Drexler. His main scientific objections, which he termed the "fat fingers problem" and the "sticky fingers problem", argued against the feasibility of molecular assemblers being able to precisely select and place individual atoms. He also believed that Drexler's speculations about apocalyptic dangers of molecular assemblers threatened the public support for development of nanotechnology. He debated Drexler in an exchange of letters which were published in "Chemical & Engineering News" as a point-counterpoint feature.

Starting in the late 1990s, Smalley advocated for the need for cheap, clean energy, which he described as the number one problem facing humanity in the 21st century. He described what he called "The Terawatt Challenge", the need to develop a new power source capable of increasing "our energy output by a minimum factor of two, the generally agreed-upon number, certainly by the middle of
the century, but preferably well before that."

He also presented a list entitled "Top Ten Problems of Humanity for Next 50 Years". It can be interesting to compare his list, in order of priority, to the Ten Threats formulated by the U.N.'s High Level Threat Panel in 2004. Smalley's list, in order of priority, was:

Smalley regarded several problems as interlinked: the lack of people entering the fields of science and engineering, the need for an alternative to fossil fuels, and the need to address global warming. He felt that improved science education was essential, and strove to encourage young students to consider careers in science. His slogan for this effort was "Be a scientist, save the world."

Smalley was a leading advocate of the National Nanotechnology Initiative in 2003. Suffering from hair loss and weakness as a result of his chemotherapy treatments, Smalley testified before the congressional testimonies, arguing for the potential benefits of nanotechnology in the development of targeted cancer therapies. Bill 189, the 21st Century Nanotechnology Research and Development Act, was introduced in the Senate on January 16, 2003 by Senator Ron Wyden, passed the Senate on November 18, 2003, and at the House of Representatives the next day with a 405–19 vote. President George W. Bush signed the act into law on December 3, 2003, as Public Law 108-
153. Smalley was invited to attend.

Smalley was married four times, to Judith Grace Sampieri (1968-1978), Mary L. Chapieski (1980-1994), JoNell M. Chauvin (1997-1998) and Deborah Sheffield (2005), and had two sons, Chad Richard Smalley (born June 8, 1969) and Preston Reed Smalley (born August 8, 1997).

In 1999, Smalley was diagnosed with cancer. Smalley died of leukemia, variously reported as non-Hodgkin's lymphoma and chronic lymphocytic leukemia, on October 28, 2005, at M.D. Anderson Cancer Center in Houston, Texas, at the age of 62.

Upon Smalley's death, the US Senate passed a resolution to honor Smalley, crediting him as the "Father of Nanotechnology."

Smalley, who had taken classes in religion as well as science at Hope College, rediscovered his Christian foundation in later life, particularly during his final years while battling cancer. During the final year of his life, Smalley wrote: "Although I suspect I will never fully understand, I now think the answer is very simple: it's true. God did create the universe about 13.7 billion years ago, and of necessity has involved Himself with His creation ever since."

At the Tuskegee University's 79th Annual Scholarship Convocation/Parents' Recognition Program he was quoted making the following statement regarding the subject of evolution while urging his audience to take seriously their role as the higher species on this planet. Genesis' was right, and there was a creation, and that Creator is still involved ... We are the only species that can destroy the Earth or take care of it and nurture all that live on this very special planet. I'm urging you to look on these things. For whatever reason, this planet was built specifically for us. Working on this planet is an absolute moral code. ... Let's go out and do what we were put on Earth to do." Old Earth creationist and astronomer Hugh Ross spoke at Smalley's funeral, November 2, 2005.






</doc>
<doc id="25815" url="https://en.wikipedia.org/wiki?curid=25815" title="Robert Curl">
Robert Curl

Robert Floyd Curl Jr. (born August 23, 1933) is a University Professor Emeritus, Pitzer–Schlumberger Professor of Natural Sciences Emeritus, and Professor of Chemistry Emeritus at Rice University. He was awarded the Nobel Prize in Chemistry in 1996 for the discovery of the nanomaterial buckminsterfullerene, along with Richard Smalley (also of Rice University) and Harold Kroto of the University of Sussex.

Born in Alice, Texas, United States, Curl was the son of a Methodist minister. Due to his father's missionary work, his family moved several times within southern and southwestern Texas, and the elder Curl was involved in starting the San Antonio Medical Center's Methodist Hospital. Curl attributes his interest in chemistry to a chemistry set he received as a nine-year-old, recalling that he ruined the finish on his mother's porcelain stove when nitric acid boiled over onto it. He is a graduate of Thomas Jefferson High School in San Antonio, Texas. His high school offered only one year of chemistry instruction, but in his senior year his chemistry teacher gave him special projects to work on.

Curl received a bachelor of science from Rice Institute (now Rice University) in 1954. He was attracted to the reputation of both the school's academics and football team, and the fact that at the time it charged no tuition. He earned his doctorate in chemistry from the University of California, Berkeley, in 1957. At Berkeley, he worked in the laboratory of Kenneth Pitzer, then dean of the College of Chemistry, with whom he would become a lifelong collaborator. Curl's graduate research involved performing infrared spectroscopy to determine the bond angle of disiloxane.

Curl was a postdoctoral fellow at Harvard University with E. B. Wilson, where he used microwave spectroscopy to study the bond rotation barriers of molecules. After that, he joined the faculty of Rice University in 1958. He inherited the equipment and graduate students of George Bird, a professor who was leaving for a job at Polaroid. Curl's early research involved the microwave spectroscopy of chlorine dioxide. His research program included both experiment and theory, mainly focused on detection and analysis of free radicals using microwave spectroscopy and tunable lasers. He used these observations to develop the theory of their fine structure and hyperfine structure, as well as information about their structure and the kinetics of their reactions.

Curl's research at Rice involved the fields of infrared and microwave spectroscopy. Curl's research inspired Richard Smalley to come to Rice in 1976 with the intention of collaborating with Curl. In 1985, Curl was contacted by Harold Kroto, who wanted to use a laser beam apparatus built by Smalley to simulate and study the formation of carbon chains in red giant stars. Smalley and Curl had previously used this apparatus to study semiconductors such as silicon and germanium. They were initially reluctant to interrupt their experiments on these semiconductor materials to use their apparatus for Kroto's experiments on carbon, but eventually gave in.

They indeed found the long carbon chains they were looking for, but also found an unexpected product that had 60 carbon atoms. Over the course of 11 days, the team studied and determined its structure and named it buckminsterfullerene after noting its similarity to the geodesic domes for which the architect Buckminster Fuller was known. This discovery was based solely on the single prominent peak on the mass spectrograph, implying a chemically inert substance that was geometrically closed with no dangling bonds. Curl was responsible for determining the optimal conditions of the carbon vapor in the apparatus, and examining the spectrograph. Curl noted that James R. Heath and Sean C. O'Brien deserve equal recognition in the work to Smalley and Kroto. The existence of this type of molecule had earlier been theorized by others, but Curl and his colleagues were at the time unaware of this. Later experiments confirmed their proposed structure, and the team moved on to synthesize endohedral fullerenes that had a metal atom inside the hollow carbon shell. The fullerenes, a class of molecules of which buckminsterfullerene was the first member discovered, are now considered to have potential applications in nanomaterials and molecular scale electronics. Robert Curl's 1985 paper entitled "C60: Buckminsterfullerine", published with colleagues H. Kroto, J. R. Heath, S. C. O’Brien, and R. E. Smalley, was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society, presented to Rice University in 2015. The discovery of fullerenes was recognized in 2010 by the designation of a National Historic Chemical Landmark by the American Chemical Society at the Richard E. Smalley Institute for Nanoscale Science and Technology at Rice University in Houston, Texas.

After winning the Nobel Prize in 1996, Curl took a quieter path than Smalley, who became an outspoken advocate of nanotechnology, and Kroto, who used his fame to further his interest in science education, saying, "After winning a Nobel, you can either become a scientific pontificator, or you can have some idea for a new science project and you can use your newfound notoriety to get the resources to do it. Or you can say, 'Well, I enjoy what I was doing, and I want to keep doing that.'"

Curl's later research interests involved physical chemistry, developing DNA genotyping and sequencing instrumentation, and creating photoacoustic sensors for trace gases using quantum cascade lasers. He is known in the residential college life at Rice University for being the first master of Lovett College.

Curl retired in 2008 at the age of 74, becoming a University Professor Emeritus, Pitzer-Schlumberger Professor of Natural Sciences Emeritus, and Professor of Chemistry Emeritus at Rice University.

Curl married Jonel Whipple in 1955, with whom he had two children. He plays bridge every week with the Rice Bridge Brigade.


Journal articles:
Technical reports:



</doc>
<doc id="25816" url="https://en.wikipedia.org/wiki?curid=25816" title="Roman Republic">
Roman Republic

The Roman Republic (, ) was the era of classical Roman civilization beginning with the overthrow of the Roman Kingdom, traditionally dated to 509 BC, and ending in 27 BC with the establishment of the Roman Empire. It was during this period that Rome's control expanded from the city's immediate surroundings to hegemony over the entire Mediterranean world.

Roman society under the Republic was a cultural mix of Latin, Etruscan, and Greek elements, which is especially visible in the Roman Pantheon. Its political organisation was strongly influenced by the Greek city states of Magna Graecia, with collective and annual magistracies, overseen by a senate. The top magistrates were the two consuls, who had an extensive range of executive, legislative, judicial, military, and religious powers. Whilst there were elections each year, the Republic was not a democracy, but an oligarchy, as a small number of large families (called "gentes") monopolised the main magistracies. Roman institutions underwent considerable changes throughout the Republic to adapt to the difficulties it faced, such as the creation of promagistracies to rule its conquered provinces, or the composition of the senate.

Unlike the "Pax Romana" of the Roman Empire, the Republic was in a state of quasi-perpetual war throughout its existence. Its first enemies were its Latin and Etruscan neighbours as well as the Gauls, who even sacked the city in 387 BC. The Republic nonetheless demonstrated extreme resilience and always managed to overcome its losses, however catastrophic. After the Gallic Sack, Rome conquered the whole Italian peninsula in a century, which turned the Republic into a major power in the Mediterranean. The Republic's greatest enemy was doubtless Carthage, against which it waged three wars. The Punic general Hannibal famously invaded Italy by crossing the Alps and inflicted on Rome two devastating defeats at Lake Trasimene and Cannae, but the Republic once again recovered and won the war thanks to Scipio Africanus at the Battle of Zama in 202 BC. With Carthage defeated, Rome became the dominant power of the ancient Mediterranean world. It then embarked in a long series of difficult conquests, after having notably defeated Philip V and Perseus of Macedon, Antiochus III of the Seleucid Empire, the Lusitanian Viriathus, the Numidian Jugurtha, the great Pontic king Mithridates VI, the Gaul Vercingetorix, and the Egyptian queen Cleopatra.

At home, the Republic similarly experienced a long streak of social and political crises, which ended in several violent civil wars. At first, the Conflict of the Orders opposed the patricians, the closed oligarchic elite, to the far more numerous "plebs", who finally achieved political equality in several steps during the 4th century BC. Later, the vast conquests of the Republic disrupted its society, as the immense influx of slaves they brought enriched the aristocracy, but ruined the peasantry and urban workers. In order to solve this issue, several social reformers, known as the "Populares", tried to pass agrarian laws, but the Gracchi brothers, Saturninus, or Clodius Pulcher were all murdered by their opponents, the "Optimates", keepers of the traditional aristocratic order. Mass slavery also caused three Servile Wars; the last of them was led by Spartacus, a skilful gladiator who ravaged Italy and left Rome powerless until his defeat in 71 BC. In this context, the last decades of the Republic were marked by the rise of great generals, who exploited their military conquests and the factional situation in Rome to gain control of the political system. Marius (between 105–86 BC), then Sulla (between 82–78 BC) dominated in turn the Republic; both used extraordinary powers to purge their opponents. These multiple tensions led to a series of civil wars; the first between the two generals Julius Caesar and Pompey. Despite his victory and appointment as dictator for life, Caesar was murdered in 44 BC. Caesar's heir Octavian and lieutenant Mark Antony defeated Caesar's assassins Brutus and Cassius in 42 BC, but then turned against each other. The final defeat of Mark Antony and his ally Cleopatra at the Battle of Actium in 31 BC, and the Senate's grant of extraordinary powers to Octavian as "Augustus" in 27 BC – which effectively made him the first Roman emperor – thus ended the Republic.

Since the foundation of Rome, its rulers had been monarchs, elected for life by the patrician noblemen who made up the Roman Senate. The last Roman king was Lucius Tarquinius Superbus ("Tarquin the Proud"). In the traditional histories, Tarquin was expelled in 509 because his son Sextus Tarquinius had raped the noblewoman Lucretia, who afterwards took her own life. Lucretia's father, her husband Lucius Tarquinius Collatinus, and Tarquin's nephew Lucius Junius Brutus mustered support from the Senate and army, and forced Tarquin into exile in Etruria.

The Senate agreed to abolish kingship. Most of the king's former functions were transferred to two consuls, who were elected to office for a term of one year. Each consul had the capacity to act as a check on his colleague, if necessary through the same power of "veto" that the kings had held. If a consul abused his powers in office, he could be prosecuted when his term expired. Brutus and Collatinus became Republican Rome's first consuls. Despite Collatinus' role in the creation of the Republic, he belonged to the same family as the former king, and was forced to abdicate his office and leave Rome. He was replaced as co-consul by Publius Valerius Publicola.

Most modern scholarship describes these events as the quasi-mythological detailing of an aristocratic coup within Tarquin's own family, not a popular revolution. They fit a narrative of a personal vengeance against a tyrant leading to his overthrow, which was common among Greek cities and even theorised by Aristotle.

According to Rome's traditional histories, Tarquin made several attempts to retake the throne, including the Tarquinian conspiracy, which involved Brutus' own sons, the war with Veii and Tarquinii and finally the war between Rome and Clusium; but none succeeded.

The first Roman republican wars were wars of both expansion and defence, aimed at protecting Rome itself from neighbouring cities and nations and establishing its territory in the region. Initially, Rome's immediate neighbours were either Latin towns and villages, or else tribal Sabines from the Apennine hills beyond. One by one Rome defeated both the persistent Sabines and the local cities, both those under Etruscan control and those that had cast off their Etruscan rulers. Rome defeated the Latin cities in the Battle of Lake Regillus in 496, the Battle of Mount Algidus in 458, the Battle of Corbio in 446, the Battle of Aricia, however it suffered a significant defeat at the Battle of the Cremera in 477 wherein it fought against the most important Etruscan city of Veii. By the end of this period, Rome had effectively completed the conquest of their immediate Etruscan and Latin neighbours, and also secured their position against the immediate threat posed by the nearby Apennine hill tribes.

Beginning with their revolt against Tarquin, and continuing through the early years of the Republic, Rome's patrician aristocrats were the dominant force in politics and society. They initially formed a closed group of about 50 large families, called "gentes", who monopolised Rome's magistracies, state priesthoods and senior military posts. The most prominent of these families were the Cornelii, followed by the Aemilii, Claudii, Fabii, and Valerii. The power, privilege and influence of leading families derived from their wealth, in particular from their landholdings, their position as patrons, and their numerous clients.

The vast majority of Roman citizens were commoners of various social degrees. They formed the backbone of Rome's economy, as smallholding farmers, managers, artisans, traders, and tenants. In times of war, they could be summoned for military service. Most had little direct political influence over the Senate's decisions or the laws it passed, including the abolition of the monarchy and the creation of the consular system. During the early Republic, the "plebs" (or plebeians) emerged as a self-organised, culturally distinct group of commoners, with their own internal hierarchy, laws, customs, and interests.

Plebeians had no access to high religious and civil office, and could be punished for offences against laws of which they had no knowledge. For the poorest, one of the few effective political tools was their withdrawal of labour and services, in a ""secessio plebis""; they would leave the city en masse, and allow their social superiors to fend for themselves. The first such secession occurred in 494, in protest at the abusive treatment of plebeian debtors by the wealthy during a famine. The Senate was compelled to give them direct access to the written civil and religious laws, and to the electoral and political process. To represent their interests, the "plebs" elected tribunes, who were personally sacrosanct, immune to arbitrary arrest by any magistrate, and had veto power over the passage of legislation.

By 390, several Gallic tribes were invading Italy from the north as their culture expanded throughout Europe. The Romans were alerted to this when a particularly warlike tribe, the Senones, invaded two Etruscan towns close to Rome's sphere of influence. These towns, overwhelmed by the enemy's numbers and ferocity, called on Rome for help. The Romans met the Gauls in pitched battle at the Battle of Allia River around 390–387 BC. The Gauls, led by the chieftain Brennus, defeated the Roman army of approximately 15,000 troops, pursued the fleeing Romans back to Rome, and sacked the city before being either driven off or bought off.

From 343 to 341, Rome won two battles against their Samnite neighbours, but were unable to consolidate their gains, due to the outbreak of war with former Latin allies.

In the Latin War (340–338), Rome defeated a coalition of Latins at the battles of Vesuvius and the Trifanum. The Latins submitted to Roman rule.

A Second Samnite War began in 327. The fortunes of the two sides fluctuated, but from 314, Rome was dominant, and offered progressively unfavourable terms for peace. The war ended with Samnite defeat at the Battle of Bovianum (305). By the following year, Rome had annexed most Samnite territory, and began to establish colonies there; but in 298 the Samnites rebelled, and defeated a Roman army, in a Third Samnite War. Following this success they built a coalition of several previous enemies of Rome.

At the Battle of Populonia in 282 Rome finished off the last vestiges of Etruscan power in the region.

In the 4th century, plebeians gradually obtained political equality with patricians. The starting point was in 400, when the first plebeian consular tribunes were elected; likewise, several subsequent consular colleges counted plebeians (in 399, 396, 388, 383, and 379). The reason behind this sudden gain is unknown, but it was limited as patrician tribunes retained preeminence over their plebeian colleagues. In 385, the former consul and saviour of the besieged Capitol Marcus Manlius Capitolinus is said to have sided with the plebeians, ruined by the Sack and largely indebted to patricians. The issue of debt relief for the plebs remained indeed pressing throughout the century. Livy tells that Capitolinus sold his estate to repay the debt of many of them, and even went over to the plebs, the first patrician to do so. Nevertheless, the growing unrest he had caused led to his trial for seeking kingly power; he was sentenced to death and thrown from the Tarpeian Rock.

Between 376 and 367, the tribunes of the plebs Gaius Licinius Stolo and Lucius Sextius Lateranus continued the plebeian agitation and pushed for an ambitious legislation, known as the "Leges Liciniae Sextiae". Two of their bills attacked patricians' economic supremacy, by creating legal protection against indebtedness and forbidding excessive use of public land, as the "Ager publicus" was monopolised by large landowners. The most important bill opened the consulship to plebeians. Other tribunes controlled by the patricians vetoed the bills, but Stolo and Lateranus retaliated by vetoing the elections for five years while being continuously re-elected by the plebs, resulting in a stalemate. In 367, they carried a bill creating the "Decemviri sacris faciundis", a college of ten priests, of whom five had to be plebeians, therefore breaking patricians' monopoly on priesthoods. Finally, the resolution of the crisis came from the dictator Camillus, who made a compromise with the tribunes; he agreed to their bills, while they in return consented to the creation of the offices of praetor and curule aediles, both reserved to patricians. Lateranus also became the first plebeian consul in 366; Stolo followed in 361.

Soon after, plebeians were able to hold both the dictatorship and the censorship, since former consuls normally filled these senior magistracies. The four time consul Gaius Marcius Rutilus became the first plebeian dictator in 356 and censor in 351. In 342, the tribune of the plebs Lucius Genucius passed his "Leges Genuciae", which abolished interest on loans, in a renewed effort to tackle indebtedness, required the election of at least one plebeian consul each year, and prohibited a magistrate from holding the same magistracy for the next ten years or two magistracies in the same year. In 339, the plebeian consul and dictator Quintus Publilius Philo passed three laws extending the powers of the plebeians. His first law followed the "Lex Genucia" by reserving one censorship to plebeians, the second made plebiscites binding on all citizens (including patricians), and the third stated that the Senate had to give its prior approval to plebiscites before becoming binding on all citizens (the "Lex Valeria-Horatia" of 449 had placed this approval after the vote). Two years later, Publilius ran for the praetorship, probably in a bid to take the last senior magistracy closed to plebeians, which he won. 
During the early republic, senators were chosen by the consuls among their supporters. Shortly before 312, the "Lex Ovinia" transferred this power to the censors, who could only remove senators for misconduct, thus appointing them for life. This law strongly increased the power of the Senate, which was by now protected from the influence of the consuls and became the central organ of government. In 312, following this law, the patrician censor Appius Claudius Caecus appointed many more senators to fill the new limit of 300, including descendants of freedmen, which was deemed scandalous. He also incorporated these freedmen in the rural tribes. His tribal reforms were nonetheless cancelled by the next censors, Quintus Fabius Maximus and Publius Decius Mus, his political enemies. Caecus also launched a vast construction program, building the first aqueduct ("Aqua Appia"), and the first Roman road ("Via Appia").

In 300, the two tribunes of the plebs Gnaeus and Quintus Ogulnius passed the "Lex Ogulnia", which created four plebeian pontiffs, therefore equalling the number of patrician pontiffs, and five plebeian augurs, outnumbering the four patricians in the college. Eventually the Conflict of the Orders ended with the last secession of the plebs in about 287. The details are not known precisely as Livy's books on the period are lost. Debt is once again mentioned by ancient authors, but it seems that the plebs revolted over the distribution of the land conquered on the Samnites. A dictator named Quintus Hortensius was appointed to negotiate with the plebeians, who had retreated to the Janiculum hill, perhaps to dodge the draft in the war against the Lucanians. Hortensius passed the "Lex Hortensia" which re-enacted the law of 339, making plebiscites binding on all citizens, but also removed the Senate's prior approval to plebiscites. Popular assemblies were by now sovereign; this put an end to the crisis, and to plebeian agitation for 150 years.

These events were a political victory of the wealthy plebeian elite who exploited the economic difficulties of the plebs for their own gain, hence why Stolo, Lateranus, and Genucius bound their bills attacking patricians' political supremacy with debt-relief measures. They had indeed little in common with the mass of plebeians; Stolo was noteworthy fined for having exceeded the limit on land occupation he had fixed in his own law. As a result of the end of the patrician monopoly on senior magistracies, many small patrician "gentes" faded into history during the 4th and 3rd centuries due to the lack of available positions; the Verginii, Horatii, Menenii, Cloelii all disappear, even the Julii entered a long eclipse. They were replaced by plebeian aristocrats, of whom the most emblematic were the Caecilii Metelli, who received 18 consulships until the end of the Republic; the Domitii, Fulvii, Licinii, Marcii, or Sempronii were as successful. About a dozen remaining patrician "gentes" and twenty plebeian ones thus formed a new elite, called the "nobiles", or "Nobilitas".

 By the beginning of the 3rd century, Rome had established itself as the major power in Italy, but had not yet come into conflict with the dominant military powers of Mediterranean: Carthage and the Greek kingdoms. In 282, several Roman warships entered the harbour of Tarentum, thus breaking a treaty between the Republic and the Greek city, which forbade the Gulf to Roman navy. It triggered a violent reaction from the Tarentine democrats, who sank some of the ships; they were in fact worried that Rome could favour the oligarchs in the city, as it had done with the other Greek cities under its control. The Roman embassy sent to investigate the affair was insulted and war was promptly declared. Facing a hopeless situation, the Tarentines (together with the Lucanians and Samnites) appealed for military aid to Pyrrhus, the very ambitious king of Epirus. A cousin of Alexander the Great, he was eager to build an empire for himself in the western Mediterranean, and saw Tarentum's plea as a perfect opportunity towards this goal.Pyrrhus and his army of 25,500 men (and 20 war elephants) landed in Italy in 280; he was immediately named "Strategos Autokrator" by the Tarentines. Publius Valerius Laevinus, the consul sent to face him, rejected the king's negotiation offer, as he had more troops and hoped to cut the invasion short. The Romans were nevertheless defeated at Heraclea, as their cavalry were afraid of the elephants of Pyrrhus, who lost a large portion of his army. Pyrrhus then marched on Rome, but could not take any Roman city on his way; facing the prospect of being flanked by the two consular armies, he moved back to Tarentum. His adviser, the orator Cineas, made a peace offer before the Roman Senate, asking Rome to return the land it took from the Samnites and Lucanians, and liberate the Greek cities under its control. The offer was rejected after Appius Caecus (the old censor of 312) spoke against it in a celebrated speech, which was the earliest recorded by the time of Cicero. In 279, Pyrrhus met the consuls Publius Decius Mus and Publius Sulpicius Saverrio at the Battle of Asculum, which remained undecided for two days, as the Romans had prepared some special chariots to counter his elephants. Finally, Pyrrhus personally charged into the melee and won the battle, but at the cost of an important part of his troops; he allegedly said "If we are victorious in one more battle with the Romans, we shall be utterly ruined."

He escaped the Italian deadlock by answering a call for help from Syracuse, which tyrant Thoenon was desperately fighting an invasion from Carthage. Pyrrhus could not let them take the whole island as it would have compromised his ambitions in the western Mediterranean and so declared war on them. At first, his Sicilian campaign was an easy triumph; he was welcomed as a liberator in every Greek city on his way, even receiving the title of king ("basileus") of Sicily. The Carthaginians lifted the siege of Syracuse before his arrival, but he could not entirely oust them from the island as he failed to take their fortress of Lilybaeum. His harsh rule, especially the murder of Thoenon, whom he did not trust, soon led to a widespread antipathy among the Sicilians; some cities even defected to Carthage. In 275, Pyrrhus left the island before he had to face a full-scale rebellion. He returned to Italy, where his Samnite allies were on the verge of losing the war, despite their earlier victory at the Cranita hills. Pyrrhus again met the Romans at the Battle of Beneventum; this time the consul Manius Dentatus was victorious, and even captured eight elephants. Pyrrhus then withdrew from Italy, but left a garrison in Tarentum, and waged a new campaign in Greece against Antigonos Gonatas. His death in battle at Argos in 272 forced Tarentum to surrender to Rome. Since it was the last independent city of Italy, Rome now dominated the entire Italian peninsula, and won an international military reputation.

Rome and Carthage were initially on friendly terms; Polybius details three treaties between them, the first dating from the first year of the Republic, the second from 348. The last one was an alliance against Pyrrhus. However, tensions rapidly built on after the departure of the Epirote king. Between 288 and 283, Messena in Sicily was taken by the Mamertines, a band of mercenaries formerly employed by Agathocles. They plundered the surroundings until Hiero II, the new tyrant of Syracuse, defeated them (in either 269 or 265). Carthage could not let him take Messena, as he would have controlled its Strait, and garrisoned the city. In effect under a Carthaginian protectorate, the remaining Mamertines appealed to Rome to regain their independence. Senators were divided on whether to help them or not, as it would have meant war with Carthage, since Sicily was in its sphere of influence (the treaties furthermore forbade the island to Rome), and also Syracuse. A supporter of the war, the consul Appius Claudius Caudex (Caecus' brother) turned to the Tribal Assembly to get a favourable vote, by notably promising booty to voters.

Caudex first secured control of the city with ease. However, Syracuse and Carthage, at war for centuries, made an alliance to counter the invasion and blockaded Messena, but Caudex defeated Hiero and Carthage separately. His successor Manius Valerius Corvinus Messalla landed with a strong 40,000 men army that conquered eastern Sicily, which prompted Hiero to shift his allegiance and forge a long lasting alliance with Rome. In 262, the Romans moved to the southern coast and besieged Akragas. In order to raise the siege, Carthage sent reinforcements, including 60 elephants – the first time they used them, but still lost the battle. Nevertheless, as Pyrrhus before, Rome could not take all of Sicily because Carthage's naval superiority prevented them from effectively besieging coastal cities, which could receive supplies from the sea. Using a captured Carthaginian ship as blueprint, Rome therefore launched a massive construction program and built 100 quinqueremes in only two months, perhaps through an assembly line organisation. They also invented a new device, the "corvus", a grappling engine which enabled a crew to board on an enemy ship. The consul for 260 Scipio Asina lost the first naval skirmish of the war against Hannibal Gisco at Lipara, but his colleague Gaius Dullius won a great victory at Mylae. He destroyed or captured 44 ships, and was the first Roman to receive a naval triumph, which also included captive Carthaginians for the first time. Although Carthage was victorious on land at Thermae in Sicily, the "corvus" made Rome invincible on the waters. The consul Lucius Cornelius Scipio (Asina's brother) captured Corsica in 259; his successors won the naval battles of Sulci in 258, Tyndaris in 257, and Cape Ecnomus in 256.
In order to hasten the end of the war, the consuls for 256 decided to carry the operations to Africa, on Carthage's homeland. The consul Marcus Atilius Regulus landed on the Cap Bon peninsula with about 18,000 soldiers. He captured the city of Aspis, then repulsed Carthage's counter-attack at Adys, and took Tunis. The Carthaginians supposedly sued him for peace, but his conditions were so harsh that they continued the war instead. They hired Spartan mercenaries, led by Xanthippus, to command their troops. In 255, the Spartan general marched on Regulus, still encamped at Tunis, who accepted the battle to avoid sharing the glory with his successor. However, the flat land near Tunis favoured the Punic elephants, which crushed the Roman infantry on the Bagradas plain; only 2,000 soldiers escaped, and Regulus was captured. The consuls for 255 nonetheless won a new sounding naval victory at Cape Hermaeum, where they captured 114 warships. This success was spoilt by a storm that annihilated the victorious navy: 184 ships of 264 sank, 25,000 soldiers and 75,000 rowers drowned. The "corvus" considerably hindered ships' navigation, and made them vulnerable during tempest. It was abandoned after another similar catastrophe took place in 253 (150 ships sank with their crew). These disasters prevented any significant campaign between 254 and 252.
Hostilities in Sicily resumed in 252, with the taking of Thermae by Rome. Carthage countered the following year, by besieging Lucius Caecilius Metellus, who held Panormos (now Palermo). The consul had dug trenches to counter the elephants, which once hurt by missiles turned back on their own army, resulting in a great victory for Metellus, who exhibited some captured beasts in the Circus. Rome then besieged the last Carthaginian strongholds in Sicily, Lilybaeum and Drepana, but these cities were impregnable by land. Publius Claudius Pulcher, the consul of 249, recklessly tried to take the latter from the sea, but he suffered a terrible defeat; his colleague Lucius Junius Pullus likewise lost his fleet off Lilybaeum. Without the "corvus", Roman warships had lost their advantage. By now, both sides were drained and could not undertake large scale operations; the number of Roman citizens who were being called up for war had been reduced by 17% in two decades, a result of the massive bloodshed. The only military activity during this period was the landing in Sicily of Hamilcar Barca in 247, who harassed the Romans with a mercenary army from a citadel he built on Mt. Eryx.

Finally, unable to take the Punic fortresses in Sicily, Rome tried to win the decision at sea and built a new navy, thanks to a forced borrowing on the rich. In 242, the 200 quinqueremes of the consul Gaius Lutatius Catulus blockaded Drepana. The rescue fleet from Carthage arrived the next year, but was largely undermanned and soundly defeated by Catulus. Exhausted and unable to bring supplies to Sicily, Carthage sued for peace. Catulus and Hamilcar negotiated a treaty, which was somewhat lenient to Carthage, but the Roman people rejected it and imposed harsher terms: Carthage had to pay 1000 talents immediately and 2200 over ten years, and evacuate Sicily. The fine was so high that Carthage could not pay Hamilcar's mercenaries, who had been shipped back to Africa. They revolted during the Mercenary War, which Carthage had enormous difficulties to suppress. Meanwhile, Rome took advantage of a similar revolt in Sardinia to seize the island from Carthage, in violation of the peace treaty. This stab-in-the-back led to permanent bitterness in Carthage, and revanchism.

After its victory, the Republic shifted its attention to its northern border as the Insubres and Boii were threatening Italy. Meanwhile, Carthage compensated the loss of Sicily and Sardinia with the conquest of Southern Hispania (up to Salamanca), and its rich silver mines. This enterprise was the work of the Barcid family, headed by Hamilcar, the former commander in Sicily. Hamilcar nonetheless died against the Oretani in 228; his son-in-law Hasdrubal the Fair – the founder of Carthago Nova – and his three sons Hannibal, Hasdrubal, and Mago, succeeded him. This rapid expansion worried Rome, which concluded a treaty with Hasdrubal in 226, stating that Carthage could not cross the Ebro river. However, the city of Saguntum, located in the south of the Ebro, appealed to Rome in 220 to act as arbitrator during a "stasis". Hannibal dismissed Roman rights on the city, and took it in 219. At Rome, the Cornelii and the Aemilii considered the capture of Saguntum a "casus belli", and won the debate against Fabius Maximus Verrucosus, who wanted to negotiate. An embassy carrying an ultimatum was sent to Carthage, asking its senate to condemn Hannibal's deeds. The Carthaginian refusal started the Second Punic War.
Initially, the plan of the Republic was to carry war outside Italy, by sending the consuls Publius Cornelius Scipio to Hispania, and Sempronius Longus to Africa, while their naval superiority prevented Carthage from attacking from the sea. This plan was thwarted by Hannibal's bold move to Italy. In May 218, he indeed crossed the Ebro with a large army of about 100,000 soldiers and 37 elephants. He passed in Gaul, crossed the Rhone, then the Alps, possibly through the Col de Clapier (2,491 meters high). This famous exploit cost him half of his troops, but he could now rely on the Boii and Insubres, still at war with Rome. Publius Scipio, who had failed to block Hannibal on the Rhone, sent his elder brother Gnaeus with the main part of his army in Hispania according to the initial plan, and went back to Italy with the rest to resist Hannibal in Italy, but he was defeated and wounded near Pavia.

Hannibal then marched south and won three outstanding victories. The first one was on the banks of the Trebia in December 218, where he defeated the other consul Sempronius Longus thanks to his brother Mago, who had concealed some elite troops behind the legions and attacked them from the rear once fighting Hannibal. More than half of the Roman army was lost. Hannibal then ravaged the country around Arretium to lure the new consul Gaius Flaminius into a trap, at the Lake Trasimene. He had hidden his troops in the hills surrounding the lake and attacked Flaminius when he was cornered on the shore. This clever ambush resulted in the death of the consul and the complete destruction of his army of 30,000 men. In 216, the new consuls Aemilius Paullus and Terentius Varro mustered the biggest army possible, with eight legions (more than 80,000 soldiers) – twice as many as the Punic army – and confronted Hannibal, who was encamped at Cannae, in Apulia. Despite his numerical disadvantage, Hannibal used his heavier cavalry to rout the Roman wings and envelop their infantry, whom he annihilated. In terms of casualties, the Battle of Cannae was the worst defeat in the history of Rome: only 14,500 soldiers escaped; Paullus was killed as well as 80 senators. Soon after, the Boii ambushed the army of the consul-elect for 215, Postumius Albinus, who died with all his army of 25,000 men in the Forest of Litana.

These disasters triggered a wave of defection among Roman allies, with the rebellions of the Samnites, Oscans, Lucanians, and Greek cities of Southern Italy. In Macedonia, Philip V also made an alliance with Hannibal in order to take Illyria and the area around Epidamnus, occupied by Rome. His attack on Apollonia started the First Macedonian War. In 215, Hiero II of Syracuse died of old age, and his young grandson Hieronymus broke the long alliance with Rome to side with Carthage. At this desperate point, the aggressive strategy against Hannibal advocated by the Scipiones was abandoned in favour of delaying tactics that avoided direct confrontation with him. Its main proponents were the consuls Fabius Maximus Verrucosus, nicknamed "Cunctator" ("the delayer"), Claudius Marcellus, and Fulvius Flaccus. The "Fabian Strategy" favoured a slow reconquest of the lost territories, since Hannibal could not be everywhere to defend them. Although he remained invincible on the battlefield, defeating all the Roman armies on his way, he could not prevent Claudius Marcellus from taking Syracuse in 212 after a long siege, nor the fall of his bases of Capua and Tarentum in 211 and 209. However, in 208 the consuls Claudius Marcellus and Quinctius Crispinus were ambushed and killed near Venusia.

In Hispania, the situation was overall much better for Rome. This theatre was mostly commanded by the brothers Publius and Gnaeus Scipio, who won the battles of Cissa in 218, soon after Hannibal's departure, and Dertosa against his brother Hasdrubal in 215, which enabled them to conquer the eastern coast of Hispania. In 211 however, Hasdrubal and Mago Barca successfully returned the Celtiberian tribes that supported the Scipiones, and attacked them simultaneously at the Battle of the Upper Baetis, in which the Scipiones brothers died. Publius' son, the future Scipio Africanus, was then elected with a special proconsulship to lead the Hispanic campaign. He soon showed outstanding skills as a commander, by winning a series of battles with ingenious tactics. In 209, he took Carthago Nova, the main Punic base in Hispania, then defeated Hasdrubal at the Battle of Baecula (208). After his defeat, Hasdrubal was ordered by Carthage to move to Italy. Since he could not use ships, he followed the same route as his brother through the Alps, but this time the surprise effect was gone. The consuls Livius Salinator and Claudius Nero were awaiting him and won the Battle of the Metaurus, where Hasdrubal died. It was the turning point of the war. The attrition campaign had indeed worked well: Hannibal's troops were now depleted; he only had one elephant left (Surus) and retreated to Bruttium, on the defensive. In Greece, Rome contained Philip V without devoting too many forces, by setting an alliance with the Aetolian League, Sparta, and Pergamon, which also prevented Philip from aiding Hannibal. The war resulted in a stalemate, with the Treaty of Phoenice signed in 205.
In Hispania, Scipio continued his triumphal campaign at the battles of Carmona in 207, and Ilipa (now Seville) in 206, which ended the Punic threat on the peninsula. Elected consul in 205, he convinced the Senate to cancel the Fabian Strategy, and instead to invade Africa by using the support of the Numidian king Massinissa, who had defected to Rome. Scipio landed in Africa in 204. He took Utica, then won the Battle of the Great Plains, which prompted Carthage to recall Hannibal from Italy and open peace negotiations with Rome. The talks nevertheless failed because Scipio wanted to impose harsher terms on Carthage, in order to avoid it from rising again as a threat to Rome. Hannibal was therefore sent to face Scipio at Zama. Scipio could now use the heavy Numidian cavalry of Massinissa – which had hitherto been so successful against Rome – to rout the Punic wings, then flank the infantry, as Hannibal had done at Cannae. Defeated for the first time, Hannibal convinced the Carthaginian Senate to pay the war indemnity, which was even harsher than that of 241: 10,000 talents in 50 instalments. Carthage furthermore had to give up all its elephants, all its fleet but ten triremes, all its possessions outside its core territory in Africa (what is now Tunisia), and could not declare war without the authorisation of Rome. In effect, Carthage was condemned to be a minor power, while Rome recovered from a desperate situation to dominate the Western Mediterranean.

Rome's preoccupation with its war with Carthage provided an opportunity for Philip V of the kingdom of Macedonia, located in the north of the Greek peninsula, to attempt to extend his power westward. Philip sent ambassadors to Hannibal's camp in Italy, to negotiate an alliance as common enemies of Rome. However, Rome discovered the agreement when Philip's emissaries were captured by a Roman fleet. The First Macedonian War saw the Romans involved directly in only limited land operations, but they ultimately achieved their objective of preoccupying Philip and preventing him from aiding Hannibal.

The past century had seen the Greek world dominated by the three primary successor kingdoms of Alexander the Great's empire: Ptolemaic Egypt, Macedonia and the Seleucid Empire. In 202, internal problems led to a weakening of Egypt's position, thereby disrupting the power balance among the successor states. Macedonia and the Seleucid Empire agreed to an alliance to conquer and divide Egypt. Fearing this increasingly unstable situation, several small Greek kingdoms sent delegations to Rome to seek an alliance. The delegation succeeded, even though prior Greek attempts to involve Rome in Greek affairs had been met with Roman apathy. Our primary source about these events, the surviving works of Polybius, do not state Rome's reason for getting involved. Rome gave Philip an ultimatum to cease his campaigns against Rome's new Greek allies. Doubting Rome's strength (a reasonable doubt, given Rome's performance in the First Macedonian War) Philip ignored the request, and Rome sent an army of Romans and Greek allies, beginning the Second Macedonian War. Despite his recent successes against the Greeks and earlier successes against Rome, Philip's army buckled under the pressure from the Roman-Greek army. In 197, the Romans decisively defeated Philip at the Battle of Cynoscephalae, and Philip was forced to give up his recent Greek conquests. The Romans declared the "Peace of the Greeks", believing that Philip's defeat now meant that Greece would be stable. They pulled out of Greece entirely, maintaining minimal contacts with their Greek allies.

With Egypt and Macedonia weakened, the Seleucid Empire made increasingly aggressive and successful attempts to conquer the entire Greek world. Now not only Rome's allies against Philip, but even Philip himself, sought a Roman alliance against the Seleucids. The situation was made worse by the fact that Hannibal was now a chief military advisor to the Seleucid emperor, and the two were believed to be planning an outright conquest not just of Greece, but of Rome itself. The Seleucids were much stronger than the Macedonians had ever been, because they controlled much of the former Persian Empire, and by now had almost entirely reassembled Alexander the Great's former empire.
Fearing the worst, the Romans began a major mobilization, all but pulling out of recently pacified Spain and Gaul. They even established a major garrison in Sicily in case the Seleucids ever got to Italy. This fear was shared by Rome's Greek allies, who had largely ignored Rome in the years after the Second Macedonian War, but now followed Rome again for the first time since that war. A major Roman-Greek force was mobilized under the command of the great hero of the Second Punic War, Scipio Africanus, and set out for Greece, beginning the Roman-Syrian War. After initial fighting that revealed serious Seleucid weaknesses, the Seleucids tried to turn the Roman strength against them at the Battle of Thermopylae (as they believed the 300 Spartans had done centuries earlier). Like the Spartans, the Seleucids lost the battle, and were forced to evacuate Greece. The Romans pursued the Seleucids by crossing the Hellespont, which marked the first time a Roman army had ever entered Asia. The decisive engagement was fought at the Battle of Magnesia, resulting in a complete Roman victory. The Seleucids sued for peace, and Rome forced them to give up their recent Greek conquests. Although they still controlled a great deal of territory, this defeat marked the decline of their empire, as they were to begin facing increasingly aggressive subjects in the east (the Parthians) and the west (the Greeks). Their empire disintegrated into a rump over the course of the next century, when it was eclipsed by Pontus. Following Magnesia, Rome again withdrew from Greece, assuming (or hoping) that the lack of a major Greek power would ensure a stable peace. In fact, it did the opposite.

In 179 Philip died. His talented and ambitious son, Perseus, took the throne and showed a renewed interest in conquering Greece. With her Greek allies facing a major new threat, Rome declared war on Macedonia again, starting the Third Macedonian War. Perseus initially had some success against the Romans. However, Rome responded by sending a stronger army. This second consular army decisively defeated the Macedonians at the Battle of Pydna in 168 and the Macedonians duly capitulated, ending the war.

Convinced now that the Greeks (and therefore the rest of the region) would not have peace if left alone, Rome decided to establish its first permanent foothold in the Greek world, and divided the Kingdom of Macedonia into four client republics. Yet, Macedonian agitation continued. The Fourth Macedonian War, 150 to 148 BC, was fought against a Macedonian pretender to the throne who was again destabilizing Greece by trying to re-establish the old kingdom. The Romans swiftly defeated the Macedonians at the Second battle of Pydna.

The Achaean League chose this moment to fight Rome but was swiftly defeated. In 146 (the same year as the destruction of Carthage), Corinth was besieged and destroyed in the Battle of Corinth (146 BC), which led to the league's surrender. After nearly a century of constant crisis management in Greece, which always led back to internal instability and war when she withdrew, Rome decided to divide Macedonia into two new Roman provinces, Achaea and Macedonia.

Carthage never recovered militarily after the Second Punic War, but quickly did so economically and the Third Punic War that followed was in reality a simple punitive mission after the neighbouring Numidians allied to Rome robbed/attacked Carthaginian merchants. Treaties had forbidden any war with Roman allies, and defence against robbing/pirates was considered as "war action": Rome decided to annihilate the city of Carthage. Carthage was almost defenceless, and submitted when besieged. However, the Romans demanded complete surrender and removal of the city into the (desert) inland far off any coastal or harbour region, and the Carthaginians refused. The city was besieged, stormed, and completely destroyed.

Ultimately, all of Carthage's North African and Iberian territories were acquired by Rome. Note that "Carthage" was not an 'empire', but a league of Punic colonies (port cities in the western Mediterranean) like the 1st and 2nd Athenian ("Attic") leagues, under leadership of Carthage. Punic Carthage was gone, but the other Punic cities in the western Mediterranean flourished under Roman rule.

Rome's rapid expansion destabilized its social organization and triggered unrest in the heart of the Republic, which ultimately led to political violence, unrest in the provinces, and ultimately a breakdown in the traditional social relations of Rome that created the Augustan Empire. The period is marked by the rise of strongmen (Marius, Sulla, Pompey, Crassus, and Julius Caesar), who turned military success into political power.

In 135, the first slave uprising, known as the First Servile War, broke out in Sicily. After initial successes, the slaves led by Eunus and Cleon were annihilated by the consul Publius Rupilius in 132 BC.

In this context, Tiberius Gracchus was elected tribune in 133 BC. He attempted to enact a law which would have limited the amount of land that any individual could own. The aristocrats, who stood to lose an enormous amount of money, were bitterly opposed to this proposal. Tiberius submitted this law to the Plebeian Council, but the law was vetoed by a tribune named Marcus Octavius. Tiberius then used the Plebeian Council to impeach Octavius. The theory, that a representative of the people ceases to be one when he acts against the wishes of the people, was counter to Roman constitutional theory. If carried to its logical end, this theory would remove all constitutional restraints on the popular will, and put the state under the absolute control of a temporary popular majority. His law was enacted, but Tiberius was murdered with 300 of his associates when he stood for reelection to the tribunate.

Tiberius' brother Gaius was elected tribune in 123. Gaius Gracchus' ultimate goal was to weaken the senate and to strengthen the democratic forces. In the past, for example, the senate would eliminate political rivals either by establishing special judicial commissions or by passing a
"senatus consultum ultimum" ("ultimate decree of the senate"). Both devices would allow the Senate to bypass the ordinary due process rights that all citizens had. Gaius outlawed the judicial commissions, and declared the "senatus consultum ultimum" to be unconstitutional. Gaius then proposed a law which would grant citizenship rights to Rome's Italian allies. This last proposal was not popular with the plebeians and he lost much of his support. He stood for election to a third term in 121, but was defeated and then murdered by representatives of the senate with 3,000 of his supporters on Capitoline Hill in Rome.

In 121, the province of Gallia Narbonensis was established after the victory of Quintus Fabius Maximus over a coalition of Arverni and Allobroges in southern Gaul in 123. The city of Narbo was founded there in 118 by Lucius Licinius Crassus.

The Jugurthine War of 111–104 was fought between Rome and Jugurtha of the North African kingdom of Numidia. It constituted the final Roman pacification of Northern Africa, after which Rome largely ceased expansion on the continent after reaching natural barriers of desert and mountain. Following Jugurtha's usurpation of the throne of Numidia, a loyal ally of Rome since the Punic Wars, Rome felt compelled to intervene. Jugurtha impudently bribed the Romans into accepting his usurpation. Jugurtha was finally captured not in battle but by treachery.

In 118, King Micipsa of Numidia (current-day Algeria and Tunisia) died. He was succeeded by two legitimate sons, Adherbal and Hiempsal, and an illegitimate son, Jugurtha. Micipsa divided his kingdom between these three sons. Jugurtha, however, turned on his brothers, killing Hiempsal and driving Adherbal out of Numidia. Adherbal fled to Rome for assistance, and initially Rome mediated a division of the country between the two brothers. Eventually, Jugurtha renewed his offensive, leading to a long and inconclusive war with Rome. He also bribed several Roman commanders, and at least two tribunes, before and during the war. His nemesis, Gaius Marius, a legate from a virtually unknown provincial family, returned from the war in Numidia and was elected consul in 107 over the objections of the aristocratic senators. Marius invaded Numidia and brought the war to a quick end, capturing Jugurtha in the process. The apparent incompetence of the Senate, and the brilliance of Marius, had been put on full display. The "populares" party took full advantage of this opportunity by allying itself with Marius.
The Cimbrian War (113–101) was a far more serious affair than the earlier clashes of 121. The Germanic tribes of the "Cimbri" and the "Teutons" migrated from northern Europe into Rome's northern territories, and clashed with Rome and her allies. At the Battle of Aquae Sextiae and the Battle of Vercellae both tribes were virtually annihilated, which ended the threat.

In 91 the Social War broke out between Rome and its former allies in Italy when the allies complained that they shared the risk of Rome's military campaigns, but not its rewards. Although they lost militarily, the allies achieved their objectives with legal proclamations which granted citizenship to more than 500,000 Italians.

The internal unrest reached its most serious state, however, in the two civil wars that were caused by the clash between generals Gaius Marius and Lucius Cornelius Sulla starting from 88. In the Battle of the Colline Gate at the very door of the city of Rome, a Roman army under Sulla bested an army of the Marius supporters and entered the city. Sulla's actions marked a watershed in the willingness of Roman troops to wage war against one another that was to pave the way for the wars which ultimately overthrew the Republic, and caused the founding of the Roman Empire.

Several years later, in 88, a Roman army was sent to put down an emerging Asian power, king Mithridates of Pontus. The army, however, was not defeated and won. One of Marius' old quaestors, Lucius Cornelius Sulla, had been elected consul for the year, and was ordered by the senate to assume command of the war against Mithridates. Marius, a member of the ""populares"" party, had a tribune revoke Sulla's command of the war against Mithridates. Sulla, a member of the aristocratic (""optimates"") party, brought his army back to Italy and marched on Rome. Sulla was so angry at Marius' tribune that he passed a law intended to permanently weaken the tribunate. He then returned to his war against Mithridates. With Sulla gone, the "populares" under Marius and Lucius Cornelius Cinna soon took control of the city.

During the period in which the "populares" party controlled the city, they flouted convention by re-electing Marius consul several times without observing the customary ten-year interval between offices. They also transgressed the established oligarchy by advancing unelected individuals to magisterial office, and by substituting magisterial edicts for popular legislation. Sulla soon made peace with Mithridates. In 83, he returned to Rome, overcame all resistance, and recaptured the city. Sulla and his supporters then slaughtered most of Marius' supporters. Sulla, having observed the violent results of radical "popular" reforms, was naturally conservative. As such, he sought to strengthen the aristocracy, and by extension the senate. Sulla made himself dictator, passed a series of constitutional reforms, resigned the dictatorship, and served one last term as consul. He died in 78.
The third and final slave uprising was the most serious, involving ultimately between 120,000 and 150,000 slaves under the command of the gladiator Spartacus.
Mithridates the Great was the ruler of Pontus, a large kingdom in Asia Minor (modern Turkey), from 120 to 63. Mithridates antagonised Rome by seeking to expand his kingdom, and Rome for its part seemed equally eager for war and the spoils and prestige that it might bring. In 88, Mithridates ordered the killing of a majority of the 80,000 Romans living in his kingdom. The massacre was the official reason given for the commencement of hostilities in the First Mithridatic War. The Roman general Lucius Cornelius Sulla forced Mithridates out of Greece proper, but then had to return to Italy to answer the internal threat posed by his rival, Gaius Marius. A peace was made between Rome and Pontus, but this proved only a temporary lull.

The Second Mithridatic War began when Rome tried to annex a province that Mithridates claimed as his own. In the Third Mithridatic War, first Lucius Licinius Lucullus and then Pompey the Great were sent against Mithridates and his Armenian ally Tigranes the Great. Mithridates was finally defeated by Pompey in the night-time Battle of the Lycus.
The Mediterranean had at this time fallen into the hands of pirates, largely from Cilicia. The pirates not only strangled shipping lanes but also plundered many cities on the coasts of Greece and Asia. Pompey was nominated as commander of a special naval task force to campaign against the pirates. It took Pompey just forty days to clear the western portion of the sea of pirates and restore communication between Iberia (Spain), Africa, and Italy.

In 77, the senate sent one of Sulla's former lieutenants, Gnaeus Pompeius Magnus ("Pompey the Great"), to put down an uprising in Hispania. By 71, Pompey returned to Rome after having completed his mission. Around the same time, another of Sulla's former lieutenants, Marcus Licinius Crassus, had just put down the Spartacus-led gladiator/slave revolt in Italy. Upon their return, Pompey and Crassus found the "populares" party fiercely attacking Sulla's constitution. They attempted to forge an agreement with the "populares" party. If both Pompey and Crassus were elected consul in 70, they would dismantle the more obnoxious components of Sulla's constitution. The two were soon elected, and quickly dismantled most of Sulla's constitution.
Around 66, a movement to use constitutional, or at least peaceful, means to address the plight of various classes began. After several failures, the movement's leaders decided to use any means that were necessary to accomplish their goals. The movement coalesced under an aristocrat named Lucius Sergius Catilina. The movement was based in the town of Faesulae, which was not a natural hotbed of agrarian agitation. The rural malcontents were to advance on Rome, and be aided by an uprising within the city. After assassinating the consuls and most of the senators, Catiline would be free to enact his reforms. The conspiracy was set in motion in 63. The consul for the year, Marcus Tullius Cicero, intercepted messages that Catiline had sent in an attempt to recruit more members. As a result, the top conspirators in Rome (including at least one former consul) were executed by authorisation (of dubious constitutionality) of the senate, and the planned uprising was disrupted. Cicero then sent an army, which cut Catiline's forces to pieces.

The most important result of the Catilinarian conspiracy was that the "populares" party became discredited. The prior 70 years had witnessed a gradual erosion in senatorial powers. The violent nature of the conspiracy, in conjunction with the senate's skill in disrupting it, did a great deal to repair the senate's image.

In 62, Pompey returned victorious from Asia. The Senate, elated by its successes against Catiline, refused to ratify the arrangements that Pompey had made. Pompey, in effect, became powerless. Thus, when Julius Caesar returned from a governorship in Spain in 61, he found it easy to make an arrangement with Pompey. Caesar and Pompey, along with Crassus, established a private agreement, now known as the First Triumvirate. Under the agreement, Pompey's arrangements would be ratified. Caesar would be elected consul in 59, and would then serve as governor of Gaul for five years. Crassus was promised a future consulship.

By 59 an unofficial political alliance known as the First Triumvirate was formed between Gaius Julius Caesar, Marcus Licinius Crassus, and Gnaeus Pompeius Magnus ("Pompey the Great") to share power and influence.

Caesar became consul in 59. His colleague, Marcus Calpurnius Bibulus, was an extreme aristocrat. Caesar submitted the laws that he had promised Pompey to the assemblies. Bibulus attempted to obstruct the enactment of these laws, and so Caesar used violent means to ensure their passage. Caesar was then made governor of three provinces. He facilitated the election of the former patrician Publius Clodius Pulcher to the tribunate for 58. Clodius set about depriving Caesar's senatorial enemies of two of their more obstinate leaders in Cato and Cicero. Clodius was a bitter opponent of Cicero because Cicero had testified against him in a sacrilege case. Clodius attempted to try Cicero for executing citizens without a trial during the Catiline conspiracy, resulting in Cicero going into self-imposed exile and his house in Rome being burnt down. Clodius also passed a bill that forced Cato to lead the invasion of Cyprus which would keep him away from Rome for some years. Clodius also passed a law to expand the previous partial grain subsidy to a fully free grain dole for citizens.

During his term as praetor in the Iberian Peninsula (modern Portugal and Spain), Pompey's contemporary Julius Caesar defeated two local tribes in battle. After his term as consul in 59, he was appointed to a five-year term as the proconsular Governor of Cisalpine Gaul (part of current northern Italy), Transalpine Gaul (current southern France) and Illyria (part of the modern Balkans). Not content with an idle governorship, Caesar strove to find reason to invade Gaul (modern France and Belgium), which would give him the dramatic military success he sought. When two local tribes began to migrate on a route that would take them near (not into) the Roman province of Transalpine Gaul, Caesar had the barely sufficient excuse he needed for his Gallic Wars, fought between 58 and 49.

Caesar defeated large armies at major battles 58 and 57. In 55 and 54 he made two expeditions into Britain, the first Roman to do so. Caesar then defeated a union of Gauls at the Battle of Alesia, completing the Roman conquest of Transalpine Gaul. By 50, all of Gaul lay in Roman hands.

Clodius formed armed gangs that terrorised the city and eventually began to attack Pompey's followers, who in response funded counter-gangs formed by Titus Annius Milo. The political alliance of the triumvirate was crumbling. Domitius Ahenobarbus ran for the consulship in 55 promising to take Caesar's command from him. Eventually, the triumvirate was renewed at Lucca. Pompey and Crassus were promised the consulship in 55, and Caesar's term as governor was extended for five years.
Beginning in the summer of 54, a wave of political corruption and violence swept Rome. This chaos reached a climax in January of 52 BC, when Clodius was murdered in a gang war by Milo.

In 53, Crassus launched a Roman invasion of the Parthian Empire (modern Iraq and Iran). After initial successes, he marched his army deep into the desert; but here his army was cut off deep in enemy territory, surrounded and slaughtered at the Battle of Carrhae in which Crassus himself perished. The death of Crassus removed some of the balance in the Triumvirate and, consequently, Caesar and Pompey began to move apart. While Caesar was fighting in Gaul, Pompey proceeded with a legislative agenda for Rome that revealed that he was at best ambivalent towards Caesar and perhaps now covertly allied with Caesar's political enemies. Pompey's wife, Julia, who was Caesar's daughter, died in childbirth. This event severed the last remaining bond between Pompey and Caesar. In 51, some Roman senators demanded that Caesar not be permitted to stand for consul unless he turned over control of his armies to the state, which would have left Caesar defenceless before his enemies. Caesar chose civil war over laying down his command and facing trial.

On 1 January 49, an agent of Caesar presented an ultimatum to the senate. The ultimatum was rejected, and the senate then passed a resolution which declared that if Caesar did not lay down his arms by July of that year, he would be considered an enemy of the Republic. Meanwhile, the senators adopted Pompey as their new champion against Caesar. On 7 January of 49, the senate passed a "senatus consultum ultimum", which vested Pompey with dictatorial powers. Pompey's army, however, was composed largely of untested conscripts.

On 10 January, Caesar with his veteran army crossed the river Rubicon, the legal boundary of Roman Italy beyond which no commander might bring his army, in violation of Roman laws, and by the spring of 49 swept down the Italian peninsula towards Rome. Caesar's rapid advance forced Pompey, the consuls and the senate to abandon Rome for Greece. Caesar entered the city unopposed. Afterwards Caesar turned his attention to the Pompeian stronghold of Hispania (modern Spain) but decided to tackle Pompey himself in Greece. Pompey initially defeated Caesar, but failed to follow up on the victory, and was decisively defeated at the Battle of Pharsalus in 48, despite outnumbering Caesar's forces two to one, albeit with inferior quality troops. Pompey fled again, this time to Egypt, where he was murdered.

Pompey's death did not end the civil war, as Caesar's many enemies fought on. In 46 Caesar lost perhaps as much as a third of his army, but ultimately came back to defeat the Pompeian army of Metellus Scipio in the Battle of Thapsus, after which the Pompeians retreated yet again to Hispania. Caesar then defeated the combined Pompeian forces at the Battle of Munda.
With Pompey defeated and order restored, Caesar wanted to achieve undisputed control over the government. The powers which he gave himself were later assumed by his imperial successors. His assumption of these powers decreased the authority of Rome's other political institutions.

Caesar held both the dictatorship and the tribunate, and alternated between the consulship and the proconsulship. In 48, Caesar was given permanent tribunician powers. This made his person sacrosanct, gave him the power to veto the senate, and allowed him to dominate the Plebeian Council. In 46, Caesar was given censorial powers, which he used to fill the senate with his own partisans. Caesar then raised the membership of the Senate to 900. This robbed the senatorial aristocracy of its prestige, and made it increasingly subservient to him. While the assemblies continued to meet, he submitted all candidates to the assemblies for election, and all bills to the assemblies for enactment. Thus, the assemblies became powerless and were unable to oppose him.

Near the end of his life, Caesar began to prepare for a war against the Parthian Empire. Since his absence from Rome would limit his ability to install his own consuls, he passed a law before his death which allowed him to appoint all magistrates, and later all consuls and tribunes. This transformed the magistrates from representatives of the people to representatives of the dictator.

Caesar was now the primary figure of the Roman state, enforcing and entrenching his powers. His enemies feared that he had ambitions to become an autocratic ruler. Arguing that the Roman Republic was in danger, a group of senators hatched a conspiracy and assassinated Caesar at a meeting of the Senate in March 44.

Caesar was assassinated on 15 March 44. The assassination was led by Gaius Cassius and Marcus Brutus. Most of the conspirators were senators, who had a variety of economic, political, or personal motivations for carrying out the assassination. Many were afraid that Caesar would soon resurrect the monarchy and declare himself king. Others feared loss of property or prestige as Caesar carried out his land reforms in favor of the landless classes. Virtually all the conspirators fled the city after Caesar's death in fear of retaliation. The civil war that followed destroyed what was left of the Republic.

Mark Antony, Caesar's lieutenant, condemned Caesar's assassination, and war broke out between the two factions. Antony was denounced as a public enemy, and Caesar's adopted son and chosen heir, Gaius Octavianus, was entrusted with the command of the war against him. At the Battle of Mutina Mark Antony was defeated by the consuls Hirtius and Pansa, who were both killed.
Octavian came to terms with Caesarians Antony and Marcus Aemilius Lepidus in 43 when the Second Triumvirate was formed. In 42 Mark Antony and Octavian fought the Battle of Philippi against Caesar's assassins Brutus and Cassius. Although Brutus defeated Octavian, Antony defeated Cassius, who committed suicide. Brutus did likewise soon afterwards.

After the assassination, Marcus Antonius (Mark Antony) formed an alliance with Caesar's adopted son and great-nephew, Gaius Octavianus (Octavian), along with Marcus Lepidus. Known as the Second Triumvirate, they held powers that were nearly identical to the powers that Caesar had held under his constitution. As such, the Senate and assemblies remained powerless, even after Caesar had been assassinated. The conspirators were then defeated at the Battle of Philippi in 42.

However, civil war flared again when the alliance failed. The ambitious Octavian built a power base of patronage and then launched a campaign against Mark Antony. At the naval Battle of Actium in 31 off the coast of Greece, Octavian decisively defeated Antony and Cleopatra of Ptolemaic Egypt. Octavian was granted a series of special powers including sole "imperium" within the city of Rome, permanent consular powers and credit for every Roman military victory, since all future generals were assumed to be acting under his command. In 27 Octavian was granted the use of the names "Augustus", indicating his primary status above all other Romans, "Princeps", which he used to refer to himself as in public, and he adopted the title "Imperator Caesar" making him the first Roman Emperor.

The constitutional history of the Roman Republic began with the revolution which overthrew the monarchy in 509, and ended with constitutional reforms that transformed the Republic into what would effectively be the Roman Empire, in 27. The Constitution of the Roman Republic was a constantly-evolving, unwritten set of guidelines and principles passed down mainly through precedent, by which the government and its politics operated. Throughout the history of the Republic, changes in the constitution were driven by conflicts of interest between the aristocracy and ordinary citizens.

The senate's ultimate authority derived from the esteem and prestige of the senators. This esteem and prestige was based on both precedent and custom, as well as the caliber and reputation of the senators. The senate passed decrees, which were called "senatus consulta". These were officially "advice" from the senate to a magistrate. In practice, however, they were usually followed by the magistrates. The focus of the Roman senate was usually directed towards foreign policy. Though it technically had no official role in the management of military conflict, the senate ultimately was the force that oversaw such affairs. This was due to the senate's explicit power over the state's budget and in military affairs. The power of the senate expanded over time as the power of the legislative assemblies declined, and the senate took a greater role in ordinary law-making. Its members were usually appointed by Roman Censors, who ordinarily selected newly elected magistrates for membership in the senate, making the senate a partially elected body. During times of military emergency, such as the civil wars of the 1st century, this practice became less prevalent, as the Roman Dictator, Triumvir or the senate itself would select its members. Towards the end of the Republic, the senate could enact a "senatus consultus ultimum" in times of emergency, instead of appointing a dictator.

The legal status of Roman citizenship was limited and was a vital prerequisite to possessing many important legal rights such as the right to trial and appeal, to marry, to vote, to hold office, to enter binding contracts, and to special tax exemptions. An adult male citizen with the full complement of legal and political rights was called "optimo jure." The optimo jure elected their assemblies, whereupon the assemblies elected magistrates, enacted legislation, presided over trials in capital cases, declared war and peace, and forged or dissolved treaties. There were two types of legislative assemblies. The first was the "comitia" ("committees"), which were assemblies of all optimo jure. The second was the "concilia" ("councils"), which were assemblies of specific groups of optimo jure.

Citizens were organized on the basis of centuries and tribes, which would each gather into their own assemblies. The Comitia Centuriata ("Centuriate Assembly") was the assembly of the centuries (i.e., soldiers). The president of the Comitia Centuriata was usually a consul. The centuries would vote, one at a time, until a measure received support from a majority of the centuries. The Comitia Centuriata would elect magistrates who had the "imperium" powers (consuls and praetors). It also elected censors. Only the Comitia Centuriata could declare war, and ratify the results of a census. It also served as the highest court of appeal in certain judicial cases.

The assembly of the tribes (i.e., the citizens of Rome), the Comitia Tributa, was presided over by a consul, and was composed of 35 tribes. The tribes were not ethnic or kinship groups, but rather geographical subdivisions. The order that the thirty-five tribes would vote in was selected randomly by lot. Once a measure received support from a majority of the tribes, the voting would end. While it did not pass many laws, the Comitia Tributa did elect quaestors, curule aediles, and military tribunes. The Plebeian Council was identical to the assembly of the tribes, but excluded the patricians. They elected their own officers, plebeian tribunes and plebeian aediles. Usually a plebeian tribune would preside over the assembly. This assembly passed most laws, and could also act as a court of appeal.

Each republican magistrate held certain constitutional powers. Each was assigned a "provincia" by the Senate. This was the scope of that particular office holder's authority. It could apply to a geographic area or to a particular responsibility or task. The powers of a magistrate came from the people of Rome (both plebeians "and" patricians). The "imperium" was held by both consuls and praetors. Strictly speaking, it was the authority to command a military force. In reality, however, it carried broad authority in the other public spheres such as diplomacy, and the justice system. In extreme cases, those with the imperium power were able to sentence Roman Citizens to death. All magistrates also had the power of "coercitio" (coercion). This was used by magistrates to maintain public order by imposing punishment for crimes. Magistrates also had both the power and the duty to look for omens. This power could also be used to obstruct political opponents.

One check on a magistrate's power was called "Collega" (collegiality). Each magisterial office would be held concurrently by at least two people. Another such check was "provocatio". While in Rome, all citizens were protected from coercion, by "provocatio", which was an early form of due process. It was a precursor to "habeas corpus". If any magistrate tried to use the powers of the state against a citizen, that citizen could appeal the decision of the magistrate to a tribune. In addition, once a magistrate's one-year term of office expired, he would have to wait ten years before serving in that office again. This created problems for some consuls and praetors, and these magistrates would occasionally have their "imperium" extended. In effect, they would retain the powers of the office (as a promagistrate), without officially holding that office.

The consuls of the Roman Republic were the highest ranking ordinary magistrates. Each served for one year. They retained several elements of the former kingly regalia, such as the "toga praetexta", and the "fasces", which represented the power to inflict physical punishment. Consular powers included the kings' former "power to command" ("imperium") and appointment of new senators. Consuls had supreme power in both civil and military matters. While in the city of Rome, the consuls were the head of the Roman government. They would preside over the senate and the assemblies. While abroad, each consul would command an army. His authority abroad would be nearly absolute. Praetors administered civil law and commanded provincial armies. Every five years, two censors were elected for an 18-month term, during which they would conduct a census. During the census, they could enroll citizens in the senate, or purge them from the senate. Aediles were officers elected to conduct domestic affairs in Rome, such as managing public games and shows. The quaestors would usually assist the consuls in Rome, and the governors in the provinces. Their duties were often financial.

Since the tribunes were considered to be the embodiment of the plebeians, they were sacrosanct. Their sacrosanctity was enforced by a pledge, taken by the plebeians, to kill any person who harmed or interfered with a tribune during his term of office. It was a capital offense to harm a tribune, to disregard his veto, or to otherwise interfere with him. In times of military emergency, a dictator would be appointed for a term of six months. Constitutional government would be dissolved, and the dictator would be the absolute master of the state. When the dictator's term ended, constitutional government would be restored.

Rome's military secured Rome's territory and borders, and helped to impose tribute on conquered peoples. Rome's armies had a formidable reputation; but Rome also "produced [its] share of incompetents" and catastrophic defeats. Nevertheless, it was generally the fate of Rome's greatest enemies, such as Pyrrhus and Hannibal, to win early battles but lose the war.

During this period, Roman soldiers seem to have been modelled after those of the Etruscans to the north, who themselves are believed to have copied their style of warfare from the Greeks. Traditionally, the introduction of the phalanx formation into the Roman army is ascribed to the city's penultimate king, Servius Tullius (ruled 578–534). According to Livy and Dionysius of Halicarnassus, the front rank was composed of the wealthiest citizens, who were able to purchase the best equipment. Each subsequent rank consisted of those with less wealth and poorer equipment than the one before it. The phalanx was effective in large, open spaces, but not on the hilly terrain of the central Italian peninsula. In the 4th century, the Romans replaced it with the more flexible manipular formation. This change is sometimes attributed to Marcus Furius Camillus and placed shortly after the Gallic invasion of 390; more likely, it was copied from Rome's Samnite enemies to the south, following the Second Samnite War (326–304).

During this period, an army formation of around 5,000 men (of both heavy and light infantry) was known as a legion. The manipular army was based upon social class, age and military experience. "Maniples" were units of 120 men each drawn from a single infantry class. They were typically deployed into three discrete lines based on the three heavy infantry types:


The three infantry classes may have retained some slight parallel to social divisions within Roman society, but at least officially the three lines were based upon age and experience rather than social class. Young, unproven men would serve in the first line, older men with some military experience would serve in the second line, and veteran troops of advanced age and experience would serve in the third line.

The heavy infantry of the maniples were supported by a number of light infantry and cavalry troops, typically 300 horsemen per manipular legion. The cavalry was drawn primarily from the richest class of equestrians. There was an additional class of troops who followed the army without specific martial roles and were deployed to the rear of the third line. Their role in accompanying the army was primarily to supply any vacancies that might occur in the maniples. The light infantry consisted of 1,200 unarmoured skirmishing troops drawn from the youngest and lower social classes. They were armed with a sword and a small shield, as well as several light javelins.

Rome's military confederation with the other peoples of the Italian peninsula meant that half of Rome's army was provided by the Socii, such as the Etruscans, Umbrians, Apulians, Campanians, Samnites, Lucani, Bruttii, and the various southern Greek cities. Polybius states that Rome could draw on 770,000 men at the beginning of the Second Punic War, of which 700,000 were infantry and 70,000 met the requirements for cavalry. Rome's Italian allies would be organized in "alae", or "wings", roughly equal in manpower to the Roman legions, though with 900 cavalry instead of 300.
A small navy had operated at a fairly low level after about 300, but it was massively upgraded about forty years later, during the First Punic War. After a period of frenetic construction, the navy mushroomed to a size of more than 400 ships on the Carthaginian ("Punic") pattern. Once completed, it could accommodate up to 100,000 sailors and embarked troops for battle. The navy thereafter declined in size.

The extraordinary demands of the Punic Wars, in addition to a shortage of manpower, exposed the tactical weaknesses of the manipular legion, at least in the short term. In 217, near the beginning of the Second Punic War, Rome was forced to effectively ignore its long-standing principle that its soldiers must be both citizens and property owners. During the 2nd century, Roman territory saw an overall decline in population, partially due to the huge losses incurred during various wars. This was accompanied by severe social stresses and the greater collapse of the middle classes. As a result, the Roman state was forced to arm its soldiers at the expense of the state, which it did not have to do in the past.

The distinction between the heavy infantry types began to blur, perhaps because the state was now assuming the responsibility of providing standard-issue equipment. In addition, the shortage of available manpower led to a greater burden being placed upon Rome's allies for the provision of allied troops. Eventually, the Romans were forced to begin hiring mercenaries to fight alongside the legions.

In a process known as the Marian reforms, Roman consul Gaius Marius carried out a programme of reform of the Roman military. In 107, all citizens, regardless of their wealth or social class, were made eligible for entry into the Roman army. This move formalised and concluded a gradual process that had been growing for centuries, of removing property requirements for military service. The distinction among the three heavy infantry classes, which had already become blurred, had collapsed into a single class of heavy legionary infantry. The heavy infantry legionaries were drawn from citizen stock, while non-citizens came to dominate the ranks of the light infantry. The army's higher-level officers and commanders were still drawn exclusively from the Roman aristocracy.

Unlike earlier in the Republic, legionaries were no longer fighting on a seasonal basis to protect their land. Instead, they received standard pay, and were employed by the state on a fixed-term basis. As a consequence, military duty began to appeal most to the poorest sections of society, to whom a salaried pay was attractive. A destabilising consequence of this development was that the proletariat "acquired a stronger and more elevated position" within the state.

The legions of the late Republic were almost entirely heavy infantry. The main legionary sub-unit was a "cohort" of approximately 480 infantrymen, further divided into six centuries of 80 men each. Each century comprised 10 "tent groups" of 8 men. Cavalry were used as scouts and dispatch riders, rather than as battlefield forces. Legions also contained a dedicated group of artillery crew of perhaps 60 men. Each legion was normally partnered with an approximately equal number of allied (non-Roman) troops.

The army's most obvious deficiency lay in its shortage of cavalry, especially heavy cavalry. Particularly in the East, Rome's slow-moving infantry legions were often confronted by fast-moving cavalry-troops, and found themselves at a tactical disadvantage.

Following Rome's subjugation of the Mediterranean, its navy declined in size although it would undergo short-term upgrading and revitalisation in the late Republic to meet several new demands. Julius Caesar assembled a fleet to cross the English Channel and invade "Britannia". Pompey raised a fleet to deal with the Cilician pirates who threatened Rome's Mediterranean trading routes. During the civil war that followed, as many as a thousand ships were either constructed or pressed into service from Greek cities.

Citizen families were headed by the family's oldest male, the "pater familias", who was lawfully entitled to exercise complete authority ("patria potestas") over family property and all family members. Brutus, co-founder of the Republic, is supposed to have exercised the extreme form of this right when he executed his own sons for treachery. Citizenship offered legal protection and rights, but citizens who offended Rome's traditional moral code could be declared infamous, and lose certain legal and social privileges. Citizenship was also taxable, and undischarged debt was potentially a capital offence. A form of limited, theoretically voluntary slavery (debt bondage, or nexum) allowed wealthy creditors to negotiate payment of debt through bonded service. Poor, landless citizens of the lowest class ("proletarii") might contract their sons to a creditor, patron or third party employer to obtain an income, or to pay off family debts. "Nexum" was only abolished when slave labour became more readily available, most notably during the Punic wars.
Slaves were simultaneously family members and family property. They could be bought, sold, acquired through warfare, or born and raised within their master's household. They could also buy their freedom with money saved or the offer of future services as a freedman or woman, and their sons could be eligible for citizenship; this degree of social mobility was unusual in the ancient world. Freed slaves and the master who freed them retained certain legal and moral mutual obligations. This was the bottom rung of one of Rome's fundamental social and economic institutions, the client-patron relationship. At the top rung were the senatorial families of the landowning nobility, both patrician and plebeian, bound by shifting allegiances and mutual competition. A plebiscite of 218 forbade senators and their sons to engage in substantial trade or money-lending. A wealthy equestrian class emerged, not subject to the same trading constraints as the senate.

Citizen men and citizen women were expected to marry, produce as many children as possible, and improve – or at worst, conserve – their family's wealth, fortune, and public profile. Marriage offered opportunities for political alliance and social advancement. Patricians usually married in a form known as "confarreatio", which transferred the bride from her father's absolute control or "hand" ("manus") to that of her husband. Patrician status could only be inherited through birth; an early law, introduced by the reactionary Decemviri but rescinded in 445, sought to prevent marriages between patricians and plebeians; any resulting offspring may not have been legally recognised. Among ordinary plebeians, different marriage forms offered married women considerable more freedom than their patrician counterparts, until "manus" marriage was replaced by "free marriage", in which the wife remained under the legal authority of her absent father, not her husband. Infant mortality was high. Towards the end of the Republic, the birthrate began to fall among the elite. Some wealthy, childless citizens resorted to adoption to provide male heirs for their estates, and to forge political alliances. Adoption was subject to the senate's approval; the notoriously unconventional patrician politician Publius Clodius Pulcher had himself and his family adopted into a plebeian clan, so that he could hold a plebeian tribunate.

The Republic was created during a time of warfare, economic recession, food shortages, and plebeian debt. In wartime, plebeian farmers were liable to conscription. In peacetime, most depended on whatever cereal crops they could produce on small farming plots, allotted to them by the state, or by patrons. Soil fertility varied from place to place, and natural water sources were unevenly distributed throughout the landscape. In good years, a pleb small-holder might trade a small surplus, to meet his family's needs, or to buy the armatures required for his military service. In other years, crop failure through soil exhaustion, adverse weather, disease or military incursions could lead to poverty, unsupported borrowing, and debt. Nobles invested much of their wealth in ever-larger, more efficient farming units, exploiting a range of soil conditions though mixed farming techniques. As farming was labour-intensive, and military conscription reduced the pool of available manpower, over time the wealthy became ever more reliant upon the increasingly plentiful slave-labour provided by successful military campaigns. Well managed agricultural estates helped provide for clients and dependents, support an urban family home, and fund the owner's public and military career. Large estates yielded cash for bribes, and security for borrowing. Later Roman moralists idealised farming as an intrinsically noble occupation: Cincinnatus left off his ploughing reluctantly, to serve as dictator, and returned once his state duties were done.

In law, land taken by conquest was "ager publicus" (public land). In practise, much of it was exploited by the nobility, using slaves rather than free labour. Rome's expansionist wars and colonisations were at least partly driven by the land-hunger of displaced peasants, who must otherwise join the swelling, dependent population of urban "plebs". At the end of the second Punic War, Rome added the fertile "ager Campanus", suitable for intense cultivation of vines, olives and cereals. Like the grain-fields of Sicily – seized after the same conflict – it was likely farmed extra-legally by leading landowners, using slave-gangs. A portion of Sicily's grain harvest was sent to Rome as tribute, for redistribution by the "aediles". The urban "plebs" increasingly relied on firstly subsidised, then free grain.
With the introduction of aqueducts (from 312), suburban market-farms could be supplied with run-off or waste aqueduct water. Perishable commodities such as flowers (for perfumes, and festival garlands), fresh grapes, vegetables and orchard fruits, and small livestock such as pigs and chickens, could be farmed close to municipal and urban markets. In the early 2nd century Cato the Elder tried to block the illicit tapping of rural aqueducts by the elite, who thus exploited the increased productivity of cheaply bought, formerly "dry" farmland; a law was duly passed, but fines for abuses, and taxes on profits, proved more realistic solutions than an outright ban. Food surpluses, no matter how obtained, kept prices low. Faced with increasing competition from provincial and allied grain suppliers, many Roman farmers turned to more profitable crops, especially grapes for wine production. By the late Republican era, Roman wine had been transformed from an indifferent local product for local consumption, to a major domestic and export commodity.

Roman writers have little to say about large-scale stock-breeding, but make passing references to its profitability. Drummond speculates that this might reflect elite preoccupations with historical grain famines, or long-standing competition between agriculturalists and pastoralists. While agriculture was a seasonal practise, pasturage was a year-round requirement. Some of Republican Rome's early agricultural legislation sought to balance the competing public grazing rights of small farmers, the farming elite, and transhumant pastoralists, who maintained an ancient right to herd, graze and water their animals between low-lying winter pastures and upland summer pastures. From the early second century, transhumance was practised on a vast scale, as an investment opportunity. Though meat and hides were valuable by products of stock-raising, cattle were primarily reared to pull carts and ploughs, and sheep were bred for their wool, the mainstay of the Roman clothing industry. Horses, mules and donkeys were bred as civil and military transport. Pigs bred prolifically, and could be raised at little cost by any small farmer with rights to pannage. Their central dietary role is reflected by their use as sacrificial victims in domestic cults, funerals, and cults to agricultural deities.

Republican Rome's religious practises harked back to Rome's quasi-mythical history. Romulus, a son of Mars, founded Rome after Jupiter granted him favourable bird-signs regarding the site. Numa Pompilius, second king of Rome, had established Rome's basic religious and political institutions after direct instructions from the gods, given through augury, dreams and oracle. Each king thereafter was credited with some form of divinely approved innovation, adaptation or reform. An Imperial-era source claims that the Republic's first consul, Brutus, effectively abolished human sacrifice to the goddess Mania, instituted by the last king, Tarquinius.

Romans acknowledged the existence of innumerable deities who controlled the natural world and human affairs. Every individual, occupation and location had a protective tutelary deity, or sometimes several. Each was associated with a particular, highly prescriptive form of prayer and sacrifice. Piety ("pietas") was the correct, dutiful and timely performance of such actions. The well-being of each Roman household was thought to depend on daily cult to its Lares and Penates (guardian deities, or spirits), ancestors, and the divine generative essence embodied within its "pater familias". A family which neglected its religious responsibilities could not expect to prosper.

The well-being of the Roman state depended on its state deities, whose opinions and will could be discerned by priests and magistrates, trained in augury, haruspicy, oracles and the interpretation of omens. Impieties in state religion could produce expressions of divine wrath such as social unrest, wars, famines and epidemics, vitiate the political process, render elections null and void, and lead to the abandonment of planned treaties, wars and any government business. Accidental errors could be remedied by repeating the rite correctly, or by an additional sacrifice; outright sacrilege threatened the bonds between the human and divine, and carried the death penalty. As divine retribution was invoked in the lawful swearing of oaths and vows, oath-breakers forfeited their right to divine protection, and might be killed with impunity.

Roman religious authorities were unconcerned with personal beliefs or privately funded cults, unless they offended natural or divine laws, or undermined the "mos maiorum" (roughly, "the way of the ancestors"); the relationship between gods and mortals should be sober, contractual, and of mutual benefit. Undignified grovelling, excessive enthusiasm ("superstitio") and secretive practises were "weak minded" and morally suspect. Magical practises were officially banned, as attempts to subvert the will of the gods for personal gain, but were probably common among all classes. Private cult organisations that seemed to threaten Rome's political and priestly hierarchy were investigated by the Senate, with advice from the priestly colleges. The Republic's most notable religious suppression was that of the Bacchanalia, a widespread, unofficial, enthusiastic cult to the Greek wine-god Bacchus. The cult organisation was ferociously suppressed, and its deity was absorbed within the official cult to Rome's own wine-god, Liber. The official recognition, adoption and supervision of foreign deities and practices, whether Etruscan, Sabine, Latin or colonial Greek, had been an important unitary feature in Rome's territorial expansion and dominance since the days of the kings. For example, king Servius Tullius had established an Aventine temple to Diana as a Roman focus for the Latin League.

The gods were thought to communicate their wrath ("ira deorum") through prodigies (unnatural or aberrant phenomena). During the crisis of the Second Punic War an unprecedented number of reported prodigies were expiated, in more than twenty days of public ritual and sacrifices. In the same period, Rome recruited the "Trojan" Magna Mater (Great Mother of the Gods) to the Roman cause, "Hellenised" the native Roman cult to Ceres; and took control of the Bacchanalia festival in Rome and its allied territories. Following Rome's disastrous defeat at Cannae, the State's most prominent written oracle recommended the living burial of human victims in the Forum Boarium to placate the gods. Livy describes this "bloodless" human sacrifice as an abhorrent but pious necessity; Rome's eventual victory confirmed the gods' approval.

Starting in the mid-Republican era, some leading Romans publicly displayed special, sometimes even intimate relationships with particular deities. For instance, Scipio Africanus claimed Jupiter as a personal mentor. Some gentes claimed a divine descent, often thanks to a false etymology of their name; the Caecilii Metelli pretended to descend from Vulcan through his son Caeculus, the Mamilii from Circe through her granddaughter Mamilia, the Julii Caesares and the Aemilii from Venus through her grandsons Iulus and Aemylos. In the 1st century, Sulla, Pompey, and Caesar made competing claims for Venus' favour.

With the abolition of monarchy, some of its sacral duties were shared by the consuls, while others passed to a Republican "rex sacrorum" (king of the sacred rites"), a patrician "king", elected for life, with great prestige but no executive or kingly powers. Rome had no specifically priestly class or caste. As every family's "pater familias" was responsible for his family's cult activities, he was effectively the senior priest of his own household. Likewise, most priests of public cult were expected to marry, produce children, and support their families. In the early Republic the patricians, as "fathers" to the Roman people, claimed the right of seniority to lead and control the state's relationship with the divine. Patrician families, in particular the "Cornelii", "Postumii" and "Valerii", monopolised the leading state priesthoods: the "flamines" of Jupiter, Mars and Quirinus, as well as the "pontifices". The patrician "Flamen Dialis" employed the "greater auspices" ("auspicia maiora") to consult with Jupiter on significant matters of State.

Twelve "lesser flaminates" ("Flamines minores"), were open to plebeians, or reserved to them. They included a "Flamen Cerealis" in service of Ceres, goddess of grain and growth, and protector of plebeian laws and tribunes. The plebs had their own forms of augury, which they credited to Marsyas, a satyr or silen in the entourage of Liber, plebeian god of grapes, wine, freedom and male fertility. The priesthoods of local urban and rustic Compitalia street-festivals, dedicated to the Lares of local communities, were open to freedmen and slaves, to whom "even the heavy-handed Cato recommended liberality during the festival"; so that the slaves, "being softened by this instance of humanity, which has something great and solemn about it, may make themselves more agreeable to their masters and be less sensible of the severity of their condition".

The "Lex Ogulnia" (300) gave patricians and plebeians more-or-less equal representation in the augural and pontifical colleges; other important priesthoods, such as the Quindecimviri ("The Fifteen"), and the "epulones" were opened to any member of the senatorial class. To restrain the accumulation and potential abuse of priestly powers, each "gens" was permitted one priesthood at any given time, and the religious activities of senators were monitored by the censors. Magistrates who held an augurate could claim divine authority for their position and policies. In the late Republic, augury came under the control of the "pontifices", whose powers were increasingly woven into the civil and military "cursus honorum". Eventually, the office of "pontifex maximus" became a "de facto" consular prerogative.

Some cults may have been exclusively female; for example, the rites of the Good Goddess ("Bona Dea"). Towards the end of the second Punic War, Rome rewarded priestesses of Demeter from "Graeca Magna" with Roman citizenship for training respectable, leading matrons as"sacerdotes" of "Greek rites" to Ceres. Every matron of a family (the wife of its "pater familias" had a religious duty to maintain the household fire, which was considered an extension of Vesta's sacred fire, tended in perpetuity by the chaste Vestal Virgins. The Vestals also made the sacrificial "mola salsa" employed in many State rituals, and represent an essential link between domestic and state religion. Rome's survival was thought to depend on their sacred status and ritual purity. Vestals found guilty of inchastity were "willingly" buried alive, to expiate their offence and avoid the imposition of blood-guilt on those who inflicted the punishment.

Rome's major public temples were contained within the city's sacred, augural boundary ("pomerium"), which had supposedly been marked out by Romulus, with Jupiter's approval. The Temple of Jupiter Optimus Maximus ("Jupiter, Best and Greatest") stood on the Capitoline Hill. Among the settled areas outside the "pomerium" was the nearby Aventine Hill. It was traditionally associated with Romulus' unfortunate twin, Remus, and in later history with the Latins, and the Roman "plebs". The Aventine seems to have functioned as a place for the introduction of "foreign" deities. In 392, Camillus established a temple there to Juno Regina, Etruscan Veii's protective goddess. Later introductions include Summanus, c. 278, Vortumnus c. 264, and at some time before the end of the 3rd century, Minerva. While Ceres' Aventine temple was most likely built at patrician expense, to mollify the "plebs", the patricians brought the Magna Mater ("Great mother of the Gods") to Rome as their own "Trojan" ancestral goddess, and installed her on the Palatine, along with her distinctively "un-Roman" Galli priesthood.

Romulus was said to have pitched his augural tent atop the Palatine. Beneath its southern slopes ran the sacred way, next to the former palace of the kings (Regia), the House of the Vestals and Temple of Vesta. Close by were the Lupercal shrine and the cave where Romulus and Remus were said to have been suckled by the she-wolf. On the flat area between the Aventine and Palatine was the Circus Maximus, which hosted chariot races and religious games. Its several shrines and temples included those to Rome's indigenous sun-god, Sol, the moon-goddess Luna, the grain-storage god, Consus, and the obscure goddess Murcia. A temple to Hercules stood in the Forum Boarium, near the Circus starting gate. Every district ("Vicus") of the city had a crossroads shrine to its own protective Lares.

Whereas Republican (and thereafter, Imperial) Romans marked the passage of years with the names of their ruling consuls, their calendars marked the anniversaries of religious foundations to particular deities, the days when official business was permitted ("fas"), and those when it was not ("nefas"). The Romans observed an eight-day week; markets were held on the ninth day. Each month was presided over by a particular, usually major deity. The oldest calendars were lunar, structured around the most significant periods in the agricultural cycle, and the religious duties required to yield a good harvest.

Before any campaign or battle, Roman commanders took auspices, or haruspices, to seek the gods' opinion regarding the likely outcome. Military success was achieved through a combination of personal and collective "virtus" (roughly, "manly virtue") and divine will. Triumphal generals dressed as Jupiter Capitolinus, and laid their victor's laurels at his feet. Religious negligence, or lack of "virtus", provoked divine wrath and led to military disaster. Military oaths dedicated the oath-takers life to Rome's gods and people; defeated soldiers were expected to take their own lives, rather than survive as captives. Examples of "devotio", as performed by the Decii Mures, in which soldiers offered and gave their lives to the "Di inferi" (gods of the underworld) in exchange for Roman victory were celebrated as the highest good.

Some of Republican Rome's leading deities were acquired through military actions. In the earliest years of the Republic, Camillus promised Veii's goddess Juno a temple in Rome as incentive for her desertion "(evocatio)". He conquered the city in her name, brought her cult statue to Rome "with miraculous ease" and dedicated a temple to her on the Aventine Hill. The first known temple to Venus was built to fulfil a vow made by Q. Fabius Gurges during battle against the Samnites. Following Rome's disastrous defeat by Carthage in the Battle of Lake Trasimene (217), Rome laid siege to Eryx, a Sicillian ally of Carthage. The city's patron deity, whom the Romans recognised as a warlike version of Venus, was "persuaded" to change her allegiance and was rewarded with a magnificent temple on the Capitoline Hill, as one of Rome's twelve Dii consentes. "Venus Victrix" was thought to grant her favourites a relatively easy victory, worthy of an ovation and myrtle crown.

Life in the Roman Republic revolved around the city of Rome, and its seven hills. The most important governing, administrative and religious institutions were concentrated at its heart, on and around the Capitoline and Palatine Hills. The city rapidly outgrew its original sacred boundary ("pomerium"), and its first city walls. Further growth was constrained by an inadequate fresh-water supply. Rome's first aqueduct (312) built during the Punic wars crisis, provided a plentiful, clean supply. The building of further aqueducts led to the city's expansion and the establishment of public baths ("thermae") as a central feature of Roman culture. The city also had several theatres, gymnasiums, and many taverns and brothels. Living space was at a premium. Some ordinary citizens and freedmen of middling income might live in modest houses but most of the population lived in apartment blocks ("insulae," literally "islands"), where the better-off might rent an entire ground floor, and the poorest a single, possibly windowless room at the top, with few or no amenities. Nobles and rich patrons lived in spacious, well-appointed town houses; they were expected to keep "open house" for their peers and clients. A semi-public "atrium" typically functioned as a meeting-space, and a vehicle for display of wealth, artistic taste, and religious piety. Noble "atria" were also display areas for ancestor-masks ("imagines").

Most Roman towns and cities had a forum and temples, as did the city of Rome itself. Aqueducts brought water to urban centres. Landlords generally resided in cities and left their estates in the care of farm managers.

The basic Roman garment was the Greek-style tunic, worn knee-length and short-sleeved (or sleeveless) for men and boys, and ankle-length and long-sleeved for women and girls. The toga was distinctively Roman. It was thought to have begun during the early Roman kingdom, as a plain woolen "shepherd's wrap", worn by both sexes, all classes, and all occupations, including the military. By the middle to late Republic, citizen women had abandoned it for the less bulky, Greek-style stola, and the military used it only for off-duty ceremonies. The toga became a mark of male citizenship, a statement of social degree. Convention also dictated the type, colour and style of "calcei" (ankle-boots) appropriate to each level of male citizenship; red for senators, brown with crescent-shaped buckles for equites, and plain tanned for "plebs".

The whitest, most voluminous togas were worn by the senatorial class. High ranking magistrates, priests and citizen's children were entitled to a purple-bordered "toga praetexta". Triumphal generals wore an all-purple, gold-embroidered toga picta, associated with the image of Jupiter and Rome's former kings – but only for a single day; Republican mores simultaneously fostered competitive display and attempted its containment, to preserve at least a notional equality between peers, and reduce the potential threats of class envy. Togas, however, were impractical for physical activities other than sitting in the theatre, public oratory, and attending the "salutiones" ("greeting sessions") of rich patrons. Most Roman citizens, particularly the lower class of plebs, seem to have opted for more comfortable and practical garments, such as tunics and cloaks.

Luxurious and highly coloured clothing had always been available to those who could afford it, particularly women of the leisured classes. There is material evidence for cloth-of-gold (lamé) as early as the 7th century. By the 3rd century, significant quantities of raw silk was being imported from China. The "Lex Oppia" (215), which restricted personal expenditure on such luxuries as purple clothing, was repealed in 195, after a mass public protest by wealthy Roman matrons. Tyrian purple, as a quasi-sacred colour, was officially reserved for the border of the "toga praetexta" and for the solid purple "toga picta"; but towards the end of the Republic, the notorious Verres was wearing a purple "pallium" at all-night parties, not long before his trial, disgrace and exile for corruption.

For most Romans, even the simplest, cheapest linen or woolen clothing represented a major expense. Worn clothing was passed down the social scale until it fell to rags, and these in turn were used for patchwork. Wool and linen were the mainstays of Roman clothing, idealised by Roman moralists as simple and frugal. Landowners were advised that female slaves not otherwise occupied should be producing homespun woolen cloth, good enough for clothing the better class of slave or supervisor. Cato the Elder recommended that slaves be given a new cloak and tunic every two years; coarse rustic homespun would likely be "too good" for the lowest class of slave, but not good enough for their masters. For most women, the carding, combing, spinning and weaving of wool were part of daily housekeeping, either for family use or for sale. In traditionalist, wealthy households, the family's wool-baskets, spindles and looms were positioned in the semi-public reception area ("atrium"), where the mater familias and her familia could thus demonstrate their industry and frugality; a largely symbolic and moral activity for those of their class, rather than practical necessity.

As the Republic wore on, its trade, territories and wealth increased. Roman conservatives deplored the apparent erosion of traditional, class-based dress distinctions, and an increasing Roman appetite for luxurious fabrics and exotic "foreign" styles among all classes, including their own. Towards the end of the Republic, the ultra-traditionalist Cato the younger publicly protested the self-indulgent greed and ambition of his peers, and the loss of Republican "manly virtues", by wearing a "skimpy" dark woolen toga, without tunic or footwear.

Modern study of the dietary habits during the Republic are hampered by various factors. Few writings have survived, and because different components of their diet are more or less likely to be preserved, the archaeological record cannot be relied on. Cato the elder's "De Agri Cultura" includes several recipes and his suggested "Rations for the hands". The list of ingredients includes cheese, honey, poppy seeds, coriander, fennel, cumin, egg, olives, bay leaves, laurel twig, and anise. He gives instructions for kneading bread, making porridge, Placenta cake, brine, various wines, preserving lentils, planting asparagus, curing ham, and fattening geese and squab. The Roman poet Horace mentions another Roman favorite, the olive, in reference to his own diet, which he describes as very simple: "As for me, olives, endives, and smooth mallows provide sustenance." Meat, fish and produce were a part of the Roman diet at all levels of society.

Romans valued fresh fruit, and had a diverse variety available to them. Wine was considered the basic drink, consumed at all meals and occasions by all classes and was quite inexpensive. Cato once advised cutting his rations in half to conserve wine for the workforce. Many types of drinks involving grapes and honey were consumed as well. Drinking on an empty stomach was regarded as boorish and a sure sign for alcoholism, the debilitating physical and psychological effects of which were known to the Romans. Accusations of alcoholism were used to discredit political rivals. Prominent Roman alcoholics included Marcus Antonius, and Cicero's own son Marcus (Cicero Minor). Even Cato the Younger was known to be a heavy drinker.

Rome's original native language was early Latin, the language of the Italic Latins. Most surviving Latin literature is written in Classical Latin, a highly stylised and polished literary language which developed from early and vernacular spoken Latin, from the 1st century. Most Latin speakers used Vulgar Latin, which significantly differed from Classical Latin in grammar, vocabulary, and eventually pronunciation.

Following various military conquests in the Greek East, Romans adapted a number of Greek educational precepts to their own fledgling system. Strenuous, disciplined physical training helped prepare boys of citizen class for their eventual citizenship and a military career. Girls generally received instruction from their mothers in the art of spinning, weaving, and sewing.
Schooling in a more formal sense was begun around 200. Education began at the age of around six, and in the next six to seven years, boys and girls were expected to learn the basics of reading, writing and counting. By the age of twelve, they would be learning Latin, Greek, grammar and literature, followed by training for public speaking. Effective oratory and good Latin were highly valued among the elite, and were essential to a career in law or politics.

In the 3rd century, Greek art taken as the spoils of war became popular, and many Roman homes were decorated with landscapes by Greek artists.

Over time, Roman architecture was modified as their urban requirements changed, and the civil engineering and building construction technology became developed and refined. The architectural style of the capital city was emulated by other urban centers under Roman control and influence. 

Early Roman literature was influenced heavily by Greek authors. From the mid-Republic, Roman authors followed Greek models, to produce free-verse and verse-form plays and other in Latin; for example, Livius Andronicus wrote tragedies and comedies. The earliest Latin works to have survived intact are the comedies of Plautus, written during the mid-Republic. Works of well-known, popular playwrights were sometimes commissioned for performance at religious festivals; many of these were Satyr plays, based on Greek models and Greek myths. The poet Naevius may be said to have written the first Roman epic poem, although Ennius was the first Roman poet to write an epic in an adapted Latin hexameter. However, only fragments of Ennius' epic, the "Annales", have survived, yet both Naevius and Ennius influenced later Latin epic, especially Virgil's Aeneid. Lucretius, in his "On the Nature of Things" explicated the tenets of Epicurean philosophy.

The politician, poet and philosopher Cicero's literary output was remarkably prolific and so influential on contemporary and later literature that the period from 83BC to 43BC has been called the 'Age of Cicero'. His oratory set new standards for centuries, and continue to influence modern speakers, while his philosophical works, which were, for the most part, Cicero's Latin adaptations of Greek Platonic and Epicurean works influenced many later philosophers. Other prominent writers of this period include the grammarian and historian of religion Varro, the politician, general and military commentator Julius Caesar, the historian Sallust and the love poet Catullus.

The city of Rome had a place called the Campus Martius ("Field of Mars"), which was a sort of drill ground for Roman soldiers. Later, the Campus became Rome's track and field playground. In the campus, the youth assembled to play and exercise, which included jumping, wrestling, boxing and racing. Equestrian sports, throwing, and swimming were also preferred physical activities. In the countryside, pastimes included fishing and hunting. Board games played in Rome included dice (Tesserae or Tali), Roman Chess (Latrunculi), Roman Checkers (Calculi), Tic-tac-toe (Terni Lapilli), and Ludus duodecim scriptorum and Tabula, predecessors of backgammon. Other activities included chariot races, and musical and theatrical performances. 






</doc>
<doc id="25817" url="https://en.wikipedia.org/wiki?curid=25817" title="RUF">
RUF

Ruf or RUF may refer to:



</doc>
<doc id="25819" url="https://en.wikipedia.org/wiki?curid=25819" title="Rickenbacker">
Rickenbacker

Rickenbacker International Corporation is a string instrument manufacturer based in Santa Ana, California. The company is credited as the first known maker of electric guitars —in 1932—and eventually produced a range of electric guitars and bass guitars. Rickenbacker twelve string guitars were favored by The Beatles, Roger McGuinn of The Byrds, and Gerry Marsden of Gerry and the Pacemakers. Players of the six string include John Fogerty of Creedence Clearwater Revival, John Kay of Steppenwolf and Tom Petty of Tom Petty and the Heartbreakers. Players who have used Rickenbacker basses include Paul McCartney, Lemmy Kilmister of Motörhead, Cliff Burton of Metallica, Glenn Hughes of Deep Purple, Paul Wilson of Snow Patrol, Chris Squire of Yes (band) and Geddy Lee of Rush.

Current line of products manufactured include electric and acoustic guitars and basses.

Adolph Rickenbacher and George Beauchamp founded the company in 1931 as the Ro-Pat-In Corporation (ElectRo-Patent-Instruments) to sell electric Hawaiian guitars. Beauchamp had designed these instruments, assisted by Paul Barth and Harry Watson, at "National String Instrument Corporation". They chose the brand name "Rickenbacher" (later changing the spelling to "Rickenbacker"). Early examples bear the brand name "Electro".

The early instruments were nicknamed "frying-pans" because of their long necks and small circular bodies. They are the first known solid-bodied electric guitars, though they were a lap-steel type. They had a single pickup with two magnetized steel covers, shaped like "horse shoes," that arched over the strings. By the time they ceased producing the "frying pan" model in 1939, they had made several thousand units.

Electro String also sold amplifiers to go with their guitars. A Los Angeles radio manufacturer named Van Nest designed the first Electro String production-model amplifier. Shortly thereafter, design engineer Ralph Robertson further developed the amplifiers, and by the 1940s at least four different Rickenbacker models were available. James B. Lansing of the Lansing Manufacturing Company designed the speaker in the Rickenbacker professional model. During the early 1940s, Rickenbacker amps were sometimes repaired by Leo Fender, whose repair shop evolved into the Fender Electric Instrument Manufacturing Company.

George Beauchamp was a vaudeville performer, violinist, and steel guitarist who, like many acoustic guitarists in the pre-electric-guitar 1920s, was looking for some way to make his instrument cut through an orchestra. He first conceived of a guitar fitted with a phonograph-like amplifying horn. He approached inventor and violin-maker John Dopyera, who made a prototype that was, by all accounts, a failure. Their next collaboration involved experiments with mounting three conical aluminum resonators into the body of the guitar beneath the bridge. These efforts produced an instrument that so pleased Beauchamp that he told Dopyera that they should go into business to manufacture them. After further refinements, Dopyera applied for a patent on the so-called tri-cone guitar on April 9, 1927. Thereafter, Dopyera and his brothers made the tri-cone guitars in their Los Angeles shop, under the brand name "National". On January 26, 1928, the National String Instrument Corporation opened, with a new factory located near a metal-stamping shop owned by Adolph Rickenbacher and staffed by experienced and competent craftsmen. The company made Spanish and Hawaiian style tri-cone guitars as well as four-string tenor guitars, mandolins, and ukuleles.

Adolph Rickenbacher was born in Basel, Switzerland in 1887 and emigrated to the United States to live with relatives after the death of his parents. Sometime after moving to Los Angeles in 1918, he changed his surname to "Rickenbacker". In 1925, Rickenbacker and two partners formed the Rickenbacker Manufacturing Company and incorporated it in 1927. By the time he met George Beauchamp and began manufacturing metal bodies for the "Nationals" being produced by the National String Instruments Corporation, Rickenbacker was a highly skilled production engineer and machinist. Adolph Rickenbacher became a shareholder in National and, with the assistance of his Rickenbacker Manufacturing Company, National boosted production to fifty guitars a day.

Unfortunately, National's line of instruments was not well diversified and, as demand for the expensive and hard-to-manufacture tri-cone guitars began to slip, the company realized that it would need to produce instruments with a lower production cost to remain competitive. Dissatisfaction with what John Dopyera felt was mismanagement led him to resign from National in January 1929. He subsequently formed the Dobro Manufacturing Corporation, later called Dobro Corporation, Ltd, and began to manufacture his own line of resonator-equipped instruments (dobros). Patent infringement disagreements between National and Dobro led to a lawsuit in 1929, with Dobro suing National for $2 million in damages. Problems within National's management as well as pressure from the deepening Great Depression led to a production slowdown at National. This ultimately resulted in part of the company's fractured management structure organizing support for George Beauchamp's newest project: development of a fully electric guitar.

By the late twenties, the idea for electrified string instruments had been around for some time, and experimental banjo, violin, and guitar pickups had been developed. George Beauchamp had experimented with electric amplification as early as 1925, but his early efforts, which used microphones, did not produce the effect he desired. Beauchamp also pursued the idea, building a one-string test guitar out of a 2X4 piece of lumber and an electric phonograph pickup. As problems at National became more apparent, Beauchamp's home experiments became more rigorous, and he began to attend night classes in electronics and collaborate with fellow National employee Paul Barth. When they finally developed a prototype electric pickup that met their satisfaction, Beauchamp asked former National shop craftsman Harry Watson to make a wooden neck and body to hold the pickup. Somebody nicknamed it the "fry-pan" because of its shape, though Rickenbacker liked to call it the pancake. The final design Beauchamp and Barth developed was an electric pickup consisting of a pair of horseshoe-shaped magnets that enclosed the pickup coil and completely surrounded the strings.

At the end of 1931, Beauchamp, Barth, Rickenbacher and several other individuals banded together and formed the Ro-Pat-In Corporation (elekt"RO"-"PAT"ent-"IN"struments) to manufacture and distribute electrically amplified musical instruments, with an emphasis on their newly developed A-25 Hawaiian Guitar, often referred to as the "fry-pan" lap-steel electric guitar as well as an Electric Spanish (standard) model and companion amplifiers. In the summer of 1932, Ro-Pat-In began to manufacture cast aluminum production versions of the Fry-Pan as well as a lesser number of standard Spanish Electrics also known as "Electro-Spanish" models, built from wooden bodies similar to those made in Chicago for the National Company. These instruments constitute the origin of the electric guitar by virtue of their string-driven electro-magnetic pick-ups. In 1933 the Ro-Pat-In company's name was changed to Electro String Instrument Corporation and its instruments labeled simply as "Electro". In 1934 the name of Rickenbacher" was added in honor of the company's principal partner, Adolph Rickenbacker.

During the early production of the A-22 Fry-Pan, Beauchamp and Rickenbacher would experiment with wooden-bodied Spanish guitars and solid body prototypes; ultimately giving birth to the Electro-Spanish Model B and the Electro-Spanish Ken Roberts. Both models had been experimental, produced as early as 1931, and officially released in 1935. The Electro-Spanish Ken Roberts model was subject to a limited production of forty-six. There were several new design elements found on the Electro-Spanish Ken Roberts. The instrument was the first of its kind to be named for an endorser. While most arch-top guitars had 14-fret neck joints, the Electro-Spanish Ken Roberts fingerboard joined the body at the 17th fret allowing much greater access to the higher frets, creating a full 25-1/2" inch scale. This addition made the Electro-Spanish Ken Roberts the first production full scale (25-1/2") electrified guitar.

Another new feature on the Electro-Spanish Ken Roberts is the stock Kauffman Vib-rola tailpiece, the world's first patented tremolo (US Patent: US2241911A). The Ken Roberts is the first instrument of any type to feature a hand-operated vibrato as standard equipment. It also marks Rickenbacker's first link to the unit's originator, Clayton Doc Kauffman, who would become a design collaborator for the company a couple of years later.

In 1935, the company introduced several new models including the Model "B" Electric Spanish guitar, which is the first known solid body electric guitar. Because the original aluminum Fry-Pans were susceptible to tuning problems from expansion of the metal under hot performing lights, they made many of the new models from cast Bakelite, an early synthetic plastic used in bowling balls.

Rickenbacker continued to specialize in steel guitars well into the 1950s, but with the advent of rock and roll, F.C. Hall, owner of Radio & Television Equipment Co. (Radio-Tel), purchased the Electro String Company from Adolph Rickenbacker in 1953. Hall overhauled the business and began focusing on standard electric and acoustic guitars rather than the steel guitars the company pioneered. In 1956, Rickenbacker introduced two instruments with the "neck through body" construction that became a standard feature of many of the company's products, including the Combo 400 guitar, the model 4000 bass, and, later, the 600 series. Neck Thru consists of a single wooden piece from the neck through the central body section.

In 1958, Hall introduced prototype called "capris" (the same name of Hall family’s cat from the pronunciation of the french noun for whim). 

In 1963, Rickenbacker developed an electric twelve-string guitar with an innovative headstock design that fit all twelve machine heads onto a standard-length headstock by mounting alternate pairs of machine heads at right-angles to each other. After including the twelve-string guitar in the Rickenbacker 300 Series.

In the 1960s, Rickenbacker benefited tremendously when a couple of Rickenbacker guitar models became permanently intertwined with the sound and look of The Beatles.

In Hamburg in 1960, Beatles guitarist John Lennon bought a Rickenbacker 325, which he used throughout the early days of The Beatles. He eventually had the guitar's natural alder body refinished in black, and made other modifications, including adding a Bigsby vibrato tailpiece and regularly changing the control knobs. Lennon played this guitar for the Beatles' 1964 debut on "The Ed Sullivan Show" (as well as for their third Sullivan appearance, pre-taped the same day but broadcast two weeks later). During Lennon's post-Beatles years in New York, he had this guitar restored to its original natural wood finish and the cracked gold pickguard replaced with a white one.

Rickenbacker made two new 325s for Lennon and shipped them to him while the Beatles were in Miami Beach, Florida, on the same 1964 visit to the United States: a one-off custom 12-string 325 model and an updated six-string model with modified electronics and vibrato. He used this newer six-string model on the Beatles' sequentially "second" appearance on The Ed Sullivan Show.

Lennon accidentally dropped the second 325 model during a 1964 Christmas show, breaking the headstock. While it was being repaired, Rickenbacker's UK distributor Rose Morris gave Lennon a model 1996 (the export version of a 325, available exclusively in a red finish and with an F-hole). Lennon later gave the 1996 to fellow Beatle Ringo Starr.
Beatles guitarist George Harrison bought a 425 during a brief visit to the United States in 1963. In February 1964, while in New York City, F.C. Hall of Rickenbacker met with the band and their manager, and gave Harrison a model 360/12 (the second electric twelve-string built by Rickenbacker). This instrument became a key part of the Beatles' sound on their LP "A Hard Day's Night" and other Beatles songs through late 1964. Harrison played this guitar sporadically throughout the remainder of his life.

On August 21, 1965, during a Beatles concert tour, Randy Resnick of B-Sharp, a Minnesota music store, presented Harrison with a second model 360/12 FG "New Style" 12-string electric guitar, distinguishable from Harrison's first 12-string by its rounded cutaways and edges. A television documentary produced by KSTP-TV in Minneapolis documents the event. Harrison used this guitar on the song "If I Needed Someone" and during the Beatles' 1966 tours. This 12-string's whereabouts are unknown, as it was stolen at some point after the band ceased touring.

After the Beatles 1965 summer tour, Paul McCartney frequently used a left-handed 1964 4001S FG Rickenbacker bass rather than the lightweight Höfner basses he had used previously. The instrument became popular with other bassists influenced by McCartney's highly melodic style.

In 1967, McCartney gave his 4001 a psychedelic paint job, as seen in the promo film for "Hello Goodbye", and in the "Magical Mystery Tour" film. A year or so later, someone sanded off the finish. A second, over-zealous sanding in the early 1970s removed the "points" of the bass' cutaways. McCartney used the Rickenbacker bass during his time with Wings, until the late 1970s.

Partly because of the Beatles' popularity and their consistent use of the Rickenbacker brand, many sixties guitarists adopted them, including John Fogerty (Creedence Clearwater Revival), Paul Kantner (Jefferson Airplane), and John Entwistle and Pete Townshend of The Who. As both the British Invasion and the 1960s wound down, Rickenbacker guitars fell out of fashion for a time. Rickenbacker basses, however, remained popular through the 1970s and beyond. In the late 1970s and early 1980s, Rickenbacker guitars experienced a renaissance as new wave and jangle pop groups turned to them for their distinctive chime. Demand is particularly high among retro groups influenced by the sound and look of the 1960s.

Some Rickenbacker models feature a stereo "Rick-O-Sound" output socket, allowing each pickup to be routed to different amplifiers or effects chains. Another feature is the use of two truss rods to correct twists and curvature in the neck. Rickenbacker guitars typically have a set neck made of multiple pieces of wood laminated together lengthwise, while their basses have a one-piece neck that extends through the entire body. Rickenbacker instruments are known for narrower necks (41.4 mm versus 43 mm at the nut for most competitors) and lacquered rosewood fingerboards, giving them a different feel.

Known for their bright jangle and chime, Rickenbacker guitars are often favoured by folk rock, and British Invasion bands such as the Searchers, The Beatles and The Who. The early models were equipped with low-output toaster pickups. With the late-1960s advent of heavy rock, these were phased out circa 1969–70, and replaced by hi-gain pickups with twice the output. Still, the early models were viewed by Pete Townshend as pivotal in his refinement of feedback techniques and the eventual development of the Marshall sound.

In recent years, a diverse cross-section of artists have played Rickenbacker guitars. In 1979, Tom Petty and Mike Campbell of Tom Petty and the Heartbreakers used vintage 1960s models to attain that toaster-pickup jangle. The hi-gain pickup sound is associated with acts such as The Jam and R.E.M.

In 2014, Rickenbacker introduced the Walnut series: the 330W, 330W/12, 360W, 360W/12, and 4003W. These models have walnut bodies with a hand-rubbed oil finish, and unfinished maple fingerboards.

The 4000 series were the first Rickenbacker bass guitars. The company began making them in 1957. They followed the 4000 with the 4001 (in 1961), 4002 (limited edition bass introduced in 1977), 4008 (an eight-string model introduced in the mid-1970s), 4003 (in 1979, replacing the 4001 entirely in 1986 and still in production in 2017), and most recently the 4004 series. They also made the 4005, a hollow-bodied bass guitar (discontinued in 1984)—which did not resemble other 4000 series basses, but rather the new style 360-370 guitars. The 4001S (introduced 1964) was basically a 4001 but with no binding and dot fingerboard inlays. It was exported to England as the RM1999. However, Paul McCartney received one of the early 4001S instruments (his unit was left-handed, and later modified to include a "zero fret"). Along with McCartney, other early adopters of the 4001 were Roger Waters (Pink Floyd), John Entwistle (The Who), Pete Quaife (The Kinks), Chris Squire of Yes and Geddy Lee of Rush.

Standard and collectible versions of the 4003 include the 4003S (Special), which was discontinued in 1995 but relaunched in 2015. This was similar to the 4001S with its dot neck markers, no body binding based loosely upon the original Rickenbacker basses, and 4001 pickups. From 1985 to 2002, the 4003 and 4003S had black hardware and black binding options available. Later special editions included the 4003 Blue Boy, 4003 CS (Chris Squire), Blackstar, Shadow Bass, Tuxedo and 4003 Redneck.

Rickenbacker basses have a distinctive tone. The 4001 and 4003 basses have neck-through construction. The 3000 series, made from the mid-1970s to mid-1980s, were cheaper instruments with bolt-on 21-fret necks. There was also a glued-in "set neck" 4000 version in 1975-76 (neck set like a Gibson Les Paul), which featured a 20-fret neck, dot inlays, no binding (similar to the 4001S) and only a single bridge-position mono pickup. Fred Turner of Bachman-Turner Overdrive used the 4000 extensively on the "Not Fragile" album, as seen in a promotional clip for "You Ain't Seen Nothing Yet." This bass also appears on the gatefold sleeve of "Four Wheel Drive".

In the 1970s, the Rickenbacker bass became a staple of progressive rock, as exemplified by British bassists Mike Rutherford (Genesis) and Chris Squire (Yes). Squire was one of the first to supercharge the 4001 by splitting the signal, sending the neck pickup output to a bass amp and the bridge pickup output to a lead guitar amp. Combined with his aggressive picking technique on Rotosound roundwound strings, the effect was a growling, grinding, "concrete mixer" tone that remains admired and emulated to this day.

In the hard rock vein, Deep Purple's Roger Glover was a prominent Rickenbacker aficionado. Geddy Lee of Rush used a Rickenbacker on the band's earlier material. Another enthusiast was Metallica bassist Cliff Burton, whose heavily modified 4001, red with white hardware and trim, debuted during the group's "Kill 'Em All" era. Also noteworthy was Motörhead vocalist/bassist Ian "Lemmy" Kilmister, for whom Rickenbacker produced a 60-bass run of "Lemmy Kilmister" signature basses: the 4004LK, fitted with three pickups, gold hardware, and elaborate wood carving in the shape of oak leaves. In 2019, the company produced a 420-bass run of Al Cisneros signature basses honoring the prominent Sleep and Om bassist, a long-time Rickenbacker proponent. Cisneros's 4003AC model features a signature pickguard, green inlays on the fingerboard, and a removable thumb rest.

The sound of Rickenbacker basses featured early on in the UK punk/new wave explosion of the late 1970s and early 80s and were used by: Glen Matlock (Sex Pistols) and Paul Simonon (The Clash)(although both soon switched to Fender Precision basses), Bruce Foxton (The Jam), Paul Gray (The Damned, Eddie & the Hot Rods), Tony James (Generation X), Mike Mills (R.E.M.), Michael Bradley (The Undertones), Youth (Killing Joke) and in the US Kira Roessler (Black Flag).

However, Brazilian bassist Alex Malheiros from Azymuth used a 4001 bass during the band's first years (most notably between 1972 and 1977). His very rich approach on the samba, the jazz and the funk with some echoes of Chris Squire and Paul McCartney is an important staple for the instrument: it is not only a rock bass and, on the right hands, can be a great fit on every genre.

Rickenbacker has produced a number of uniquely designed and distinctively trimmed acoustic guitars. Although a small number of Rickenbacker acoustics were sold in the 1950s and were seen in the hands of stars like Ricky Nelson and Sam Cooke, the company concentrated on their electric guitar and western steel guitar business from the early 1960s onward. From about 1959 through 1994, very few Rickenbacker acoustic guitars were made.

In 1995, an effort was made to re-introduce Rickenbacker acoustics, with factory production beginning in their Santa Ana manufacturing facility in 1996. Four models of flat top acoustic Rickenbackers were depicted in factory literature (maple or rosewood back and sides, jumbo or dreadnaught shape). Each of these four models was also available in both six- and twelve-string configurations, yielding a range of eight distinct instruments. (The 760J "Jazzbo," an archtop model, was only built as a prototype, with three examples known to exist.) It is estimated that fewer than 500 Rickenbacker acoustic guitars were built before the factory shut down the acoustic department in mid-2006.

In late 2006, Rickenbacker gave a license to build Rickenbacker-branded acoustics to Paul Wilczynski, a luthier with a workshop in San Francisco, California. He continued to offer all eight models of the Rickenbacker flat top guitar line, building each instrument to order, until his license expired in February 1, 2013.

Rickenbacker manufactures three pickups for their current standard models: hi-gain single coil, Vintage Toaster™ single coil, and humbucking. All three pickup designs share the same footprint, so they can retrofit into most current or vintage models. The tone varies from one style to the next, partially because of the types of magnets used but also due to the amount of wire wound around the pickup's bobbin.

Most contemporary models come with single-coil hi-gain pickups as standard equipment. Many post-British-Invasion players such as Peter Buck, Paul Weller, and Johnny Marr have used instruments with these pickups. Rickenbacker's HB1 humbucker/dual coil pickup has a similar tone to a Gibson mini-humbucker pickup, and comes standard on the Rickenbacker 650C and 4004 basses. 

Vintage reissue models, and some signature models, come with Toaster™ Top pickups, which resemble a classic two-slotted chrome toaster. Despite their slightly lower output, Toasters produce a brighter, cleaner sound, and are generally seen as key to obtaining the true British Invasion guitar tone, as they were original equipment of the era.

In addition to the standard pickups, vintage reissue bass models are equipped with Horseshoe wrap-around style pickups, very similar to the pickups on the earliest Rickenbacker Frying Pan models.

Rickenbacker are known for their pro-active approach to preventing the sale of copies or 'clones' of their instruments. The company has issued legal threats to websites where individuals might offer secondhand instruments for sale. Doing so has harmed the reputation of the company and polarised opinion of the brand among enthusiasts, many of whom aspire to own the ‘real thing’.



</doc>
<doc id="25821" url="https://en.wikipedia.org/wiki?curid=25821" title="Romeo and Juliet">
Romeo and Juliet

Romeo and Juliet is a tragedy written by William Shakespeare early in his career about two young star-crossed lovers whose deaths ultimately reconcile their feuding families. It was among Shakespeare's most popular plays during his lifetime and along with "Hamlet", is one of his most frequently performed plays. Today, the title characters are regarded as archetypal young lovers.

"Romeo and Juliet" belongs to a tradition of tragic romances stretching back to antiquity. The plot is based on an Italian tale translated into verse as "The Tragical History of Romeus and Juliet" by Arthur Brooke in 1562 and retold in prose in "Palace of Pleasure" by William Painter in 1567. Shakespeare borrowed heavily from both but expanded the plot by developing a number of supporting characters, particularly Mercutio and Paris. Believed to have been written between 1591 and 1595, the play was first published in a quarto version in 1597. The text of the first quarto version was of poor quality, however, and later editions corrected the text to conform more closely with Shakespeare's original.

Shakespeare's use of his poetic dramatic structure (especially effects such as switching between comedy and tragedy to heighten tension, his expansion of minor characters, and his use of sub-plots to embellish the story) has been praised as an early sign of his dramatic skill. The play ascribes different poetic forms to different characters, sometimes changing the form as the character develops. Romeo, for example, grows more adept at the sonnet over the course of the play.

"Romeo and Juliet" has been adapted numerous times for stage, film, musical, and opera venues. During the English Restoration, it was revived and heavily revised by William Davenant. David Garrick's 18th-century version also modified several scenes, removing material then considered indecent, and Georg Benda's "Romeo und Julie" omitted much of the action and added a happy ending. Performances in the 19th century, including Charlotte Cushman's, restored the original text and focused on greater realism. John Gielgud's 1935 version kept very close to Shakespeare's text and used Elizabethan costumes and staging to enhance the drama. In the 20th and into the 21st century, the play has been adapted in versions as diverse as George Cukor's 1936 film "Romeo and Juliet", Franco Zeffirelli's 1968 version "Romeo and Juliet", and Baz Luhrmann's 1996 MTV-inspired "Romeo + Juliet".




The play, set in Verona, Italy, begins with a street brawl between Montague and Capulet servants who, like their masters, are sworn enemies. Prince Escalus of Verona intervenes and declares that further breach of the peace will be punishable by death. Later, Count Paris talks to Capulet about marrying his daughter Juliet, but Capulet asks Paris to wait another two years and invites him to attend a planned Capulet ball. Lady Capulet and Juliet's nurse try to persuade Juliet to accept Paris's courtship.

Meanwhile, Benvolio talks with his cousin Romeo, Montague's son, about Romeo's recent depression. Benvolio discovers that it stems from unrequited infatuation for a girl named Rosaline, one of Capulet's nieces. Persuaded by Benvolio and Mercutio, Romeo attends the ball at the Capulet house in hopes of meeting Rosaline. However, Romeo instead meets and falls in love with Juliet. Juliet's cousin, Tybalt, is enraged at Romeo for sneaking into the ball but is only stopped from killing Romeo by Juliet's father, who does not wish to shed blood in his house. After the ball, in what is now called the "balcony scene", Romeo sneaks into the Capulet orchard and overhears Juliet at her window vowing her love to him in spite of her family's hatred of the Montagues. Romeo makes himself known to her, and they agree to be married. With the help of Friar Laurence, who hopes to reconcile the two families through their children's union, they are secretly married the next day.

Tybalt, meanwhile, still incensed that Romeo had sneaked into the Capulet ball, challenges him to a duel. Romeo, now considering Tybalt his kinsman, refuses to fight. Mercutio is offended by Tybalt's insolence, as well as Romeo's "vile submission", and accepts the duel on Romeo's behalf. Mercutio is fatally wounded when Romeo attempts to break up the fight. Grief-stricken and wracked with guilt, Romeo confronts and slays Tybalt.

Benvolio argues that Romeo has justly executed Tybalt for the murder of Mercutio. The Prince, now having lost a kinsman in the warring families' feud, exiles Romeo from Verona, under penalty of death if he ever returns. Romeo secretly spends the night in Juliet's chamber, where they consummate their marriage. Capulet, misinterpreting Juliet's grief, agrees to marry her to Count Paris and threatens to disown her when she refuses to become Paris's "joyful bride". When she then pleads for the marriage to be delayed, her mother rejects her.

Juliet visits Friar Laurence for help, and he offers her a potion that will put her into a deathlike coma for "two and forty hours". The Friar promises to send a messenger to inform Romeo of the plan so that he can rejoin her when she awakens. On the night before the wedding, she takes the drug and, when discovered apparently dead, she is laid in the family crypt.

The messenger, however, does not reach Romeo and, instead, Romeo learns of Juliet's apparent death from his servant, Balthasar. Heartbroken, Romeo buys poison from an apothecary and goes to the Capulet crypt. He encounters Paris who has come to mourn Juliet privately. Believing Romeo to be a vandal, Paris confronts him and, in the ensuing battle, Romeo kills Paris. Still believing Juliet to be dead, he drinks the poison. Juliet then awakens and, discovering that Romeo is dead, stabs herself with his dagger and joins him in death. The feuding families and the Prince meet at the tomb to find all three dead. Friar Laurence recounts the story of the two "star-cross'd lovers". The families are reconciled by their children's deaths and agree to end their violent feud. The play ends with the Prince's elegy for the lovers: "For never was a story of more woe / Than this of Juliet and her Romeo."

"Romeo and Juliet" borrows from a tradition of tragic love stories dating back to antiquity. One of these is Pyramus and Thisbe, from Ovid's "Metamorphoses", which contains parallels to Shakespeare's story: the lovers' parents despise each other, and Pyramus falsely believes his lover Thisbe is dead. The "Ephesiaca" of Xenophon of Ephesus, written in the 3rd century, also contains several similarities to the play, including the separation of the lovers, and a potion that induces a deathlike sleep.

One of the earliest references to the names "Montague" and "Capulet" is from Dante's "Divine Comedy", who mentions the Montecchi ("Montagues") and the Cappelletti ("Capulets") in canto six of Purgatorio:

However, the reference is part of a polemic against the moral decay of Florence, Lombardy, and the Italian Peninsula as a whole; Dante, through his characters, chastises German King Albert I for neglecting his responsibilities towards Italy ("you who are negligent"), and successive popes for their encroachment from purely spiritual affairs, thus leading to a climate of incessant bickering and warfare between rival political parties in Lombardy. History records the name of the family "Montague" as being lent to such a political party in Verona, but that of the "Capulets" as from a Cremonese family, both of whom play out their conflict in Lombardy as a whole rather than within the confines of Verona. Allied to rival political factions, the parties are grieving ("One lot already grieving") because their endless warfare has led to the destruction of both parties, rather than a grief from the loss of their ill-fated offspring as the play sets forth, which appears to be a solely poetic creation within this context.

The earliest known version of the "Romeo and Juliet" tale akin to Shakespeare's play is the story of Mariotto and Gianozza by Masuccio Salernitano, in the 33rd novel of his "Il Novellino" published in 1476. Salernitano sets the story in Siena and insists its events took place in his own lifetime. His version of the story includes the secret marriage, the colluding friar, the fray where a prominent citizen is killed, Mariotto's exile, Gianozza's forced marriage, the potion plot, and the crucial message that goes astray. In this version, Mariotto is caught and beheaded and Gianozza dies of grief.

Luigi da Porto (1485–1529) adapted the story as "Giulietta e Romeo" and included it in his "Historia novellamente ritrovata di due Nobili Amanti", written in 1524 and published posthumously in 1531 in Venice. Da Porto drew on "Pyramus and Thisbe", Boccaccio's "Decameron", and Salernitano's "Mariotto e Ganozza", but it is likely that his story is also autobiographical: present as a soldier at a ball on 26 February 1511, at a residence of the Savorgnan clan in Udine, following a peace ceremony with the opposite Strumieri, Da Porto fell in love with Lucina, the daughter of the house, but relationships of their mentors prevented advances. The next morning, the Savorgnans led an attack on the city, and many members of the Strumieri were murdered. When years later, half-paralyzed from a battle-wound, he wrote "Giulietta e Romeo" in Montorso Vicentino (from where he could see the "castles" of Verona), he dedicated the "novella" to "bellisima e leggiadra madonna" Lucina Savorgnan. Da Porto presented his tale as historically true and claimed it took place at least a century earlier than Salernitano had it, in the days Verona was ruled by Bartolomeo della Scala (anglicized as Prince Escalus).

Da Porto gave "Romeo and Juliet" most of its modern form, including the names of the lovers, the rival families of Montecchi and Capuleti, and the location in Verona. He named the friar Laurence ("frate Lorenzo") and introduced the characters Mercutio ("Marcuccio Guertio"), Tybalt ("Tebaldo Cappelleti"), Count Paris ("conti (Paride) di "), the faithful servant, and Giulietta's nurse. Da Porto originated the remaining basic elements of the story: the feuding families, Romeo—left by his mistress—meeting Giulietta at a dance at her house, the love scenes (including the balcony scene), the periods of despair, Romeo killing Giulietta's cousin (Tebaldo), and the families' reconciliation after the lovers' suicides. In da Porto's version, Romeo takes poison and Giulietta stabs herself with his dagger.

In 1554, Matteo Bandello published the second volume of his "Novelle", which included his version of "Giuletta e Romeo", probably written between 1531 and 1545. Bandello lengthened and weighed down the plot while leaving the storyline basically unchanged (though he did introduce Benvolio). Bandello's story was translated into French by Pierre Boaistuau in 1559 in the first volume of his "Histories Tragiques". Boaistuau adds much moralising and sentiment, and the characters indulge in rhetorical outbursts.

In his 1562 narrative poem "The Tragical History of Romeus and Juliet", Arthur Brooke translated Boaistuau faithfully but adjusted it to reflect parts of Chaucer's "Troilus and Criseyde". There was a trend among writers and playwrights to publish works based on Italian "novelle"—Italian tales were very popular among theatre-goers—and Shakespeare may well have been familiar with William Painter's 1567 collection of Italian tales titled "Palace of Pleasure". This collection included a version in prose of the "Romeo and Juliet" story named ""The goodly History of the true and constant love of Romeo and Juliett"". Shakespeare took advantage of this popularity: "The Merchant of Venice", "Much Ado About Nothing", "All's Well That Ends Well", "Measure for Measure", and "Romeo and Juliet" are all from Italian "novelle". "Romeo and Juliet" is a dramatisation of Brooke's translation, and Shakespeare follows the poem closely but adds extra detail to both major and minor characters (in particular the Nurse and Mercutio).

Christopher Marlowe's "Hero and Leander" and "Dido, Queen of Carthage", both similar stories written in Shakespeare's day, are thought to be less of a direct influence, although they may have helped create an atmosphere in which tragic love stories could thrive.

It is unknown when exactly Shakespeare wrote "Romeo and Juliet". Juliet's nurse refers to an earthquake she says occurred 11 years ago. This may refer to the Dover Straits earthquake of 1580, which would date that particular line to 1591. Other earthquakes—both in England and in Verona—have been proposed in support of the different dates. But the play's stylistic similarities with "A Midsummer Night's Dream" and other plays conventionally dated around 1594–95, place its composition sometime between 1591 and 1595. One conjecture is that Shakespeare may have begun a draft in 1591, which he completed in 1595.

Shakespeare's "Romeo and Juliet" was published in two quarto editions prior to the publication of the First Folio of 1623. These are referred to as Q1 and Q2. The first printed edition, Q1, appeared in early 1597, printed by John Danter. Because its text contains numerous differences from the later editions, it is labelled a so-called 'bad quarto'; the 20th-century editor T. J. B. Spencer described it as "a detestable text, probably a reconstruction of the play from the imperfect memories of one or two of the actors", suggesting that it had been pirated for publication. An alternative explanation for Q1's shortcomings is that the play (like many others of the time) may have been heavily edited before performance by the playing company. However, "the theory, formulated by [Alfred] Pollard," that the 'bad quarto' was "reconstructed from memory by some of the actors is now under attack. Alternative theories are that some or all of 'the bad quartos' are early versions by Shakespeare or abbreviations made either for Shakespeare's company or for other companies." In any event, its appearance in early 1597 makes 1596 the latest possible date for the play's composition.
The superior Q2 called the play "The Most Excellent and Lamentable Tragedie of Romeo and Juliet". It was printed in 1599 by Thomas Creede and published by Cuthbert Burby. Q2 is about 800 lines longer than Q1. Its title page describes it as "Newly corrected, augmented and amended". Scholars believe that Q2 was based on Shakespeare's pre-performance draft (called his foul papers) since there are textual oddities such as variable tags for characters and "false starts" for speeches that were presumably struck through by the author but erroneously preserved by the typesetter. It is a much more complete and reliable text and was reprinted in 1609 (Q3), 1622 (Q4) and 1637 (Q5). In effect, all later Quartos and Folios of "Romeo and Juliet" are based on Q2, as are all modern editions since editors believe that any deviations from Q2 in the later editions (whether good or bad) are likely to have arisen from editors or compositors, not from Shakespeare.

The First Folio text of 1623 was based primarily on Q3, with clarifications and corrections possibly coming from a theatrical prompt book or Q1. Other Folio editions of the play were printed in 1632 (F2), 1664 (F3), and 1685 (F4). Modern versions—that take into account several of the Folios and Quartos—first appeared with Nicholas Rowe's 1709 edition, followed by Alexander Pope's 1723 version. Pope began a tradition of editing the play to add information such as stage directions missing in Q2 by locating them in Q1. This tradition continued late into the Romantic period. Fully annotated editions first appeared in the Victorian period and continue to be produced today, printing the text of the play with footnotes describing the sources and culture behind the play.

Scholars have found it extremely difficult to assign one specific, overarching theme to the play. Proposals for a main theme include a discovery by the characters that human beings are neither wholly good nor wholly evil, but instead are more or less alike, awaking out of a dream and into reality, the danger of hasty action, or the power of tragic fate. None of these have widespread support. However, even if an overall theme cannot be found it is clear that the play is full of several small, thematic elements that intertwine in complex ways. Several of those most often debated by scholars are discussed below.

"Romeo and Juliet" is sometimes considered to have no unifying theme, save that of young love. Romeo and Juliet have become emblematic of young lovers and doomed love. Since it is such an obvious subject of the play, several scholars have explored the language and historical context behind the romance of the play.

On their first meeting, Romeo and Juliet use a form of communication recommended by many etiquette authors in Shakespeare's day: metaphor. By using metaphors of saints and sins, Romeo was able to test Juliet's feelings for him in a non-threatening way. This method was recommended by Baldassare Castiglione (whose works had been translated into English by this time). He pointed out that if a man used a metaphor as an invitation, the woman could pretend she did not understand him, and he could retreat without losing honour. Juliet, however, participates in the metaphor and expands on it. The religious metaphors of "shrine", "pilgrim", and "saint" were fashionable in the poetry of the time and more likely to be understood as romantic rather than blasphemous, as the concept of sainthood was associated with the Catholicism of an earlier age. Later in the play, Shakespeare removes the more daring allusions to Christ's resurrection in the tomb he found in his source work: Brooke's "Romeus and Juliet".
In the later balcony scene, Shakespeare has Romeo overhear Juliet's soliloquy, but in Brooke's version of the story, her declaration is done alone. By bringing Romeo into the scene to eavesdrop, Shakespeare breaks from the normal sequence of courtship. Usually, a woman was required to be modest and shy to make sure that her suitor was sincere, but breaking this rule serves to speed along the plot. The lovers are able to skip courting and move on to plain talk about their relationship—agreeing to be married after knowing each other for only one night. In the final suicide scene, there is a contradiction in the message—in the Catholic religion, suicides were often thought to be condemned to hell, whereas people who die to be with their loves under the "Religion of Love" are joined with their loves in paradise. Romeo and Juliet's love seems to be expressing the "Religion of Love" view rather than the Catholic view. Another point is that although their love is passionate, it is only consummated in marriage, which keeps them from losing the audience's sympathy.

The play arguably equates love and sex with death. Throughout the story, both Romeo and Juliet, along with the other characters, fantasise about it as a dark being, often equating it with a lover. Capulet, for example, when he first discovers Juliet's (faked) death, describes it as having deflowered his daughter. Juliet later erotically compares Romeo and death. Right before her suicide, she grabs Romeo's dagger, saying "O happy dagger! This is thy sheath. There rust, and let me die."

Scholars are divided on the role of fate in the play. No consensus exists on whether the characters are truly fated to die together or whether the events take place by a series of unlucky chances. Arguments in favour of fate often refer to the description of the lovers as "star-cross'd". This phrase seems to hint that the stars have predetermined the lovers' future. John W. Draper points out the parallels between the Elizabethan belief in the four humours and the main characters of the play (for example, Tybalt as a choleric). Interpreting the text in the light of humours reduces the amount of plot attributed to chance by modern audiences. Still, other scholars see the play as a series of unlucky chances—many to such a degree that they do not see it as a tragedy at all, but an emotional melodrama. Ruth Nevo believes the high degree to which chance is stressed in the narrative makes "Romeo and Juliet" a "lesser tragedy" of happenstance, not of character. For example, Romeo's challenging Tybalt is not impulsive; it is, after Mercutio's death, the expected action to take. In this scene, Nevo reads Romeo as being aware of the dangers of flouting social norms, identity, and commitments. He makes the choice to kill, not because of a tragic flaw, but because of circumstance.

Scholars have long noted Shakespeare's widespread use of light and dark imagery throughout the play. Caroline Spurgeon considers the theme of light as "symbolic of the natural beauty of young love" and later critics have expanded on this interpretation. For example, both Romeo and Juliet see the other as light in a surrounding darkness. Romeo describes Juliet as being like the sun, brighter than a torch, a jewel sparkling in the night, and a bright angel among dark clouds. Even when she lies apparently dead in the tomb, he says her "beauty makes This vault a feasting presence full of light." Juliet describes Romeo as "day in night" and "Whiter than snow upon a raven's back." This contrast of light and dark can be expanded as symbols—contrasting love and hate, youth and age in a metaphoric way. Sometimes these intertwining metaphors create dramatic irony. For example, Romeo and Juliet's love is a light in the midst of the darkness of the hate around them, but all of their activity together is done in night and darkness while all of the feuding is done in broad daylight. This paradox of imagery adds atmosphere to the moral dilemma facing the two lovers: loyalty to family or loyalty to love. At the end of the story, when the morning is gloomy and the sun hiding its face for sorrow, light and dark have returned to their proper places, the outward darkness reflecting the true, inner darkness of the family feud out of sorrow for the lovers. All characters now recognise their folly in light of recent events, and things return to the natural order, thanks to the love and death of Romeo and Juliet. The "light" theme in the play is also heavily connected to the theme of time since light was a convenient way for Shakespeare to express the passage of time through descriptions of the sun, moon, and stars.

Time plays an important role in the language and plot of the play. Both Romeo and Juliet struggle to maintain an imaginary world void of time in the face of the harsh realities that surround them. For instance, when Romeo swears his love to Juliet by the moon, she protests "O swear not by the moon, th'inconstant moon, / That monthly changes in her circled orb, / Lest that thy love prove likewise variable." From the very beginning, the lovers are designated as "star-cross'd" referring to an astrologic belief associated with time. Stars were thought to control the fates of humanity, and as time passed, stars would move along their course in the sky, also charting the course of human lives below. Romeo speaks of a foreboding he feels in the stars' movements early in the play, and when he learns of Juliet's death, he defies the stars' course for him.

Another central theme is haste: Shakespeare's "Romeo and Juliet" spans a period of four to six days, in contrast to Brooke's poem's spanning nine months. Scholars such as G. Thomas Tanselle believe that time was "especially important to Shakespeare" in this play, as he used references to "short-time" for the young lovers as opposed to references to "long-time" for the "older generation" to highlight "a headlong rush towards doom". Romeo and Juliet fight time to make their love last forever. In the end, the only way they seem to defeat time is through a death that makes them immortal through art.

Time is also connected to the theme of light and dark. In Shakespeare's day, plays were most often performed at noon or in the afternoon in broad daylight. This forced the playwright to use words to create the illusion of day and night in his plays. Shakespeare uses references to the night and day, the stars, the moon, and the sun to create this illusion. He also has characters frequently refer to days of the week and specific hours to help the audience understand that time has passed in the story. All in all, no fewer than 103 references to time are found in the play, adding to the illusion of its passage.

The earliest known critic of the play was diarist Samuel Pepys, who wrote in 1662: "it is a play of itself the worst that I ever heard in my life." Poet John Dryden wrote 10 years later in praise of the play and its comic character Mercutio: "Shakespear show'd the best of his skill in his "Mercutio", and he said himself, that he was forc'd to kill him in the third Act, to prevent being killed by him." Criticism of the play in the 18th century was less sparse but no less divided. Publisher Nicholas Rowe was the first critic to ponder the theme of the play, which he saw as the just punishment of the two feuding families. In mid-century, writer Charles Gildon and philosopher Lord Kames argued that the play was a failure in that it did not follow the classical rules of drama: the tragedy must occur because of some character flaw, not an accident of fate. Writer and critic Samuel Johnson, however, considered it one of Shakespeare's "most pleasing" plays.

In the later part of the 18th and through the 19th century, criticism centred on debates over the moral message of the play. Actor and playwright David Garrick's 1748 adaptation excluded Rosaline: Romeo abandoning her for Juliet was seen as fickle and reckless. Critics such as Charles Dibdin argued that Rosaline had been purposely included in the play to show how reckless the hero was and that this was the reason for his tragic end. Others argued that Friar Laurence might be Shakespeare's spokesman in his warnings against undue haste. With the advent of the 20th century, these moral arguments were disputed by critics such as Richard Green Moulton: he argued that accident, and not some character flaw, led to the lovers' deaths.

In "Romeo and Juliet", Shakespeare employs several dramatic techniques that have garnered praise from critics; most notably the abrupt shifts from comedy to tragedy (an example is the punning exchange between Benvolio and Mercutio just before Tybalt arrives). Before Mercutio's death in Act three, the play is largely a comedy. After his accidental demise, the play suddenly becomes serious and takes on a tragic tone. When Romeo is banished, rather than executed, and Friar Laurence offers Juliet a plan to reunite her with Romeo, the audience can still hope that all will end well. They are in a "breathless state of suspense" by the opening of the last scene in the tomb: If Romeo is delayed long enough for the Friar to arrive, he and Juliet may yet be saved. These shifts from hope to despair, reprieve, and new hope serve to emphasise the tragedy when the final hope fails and both the lovers die at the end.

Shakespeare also uses sub-plots to offer a clearer view of the actions of the main characters. For example, when the play begins, Romeo is in love with Rosaline, who has refused all of his advances. Romeo's infatuation with her stands in obvious contrast to his later love for Juliet. This provides a comparison through which the audience can see the seriousness of Romeo and Juliet's love and marriage. Paris' love for Juliet also sets up a contrast between Juliet's feelings for him and her feelings for Romeo. The formal language she uses around Paris, as well as the way she talks about him to her Nurse, show that her feelings clearly lie with Romeo. Beyond this, the sub-plot of the Montague–Capulet feud overarches the whole play, providing an atmosphere of hate that is the main contributor to the play's tragic end.

Shakespeare uses a variety of poetic forms throughout the play. He begins with a 14-line prologue in the form of a Shakespearean sonnet, spoken by a Chorus. Most of "Romeo and Juliet" is, however, written in blank verse, and much of it in strict iambic pentameter, with less rhythmic variation than in most of Shakespeare's later plays. In choosing forms, Shakespeare matches the poetry to the character who uses it. Friar Laurence, for example, uses sermon and sententiae forms and the Nurse uses a unique blank verse form that closely matches colloquial speech. Each of these forms is also moulded and matched to the emotion of the scene the character occupies. For example, when Romeo talks about Rosaline earlier in the play, he attempts to use the Petrarchan sonnet form. Petrarchan sonnets were often used by men to exaggerate the beauty of women who were impossible for them to attain, as in Romeo's situation with Rosaline. This sonnet form is used by Lady Capulet to describe Count Paris to Juliet as a handsome man. When Romeo and Juliet meet, the poetic form changes from the Petrarchan (which was becoming archaic in Shakespeare's day) to a then more contemporary sonnet form, using "pilgrims" and "saints" as metaphors. Finally, when the two meet on the balcony, Romeo attempts to use the sonnet form to pledge his love, but Juliet breaks it by saying "Dost thou love me?" By doing this, she searches for true expression, rather than a poetic exaggeration of their love. Juliet uses monosyllabic words with Romeo but uses formal language with Paris. Other forms in the play include an epithalamium by Juliet, a rhapsody in Mercutio's Queen Mab speech, and an elegy by Paris. Shakespeare saves his prose style most often for the common people in the play, though at times he uses it for other characters, such as Mercutio. Humour, also, is important: scholar Molly Mahood identifies at least 175 puns and wordplays in the text. Many of these jokes are sexual in nature, especially those involving Mercutio and the Nurse.

Early psychoanalytic critics saw the problem of "Romeo and Juliet" in terms of Romeo's impulsiveness, deriving from "ill-controlled, partially disguised aggression", which leads both to Mercutio's death and to the double suicide. "Romeo and Juliet" is not considered to be exceedingly psychologically complex, and sympathetic psychoanalytic readings of the play make the tragic male experience equivalent with sicknesses. Norman Holland, writing in 1966, considers Romeo's dream as a realistic "wish fulfilling fantasy both in terms of Romeo's adult world and his hypothetical childhood at stages oral, phallic and oedipal" – while acknowledging that a dramatic character is not a human being with mental processes separate from those of the author. Critics such as Julia Kristeva focus on the hatred between the families, arguing that this hatred is the cause of Romeo and Juliet's passion for each other. That hatred manifests itself directly in the lovers' language: Juliet, for example, speaks of "my only love sprung from my only hate" and often expresses her passion through an anticipation of Romeo's death. This leads on to speculation as to the playwright's psychology, in particular to a consideration of Shakespeare's grief for the death of his son, Hamnet.

Feminist literary critics argue that the blame for the family feud lies in Verona's patriarchal society. For Coppélia Kahn, for example, the strict, masculine code of violence imposed on Romeo is the main force driving the tragedy to its end. When Tybalt kills Mercutio, Romeo shifts into this violent mode, regretting that Juliet has made him so "effeminate". In this view, the younger males "become men" by engaging in violence on behalf of their fathers, or in the case of the servants, their masters. The feud is also linked to male virility, as the numerous jokes about maidenheads aptly demonstrate. Juliet also submits to a female code of docility by allowing others, such as the Friar, to solve her problems for her. Other critics, such as Dympna Callaghan, look at the play's feminism from a historicist angle, stressing that when the play was written the feudal order was being challenged by increasingly centralised government and the advent of capitalism. At the same time, emerging Puritan ideas about marriage were less concerned with the "evils of female sexuality" than those of earlier eras and more sympathetic towards love-matches: when Juliet dodges her father's attempt to force her to marry a man she has no feeling for, she is challenging the patriarchal order in a way that would not have been possible at an earlier time.

A number of critics have found the character of Mercutio to have unacknowledged homoerotic desire for Romeo. Jonathan Goldberg examined the sexuality of Mercutio and Romeo utilising queer theory in "Queering the Renaissance" (1994), comparing their friendship with sexual love. Mercutio, in friendly conversation, mentions Romeo's phallus, suggesting traces of homoeroticism. An example is his joking wish "To raise a spirit in his mistress' circle ... letting it there stand / Till she had laid it and conjured it down." Romeo's homoeroticism can also be found in his attitude to Rosaline, a woman who is distant and unavailable and brings no hope of offspring. As Benvolio argues, she is best replaced by someone who will reciprocate. Shakespeare's procreation sonnets describe another young man who, like Romeo, is having trouble creating offspring and who may be seen as being a homosexual. Goldberg believes that Shakespeare may have used Rosaline as a way to express homosexual problems of procreation in an acceptable way. In this view, when Juliet says "...that which we call a rose, by any other name would smell as sweet", she may be raising the question of whether there is any difference between the beauty of a man and the beauty of a woman.

The balcony scene was introduced by Da Porto in 1524. He had Romeo walk frequently by her house, "sometimes climbing to her chamber window", and wrote, "It happened one night, as love ordained, when the moon shone unusually bright, that whilst Romeo was climbing the balcony, the young lady ... opened the window, and looking out saw him". After this they have a conversation in which they declare eternal love to each other. A few decades later, Bandello greatly expanded this scene, diverging from the familiar one: Julia has her nurse deliver a letter asking Romeo to come to her window with a rope ladder, and he climbs the balcony with the help of his servant, Julia and the nurse (the servants discreetly withdraw after this).
Nevertheless, in October 2014, Lois Leveen speculated in "The Atlantic" that the original Shakespeare play did not contain a balcony. The word, "balcone", did not exist in the English language until two years after Shakespeare's death. The balcony was certainly used in Thomas Otway's 1679 play, "The History and Fall of Caius Marius", which had borrowed much of its story from "Romeo and Juliet" and placed the two lovers in a balcony reciting a speech similar to that between Romeo and Juliet. Leveen suggested that during the 18th century, David Garrick chose to use a balcony in his adaptation and revival of "Romeo and Juliet" and modern adaptations have continued this tradition.

"Romeo and Juliet" ranks with "Hamlet" as one of Shakespeare's most performed plays. Its many adaptations have made it one of his most enduring and famous stories. Even in Shakespeare's lifetime, it was extremely popular. Scholar Gary Taylor measures it as the sixth most popular of Shakespeare's plays, in the period after the death of Christopher Marlowe and Thomas Kyd but before the ascendancy of Ben Jonson during which Shakespeare was London's dominant playwright. The date of the first performance is unknown. The First Quarto, printed in 1597, says that "it hath been often (and with great applause) plaid publiquely", setting the first performance before that date. The Lord Chamberlain's Men were certainly the first to perform it. Besides their strong connections with Shakespeare, the Second Quarto actually names one of its actors, Will Kemp, instead of Peter, in a line in Act Five. Richard Burbage was probably the first Romeo, being the company's actor, and Master Robert Goffe (a boy) the first Juliet. The premiere is likely to have been at "The Theatre", with other early productions at "The Curtain". "Romeo and Juliet" is one of the first Shakespearean plays to have been performed outside England: a shortened and simplified version was performed in Nördlingen in 1604.

All theatres were closed down by the puritan government on 6 September 1642. Upon the restoration of the monarchy in 1660, two patent companies (the King's Company and the Duke's Company) were established, and the existing theatrical repertoire divided between them.
Sir William Davenant of the Duke's Company staged a 1662 adaptation in which Henry Harris played Romeo, Thomas Betterton Mercutio, and Betterton's wife Mary Saunderson Juliet: she was probably the first woman to play the role professionally. Another version closely followed Davenant's adaptation and was also regularly performed by the Duke's Company. This was a tragicomedy by James Howard, in which the two lovers survive.

Thomas Otway's "The History and Fall of Caius Marius", one of the more extreme of the Restoration adaptations of Shakespeare, debuted in 1680. The scene is shifted from Renaissance Verona to ancient Rome; Romeo is Marius, Juliet is Lavinia, the feud is between patricians and plebeians; Juliet/Lavinia wakes from her potion before Romeo/Marius dies. Otway's version was a hit, and was acted for the next seventy years. His innovation in the closing scene was even more enduring, and was used in adaptations throughout the next 200 years: Theophilus Cibber's adaptation of 1744, and David Garrick's of 1748 both used variations on it. These versions also eliminated elements deemed inappropriate at the time. For example, Garrick's version transferred all language describing Rosaline to Juliet, to heighten the idea of faithfulness and downplay the love-at-first-sight theme. In 1750, a "Battle of the Romeos" began, with Spranger Barry and Susannah Maria Arne (Mrs. Theophilus Cibber) at Covent Garden versus David Garrick and George Anne Bellamy at Drury Lane.

The earliest known production in North America was an amateur one: on 23 March 1730, a physician named Joachimus Bertrand placed an advertisement in the "Gazette" newspaper in New York, promoting a production in which he would play the apothecary. The first professional performances of the play in North America were those of the Hallam Company.

Garrick's altered version of the play was very popular, and ran for nearly a century. Not until 1845 did Shakespeare's original return to the stage in the United States with the sisters Susan and Charlotte Cushman as Juliet and Romeo, respectively, and then in 1847 in Britain with Samuel Phelps at Sadler's Wells Theatre. Cushman adhered to Shakespeare's version, beginning a string of eighty-four performances. Her portrayal of Romeo was considered genius by many. "The Times" wrote: "For a long time Romeo has been a convention. Miss Cushman's Romeo is a creative, a living, breathing, animated, ardent human being." Queen Victoria wrote in her journal that "no-one would ever have imagined she was a woman". Cushman's success broke the Garrick tradition and paved the way for later performances to return to the original storyline.

Professional performances of Shakespeare in the mid-19th century had two particular features: firstly, they were generally star vehicles, with supporting roles cut or marginalised to give greater prominence to the central characters. Secondly, they were "pictorial", placing the action on spectacular and elaborate sets (requiring lengthy pauses for scene changes) and with the frequent use of tableaux. Henry Irving's 1882 production at the Lyceum Theatre (with himself as Romeo and Ellen Terry as Juliet) is considered an archetype of the pictorial style. In 1895, Sir Johnston Forbes-Robertson took over from Irving and laid the groundwork for a more natural portrayal of Shakespeare that remains popular today. Forbes-Robertson avoided the showiness of Irving and instead portrayed a down-to-earth Romeo, expressing the poetic dialogue as realistic prose and avoiding melodramatic flourish.

American actors began to rival their British counterparts. Edwin Booth (brother to John Wilkes Booth) and Mary McVicker (soon to be Edwin's wife) opened as Romeo and Juliet at the sumptuous Booth's Theatre (with its European-style stage machinery, and an air conditioning system unique in New York) on 3 February 1869. Some reports said it was one of the most elaborate productions of "Romeo and Juliet" ever seen in America; it was certainly the most popular, running for over six weeks and earning over $60,000 (). The programme noted that: "The tragedy will be produced in strict accordance with historical propriety, in every respect, following closely the text of Shakespeare."

The first professional performance of the play in Japan may have been George Crichton Miln's company's production, which toured to Yokohama in 1890. Throughout the 19th century, "Romeo and Juliet" had been Shakespeare's most popular play, measured by the number of professional performances. In the 20th century it would become the second most popular, behind "Hamlet".

In 1933, the play was revived by actress Katharine Cornell and her director husband Guthrie McClintic and was taken on a seven-month nationwide tour throughout the United States. It starred Orson Welles, Brian Aherne and Basil Rathbone. The production was a modest success, and so upon the return to New York, Cornell and McClintic revised it, and for the first time the play was presented with almost all the scenes intact, including the Prologue. The new production opened on Broadway in December 1934. Critics wrote that Cornell was "the greatest Juliet of her time", "endlessly haunting", and "the most lovely and enchanting Juliet our present-day theatre has seen".
John Gielgud's New Theatre production in 1935 featured Gielgud and Laurence Olivier as Romeo and Mercutio, exchanging roles six weeks into the run, with Peggy Ashcroft as Juliet. Gielgud used a scholarly combination of Q1 and Q2 texts and organised the set and costumes to match as closely as possible the Elizabethan period. His efforts were a huge success at the box office, and set the stage for increased historical realism in later productions. Olivier later compared his performance and Gielgud's: "John, all spiritual, all spirituality, all beauty, all abstract things; and myself as all earth, blood, humanity ... I've always felt that John missed the lower half and that made me go for the other ... But whatever it was, when I was playing Romeo I was carrying a torch, I was trying to sell realism in Shakespeare."

Peter Brook's 1947 version was the beginning of a different style of "Romeo and Juliet" performances. Brook was less concerned with realism, and more concerned with translating the play into a form that could communicate with the modern world. He argued, "A production is only correct at the moment of its correctness, and only good at the moment of its success." Brook excluded the final reconciliation of the families from his performance text.

Throughout the century, audiences, influenced by the cinema, became less willing to accept actors distinctly older than the teenage characters they were playing. A significant example of more youthful casting was in Franco Zeffirelli's Old Vic production in 1960, with John Stride and Judi Dench, which would serve as the basis for his 1968 film. Zeffirelli borrowed from Brook's ideas, altogether removing around a third of the play's text to make it more accessible. In an interview with "The Times", he stated that the play's "twin themes of love and the total breakdown of understanding between two generations" had contemporary relevance.

Recent performances often set the play in the contemporary world. For example, in 1986, the Royal Shakespeare Company set the play in modern Verona. Switchblades replaced swords, feasts and balls became drug-laden rock parties, and Romeo committed suicide by hypodermic needle.
Neil Bartlett's production of Romeo and Juliet themed the play very contemporary with a cinematic look which started its life at the Lyric Hammersmith, London then went to West Yorkshire Playhouse for an exclusive run in 1995. The cast included Emily Woof as Juliet, Stuart Bunce as Romeo, Sebastian Harcombe as Mercutio, Ashley Artus as Tybalt, Souad Faress as Lady Capulet and Silas Carson as Paris. In 1997, the Folger Shakespeare Theatre produced a version set in a typical suburban world. Romeo sneaks into the Capulet barbecue to meet Juliet, and Juliet discovers Tybalt's death while in class at school.

The play is sometimes given a historical setting, enabling audiences to reflect on the underlying conflicts. For example, adaptations have been set in the midst of the Israeli–Palestinian conflict, in the apartheid era in South Africa, and in the aftermath of the Pueblo Revolt. Similarly, Peter Ustinov's 1956 comic adaptation, "Romanoff and Juliet", is set in a fictional mid-European country in the depths of the Cold War. A mock-Victorian revisionist version of "Romeo and Juliet" final scene (with a happy ending, Romeo, Juliet, Mercutio, and Paris restored to life, and Benvolio revealing that he is Paris's love, Benvolia, in disguise) forms part of the 1980 stage-play "The Life and Adventures of Nicholas Nickleby". "Shakespeare's R&J", by Joe Calarco, spins the classic in a modern tale of gay teenage awakening. A recent comedic musical adaptation was "The Second City's Romeo and Juliet Musical: The People vs. Friar Laurence, the Man Who Killed Romeo and Juliet", set in modern times.

In the 19th and 20th century, "Romeo and Juliet" has often been the choice of Shakespeare plays to open a classical theatre company, beginning with Edwin Booth's inaugural production of that play in his theatre in 1869, the newly re-formed company of the Old Vic in 1929 with John Gielgud, Martita Hunt, and Margaret Webster, as well as the Riverside Shakespeare Company in its founding production in New York City in 1977, which used the 1968 film of Franco Zeffirelli's production as its inspiration.

In 2013, "Romeo and Juliet" ran on Broadway at Richard Rodgers Theatre from 19 September to 8 December for 93 regular performances after 27 previews starting on 24 August with Orlando Bloom and Condola Rashad in the starring roles.

The best-known ballet version is Prokofiev's "Romeo and Juliet". Originally commissioned by the Kirov Ballet, it was rejected by them when Prokofiev attempted a happy ending and was rejected again for the experimental nature of its music. It has subsequently attained an "immense" reputation, and has been choreographed by John Cranko (1962) and Kenneth MacMillan (1965) among others.

In 1977, Michael Smuin's production of one of the play's most dramatic and impassioned dance interpretations was debuted in its entirety by San Francisco Ballet. This production was the first full-length ballet to be broadcast by the PBS series "Great Performances: Dance in America"; it aired in 1978.

Dada Masilo, a South African dancer and choreographer, reinterpreted Romeo and Juliet in a new modern light. She introduced changes to the story, notably that of presenting the two families as multiracial.

At least 24 operas have been based on Romeo and Juliet. The earliest, "Romeo und Julie" in 1776, a Singspiel by Georg Benda, omits much of the action of the play and most of its characters and has a happy ending. It is occasionally revived. The best-known is Gounod's 1867 "Roméo et Juliette" (libretto by Jules Barbier and Michel Carré), a critical triumph when first performed and frequently revived today. Bellini's "I Capuleti e i Montecchi" is also revived from time to time, but has sometimes been judged unfavourably because of its perceived liberties with Shakespeare; however, Bellini and his librettist, Felice Romani, worked from Italian sources—principally Romani's libretto for "Giulietta e Romeo" by Nicola Vaccai—rather than directly adapting Shakespeare's play. Among later operas, there is Heinrich Sutermeister's 1940 work "Romeo und Julia".

"Roméo et Juliette" by Berlioz is a "symphonie dramatique", a large-scale work in three parts for mixed voices, chorus, and orchestra, which premiered in 1839. Tchaikovsky's "Romeo and Juliet" Fantasy-Overture (1869, revised 1870 and 1880) is a 15-minute symphonic poem, containing the famous melody known as the "love theme". Tchaikovsky's device of repeating the same musical theme at the ball, in the balcony scene, in Juliet's bedroom and in the tomb has been used by subsequent directors: for example, Nino Rota's love theme is used in a similar way in the 1968 film of the play, as is Des'ree's Kissing You in the 1996 film. Other classical composers influenced by the play include Henry Hugh Pearson ("Romeo and Juliet, overture for orchestra", Op. 86), Svendsen ("Romeo og Julie", 1876), Delius ("A Village Romeo and Juliet", 1899–1901), Stenhammar ("Romeo och Julia", 1922), and Kabalevsky ("Incidental Music to Romeo and Juliet", Op. 56, 1956).

The play influenced several jazz works, including Peggy Lee's "Fever". Duke Ellington's "Such Sweet Thunder" contains a piece entitled "The Star-Crossed Lovers" in which the pair are represented by tenor and alto saxophones: critics noted that Juliet's sax dominates the piece, rather than offering an image of equality. The play has frequently influenced popular music, including works by The Supremes, Bruce Springsteen, Tom Waits, Lou Reed, and Taylor Swift. The most famous such track is Dire Straits' "Romeo and Juliet".

The most famous musical theatre adaptation is "West Side Story" with music by Leonard Bernstein and lyrics by Stephen Sondheim. It débuted on Broadway in 1957 and in the West End in 1958 and was adapted as a popular film in 1961. This version updated the setting to mid-20th-century New York City and the warring families to ethnic gangs. Other musical adaptations include Terrence Mann's 1999 rock musical "William Shakespeare's Romeo and Juliet", co-written with Jerome Korman, Gérard Presgurvic's 2001 "Roméo et Juliette, de la Haine à l'Amour", Riccardo Cocciante's 2007 "Giulietta & Romeo" and J.C. Schütz and Johan Petterssons's 2013 adaptation "Carnival Tale ()" which takes place at a travelling carnival.

"Romeo and Juliet" had a profound influence on subsequent literature. Before then, romance had not even been viewed as a worthy topic for tragedy. In Harold Bloom's words, Shakespeare "invented the formula that the sexual becomes the erotic when crossed by the shadow of death". Of Shakespeare's works, "Romeo and Juliet" has generated the most—and the most varied—adaptations, including prose and verse narratives, drama, opera, orchestral and choral music, ballet, film, television, and painting. The word "Romeo" has even become synonymous with "male lover" in English.

"Romeo and Juliet" was parodied in Shakespeare's own lifetime: Henry Porter's "Two Angry Women of Abingdon" (1598) and Thomas Dekker's "Blurt, Master Constable" (1607) both contain balcony scenes in which a virginal heroine engages in bawdy wordplay. The play directly influenced later literary works. For example, the preparations for a performance form a major plot arc in Charles Dickens' "Nicholas Nickleby".

"Romeo and Juliet" is one of Shakespeare's most-illustrated works. The first known illustration was a woodcut of the tomb scene, thought to be by Elisha Kirkall, which appeared in Nicholas Rowe's 1709 edition of Shakespeare's plays. Five paintings of the play were commissioned for the Boydell Shakespeare Gallery in the late 18th century, one representing each of the five acts of the play. The 19th-century fashion for "pictorial" performances led to directors drawing on paintings for their inspiration, which, in turn, influenced painters to depict actors and scenes from the theatre. In the 20th century, the play's most iconic visual images have derived from its popular film versions.

In 2014, Simon & Schuster published "Juliet's Nurse", a novel by historian and former college professor Lois M. Leveen imagining the fourteen years leading up to the events in the play from the point of view of the nurse. The nurse has the third largest number of lines in the original play; only the eponymous characters have more lines.

The play was the subject of a 2017 GCSE question by the Oxford, Cambridge and RSA Examinations board that was administered to students. The board attracted widespread media criticism and derision after the question appeared to confuse the Capulets and the Montagues, with exams regulator Ofqual describing the error as unacceptable.

"Romeo and Juliet" was adapted into Manga format by publisher UDON Entertainment's Manga Classics imprint and was released in May 2018.

"Romeo and Juliet" may be the most-filmed play of all time. The most notable theatrical releases were George Cukor's multi-Oscar-nominated 1936 production, Franco Zeffirelli's 1968 version, and Baz Luhrmann's 1996 MTV-inspired "Romeo + Juliet". The latter two were both, in their time, the highest-grossing Shakespeare film ever. "Romeo and Juliet" was first filmed in the silent era, by Georges Méliès, although his film is now lost. The play was first heard on film in "The Hollywood Revue of 1929", in which John Gilbert recited the balcony scene opposite Norma Shearer.
Shearer and Leslie Howard, with a combined age over 75, played the teenage lovers in George Cukor's MGM 1936 film version. Neither critics nor the public responded enthusiastically. Cinemagoers considered the film too "arty", staying away as they had from Warner's "A Midsummer Night Dream" a year before: leading to Hollywood abandoning the Bard for over a decade. Renato Castellani won the "Grand Prix" at the Venice Film Festival for his 1954 film of "Romeo and Juliet". His Romeo, Laurence Harvey, was already an experienced screen actor. By contrast, Susan Shentall, as Juliet, was a secretarial student who was discovered by the director in a London pub and was cast for her "pale sweet skin and honey-blonde hair".

Stephen Orgel describes Franco Zeffirelli's 1968 "Romeo and Juliet" as being "full of beautiful young people, and the camera and the lush technicolour, make the most of their sexual energy and good looks". Zeffirelli's teenage leads, Leonard Whiting and Olivia Hussey, had virtually no previous acting experience but performed capably and with great maturity. Zeffirelli has been particularly praised, for his presentation of the duel scene as bravado getting out-of-control. The film courted controversy by including a nude wedding-night scene while Olivia Hussey was only fifteen.

Baz Luhrmann's 1996 "Romeo + Juliet" and its accompanying soundtrack successfully targeted the "MTV Generation": a young audience of similar age to the story's characters. Far darker than Zeffirelli's version, the film is set in the "crass, violent and superficial society" of Verona Beach and Sycamore Grove. Leonardo DiCaprio was Romeo and Claire Danes was Juliet.

The play has been widely adapted for TV and film. In 1960, Peter Ustinov's cold-war stage parody, "Romanoff and Juliet" was filmed. The 1961 film "West Side Story"—set among New York gangs—featured the Jets as white youths, equivalent to Shakespeare's Montagues, while the Sharks, equivalent to the Capulets, are Puerto Rican. In 2006, Disney's "High School Musical" made use of "Romeo and Juliet" plot, placing the two young lovers in different high school cliques instead of feuding families. Film-makers have frequently featured characters performing scenes from "Romeo and Juliet". The conceit of dramatising Shakespeare writing "Romeo and Juliet" has been used several times, including John Madden's 1998 "Shakespeare in Love", in which Shakespeare writes the play against the backdrop of his own doomed love affair. An anime series produced by Gonzo and SKY Perfect Well Think, called "Romeo x Juliet", was made in 2007 and the 2013 version is the latest English-language film based on the play. In 2013, Sanjay Leela Bhansali directed the Bollywood film "Goliyon Ki Raasleela Ram-Leela", a contemporary version of the play which starred Ranveer Singh and Deepika Padukone in leading roles. The film was a commercial and critical success. In February 2014, BroadwayHD released a filmed version of the 2013 Broadway Revival of "Romeo and Juliet". The production starred Orlando Bloom and Condola Rashad.

In April and May 2010, the Royal Shakespeare Company and the Mudlark Production Company presented a version of the play, entitled "Such Tweet Sorrow", as an improvised, real-time series of tweets on Twitter. The production used RSC actors who engaged with the audience as well each other, performing not from a traditional script but a "Grid" developed by the Mudlark production team and writers Tim Wright and Bethan Marlow. The performers also make use of other media sites such as YouTube for pictures and video.


All references to "Romeo and Juliet", unless otherwise specified, are taken from the Arden Shakespeare second edition (Gibbons, 1980) based on the Q2 text of 1599, with elements from Q1 of 1597. Under its referencing system, which uses Roman numerals, II.ii.33 means act 2, scene 2, line 33, and a 0 in place of a scene number refers to the prologue to the act.




</doc>
<doc id="25822" url="https://en.wikipedia.org/wiki?curid=25822" title="Rube Goldberg">
Rube Goldberg

Reuben Garrett Lucius Goldberg (July 4, 1883 – December 7, 1970), known best as Rube Goldberg, was an American cartoonist, sculptor, author, engineer, and inventor.

Goldberg is best known for his popular cartoons depicting complicated gadgets performing simple tasks in indirect, convoluted ways. The cartoons led to the expression "Rube Goldberg machines" to describe similar gadgets and processes. Goldberg received many honors in his lifetime, including a Pulitzer Prize for political cartooning in 1948 and the Banshees' Silver Lady Award in 1959. He was a founding member and first president of the National Cartoonists Society and the namesake of the Reuben Award, which the organization awards to its Cartoonist of the Year. He is the inspiration for international competitions known as Rube Goldberg Machine Contests which challenge participants to create a complicated machine to perform a simple task.

Goldberg was born in San Francisco, California, to Jewish parents Max and Hannah (Cohen) Goldberg. He was the third of seven children, three of whom died as children; older brother Garrett, younger brother Walter, and younger sister Lillian also survived. Goldberg began tracing illustrations when he was four years old, and first took professional drawing lessons when he was 11.

Goldberg married Irma Seeman on October 17, 1916. They lived at 98 Central Park West in New York City and had sons Thomas and George. During World War II, Goldberg insisted that his sons change their surname because of the amount of hatred towards him stemming from the political nature of his cartoons. Thomas chose the surname of George, in order to honor his brother; George, wanting to keep a sense of family cohesiveness, adopted the same surname.

Goldberg's father was a San Francisco police and fire commissioner, who encouraged the young Reuben to pursue a career in engineering. Rube graduated from the University of California, Berkeley in 1904 with a degree in Engineering and was hired by the city of San Francisco as an engineer for the Water and Sewers Department. After six months he resigned his position with the city to join the "San Francisco Chronicle" where he became a sports cartoonist. The following year, he took a job with the "San Francisco Bulletin", where he remained until he moved to New York City in 1907, finding employment as a cartoonist with the "New York Evening Mail".

The "New York Evening Mail" was syndicated to the first newspaper syndicate, the McClure Newspaper Syndicate, giving Goldberg's cartoons a wider distribution, and by 1915 he was earning $25,000 per year and being billed by the paper as America's most popular cartoonist. Arthur Brisbane had offered Goldberg $2,600 per year in 1911 in an unsuccessful attempt to get him to move to William Randolph Hearst's newspaper chain, and in 1915 raised the offer to $50,000 per year. Rather than lose Goldberg to Hearst, the "New York Evening Mail" matched the salary offer and formed the Evening Mail Syndicate to syndicate Goldberg's cartoons nationally.

In 1916, Goldberg created a series of seven short animated films, finding humorous aspects to details of everyday life in the form of an animated newsreel. The seven films were released on these dates in 1916: May 8, "The Boob Weekly"; May 22, "Leap Year"; June 5, "The Fatal Pie"; Jun 19, "From Kitchen Mechanic to Movie Star"; July 3, "Nutty News"; July 17, "Home Sweet Home"; July 31, "Losing Weight".

Goldberg was syndicated by the McNaught Syndicate from 1922 until 1934.

A prolific artist, Goldberg produced several cartoon series simultaneously, including "Mike and Ike (They Look Alike)", "Boob McNutt", "Foolish Questions", "What Are You Kicking About", "Telephonies", "Lala Palooza", "The Weekly Meeting of the Tuesday Women's Club", and the uncharacteristically serious soap-opera strip, "Doc Wright", which ran for 10 months beginning January 29, 1933. The cartoons that brought him lasting fame involved a character named Professor Lucifer Gorgonzola Butts. In that series, Goldberg drew labeled schematics of the comical "inventions" that would later bear his name. Professor Butts was based on a couple of college professors he studied with (and found boring) while earning his degree from the College of Mining and Engineering at the University of California from 1901 to 1903, Samuel B Christie and Frederick Slate.

From 1938 to 1941, Goldberg drew two weekly strips for the Register and Tribune Syndicate: "Brad and Dad" (1939–1941) and "Side Show" (1938–1941).

The popularity of Goldberg's cartoons was such that the term "Goldbergian" was in use in print by 1915, and "Rube Goldberg" by 1928. "Rube Goldberg" appeared in the "Random House Dictionary of the English Language" in 1966 meaning "having a fantastically complicated improvised appearance", or "deviously complex and impractical." The 1915 usage of "Goldbergian" was in reference to Goldberg's early comic strip "Foolish Questions" which he drew from 1909 to 1934, while later use of the terms "Goldbergian", "Rube Goldberg" and "Rube Goldberg machine" refer to the crazy inventions for which he is now best known from his strip "The Inventions of Professor Lucifer Gorgonzola Butts", drawn from 1914 to 1964.

The corresponding term in the UK was, and still is, "Heath Robinson", after the English illustrator with an equal devotion to odd machinery, also portraying sequential or chain reaction elements.

Goldberg's work was commemorated posthumously in 1995 with the inclusion of "Rube Goldberg's Inventions", depicting his 1931 "Self-Operating Napkin" in the Comic Strip Classics series of U.S. postage stamps.

Rube Goldberg wrote a feature film featuring his machines and sculptures called "Soup to Nuts", which was released in 1930 and starred Ted Healy and the pre-Curly Howard version of The Three Stooges.

In the 1962 John Wayne movie "Hatari!," an invention to catch monkeys by character Pockets, played by Red Buttons, is described as a "Rube Goldberg."

In the late 1960s and early 70s, educational shows like "Sesame Street", "Vision On" and "The Electric Company" routinely showed bits that involved Rube Goldberg devices, including the "Rube Goldberg Alphabet Contraption", and the "What Happens Next Machine".

Various other films and cartoons have included highly complicated machines that perform simple tasks. Among these are "Flåklypa Grand Prix", "Looney Tunes", "Tom and Jerry", "Wallace and Gromit", "Pee-wee's Big Adventure", "The Way Things Go", "Edward Scissorhands", "Back to the Future", "Honey, I Shrunk the Kids", "The Goonies", "Gremlins", the "Saw" film series, "Chitty Chitty Bang Bang", "The Cat from Outer Space", "Malcolm", "Hotel For Dogs", the "Home Alone" film series, "Family Guy", "American Dad!", and "Waiting..."

Also in the "Final Destination" film series the characters often die in Rube Goldberg-esque ways. In the film "The Great Mouse Detective", the villain Ratigan attempts to kill the film's heroes, Basil of Baker Street and David Q. Dawson, with a Rube Goldberg style device.
The classic video in this genre was done by the artist duo Peter Fischli & David Weiss in 1987 with their 30-minute video "Der Lauf der Dinge" or "The Way Things Go".

Honda produced a video in 2003 called "The Cog" using many of the same principles that Fischli and Weiss had done in 1987.

In 2005, the American alternative rock/indie band The Bravery released a video for their debut single, "An Honest Mistake," which features the band performing the song in the middle of a Rube Goldberg machine.

In 1999, an episode of "The X-Files" was titled "The Goldberg Variation". The episode intertwined characters FBI agents Mulder and Scully, a simple apartment super, Henry Weems (Willie Garson) and an ailing young boy, Ritchie Lupone (Shia LaBeouf) in a real-life Goldberg device.

The 2010 music video "This Too Shall Pass – RGM Version" by the rock band OK Go features a machine that, after four minutes of kinetic activity, shoots the band members in the face with paint. "RGM" presumably stands for Rube Goldberg Machine.

2012 The CBS show "Elementary" features a machine in its opening sequence.

2014 The Web Series, "Deadbeat", on Hulu features an episode titled, "The Ghost in the Machine," which features the protagonist, Kevin, helping the ghost of Rube Goldberg complete a contraption that will bring his grandchildren together after making a collection of random items into a machine that ends up systematically injuring two of his grandchildren so they end up in the same hospital and finally meet.

Both board games and video games have been inspired by Goldberg's creations, such as the 60's board game "Mouse Trap", the 1990s series of "The Incredible Machine" games, and "Crazy Machines". The "Humongous Entertainment" game "" involves searching for the missing pieces to a Rube Goldberg machine to complete the game.

In 1909 Goldberg invented the "Foolish Questions" game based on his successful cartoon by the same name. The game was published in many versions from 1909 to 1934.

"", the first game authorized by The Heirs of Rube Goldberg, was published by Unity Games (the publishing arm of Unity Technologies) in November 2013.




</doc>
<doc id="25823" url="https://en.wikipedia.org/wiki?curid=25823" title="Robert Stickgold">
Robert Stickgold

Robert Stickgold is an associate professor of psychiatry at the Harvard Medical School and the Beth Israel Deaconess Medical Center. A sleep researcher, his work focuses on the relationship between sleep and learning. His articles in the popular press are intended to illustrate the dangers of sleep deprivation.

Stickgold was born in Chicago. He graduated from Harvard University before attending the University of Wisconsin–Madison, where he received his doctorate in biochemistry. He worked with the sleep researcher J. Allan Hobson for many years and has been known to quote Hobson's quip: "The only known function of sleep is to cure sleepiness". Stickgold's research has focused on sleep and cognition, dreaming, and conscious states. He has been a proponent of the role of sleep in memory consolidation. Additional research has focused on dreaming. In one experiment, participants played the computer game Tetris for three days and reported dreaming about falling geometric shapes--a phenomenon now known as the Tetris effect. Even patients with anterograde amnesia, who did not remember playing the game, had similar dreams as normal participants. Similar results were found in another study utilizing the video game Alpine Racer 2. Participants reported dreaming about skiing.

Stickgold lives in Cambridge, Massachusetts and has four children.



</doc>
<doc id="25824" url="https://en.wikipedia.org/wiki?curid=25824" title="Religion and mythology">
Religion and mythology

Mythology is the main component of Religion. It refers to systems of concepts that are of high importance to a certain community, making statements concerning the supernatural or sacred. Religion is the broader term, besides mythological system, it includes ritual. A given mythology is almost always associated with a certain religion such as Greek mythology with Ancient Greek religion. Disconnected from its religious system, a myth may lose its immediate relevance to the community and evolve—away from sacred importance—into a legend or folktale. 

There is a complex relationship between recital of myths and enactment of rituals.

The relationship between religion and myth depends on what definition of "myth" one uses. By Robert Graves's definition, a religion's traditional stories are "myths" if and only if one does not belong to the religion in question. By Segal's definition, all religious stories are myths—but simply because nearly all stories are myths. By the folklorists' definition, all myths are religious (or "sacred") stories, but not all religious stories are myths: religious stories that involve the creation of the world (e.g., the stories in the Book of Genesis) are myths; however, some religious stories that don't explain how things came to be in their present form (e.g., hagiographies of famous saints) are not myths. Generally, mythology is the main component of religion alongside ritual. For example, in the early modern period, distinguished Christian theologians developed elaborated witch mythologies which contributed to the intensification of witch trials. "The Oxford Companion to World Mythology" provides the following summary and examples:Religious stories are “holy scripture” to believers—narratives used to support, explain, or justify a particular system’s rituals, theology, and ethics—and are myths to people of other cultures or belief systems. […] It is difficult to believe that the Buddha was conceived in a dream by a white elephant, so we call that story a myth as well. But, of course, stories such as the parting of the Sea of Reeds for the fleeing Hebrews, Muhammad’s Night Journey, and the dead Jesus rising from the tomb are just as clearly irrational narratives to which a Hindu or a Buddhist might understandably apply the word “myth.” All of these stories are definable as myths because they contain events that contradict both our intellectual and physical experience of reality.

Most definitions of "myth" limit myths to stories. Thus, non-narrative elements of religion, such as ritual, are not myths.
The term theology for the first time appears in the writings of the Greek philosophers Plato and Aristotle. Initially, theology and mythology were synonymous. With time, both terms gained distinctive qualities:

In the first place, theology is a spiritual or religious attempt of “believers” to explicate their faith. In this sense, it is not neutral and is not attempted from the perspective of removed observation—in contrast to a general history of religions. The implication derived from the religious approach is that it does not provide a formal and indifferent scheme devoid of presuppositions within which all religions could be subsumed. In the second place, theology is influenced by its origins in the Greek and Christian traditions, with the implication that the transmutation of this concept to other religions is endangered by the very circumstances of origination.

According to Hege, both primitive and modern theology is inescapably constrained by its mythical backbone:

Hermeneutically, theologians must recognize that mythical thought permeates the biblical texts. Dogmatically, theologians must be aware of the mythological elements of theology and of how extensively theology relies on mythical forms and functions, especially in light of our awareness of the ubiquity of myth.
"Religion" is a belief concerning the supernatural, sacred, or divine, and the moral codes, practices, values, and institutions associated with such belief, although some scholars, such as Durkheim, would argue that the supernatural and the divine are not aspects of all religions. Religious beliefs and practices may include the following: a deity or higher being, eschatology, practices of worship, practices of ethics and politics. Some religions do not include all these features.

The term "mythology" usually refers either to a system of myths or to the study of myths. However, the word "myth" itself has multiple (and some contradictory) definitions:

In regards to the study of culture and religion, these are some of the definitions scholars have used:


Given any of the above definitions of "myth", the myths of many religions, both ancient and modern, share common elements. Widespread similarities between religious mythologies include the following:

The similarities between cultures and time periods can be useful, but it is usually not easy to combine beliefs and histories from different groups. Simplification of cultures and time periods by eliminating detailed data remain vulnerable or flimsy in this area of research.

Though there are similarities among most religious mythologies, there are also contrasts. Many mythologies focus on explanations of the universe, natural phenomena, or other themes of human existence, often ascribing agency to one or more deities or other supernatural forces. However, some religions have very few of this kind of story of cosmic explanation. For instance, the Buddhist parable of the arrow warns against such speculations as "[Is] the world eternal or not eternal? [Is] the soul different from the body? [Does] the enlightened exist after death or not?", viewing them as irrelevant to the goal of escaping suffering.

In academia, the term "myth" often refers to stories whose culture regards them as true (as opposed to fictitious). Thus, many scholars will call a body of stories "mythology", leaving open the question of whether the stories are true or false. For example, in "Tree of Souls: The Mythology of Judaism", English professor Howard Schwartz writes, "the definition of 'mythology' offered here does not attempt to determine if biblical or subsequent narratives are true or false, i.e., historically accurate or not".

Since the beginning of modern philosophy and science in the 16th century, many Western intellectuals have seen myth as outdated. In fact, some argued that the Christian religion would be better off without mythology, or even that Christianity would be better off without religion:
[J. A. T.] Robinson argued in favor of "the detaching of the Christian doctrine of God from any necessary dependence on a 'supernaturalistic' worldview". He understood this as a prophetic aspect of the Church's ministry to the world. [...] At this time atheism was regarded as the Christian Gospel that should be preached to the world. J. J. Altizer, for example, maintained [this] boldly by stating, "Throughout its history Christian theology has been thwarted from reaching its intrinsic goal by its bondage to a transcendent, a sovereign, and an impassive God". [...] [Dietrich] Bonhoffer called persistently for "Religionless Christianity".
In the 20th century, many scholars have resisted this trend, defending myth from modern criticism. Mircea Eliade, a professor of the history of religions, declared that myth did not hold religion back, that myth was an essential foundation of religion, and that eliminating myth would eliminate a piece of the human psyche. Eliade approached myth sympathetically at a time when religious thinkers were trying to purge religion of its mythological elements:
Eliade wrote about "sky and sky gods" when Christian theology was shaken at its very foundations by the "death of God" theology. He spoke of "God up there" when theologians such as J. A. T. Robinson were busy with erasing the mythical language of [a] three-storied universe that underlies the early Christian thought and experience.
Similarly, Joseph Campbell believed that people could not understand their individual lives without mythology to aid them. By recalling the significance of old myths, he encouraged awareness of them. In responding to the interview question "How would you define mythology?", Joseph Campbell answered:
My favorite definition of mythology: other people's religion. My favorite definition of religion: misunderstanding of mythology.

Most religions contain a body of traditional sacred stories that are believed to express profound truth. Some religious organizations and practitioners believe that some or all of their traditional stories are not only sacred and "true" but also historically accurate and divinely revealed and that calling such stories "myths" disrespects their special status. Other religious organizations and practitioners have no problem with categorizing their sacred stories as myths.

Some religious believers take offense when what they consider to be historical aspects of their faith are labeled as "myth". Such believers distinguish between religious fables or myths, on one hand, and those sacred narratives which are described by their tradition as being history or revelation, on the other. For instance, Catholic priest Father John A. Hardon insists that "Christianity is not mythology. What we believe in is not religious fantasies, no matter how pious." Evangelical Christian theologian Carl F. H. Henry insisted that "Judeo-Christian revelation has nothing in common with the category of myth".

Especially within Christianity, objection to the word "myth" rests on a historical basis. By the time of Christ, the Greco-Roman world had started to use the term "myth" (Greek "muthos") to mean "fable, fiction, lie"; as a result, the early Christian theologians used "myth" in this sense. Thus, the derogatory meaning of the word "myth" is the traditional Christian meaning, and the expression "Christian mythology", as used in academic discourse, may offend Christians for this reason.

In addition, this early Christian use of the term "myth" passed into popular usage. Thus, when essential sacred mysteries and teachings are described as "myth", in modern English, the word often still implies that it is "idle fancy, fiction, or falsehood". This description could be taken as a direct attack on religious belief, quite contrary to the meaning ostensibly intended by the academic use of the term. Further, in academic writing, though "myth" usually means a fundamental worldview story, even there it is occasionally ambiguous or clearly denotes "falsehood", as in the "Christ myth theory". The original term "mythos" (which has no pejorative connotation in English) may be a better word to distinguish the positive definition from the negative.

Modern day clergy and practitioners within some religious movements have no problem classifying the religion's sacred stories as "myths". They see the sacred texts as indeed containing religious truths, divinely inspired but delivered in the language of mankind. Some examples follow.

J.R.R. Tolkien's love of myths and devout Catholic faith came together in his assertion that mythology is the divine echo of "the Truth". Tolkien wrote that myths held "fundamental things". He expressed these beliefs in his poem "Mythopoeia" circa 1931, which describes myth-making as an act of "sub-creation" within God's primary creation. The poem in part says creation is "myth-woven and elf-patterned":

Tolkien's opinion was adopted by another Christian writer, C. S. Lewis, in their conversations: "Tolkien explained to Lewis that the story of Christ was the true myth at the very heart of history and at the very root of reality." C. S. Lewis freely called the Christ story a "true myth", and he believed that even pagan myths express spiritual truths. In his opinion, the difference between the Christ story and pagan myths is that the Christ story is historically as well as spiritually true. Lewis writes,
The story of Christ is simply a true myth: a myth working on us in the same way as the others, but with this tremendous difference that it really happened: and one must be content to accept it in the same way, remembering that it is God's myth where the others are men's myths: i. e. the Pagan stories are God expressing Himself through the minds of poets, using such images as He found there, while Christianity is God expressing Himself through what we call real things.

Another Christian writer, the Catholic priest Father Andrew Greeley, freely applies the term "myth" to Christianity. In his book "Myths of Religion", he defends this terminology:
Many Christians have objected to my use of this word [myth] even when I define it specifically. They are terrified by a word which may even have a slight suggestion of fantasy. However, my usage is the one that is common among historians of religion, literary critics, and social scientists. It is a valuable and helpful usage; there is no other word which conveys what these scholarly traditions mean when they refer to myth. The Christian would be well advised to get over his fear of the word and appreciate how important a tool it can be for understanding the content of his faith.

At a "Consultation on the Relationship Between the Wesleyan Tradition
and the Natural Sciences" in Kansas City, Missouri, on October 19, 1991, Dennis Bratcher presented a discussion of the adaptation of Near Eastern mythical thought by the Israelites. Bratcher argued that the Old Testament absorbed Near Eastern pagan mythology (although he drew a sharp distinction between the literally-interpreted myths of the Near Eastern pagans and the "mythopoetic" use of imagery from pagan myths by the Hebrews). During this presentation, he gave the following disclaimer:
the term "myth" as used here does not mean "false" or "fiction." Even in my old and yellowed Webster's, "fiction" is the "third" meaning of the word. In its primary and more technical meaning "myth" refers to a story or group of stories that serve to explain how a particular society views their world.

Some Jewish scholars, including Dov Noy, a professor of folklore at Hebrew University and founder of the Israel Folktale Archives, and Howard Schwartz, Jewish anthologist and English professor at the University of Missouri – St. Louis, have discussed traditional Jewish stories as "mythology".

Schwartz authored the book "Tree of Souls: The Mythology of Judaism". It consists of myths and belief statements excerpted from—and, in some cases, synthesized from a number of excerpts from—both Biblical and non-Biblical Jewish texts. According to Schwartz, the Jewish people continue to elaborate on, and compose additions to, their traditional mythology. In the book's introduction, Schwartz states that the word "myth", as used in the book, "is not offered to mean something that is not true, as in the current popular usage".

Neopagans frequently refer to their sacred stories as "myths". Asatru, a modern-day revival of Germanic Paganism, holds "that the Eddas, Myths and Norse Sagas are the divinely inspired wisdom of [its] religion". Wicca, another Neopagan movement, also applies the term "mythology" to its stories.

The Dewey Decimal system covers "religion" in the 200 range, with books on ""Religious mythology & social theology"", a subset listed under 201.





</doc>
