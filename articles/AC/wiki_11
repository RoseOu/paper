<doc id="25231" url="https://en.wikipedia.org/wiki?curid=25231" title="QuickTime">
QuickTime

QuickTime is an extensible multimedia framework developed by Apple Inc., capable of handling various formats of digital video, picture, sound, panoramic images, and interactivity. First made in 1991, the latest Mac version, QuickTime X, is currently available on Mac OS X Snow Leopard and newer. Apple ceased support for the Windows version of QuickTime in 2016, and ceased support for QuickTime 7 on macOS in 2018.

As of Mac OS X Lion, the underlying media framework for QuickTime, QTKit, was deprecated in favor of a newer graphics framework, AVFoundation, and completely discontinued as of macOS Catalina.

QuickTime is bundled with macOS. QuickTime for Microsoft Windows is downloadable as a standalone installation, and was bundled with Apple's iTunes prior to iTunes 10.5, but is no longer supported and therefore security vulnerabilities will no longer be patched. Already, at the time of the Windows version's discontinuation, two such zero-day vulnerabilities (both of which permitted arbitrary code execution) were identified and publicly disclosed by Trend Micro; consequently, Trend Micro strongly advised users to uninstall the product from Windows systems.

Software development kits (SDK) for QuickTime are available to the public with an Apple Developer Connection (ADC) subscription.

It is available free of charge for both macOS and Windows operating systems. There are some other free player applications that rely on the QuickTime framework, providing features not available in the basic QuickTime Player. For example, iTunes can export audio in WAV, AIFF, MP3, AAC, and Apple Lossless. In addition, macOS has a simple AppleScript that can be used to play a movie in full-screen mode, but since version 7.2 full-screen viewing is now supported in the non-Pro version.

QuickTime Player 7 is limited to only basic playback operations unless a QuickTime Pro license key is purchased from Apple. Until recently, Apple's professional applications (e.g. Final Cut Studio, Logic Studio) included a QuickTime Pro license. Pro keys are specific to the major version of QuickTime for which they are purchased and unlock additional features of the QuickTime Player application on macOS or Windows. The Pro key does not require any additional downloads; entering the registration code immediately unlocks the hidden features.

QuickTime 7 is still available for download from Apple, but as of mid 2016, Apple stopped selling registration keys for the Pro version.

Features enabled by the Pro license include, but are not limited to:

Mac OS X Snow Leopard includes QuickTime X. QuickTime Player X lacks cut, copy and paste and will only export to four formats, but its limited export feature is free. Users do not have an option to upgrade to a Pro version of QuickTime X, but those who have already purchased QuickTime 7 Pro and are upgrading to Snow Leopard from a previous version of Mac OS X will have QuickTime 7 stored in the Utilities or user defined folder. Otherwise, users will have to install QuickTime 7 from the "Optional Installs" directory of the Snow Leopard DVD after installing the OS.

Mac OS X Lion and later also include QuickTime X. No installer for QuickTime 7 is included with these software packages, but users can download the QuickTime 7 installer from the Apple support site. QuickTime X on later versions of macOS support cut, copy and paste functions similarly to the way QuickTime 7 Pro did; the interface has been significantly modified to simplify these operations, however.

On September 24, 2018 Apple ended support for QuickTime 7 and QuickTime Pro, and updated many download and support pages on their website stating that QuickTime 7 "will not be compatible with future macOS releases".

The QuickTime framework provides the following:


As of early 2008, the framework hides many older codecs listed below from the user although the option to "Show legacy encoders" exists in QuickTime Preferences to use them. The framework supports the following file types and codecs natively:

Due to macOS Mojave being the last version to include support for 32-bit APIs and Apple's plans to drop 32-bit application support in future macOS releases, many codecs will no longer be supported in newer macOS releases, starting with macOS Catalina, which was released on October 7, 2019.

PictureViewer is a component of QuickTime for Microsoft Windows and the Mac OS 8 and Mac OS 9 operating systems. It is used to view picture files from the still image formats that QuickTime supports. In macOS, it is replaced by Preview.

As of version 7.7.9, the Windows version requires one to go to their "Windows Uninstall Or Change A Program" screen to "modify" their installation of QuickTime 7 to include the "Legacy QuickTime Feature" of "QuickTime PictureViewer".

The native file format for QuickTime video, QuickTime File Format, specifies a multimedia container file that contains one or more tracks, each of which stores a particular type of data: audio, video, effects, or text (e.g. for subtitles). Each track either contains a digitally encoded media stream (using a specific format) or a data reference to the media stream located in another file. The ability to contain abstract data references for the media data, and the separation of the media data from the media offsets and the track edit lists means that QuickTime is particularly suited for editing, as it is capable of importing and editing in place (without data copying).

Other file formats that QuickTime supports natively (to varying degrees) include AIFF, WAV, DV-DIF, MP3, and MPEG program stream. With additional QuickTime Components, it can also support ASF, DivX Media Format, Flash Video, Matroska, Ogg, and many others.

On February 11, 1998, the ISO approved the QuickTime file format as the basis of the MPEG‑4 file format. The MPEG-4 file format specification was created on the basis of the QuickTime format specification published in 2001. The MP4 (codice_1) file format was published in 2001 as the revision of the MPEG-4 Part 1: Systems specification published in 1999 (ISO/IEC 14496-1:2001). In 2003, the first version of MP4 format was revised and replaced by MPEG-4 Part 14: MP4 file format (ISO/IEC 14496-14:2003). The MP4 file format was generalized into the ISO Base Media File Format ISO/IEC 14496-12:2004, which defines a general structure for time-based media files. It in turn is used as the basis for other multimedia file formats (for example 3GP, Motion JPEG 2000). A list of all registered extensions for ISO Base Media File Format is published on the official registration authority website www.mp4ra.org. This registration authority for code-points in "MP4 Family" files is Apple Computer Inc. and it is named in Annex D (informative) in MPEG-4 Part 12.

By 2000, MPEG-4 formats became industry standards, first appearing with support in QuickTime 6 in 2002. Accordingly, the MPEG-4 container is designed to capture, edit, archive, and distribute media, unlike the simple file-as-stream approach of MPEG-1 and MPEG-2.

QuickTime 6 added limited support for MPEG-4; specifically encoding and decoding using Simple Profile (SP). Advanced Simple Profile (ASP) features, like B-frames, were unsupported (in contrast with, for example, encoders such as XviD or 3ivx). QuickTime 7 supports the H.264 encoder and decoder.

Because both MOV and MP4 containers can use the same MPEG-4 codecs, they are mostly interchangeable in a QuickTime-only environment. MP4, being an international standard, has more support. This is especially true on hardware devices, such as the Sony PSP and various DVD players; on the software side, most DirectShow / Video for Windows codec packs include a MP4 parser, but not one for MOV.

In QuickTime Pro's MPEG-4 Export dialog, an option called "Passthrough" allows a clean export to MP4 without affecting the audio or video streams. QuickTime 7 now supports multi-channel AAC-LC and HE-AAC audio (used, for example, in the high-definition trailers on Apple's site), for both .MOV and .MP4 containers.

Apple released the first version of QuickTime on December 2, 1991 as a multimedia add-on for System 6 and later. The lead developer of QuickTime, Bruce Leak, ran the first public demonstration at the May 1991 Worldwide Developers Conference, where he played Apple's famous 1984 advertisement in a window at 320×240 pixels resolution.

The original video codecs included:


The first commercial project produced using QuickTime 1.0 was the CD-ROM From Alice to Ocean. The first publicly visible use of QuickTime was Ben & Jerry's interactive factory tour (dubbed "The Rik & Joe Show" after its in-house developers). "The Rik and Joe Show" was demonstrated onstage at MacWorld in San Francisco when John Sculley announced QuickTime.

Apple released QuickTime 1.5 for Mac OS in the latter part of 1992. This added the SuperMac-developed Cinepak vector-quantization video codec (initially known as Compact Video). It could play video at 320×240 resolution at 30 frames per second on a 25 MHz Motorola 68040 CPU. It also added "text" tracks, which allowed for captioning, lyrics and other potential uses.

Apple contracted San Francisco Canyon Company to port QuickTime to the Windows platform. Version 1.0 of QuickTime for Windows provided only a subset of the full QuickTime API, including only movie playback functions driven through the standard movie controller.

QuickTime 1.6 came out the following year. Version 1.6.2 first incorporated the "QuickTime PowerPlug" which replaced some components with PowerPC-native code when running on PowerPC Macs.

Apple released QuickTime 2.0 for System Software 7 in June 1994—the only version never released for free. It added support for music tracks, which contained the equivalent of MIDI data and which could drive a sound-synthesis engine built into QuickTime itself (using a limited set of instrument sounds licensed from Roland), or any external MIDI-compatible hardware, thereby producing sounds using only small amounts of movie data.

Following Bruce Leak's departure to Web TV, the leadership of the QuickTime team was taken over by Peter Hoddie.

QuickTime 2.0 for Windows appeared in November 1994 under the leadership of Paul Charlton. As part of the development effort for cross-platform QuickTime, Charlton (as architect and technical lead), along with ace individual contributor Michael Kellner and a small highly effective team including Keith Gurganus, ported a subset of the Macintosh Toolbox to Intel and other platforms (notably, MIPS and SGI Unix variants) as the enabling infrastructure for the QuickTime Media Layer (QTML) which was first demonstrated at the Apple Worldwide Developers Conference (WWDC) in May 1996. The QTML later became the foundation for the Carbon API which allowed legacy Macintosh applications to run on the Darwin kernel in Mac OS X.

The next versions, 2.1 and 2.5, reverted to the previous model of giving QuickTime away for free. They improved the music support and added sprite tracks which allowed the creation of complex animations with the addition of little more than the static sprite images to the size of the movie. QuickTime 2.5 also fully integrated QuickTime VR 2.0.1 into QuickTime as a QuickTime extension. On January 16, 1997, Apple released the QuickTime MPEG Extension (PPC only) as an add-on to QuickTime 2.5, which added software MPEG-1 playback capabilities to QuickTime.

In 1994, Apple filed suit against software developer San Francisco Canyon for intellectual property infringement and breach of contract. Apple alleged that San Francisco Canyon had helped develop Video for Windows using several hundred lines of unlicensed QuickTime source code, which was subsequently unilaterally removed. Microsoft and Intel were added to the lawsuit in 1995. The suit ended in a settlement in 1997.

The release of QuickTime 3.0 for Mac OS on March 30, 1998 introduced the now-standard revenue model of releasing the software for free, but with additional features of the Apple-provided MoviePlayer application that end-users could only unlock by buying a QuickTime Pro license code. Since the "Pro" features were the same as the existing features in QuickTime 2.5, any previous user of QuickTime could continue to use an older version of the central MoviePlayer application for the remaining lifespan of Mac OS to 2002; indeed, since these additional features were limited to MoviePlayer, any other QuickTime-compatible application remained unaffected.

QuickTime 3.0 added support for graphics importer components that could read images from GIF, JPEG, TIFF and other file formats, and video output components which served primarily to export movie data via FireWire. Apple also licensed several third-party technologies for inclusion in QuickTime 3.0, including the Sorenson Video codec for advanced video compression, the QDesign Music codec for substantial audio compression, and the complete Roland Sound Canvas instrument set and GS Format extensions for improved playback of MIDI music files. It also added video "effects" which programmers could apply in real-time to video tracks. Some of these effects would even respond to mouse clicks by the user, as part of the new movie interaction support (known as wired movies).

During the development cycle for QuickTime 3.0, part of the engineering team was working on a more advanced version of QuickTime to be known as QuickTime interactive or QTi. Although similar in concept to the wired movies feature released as part of QuickTime 3.0, QuickTime interactive was much more ambitious. It allowed any QuickTime movie to be a fully interactive and programmable container for media. A special track type was added that contained an interpreter for a custom programming language based on 68000 assembly language. This supported a comprehensive user interaction model for mouse and keyboard event handling based in part on the AML language from the Apple Media Tool.

The QuickTime interactive movie was to have been the playback format for the next generation of HyperCard authoring tool. Both the QuickTime interactive and the HyperCard 3.0 projects were canceled in order to concentrate engineering resources on streaming support for QuickTime 4.0, and the projects were never released to the public.

Apple released QuickTime 4.0 on June 8, 1999 for Mac OS 7.5.5 through 8.6 (later Mac OS 9) and Windows 95, Windows 98, and Windows NT. Three minor updates (versions 4.0.1, 4.0.2, and 4.0.3) followed.
It introduced features that most users now consider basic:


On December 17, 1999, Apple provided QuickTime 4.1, this version's first major update. Two minor versions (4.1.1 and 4.1.2) followed. The most notable improvements in the 4.1.x family were:


QuickTime 5 was one of the shortest-lived versions of QuickTime, released in April 2001 and superseded by QuickTime 6 a little over a year later. This version was the last to have greater capabilities under Mac OS 9 than under Mac OS X, and the last version of QuickTime to support Mac OS versions 7.5.5 through 8.5.1 on a PowerPC Mac and Windows 95. Version 5.0 was initially only released for Mac OS and Mac OS X on April 14, 2001, and version 5.0.1 followed shortly thereafter on April 23, 2001, supporting the classic Mac OS, Mac OS X, and Windows. Three more updates to QuickTime 5 (versions 5.0.2, 5.0.4, and 5.0.5) were released over its short lifespan.

QuickTime 5 delivered the following enhancements:

On July 15, 2002, Apple released QuickTime 6.0, providing the following features:


QuickTime 6 was initially available for Mac OS 8.6 – 9.x, Mac OS X (10.1.5 minimum), and Windows 98, Me, 2000, and XP. Development of QuickTime 6 for Mac OS slowed considerably in early 2003, after the release of Mac OS X v10.2 in August 2002. QuickTime 6 for Mac OS continued on the 6.0.x path, eventually stopping with version 6.0.3.

QuickTime 6.1 & 6.1.1 for Mac OS X v10.1 and Mac OS X v10.2 (released October 22, 2002) and QuickTime 6.1 for Windows (released March 31, 2003) offered ISO-Compliant MPEG-4 file creation and fixed the CAN-2003-0168 vulnerability.

Apple released QuickTime 6.2 exclusively for Mac OS X on April 29, 2003 to provide support for iTunes 4, which allowed AAC encoding for songs in the iTunes library. (iTunes was not available for Windows until October 2003.)

On June 3, 2003, Apple released QuickTime 6.3, delivering the following:


QuickTime 6.4, released on October 16, 2003 for Mac OS X v10.2, Mac OS X v10.3, and Windows, added the following:


On December 18, 2003, Apple released QuickTime 6.5, supporting the same systems as version 6.4. Versions 6.5.1 and 6.5.2 followed on April 28, 2004 and October 27, 2004. These versions would be the last to support Windows 98 and Me. The 6.5 family added the following features:


QuickTime 6.5.3 was released on October 12, 2005 for Mac OS X v10.2.8 after the release of QuickTime 7.0, fixing a number of security issues.

Initially released on April 29, 2005 in conjunction with Mac OS X v10.4 (for version 10.3.9 and 10.4.x), QuickTime 7.0 featured the following:

After a couple of preview Windows releases, Apple released 7.0.2 as the first stable release on September 7, 2005 for Windows 2000 and Windows XP. Version 7.0.4, released on January 10, 2006 was the first universal binary version. But it suffered numerous bugs, including a buffer overrun, which is more problematic to most users.

Apple dropped support for Windows 2000 with the release of QuickTime 7.2 on July 11, 2007. The last version available for Windows 2000, 7.1.6, contains numerous security vulnerabilities. References to this version have been removed from the QuickTime site, but it can be downloaded from Apple's support section. Apple has not indicated that they will be providing any further security updates for older versions. QuickTime 7.2 is the first version for Windows Vista.

Apple dropped support for Flash content in QuickTime 7.3, breaking content that relied on Flash for interactivity, or animation tracks. Security concerns seem to be part of the decision. Flash flv files can still be played in QuickTime if the free Perian plugin is added.

In QuickTime 7.3, a processor that supports SSE is required. QuickTime 7.4 does not require SSE. Unlike versions 7.2 and 7.3, QuickTime 7.4 cannot be installed on Windows XP without service packs or with Service Pack 1/1A installed (its setup program checks if Service Pack 2 is installed).

QuickTime 7.5 was released on June 10, 2008. QuickTime 7.5.5 was released on September 9, 2008, which requires Mac OS X v10.4 or higher, dropping 10.3 support. QuickTime 7.6 was released on January 21, 2009. QuickTime 7.7 was released on August 23, 2011.

QuickTime 7.6.6 is available for OS X, 10.6.3 Snow Leopard until 10.14 Mojave, as 10.15 Catalina will only support 64-bit applications. There is a 7.7 release of QuickTime 7 for OS X, but it is only for Leopard 10.5.

QuickTime 7.7.6 is the last release for Windows XP. As it's since version 7.4, they can be installed here only when Service Pack 2 or 3 is installed.

QuickTime 7.7.9 is the last Windows release of QuickTime. Apple stopped supporting QuickTime on Windows afterwards.

Safari 12, released on September 17, 2018 for macOS Sierra and macOS High Sierra (and the default browser included on macOS Mojave released on September 24, 2018), which drops support for NPAPI plug-ins (except for Adobe Flash) dropped its support for QuickTime 7's web plugin. On September 24, 2018 Apple dropped support for the macOS version of QuickTime 7. This effectively marked the end of the technology in Apple's codec and web development.

Starting with macOS Catalina, QuickTime 7 applications, image, audio and video codecs will no longer be compatible with macOS or supported by Apple.

QuickTime X (pronounced "QuickTime Ten") was initially demonstrated at WWDC on June 8, 2009, and shipped with Mac OS X v10.6.

It includes visual chapters, conversion, sharing to YouTube, video editing, capture of video and audio streams, screen recording, GPU acceleration, and live streaming.

But it removed support for various widely used formats; in particular the omission of MIDI caused significant inconvenience and trouble to many musicians and their potential audiences.

In addition, a screen recorder is featured which records whatever is on the screen. However, to prevent bootlegging the user is unable to record any video that is played on the DVD Player or purchased content from iTunes, thus being greyed out.

The reason for the jump in numbering from 7 to 10 (X) was to indicate a similar break with the previous versions of the product that Mac OS X indicated. QuickTime X is fundamentally different from previous versions, in that it is provided as a Cocoa (Objective-C) framework and breaks compatibility with the previous QuickTime 7 C-based APIs that were previously used. QuickTime X was completely rewritten to implement modern audio video codecs in 64-bit. QuickTime X is a combination of two technologies: QuickTime Kit Framework (QTKit) and QuickTime X Player. QTKit is used by QuickTime player to display media. QuickTime X does not implement all of the functionality of the previous QuickTime as well as some of the codecs. When QuickTime X attempts to operate with a 32-bit codec or perform an operation not supported by QuickTime X, it will start a 32-bit helper process to perform the requested operation. The website "Ars Technica" revealed that QuickTime X uses QuickTime 7.x via QTKit to run older codecs that have not made the transition to 64-bit.

QuickTime X does not support .SRT subtitle files. It has been suggested using the program Subler to interleave the MP4 and SRT files will fix this oversight, which can be downloaded at Bitbucket. 

QuickTime 7 may still be required to support older formats on Snow Leopard such as QTVR, interactive QuickTime movies, and MIDI files. In such cases, a compatible version of QuickTime 7 is included on Snow Leopard installation disc and may be installed side-by-side with QuickTime X. Users who have a Pro license for QuickTime 7 can then activate their license.

A Snow Leopard compatible version of QuickTime 7 may also be downloaded from Apple Support website.

The software got an increment with the release of Mavericks, and as of August 2018, the current version is v10.5. It contains more sharing options (email, YouTube, Facebook, Flickr etc.), more export options (including web export in multiple sizes, and export for iPhone 4/iPad/Apple TV (but not Apple TV 2) ). It also includes a new way of fast forwarding through a video and mouse support for scrolling.

Starting with macOS Catalina, Apple only provides QuickTime X, as QuickTime 7 was never updated to 64-bit, affecting many applications, image, audio and video formats utilizing QuickTime 7, and compatibility with these codecs in QuickTime X.

QuickTime X previously provided the QTKit Framework on Mac OS 10.6 until 10.14.<ref>


</doc>
<doc id="25232" url="https://en.wikipedia.org/wiki?curid=25232" title="Quoin (disambiguation)">
Quoin (disambiguation)

Quoins are masonry blocks at the corner of a wall.

Quoin or Du Quoin may also refer to:





</doc>
<doc id="25233" url="https://en.wikipedia.org/wiki?curid=25233" title="Quartz">
Quartz

Quartz is a hard, crystalline mineral composed of silicon and oxygen atoms. The atoms are linked in a continuous framework of SiO silicon–oxygen tetrahedra, with each oxygen being shared between two tetrahedra, giving an overall chemical formula of SiO. Quartz is the second most abundant mineral in Earth's continental crust, behind feldspar.

Quartz exists in two forms, the normal α-quartz and the high-temperature β-quartz, both of which are chiral. The transformation from α-quartz to β-quartz takes place abruptly at . Since the transformation is accompanied by a significant change in volume, it can easily induce fracturing of ceramics or rocks passing through this temperature threshold.

There are many different varieties of quartz, several of which are semi-precious gemstones. Since antiquity, varieties of quartz have been the most commonly used minerals in the making of jewelry and hardstone carvings, especially in Eurasia.

The word "quartz" is derived from the German word "Quarz", which had the same form in the first half of the 14th century in Middle High German in East Central German and which came from the Polish dialect term "kwardy", which corresponds to the Czech term "tvrdý" ("hard").

The Ancient Greeks referred to quartz as κρύσταλλος ("krustallos") derived from the Ancient Greek "κρύος" ("kruos") meaning "icy cold", because some philosophers (including Theophrastus) apparently believed the mineral to be a form of supercooled ice. Today, the term "rock crystal" is sometimes used as an alternative name for the purest form of quartz.

Quartz belongs to the trigonal crystal system. The ideal crystal shape is a six-sided prism terminating with six-sided pyramids at each end. In nature quartz crystals are often twinned (with twin right-handed and left-handed quartz crystals), distorted, or so intergrown with adjacent crystals of quartz or other minerals as to only show part of this shape, or to lack obvious crystal faces altogether and appear massive. Well-formed crystals typically form in a 'bed' that has unconstrained growth into a void; usually the crystals are attached at the other end to a matrix and only one termination pyramid is present. However, doubly terminated crystals do occur where they develop freely without attachment, for instance within gypsum. A quartz geode is such a situation where the void is approximately spherical in shape, lined with a bed of crystals pointing inward.

α-quartz crystallizes in the trigonal crystal system, space group "P"321 or "P"321 depending on the chirality. β-quartz belongs to the hexagonal system, space group "P"622 and "P"622, respectively. These space groups are truly chiral (they each belong to the 11 enantiomorphous pairs). Both α-quartz and β-quartz are examples of chiral crystal structures composed of achiral building blocks (SiO tetrahedra in the present case). The transformation between α- and β-quartz only involves a comparatively minor rotation of the tetrahedra with respect to one another, without change in the way they are linked.

Although many of the varietal names historically arose from the color of the mineral, current scientific naming schemes refer primarily to the microstructure of the mineral. Color is a secondary identifier for the cryptocrystalline minerals, although it is a primary identifier for the macrocrystalline varieties.

Pure quartz, traditionally called rock crystal or clear quartz, is colorless and transparent or translucent, and has often been used for hardstone carvings, such as the Lothair Crystal. Common colored varieties include citrine, rose quartz, amethyst, smoky quartz, milky quartz, and others. These color differentiations arise from chromophores which have been incorporated into the crystal structure of the mineral. Polymorphs of quartz include: α-quartz (low), β-quartz, tridymite, moganite, cristobalite, coesite, and stishovite.

The most important distinction between types of quartz is that of "macrocrystalline" (individual crystals visible to the unaided eye) and the microcrystalline or cryptocrystalline varieties (aggregates of crystals visible only under high magnification). The cryptocrystalline varieties are either translucent or mostly opaque, while the transparent varieties tend to be macrocrystalline. Chalcedony is a cryptocrystalline form of silica consisting of fine intergrowths of both quartz, and its monoclinic polymorph moganite. Other opaque gemstone varieties of quartz, or mixed rocks including quartz, often including contrasting bands or patterns of color, are agate, carnelian or sard, onyx, heliotrope, and jasper.

Amethyst is a form of quartz that ranges from a bright vivid violet to dark or dull lavender shade. The world's largest deposits of amethysts can be found in Brazil, Mexico, Uruguay, Russia, France, Namibia and Morocco. Sometimes amethyst and citrine are found growing in the same crystal. It is then referred to as ametrine. An amethyst is formed when there is iron in the area where it was formed.

Blue quartz contains inclusions of fibrous magnesio-riebeckite or crocidolite.

Inclusions of the mineral dumortierite within quartz pieces often result in silky-appearing splotches with a blue hue, shades giving off purple and/or grey colors additionally being found. "Dumortierite quartz" (sometimes called "blue quartz") will sometimes feature contrasting light and dark color zones across the material. Interest in the certain quality forms of blue quartz as a collectible gemstone particularly arises in India and in the United States.

Citrine is a variety of quartz whose color ranges from a pale yellow to brown due to ferric impurities. Natural citrines are rare; most commercial citrines are heat-treated amethysts or smoky quartzes. However, a heat-treated amethyst will have small lines in the crystal, as opposed to a natural citrine's cloudy or smokey appearance. It is nearly impossible to differentiate between cut citrine and yellow topaz visually, but they differ in hardness. Brazil is the leading producer of citrine, with much of its production coming from the state of Rio Grande do Sul. The name is derived from the Latin word "citrina" which means "yellow" and is also the origin of the word "citron". Sometimes citrine and amethyst can be found together in the same crystal, which is then referred to as ametrine. Citrine has been referred to as the "merchant's stone" or "money stone", due to a superstition that it would bring prosperity.

Citrine was first appreciated as a golden-yellow gemstone in Greece between 300 and 150 BC, during the Hellenistic Age. The yellow quartz was used prior to that to decorate jewelry and tools but it was not highly sought after.

Milk quartz or milky quartz is the most common variety of crystalline quartz. The white color is caused by minute fluid inclusions of gas, liquid, or both, trapped during crystal formation, making it of little value for optical and quality gemstone applications.

Rose quartz is a type of quartz which exhibits a pale pink to rose red hue. The color is usually considered as due to trace amounts of titanium, iron, or manganese, in the material. Some rose quartz contains microscopic rutile needles which produces an asterism in transmitted light. Recent X-ray diffraction studies suggest that the color is due to thin microscopic fibers of possibly dumortierite within the quartz.

Additionally, there is a rare type of pink quartz (also frequently called crystalline rose quartz) with color that is thought to be caused by trace amounts of phosphate or aluminium. The color in crystals is apparently photosensitive and subject to fading. The first crystals were found in a pegmatite found near Rumford, Maine, US and in Minas Gerais, Brazil.

Smoky quartz is a gray, translucent version of quartz. It ranges in clarity from almost complete transparency to a brownish-gray crystal that is almost opaque. Some can also be black. The translucency results from natural irradiation creating free silicon within the crystal.

Prasiolite, also known as "vermarine", is a variety of quartz that is green in color. Since 1950, almost all natural prasiolite has come from a small Brazilian mine, but it is also seen in Lower Silesia in Poland. Naturally occurring prasiolite is also found in the Thunder Bay area of Canada. It is a rare mineral in nature; most green quartz is heat-treated amethyst.

Not all varieties of quartz are naturally occurring. Some clear quartz crystals can be treated using heat or gamma-irradiation to induce color where it would not otherwise have occurred naturally. Susceptibility to such treatments depends on the location from which the quartz was mined.

Prasiolite, an olive colored material, is produced by heat treatment; natural prasiolite has also been observed in Lower Silesia in Poland. Although citrine occurs naturally, the majority is the result of heat-treating amethyst or smoky quartz. Carnelian is widely heat-treated to deepen its color.

Because natural quartz is often twinned, synthetic quartz is produced for use in industry. Large, flawless, single crystals are synthesized in an autoclave via the hydrothermal process; emeralds are also synthesized in this fashion.

Like other crystals, quartz may be coated with metal vapors to give it an attractive sheen.

Quartz is a defining constituent of granite and other felsic igneous rocks. It is very common in sedimentary rocks such as sandstone and shale. It is a common constituent of schist, gneiss, quartzite and other metamorphic rocks. Quartz has the lowest potential for weathering in the Goldich dissolution series and consequently it is very common as a residual mineral in stream sediments and residual soils.

While the majority of quartz crystallizes from molten magma, much quartz also chemically precipitates from hot hydrothermal veins as gangue, sometimes with ore minerals like gold, silver and copper. Large crystals of quartz are found in magmatic pegmatites. Well-formed crystals may reach several meters in length and weigh hundreds of kilograms.

Naturally occurring quartz crystals of extremely high purity, necessary for the crucibles and other equipment used for growing silicon wafers in the semiconductor industry, are expensive and rare. A major mining location for high purity quartz is the Spruce Pine Gem Mine in Spruce Pine, North Carolina, United States. Quartz may also be found in Caldoveiro Peak, in Asturias, Spain.

The largest documented single crystal of quartz was found near Itapore, Goiaz, Brazil; it measured approximately 6.1×1.5×1.5 m and weighed 39,916 kilograms.

Quartz is extracted from open pit mines. Miners only use explosives on rare occasions when they need to expose a deep seam of quartz. The reason for this is that although quartz is known for its hardness, it damages easily if it is suddenly exposed to a change in temperature, such as that caused by a blast. Instead, mining operations use bulldozers and backhoes to remove soil and clay, and expose the quartz crystal veins in the rock. 

Tridymite and cristobalite are high-temperature polymorphs of SiO that occur in high-silica volcanic rocks. Coesite is a denser polymorph of SiO found in some meteorite impact sites and in metamorphic rocks formed at pressures greater than those typical of the Earth's crust. Stishovite is a yet denser and higher-pressure polymorph of SiO found in some meteorite impact sites. Lechatelierite is an amorphous silica glass SiO which is formed by lightning strikes in quartz sand.

As quartz is a form of silica, it is a possible cause for concern in various workplaces. Cutting, grinding, chipping, sanding, drilling, and polishing natural and manufactured stone products can release hazardous levels of very small, crystalline silica dust particles into the air that workers breathe. Crystalline silica of respirable size is a recognized human carcinogen and may lead to other diseases of the lungs such as silicosis and pulmonary fibrosis.

The word "quartz" comes from the German , which is of Slavic origin (Czech miners called it "křemen"). Other sources attribute the word's origin to the Saxon word "Querkluftertz", meaning "cross-vein ore".

Quartz is the most common material identified as the mystical substance maban in Australian Aboriginal mythology. It is found regularly in passage tomb cemeteries in Europe in a burial context, such as Newgrange or Carrowmore in Ireland. The Irish word for quartz is "grianchloch", which means 'sunstone'. Quartz was also used in Prehistoric Ireland, as well as many other countries, for stone tools; both vein quartz and rock crystal were knapped as part of the lithic technology of the prehistoric peoples.

While jade has been since earliest times the most prized semi-precious stone for carving in East Asia and Pre-Columbian America, in Europe and the Middle East the different varieties of quartz were the most commonly used for the various types of jewelry and hardstone carving, including engraved gems and cameo gems, rock crystal vases, and extravagant vessels. The tradition continued to produce objects that were very highly valued until the mid-19th century, when it largely fell from fashion except in jewelry. Cameo technique exploits the bands of color in onyx and other varieties.

Roman naturalist Pliny the Elder believed quartz to be water ice, permanently frozen after great lengths of time. (The word "crystal" comes from the Greek word "κρύσταλλος", "ice".) He supported this idea by saying that quartz is found near glaciers in the Alps, but not on volcanic mountains, and that large quartz crystals were fashioned into spheres to cool the hands. This idea persisted until at least the 17th century. He also knew of the ability of quartz to split light into a spectrum.

In the 17th century, Nicolas Steno's study of quartz paved the way for modern crystallography. He discovered that regardless of a quartz crystal's size or shape, its long prism faces always joined at a perfect 60° angle.

Quartz's piezoelectric properties were discovered by Jacques and Pierre Curie in 1880. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927.

Efforts to synthesize quartz began in the mid nineteenth century as scientists attempted to create minerals under laboratory conditions that mimicked the conditions in which the minerals formed in nature: German geologist Karl Emil von Schafhäutl (1803–1890) was the first person to synthesize quartz when in 1845 he created microscopic quartz crystals in a pressure cooker. However, the quality and size of the crystals that were produced by these early efforts were poor.

By the 1930s, the electronics industry had become dependent on quartz crystals. The only source of suitable crystals was Brazil; however, World War II disrupted the supplies from Brazil, so nations attempted to synthesize quartz on a commercial scale. German mineralogist Richard Nacken (1884–1971) achieved some success during the 1930s and 1940s. After the war, many laboratories attempted to grow large quartz crystals. In the United States, the U.S. Army Signal Corps contracted with Bell Laboratories and with the Brush Development Company of Cleveland, Ohio to synthesize crystals following Nacken's lead. (Prior to World War II, Brush Development produced piezoelectric crystals for record players.) By 1948, Brush Development had grown crystals that were 1.5 inches (3.8 cm) in diameter, the largest to date. By the 1950s, hydrothermal synthesis techniques were producing synthetic quartz crystals on an industrial scale, and today virtually all the quartz crystal used in the modern electronics industry is synthetic.

Some types of quartz crystals have piezoelectric properties; they develop an electric potential upon the application of mechanical stress. An early use of this property of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz clock is a familiar device using the mineral. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.




</doc>
<doc id="25234" url="https://en.wikipedia.org/wiki?curid=25234" title="Quadrivium">
Quadrivium

In liberal arts education, the quadrivium (plural: quadrivia) is the four subjects, or arts (namely arithmetic, geometry, music, and astronomy), taught after teaching the trivium. The word is Latin, meaning "four ways", and its use for the four subjects has been attributed to Boethius or Cassiodorus in the 6th century. Together, the trivium and the quadrivium comprised the seven liberal arts (based on thinking skills), as distinguished from the practical arts (such as medicine and architecture).
The quadrivium consisted of arithmetic, geometry, music, and astronomy. These followed the preparatory work of the trivium, consisting of grammar, logic, and rhetoric. In turn, the quadrivium was considered the foundation for the study of philosophy (sometimes called the "liberal art "par excellence"") and theology. The quadrivium was the upper division of the medieval education in the liberal arts, which comprised arithmetic (number), geometry (number in space), music (number in time), and astronomy (number in space and time). Educationally, the trivium and the quadrivium imparted to the student the seven liberal arts (essential thinking skills) of classical antiquity. 

These four studies compose the secondary part of the curriculum outlined by Plato in "The Republic" and are described in the seventh book of that work (in the order Arithmetic, Geometry, Astronomy, Music). 
The quadrivium is implicit in early Pythagorean writings and in the "De nuptiis" of Martianus Capella, although the term "quadrivium" was not used until Boethius, early in the sixth century. As Proclus wrote:
The Pythagoreans considered all mathematical science to be divided into four parts: one half they marked off as concerned with quantity, the other half with magnitude; and each of these they posited as twofold. A quantity can be considered in regard to its character by itself or in its relation to another quantity, magnitudes as either stationary or in motion. Arithmetic, then, studies quantities as such, music the relations between quantities, geometry magnitude at rest, spherics [astronomy] magnitude inherently moving.

At many medieval universities, this would have been the course leading to the degree of Master of Arts (after the BA). After the MA, the student could enter for bachelor's degrees of the higher faculties (Theology, Medicine or Law). To this day, some of the postgraduate degree courses lead to the degree of Bachelor (the B.Phil and B.Litt. degrees are examples in the field of philosophy).

The study was eclectic, approaching the philosophical objectives sought by considering it from each aspect of the quadrivium within the general structure demonstrated by Proclus (AD 412–485), namely arithmetic and music on the one hand and geometry and cosmology on the other.

The subject of music within the quadrivium was originally the classical subject of harmonics, in particular the study of the proportions between the musical intervals created by the division of a monochord. A relationship to music as actually practised was not part of this study, but the framework of classical harmonics would substantially influence the content and structure of music theory as practised in both European and Islamic cultures.

In modern applications of the liberal arts as curriculum in colleges or universities, the quadrivium may be considered to be the study of number and its relationship to space or time: arithmetic was pure number, geometry was number in space, music was number in time, and astronomy was number in space and time. Morris Kline classified the four elements of the quadrivium as pure (arithmetic), stationary (geometry), moving (astronomy), and applied (music) number.

This schema is sometimes referred to as "classical education", but it is more accurately a development of the 12th- and 13th-century Renaissance with recovered classical elements, rather than an organic growth from the educational systems of antiquity. The term continues to be used by the Classical education movement and at the independent Oundle School, in the United Kingdom.



</doc>
<doc id="25236" url="https://en.wikipedia.org/wiki?curid=25236" title="Quadrupedalism">
Quadrupedalism

Quadrupedalism is a form of terrestrial locomotion in animals using four limbs or legs. An animal or machine that usually moves in a quadrupedal manner is known as a quadruped, meaning "four feet" (from the Latin "quattuor" for "four" and "pes" for "foot"). The majority of quadrupeds are vertebrate animals, including mammals such as cattle, dogs and cats, and reptiles such as lizards.

Few other animals are quadrupedal, though a few birds like the shoebill sometimes use their wings to right themselves after lunging at prey.

Although the words "quadruped" and "tetrapod" are both derived from terms meaning "four-footed", they have distinct meanings. A tetrapod is any member of the taxonomic unit "Tetrapoda" (which is defined by descent from a specific four-limbed ancestor) whereas a quadruped actually uses four limbs for locomotion. Not all tetrapods are quadrupeds and not all quadrupeds are tetrapods.

The distinction between quadrupeds and tetrapods is important in evolutionary biology, particularly in the context of tetrapods whose limbs have adapted to other roles (e.g., hands in the case of humans, wings in the case of birds, and fins in the case of whales). All of these animals are tetrapods, but none is a quadruped. Even snakes, whose limbs have become vestigial or lost entirely, are nevertheless tetrapods.

Most quadrupedal animals are tetrapods but there are a few exceptions. For instance, among the insects, the praying mantis is a quadruped.

In July 2005, in rural Turkey, scientists discovered five Kurdish siblings who had learned to walk naturally on their hands and feet. Unlike chimpanzees, which ambulate on their knuckles, the Kurdish siblings walked on their palms, allowing them to preserve the dexterity of their fingers.

Many people, especially practitioners of parkour and freerunning and Georges Hébert's Natural Method, find benefit in quadrupedal movements to build full body strength. Kenichi Ito is a Japanese man famous for speed running on four limbs. Quadrupedalism is sometimes referred to as being on all fours, and is observed in crawling especially by infants.

BigDog is a dynamically stable quadruped robot created in 2005 by Boston Dynamics with Foster-Miller, the NASA Jet Propulsion Laboratory, and the Harvard University Concord Field Station.

Also by NASA JPL, in collaboration with University of California, Santa Barbara Robotics Lab, is RoboSimian, with emphasis on stability and deliberation. It has been demonstrated at the DARPA Robotics Challenge.

A related concept to quadrupedalism is pronogrady, or having a horizontal posture of the trunk. Although nearly all quadrupedal animals are pronograde, there are also bipedal animals with that posture, including many living birds and extinct dinosaurs.

Non-human apes with orthograde (vertical) backs may walk quadrupedally in what is called knuckle-walking.




</doc>
<doc id="25237" url="https://en.wikipedia.org/wiki?curid=25237" title="Quarantine">
Quarantine

A quarantine is a restriction on the movement of people and goods which is intended to prevent the spread of disease or pests. It is often used in connection to disease and illness, preventing the movement of those who may have been exposed to a communicable disease, but do not have a confirmed medical diagnosis. The term is often used synonymously with medical isolation, in which those confirmed to be infected with a communicable disease are isolated from the healthy population.

Quarantine may be used interchangeably with "cordon sanitaire", and although the terms are related, "cordon sanitaire" refers to the restriction of movement of people into or out of a defined geographic area, such as a community, in order to prevent an infection from spreading.

The word "quarantine" comes from a seventeenth-century Venetian variant of the Italian "quaranta giorni", meaning forty days, the period that all ships were required to be isolated before passengers and crew could go ashore during the Black Death plague epidemic. Quarantine can be applied to humans, but also to animals of various kinds, and both as part of border control as well as within a country.

The quarantining of people often raises questions of civil rights, especially in cases of long confinement or segregation from society, such as that of Mary Mallon (also known as Typhoid Mary), a typhoid fever carrier who was arrested and quarantined in 1907 and later spent the last 24 years and 7 months of her life in medical isolation at Riverside Hospital on North Brother Island.

Quarantine periods can be very short, such as in the case of a suspected anthrax attack, in which people are allowed to leave as soon as they shed their potentially contaminated garments and undergo a decontamination shower. For example, an article entitled "Daily News workers quarantined" describes a brief quarantine that lasted until people could be showered in a decontamination tent.

The February/March 2003 issue of "HazMat Magazine" suggests that people be "locked in a room until proper decon could be performed", in the event of "suspect anthrax".

"Standard-Times" senior correspondent Steve Urbon (14 February 2003) describes such temporary quarantine powers:

Civil rights activists in some cases have objected to people being rounded up, stripped and showered against their will. But Capt. Chmiel said local health authorities have "certain powers to quarantine people".
The purpose of such quarantine-for-decontamination is to prevent the spread of contamination, and to contain the contamination such that others are not put at risk from a person fleeing a scene where contamination is suspect. It can also be used to limit exposure, as well as eliminate a vector.

New developments for quarantine include new concepts in quarantine vehicles such as the ambulance bus, mobile hospitals, and lockdown/invacuation (inverse evacuation) procedures, as well as docking stations for an ambulance bus to dock to a facility under lockdown.

One of the earliest mentions of isolation is found in the Biblical book of Leviticus, written in the seventh century BC or perhaps earlier, which describes how infected people were separated to prevent spread of disease under the Mosaic Law:

"If the shiny spot on the skin is white but does not appear to be more than skin deep and the hair in it has not turned white, the priest is to isolate the affected person for seven days. On the seventh day the priest is to examine him, and if he sees that the sore is unchanged and has not spread in the skin, he is to isolate him for another seven days."

The word "quarantine" originates from the Venetian dialect form of the Italian "quaranta giorni", meaning 'forty days'. This is due to the 40-day isolation of ships and people before entering the city-state of Ragusa (modern Dubrovnik, Croatia). This was practiced as a measure of disease prevention related to the Black Death. Between 1348 and 1359, the Black Death wiped out an estimated 30% of Europe's population, and a significant percentage of Asia's population. The original document from 1377, which is kept in the Archives of Dubrovnik, states that before entering the city, newcomers had to spend 30 days (a "trentine") in a restricted place (originally nearby islands) waiting to see whether the symptoms of Black Death would develop. In 1448, the Venetian Senate prolonged the waiting period to 40 days, thus giving birth to the term quarantine. The forty-day quarantine proved to be an effective formula for handling outbreaks of the plague. According to current estimates, the bubonic plague had a 37-day period from infection to death; therefore, the European quarantines would have been highly successful in determining the health of crews from potential trading and supply ships.

Other diseases lent themselves to the practice of quarantine before and after the devastation of the plague. Those afflicted with leprosy were historically isolated from society, as were the attempts to check the spread of syphilis in northern Europe after 1492, the advent of yellow fever in Spain at the beginning of the 19th century, and the arrival of Asiatic cholera in 1831.

Venice took the lead in measures to check the spread of plague, having appointed three guardians of public health in the first years of the Black Death (1348). The next record of preventive measures comes from Reggio in Modena in 1374. The first lazaret was founded by Venice in 1403, on a small island adjoining the city. In 1467, Genoa followed the example of Venice, and in 1476 the old leper hospital of Marseille was converted into a plague hospital. The great lazaret of Marseille, perhaps the most complete of its kind, was founded in 1526 on the island of Pomègues. The practice at all the Mediterranean lazarets was not different from the English procedure in the Levantine and North African trade. On the arrival of cholera in 1831 some new lazarets were set up at western ports, notably a very extensive establishment near Bordeaux, afterwards turned to another use.

The involuntary hospital quarantine of special groups of patients, including those with leprosy, started early in Islamic history. In 1307 the sixth Umayyad caliph built the first hospital in Damascus and issued an order to isolate those infected with leprosy from other patients in the hospital. The practice of involuntary quarantine of leprosy in general hospitals continued until the year 1431 when the Ottomans built a leprosy hospital in Edirne. Incidents of quarantine occurred throughout the Muslim world, with evidence of voluntary community quarantine in some of these reported incidents. The first documented involuntary community quarantine was established by the Ottoman quarantine reform in 1838.

Epidemics of yellow fever ravaged urban communities in North America throughout the late eighteenth and early nineteenth centuries, the best-known examples being the 1793 Philadelphia yellow fever epidemic and outbreaks in Georgia (1856) and Florida (1888). Cholera and smallpox epidemics continued throughout the nineteenth century, and plague epidemics affected Honolulu and San Francisco from 1899 until 1901. State governments generally relied on the "cordon sanitaire" as a geographic quarantine measure to control the movement of people into and out of affected communities. During the 1918 influenza pandemic, some communities instituted protective sequestration (sometimes referred to as "reverse quarantine") to keep the infected from introducing influenza into healthy populations.

In the 21st century, people suspected of carrying infectious diseases have been quarantined, as in the cases of Andrew Speaker (multi-drug-resistant tuberculosis, 2007) and Kaci Hickox (Ebola, 2014). Moving infected patients to isolation wards and home self-quarantine of people potentially exposed was the main way the Western African Ebola virus epidemic was ended in 2016; health experts criticized international travel restrictions imposed during the epidemic as ineffective due to difficulty of enforcement, and counterproductive as they slowed down aid efforts. The People's Republic of China has employed mass quarantines, firstly of the city of Wuhan and then all of Hubei province (population 55.5 million) in the 2019–20 Wuhan coronavirus outbreak.

Since 1852 several conferences were held involving European powers, with a view to uniform action in keeping out infection from the East and preventing its spread within Europe. All but that of 1897 were concerned with cholera. No result came of those at Paris (1852), Constantinople (1866), Vienna (1874), and Rome (1885), but each of the subsequent ones doctrine of constructive infection of a ship as coming from a scheduled port, and an approximation to the principles advocated by Great Britain for many years. The principal countries which retained the old system at the time were Spain, Portugal, Turkey, Greece and Russia (the British possessions at the time, Gibraltar, Malta and Cyprus, being under the same influence). The aim of each international sanitary convention had been to bind the governments to a uniform minimum of preventive action, with further restrictions permissible to individual countries. The minimum specified by international conventions was very nearly the same as the British practice, which had been in turn adapted to continental opinion in the matter of the importation of rags.
The Venice convention of 30 January 1892 dealt with cholera by the Suez Canal route; that of Dresden of 15 April 1893, with cholera within European countries; that of Paris of 3 April 1894, with cholera by the pilgrim traffic; and that of Venice, on 19 March 1897, was in connection with the outbreak of plague in the East, and the conference met to settle on an international basis the steps to be taken to prevent, if possible, its spread into Europe. An additional convention was signed in Paris on 3 December 1903.

A multilateral international sanitary convention was concluded at Paris on 17 January 1912. This convention was most comprehensive and was designated to replace all previous conventions on that matter. It was signed by 40 countries, and consisted of 160 articles. Ratifications by 16 of the signatories were exchanged in Paris on 7 October 1920. Another multilateral convention was signed in Paris on 21 June 1926, to replace that of 1912. It was signed by 58 countries worldwide, and consisted of 172 articles.

In Latin America, a series of regional sanitary conventions were concluded. Such a convention was concluded in Rio de Janeiro on 12 June 1904. A sanitary convention between the governments of Argentina, Brazil, Paraguay and Uruguay was concluded in Montevideo on 21 April 1914. The convention covers cases of Asiatic cholera, oriental plague and yellow fever. It was ratified by the Uruguayan government on 13 October 1914, by the Paraguayan government on 27 September 1917 and by the Brazilian government on 18 January 1921.

Sanitary conventions were also concluded between European states. A Soviet-Latvian sanitary convention was signed on 24 June 1922, for which ratifications were exchanged on 18 October 1923. A bilateral sanitary convention was concluded between the governments of Latvia and Poland on 7 July 1922, for which ratifications were exchanged on 7 April 1925. Another was concluded between the governments of Germany and Poland in Dresden on 18 December 1922, and entered into effect on 15 February 1923. Another one was signed between the governments of Poland and Romania on 20 December 1922. Ratifications were exchanged on 11 July 1923. The Polish government also concluded such a convention with the Soviet government on 7 February 1923, for which ratifications were exchanged on 8 January 1924. A sanitary convention was also concluded between the governments of Poland and Czechoslovakia on 5 September 1925, for which ratifications were exchanged on 22 October 1926. A convention was signed between the governments of Germany and Latvia on 9 July 1926, for which ratifications were exchanged on 6 July 1927.

One of the first points to be dealt with in 1897 was to settle the incubation period for this disease, and the period to be adopted for administrative purposes. It was admitted that the incubation period was, as a rule, a comparatively short one, namely, of some three or four days. After much discussion ten days was accepted by a very large majority. The principle of disease notification was unanimously adopted. Each government had to notify to other governments on the existence of plague within their several jurisdictions, and at the same time state the measures of prevention which are being carried out to prevent its diffusion. The area deemed to be infected was limited to the actual district or village where the disease prevailed, and no locality was deemed to be infected merely because of the importation into it of a few cases of plague while there has been no diffusion of the malady. As regards the precautions to be taken on land frontiers, it was decided that during the prevalence of plague every country had the inherent right to close its land frontiers against traffic. As regards the Red Sea, it was decided after discussion that a healthy vessel could pass through the Suez Canal, and continue its voyage in the Mediterranean during the period of incubation of the disease the prevention of which is in question. It was also agreed that vessels passing through the Canal in quarantine might, subject to the use of the electric light, coal in quarantine at Port Said by night as well as by day, and that passengers might embark in quarantine at that port. Infected vessels, if these carry a doctor and are provided with a disinfecting stove, have a right to navigate the Canal, in quarantine, subject only to the landing of those who were suffering from plague.

Guidance on when and how human rights can be restricted to prevent the spread of infectious disease is found in The Siracusa Principles, a non-binding document developed by the Siracusa International Institute for Criminal Justice and Human Rights and adopted by the United Nations Economic and Social Council in 1984. The Siracusa Principles state that restrictions on human rights under the International Covenant on Civil and Political Rights must meet standards of legality, evidence-based necessity, proportionality, and gradualism, noting that public health can be used as grounds for limiting certain rights if the state needs to take measures ‘aimed at preventing disease or injury or providing care for the sick and injured.’ Limitations on rights (such as quarantine) must be ‘strictly necessary,’ meaning that they must: 

In addition, when quarantine is imposed, public health ethics specify that:


Finally, the state is ethically obligated to offer certain guarantees:


Plain yellow, green, and even black flags have been used to symbolize disease in both ships and ports, with the color yellow having a longer historical precedent, as a color of marking for houses of infection, previous to its use as a maritime marking color for disease. The present flag used for the purpose is the "Lima" (L) flag, which is a mixture of yellow and black flags previously used. It is sometimes called the "yellow jack" but this was also a name for yellow fever, which probably derives its common name from the flag, not the color of the victims (cholera ships also used a yellow flag). The plain yellow flag ("Quebec" or Q in international maritime signal flags) probably derives its letter symbol for its initial use in "quarantine", but this flag in modern times indicates the opposite—a ship that declares itself free of quarantinable disease, and requests boarding and routine port inspection.

Australia has perhaps the world's strictest quarantine standards. Quarantine in northern Australia is important because of its proximity to South-east Asia and the Pacific, which have many pests and diseases not present in Australia. For this reason, the region from Cairns to Broome—including the Torres Strait—is the focus for many important quarantine activities that protect all Australians. As Australia has been geographically isolated from other major continents for millions of years, there is an endemically unique ecosystem free of several severe pests and diseases that are present in many parts of the world. If other products are brought inside along with pests and diseases, it would damage the ecosystem seriously and add millions of costs in the local agricultural businesses.

The Australian Quarantine and Inspection Service is responsible for border-inspection of any products which are brought into Australia, and assess the potential risks the products might harm Australian environment. Visitors are required to fill in the information card truthfully before arriving in Australia, and declare what food and any products made of wood and other natural materials they have processed. If the visitor fails to do so, usually a quarantine infringement fine of 220 Australian dollars is to be paid, and if not, the visitor may face criminal convictions, be fined up to 100,000 Australian dollars or be imprisoned for up to 10 years.

There are three quarantine Acts of Parliament in Canada: "Quarantine Act" (humans) and "Health of Animals Act" (animals) and "Plant Protection Act" (vegetations). The first legislation is enforced by the Canada Border Services Agency after a complete rewrite in 2005. The second and third legislations are enforced by the Canadian Food Inspection Agency. If a health emergency exists, the Governor in Council can prohibit importation of anything that it deems necessary under the "Quarantine Act".

Under the "Quarantine Act", all travelers must submit to screening and if they believe they might have come into contact with communicable diseases or vectors, they must disclose their whereabouts to a Border Services Officer. If the officer has reasonable grounds to believe that the traveller is or might have been infected with a communicable disease or refused to provide answers, a quarantine officer (QO) must be called and the person is to be isolated. If a person refuses to be isolated, any peace officer may arrest without warrant.

A QO who has reasonable grounds to believe that the traveler has or might have a communicable disease or is infested with vectors, after the medical examination of a traveler, can order him/her into treatment or measures to prevent the person from spreading the disease. QO can detain any traveler who refuses to comply with his/her orders or undergo health assessments as required by law.

Under the "Health of Animals Act" and "Plant Protection Act", inspectors can prohibit access to an infected area, dispose or treat any infected or suspected to be infected animals or plants. The Minister can order for compensation to be given if animals/plants were destroyed pursuant to these acts.

Each province also enacts its own quarantine/environmental health legislation.

Under the "Prevention and Control of Disease Ordinance" (HK Laws. Chap 599), a health officer may seize articles they believe to be infectious or containing infectious agents. All travellers, if requested, must submit themselves to a health officer. Failure to do so is against the law and is subject to arrest and prosecution.

The law allows for a health officer who have reasonable grounds to detain, isolate, quarantine anyone or anything believed to be infected and to restrict any articles from leaving a designated quarantine area. He/she may also order the Civil Aviation Department to prohibit the landing or leaving, embarking or disembarking of an aircraft. This power also extends to land, sea or air crossings.

Under the same ordinance, any police officer, health officer, member of the Civil Aid Service, or member of the Auxiliary Medical Service can arrest a person who obstructs or escapes from detention.

To reduce the risk of introducing rabies from continental Europe, the United Kingdom used to require that dogs, and most other animals introduced to the country, spend six months in quarantine at an HM Customs and Excise pound; this policy was abolished in 2000 in favour of a scheme generally known as Pet Passports, where animals can avoid quarantine if they have documentation showing they are up to date on their appropriate vaccinations.

The plague had disappeared from England for more than thirty years before the practice of quarantine against it was definitely established by the Quarantine Act 1710 ("9 Ann.") The first act was called for due to fears that the plague might be imported from Poland and the Baltic states. The second act of 1721 was due to the prevalence of plague at Marseille and other places in Provence, France. It was renewed in 1733 after a new outbreak in continental Europe, and again in 1743, due to an epidemic in Messina. In 1752 a rigorous quarantine clause was introduced into an act regulating trade with the Levant, and various arbitrary orders were issued during the next twenty years to meet the supposed danger of infection from the Baltic states. Although no plague cases ever came to England during that period, the restrictions on traffic became more stringent, and in 1788 a very strict Quarantine Act was passed, with provisions affecting cargoes in particular. The act was revised in 1801 and 1805, and in 1823–24 an elaborate inquiry was followed by an act making quarantine only at discretion of the privy council, which recognized yellow fever or other highly infectious diseases as calling for quarantine, along with plague. The threat of cholera in 1831 was the last occasion in England of the use of quarantine restrictions. Cholera affected every country in Europe despite all efforts to keep it out. When cholera returned to England in 1849, 1853 and 1865–66, no attempt was made to seal the ports. In 1847 the privy council ordered all arrivals with a clean bill of health from the Black Sea and the Levant to be admitted, provided there had been no case of plague during the voyage, and afterwards the practice of quarantine was discontinued.

After the passing of the first Quarantine Act (1710) the protective practices in England were haphazard and arbitrary. In 1721 two vessels carrying cotton goods from Cyprus, then affected by the plague, were ordered to be burned with their cargoes, the owners receiving an indemnity. By the clause in the Levant Trade Act of 1752, ships arriving in the United Kingdom with a "foul bill" (i.e. coming from a country where plague existed) had to return to the lazarets of Malta, Venice, Messina, Livorno, Genoa or Marseille, to complete a quarantine or to have their cargoes opened and aired. Since 1741 Stangate Creek (on the Medway) had been the quarantine station but it was available only for vessels with clean bills of health. In 1755 lazarets in the form of floating hulks were established in England for the first time, the cleansing of cargo (particularly by exposure to dews) having been done previously on the ship's deck. No medical inspections were conducted, but control was the responsibility of the Officers of Royal Customs and quarantine. In 1780, when plague was in Poland, even vessels with grain from the Baltic had to spend forty days in quarantine, and unpack and air their cargoes, but due to complaints mainly from Edinburgh and Leith, an exception was made for grain after that date. About 1788 an order of the council required every ship liable to quarantine to hoist a yellow flag in the daytime and show a light at the main topmast head at night, in case of meeting any vessel at sea, or upon arriving within four leagues of the coast of Great Britain or Ireland.

After 1800, ships from plague-affected countries (or with foul bills) were permitted to complete their quarantine in the Medway instead of at a Mediterranean port on the way, and an extensive lazaret was built on Chetney Hill near Chatham (although it was later demolished). The use of floating hulks as lazarets continued as before. In 1800 two ships with hides from Mogador in Morocco were ordered to be sunk with their cargoes at the Nore, the owners receiving an indemnity. Animal hides were suspected of harboring infections, along with a long list of other items, and these had to be exposed on the ship's deck for twenty-one days or less (six days for each installment of the cargo), and then transported to the lazaret, where they were opened and aired for another forty days. The whole detention of the vessel was from sixty to sixty-five days, including the time for reshipment of her cargo. Pilots had to pass fifteen days on board a convalescent ship. From 1846 onwards the quarantine establishments in the United Kingdom were gradually reduced, while the last vestige of the British quarantine law was removed by the Public Health Act of 1896, which repealed the Quarantine Act of 1825 (with dependent clauses of other acts), and transferred from the privy council to the Local Government Board the powers to deal with ships arriving infected with yellow fever or plague. The powers to deal with cholera ships had been already transferred by the Public Health Act 1875.

British regulations of 9 November 1896 applied to yellow fever, plague and cholera. Officers of the Customs, as well as of Royal Coast Guard and the Board of Trade (for signalling), were empowered to take the initial steps. They certified in writing the master of a supposedly infected ship, and detained the vessel provisionally for not more than twelve hours, giving notice meanwhile to the port sanitary authority. The medical officer of the port boarded the ship and examined every person in it. Every person found infected was taken to a hospital and quarantined under the orders of the medical officer, and the vessel remained under his orders. Every person suspected could be detained on board for 48 hours or removed to the hospital for a similar period. All others were free to land upon giving the addresses of their destinations to be sent to the respective local authorities, so that the dispersed passengers and crew could be kept individually under observation for a few days. The ship was then disinfected, dead bodies buried at sea, infected clothing, bedding, etc., destroyed or disinfected, and bilge-water and water-ballast pumped out at a suitable distance before the ship entered a dock or basin. Mail was subject to no detention. A stricken ship within 3 miles of the shore had to fly a yellow and black flag at the main mast from sunrise to sunset.

Quarantine law began in Colonial America in 1663, when in an attempt to curb an outbreak of smallpox, the city of New York established a quarantine. In the 1730s, the city built a quarantine station on the Bedloe's Island. The Philadelphia Lazaretto was the first quarantine hospital in the United States, built in 1799, in Tinicum Township, Delaware County, Pennsylvania. There are similar national landmarks such as Swinburne Island and Angel Island. The Pest House in Concord, Massachusetts was used as early as 1752 to quarantine those suffering from cholera, tuberculosis and smallpox.

In early June 1832, during the cholera epidemic in New York, Governor Enos Throop called a special session of the Legislature for June 21, to pass a Public Health Act by both Houses of the State Legislature. It included to a strict quarantine along the Upper and Lower New York-Canadian frontier. In addition, New York City Mayor Walter Browne established a quarantine against all peoples and products of Europe and Asia, which prohibited ships from approaching closer than 300 yards to the city, and all vehicles were ordered to stop 1.5 miles away.

The Immigrant Inspection Station on Ellis Island, built in 1892, is often mistakenly assumed to have been a quarantine station, however its marine hospital (Ellis Island Immigrant Hospital) only qualified as a contagious disease facility to handle less virulent diseases like measles, trachoma and less advanced stages of tuberculosis and diphtheria; those afflicted with smallpox, yellow fever, cholera, leprosy or typhoid fever, could neither be received nor treated there.

Mary Mallon was quarantined in 1907 under the Greater New York Charter, Sections 1169-1170, which permitted the New York City Board of Health to “remove to a proper place…any person sick with any contagious, pestilential or infectious disease.”

During the 1918 flu pandemic, people were also quarantined. Most commonly suspect cases of infectious diseases are requested to voluntarily quarantine themselves, and Federal and local quarantine statutes only have been uncommonly invoked since then, including for a suspected smallpox case in 1963.

The 1944 Public Health Service Act “to apprehend, detain, and examine certain infected persons who are peculiarly likely to cause the interstate spread of disease” clearly established the federal government's quarantine authority for the first time. It gave the United States Public Health Service responsibility for preventing the introduction, transmission and spread of communicable diseases from foreign countries into the United States, and expanded quarantine authority to include incoming aircraft. The act states that “...any individual reasonably believed to be infected with a communicable disease in a qualifying stage and...if found to be infected, may be detained for such time and in such manner as may be reasonably necessary.”

Communicable diseases for which apprehension, detention, or conditional release of people are authorized must be specified in Executive Orders of the President. Executive Order 13295 (Revised List of Quarantinable Communicable Diseases, April 4, 2003) and its amendments (executive orders 13375 and 13674) specify the following infectious diseases: 
In the event of conflict of federal, state, local, and/or tribal health authorities in the use of legal quarantine power, federal law is supreme.

On January 19, 2017, the Department of Health and Human Services (HHS) and the Centers for Disease Control and Prevention (CDC) published the Final Rule for Control of Communicable Diseases: Interstate and Foreign. This Final Rule enhances HHS/CDC’s ability to prevent the introduction, transmission, and spread of communicable diseases into the United States and interstate by clarifying and providing greater transparency regarding its response capabilities and practices. This regulation became effective on March 21, 2017. It specifies that:


The new rules:


The Division of Global Migration and Quarantine (DGMQ) of the US Center for Disease Control (CDC) operates small quarantine facilities at a number of US ports of entry. As of 2014, these included one land crossing (in El Paso, Texas) and 19 international airports.
Besides the port of entry where it is located, each station is also responsible for quarantining potentially infected travelers entering through any ports of entry in its assigned region. These facilities are fairly small; each one is operated by a few staff members and capable of accommodating 1-2 travelers for a short observation period. Cost estimates for setting up a temporary larger facility, capable of accommodating 100 to 200 travelers for several weeks, have been published by the Airport Cooperative Research Program (ACRP) in 2008 of the Transportation Research Board.

The United States puts immediate quarantines on imported products if a contagious disease is identified and can be traced back to a certain shipment or product. All imports will also be quarantined if the disease appears in other countries. According to Title 42 U.S.C. §§264 and 266, these statutes provide the Secretary of Health and Human Services peacetime and wartime authority to control the movement of people into and within the United States to prevent the spread of communicable disease.

U.S. President John F. Kennedy euphemistically referred to the U.S. Navy's interdiction of shipping en route to Cuba during the Cuban Missile Crisis as a "quarantine" rather than a blockade, because a quarantine is a legal act in peacetime, whereas a blockade is defined as an act of aggression under the U.N. Charter.

In computer science, "quarantining" describes putting files infected by computer viruses into a special directory, so as to eliminate the threat they pose, without irreversibly deleting them.

The Spanish term for quaratine, "la cuarantina", refers also to the period of postpartum confinement in which a new mother and her baby are sheltered from the outside world.







</doc>
<doc id="25239" url="https://en.wikipedia.org/wiki?curid=25239" title="Quasar">
Quasar

A quasar () (also known as a quasi-stellar object abbreviated QSO) is an extremely luminous active galactic nucleus (AGN), in which a supermassive black hole with mass ranging from millions to billions of times the mass of the Sun is surrounded by a gaseous accretion disk. As gas in the disk falls towards the black hole, energy is released in the form of electromagnetic radiation, which can be observed across the electromagnetic spectrum. The power radiated by quasars is enormous: the most powerful quasars have luminosities thousands of times greater than a galaxy such as the Milky Way.

The term originated as a contraction of quasi-stellar "[star-like]" radio source, because quasars were first identified during the 1950s as sources of radio-wave emission of unknown physical origin, and when identified in photographic images at visible wavelengths they resembled faint star-like points of light. High-resolution images of quasars, particularly from the Hubble Space Telescope, have demonstrated that quasars occur in the centers of galaxies, and that some host-galaxies are strongly interacting or merging galaxies. As with other categories of AGN, the observed properties of a quasar depend on many factors including the mass of the black hole, the rate of gas accretion, the orientation of the accretion disk relative to the observer, the presence or absence of a jet, and the degree of obscuration by gas and dust within the host galaxy.

Quasars are found over a very broad range of distances, and quasar discovery surveys have demonstrated that quasar activity was more common in the distant past. The peak epoch of quasar activity was approximately 10 billion years ago. , the most distant known quasar is ULAS J1342+0928 at redshift "z" = 7.54; light observed from this quasar was emitted when the universe was only 690 million years old. The supermassive black hole in this quasar, estimated at 800 million solar masses, is the most distant black hole identified to date.

The term "quasar" was first used in a paper by Taiwanese-born U.S. astrophysicist Hong-Yee Chiu in May 1964, in "Physics Today", to describe certain astronomically-puzzling objects:

Between 1917 and 1922, it became clear from work by Heber Curtis, Ernst Öpik and others, that some objects ("nebulae") seen by astronomers were in fact distant galaxies like our own. But when radio astronomy commenced in the 1950s, astronomers detected, among the galaxies, a small number of anomalous objects with properties that defied explanation.

The objects emitted large amounts of radiation of many frequencies, but no source could be located optically, or in some cases only a faint and point-like object somewhat like a distant star. The spectral lines of these objects, which identify the chemical elements of which the object is composed, were also extremely strange and defied explanation. Some of them changed their luminosity very rapidly in the optical range and even more rapidly in the X-ray range, suggesting an upper limit on their size, perhaps no larger than our own Solar System. This implies an extremely high power density. Considerable discussion took place over what these objects might be. They were described as ""quasi-stellar" [meaning: star-like] "radio sources"", or ""quasi-stellar objects"" (QSOs), a name which reflected their unknown nature, and this became shortened to "quasar".

The first quasars (3C 48 and 3C 273) were discovered in the late 1950s, as radio sources in all-sky radio surveys. They were first noted as radio sources with no corresponding visible object. Using small telescopes and the Lovell Telescope as an interferometer, they were shown to have a very small angular size. Hundreds of these objects were recorded by 1960 and published in the Third Cambridge Catalogue as astronomers scanned the skies for their optical counterparts. In 1963, a definite identification of the radio source 3C 48 with an optical object was published by Allan Sandage and Thomas A. Matthews. Astronomers had detected what appeared to be a faint blue star at the location of the radio source and obtained its spectrum, which contained many unknown broad emission lines. The anomalous spectrum defied interpretation.

British-Australian astronomer John Bolton made many early observations of quasars, including a breakthrough in 1962. Another radio source, 3C 273, was predicted to undergo five occultations by the Moon. Measurements taken by Cyril Hazard and John Bolton during one of the occultations using the Parkes Radio Telescope allowed Maarten Schmidt to find a visible counterpart to the radio source and obtain an optical spectrum using the 200-inch Hale Telescope on Mount Palomar. This spectrum revealed the same strange emission lines. Schmidt was able to demonstrate that these were likely to be the ordinary spectral lines of hydrogen redshifted by 15.8 percent—at the time, a high redshift (with only a handful of much fainter galaxies known with higher redshift). If this was due to the physical motion of the "star", then 3C 273 was receding at an enormous velocity, around 47,000 km/s, far beyond the speed of any known star and defying any obvious explanation. Nor would an extreme velocity help to explain 3C 273's huge radio emissions. If the redshift was cosmological (now known to be correct), the large distance implied that 3C 273 was far more luminous than any galaxy, but much more compact. Also, 3C 273 was bright enough to detect on archival photographs dating back to the 1900s; it was found to be variable on yearly timescales, implying that a substantial fraction of the light was emitted from a region less than 1 light-year in size, tiny compared to a galaxy.

Although it raised many questions, Schmidt's discovery quickly revolutionized quasar observation. The strange spectrum of 3C 48 was quickly identified by Schmidt, Greenstein and Oke as hydrogen and magnesium redshifted by 37%. Shortly afterwards, two more quasar spectra in 1964 and five more in 1965 were also confirmed as ordinary light that had been redshifted to an extreme degree.

Although the observations and redshifts themselves were not doubted, their correct interpretation was heavily debated, and Bolton's suggestion that the radiation detected from quasars were ordinary spectral lines from distant highly redshifted sources with extreme velocity was not widely accepted at the time.

An extreme redshift could imply great distance and velocity but could also be due to extreme mass or perhaps some other unknown laws of nature. Extreme velocity and distance would also imply immense power output, which lacked explanation, and conflicted with the traditional and predominant steady state theory of the universe. The small sizes were confirmed by interferometry and by observing the speed with which the quasar as a whole varied in output, and by their inability to be seen in even the most powerful visible light telescopes as anything more than faint starlike points of light. But if they were small and far away in space, their power output would have to be immense, and difficult to explain. Equally if they were very small and much closer to our galaxy, it would be easy to explain their apparent power output, but less easy to explain their redshifts and lack of detectable movement against the background of the universe.

Schmidt noted that redshift is also associated with the expansion of the universe, as codified in Hubble's law. If the measured redshift was due to expansion, then this would support an interpretation of very distant objects with extraordinarily high luminosity and power output, far beyond any object seen to date. This extreme luminosity would also explain the large radio signal. Schmidt concluded that 3C 273 could either be an individual star around 10 km wide within (or near to) our galaxy, or a distant active galactic nucleus. He stated that a distant and extremely powerful object seemed more likely to be correct.

Schmidt's explanation for the high redshift was not widely accepted at the time. A major concern was the enormous amount of energy these objects would have to be radiating, if they were distant. In the 1960s no commonly-accepted mechanism could account for this. The currently accepted explanation, that it is due to matter in an accretion disc falling into a supermassive black hole, was only suggested in 1964 by Edwin Salpeter and Yakov Zel'dovich, and even then it was rejected by many astronomers, because in the 1960s, the existence of black holes was still widely seen as theoretical and too exotic, and because it was not yet confirmed that many galaxies (including our own) have supermassive black holes at their center. The strange spectral lines in their radiation, and the speed of change seen in some quasars, also suggested to many astronomers and cosmologists that the objects were comparatively small and therefore perhaps bright, massive and not far away; accordingly that their redshifts were not due to distance or velocity, and must be due to some other reason or an unknown process, meaning that the quasars were not really powerful objects nor at extreme distances, as their redshifted light implied. A common alternative explanation was that the redshifts were caused by extreme mass (gravitational redshifting explained by general relativity) and not by extreme velocity (explained by special relativity).

Various explanations were proposed during the 1960s and 1970s, each with their own problems. It was suggested that quasars were nearby objects, and that their redshift was not due to the expansion of space (special relativity) but rather to light escaping a deep gravitational well (general relativity). This would require a massive object, which would also explain the high luminosities. However a star of sufficient mass to produce the measured redshift would be unstable and in excess of the Hayashi limit. Quasars also show forbidden spectral emission lines which were previously only seen in hot gaseous nebulae of low density, which would be too diffuse to both generate the observed power and fit within a deep gravitational well. There were also serious concerns regarding the idea of cosmologically distant quasars. One strong argument against them was that they implied energies that were far in excess of known energy conversion processes, including nuclear fusion. There were some suggestions that quasars were made of some hitherto unknown form of stable antimatter regions and that this might account for their brightness. Others speculated that quasars were a white hole end of a wormhole, or a chain reaction of numerous supernovae.

Eventually, starting from about the 1970s, many lines of evidence (including the first X-Ray space observatories, knowledge of black holes and modern models of cosmology) gradually demonstrated that the quasar redshifts are genuine, and due to the expansion of space, that quasars are in fact as powerful and as distant as Schmidt and some other astronomers had suggested, and that their energy source is matter from an accretion disc falling onto a supermassive black hole. This included crucial evidence from optical and X-Ray viewing of quasar host galaxies, finding of 'intervening' absorption lines which explained various spectral anomalies, observations from gravitational lensing, Peterson and Gunn's 1971 finding that galaxies containing quasars showed the same redshift as the quasars, and Kristian's 1973 finding that the "fuzzy" surrounding of many quasars was consistent with a less luminous host galaxy.

This model also fits well with other observations that suggest many or even most galaxies have a massive central black hole. It would also explain why quasars are more common in the early universe: as a quasar draws matter from its accretion disc, there comes a point when there is less matter nearby, and energy production falls off or ceases as the quasar becomes a more ordinary type of galaxy.

The accretion disc energy-production mechanism was finally modeled in the 1970s, and black holes were also directly detected (including evidence showing that supermassive black holes could be found at the centers of our own and many other galaxies), which resolved the concern that quasars were too luminous to be a result of very distant objects or that a suitable mechanism could not be confirmed to exist in nature. By 1987 it was "well accepted" that this was the correct explanation for quasars, and the cosmological distance and energy output of quasars was accepted by almost all researchers.

Later it was found that not all quasars have strong radio emission; in fact only about 10% are "radio-loud". Hence the name 'QSO' (quasi-stellar object) is used (in addition to "quasar") to refer to these objects, including the 'radio-loud' and the 'radio-quiet' classes. The discovery of the quasar had large implications for the field of astronomy in the 1960s, including drawing physics and astronomy closer together.

In 1979 the gravitational lens effect predicted by Einstein's General Theory of Relativity was confirmed observationally for the first time with images of the double quasar 0957+561.

It is now known that quasars are distant but extremely luminous objects, so any light which reaches the Earth is redshifted due to the metric expansion of space.

Quasars inhabit the center of active galaxies, and are among the most luminous, powerful, and energetic objects known in the universe, emitting up to a thousand times the energy output of the Milky Way, which contains 200 billion–400 billion stars. This radiation is emitted across the electromagnetic spectrum, almost uniformly, from X-rays to the far-infrared with a peak in the ultraviolet-optical bands, with some quasars also being strong sources of radio emission and of gamma-rays. With high-resolution imaging from ground-based telescopes and the Hubble Space Telescope, the "host galaxies" surrounding the quasars have been detected in some cases. These galaxies are normally too dim to be seen against the glare of the quasar, except with special techniques. Most quasars, with the exception of 3C 273 whose average apparent magnitude is 12.9, cannot be seen with small telescopes.

Quasars are believed—and in many cases confirmed—to be powered by accretion of material into supermassive black holes in the nuclei of distant galaxies, as suggested in 1964 by Edwin Salpeter and Yakov Zel'dovich. Light and other radiation cannot escape from within the event horizon of a black hole. The energy produced by a quasar is generated "outside" the black hole, by gravitational stresses and immense friction within the material nearest to the black hole, as it orbits and falls inward. The huge luminosity of quasars results from the accretion discs of central supermassive black holes, which can convert between 6% and 32% of the mass of an object into energy, compared to just 0.7% for the p-p chain nuclear fusion process that dominates the energy production in Sun-like stars. Central masses of 10 to 10 solar masses have been measured in quasars by using reverberation mapping. Several dozen nearby large galaxies, including our own Milky Way galaxy, that do not have an active center and do not show any activity similar to a quasar, are confirmed to contain a similar supermassive black hole in their nuclei (galactic center). Thus it is now thought that all large galaxies have a black hole of this kind, but only a small fraction have sufficient matter in the right kind of orbit at their center to become active and power radiation in such a way as to be seen as quasars.

This also explains why quasars were more common in the early universe, as this energy production ends when the supermassive black hole consumes all of the gas and dust near it. This means that it is possible that most galaxies, including the Milky Way, have gone through an active stage, appearing as a quasar or some other class of active galaxy that depended on the black hole mass and the accretion rate, and are now quiescent because they lack a supply of matter to feed into their central black holes to generate radiation.

The matter accreting onto the black hole is unlikely to fall directly in, but will have some angular momentum around the black hole that will cause the matter to collect into an accretion disc. Quasars may also be ignited or re-ignited when normal galaxies merge and the black hole is infused with a fresh source of matter. In fact, it has been suggested that a quasar could form when the Andromeda Galaxy collides with our own Milky Way galaxy in approximately 3–5 billion years.

In the 1980s, unified models were developed in which quasars were classified as a particular kind of active galaxy, and a consensus emerged that in many cases it is simply the viewing angle that distinguishes them from other active galaxies, such as blazars and radio galaxies.

The highest redshift quasar known () is ULAS J1342+0928, with a redshift of 7.54, which corresponds to a comoving distance of approximately 29.36 billion light-years from Earth (these distances are much larger than the distance light could travel in the universe's 13.8 billion year history because space itself has also been expanding).

More than 200,000 quasars are known, most from the Sloan Digital Sky Survey. All observed quasar spectra have redshifts between 0.056 and 7.54 (as of 2017). Applying Hubble's law to these redshifts, it can be shown that they are between 600 million and 29.36 billion light-years away (in terms of comoving distance). Because of the great distances to the farthest quasars and the finite velocity of light, they and their surrounding space appear as they existed in the very early universe.

The power of quasars originates from supermassive black holes that are believed to exist at the core of most galaxies. The Doppler shifts of stars near the cores of galaxies indicate that they are rotating around tremendous masses with very steep gravity gradients, suggesting black holes.

Although quasars appear faint when viewed from Earth, they are visible from extreme distances, being the most luminous objects in the known universe. The brightest quasar in the sky is 3C 273 in the constellation of Virgo. It has an average apparent magnitude of 12.8 (bright enough to be seen through a medium-size amateur telescope), but it has an absolute magnitude of −26.7. From a distance of about 33 light-years, this object would shine in the sky about as brightly as our sun. This quasar's luminosity is, therefore, about 4 trillion (4 × 10) times that of the Sun, or about 100 times that of the total light of giant galaxies like the Milky Way. This assumes the quasar is radiating energy in all directions, but the active galactic nucleus is believed to be radiating preferentially in the direction of its jet. In a universe containing hundreds of billions of galaxies, most of which had active nuclei billions of years ago but only seen today, it is statistically certain that thousands of energy jets should be pointed toward the Earth, some more directly than others. In many cases it is likely that the brighter the quasar, the more directly its jet is aimed at the Earth. Such quasars are called blazars.

The hyperluminous quasar APM 08279+5255 was, when discovered in 1998, given an absolute magnitude of −32.2. High resolution imaging with the Hubble Space Telescope and the 10 m Keck Telescope revealed that this system is gravitationally lensed. A study of the gravitational lensing of this system suggests that the light emitted has been magnified by a factor of ~10. It is still substantially more luminous than nearby quasars such as 3C 273.

Quasars were much more common in the early universe than they are today. This discovery by Maarten Schmidt in 1967 was early strong evidence against Steady State cosmology and in favor of the Big Bang cosmology. Quasars show the locations where massive black holes are growing rapidly (via accretion). These black holes grow in step with the mass of stars in their host galaxy in a way not understood at present. One idea is that jets, radiation and winds created by the quasars, shut down the formation of new stars in the host galaxy, a process called 'feedback'. The jets that produce strong radio emission in some quasars at the centers of clusters of galaxies are known to have enough power to prevent the hot gas in those clusters from cooling and falling onto the central galaxy.

Quasars' luminosities are variable, with time scales that range from months to hours. This means that quasars generate and emit their energy from a very small region, since each part of the quasar would have to be in contact with other parts on such a time scale as to allow the coordination of the luminosity variations. This would mean that a quasar varying on a time scale of a few weeks cannot be larger than a few light-weeks across. The emission of large amounts of power from a small region requires a power source far more efficient than the nuclear fusion that powers stars. The conversion of gravitational potential energy to radiation by infalling to a black hole converts between 6% and 32% of the mass to energy, compared to 0.7% for the conversion of mass to energy in a star like our sun. It is the only process known that can produce such high power over a very long term. (Stellar explosions such as supernovas and gamma-ray bursts, and direct matter-antimatter annihilation, can also produce very high power output, but supernovae only last for days, and the universe does not appear to have had large amounts of antimatter at the relevant times).

Since quasars exhibit all the properties common to other active galaxies such as Seyfert galaxies, the emission from quasars can be readily compared to those of smaller active galaxies powered by smaller supermassive black holes. To create a luminosity of 10 watts (the typical brightness of a quasar), a super-massive black hole would have to consume the material equivalent of 10 stars per year. The brightest known quasars devour 1000 solar masses of material every year. The largest known is estimated to consume matter equivalent to 10 Earths per second. Quasar luminosities can vary considerably over time, depending on their surroundings. Since it is difficult to fuel quasars for many billions of years, after a quasar finishes accreting the surrounding gas and dust, it becomes an ordinary galaxy.

Radiation from quasars is partially 'nonthermal' (i.e., not due to black body radiation), and approximately 10 percent are observed to also have jets and lobes like those of radio galaxies that also carry significant (but poorly understood) amounts of energy in the form of particles moving at relativistic speeds. Extremely high energies might be explained by several mechanisms (see Fermi acceleration and Centrifugal mechanism of acceleration). Quasars can be detected over the entire observable electromagnetic spectrum including radio, infrared, visible light, ultraviolet, X-ray and even gamma rays. Most quasars are brightest in their rest-frame near-ultraviolet wavelength of 121.6 nm Lyman-alpha emission line of hydrogen, but due to the tremendous redshifts of these sources, that peak luminosity has been observed as far to the red as 900.0 nm, in the near infrared. A minority of quasars show strong radio emission, which is generated by jets of matter moving close to the speed of light. When viewed downward, these appear as blazars and often have regions that seem to move away from the center faster than the speed of light (superluminal expansion). This is an optical illusion due to the properties of special relativity.

Quasar redshifts are measured from the strong spectral lines that dominate their visible and ultraviolet emission spectra. These lines are brighter than the continuous spectrum. They exhibit Doppler broadening corresponding to mean speed of several percent of the speed of light. Fast motions strongly indicate a large mass. Emission lines of hydrogen (mainly of the Lyman series and Balmer series), helium, carbon, magnesium, iron and oxygen are the brightest lines. The atoms emitting these lines range from neutral to highly ionized, leaving it highly charged. This wide range of ionization shows that the gas is highly irradiated by the quasar, not merely hot, and not by stars, which cannot produce such a wide range of ionization.

Like all (unobscured) active galaxies, quasars can be strong X-ray sources. Radio-loud quasars can also produce X-rays and gamma rays by inverse Compton scattering of lower-energy photons by the radio-emitting electrons in the jet.

"Iron quasars" show strong emission lines resulting from low ionization iron (FeII), such as IRAS 18508-7815.

Quasars also provide some clues as to the end of the Big Bang's reionization. The oldest known quasars (redshift = 6) display a Gunn-Peterson trough and have absorption regions in front of them indicating that the intergalactic medium at that time was neutral gas. More recent quasars show no absorption region but rather their spectra contain a spiky area known as the Lyman-alpha forest; this indicates that the intergalactic medium has undergone reionization into plasma, and that neutral gas exists only in small clouds.

The intense production of ionizing ultraviolet radiation is also significant, as it would provide a mechanism for reionization to occur as galaxies form. Despite this, current theories suggest that quasars were not the primary source of reionization; the primary causes of reionization were probably the earliest generations of stars, known as Population III stars (possibly 70%), and dwarf galaxies (very early small high-energy galaxies) (possibly 30%).

Quasars show evidence of elements heavier than helium, indicating that galaxies underwent a massive phase of star formation, creating population III stars between the time of the Big Bang and the first observed quasars. Light from these stars may have been observed in 2005 using NASA's Spitzer Space Telescope, although this observation remains to be confirmed.

The taxonomy of quasars includes various subtypes representing subsets of the quasar population having distinct properties.


Because quasars are extremely distant, bright, and small in apparent size, they are useful reference points in establishing a measurement grid on the sky.
The International Celestial Reference System (ICRS) is based on hundreds of extra-galactic radio sources, mostly quasars, distributed around the entire sky. Because they are so distant, they are apparently stationary to our current technology, yet their positions can be measured with the utmost accuracy by very-long-baseline interferometry (VLBI). The positions of most are known to 0.001 arcsecond or better, which is orders of magnitude more precise than the best optical measurements.

A grouping of two or more quasars on the sky can result from a chance alignment where the quasars are not physically associated, actual physical proximity, or the effects of gravity bending the light of a single quasar into two or more images via gravitational lensing. 

When two quasars appear to be very close to each other as seen from Earth (separated by a few arcseconds or less), they are commonly referred to as a "double quasar". When the two are also close together in space (i.e. observed to have similar redshifts), they are termed a "quasar pair", or as a "binary quasar" if they are close enough that their host galaxies are likely to be physically interacting.
As quasars are overall rare objects in the universe, the probability of three or more separate quasars being found near the same physical location is very low, and determining whether the system is closely separated physically requires significant observational effort. The first true triple quasar was found in 2007 by observations at the W. M. Keck Observatory Mauna Kea, Hawaii.
LBQS 1429-008 (or QQQ J1432-0106) was first observed in 1989 and at the time was found to be a double quasar. When astronomers discovered the third member, they confirmed that the sources were separate and not the result of gravitational lensing. This triple quasar has a redshift of "z" = 2.076.

The components are separated by an estimated 30–50 kpc, which is typical of interacting galaxies.

In 2013, the second true triplet of quasars--QQQ J1519+0627--was found with a redshift "z" = 1.51, the whole system fitting within a physical separation of 25kpc.
The first true quadruple quasar system was discovered in 2015 at a redshift "z"=2.0412, and has an overall physical scale of about 200kpc.
A multiple-image quasar is a quasar whose light undergoes gravitational lensing, resulting in double, triple or quadruple images of the same quasar. The first such gravitational lens to be discovered was the double-imaged quasar Q0957+561 (or Twin Quasar) in 1979.

An example of a triply lensed quasar is PG1115 +08.

Several quadruple-image quasars are known, including the Einstein Cross and the Cloverleaf Quasar, with the first such discoveries happening in the mid-1980s.




</doc>
<doc id="25240" url="https://en.wikipedia.org/wiki?curid=25240" title="Quinquagesima">
Quinquagesima

Quinquagesima () is one of the names used in the Western Church for the Sunday before Ash Wednesday. It is also called Quinquagesima Sunday, Quinquagesimae, Estomihi, Shrove Sunday, or the Sunday next before Lent.

The name Quinquagesima originates from Latin "quinquagesimus" (fiftieth). This is in reference to the fifty days before Easter Day using inclusive counting which counts both Sundays (normal counting would count only one of these). Since the forty days of the Lent do not include Sundays, the first day of Lent, Ash Wednesday, succeeds Quinquagesima Sunday by only three days. 

The name Estomihi is derived from the incipit or opening words of the Introit for the Sunday, "Esto mihi in Deum protectorem, et in locum refugii, ut salvum me facias", ("Be Thou unto me a God, a Protector, and a place of refuge, to save me") .

The earliest Quinquagesima Sunday can occur is February 1 and the latest is March 7. Recent and upcoming dates:

In the Roman Catholic Church, the terms for this Sunday (and the two immediately before it — Sexagesima and Septuagesima Sundays) were eliminated in the reforms following the Second Vatican Council, and these Sundays are part of Ordinary Time.

According to the reformed Roman Rite Roman Catholic calendar, this Sunday is now known by its number within Ordinary Time — fourth through ninth, depending upon the date of Easter. The earlier form of the Roman Rite, with its references to Quinquagesima Sunday, and to the Sexagesima and Septuagesima Sundays, continues to be observed in some communities.

In traditional lectionaries, the Sunday concentrates on , "Jesus took the twelve aside and said, 'Lo, we go to Jerusalem, and everything written by the prophets about the Son of Man shall be fulfilled' ... The disciples, however, understood none of this," which from verse 35 is followed by Luke's version of Healing the blind near Jericho. The passage presages the themes of Lent and Holy Week.

In most churches, palms blessed on Palm Sunday of the previous year are burned on this day after the last mass of the day, the ashes of these burned palms are used for the liturgy of Ash Wednesday.

This Sunday has different names in the two different calendars used in the Church of England: in the "Book of Common Prayer" calendar (1662) this Sunday is known as "Quinquagesima", while in the "Common Worship" calendar (2000) it is known as the "Sunday next before Lent". In this latter calendar it is part of the period of Ordinary Time that falls between the feasts of the Presentation of Christ in the Temple (the end of the Epiphany season) and Ash Wednesday.

In the Revised Common Lectionary the Sunday before Lent is designated "Transfiguration Sunday", and the gospel reading is the story of the Transfiguration of Jesus from Matthew, Mark, or Luke. Some churches whose lectionaries derive from the RCL, e.g. the Church of England, use these readings but do not designate the Sunday "Transfiguration Sunday".

In the Eastern Orthodox Church, its equivalent, the Sunday before Great Lent, is called "Forgiveness Sunday", "Maslenitsa Sunday", or "Cheesefare Sunday". The latter name comes because this Sunday concludes Maslenitsa, the week in which butter and cheese may be eaten, which are prohibited during Great Lent. The former name derives from the fact that this Sunday is followed by a special Vespers called "Forgiveness Vespers" which opens Great Lent.
On this day the Eastern Orthodox Church Christians at the liturgy listen to the Gospel speaking of forgiveness of sins, fasting, and the gathering of treasures in heaven. On this day, all Orthodox Christians ask each other for forgiveness to begin the Great Lent with a good heart, to focus on the spiritual life, to purify the heart from sin in confession, and to meet Easter - the day of the Resurrection of Jesus with a pure heart.
This is the last day before Lent when non-lenten food is eaten.

In Lutheranism is combined with (Paul's praise of love).

Composers writing cantatas for Estomihi Sunday include:



</doc>
<doc id="25241" url="https://en.wikipedia.org/wiki?curid=25241" title="Quassia">
Quassia

Quassia ( or ) is a plant genus in the family Simaroubaceae. Its size is disputed; some botanists treat it as consisting of only one species, "Quassia amara" from tropical South America, while others treat it in a wide circumscription as a pantropical genus containing up to 40 species of trees and shrubs. The genus was named after a former slave from Suriname, Graman Quassi in the eighteenth century. He discovered the medicinal properties of the bark of "Quassia amara".

Broader treatments of the genus include the following and other species:

It is the source of the quassinoids quassin and neo-quassin.



</doc>
<doc id="25243" url="https://en.wikipedia.org/wiki?curid=25243" title="Quisling">
Quisling

Quisling (; ) is a term originating in Norway, which is used in Scandinavian languages and in English for a person who collaborates with an enemy occupying force – or more generally as a synonym for traitor. The word originates from the surname of the Norwegian war-time leader Vidkun Quisling, who headed a domestic Nazi collaborationist regime during World War II.

Use of Quisling's surname as a term predates World War II. The first recorded use of the term was by Norwegian Labour Party politician Oscar Torp in a 2 January 1933 newspaper interview, where he used it as a general term for followers of Vidkun Quisling. Quisling was at this point in the process of establishing the Nasjonal Samling (National Unity) party, a fascist party modelled on the German Nazi Party. Further uses of the term were made by Aksel Sandemose, in a newspaper article in "Dagbladet" in 1934, and by the newspaper "Vestfold Arbeiderblad", in 1936. The term with the opposite meaning, a Norwegian patriot, is Jøssing.

The use of the name as a term for collaborators or traitors in general probably came about upon Quisling's unsuccessful 1940 coup d'état, when he attempted to seize power and make Norway cease resisting the invading Germans. The term was widely introduced to an English-speaking audience by the British newspaper "The Times". It published an editorial on 19 April 1940 titled "Quislings everywhere", in which it was asserted that "To writers, the word Quisling is a gift from the gods. If they had been ordered to invent a new word for traitor... they could hardly have hit upon a more brilliant combination of letters. Aurally it contrives to suggest something at once slippery and tortuous." The "Daily Mail" picked up the term four days after "The Times" editorial was published. "The War Illustrated" discussed "potential Quislings" among the Dutch during the German invasion of the Netherlands. Subsequently, the BBC brought the word into common use internationally.

Chips Channon described how during the Norway Debate of 7–8 May 1940, he and other Conservative MPs who supported Prime Minister of the United Kingdom Neville Chamberlain called those who voted against a motion of no confidence "Quislings". Chamberlain's successor Winston Churchill used the term during an address to the Allied Delegates at St. James's Palace on 21 June 1941, when he said: "A vile race of Quislings—to use a new word which will carry the scorn of mankind down the centuries—is hired to fawn upon the conqueror, to collaborate in his designs and to enforce his rule upon their fellow countrymen while grovelling low themselves." He used the term again in an address to both houses of Congress in the United States of America on 26 December 1941. Commenting upon the effect of a number of Allied victories against Axis forces, and moreover the United States’ decision to enter the war, Churchill opined: "Hope has returned to the hearts of scores of millions of men and women, and with that hope there burns the flame of anger against the brutal, corrupt invader. And still more fiercely burn the fires of hatred and contempt for the filthy Quislings whom he has suborned." The term subsequently entered the language and became a target for political cartoonists.

In the United States it was used often. Some examples include: In the Warner Bros. cartoon "Tom Turk and Daffy" (1944), it was uttered by a Thanksgiving turkey whose presence is betrayed to Porky Pig by Daffy Duck. In the American film "Edge of Darkness" (1943), about the Resistance in Norway, the heroine's brother is often described as a quisling.

The back-formed verb, "to quisle" () existed. This back-formed verb gave rise to a much less common version of the noun: "quisler".

However, H. L. Mencken, an authority on American English (he wrote "The American Language", a multi-volume scholarly work) even in 1944 appeared not to be aware of the existence of the verb form, and "to quisle" has entirely disappeared from contemporary usage.

"Quisling" was applied to some Communist figures who participated in the establishment of Communist regimes. As an illustration, the renegade socialist Zdeněk Fierlinger of Czechoslovakia was frequently derided as "Quislinger" for his collaboration with the Communist Party of Czechoslovakia.

"The Patriot Game", one of the best known songs to emerge from the Irish nationalist struggle, includes the line "...those quislings who sold out the Patriot Game" in some versions (although the original uses "cowards" and other versions substitute "rebels" or "traitors").

In a June 2018 New York Times column, Paul Krugman called US President Donald Trump a "quisling", in reference to what Krugman described as Trump's "serv[ing] the interests of foreign masters at his own country’s expense" and "defend[ing] Russia while attacking our closest allies". Other publications also revived the term in the 2010s for use in describing President Trump and his associates and supporters.

In the Norwegian television series Occupied, Norwegians who are seen as collaborating with the Russian invaders and later with European Union peacekeepers are called Quislings.



</doc>
<doc id="25244" url="https://en.wikipedia.org/wiki?curid=25244" title="Quadrangle">
Quadrangle

Quadrangle or The Quadrangle may refer to:


Various specific quadrangles, often called "the quad" or "the quadrangle":







</doc>
<doc id="25247" url="https://en.wikipedia.org/wiki?curid=25247" title="Quill">
Quill

A quill pen is a writing implement made from a moulted flight feather (preferably a primary wing-feather) of a large bird. Quills were used for writing with ink before the invention of the dip pen, the metal-nibbed pen, the fountain pen, and, eventually, the ballpoint pen. The hand-cut goose quill is rarely used as a calligraphy tool, because many papers are now derived from wood pulp and wear down the quill very quickly. However, it is still the tool of choice for a few scribes who noted that quills provide an unmatched sharp stroke as well as greater flexibility than a steel pen.

In a carefully prepared quill, the slit does not widen through wetting and drying with ink. It will retain its shape adequately and only requires infrequent sharpening and can be used time and time again until there is little left of it. The hollow shaft of the feather (the "calamus") acts as an ink reservoir and ink flows to the tip by capillary action.

The strongest quills come from the primary flight feathers discarded by birds during their annual moult. Generally, feathers from the left wing (it is supposed) are favored by the right-handed majority of British writers because the feather curves away from the sight line, over the back of the hand. The quill barrel is cut to six or seven inches in length, so no such consideration of curvature or 'sight-line' is necessary. Additionally, writing with the left hand in the era in which the quill was popular was discouraged, and quills were never sold as left and right-handed, only by their size and species.

Goose feathers are most commonly used; scarcer, more expensive swan feathers are used for larger lettering. Depending on availability and strength of the feather, as well as quality and characteristic of the line wanted by the writer, other feathers used for quill-pen making include those from the crow, eagle, owl, hawk, and turkey. Crow feathers were particularly useful as quills when fine work, such as accounting books, was required. Each bird could supply only about 10 to 12 good-quality quills.

On a true quill, the barbs are stripped off completely on the trailing edge. (The pinion for example only has significant barbs on one side of the barrel.) Later, a fashion developed for stripping partially and leaving a decorative top of a few barbs. The fancy, fully plumed quill is mostly a Hollywood invention and has little basis in reality. Most, if not all, manuscript illustrations of scribes show a quill devoid of decorative barbs, or at least mostly stripped.

Quill pens were used to write the vast majority of medieval manuscripts. Quill pens were also used to write the "Magna Carta" and the Declaration of Independence. U.S. President Thomas Jefferson bred geese specially at Monticello to supply his tremendous need for quills.
Quill pens are still used today mainly by professional scribes and calligraphers.

Quills are also used as the plectrum material in string instruments, particularly the harpsichord.

Quills were the primary writing instrument in the western world from the 6th to the 19th century. The best quills were usually made from goose, swan, and later turkey feathers. Quills went into decline after the invention of the metal pen, mass production beginning in Great Britain as early as 1822 by John Mitchell of Birmingham.

Quill pens were the instrument of choice during the medieval era due to their compatibility with parchment and vellum. Before this the reed pen had been used, but a finer letter was achieved on animal skin using a cured quill. Other than written text, they were often used to create figures, decorations, and images on manuscripts, although many illuminators and painters preferred fine brushes for their work. The variety of different strokes in formal hands was accomplished by good penmanship as the tip was square cut and rigid, exactly as it is today with modern steel pens.

It was much later, in the 1600s, with the increased popularity of writing, especially in the copperplate script promoted by the many printed manuals available from the 'Writing Masters', that quills became more pointed and flexible.

According to the Supreme Court Historical Society, 20 goose-quill pens, neatly crossed, are placed at the four counsel tables each day the U.S. Supreme Court is in session; "most lawyers appear before the Court only once, and gladly take the quills home as souvenirs." This has been done since the earliest sessions of the Court.

Quills are denominated from the order in which they are fixed in the wing; the first is favoured by the expert calligrapher, the second and third quills being very satisfactory also, plus the pinion feather. Flags the 5th and 6th feathers are also used. No other feather on the wing would be considered suitable by a professional scribe.

Information can be obtained on the techniques of curing and cutting quills

In order to harden a quill that is soft, thrust the barrel into hot ashes, stirring it till it is soft; then taking it out, press it almost flat upon your knees with the back of a penknife, and afterwards reduce it to a roundness with your fingers. If you have a number to harden, set water and alum over the fire; and while it is boiling put in a handful of quills, the barrels only, for a minute, and then lay them by.

An accurate account of the Victorian process by William Bishop, from researches with one of the last London quill dressers, is recorded in the "Calligrapher's Handbook" cited on this page.

From the 19th century in radical and socialist symbolism, quill is used to symbolize clerks and intelligentsia. Best known examples is the Radical Civic Union, the Czech National Social Party in combination with the hammer, symbol of the labour movement, or the Democratic Party of Socialists of Montenegro.

A quill knife was the original primary tool used for cutting and sharpening quills, known as "dressing".

Following the decline of the quill in the 1820s, after the introduction of the maintenance-free, mass-produced steel dip nib by John Mitchell, knives were still manufactured but became known as desk knives, stationery knives or latterly as the name stuck "pen" knives.

There is a small but significant difference between a pen knife and a quill knife, in that the quill knife has a blade that is flat on one side and convex on the other which facilitates the round cuts required to shape a quill.

A "pen" knife by contrast has two flat sides. This distinction is not recognised by modern traders, dealers or collectors, who define a quill knife as any small knife with a fixed or hinged blade, including such items as ornamental fruit knives.

Plectra for psalteries and lutes can be cut similarly to writing pens. The rachis, the portion of the stem between the barbs, not the calamus, of the primary flight feathers of birds of the crow family was preferred for harpsichords. In modern instruments, plastic is more common, but they are often still called "quills". The lesiba uses a quill attached to a string to produce sound.




</doc>
<doc id="25248" url="https://en.wikipedia.org/wiki?curid=25248" title="Quo vadis?">
Quo vadis?

Quō vādis? (, ) is a Latin phrase meaning "Where are you marching?". It is also commonly translated as "Where are you going?" or, poetically, "Whither goest thou?".

It also may refer to a Christian tradition regarding Saint Peter. According to the apocryphal Acts of Peter (Vercelli Acts XXXV), Peter flees from crucifixion in Rome at the hands of the government, and along the road outside the city, he meets the risen Jesus. In the Latin translation, Peter asks Jesus, ""Quō vādis?"" He replies, ""Rōmam eō iterum crucifīgī"" ("I am going to Rome to be crucified again"). Peter then gains the courage to continue his ministry and returns to the city, where he is martyred by being crucified upside-down. The Church of Domine Quo Vadis in Rome is built where the meeting between Peter and Jesus allegedly took place.
The words "quo vadis" as a question also occur at least seven times in the Latin Vulgate.

The Polish writer Henryk Sienkiewicz wrote the novel "Quo Vadis: A Narrative of the Time of Nero" (1895), which in turn has been made into motion pictures several times, including a 1951 version that was nominated for eight Academy Awards. For this and other novels, Sienkiewicz received the 1905 Nobel Prize for Literature.

In a season four episode of "M*A*S*H" entitled "Quo Vadis, Captain Chandler?" the reference pertains to Jesus Christ. A shellshocked officer arrives at the Hospital believing he is the Christ. He has numerous conversations with the characters, including Father Mulcahy. He ultimately leaves the MASH unit for an evacuation hospital, still unrecovered and leaving the audience wanting for resolution. Thus the title, "where are you going, Captain Chandler?"


</doc>
<doc id="25249" url="https://en.wikipedia.org/wiki?curid=25249" title="QED">
QED

QED may refer to:






</doc>
<doc id="25254" url="https://en.wikipedia.org/wiki?curid=25254" title="Qantas">
Qantas

Qantas Airways Limited () is the flag carrier of Australia and its largest airline by fleet size, international flights and international destinations. It is the third oldest airline in the world, after KLM and Avianca, having been founded in November 1920; it began international passenger flights in May 1935. The Qantas name comes from ""QANTAS"", an acronym for its original name, ""Queensland and Northern Territory Aerial Services"", and it is nicknamed "The Flying Kangaroo". Qantas is a founding member of the Oneworld airline alliance, along with American Airlines, British Airways, Cathay Pacific, and the now-defunct Canadian Airlines.

The airline is based in the Sydney suburb of Mascot, adjacent to its main hub at Sydney Airport. , Qantas had a 65% share of the Australian domestic market and carried 14.9% of all passengers travelling in and out of Australia. Various subsidiary airlines operate to regional centres and on some trunk routes within Australia under the QantasLink banner. Qantas also owns Jetstar Airways, a low-cost airline that operates both international services from Australia and domestic services within Australia and New Zealand; and holds stakes in a number of other Jetstar-branded airlines.

Qantas was founded in Winton, Queensland on 16 November 1920 by Hudson Fysh, Paul McGinness and Fergus McMaster as Queensland and Northern Territory Aerial Services Limited. The airline's first aircraft was an Avro 504K. It moved its headquarters to Longreach, Queensland in 1921 and Brisbane, Queensland in 1930.

In 1934, QANTAS and Britain's Imperial Airways (a forerunner of British Airways) formed a new company, Qantas Empire Airways Limited (QEA). The new airline commenced operations in December 1934, flying between Brisbane and Darwin. QEA flew internationally from May 1935, when the service from Darwin was extended to Singapore (Imperial Airways operated the rest of the service through to London). When World War II began, enemy action and accidents destroyed half of the fleet of ten, and most of the fleet was taken over by the Australian government for war service.

Flying boat services were resumed in 1943, with flights between the Swan River at Crawley in Perth, Western Australia and Koggala Lake in Ceylon (now Sri Lanka). This linked up with the British Overseas Airways Corporation (BOAC, the successor airline to Imperial Airways) service to London. Qantas' kangaroo logo was first used on the "Kangaroo Route", begun in 1944, from Sydney to Karachi, where BOAC crews took over for the rest of the journey to the UK.

In 1947, QEA was nationalised by the Australian government led by Labor Prime Minister Ben Chifley. QANTAS Limited was then wound up. After nationalisation, Qantas' remaining domestic network, in Queensland, was transferred to the also nationally owned Trans-Australia Airlines, leaving Qantas with a purely international network. Shortly after nationalisation, QEA began its first services outside the British Empire, to Tokyo. Services to Hong Kong began around the same time. In 1957 a head office, Qantas House, opened in Sydney. In June 1959 Qantas entered the jet age when the first Boeing 707-138 was delivered.

On , Qantas merged with nationally owned domestic airline Australian Airlines (renamed from Trans-Australia Airlines in 1986). The airline started to be rebranded to Qantas in the following year. Qantas was gradually privatised between 1993 and 1997. Under the legislation passed to allow the privatisation, Qantas must be at least 51% owned by Australian shareholders.

In 1998, Qantas co-founded the Oneworld alliance with American Airlines, British Airways, Canadian Airlines and Cathay Pacific, with other airlines joining subsequently.

With the entry of new budget airline Virgin Blue (now Virgin Australia) into the domestic market in 2000, Qantas' market share fell. Qantas created the budget Jetstar Airways in 2001 to compete. The main domestic competitor to Qantas, Ansett Australia, collapsed on 14 September 2001. Market share for Qantas immediately neared 90%, but competition with Virgin increased as it expanded; the market share of the Qantas Group eventually settled at a relatively stable position of about 65%, with 30% for Virgin and other regional airlines accounting for the rest of the market.

Qantas briefly revived the Australian Airlines name for a short-lived international budget airline between 2002 and 2006, but this subsidiary was shut down in favour of expanding Jetstar internationally, including to New Zealand. In 2004, the Qantas group expanded into the Asian budget airline market with Jetstar Asia Airways, in which Qantas owns a minority stake. A similar model was used for the investment into Jetstar Pacific, headquartered in Vietnam, in 2007, and Jetstar Japan, launched in 2012.

In December 2006, Qantas was the subject of a failed bid from a consortium calling itself Airline Partners Australia. Merger talks with British Airways in 2008 also did not proceed to an agreement. In 2011, an industrial relations dispute between Qantas and the Transport Workers Union of Australia resulted in the grounding of all Qantas aircraft and lock-out of the airline's staff for two days.

On 25 March 2018, a Qantas Boeing 787 Dreamliner became the first aircraft to operate a scheduled non-stop commercial flight between Australia and Europe, with the inaugural arrival in London of Flight 9 (QF9). QF9 was a 17-hour, 14,498 km (9,009-mile) journey from Perth Airport in Western Australia to London Heathrow.

On October 20, 2019, Qantas Airways completed the longest commercial flight to date between New York and Sydney using Boeing 787-9 Dreamliner in 19hr 20mins.

The key trends for the Qantas Group (Qantas Airways Ltd and Controlled Entities, which includes Jetstar and Qantas Cargo), are shown below (as at year ending 30 June):

Qantas' headquarters are located at the Qantas Centre in the Bayside Council suburb of Mascot, Sydney, New South Wales. The headquarters underwent a redevelopment which was completed in December 2013.

Qantas has operated a number of passenger airline subsidiaries since inception, including:


Qantas operates a freight service under the name Qantas Freight (which uses aircraft operated by Qantas subsidiary Express Freighters Australia and also leases aircraft from Atlas Air) and also wholly owns the logistics-and-air-freight company Australian air Express.

Qantas, through its Aboriginal and Torres Strait Islander Programme, has some links with the Aboriginal Australian community. As of 2007, the company has run the programme for more than ten years and 1–2% of its staff are Aboriginal and Torres Strait Islander. Qantas employs a full-time Diversity Coordinator, who is responsible for the programme.

Qantas has also bought and donated some Aboriginal art. In 1993, the airline bought a painting — "Honey Ant and Grasshopper Dreaming" — from the Central Australian desert region. As of 2007, this painting is on permanent loan to Yiribana at the Art Gallery of New South Wales. In 1996, Qantas donated five extra bark paintings to the gallery. Qantas has also sponsored and supported Aboriginal artists in the past.

An early television campaign, starting in 1969 and running for several decades, was aimed at American audiences; it featured a live koala, voiced by Howard Morris, who complained that too many tourists were coming to Australia and concluded "I hate Qantas." The koala ads have been ranked among the greatest commercials of all time. A long-running advertising campaign features renditions by children's choirs of Peter Allen's "I Still Call Australia Home", at various famous landmarks in Australia and foreign locations such as Venice.

Qantas is the main sponsor of the Australia national rugby union team. It also sponsors the Socceroos, Australia's national association football team. Qantas was the naming rghts sponsor for the Formula One Australian Grand Prix from 2010 until 2012. On 26 December 2011, Qantas signed a four-year deal with Australian cricket's governing body Cricket Australia, to be the official carrier of the Australia national cricket team.

Qantas management has expressed strong support for Marriage Equality and LGBTIQ issues, with CEO Alan Joyce said to be, "arguably the most prominent corporate voice in the marriage equality campaign." As official airline partner for the Sydney Mardi Gras, Qantas decorated one of its aircraft with rainbow wording and positioned a rainbow flag next to the tail's flying kangaroo. Qantas also served pride cookies to its passengers. It had a rainbow roo float in the Mardi Gras parade. There has been criticism of Qantas using its corporate power to prosecute the private interests on their staff and the community. Peter Dutton has said that chief executives such as Alan Joyce at Qantas should "stick to their knitting" rather than using the company's brand to advocate for political causes. A senior church leader has made similar comments. Despite the criticism, Qantas will continue to advocate for marriage equality which will include offering customers specially commissioned rings with the phrase, "until we all belong". This phrase will also appear on Qantas boarding passes and other paraphernalia. The cost of the campaign by Qantas and other participating companies is expected to be more than $5 million.

Joyce has pledged Qantas will, "continue social-justice campaigning". In relation to a rugby player, sacked by Rugby Australia which is financially supported by Qantas, following his social media postings on homosexuality.

In August 2011, the company announced that following financial losses of A$200 million ($209 million) for the year ending June 2011 and a decline in market share, major structural changes would be made. As part of the changes up to 1,000 jobs would be lost in Australia, and a new Asia-based premium airline was to be set up and operate under a different name. The new airline did not eventuate. Also announced was an intention to launch a budget airline, Jetstar Japan, in partnership with Japan Airlines and Mitsubishi Corporation. The direction was deemed necessary because of losses in the airline's international operations as a result of increased competition from airlines such as Emirates and Singapore Airlines along with the deregulation of Australian international routes during the mid-to-late 1980s. The reforms included route changes, in particular the cessation of services to London via Hong Kong and Bangkok. While Qantas would still operate to these cities, onward flights to London would be via its Oneworld partner British Airways under a code-share service.
The following year Qantas reported a A$245 million full-year loss to the end of June 2012, citing high fuel prices, intense competition and industrial disputes. This was the first full year loss since Qantas was fully privatised 17 years previously, in 1995, and led to the airline cancelling its order of 35 new Boeing 787 Dreamliner aircraft, to reduce its spending. In focusing on core business, Qantas also divested itself of its 50% holding of StarTrack, Australia's largest road freight company, in part for acquiring full interest in Australian Air Express. In that year on 26 March 2012, Qantas announced it would set up Jetstar Hong Kong with China Eastern Airlines Corporation, which was intended to begin flights in 2013, but became embroiled in a protracted approval process.

Qantas and Emirates began an alliance on 31 March 2013, in which their combined carriers offered 98 flights per week to Dubai, that saw bookings up six-fold. In September 2013, following the announcement the carrier expected another A$250 million ( million) net loss for the half-year period that ended on 31 December and the implementation of further cost-cutting measures that would see the cut of 1,000 jobs within a year, S&P downgraded Qantas credit from BBB- (the lowest investment grade) to BB+. Moody's applied a similar downgrading a month later.

Losses continued into 2014 reporting year, with the Qantas Group reporting a half year loss of A$235 million ( million) and eventual full year loss of A$2.84 billion. In February 2014 additional cost-cutting measures to save A$2 billion, including the loss of 5,000 jobs that will see the workforce lowered from 32,000 to 27,000 by 2017 were announced. In May 2014 the company stated it expected to shed 2,200 jobs by June 2014, including those of 100 pilots. The carrier also reduced the size of its fleet by retiring aircraft and deferring deliveries; and planned to sell some of its assets. With 2,200 employees laid off by June 2014, another 1,800 job positions were planned to be cut by June 2015. Also during 2014 the "Qantas Sale Act", under which the airline was privatised, was amended to repeal parts of section 7. That act limits foreign ownership of Qantas to 49 percent, with foreign airlines subject to further restrictions, including a 35-percent limit for all foreign airline shareholdings combined. In addition, a single foreign entity can hold no more than 25 percent of the airline's shares.

The airline returned to profit in 2015, announcing a A$557 million after tax profit in August 2015, in contrast with a A$2.84 billion loss the year earlier. In 2015, Qantas sold its lease of Terminal 3 at Sydney Airport, which was due to continue until 2019, back to Sydney Airport Corporation for $535 million. This meant Sydney Airport resumed operational responsibility of the terminal, including the lucrative retail areas.

Paris-based Australian designer Martin Grant is responsible for the new Qantas airline staff uniforms that were publicly unveiled on 16 April 2013. These were to replace the previous uniforms, dubbed colloquially as "Morrisey" by staff after the designer, Peter Morrissey. Qantas ambassador and model Miranda Kerr assisted with the launch of the new outfits for which the colours of navy blue, red and fuchsia pink are combined. Qantas chief executive Alan Joyce stated that the new design "speaks of Australian style on the global stage" at the launch event that involved Qantas employees modelling the uniforms. Grant consulted with Qantas staff members over the course of one year to finalise the 35 styles that were eventually created. Not all employees were happy with the new uniform, however, with one flight attendant being quoted as saying "The uniforms are really tight and they are simply not practical for the very physical job we have to do."

Qantas operates flightseeing charters to Antarctica on behalf of Croydon Travel. It first flew Antarctic flightseeing trips in 1977. They were suspended for a number of years due to the crash of Air New Zealand Flight 901 on Mount Erebus in 1979. Qantas restarted the flights in 1994. Although these flights do not touch down, they require specific polar operations and crew training due to factors like sector whiteout, which contributed to the 1979 Air New Zealand disaster.

With Flights 7 and 8 – a non-stop service between Sydney and Dallas/Fort Worth operated by the Airbus A380 – commencing on 29 September 2014, Qantas operated the world's longest passenger flight on the world's largest passenger aircraft. This was overtaken on 1 March 2016 by Emirates' new Auckland-Dubai service. After it ordered Boeing 787 aircraft, Qantas announced an intention to launch non-stop flights between Australia and the United Kingdom during March 2018 from Perth, Western Australia to London. The inaugural flight left Perth on 24 March.

Qantas is a member and one of the founders of Oneworld, an airline alliance.

, Qantas had codeshare agreements with the following airlines:
In addition to the above codeshares, Qantas has currently entered into joint ventures with the following airlines:

, the Qantas mainline fleet consists of the following aircraft:

, Qantas and its subsidiaries operated 297 aircraft, including 71 aircraft by Jetstar Airways; 90 by the various QantasLink-branded airlines and six by Express Freighters Australia (on behalf of Qantas Freight, which also wet leases three Atlas Air Boeing 747-400Fs).

On 22 August 2012, Qantas announced that, due to losses and to conserve capital, it had cancelled its 35-aircraft Boeing 787-9 order while keeping the 15-aircraft 787-8 order for Jetstar Airways and moving forward 50 purchase rights. On 20 August 2015 Qantas announced that it had ordered eight Boeing 787-9s for delivery from 2017.

In February 2019, Qantas cancelled its remaining orders for a further eight Airbus A380-800 aircraft.

In June 2019, during the Paris Air Show, Qantas Group converted 26 Airbus A321neo orders to the A321XLR variant and another ten A321neo orders to the A321LR variant; and ordered an additional ten A321XLRs. This brought Qantas Group's total Airbus A320neo family order to 109 aircraft, consisting of 45 A320neos, 28 A321LRs, and 36 A321XLRs. At the time of the announcement, Qantas CEO Alan Joyce stated that a decision had not yet been made on how the aircraft would be distributed between Qantas and Jetstar Airways, or whether they were to be used for network growth or the replacement of older aircraft.

In December 2019, Qantas announced it had selected the Airbus A350-1000 for its "Project Sunrise" program of non-stop flights from Sydney, Melbourne and Brisbane to cities such as London, New York, Paris, Rio de Janeiro and Cape Town. No orders have been placed but Qantas will work closely with Airbus to prepare contract terms for up to 12 aircraft ahead of a final decision by the Qantas Board.

Qantas has named its aircraft since 1926. Themes have included Greek gods; stars; people in Australian aviation history; and Australian birds. Since 1959, the majority of Qantas aircraft have been named after Australian cities. The Airbus A380 series, the flagship of the airline, is named after Australian aviation pioneers, with the first A380 named "Nancy-Bird Walton".

Two Qantas aircraft are currently decorated with an Indigenous Australian art scheme. One aircraft, a Boeing 737-800, wears a livery called "Mendoowoorrji", which was revealed in November 2013. The design was drawn from the late West Australian Aboriginal artist Paddy Bedford.

A Boeing 787-9 Dreamliner is adorned in a paint scheme inspired by the late Emily Kame Kngwarreye's 1991 painting "Yam Dreaming". The adaptation of "Yam Dreaming" to the aircraft, led by Balarinji, a Sydney-based and Aboriginal-owned design firm, incorporates the red Qantas tailfin into the design, which includes white dots with red and orange tones. The design depicts the yam plant, an important and culturally significant symbol in Kngwarreye's Dreaming stories, and a staple food source in her home region of Utopia. The design was applied to the aircraft during manufacture, prior to its delivery in March 2018 to Alice Springs Airport, situated 230 kilometers southeast of Utopia, where the aircraft was met by Kngwarreye's descendants, the local community, and Qantas executives. The aircraft would later operate Qantas' inaugural nonstop services between Perth and London Heathrow, and between Melbourne and San Francisco, scheduled with Boeing 787 aircraft.

Australian Aboriginal art designs have previously adorned some Qantas aircraft; the first design was called "Wunala Dreaming", which was unveiled in 1994 and had been painted on now-retired Boeing 747-400 and 747-400ER aircraft between 1994 and 2012. The "motif" was an overall-red design depicting ancestral spirits in the form of kangaroos travelling in the outback.

The second design was called "Nalanji Dreaming" and was depicted on a Boeing 747-300 from 1995 until its retirement in 2005. "Nalanji Dreaming" was a bright blue design inspired by rainforest landscape and tropical seas.

The third design was titled "Yananyi Dreaming," and featured a depiction of Uluru. The scheme was designed by Uluru-based artist Rene Kulitja, in collaboration with Balarinji. It was painted on the 737 at the Boeing factory prior to its delivery in 2002. It was repainted into the standard livery in 2014.

In November 2014 the airline revealed that the 75th Boeing 737-800 jet to be delivered would carry a 'retro-livery' based on the airline's 1971 'ochre' colour scheme design featuring the iconic 'Flying Kangaroo' on its tail and other aspects drawn from its 1970s fleet. The aircraft was delivered on 17 November.

Qantas announced a second 737-800 would receive a 'retro roo' livery in October 2015. On 16 November 2015 the airline unveiled the second 'retro roo' 737, bearing a replica livery from 1959 to celebrate the airline's 95th birthday.

Several Qantas aircraft have been decorated with promotional liveries, promoting telecommunications company Optus; the Disney motion picture "Planes"; the Australian national association football team, the Socceroos; and the Australian national rugby union team, the Wallabies. Two aircraft – an Airbus A330-200 and a Boeing 747-400ER – were decorated with special liveries promoting the Oneworld airline alliance (of which Qantas is a member) in 2009. On 29 September 2014, nonstop Airbus A380 service to Dallas/Fort Worth International Airport was inaugurated using an A380 decorated with a commemorative cowboy hat and bandana on the kangaroo tail logo. Prior to the 2017 Sydney Mardi Gras, Qantas decorated one of its Airbus A330-300 aircraft with rainbow lettering and depicted a rainbow flag on the tail of the aircraft.

Qantas domestic flights are primarily operated by Boeing 737-800 and Airbus A330-200 aircraft. A two class configuration of Business and Economy is offered. On small routes, an all-Economy configuration may be available.

Domestic Business Class is offered on all Boeing 737, Airbus A330-300 and Airbus A330-200 aircraft. On the Boeing 737, Business is exclusively available in the first three rows of the cabin, with a seat configuration of 2-2, seat recline and a larger pitch between seats. As offered on International flights, Business Suites on Airbus A330s are sometimes available on Domestic routes. These seats feature all-aisle access in a 1-2-1 configuration and a fully flat bed.

Domestic Economy Class is offered on all Boeing 737, Airbus A330-300 and Airbus A330-200 aircraft. Seat pitch is usually and seat width ranges from . Layouts are 3–3 on the 737 and 2-4-2 on the A330.

Qantas international flights are primarily operated on Airbus A380s, A330-300s, Boeing 747s, 787s and sometimes on Airbus A330-200s and Boeing 737-800s. Passenger class configuration varies by aircraft, with the Airbus A330-300 offering a two class configuration of Business and Economy on short to medium haul flights. This compares to the Airbus A380, which offers a four class configuration of First, Business, Premium Economy and Economy on selected long haul flights.

First class is offered exclusively on the Airbus A380.

It offers 14 individual suites in a 1-1-1 layout. The seats rotate, facing forward for takeoff, but rotating to the side for dining and sleeping, with 83.5 in seat pitch (extending to a 212 cm fully flat bed) and a width of . Each suite has a widescreen HD monitor with 1,000 AVOD programs. In addition to 110 V AC power outlets, USB ports are offered for connectivity. Passengers are also able to make use of the on-board business lounge on the upper deck. Complimentary access to both the first class and business class lounges (or affiliated lounges) is offered.

Updated versions of this seat were fitted to the airline's refurbished Airbus A380 aircraft from late 2019. This seat featured refreshed cushioning and enhanced entertainment screens compared to the older version seat.

International Business class is offered on all Qantas mainline passenger aircraft.

On all International and selected Domestic flights, Qantas offers two different types of Business Class seats, as listed below.

Business Suites are offered exclusively on all Boeing 787, Airbus A330-300 and selected Airbus A330-200 and A380 aircraft. These seats, designed by Mark Newson feature all-aisle access in a 1-2-1 configuration. The Business Suite, which was introduced on the A330 in October 2014, also features a fully flat 198 cm bed. This seat can be reclined during takeoff and landing while sporting the latest Panasonic eX3 system with a touchscreen. By the end of 2016, business class of Qantas' entire fleet of Airbus A330 aircraft were refitted. Airbus A330 Business Suites are available on Asian, transcontinental routes across Australia and smaller routes such as the East Coast triangle.

Newer versions of this seat were fitted to the airline's new Boeing 787 fleet in late 2017. This seat featured enhanced privacy options compared to the A330 seat and has been used on the 17-hour Perth to London sector since March 2018.

Business Skybeds are offered on all Boeing 747 and selected A380 aircraft. On the Boeing 747, seating is in a 2-3-2 configuration on the main deck and a 2–2 configuration on the upper deck.

Older versions of the lie-flat Skybeds featured of seat pitch and width; however passengers slept at a distinct slope to the cabin floor. Later versions of the Skybed have an pitch, and lie fully horizontal. Skybed seats on Boeing 747s feature a touchscreen monitor with 400 AVOD programs. The Boeing 747 Business Skybeds are available on Asian, African and South America routes.

On the Airbus A380, 64 fully flat Skybed seats are available with seat pitch (converting to a 200 cm long bed). These seats are located on the upper-deck in a 2-2-2 configuration in two separate cabins. Features include a 30 cm touchscreen monitor with 1,000 AVOD programmes and an on-board lounge. Airbus A380 Business Skybeds are available on Qantas' flagship routes such as Australia to/from: London via Singapore, Los Angeles, Dallas and Hong Kong (seasonal).

In 2019, Qantas began the process of retrofitting its Airbus A380 aircraft with new Business Suites as offered on Airbus A330 and Boeing 787 aircraft. The aircraft will gain six business class seats compared to the previous configuration and all aircraft will be completed by the end of 2020.

Complimentary access to the Qantas business class lounge (or affiliated lounges) is also offered.

Premium economy class is offered exclusively on all Airbus A380, Boeing 787-9 and Boeing 747-400 aircraft. It has a seat pitch of on the Boeing 747 and it ranges from on the Airbus A380, with a width of . On the Boeing 747, it is configured in a 2-3-2 seating arrangement around the middle of the main deck, whilst it is in a 2-3-2 at the rear of the upper deck on the A380. On the Boeing 787, it is configured in a 2-3-2 seating arrangement around the middle of the aircraft. The total number of seats depends on the aircraft type, as A380s have 35 seats, 747s have 36 seats and 787s have 28 seats.

Qantas premium economy is presented as a lighter business class product rather than most other airlines' premium economy, which is often presented as a higher economy class, however Qantas premium economy does not offer access to premium lounges, and meals are only a slightly uprated version of economy class meals.

In 2019, Qantas began the process of retrofitting its Airbus A380 aircraft with new Premium Economy seats, as offered on Boeing 787 aircraft. The aircraft will gain 25 premium economy seats compared to the previous configuration and all aircraft will be completed by the end of 2020.

International Economy class is available on all Qantas mainline passenger aircraft.

Seat pitch is usually and seat width ranges from . Layouts are 3–3 on the 737, 2-4-2 on the A330, 3-3-3 on the B787-9 and 3-4-3 on the 747. On the A380, the layout is 3-4-3 and there are four self-service snack bars located in between cabins.

Every Qantas mainline aircraft has some form of video audio entertainment. Qantas has several types of in-flight entertainment (IFE) systems installed on its aircraft and refers to the in-flight experience as "On:Q".

The "Total Entertainment System" by Rockwell Collins was featured on selected domestic and international aircraft between 2000 and 2019. This AVOD system included personal LCD screens in all classes, located in the seat back for economy and business class, and in the armrest for premium economy and first class.

The Mainscreen System, featured on all domestic configured Boeing 737-800s delivered before 2011 has overhead video screens as the main form of entertainment. Movies are shown on the screens for lengthier flights, or TV programmes on shorter flights. A news telecast will usually feature at the start of the flight. Audio options are less varied than on Q, iQ or the Total Entertainment System.

The "iQ" entertainment system by Panasonic Avionics Corporation is featured on all Boeing 747, and selected Airbus A380 and Boeing 737-800 aircraft. This audio video on demand (AVOD) experience, introduced in 2008, is based on the Panasonic Avionics system and features expanded entertainment options; touch screens; and new communications-related features such as Wi-Fi and mobile phone functionality; as well as increased support for electronics (such as USB and iPod connectivity).

The "Q Inflight Entertainment System" by Panasonic Avionics Corporation in collaboration with Massive Interactive is featured on all Airbus A330-300, A330-200, Boeing 787 and selected Airbus A380 aircraft. This audio video on demand (AVOD) experience, introduced in 2014 and updated in 2018 on selected aircraft, is based on the Panasonic eX3 system and features extensive entertainment options; enhanced touch screens; and communications-related features such as Wi-Fi and mobile phone functionality; as well as increased support for electronics (such as USB and iPod connectivity). A "my flight" feature offers access to maps, playlists and a service timeline showing when drinks and meals will be served and the best time for resting on long-haul flights.

Q Streaming is an in-flight entertainment system in which entertainment is streamed to iPads or personal devices available in all classes on selected aircraft. A selection of movies, TV, music and a kids' choice are available.

In 2007, Qantas conducted a trial for use of mobile telephones with AeroMobile, during domestic services for three months on a Boeing 767. During the trial, passengers were allowed to send and receive text messages and emails, but were not able to make or receive calls.

Since 2014, Sky News Australia has provided multiple news bulletins both in-flight and in Qantas branded lounges. Previously, the Australian Nine Network provided a news bulletin for Qantas entitled "Nine's Qantas Inflight News", which was the same broadcast as Nine's "Early Morning News", however Nine lost the contract to Sky News.

In July 2015, Qantas signed a deal with American cable network HBO to provide over 120 hours of television programming in-flight from the network which will be updated monthly, as well as original lifestyle and entertainment programming from both Foxtel and the National Geographic Channel.

In 2017 Qantas commenced rolling out complimentary high speed Wi-Fi on domestic aircraft. The services utilises NBN Co Sky Muster satellites to deliver higher speeds than generally offered by onboard Wi-Fi. Previously, in July 2007 Qantas had announced Wi-Fi on would be available on its long haul A380s and 747-400s although that system ultimately did not proceed following trials.

"Qantas The Australian Way" is the airline's in-flight magazine. In mid-2015, the magazine ended a 14-year publishing deal with Bauer Media, switching its publisher to Medium Rare.

The Qantas Club is the airline lounge for Qantas with airport locations around Australia and the world. Additionally, Qantas operates dedicated international first-class lounges in Sydney, Melbourne, Auckland, Los Angeles and Singapore. Domestically, Qantas also offers dedicated Business Lounges at Sydney, Melbourne, Brisbane, Canberra and Perth for domestic Business Class, Qantas Platinum and Platinum One, and OneWorld Emerald frequent flyers.

In April 2013, Qantas opened its new flagship lounge in Singapore, the Qantas Singapore Lounge. This replaced the former separate first- and business-class lounges as a result of the new Emirates alliance. Similar combined lounges were also opened in Hong Kong in April 2014 and in Brisbane in October 2016. These new lounges provide the same service currently offered by Sofitel in its flagship First lounges in Sydney and Melbourne and a dining experience featuring Neil Perry's Spice Temple inspired dishes and signature cocktails.

Qantas Club Members, Gold Frequent Flyers and Oneworld Sapphire holders are permitted to enter domestic Qantas Clubs when flying on Qantas or Jetstar flights along with one guest who need not be travelling. Platinum and Oneworld Emerald Members are permitted to bring in two guests who do not need to be travelling. Internationally, members use Qantas International Business Class lounges (or the Oneworld equivalent). Guests of the member must be travelling to gain access to international lounges. When flying with American Airlines, members have access to Admirals Club lounges and when flying on British Airways, members have access to British Airways' Terraces and Galleries Lounges.

Platinum Frequent Flyers had previously been able to access the Qantas Club in Australian domestic terminals at any time, regardless of whether they were flying that day. Travellers holding Oneworld Sapphire or Emerald status are also allowed in Qantas Club lounges worldwide.

Access to Qantas First lounges is open to passengers travelling on internationally operated Qantas or Oneworld first-class flights, as well as Qantas platinum and Oneworld emerald frequent flyers. Emirates first-class passengers are also eligible for access to the Qantas first lounges in Sydney and Melbourne.

The Qantas Club also offers membership by paid subscription (one, two, or four years) or by achievement of Gold or Platinum frequent flyer status. Benefits of membership include lounge access, priority check-in, priority luggage handling and increased luggage allowances.

The Qantas frequent-flyer program is aimed at rewarding customer loyalty. The program is long-standing, although the date of the actual inception has been a matter that has generated some commentary. Qantas state the program launched in 1987 although other sources claim what is the current program was launched in the early 1990s, with a Captain's Club program existing before that.

Points are accrued based on distance flown, with bonuses that vary by travel class. Points can also be earned on other Oneworld airlines as well as through other non-airline partners. Points can be redeemed for flights or upgrades on flights operated by Qantas, Oneworld airlines, and other partners. Other partners include credit cards, car rental companies, hotels and many others. Flights with Qantas and selected partner airlines earn Status Credits — and accumulation of these allows progression to Silver status (Oneworld Ruby), Gold status (Oneworld Sapphire), Platinum and Platinum One status (Oneworld Emerald).

Membership of the program has grown significantly since 2000, when the program had 2.4 million members. By 2005 membership had grown to 4.3 million, then to 7.2 million by 2010 and 10.8 million in 2015. As at 2018, the program has 12.3 million members, or approaching the equivalent of half of the Australian population.

Qantas has faced criticism regarding availability of seats for members redeeming points. In 2004, the Australian Competition and Consumer Commission directed Qantas to provide greater disclosure to members regarding the availability of frequent-flyer seats.

In March 2008, an analyst at JPMorgan Chase suggested that the Qantas frequent-flyer program could be worth A$2 billion (US$1.9 billion), representing more than a quarter of the total market value of Qantas.

On 1 July 2008 a major overhaul of the program was announced. The two key new features of the program were Any Seat rewards, in which members could now redeem any seat on an aircraft, rather than just selected seats — at a price. The second new feature was Points Plus Pay, which has enabled members to use a combination of cash and points to redeem an award. Additionally, the Frequent Flyer store was also expanded to include a greater range of products and services.
Announcing the revamp, Qantas confirmed it would be seeking to raise about A$1 billion in 2008 by selling up to 40% of the frequent flyer program. However, in September 2008, it stated it would defer the float, citing volatile market conditions.

It is often claimed that Qantas has never had an aircraft crash. While it is true that the company has neither lost a jet airliner nor had any jet fatalities, it had eight fatal accidents and an aircraft shot down between 1927 and 1945, with the loss of 63 people. Half of these accidents and the shoot-down occurred during World War II, when the Qantas aircraft were operating on behalf of Allied military forces. Post-war, it lost another four aircraft (one was owned by BOAC and operated by Qantas in a pooling arrangement) with a total of 21 people killed. The last fatal accidents suffered by Qantas were in 1951, with three fatal crashes in five months. Qantas' safety record allows the airline to be officially known as the world's safest airline for seven years in a row from 2012 until 2019 (current).

Since the end of World War II, the following accidents and incidents have occurred:

On 26 May 1971 Qantas received a call from a "Mr. Brown" claiming that there was a bomb planted on a Hong Kong-bound jet and demanding $500,000 in unmarked $20 notes. The caller and threat were taken seriously when he directed police to an airport locker where a functional bomb was found. Arrangements were made to pick up the money in front of the head office of the airline in the heart of the Sydney business district. Qantas paid the money and it was collected, after which Mr. Brown called again, advising the "bomb on the plane" story was a hoax. The initial pursuit of the perpetrator was bungled by the New South Wales Police Force which, despite having been advised of the matter from the time of the first call, failed to establish adequate surveillance of the pick-up of the money. Directed not to use their radios (for fear of being "overheard"), the police were unable to communicate adequately. Tipped off by a still-unidentified informer, the police arrested an Englishman, Peter Macari, finding more than $138,000 hidden in an Annandale property. Convicted and sentenced to 15 years in prison, Macari served nine years before being deported to Britain. More than $224,000 remains unaccounted for. The 1986 telemovie "Call Me Mr. Brown", directed by Scott Hicks and produced by Terry Jennings, relates to this incident. On 4 July 1997 a copycat extortion attempt was thwarted by police and Qantas security staff.

In November 2005 it was revealed that Qantas had a policy of not seating adult male passengers next to unaccompanied children. This led to accusations of discrimination. The policy came to light following an incident in 2004 when Mark Wolsay, who was seated next to a young boy on a Qantas flight in New Zealand, was asked to change seats with a female passenger. A steward informed him that "it was the airline's policy that only women were allowed to sit next to unaccompanied children". Cameron Murphy of the NSW Council for Civil Liberties president criticised the policy and stated that "there was no basis for the ban". He said it was wrong to assume that all adult males posed a danger to children. The policy has also been criticised for failing to take female abusers into consideration.

In 2010, when British Airways was successfully sued to change its child seating policy, Qantas argued again that banning men from sitting next to unaccompanied children "reflected parents' concerns". In August 2012, the controversy resurfaced when a male passenger had to swap seats with a female passenger after the crew noticed he was sitting next to an unrelated girl travelling alone. The man felt discriminated against and humiliated before the other passengers as a possible paedophile. A Qantas spokesman defended the policy as consistent with that of other airlines in Australia and around the globe.

In 2006 a class action lawsuit, alleging price-fixing on air cargo freight, was commenced in Australia. The lawsuit was settled early in 2011 with Qantas agreeing to pay in excess of $21 million to settle the case.

Qantas has pleaded guilty to participating in a cartel that fixed the price of air cargo. Qantas Airways Ltd. was fined CAD$155,000 after it admitted that its freight division fixed surcharges on cargo exported on certain routes from Canada between May 2002 and February 2006. In July 2007, Qantas pleaded guilty in the United States to price fixing and was fined a total of $61 million through the Department of Justice investigation. The executive in charge was jailed for six months. Other Qantas executives were granted immunity after the airline agreed to co-operate with authorities. In 2008 the Australian Competition and Consumer Commission fined the airline $20 million for breaches of the acts associated with protecting consumers. In November 2010 Qantas was fined 8.8 million euros for its part in an air cargo cartel involving up to 11 other airlines. Qantas was fined NZ$6.5 million in April 2011 when it pleaded guilty in the New Zealand High Court to the cartel operation.

In response to ongoing industrial unrest over failed negotiations involving three unions (the Australian Licensed Aircraft Engineers Association (ALAEA), the Australian and International Pilots Association (AIPA) and the Transport Workers Union of Australia (TWU)), the company grounded its entire domestic and international fleet from 5 pm AEDT on 29 October. Employees involved would be locked out from 8 p.m. AEDT on 31 October. It was reported that the grounding would have a daily financial impact of A$20 million. In the early hours of 31 October, Fair Work Australia ordered that all industrial action taken by Qantas and the involved trade unions be terminated immediately. The order was requested by the federal government amid fears that an extended period of grounding would do significant damage to the national economy, especially the tourism and mining sectors. The grounding affected an estimated 68,000 customers worldwide.

Qantas has been subject to protests in relation to asylum seekers deportations leading to disruptions of flights. In 2015 activists prevented the transfer of a Tamil man from Melbourne to Darwin (from where he was to be deported to Colombo) by refusing to take their seats on a Qantas flight. It was reported that Qantas banned the student from taking Qantas flights in the future. A nameless head of security from Qantas sent a letter to the Melbourne student's email account saying her "actions are unacceptable and will not be tolerated by the Qantas Group or the Jetstar Group". Also in 2015, another Tamil man was to be sent from Melbourne to Darwin to later be deported. A protest by the man led to him not being put on the plane. A spokesman for Qantas said flight QF838 was delayed almost two hours. The delays reportedly caused inconvenience to multiple passengers, especially those with connecting flights. A spokesperson from Qantas stated that, "[s]afety and security is the number-one priority for all airlines and an aircraft is not the right place for people to conduct protests." Campaigners also asked Qantas to rule out deporting Iraqi man Saeed in 2017. Campaigners have asked Qantas not to participate in the deportation of the high-profile case of Priya and Nades. In response a Qantas spokesperson stated: "We appreciate that this is a sensitive issue. The government and courts are best placed to make decisions on complex immigration matters, not airlines".

In 2009, Qantas was one of the inaugural inductees into the Queensland Business Leaders Hall of Fame.




</doc>
<doc id="25256" url="https://en.wikipedia.org/wiki?curid=25256" title="QED (text editor)">
QED (text editor)

QED is a line-oriented computer text editor that was developed by Butler Lampson and L. Peter Deutsch for the Berkeley Timesharing System running on the SDS 940. It was implemented by L. Peter Deutsch and Dana Angluin between 1965 and 1966.

QED (for "quick editor") addressed teleprinter usage, but systems "for CRT displays [were] not considered, since many of their design considerations [were] quite different."
Ken Thompson later wrote a version for CTSS; this version was notable for introducing regular expressions. Thompson rewrote QED in BCPL for Multics. The Multics version was ported to the GE-600 system used at Bell Labs in the late 1960s under GECOS and later GCOS after Honeywell took over GE's computer business. The GECOS-GCOS port used I/O routines written by A. W. Winklehoff. Dennis Ritchie, Ken Thompson and Brian Kernighan wrote the QED manuals used at Bell Labs.
Given that the authors were the primary developers of the Unix operating system, it is natural that QED had a strong influence on the classic UNIX text editors ed, sed and their descendants such as ex and sam, and more distantly AWK and Perl.

A version of QED named FRED (Friendly Editor) was written at the
University of Waterloo for Honeywell systems by Peter Fraser. A University of Toronto team consisting of Tom Duff, Rob Pike, Hugh Redelmeier, and David Tilbrook implemented a version of QED that runs on UNIX; David Tilbrook later included QED as part of his QEF tool set.

QED was also used as a character-oriented editor on the Norwegian-made Norsk Data systems, first Nord TSS, then Sintran III. It was implemented for the Nord-1 computer in 1971 by Bo Lewendal who after working with Deutsch and Lampson at Project Genie and at the Berkeley Computer Corporation, had taken a job with Norsk Data (and who developed the Nord TSS later in 1971). 




</doc>
<doc id="25257" url="https://en.wikipedia.org/wiki?curid=25257" title="Qusay Hussein">
Qusay Hussein

Qusay Saddam Hussein al-Tikriti (or Qusai, ; – ) was an Iraqi politician. He was the second son of former Iraqi President Saddam Hussein. He was appointed as his father's heir apparent in 2000. He was also in charge of the Military.

Hussein was born in Baghdad in 1966 to Ba'athist revolutionary Saddam Hussein, who was in prison at the time, and his wife and cousin, Sajida Talfah. Unlike other members of his family and the government, little is known about Hussein, politically or personally. He married Sahar Maher Abd al-Rashid; the daughter of Maher Abd al-Rashid, a top ranking military official, and had three sons: Mustapha Qusay (born 3 January 1989 – 22 July 2003); Yahya Qusay (born 1991) and Yaqub Qusay (birthyear unknown).

Hussein played a role in crushing the Shiite uprising in the aftermath of the 1991 Gulf War and is also thought to have masterminded the destruction of the southern marshes of Iraq. The wholesale destruction of these marshes ended a centuries-old way of life that prevailed among the Shiite Marsh Arabs who made the wetlands their home, and ruined the habitat for dozens of species of migratory birds. The Iraqi government stated that the action was intended to produce usable farmland, though a number of outsiders believe the destruction was aimed against the Marsh Arabs as retribution for their participation in the 1991 uprising.

Hussein's older brother Uday was viewed as their father's heir-apparent until he sustained serious injuries in a 1996 assassination attempt. Unlike Uday, who was known for extravagance and erratic, violent behavior, Qusay kept a low profile so details regarding his actions and roles are obscure.

Iraqi dissidents claim that Hussein was responsible for the killing of many political activists. "The Sunday Times" reported that Hussein ordered the killing of Khalis Mohsen al-Tikriti, an engineer at the military industrialization organization, because he believed Mohsen was planning to leave Iraq. In 1998, Iraqi opposition groups accused Hussein of ordering the execution of thousands of political prisoners after hundreds of inmates were similarly executed to make room for new prisoners in crowded jails.

Hussein's service in the Iraqi Republican Guard began in 2000. It is believed that he became the supervisor of the Guard and the head of internal security forces (possibly the Special Security Organization (SSO)), and had authority over other Iraqi military units.

On the afternoon of 22 July 2003, troops of the 101st Airborne 3/327th Infantry HQ and C-Company, aided by U.S. Special Forces, killed Hussein, his 14-year-old son Mustapha, and his older brother Uday, during a raid on a house in the northern Iraqi city of Mosul. Acting on a tip from Hussein's cousin, a special forces team attempted to apprehend everyone in the house at the time. After being fired on, the special forces moved back and called for backup. After Task Force 121 members were wounded, the 3/327th Infantry surrounded and fired on the house with a TOW missile, Mark 19 Automatic Grenade Launcher, M2 50 Caliber Machine guns and small arms. After about four hours of battle (the whole operation lasted 6 hours), the soldiers entered the house and found four dead, including the two brothers and their bodyguard. There were reports that Hussein's 14-year-old son Mustapha was the fourth body found. Brigadier general Frank Helmick, the assistant commander of 101st Airborne, commented that all occupants of the house died during the gun battle before U.S. troops were able to enter.

On 23 July 2003, the American command stated that it had conclusively identified two of the dead men as Saddam Hussein's sons from dental records. Because many Iraqis were skeptical of news of the deaths, the U.S. Government released photos of the corpses and allowed Iraq's governing council to identify the bodies despite the U.S. objection to the publication of American corpses on Arab television. Afterwards, their bodies were reconstructed by morticians. For example, Qusay's beard was shaved and gashes from the battle were removed. They also announced that the informant, possibly the owner of the house, would receive the combined $30 million reward on the pair. Hussein was the ace of clubs in the coalition forces' most-wanted Iraqi playing cards. His father was the ace of spades and his brother was the ace of hearts.

Hussein's other two sons, Yahya Qusay and Yaqub Qusay, are presumed alive, but their whereabouts are unknown.



</doc>
<doc id="25263" url="https://en.wikipedia.org/wiki?curid=25263" title="Quatrain">
Quatrain

A quatrain is a type of stanza, or a complete poem, consisting of four lines.

Existing in a variety of forms, the quatrain appears in poems from the poetic traditions of various ancient civilizations including Ancient India, Ancient Greece, Ancient Rome, and China, and continues into the 21st century, where it is seen in works published in many languages. During Europe's Dark Ages, in the Middle East and especially Iran, polymath poets such as Omar Khayyam continued to popularize this form of poetry, also known as Ruba'i, well beyond their borders and time. Michel de Nostredame (Nostradamus) used the quatrain form to deliver his famous prophecies in the 16th century.

There are fifteen possible rhyme schemes, but the most traditional and common are: AAAA, ABAB, and ABBA.

<poem style="margin-left:3em">
The curfew tolls the knell of parting day,
The lowing herd wind slowly o'er the lea,
The plowman homeward plods his weary way,
And leaves the world to darkness and to me.
</poem>
<poem style="margin-left:3em">
Come, fill the Cup, and in the fire of Spring
Your Winter garment of Repentance fling:
To flutter—and the Bird is on the Wing.
</poem>




</doc>
<doc id="25264" url="https://en.wikipedia.org/wiki?curid=25264" title="Quantum chromodynamics">
Quantum chromodynamics

In theoretical physics, quantum chromodynamics (QCD) is the theory of the strong interaction between quarks and gluons, the fundamental particles that make up composite hadrons such as the proton, neutron and pion. QCD is a type of quantum field theory called a non-abelian gauge theory, with symmetry group SU(3). The QCD analog of electric charge is a property called "color". Gluons are the force carrier of the theory, like photons are for the electromagnetic force in quantum electrodynamics. The theory is an important part of the Standard Model of particle physics. A large body of experimental evidence for QCD has been gathered over the years.

QCD exhibits two main properties:


Physicist Murray Gell-Mann coined the word "quark" in its present sense. It originally comes from the phrase "Three quarks for Muster Mark" in "Finnegans Wake" by James Joyce. On June 27, 1978, Gell-Mann wrote a private letter to the editor of the "Oxford English Dictionary", in which he related that he had been influenced by Joyce's words: "The allusion to three quarks seemed perfect." (Originally, only three quarks had been discovered.)

The three kinds of charge in QCD (as opposed to one in quantum electrodynamics or QED) are usually referred to as "color charge" by loose analogy to the three kinds of color (red, green and blue) perceived by humans. Other than this nomenclature, the quantum parameter "color" is completely unrelated to the everyday, familiar phenomenon of color.

The force between quarks is known as the colour force (or color force ) or strong interaction, and is responsible for the strong nuclear force.

Since the theory of electric charge is dubbed "electrodynamics", the Greek word χρῶμα "chroma" "color" is applied to the theory of color charge, "chromodynamics".

With the invention of bubble chambers and spark chambers in the 1950s, experimental particle physics discovered a large and ever-growing number of particles called hadrons. It seemed that such a large number of particles could not all be fundamental. First, the particles were classified by charge and isospin by Eugene Wigner and Werner Heisenberg; then, in 1953–56, according to strangeness by Murray Gell-Mann and Kazuhiko Nishijima (see Gell-Mann–Nishijima formula). To gain greater insight, the hadrons were sorted into groups having similar properties and masses using the "eightfold way", invented in 1961 by Gell-Mann and Yuval Ne'eman. Gell-Mann and George Zweig, correcting an earlier approach of Shoichi Sakata, went on to propose in 1963 that the structure of the groups could be explained by the existence of three flavors of smaller particles inside the hadrons: the quarks.

Perhaps the first remark that quarks should possess an additional quantum number was made as a short footnote in the preprint of Boris Struminsky in connection with the Ω hyperon being composed of three strange quarks with parallel spins (this situation was peculiar, because since quarks are fermions, such a combination is forbidden by the Pauli exclusion principle): Boris Struminsky was a PhD student of Nikolay Bogolyubov. The problem considered in this preprint was suggested by Nikolay Bogolyubov, who advised Boris Struminsky in this research. In the beginning of 1965, Nikolay Bogolyubov, Boris Struminsky and Albert Tavkhelidze wrote a preprint with a more detailed discussion of the additional quark quantum degree of freedom. This work was also presented by Albert Tavkhelidze without obtaining consent of his collaborators for doing so at an international conference in Trieste (Italy), in May 1965.

A similar mysterious situation was with the Δ baryon; in the quark model, it is composed of three up quarks with parallel spins. In 1964–65, Greenberg and Han–Nambu independently resolved the problem by proposing that quarks possess an additional SU(3) gauge degree of freedom, later called color charge. Han and Nambu noted that quarks might interact via an octet of vector gauge bosons: the gluons.

Since free quark searches consistently failed to turn up any evidence for the new particles, and because an elementary particle back then was "defined" as a particle which could be separated and isolated, Gell-Mann often said that quarks were merely convenient mathematical constructs, not real particles. The meaning of this statement was usually clear in context: He meant quarks are confined, but he also was implying that the strong interactions could probably not be fully described by quantum field theory.

Richard Feynman argued that high energy experiments showed quarks are real particles: he called them "partons" (since they were parts of hadrons). By particles, Feynman meant objects which travel along paths, elementary particles in a field theory.

The difference between Feynman's and Gell-Mann's approaches reflected a deep split in the theoretical physics community. Feynman thought the quarks have a distribution of position or momentum, like any other particle, and he (correctly) believed that the diffusion of parton momentum explained diffractive scattering. Although Gell-Mann believed that certain quark charges could be localized, he was open to the possibility that the quarks themselves could not be localized because space and time break down. This was the more radical approach of S-matrix theory.

James Bjorken proposed that pointlike partons would imply certain relations in deep inelastic scattering of electrons and protons, which were verified in experiments at SLAC in 1969. This led physicists to abandon the S-matrix approach for the strong interactions.

In 1973 the concept of color as the source of a "strong field" was developed into the theory of QCD by physicists Harald Fritzsch and Heinrich Leutwyler, together with physicist Murray Gell-Mann. In particular, they employed the general field theory developed in 1954 by Chen Ning Yang and Robert Mills (see Yang–Mills theory), in which the carrier particles of a force can themselves radiate further carrier particles. (This is different from QED, where the photons that carry the electromagnetic force do not radiate further photons.)

The discovery of asymptotic freedom in the strong interactions by David Gross, David Politzer and Frank Wilczek allowed physicists to make precise predictions of the results of many high energy experiments using the quantum field theory technique of perturbation theory. Evidence of gluons was discovered in three-jet events at PETRA in 1979. These experiments became more and more precise, culminating in the verification of perturbative QCD at the level of a few percent at the LEP in CERN.

The other side of asymptotic freedom is confinement. Since the force between color charges does not decrease with distance, it is believed that quarks and gluons can never be liberated from hadrons. This aspect of the theory is verified within lattice QCD computations, but is not mathematically proven. One of the Millennium Prize Problems announced by the Clay Mathematics Institute requires a claimant to produce such a proof. Other aspects of non-perturbative QCD are the exploration of phases of quark matter, including the quark–gluon plasma.

The relation between the short-distance particle limit and the confining long-distance limit is one of the topics recently explored using string theory, the modern form of S-matrix theory.

Every field theory of particle physics is based on certain symmetries of nature whose existence is deduced from observations. These can be

QCD is a non-abelian gauge theory (or Yang–Mills theory) of the SU(3) gauge group obtained by taking the color charge to define a local symmetry.

Since the strong interaction does not discriminate between different flavors of quark, QCD has approximate flavor symmetry, which is broken by the differing masses of the quarks.

There are additional global symmetries whose definitions require the notion of chirality, discrimination between left and right-handed. If the spin of a particle has a positive projection on its direction of motion then it is called left-handed; otherwise, it is right-handed. Chirality and handedness are not the same, but become approximately equivalent at high energies.

As mentioned, "asymptotic freedom" means that at large energy – this corresponds also to "short distances" – there is practically no interaction between the particles. This is in contrast – more precisely one would say "dual"– to what one is used to, since usually one connects the absence of interactions with "large" distances. However, as already mentioned in the original paper of Franz Wegner, a solid state theorist who introduced 1971 simple gauge invariant lattice models, the high-temperature behaviour of the "original model", e.g. the strong decay of correlations at large distances, corresponds to the low-temperature behaviour of the (usually ordered!) "dual model", namely the asymptotic decay of non-trivial correlations, e.g. short-range deviations from almost perfect arrangements, for short distances. Here, in contrast to Wegner, we have only the dual model, which is that one described in this article.

The color group SU(3) corresponds to the local symmetry whose gauging gives rise to QCD. The electric charge labels a representation of the local symmetry group U(1) which is gauged to give QED: this is an abelian group. If one considers a version of QCD with "N" flavors of massless quarks, then there is a global (chiral) flavor symmetry group SU("N") × SU("N") × U(1) × U(1). The chiral symmetry is spontaneously broken by the QCD vacuum to the vector (L+R) SU("N") with the formation of a chiral condensate. The vector symmetry, U(1) corresponds to the baryon number of quarks and is an exact symmetry. The axial symmetry U(1) is exact in the classical theory, but broken in the quantum theory, an occurrence called an anomaly. Gluon field configurations called instantons are closely related to this anomaly.

There are two different types of SU(3) symmetry: there is the symmetry that acts on the different colors of quarks, and this is an exact gauge symmetry mediated by the gluons, and there is also a flavor symmetry which rotates different flavors of quarks to each other, or "flavor SU(3)". Flavor SU(3) is an approximate symmetry of the vacuum of QCD, and is not a fundamental symmetry at all. It is an accidental consequence of the small mass of the three lightest quarks.

In the QCD vacuum there are vacuum condensates of all the quarks whose mass is less than the QCD scale. This includes the up and down quarks, and to a lesser extent the strange quark, but not any of the others. The vacuum is symmetric under SU(2) isospin rotations of up and down, and to a lesser extent under rotations of up, down and strange, or full flavor group SU(3), and the observed particles make isospin and SU(3) multiplets.

The approximate flavor symmetries do have associated gauge bosons, observed particles like the rho and the omega, but these particles are nothing like the gluons and they are not massless. They are emergent gauge bosons in an approximate string description of QCD.

The dynamics of the quarks and gluons are controlled by the quantum chromodynamics Lagrangian. The gauge invariant QCD Lagrangian is

where formula_1 is the quark field, a dynamical function of spacetime, in the fundamental representation of the SU(3) gauge group, indexed by formula_2; formula_3 is the gauge covariant derivative; the γ are Dirac matrices connecting the spinor representation to the vector representation of the Lorentz group.

The symbol formula_4 represents the gauge invariant gluon field strength tensor, analogous to the electromagnetic field strength tensor, "F", in quantum electrodynamics. It is given by:

where formula_6 are the gluon fields, dynamical functions of spacetime, in the adjoint representation of the SU(3) gauge group, indexed by "a", "b"...; and "f" are the structure constants of SU(3). Note that the rules to move-up or pull-down the "a", "b", or "c" indices are "trivial", (+, ..., +), so that "f" = "f" = "f" whereas for the "μ" or "ν" indices one has the non-trivial "relativistic" rules corresponding to the metric signature (+ − − −).

The variables "m" and "g" correspond to the quark mass and coupling of the theory, respectively, which are subject to renormalization.

An important theoretical concept is the "Wilson loop" (named after Kenneth G. Wilson). In lattice QCD, the final term of the above Lagrangian is discretized via Wilson loops, and more generally the behavior of Wilson loops can distinguish confined and deconfined phases.

Quarks are massive spin- fermions which carry a color charge whose gauging is the content of QCD. Quarks are represented by Dirac fields in the fundamental representation 3 of the gauge group SU(3). They also carry electric charge (either − or +) and participate in weak interactions as part of weak isospin doublets. They carry global quantum numbers including the baryon number, which is for each quark, hypercharge and one of the flavor quantum numbers.

Gluons are spin-1 bosons which also carry color charges, since they lie in the adjoint representation 8 of SU(3). They have no electric charge, do not participate in the weak interactions, and have no flavor. They lie in the singlet representation 1 of all these symmetry groups.

Every quark has its own antiquark. The charge of each antiquark is exactly the opposite of the corresponding quark.

According to the rules of quantum field theory, and the associated Feynman diagrams, the above theory gives rise to three basic interactions: a quark may emit (or absorb) a gluon, a gluon may emit (or absorb) a gluon, and two gluons may directly interact. This contrasts with QED, in which only the first kind of interaction occurs, since photons have no charge. Diagrams involving Faddeev–Popov ghosts must be considered too (except in the unitarity gauge).

Detailed computations with the above-mentioned Lagrangian show that the effective potential between a quark and its anti-quark in a meson contains a term that increases in proportion to the distance between the quark and anti-quark (formula_7), which represents some kind of "stiffness" of the interaction between the particle and its anti-particle at large distances, similar to the entropic elasticity of a rubber band (see below). This leads to "confinement"  of the quarks to the interior of hadrons, i.e. mesons and nucleons, with typical radii "R", corresponding to former "Bag models" of the hadrons The order of magnitude of the "bag radius" is 1 fm (= 10 m). Moreover, the above-mentioned stiffness is quantitatively related to the so-called "area law" behaviour of the expectation value of the Wilson loop product "P" of the ordered coupling constants around a closed loop "W"; i.e. formula_8 is proportional to the "area" enclosed by the loop. For this behaviour the non-abelian behaviour of the gauge group is essential.

Further analysis of the content of the theory is complicated. Various techniques have been developed to work with QCD. Some of them are discussed briefly below.

This approach is based on asymptotic freedom, which allows perturbation theory to be used accurately in experiments performed at very high energies. Although limited in scope, this approach has resulted in the most precise tests of QCD to date.

Among non-perturbative approaches to QCD, the most well established one is lattice QCD. This approach uses a discrete set of spacetime points (called the lattice) to reduce the analytically intractable path integrals of the continuum theory to a very difficult numerical computation which is then carried out on supercomputers like the QCDOC which was constructed for precisely this purpose. While it is a slow and resource-intensive approach, it has wide applicability, giving insight into parts of the theory inaccessible by other means, in particular into the explicit forces acting between quarks and antiquarks in a meson. However, the numerical sign problem makes it difficult to use lattice methods to study QCD at high density and low temperature (e.g. nuclear matter or the interior of neutron stars).

A well-known approximation scheme, the expansion, starts from the idea that the number of colors is infinite, and makes a series of corrections to account for the fact that it is not. Until now, it has been the source of qualitative insight rather than a method for quantitative predictions. Modern variants include the AdS/CFT approach.

For specific problems effective theories may be written down which give qualitatively correct results in certain limits. In the best of cases, these may then be obtained as systematic expansions in some parameter of the QCD Lagrangian. One such effective field theory is chiral perturbation theory or ChiPT, which is the QCD effective theory at low energies. More precisely, it is a low energy expansion based on the spontaneous chiral symmetry breaking of QCD, which is an exact symmetry when quark masses are equal to zero, but for the u, d and s quark, which have small mass, it is still a good approximate symmetry. Depending on the number of quarks which are treated as light, one uses either SU(2) ChiPT or SU(3) ChiPT . Other effective theories are heavy quark effective theory (which expands around heavy quark mass near infinity), and soft-collinear effective theory (which expands around large ratios of energy scales). In addition to effective theories, models like the Nambu–Jona-Lasinio model and the chiral model are often used when discussing general features.

Based on an Operator product expansion one can derive sets of relations that connect different observables with each other.

In one of his recent works, Kei-Ichi Kondo derived as a low-energy limit of QCD, a theory linked to the Nambu–Jona-Lasinio model since it is basically a particular non-local version of the Polyakov–Nambu–Jona-Lasinio model. The later being in its local version, nothing but the Nambu–Jona-Lasinio model in which one has included the Polyakov loop effect, in order to describe a 'certain confinement'.

The Nambu–Jona-Lasinio model in itself is, among many other things, used because it is a 'relatively simple' model of chiral symmetry breaking, phenomenon present up to certain conditions (Chiral limit i.e. massless fermions) in QCD itself.
In this model, however, there is no confinement. In particular, the energy of an isolated quark in the physical vacuum turns out well defined and finite.

The notion of quark flavors was prompted by the necessity of explaining the properties of hadrons during the development of the quark model. The notion of color was necessitated by the puzzle of the . This has been dealt with in the section on the history of QCD.

The first evidence for quarks as real constituent elements of hadrons was obtained in deep inelastic scattering experiments at SLAC. The first evidence for gluons came in three-jet events at PETRA.

Several good quantitative tests of perturbative QCD exist:

Quantitative tests of non-perturbative QCD are fewer, because the predictions are harder to make. The best is probably the running of the QCD coupling as probed through lattice computations of heavy-quarkonium spectra. There is a recent claim about the mass of the heavy meson B . Other non-perturbative tests are currently at the level of 5% at best. Continuing work on masses and form factors of hadrons and their weak matrix elements are promising candidates for future quantitative tests. The whole subject of quark matter and the quark–gluon plasma is a non-perturbative test bed for QCD which still remains to be properly exploited.

One qualitative prediction of QCD is that there exist composite particles made solely of gluons called glueballs that have not yet been definitively observed experimentally. A definitive observation of a glueball with the properties predicted by QCD would strongly confirm the theory. In principle, if glueballs could be definitively ruled out, this would be a serious experimental blow to QCD. But, as of 2013, scientists are unable to confirm or deny the existence of glueballs definitively, despite the fact that particle accelerators have sufficient energy to generate them.

There are unexpected cross-relations to solid state physics. For example, the notion of gauge invariance forms the basis of the well-known Mattis spin glasses, which are systems with the usual spin degrees of freedom formula_9 for "i" =1...,N, with the special fixed "random" couplings formula_10 Here the ε and ε quantities can independently and "randomly" take the values ±1, which corresponds to a most-simple gauge transformation formula_11 This means that thermodynamic expectation values of measurable quantities, e.g. of the energy formula_12 are invariant.

However, here the "coupling degrees of freedom" formula_13, which in the QCD correspond to the "gluons", are "frozen" to fixed values (quenching). In contrast, in the QCD they "fluctuate" (annealing), and through the large number of gauge degrees of freedom the entropy plays an important role (see below).

For positive "J" the thermodynamics of the Mattis spin glass corresponds in fact simply to a "ferromagnet in disguise", just because these systems have no "frustration" at all. This term is a basic measure in spin glass theory. Quantitatively it is identical with the loop product formula_14 along a closed loop "W". However, for a Mattis spin glass – in contrast to "genuine" spin glasses – the quantity "P" never becomes negative.

The basic notion "frustration" of the spin-glass is actually similar to the Wilson loop quantity of the QCD. The only difference is again that in the QCD one is dealing with SU(3) matrices, and that one is dealing with a "fluctuating" quantity. Energetically, perfect absence of frustration should be non-favorable and atypical for a spin glass, which means that one should add the loop product to the Hamiltonian, by some kind of term representing a "punishment". In the QCD the Wilson loop is essential for the Lagrangian rightaway.

The relation between the QCD and "disordered magnetic systems" (the spin glasses belong to them) were additionally stressed in a paper by Fradkin, Huberman and Shenker, which also stresses the notion of duality.

A further analogy consists in the already mentioned similarity to polymer physics, where, analogously to Wilson Loops, so-called "entangled nets" appear, which are important for the formation of the entropy-elasticity (force proportional to the length) of a rubber band. The non-abelian character of the SU(3) corresponds thereby to the non-trivial "chemical links", which glue different loop segments together, and "asymptotic freedom" means in the polymer analogy simply the fact that in the short-wave limit, i.e. for formula_15 (where "R" is a characteristic correlation length for the glued loops, corresponding to the above-mentioned "bag radius", while λ is the wavelength of an excitation) any non-trivial correlation vanishes totally, as if the system had crystallized.

There is also a correspondence between confinement in QCD – the fact that the color field is only different from zero in the interior of hadrons – and the behaviour of the usual magnetic field in the theory of type-II superconductors: there the magnetism is confined to the interior of the Abrikosov flux-line lattice,   i.e., the London penetration depth "λ" of that theory is analogous to the confinement radius "R" of quantum chromodynamics. Mathematically, this correspondendence is supported by the second term, formula_16 on the r.h.s. of the Lagrangian.





</doc>
<doc id="25265" url="https://en.wikipedia.org/wiki?curid=25265" title="Queue (abstract data type)">
Queue (abstract data type)

In computer science, a queue is a collection of entities that are maintained in a sequence and can be modified by the addition of entities at one end of the sequence and removal from the other end of the sequence. By convention, the end of the sequence at which elements are added is called the back or rear of the queue and the end at which elements are removed is called the head or front of the queue, analogously to the words used when people line up to wait for goods or services.

The operation of adding an element to the rear of the queue is known as "enqueue", and the operation of removing an element from the front is known as "dequeue". Other operations may also be allowed, often including a "peek" or "front" operation that returns the value of the next element to be dequeued without dequeuing it.

The operations of a queue make it a first-in-first-out (FIFO) data structure. In a FIFO data structure, the first element added to the queue will be the first one to be removed. This is equivalent to the requirement that once a new element is added, all elements that were added before have to be removed before the new element can be removed. A queue is an example of a linear data structure, or more abstractly a sequential collection.
Queues are common in computer programs, where they are implemented as data structures coupled with access routines, as an abstract data structure or in object-oriented languages as classes. Common implementations are circular buffers and linked lists.

Queues provide services in computer science, transport, and operations research where various entities such as data, objects, persons, or events are stored and held to be processed later. In these contexts, the queue performs the function of a buffer.
Another usage of queues is in the implementation of breadth-first search.

Theoretically, one characteristic of a queue is that it does not have a specific capacity. Regardless of how many elements are already contained, a new element can always be added. It can also be empty, at which point removing an element will be impossible until a new element has been added again.

Fixed length arrays are limited in capacity, but it is not true that items need to be copied towards the head of the queue. The simple trick of turning the array into a closed circle and letting the head and tail drift around endlessly in that circle makes it unnecessary to ever move items stored in the array. If n is the size of the array, then computing indices modulo n will turn the array into a circle. This is still the conceptually simplest way to construct a queue in a high level language, but it does admittedly slow things down a little, because the array indices must be compared to zero and the array size, which is comparable to the time taken to check whether an array index is out of bounds, which some languages do, but this will certainly be the method of choice for a quick and dirty implementation, or for any high level language that does not have pointer syntax. The array size must be declared ahead of time, but some implementations simply double the declared array size when overflow occurs. Most modern languages with objects or pointers can implement or come with libraries for dynamic lists. Such data structures may have not specified fixed capacity limit besides memory constraints. Queue "overflow" results from trying to add an element onto a full queue and queue "underflow" happens when trying to remove an element from an empty queue.

A "bounded queue" is a queue limited to a fixed number of items.

There are several efficient implementations of FIFO queues. An efficient implementation is one that can perform the operations—enqueuing and dequeuing—in O(1) time.

Queues may be implemented as a separate data type, or may be considered a special case of a double-ended queue (deque) and not implemented separately. For example, Perl and Ruby allow pushing and popping an array from both ends, so one can use push and unshift functions to enqueue and dequeue a list (or, in reverse, one can use shift and pop), although in some cases these operations are not efficient.

C++'s Standard Template Library provides a "codice_1" templated class which is restricted to only push/pop operations. Since J2SE5.0, Java's library contains a interface that specifies queue operations; implementing classes include and (since J2SE 1.6) . PHP has an SplQueue class and third party libraries like beanstalk'd and Gearman.

A simple queue implemented in Ruby:

Queues can also be implemented as a purely functional data structure. Two versions of the implementation exist. The first one, called real-time queue, presented below, allows the queue to be persistent with operations in O(1) worst-case time, but requires lazy lists with memoization. The second one, with no lazy lists nor memoization is presented at the end of the sections. Its amortized time is formula_1 if the persistency is not used; but its worst-time complexity is formula_2 where "n" is the number of elements in the queue.

Let us recall that, for formula_3 a list, formula_4 denotes its length, that "NIL" represents an empty list and formula_5 represents the list whose head is "h" and whose tail is "t".

The data structure used to implement our queues consists of three linked lists formula_6 where "f" is the front of the queue, "r" is the rear of the queue in reverse order. The invariant of the structure is that "s" is the rear of "f" without its formula_7 first elements, that is formula_8. The tail of the queue formula_9 is then almost formula_6 and
inserting an element "x" to formula_6 is almost formula_12. It is said almost, because in both of those results, formula_13. An auxiliary function formula_14 must then be called for the invariant to be satisfied. Two cases must be considered, depending on whether formula_15 is the empty list, in which case formula_16, or not. The formal definition is formula_17 and formula_18 where formula_19 is "f" followed by "r" reversed.

Let us call formula_20 the function which returns "f" followed by "r" reversed. Let us furthermore assume that formula_16, since it is the case when this function is called. More precisely, we define a lazy function formula_22 which takes as input three list such that formula_16, and return the concatenation of "f", of "r" reversed and of "a". Then formula_24.
The inductive definition of rotate is formula_25 and formula_26. Its running time is formula_27, but, since lazy evaluation is used, the computation is delayed until the results is forced by the computation.

The list "s" in the data structure has two purposes. This list serves as a counter for formula_28, indeed, formula_29 if and only if "s" is the empty list. This counter allows us to ensure that the rear is never longer than the front list. Furthermore, using "s", which is a tail of "f", forces the computation of a part of the (lazy) list "f" during each "tail" and "insert" operation. Therefore, when formula_29, the list "f" is totally forced. If it was not the case, the internal representation of "f" could be some append of append of... of append, and forcing would not be a constant time operation anymore.

Note that, without the lazy part of the implementation, the real-time queue would be a non-persistent implementation of queue in formula_1 amortized time. In this case, the list "s" can be replaced by the integer formula_28, and the reverse function would be called when formula_15 is 0.





</doc>
<doc id="25266" url="https://en.wikipedia.org/wiki?curid=25266" title="Quake (video game)">
Quake (video game)

Quake is a first-person shooter video game developed by id Software and published by GT Interactive in 1996. It is the first game in the "Quake" series. In the game, players must find their way through various maze-like, medieval environments while battling a variety of monsters using an array of weaponry. The overall atmosphere is dark and gritty, with lots of stone textures and a rusty, capitalized font.

The successor to id Software's "Doom" series, "Quake" built upon the technology and gameplay of its predecessor. Unlike the "Doom" engine before it, the "Quake" engine offered full real-time 3D rendering and had early support for 3D acceleration through OpenGL. After "Doom" helped to popularize multiplayer deathmatches in 1993, "Quake" added various multiplayer options. Online multiplayer became increasingly common, with the QuakeWorld update and software such as QuakeSpy making the process of finding and playing against others on the Internet easier and more reliable.

"Quake" features music composed by Trent Reznor and his band, Nine Inch Nails..

In "Quake" single-player mode, players explore and navigate to the exit of each Gothic and dark level, facing monsters and finding secret areas along the way. Usually there are switches to activate or keys to collect in order to open doors before the exit can be reached. Reaching the exit takes the player to the next level. Before accessing an episode, there is a set of three pathways with easy, medium, and hard skill levels. The fourth skill level, "Nightmare", was "so bad that it was hidden, so people won't wander in by accident"; the player must drop through water before the episode four entrance and go into a secret passage to access it.

"Quake" single-player campaign is organized into four individual episodes with seven to eight levels in each (including one secret level per episode, one of which is a "low gravity" level that challenges the player's abilities in a different way). As items are collected, they are carried to the next level, each usually more challenging than the last. If the player's character dies, he must restart at the beginning of the level. The game may be saved at any time. Upon completing an episode, the player is returned to the hub "START" level, where another episode can be chosen. Each episode starts the player from scratch, without any previously collected items. Episode one (which formed the shareware or downloadable demo version of "Quake") has the most traditional ideology of a boss in the last level. The ultimate objective at the end of each episode is to recover a magic rune. After all of the runes are collected, the floor of the hub level opens up to reveal an entrance to the "END" level which contains the final boss of the game.

In multiplayer mode, players on several computers connect to a server (which may be a dedicated machine or on one of the player's computers), where they can either play the single-player campaign together in co-op (cooperative) mode, or play against each other in multiplayer. When players die in multiplayer mode, they can immediately respawn, but will lose any items that were collected. Similarly, items that have been picked up previously respawn after some time, and may be picked up again. The most popular multiplayer modes are all forms of deathmatch. Deathmatch modes typically consist of either "free-for-all" (no organization or teams involved), one-on-one "duels", or organized "teamplay" with two or more players per team (or clan). Teamplay is also frequently played with one or another mod. Monsters are not normally present in teamplay, as they serve no purpose other than to get in the way and reveal the positions of the players.

The gameplay in "Quake" was considered unique for its time because of the different ways the player can maneuver through the game. For example: bunny hopping or strafe jumping can be used to move faster than normal, while rocket jumping enables the player to reach otherwise-inaccessible areas at the cost of some self-damage. The player can start and stop moving suddenly, jump unnaturally high, and change direction while moving through the air. Many of these non-realistic behaviors contribute to "Quake"s appeal. Multiplayer "Quake" was one of the first games singled out as a form of electronic sport. A notable participant was Dennis Fong who won John Carmack's Ferrari 328 at the Microsoft-sponsored Red Annihilation tournament in 1997.

In the single-player game, the player takes the role of the protagonist known as Ranger (voiced by Trent Reznor) who was sent into a portal in order to stop an enemy code-named "Quake". The government had been experimenting with teleportation technology and developed a working prototype called a "Slipgate"; the mysterious Quake compromised the Slipgate by connecting it with its own teleportation system, using it to send death squads to the "Human" dimension in order to test the martial capabilities of humanity.

The sole surviving protagonist in "Operation Counterstrike" is Ranger, who must advance, starting each of the four episodes from an overrun human military base, before fighting his way into other dimensions, reaching them via the Slipgate or their otherworld equivalent. After passing through the Slipgate, Ranger's main objective is to collect four magic runes from four dimensions of Quake; these are the key to stopping the enemy later discovered as Shub-Niggurath and ending the invasion of Earth.

The single-player campaign consists of 30 separate levels, or "maps", divided into four episodes (with a total of 26 regular maps and four secret ones), as well as a hub level to select a difficulty setting and episode, and the game's final boss level. Each episode represents individual dimensions that the player can access through magical portals (as opposed to the technological Slipgate) that are discovered over the course of the game. The various realms consist of a number of gothic, medieval, and lava-filled caves and dungeons, with a recurring theme of hellish and satanic imagery reminiscent of "Doom" (such as pentagrams and images of demons on the walls). The game's setting is inspired by several dark fantasy influences, most notably that of H. P. Lovecraft. Dimensional Shamblers appear as enemies, the "Spawn" enemies are called "Formless Spawn of Tsathoggua" in the manual, the boss of the first episode is named Chthon, and the main villain is named Shub-Niggurath (though actually resembling a Dark Young). Some levels have Lovecraftian names, such as the Vaults of Zin and The Nameless City. In addition, six levels exclusively designed for multiplayer deathmatch are also included. Originally, the game was supposed to include more Lovecraftian bosses, but this concept was scrapped due to time constraints.

A preview included with id's very first release, 1990's "Commander Keen", advertised a game entitled "The Fight for Justice" as a follow-up to the "Commander Keen" trilogy. It would feature a character named Quake, "the strongest, most dangerous person on the continent", armed with thunderbolts and a "Ring of Regeneration". Conceived as a VGA full-color side-scrolling role-playing game, "The Fight for Justice" was never released.

Lead designer and director John Romero later conceived of "Quake" as an action game taking place in a fully 3D world, inspired by Sega AM2's 3D fighting game "Virtua Fighter". "Quake" was also intended to feature "Virtua Fighter" influenced third-person melee combat. However, id Software considered it to be risky. Because the project was taking too long, the third-person melee was eventually dropped. This led to creative differences between Romero and id Software, and eventually his departure from the company after "Quake" was released. Even though he led the project, Romero did not receive any money from "Quake". In 2000, Romero released "Daikatana", the game that he envisioned what "Quake" was supposed to be and despite its shaky development and considered to be one of the worst games of all time, he said "Daikatana" was "more fun to make than "Quake"" due to the lack of creative interference.

"Quake" was given as a title to the game that id Software was working on shortly after the release of "". The earliest information released described "Quake" as focusing on a Thor-like character who wields a giant hammer, and is able to knock away enemies by throwing the hammer (complete with real-time inverse kinematics). Initially, the levels were supposed to be designed in an Aztec style, but the choice was dropped some months into the project. Early screenshots then showed medieval environments and dragons. The plan was for the game to have more RPG-style elements. However, work was very slow on the engine, since John Carmack, the main programmer of "Quake", was not only developing a fully 3D engine, but also a TCP/IP networking model (Carmack later said that he should have done two separate projects which developed those things). Working with a game engine that was still in development presented difficulties for the designers.

Eventually, the whole id Software team began to think that the original concept may not have been as wise a choice as they first believed. Thus, the final game was very stripped down from its original intentions, and instead featured gameplay similar to "Doom" and its sequel, although the levels and enemies were closer to medieval RPG style rather than science-fiction. In a December 1, 1994 post to an online bulletin board, John Romero wrote, "Okay, people. It seems that everyone is speculating on whether Quake is going to be a slow, RPG-style light-action game. Wrong! What does id do best and dominate at? Can you say "action"? I knew you could. Quake will be constant, hectic action throughout – probably more so than Doom."

"Quake" was programmed by John Carmack, Michael Abrash, and John Cash. The levels and scenarios were designed by American McGee, Sandy Petersen, John Romero, and Tim Willits, and the graphics were designed by Adrian Carmack, Kevin Cloud and Paul Steed. Cloud created the monster and player graphics using Alias.

The game engine developed for "Quake", the "Quake" engine, popularized several major advances in the first-person shooter genre: polygonal models instead of prerendered sprites; full 3D level design instead of a 2.5D map; prerendered lightmaps; and allowing end users to partially program the game (in this case with QuakeC), which popularized fan-created modifications (mods).

Before the release of the full game or the shareware version of "Quake", id Software released "QTest" on February 24, 1996. It was described as a technology demo and was limited to three multiplayer maps. There was no single-player support and some of the gameplay and graphics were unfinished or different from their final versions. "QTest" gave gamers their first peek into the filesystem and modifiability of the "Quake" engine, and many entity mods (that placed monsters in the otherwise empty multiplayer maps) and custom player skins began appearing online before the full game was even released.

Initially, the game was designed so that when the player ran out of ammunition, the player character would hit enemies with a gun-butt. Shortly before release this was replaced with an axe.

"Quake"s music and sound design was done by Trent Reznor and Nine Inch Nails, using ambient soundscapes and synthesized drones to create atmospheric tracks. In an interview, Reznor remarked that the "Quake" soundtrack "is not music, it's textures and ambiences and whirling machine noises and stuff. We tried to make the most sinister, depressive, scary, frightening kind of thing... It's been fun." The game also has some ammo boxes decorated with the Nine Inch Nails logo.

Digital re-releases lack the CD soundtrack that came with the original shareware release. Players can download the soundtrack online to recover it.

The first port to be completed was the Linux port by id Software employee Dave D. Taylor on July 5, 1996, followed by a SPARC Solaris port later that year also by Taylor. The first commercially released port was the port to Mac OS, done by MacSoft and Lion Entertainment, Inc. (the latter company ceased to exist just prior to the port's release, leading to MacSoft's involvement) in late August 1997. ClickBOOM announced version for Amiga-computers in 1998. Finally in 1999, a retail version of the Linux port was distributed by Macmillan Digital Publishing USA in a bundle with the three add-ons as "Quake: The Offering".

"Quake" was also ported to home console systems. On December 2, 1997, the game was released for the Sega Saturn. Initially GT Interactive was to publish this version itself, but it later cancelled the release and the Saturn rights were picked up by Sega. Sega then took the project away from the original development team, who had been encountering difficulties getting the port to run at a decent frame rate, and assigned it to Lobotomy Software. The Sega Saturn port used Lobotomy Software's own 3D game engine, "SlaveDriver" (the same game engine that powered the Sega Saturn versions of "PowerSlave" and "Duke Nukem 3D"), instead of the original "Quake" engine. It is the only version of "Quake" that is rated "T" for Teen instead of "M" for Mature.

"Quake" had also been ported to the Sony PlayStation by Lobotomy Software, but the port was cancelled due to difficulties in finding a publisher. A port of "Quake" for the Atari Jaguar was also advertized as 30% complete in a May 1996 issue of "Ultimate Future Games" magazine, but it was never released. Another port of "Quake" was also slated for Panasonic M2 but never occurred due to the cancellation of the system.

On March 24, 1998, the game was released for the Nintendo 64 by Midway Games. This version was developed by the same programming team that worked on "Doom 64", at id Software's request.

Both console ports required compromises because of the limited CPU power and ROM storage space for levels. For example, the levels were rebuilt in the Saturn version in order to simplify the architecture, thereby reducing demands on the CPU. The Sega Saturn version includes 28 of the 32 single-player levels from the original PC version of the game, though the secret levels, Ziggurat Vertigo (E1M8), The Underearth (E2M7), The Haunted Halls (E3M7), and The Nameless City (E4M8), were removed. Instead, it has four exclusive secret levels: Purgatorium, Hell's Aerie, The Coliseum, and Watery Grave. It also contains an exclusive unlockable, "Dank & Scuz", which is a story set in the Quake milieu and presented in the form of a slide show with voice acting. There are no multiplayer modes in the Sega Saturn version; as a result of this, all of the deathmatch maps from the PC version were removed from the Sega Saturn port. The Nintendo 64 version includes 25 single-player levels from the PC version, though it is missing The Grisly Grotto (E1M4), The Installation (E2M1), The Ebon Fortress (E2M4), The Wind Tunnels (E3M5), The Sewage System (E4M1), and Hell's Atrium (E4M5) levels. It also does not use the hub "START" map where the player chooses a difficulty level and an episode; the difficulty level is chosen from a menu when starting the game, and all of the levels are played in sequential order from The Slipgate Complex (E1M1) to Shub Niggurath's Pit (END). The Nintendo 64 version, while lacking the cooperative multiplayer mode, includes two player deathmatch. All six of the deathmatch maps from the PC version are in the Nintendo 64 port, and an exclusive deathmatch level, The Court of Death, is also included.

Two ports of "Quake" for the Nintendo DS exist, "QuakeDS" and "CQuake". Both run well, however, multiplayer does not work on "QuakeDS". Since the source code for "Quake" was released, a number of unofficial ports have been made available for PDAs and mobile phones, such as PocketQuake, as well as versions for the Symbian S60 series of mobile phones and Android mobile phones. The Rockbox project also distributes a version of "Quake" that runs on some MP3 players.

In 2005, id Software signed a deal with publisher Pulse Interactive to release a version of "Quake" for mobile phones. The game was engineered by Californian company Bear Naked Productions. Initially due to be released on only two mobile phones, the Samsung Nexus (for which it was to be an embedded game) and the LG VX360. "Quake mobile" was reviewed by GameSpot on the Samsung Nexus and they cited its US release as October 2005; they also gave it a "Best Mobile Game" in their E3 2005 Editor's Choice Awards. "It is unclear as to whether the game actually did ship with the Samsung Nexus. The game is only available for the DELL x50v and x51v both of which are PDAs not mobile phones. "Quake Mobile" does not feature the Nine Inch Nails soundtrack due to space constraints. "Quake Mobile" runs the most recent version of GL Quake (Quake v.1.09 GL 1.00) at 800x600 resolution and 25 fps. The most recent version of "Quake Mobile" is v.1.20 which has stylus support. There was an earlier version v.1.19 which lacked stylus support. The two "Quake" expansion packs, "Scourge of Armagon" and "Dissolution of Eternity", are also available for "Quake Mobile".

A Flash-based version of the game by Michael Rennie runs "Quake" at full speed in any Flash-enabled web browser. Based on the shareware version of the game, it includes only the first episode and is available for free on the web.

"Quake" can be heavily modified by altering the graphics, audio, or scripting in QuakeC, and has been the focus of many fan created "mods". The first mods were small gameplay fixes and patches initiated by the community, usually enhancements to weapons or gameplay with new enemies. Later mods were more ambitious and resulted in "Quake" fans creating versions of the game that were drastically different from id Software's original release.

The first major "Quake" mod was "Team Fortress". This mod consists of Capture the Flag gameplay with a class system for the players. Players choose a class, which creates various restrictions on weapons and armor types available to that player, and also grants special abilities. For example, the bread-and-butter "Soldier" class has medium armor, medium speed, and a well-rounded selection of weapons and grenades, while the "Scout" class is lightly armored, very fast, has a scanner that detects nearby enemies, but has very weak offensive weapons. One of the other differences with CTF is the fact that the flag is not returned automatically when a player drops it: running over one's flag in "Threewave CTF" would return the flag to the base, and in "TF" the flag remains in the same spot for preconfigured time and it has to be defended on remote locations. This caused a shift in defensive tactics compared to "Threewave CTF". "Team Fortress" maintained its standing as the most-played online "Quake" modification for many years.

Another popular mod was "Threewave Capture the Flag" (CTF), primarily authored by Dave 'Zoid' Kirsch. "Threewave CTF" is a partial conversion consisting of new levels, a new weapon (a grappling hook), power-ups, new textures, and new gameplay rules. Typically, two teams (red and blue) would compete in a game of Capture the flag, though a few maps with up to four teams (red, blue, green, and yellow) were created. Capture the Flag soon became a standard game mode included in most popular multiplayer games released after "Quake". "Rocket Arena" provides the ability for players to face each other in small, open arenas with changes in the gameplay rules so that item collection and detailed level knowledge are no longer factors. A series of short rounds, with the surviving player in each round gaining a point, instead tests the player's aiming and dodging skills and reflexes. "Clan Arena" is a further modification that provides team play using "Rocket Arena" rules. One mod category, "bots", was introduced to provide surrogate players in multiplayer mode.

"Arcane Dimensions" is a singleplayer mod. It's a partial conversion with breakable objects and walls, enhanced particle system, numerous visual improvements and new enemies and weapons. The level design is much more complex in terms of geometry and gameplay than in the original game.

There are a large number of custom levels that have been made by users and fans of "Quake". , new maps are still being made, over twenty years since the game's release. Custom maps are new maps that are playable by loading them into the original game. Custom levels of various gameplay types have been made, but most are in the single-player and deathmatch genres. More than 1500 single-player and a similar number of deathmatch maps have been made for "Quake".

According to David Kushner in "Masters of Doom", id Software released a retail shareware version of "Quake" before the game's full retail distribution by GT Interactive. These shareware copies could be converted into complete versions through passwords purchased via phone. However, Kushner wrote that "gamers wasted no time hacking the shareware to unlock the full version of the game for free." This problem, combined with the scale of the operation, led id Software to cancel the plan. As a result, the company was left with 150,000 unsold shareware copies in storage. The venture damaged "Quake"s initial sales and caused its retail push by GT Interactive to miss the holiday shopping season. Following the game's full release, Kushner remarked that its early "sales were good — with 250,000 units shipped — but not a phenomenon like "Doom II"."

In the United States, "Quake" placed sixth on PC Data's monthly computer game sales charts for November and December 1996. Its shareware edition was the sixth-best-selling computer game of 1996 overall, while its retail SKU claimed 20th place. It remained in PC Data's monthly top 10 from January to April 1997, but was absent by May. During its first 12 months, "Quake" sold 373,000 retail copies and earned $18 million in the United States, according to PC Data. Its final retail sales for 1997 were 273,936 copies, which made it the country's 16th-highest computer game seller for the year.

Sales of "Quake" reached 550,000 units in the United States alone by December 1999. Worldwide, it sold 1.1 million units by that date.

"Quake" was critically acclaimed on the PC. Aggregating review websites GameRankings and Metacritic gave the original PC version 93% and 94/100, and the Nintendo 64 port 76% and 74/100. A "Next Generation" critic lauded the game's realistic 3D physics and genuinely unnerving sound effects. "GamePro" said "Quake" had been over-hyped but is excellent nonetheless, particularly its usage of its advanced 3D engine. The review also praised the sound effects, atmospheric music, and graphics, though it criticized that the polygons used to construct the enemies are too obvious at close range.

Less than a month after "Quake" was released (and a month before they actually reviewed the game), "Next Generation" listed it as number 9 on their "Top 100 Games of All Time", saying that it is similar to "Doom" but supports a maximum of eight players instead of four. In 1996, "Computer Gaming World" declared "Quake" the 36th-best computer game ever released, and listed "telefragged" as #1 on its list of "the 15 best ways to die in computer gaming". In 1997, the Game Developers Choice Awards gave Quake three spotlight awards for Best Sound Effects, Best Music or Soundtrack and Best On-Line/Internet Game.

"Entertainment Weekly" gave the game a B+ and wrote that "an extended bit of subterranean mayhem that offers three major improvements over its immediate predecessor ["Doom"]."

"Next Generation" reviewed the Macintosh version of the game, rating it four stars out of five, and stated that "Though replay value is limited by the lack of interactive environments or even the semblance of a plot, there's no doubt that "Quake" and its engine are something powerful and addictive."

"Next Generation" reviewed the Saturn version of the game, rating it three stars out of five, and stated that ""Quake" for Saturn is simply a latecoming showpiece for the system's power."

"Next Generation" reviewed the Nintendo 64 version of the game, rating it three stars out of five, and stated that "As a whole, "Quake 64" doesn't live up to the experience offered by the high-end, 3D-accelerated PC version; it is, however, an entertaining gaming experience that is worthy of a close look and a nice addition to the blossoming number of first-person shooters for Nintendo 64."

"Next Generation" reviewed the arcade version of the game, rating it three stars out of five, and stated that "For those who don't have LAN or internet capabilities, check out arcade "Quake". It's a blast."

In 1998, "PC Gamer" declared it the 28th-best computer game ever released, and the editors called it "one of the most addictive, adaptable, and pulse-pounding 3D shooters ever created".

As an example of the dedication that "Quake" has inspired in its fan community, a group of expert players recorded speedrun demos (replayable recordings of the player's movement) of "Quake" levels completed in record time on the "Nightmare" skill level. The footage was edited into a continuous 19 minutes, 49 seconds demo called "Quake done Quick" and released on June 10, 1997. Owners of "Quake" could replay this demo in the game engine, watching the run unfold as if they were playing it themselves.

This involved a number of players recording run-throughs of individual levels, using every trick and shortcut they could discover in order to minimize the time it took to complete, usually to a degree that even the original level designers found difficult to comprehend, and in a manner that often bypassed large areas of the level. Stitching a series of the fastest runs together into a coherent whole created a demonstration of the entire game. "Recamming" is also used with speedruns in order to make the experience more movie-like, with arbitrary control of camera angles, editing, and sound that can be applied with editing software after the runs are first recorded. However, the fastest possible time for a given level will not necessarily result in the fastest time used to contribute to "running" the entire game. One example is acquiring the grenade launcher in an early level, an act that slows down the time for that level over the best possible, but speeds up the overall game time by allowing the runner to bypass a big area in a later level that they could not otherwise do.

A second attempt, "Quake done Quicker", reduced the completion time to 16 minutes, 35 seconds (a reduction of 3 minutes, 14 seconds). "Quake done Quicker" was released on September 13, 1997. One of the levels included was the result of an online competition to see who could get the fastest time. The culmination of this process of improvement was "Quake done Quick with a Vengeance". Released three years to the day after "Quake done Quicker", this pared down the time taken to complete all four episodes, on Nightmare (hardest) difficulty, to 12 minutes, 23 seconds (a further reduction of 4 minutes, 12 seconds), partly by using techniques that had formerly been shunned in such films as being less aesthetically pleasing. This run was recorded as an in-game demo, but interest was such that an .avi video clip was created to allow those without the game to see the run.

Most full-game speedruns are a collaborative effort by a number of runners (though some have been done by single runners on their own). Although each particular level is credited to one runner, the ideas and techniques used are iterative and collaborative in nature, with each runner picking up tips and ideas from the others, so that speeds keep improving beyond what was thought possible as the runs are further optimized and new tricks or routes are discovered. Further time improvements of the continuous whole game run were achieved into the 21st century. In addition, many thousands of individual level runs are kept at Speed Demos Archive's "Quake" section, including many on custom maps. Speedrunning is a counterpart to multiplayer modes in making "Quake" one of the first games promoted as a virtual sport.

The source code of the "Quake" and "QuakeWorld" engines was licensed under the GPL on December 21, 1999. The id Software maps, objects, textures, sounds, and other creative works remain under their original proprietary license. The shareware distribution of "Quake" is still freely redistributable and usable with the GPLed engine code. One must purchase a copy of "Quake" in order to receive the registered version of the game which includes more single-player episodes and the deathmatch maps. Based on the success of the first "Quake" game, and later published "Quake II" and "Quake III Arena"; "Quake 4" was released in October 2005, developed by Raven Software using the "Doom 3" engine.

"Quake" was the game primarily responsible for the emergence of the machinima artform of films made in game engines, thanks to edited "Quake" demos such as "Ranger Gone Bad" and "Blahbalicious", the in-game film "The Devil's Covenant", and the in-game-rendered, four-hour epic film "The Seal of Nehahra". On June 22, 2006, it had been 10 years since the original uploading of the game to cdrom.com archives. Many Internet forums had topics about it, and it was a front-page story on Slashdot. On October 11, 2006, John Romero released the original map files for all of the levels in "Quake" under the GPL.

"Quake" has four sequels: "Quake II", "Quake III Arena", "Quake 4", and "". In 2002, a version of "Quake" was produced for mobile phones. A copy of "Quake" was also released as a compilation in 2001, labeled "Ultimate Quake", which included the original "Quake", "Quake II", and "Quake III Arena" which was published by Activision. In 2008, "Quake" was honored at the 59th Annual Technology & Engineering Emmy Awards for advancing the art form of user modifiable games. John Carmack accepted the award. Years after its original release, "Quake" is still regarded by many critics as one of the greatest and most influential games ever made.

There were two official expansion packs released for "Quake". The expansion packs pick up where the first game left off, include all of the same weapons, power-ups, monsters, and gothic atmosphere/architecture, and continue/finish the story of the first game and its protagonist. An unofficial third expansion pack, "Abyss of Pandemonium", was developed by the Impel Development Team, published by Perfect Publishing, and released on April 14, 1998; an updated version, version 2.0, titled "Abyss of Pandemonium – The Final Mission" was released as freeware. An authorized expansion pack, "Q!ZONE" was developed and published by WizardWorks, and released in 1996. In honor of "Quake"'s 20th anniversary, MachineGames, an internal development studio of ZeniMax Media, who are the current owners of the "Quake" IP, released online a new expansion pack for free, called "Episode 5: Dimension of the Past".

"Quake Mission Pack No. 1: Scourge of Armagon" was the first official mission pack, released on March 5, 1997. Developed by Hipnotic Interactive, it features three episodes divided into seventeen new single-player levels (three of which are secret), a new multiplayer level, a new soundtrack composed by Jeehun Hwang, and gameplay features not originally present in "Quake", including rotating structures and breakable walls. Unlike the main "Quake" game and Mission Pack No. 2, "Scourge" does away with the episode hub, requiring the three episodes to be played sequentially. The three new enemies include Centroids, large cybernetic scorpions with nailguns; Gremlins, small goblins that can steal weapons and multiply by feeding on enemy corpses; and Spike Mines, floating orbs that detonate when near the player. The three new weapons include the Mjolnir, a large lightning emitting hammer; the Laser Cannon, which shoots bouncing bolts of energy; and the Proximity Mine Launcher, which fires grenades that attach to surfaces and detonate when an opponent comes near. The three new power-ups include the Horn of Conjuring, which summons an enemy to protect the player; the Empathy Shield, which halves the damage taken by the player between the player and the attacking enemy; and the Wetsuit, which renders the player invulnerable to electricity and allows the player to stay underwater for a period of time. The storyline follows Armagon, a general of Quake's forces, planning to invade Earth via a portal known as the 'Rift'. Armagon resembles a giant gremlin with cybernetic legs and a combined rocket launcher/laser cannon for arms.

Tim Soete of "GameSpot" gave it a score 8.6 out of 10.

"Quake Mission Pack No. 2: Dissolution of Eternity" was the second official mission pack, released on March 19, 1997. Developed by Rogue Entertainment, it features two episodes divided into fifteen new single-player levels, a new multiplayer level, a new soundtrack, and several new enemies and bosses. Notably, the pack lacks secret levels. The eight new enemies include Electric Eels, Phantom Swordsmen, Multi-Grenade Ogres (which fire cluster grenades), Hell Spawn, Wraths (floating, robed undead), Guardians (resurrected ancient Egyptian warriors), Mummies, and statues of various enemies that can come to life. The four new types of bosses include Lava Men, Overlords, large Wraths, and a dragon guarding the "temporal energy converter". The two new power-ups include the Anti Grav Belt, which allows the player to jump higher; and the Power Shield, which lowers the damage the player receives. Rather than offering new weapons, the mission pack gives the player four new types of ammo for existing weapons, such as "lava nails" for the Nailgun, cluster grenades for the Grenade Launcher, rockets that split into four in a horizontal line for the Rocket Launcher, and plasma cells for the Thunderbolt, as well as a grappling hook to help with moving around the levels.

Tim Soete of "GameSpot" gave it a score of 7.7 out of 10.

In late 1996, id Software released "VQuake", a port of the "Quake" engine to support hardware accelerated rendering on graphics cards using the Rendition Vérité chipset. Aside from the expected benefit of improved performance, "VQuake" offered numerous visual improvements over the original software-rendered "Quake". It boasted full 16-bit color, bilinear filtering (reducing pixelation), improved dynamic lighting, optional anti-aliasing, and improved source code clarity, as the improved performance finally allowed the use of gotos to be abandoned in favor of proper loop constructs. As the name implied, "VQuake" was a proprietary port specifically for the Vérité; consumer 3D acceleration was in its infancy at the time, and there was no standard 3D API for the consumer market. After completing "VQuake", John Carmack vowed to never write a proprietary port again, citing his frustration with Rendition's Speedy3D API.

To improve the quality of online play, id Software released "QuakeWorld" on December 17, 1996, a build of "Quake" that featured significantly revamped network code including the addition of client-side prediction. The original "Quake" network code would not show the player the results of his actions until the server sent back a reply acknowledging them. For example, if the player attempted to move forward, his client would send the request to move forward to the server, and the server would determine whether the client was actually able to move forward or if he ran into an obstacle, such as a wall or another player. The server would then respond to the client, and only then would the client display movement to the player. This was fine for play on a LAN, a high bandwidth, very low latency connection, but the latency over a dial-up Internet connection is much larger than on a LAN, and this caused a noticeable delay between when a player tried to act and when that action was visible on the screen. This made gameplay much more difficult, especially since the unpredictable nature of the Internet made the amount of delay vary from moment to moment. Players would experience jerky, laggy motion that sometimes felt like ice skating, where they would slide around with seemingly no ability to stop, due to a build-up of previously-sent movement requests. John Carmack has admitted that this was a serious problem which should have been fixed before release, but it was not caught because he and other developers had high-speed Internet access at home.

With the help of client-side prediction, which allowed players to see their own movement immediately without waiting for a response from the server, "QuakeWorld" network code allowed players with high-latency connections to control their character's movement almost as precisely as when playing in single-player mode. The Netcode parameters could be adjusted by the user so that "QuakeWorld" performed well for users with high and low latency.

The trade off to client-side prediction was that sometimes other players or objects would no longer be quite where they had appeared to be, or, in extreme cases, that the player would be pulled back to a previous position when the client received a late reply from the server which overrode movement the client had already previewed; this was known as "warping". As a result, some serious players, particularly in the U.S., still preferred to play online using the original "Quake" engine (commonly called "NetQuake") rather than "QuakeWorld". However, the majority of players, especially those on dial-up connections, preferred the newer network model, and "QuakeWorld" soon became the dominant form of online play. Following the success of "QuakeWorld", client-side prediction has become a standard feature of nearly all real-time online games. As with all other "Quake" upgrades, "QuakeWorld" was released as a free, unsupported add-on to the game and was updated numerous times through 1998.

On January 22, 1997, id Software released "GLQuake". This was designed to use the OpenGL 3D API to access hardware 3D graphics acceleration cards to rasterize the graphics, rather than having the computer's CPU fill in every pixel. In addition to higher framerates for most players, "GLQuake" provided higher resolution modes and texture filtering. "GLQuake" also experimented with reflections, transparent water, and even rudimentary shadows. "GLQuake" came with a driver enabling the subset of OpenGL used by the game to function on the 3dfx "Voodoo Graphics" card, the only consumer-level card at the time capable of running "GLQuake" well. Previously, John Carmack had experimented with a version of Quake specifically written for the Rendition Vérité chip used in the Creative Labs "PCI 3D Blaster" card. This version had met with only limited success, and Carmack decided to write for generic APIs in the future rather than tailoring for specific hardware.

On March 11, 1997, id Software released "WinQuake", a version of the non-OpenGL engine designed to run under Microsoft Windows; the original "Quake" had been written for DOS, allowing for launch from Windows 95, but could not run under Windows NT-based operating systems because it required direct access to hardware. "WinQuake" instead accessed hardware via Win32-based APIs such as DirectSound, DirectInput, and DirectDraw that were supported on Windows 95, Windows NT 4.0 and later releases. Like "GLQuake", "WinQuake" also allowed higher resolution video modes. This removed the last barrier to widespread popularity of the game. In 1998, LBE Systems and Laser-Tron released "Quake: Arcade Tournament Edition" in the arcades in limited quantities.

To celebrate "Quake"s 20th anniversary, a mission pack was developed by MachineGames and released on June 24, 2016. It features 10 new single-player levels and a new multiplayer level, but does not use new gameplay additions from "Scourge of Armagon" and "Dissolution of Eternity". Chronologically, it is set between the main game and the expansions.

After the departure of Sandy Petersen, the remaining id employees chose to change the thematic direction substantially for "Quake II", making the design more technological and futuristic, rather than maintaining the focus on Lovecraftian fantasy. "Quake 4" followed the design themes of "Quake II", whereas "Quake III Arena" mixed these styles; it had a parallel setting that housed several "id all-stars" from various games as playable characters. The mixed settings occurred because "Quake II" originally began as a separate product line. The id designers fell back on the project's nickname of ""Quake II"" because the game's fast-paced, tactile feel felt closer to a Quake game than a new franchise. Since any sequel to the original "Quake" had already been vetoed, it became a way of continuing the series without continuing the storyline or setting of the first game. In June 2011, John Carmack made an offhand comment that id Software was considering going back to the "...mixed up Cthulhu-ish Quake 1 world and rebooting [in] that direction." There was also another game released called "Quake Live" which is the latest game in the series. At E3 2016,Quake Champions was announced at the Bethesda press conference. The game will be a multiplayer-only shooter in the style of "Quake III Arena" and will be released exclusively for Windows.

On July 20, 2016, Axel Gneiting, an id Tech employee responsible for implementing the Vulkan rendering path to the id Tech 6 engine used in "Doom" (2016), released a port called "vkQuake" under the GPLv2.




</doc>
<doc id="25267" url="https://en.wikipedia.org/wiki?curid=25267" title="Quantum field theory">
Quantum field theory

In theoretical physics, quantum field theory (QFT) is a theoretical framework that combines classical field theory, special relativity, and quantum mechanics (but notably not general relativity's description of gravity) and is used to construct physical models of subatomic particles (in particle physics) and quasiparticles (in condensed matter physics).

QFT treats particles as excited states (also called quanta) of their underlying fields, which are more fundamental than the particles. Interactions between particles are described by interaction terms in the Lagrangian involving their corresponding fields. Each interaction can be visually represented by Feynman diagrams, which are formal computational tools, in the process of relativistic perturbation theory.

As a successful theoretical framework today, quantum field theory emerged from the work of generations of theoretical physicists spanning much of the 20th century. Its development began in the 1920s with the description of interactions between light and electrons, culminating in the first quantum field theory — quantum electrodynamics. A major theoretical obstacle soon followed with the appearance and persistence of various infinities in perturbative calculations, a problem only resolved in the 1950s with the invention of the renormalization procedure. A second major barrier came with QFT's apparent inability to describe the weak and strong interactions, to the point where some theorists called for the abandonment of the field theoretic approach. The development of gauge theory and the completion of the Standard Model in the 1970s led to a renaissance of quantum field theory.

Quantum field theory is the result of the combination of classical field theory, quantum mechanics, and special relativity. A brief overview of these theoretical precursors is in order.

The earliest successful classical field theory is one that emerged from Newton's law of universal gravitation, despite the complete absence of the concept of fields from his 1687 treatise "Philosophiæ Naturalis Principia Mathematica". The force of gravity as described by Newton is an "action at a distance" — its effects on faraway objects are instantaneous, no matter the distance. In an exchange of letters with Richard Bentley, however, Newton stated that "it is inconceivable that inanimate brute matter should, without the mediation of something else which is not material, operate upon and affect other matter without mutual contact." It was not until the 18th century that mathematical physicists discovered a convenient description of gravity based on fields — a numerical quantity (a vector) assigned to every point in space indicating the action of gravity on any particle at that point. However, this was considered merely a mathematical trick.

Fields began to take on an existence of their own with the development of electromagnetism in the 19th century. Michael Faraday coined the English term "field" in 1845. He introduced fields as properties of space (even when it is devoid of matter) having physical effects. He argued against "action at a distance", and proposed that interactions between objects occur via space-filling "lines of force". This description of fields remains to this day.

The theory of classical electromagnetism was completed in 1862 with Maxwell's equations, which described the relationship between the electric field, the magnetic field, electric current, and electric charge. Maxwell's equations implied the existence of electromagnetic waves, a phenomenon whereby electric and magnetic fields propagate from one spatial point to another at a finite speed, which turns out to be the speed of light. Action-at-a-distance was thus conclusively refuted.

Despite the enormous success of classical electromagnetism, it was unable to account for the discrete lines in atomic spectra, nor for the distribution of blackbody radiation in different wavelengths. Max Planck's study of blackbody radiation marked the beginning of quantum mechanics. He treated atoms, which absorb and emit electromagnetic radiation, as tiny oscillators with the crucial property that their energies can only take on a series of discrete, rather than continuous, values. These are known as quantum harmonic oscillators. This process of restricting energies to discrete values is called quantization. Building on this idea, Albert Einstein proposed in 1905 an explanation for the photoelectric effect, that light is composed of individual packets of energy called photons (the quanta of light). This implied that the electromagnetic radiation, while being waves in the classical electromagnetic field, also exists in the form of particles.

In 1913, Niels Bohr introduced the Bohr model of atomic structure, wherein electrons within atoms can only take on a series of discrete, rather than continuous, energies. This is another example of quantization. The Bohr model successfully explained the discrete nature of atomic spectral lines. In 1924, Louis de Broglie proposed the hypothesis of wave-particle duality, that microscopic particles exhibit both wave-like and particle-like properties under different circumstances. Uniting these scattered ideas, a coherent discipline, quantum mechanics, was formulated between 1925 and 1926, with important contributions from de Broglie, Werner Heisenberg, Max Born, Erwin Schrödinger, Paul Dirac, and Wolfgang Pauli.

In the same year as his paper on the photoelectric effect, Einstein published his theory of special relativity, built on Maxwell's electromagnetism. New rules, called Lorentz transformation, were given for the way time and space coordinates of an event change under changes in the observer's velocity, and the distinction between time and space was blurred. It was proposed that all physical laws must be the same for observers at different velocities, "i.e." that physical laws be invariant under Lorentz transformations.

Two difficulties remained. Observationally, the Schrödinger equation underlying quantum mechanics could explain the stimulated emission of radiation from atoms, where an electron emits a new photon under the action of an external electromagnetic field, but it was unable to explain spontaneous emission, where an electron spontaneously decreases in energy and emits a photon even without the action of an external electromagnetic field. Theoretically, the Schrödinger equation could not describe photons and was inconsistent with the principles of special relativity — it treats time as an ordinary number while promoting spatial coordinates to linear operators.

Quantum field theory naturally began with the study of electromagnetic interactions, as the electromagnetic field was the only known classical field as of the 1920s.

Through the works of Born, Heisenberg, and Pascual Jordan in 1925-1926, a quantum theory of the free electromagnetic field (one with no interactions with matter) was developed via canonical quantization by treating the electromagnetic field as a set of quantum harmonic oscillators. With the exclusion of interactions, however, such a theory was yet incapable of making quantitative predictions about the real world.

In his seminal 1927 paper "The quantum theory of the emission and absorption of radiation", Dirac coined the term quantum electrodynamics (QED), a theory that adds upon the terms describing the free electromagnetic field an additional interaction term between electric current density and the electromagnetic vector potential. Using first-order perturbation theory, he successfully explained the phenomenon of spontaneous emission. According to the uncertainty principle in quantum mechanics, quantum harmonic oscillators cannot remain stationary, but they have a non-zero minimum energy and must always be oscillating, even in the lowest energy state (the ground state). Therefore, even in a perfect vacuum, there remains an oscillating electromagnetic field having zero-point energy. It is this quantum fluctuation of electromagnetic fields in the vacuum that "stimulates" the spontaneous emission of radiation by electrons in atoms. Dirac's theory was hugely successful in explaining both the emission and absorption of radiation by atoms; by applying second-order perturbation theory, it was able to account for the scattering of photons, resonance fluorescence, as well as non-relativistic Compton scattering. Nonetheless, the application of higher-order perturbation theory was plagued with problematic infinities in calculations.

In 1928, Dirac wrote down a wave equation that described relativistic electrons — the Dirac equation. It had the following important consequences: the spin of an electron is 1/2; the electron "g"-factor is 2; it led to the correct Sommerfeld formula for the fine structure of the hydrogen atom; and it could be used to derive the Klein-Nishina formula for relativistic Compton scattering. Although the results were fruitful, the theory also apparently implied the existence of negative energy states, which would cause atoms to be unstable, since they could always decay to lower energy states by the emission of radiation.

The prevailing view at the time was that the world was composed of two very different ingredients: material particles (such as electrons) and quantum fields (such as photons). Material particles were considered to be eternal, with their physical state described by the probabilities of finding each particle in any given region of space or range of velocities. On the other hand, photons were considered merely the excited states of the underlying quantized electromagnetic field, and could be freely created or destroyed. It was between 1928 and 1930 that Jordan, Eugene Wigner, Heisenberg, Pauli, and Enrico Fermi discovered that material particles could also be seen as excited states of quantum fields. Just as photons are excited states of the quantized electromagnetic field, so each type of particle had its corresponding quantum field: an electron field, a proton field, etc. Given enough energy, it would now be possible to create material particles. Building on this idea, Fermi proposed in 1932 an explanation for "β" decay known as Fermi's interaction. Atomic nuclei do not contain electrons "per se", but in the process of decay, an electron is created out of the surrounding electron field, analogous to the photon created from the surrounding electromagnetic field in the radiative decay of an excited atom.

It was realized in 1929 by Dirac and others that negative energy states implied by the Dirac equation could be removed by assuming the existence of particles with the same mass as electrons but opposite electric charge. This not only ensured the stability of atoms, but it was also the first proposal of the existence of antimatter. Indeed, the evidence for positrons was discovered in 1932 by Carl David Anderson in cosmic rays. With enough energy, such as by absorbing a photon, an electron-positron pair could be created, a process called pair production; the reverse process, annihilation, could also occur with the emission of a photon. This showed that particle numbers need not be fixed during an interaction. Historically, however, positrons were at first thought of as "holes" in an infinite electron sea, rather than a new kind of particle, and this theory was referred to as the Dirac hole theory. QFT naturally incorporated antiparticles in its formalism.

Robert Oppenheimer showed in 1930 that higher-order perturbative calculations in QED always resulted in infinite quantities, such as the electron self-energy and the vacuum zero-point energy of the electron and photon fields, suggesting that the computational methods at the time could not properly deal with interactions involving photons with extremely high momenta. It was not until 20 years later that a systematic approach to remove such infinities was developed.

A series of papers was published between 1934 and 1938 by Ernst Stueckelberg that established a relativistically invariant formulation of QFT. In 1947, Stueckelberg also independently developed a complete renormalization procedure. Unfortunately, such achievements were not understood and recognized by the theoretical community.

Faced with these infinities, John Archibald Wheeler and Heisenberg proposed, in 1937 and 1943 respectively, to supplant the problematic QFT with the so-called S-matrix theory. Since the specific details of microscopic interactions are inaccessible to observations, the theory should only attempt to describe the relationships between a small number of observables ("e.g." the energy of an atom) in an interaction, rather than be concerned with the microscopic minutiae of the interaction. In 1945, Richard Feynman and Wheeler daringly suggested abandoning QFT altogether and proposed action-at-a-distance as the mechanism of particle interactions.

In 1947, Willis Lamb and Robert Retherford measured the minute difference in the "S" and "P" energy levels of the hydrogen atom, also called the Lamb shift. By ignoring the contribution of photons whose energy exceeds the electron mass, Hans Bethe successfully estimated the numerical value of the Lamb shift. Subsequently, Norman Myles Kroll, Lamb, James Bruce French, and Victor Weisskopf again confirmed this value using an approach in which infinities cancelled other infinities to result in finite quantities. However, this method was clumsy and unreliable and could not be generalized to other calculations.

The breakthrough eventually came around 1950 when a more robust method for eliminating infinities was developed by Julian Schwinger, Feynman, Freeman Dyson, and Shinichiro Tomonaga. The main idea is to replace the initial, so-called "bare", parameters (mass, electric charge, etc.), which have no physical meaning, by their finite measured values. To cancel the apparently infinite parameters, one has to introduce additional, infinite, "counterterms" into the Lagrangian. This systematic computational procedure is known as renormalization and can be applied to arbitrary order in perturbation theory.

By applying the renormalization procedure, calculations were finally made to explain the electron's anomalous magnetic moment (the deviation of the electron "g"-factor from 2) and vacuum polarisation. These results agreed with experimental measurements to a remarkable degree, thus marking the end of a "war against infinities".

At the same time, Feynman introduced the path integral formulation of quantum mechanics and Feynman diagrams. The latter can be used to visually and intuitively organise and to help compute terms in the perturbative expansion. Each diagram can be interpreted as paths of particles in an interaction, with each vertex and line having a corresponding mathematical expression, and the product of these expressions gives the scattering amplitude of the interaction represented by the diagram.

It was with the invention of the renormalization procedure and Feynman diagrams that QFT finally arose as a complete theoretical framework.

Given the tremendous success of QED, many theorists believed, in the few years after 1949, that QFT could soon provide an understanding of all microscopic phenomena, not only the interactions between photons, electrons, and positrons. Contrary to this optimism, QFT entered yet another period of depression that lasted for almost two decades.

The first obstacle was the limited applicability of the renormalization procedure. In perturbative calculations in QED, all infinite quantities could be eliminated by redefining a small (finite) number of physical quantities (namely the mass and charge of the electron). Dyson proved in 1949 that this is only possible for a small class of theories called "renormalizable theories", of which QED is an example. However, most theories, including the Fermi theory of the weak interaction, are "non-renormalizable". Any perturbative calculation in these theories beyond the first order would result in infinities that could not be removed by redefining a finite number of physical quantities.

The second major problem stemmed from the limited validity of the Feynman diagram method, which is based on a series expansion in perturbation theory. In order for the series to converge and low-order calculations to be a good approximation, the coupling constant, in which the series is expanded, must be a sufficiently small number. The coupling constant in QED is the fine-structure constant , which is small enough that only the simplest, lowest order, Feynman diagrams need to be considered in realistic calculations. In contrast, the coupling constant in the strong interaction is roughly of the order of one, making complicated, higher order, Feynman diagrams just as important as simple ones. There was thus no way of deriving reliable quantitative predictions for the strong interaction using perturbative QFT methods.

With these difficulties looming, many theorists began to turn away from QFT. Some focused on symmetry principles and conservation laws, while others picked up the old S-matrix theory of Wheeler and Heisenberg. QFT was used heuristically as guiding principles, but not as a basis for quantitative calculations.

In 1954, Yang Chen-Ning and Robert Mills generalised the local symmetry of QED, leading to non-Abelian gauge theories (also known as Yang–Mills theories), which are based on more complicated local symmetry groups. In QED, (electrically) charged particles interact via the exchange of photons, while in non-Abelian gauge theory, particles carrying a new type of "charge" interact via the exchange of massless gauge bosons. Unlike photons, these gauge bosons themselves carry charge.

Sheldon Glashow developed a non-Abelian gauge theory that unified the electromagnetic and weak interactions in 1960. In 1964, Abdus Salam and John Clive Ward arrived at the same theory through a different path. This theory, nevertheless, was non-renormalizable.

Peter Higgs, Robert Brout, and François Englert proposed in 1964 that the gauge symmetry in Yang–Mills theories could be broken by a mechanism called spontaneous symmetry breaking, through which originally massless gauge bosons could acquire mass.

By combining the earlier theory of Glashow, Salam, and Ward with the idea of spontaneous symmetry breaking, Steven Weinberg wrote down in 1967 a theory describing electroweak interactions between all leptons and the effects of the Higgs boson. His theory was at first mostly ignored, until it was brought back to light in 1971 by Gerard 't Hooft's proof that non-Abelian gauge theories are renormalizable. The electroweak theory of Weinberg and Salam was extended from leptons to quarks in 1970 by Glashow, John Iliopoulos, and Luciano Maiani, marking its completion.

Harald Fritzsch, Murray Gell-Mann, and Heinrich Leutwyler discovered in 1971 that certain phenomena involving the strong interaction could also be explained by non-Abelian gauge theory. Quantum chromodynamics (QCD) was born. In 1973, David Gross, Frank Wilczek, and Hugh David Politzer showed that non-Abelian gauge theories are "asymptotically free", meaning that under renormalization, the coupling constant of the strong interaction decreases as the interaction energy increases. (Similar discoveries had been made numerous times previously, but they had been largely ignored.) Therefore, at least in high-energy interactions, the coupling constant in QCD becomes sufficiently small to warrant a perturbative series expansion, making quantitative predictions for the strong interaction possible.

These theoretical breakthroughs brought about a renaissance in QFT. The full theory, which includes the electroweak theory and chromodynamics, is referred to today as the Standard Model of elementary particles. The Standard Model successfully describes all fundamental interactions except gravity, and its many predictions have been met with remarkable experimental confirmation in subsequent decades. The Higgs boson, central to the mechanism of spontaneous symmetry breaking, was finally detected in 2012 at CERN, marking the complete verification of the existence of all constituents of the Standard Model.

The 1970s saw the development of non-perturbative methods in non-Abelian gauge theories. The 't Hooft–Polyakov monopole was discovered by 't Hooft and Alexander Polyakov, flux tubes by Holger Bech Nielsen and Poul Olesen, and instantons by Polyakov "et al.". These objects are inaccessible through perturbation theory.

Supersymmetry also appeared in the same period. The first supersymmetric QFT in four dimensions was built by Yuri Golfand and Evgeny Likhtman in 1970, but their result failed to garner widespread interest due to the Iron Curtain. Supersymmetry only took off in the theoretical community after the work of Julius Wess and Bruno Zumino in 1973.

Among the four fundamental interactions, gravity remains the only one that lacks a consistent QFT description. Various attempts at a theory of quantum gravity led to the development of string theory, itself a type of two-dimensional QFT with conformal symmetry. Joël Scherk and John Schwarz first proposed in 1974 that string theory could be "the" quantum theory of gravity.

Although quantum field theory arose from the study of interactions between elementary particles, it has been successfully applied to other physical systems, particularly to many-body systems in condensed matter physics.

Historically, the Higgs mechanism of spontaneous symmetry breaking was a result of Yoichiro Nambu's application of superconductor theory to elementary particles, while the concept of renormalization came out of the study of second-order phase transitions in matter.

Soon after the introduction of photons, Einstein performed the quantization procedure on vibrations in a crystal, leading to the first quasiparticle — phonons. Lev Landau claimed that low-energy excitations in many condensed matter systems could be described in terms of interactions between a set of quasiparticles. The Feynman diagram method of QFT was naturally well suited to the analysis of various phenomena in condensed matter systems.

Gauge theory is used to describe the quantization of magnetic flux in superconductors, the resistivity in the quantum Hall effect, as well as the relation between frequency and voltage in the AC Josephson effect.

For simplicity, natural units are used in the following sections, in which the reduced Planck constant and the speed of light are both set to one.

A classical field is a function of spatial and time coordinates. Examples include the gravitational field in Newtonian gravity and the electric field and magnetic field in classical electromagnetism. A classical field can be thought of as a numerical quantity assigned to every point in space that changes in time. Hence, it has infinitely many degrees of freedom. 

Many phenomena exhibiting quantum mechanical properties cannot be explained by classical fields alone. Phenomena such as the photoelectric effect are best explained by discrete particles (photons), rather than a spatially continuous field. The goal of quantum field theory is to describe various quantum mechanical phenomena using a modified concept of fields.

Canonical quantisation and path integrals are two common formulations of QFT. To motivate the fundamentals of QFT, an overview of classical field theory is in order.

The simplest classical field is a real scalar field — a real number at every point in space that changes in time. It is denoted as , where is the position vector, and is the time. Suppose the Lagrangian of the field is
where formula_2 is the time-derivative of the field, is the gradient operator, and is a real parameter (the "mass" of the field). Applying the Euler–Lagrange equation on the Lagrangian:
we obtain the equations of motion for the field, which describe the way it varies in time and space:
This is known as the Klein–Gordon equation.

The Klein–Gordon equation is a wave equation, so its solutions can be expressed as a sum of normal modes (obtained via Fourier transform) as follows:
where is a complex number (normalised by convention), denotes complex conjugation, and is the frequency of the normal mode:
where , and denotes the covariant derivative. The Lagrangian of a QFT, hence its calculational results and physical predictions, depends on the geometry of the spacetime background.

The correlation functions and physical predictions of a QFT depend on the spacetime metric . For a special class of QFTs called topological quantum field theories (TQFTs), all correlation functions are independent of continuous changes in the spacetime metric. QFTs in curved spacetime generally change according to the "geometry" (local structure) of the spacetime background, while TQFTs are invariant under spacetime diffeomorphisms but are sensitive to the "topology" (global structure) of spacetime. This means that all calculational results of TQFTs are topological invariants of the underlying spacetime. Chern–Simons theory is an example of TQFT and has been used to construct models of quantum gravity. Applications of TQFT include the fractional quantum Hall effect and topological quantum computers.

Using perturbation theory, the total effect of a small interaction term can be approximated order by order by a series expansion in the number of virtual particles participating in the interaction. Every term in the expansion may be understood as one possible way for (physical) particles to interact with each other via virtual particles, expressed visually using a Feynman diagram. The electromagnetic force between two electrons in QED is represented (to first order in perturbation theory) by the propagation of a virtual photon. In a similar manner, the W and Z bosons carry the weak interaction, while gluons carry the strong interaction. The interpretation of an interaction as a sum of intermediate states involving the exchange of various virtual particles only makes sense in the framework of perturbation theory. In contrast, non-perturbative methods in QFT treat the interacting Lagrangian as a whole without any series expansion. Instead of particles that carry interactions, these methods have spawned such concepts as 't Hooft–Polyakov monopole, domain wall, flux tube, and instanton. Examples of QFTs that are completely solvable non-perturbatively include minimal models of conformal field theory and the Thirring model.

In spite of its overwhelming success in particle physics and condensed matter physics, QFT itself lacks a formal mathematical foundation. For example, according to Haag's theorem, there does not exist a well-defined interaction picture for QFT, which implies that perturbation theory of QFT, which underlies the entire Feynman diagram method, is fundamentally ill defined.

Since the 1950s, theoretical physicists and mathematicians have attempted to organise all QFTs into a set of axioms, in order to establish the existence of concrete models of relativistic QFT in a mathematically rigorous way and to study their properties. This line of study is called constructive quantum field theory, a subfield of mathematical physics, which has led to such results as CPT theorem, spin–statistics theorem, and Goldstone's theorem.

Compared to ordinary QFT, topological quantum field theory and conformal field theory are better supported mathematically — both can be classified in the framework of representations of cobordisms.

Algebraic quantum field theory is another approach to the axiomatisation of QFT, in which the fundamental objects are local operators and the algebraic relations between them. Axiomatic systems following this approach include Wightman axioms and Haag–Kastler axioms. One way to construct theories satisfying Wightman axioms is to use Osterwalder–Schrader axioms, which give the necessary and sufficient conditions for a real time theory to be obtained from an imaginary time theory by analytic continuation (Wick rotation).

Yang–Mills existence and mass gap, one of the Millenium Prize Problems, concerns the well-defined existence of Yang–Mills theories as set out by the above axioms. The full problem statement is as follows.






</doc>
<doc id="25268" url="https://en.wikipedia.org/wiki?curid=25268" title="Quantum electrodynamics">
Quantum electrodynamics

In particle physics, quantum electrodynamics (QED) is the relativistic quantum field theory of electrodynamics. In essence, it describes how light and matter interact and is the first theory where full agreement between quantum mechanics and special relativity is achieved. QED mathematically describes all phenomena involving electrically charged particles interacting by means of exchange of photons and represents the quantum counterpart of classical electromagnetism giving a complete account of matter and light interaction.

In technical terms, QED can be described as a perturbation theory of the electromagnetic quantum vacuum. Richard Feynman called it "the jewel of physics" for its extremely accurate predictions of quantities like the anomalous magnetic moment of the electron and the Lamb shift of the energy levels of hydrogen.

The first formulation of a quantum theory describing radiation and matter interaction is attributed to British scientist Paul Dirac, who (during the 1920s) was able to compute the coefficient of spontaneous emission of an atom.

Dirac described the quantization of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles. In the following years, with contributions from Wolfgang Pauli, Eugene Wigner, Pascual Jordan, Werner Heisenberg and an elegant formulation of quantum electrodynamics due to Enrico Fermi, physicists came to believe that, in principle, it would be possible to perform any computation for any physical process involving photons and charged particles. However, further studies by Felix Bloch with Arnold Nordsieck, and Victor Weisskopf, in 1937 and 1939, revealed that such computations were reliable only at a first order of perturbation theory, a problem already pointed out by Robert Oppenheimer. At higher orders in the series infinities emerged, making such computations meaningless and casting serious doubts on the internal consistency of the theory itself. With no solution for this problem known at the time, it appeared that a fundamental incompatibility existed between special relativity and quantum mechanics.
Difficulties with the theory increased through the end of the 1940s. Improvements in microwave technology made it possible to take more precise measurements of the shift of the levels of a hydrogen atom, now known as the Lamb shift and magnetic moment of the electron. These experiments exposed discrepancies which the theory was unable to explain.

A first indication of a possible way out was given by Hans Bethe in 1947, after attending the Shelter Island Conference. While he was traveling by train from the conference to Schenectady he made the first non-relativistic computation of the shift of the lines of the hydrogen atom as measured by Lamb and Retherford. Despite the limitations of the computation, agreement was excellent. The idea was simply to attach infinities to corrections of mass and charge that were actually fixed to a finite value by experiments. In this way, the infinities get absorbed in those constants and yield a finite result in good agreement with experiments. This procedure was named renormalization.

Based on Bethe's intuition and fundamental papers on the subject by Shin'ichirō Tomonaga, Julian Schwinger, Richard Feynman and Freeman Dyson, it was finally possible to get fully covariant formulations that were finite at any order in a perturbation series of quantum electrodynamics. Shin'ichirō Tomonaga, Julian Schwinger and Richard Feynman were jointly awarded with the 1965 Nobel Prize in Physics for their work in this area. Their contributions, and those of Freeman Dyson, were about covariant and gauge-invariant formulations of quantum electrodynamics that allow computations of observables at any order of perturbation theory. Feynman's mathematical technique, based on his diagrams, initially seemed very different from the field-theoretic, operator-based approach of Schwinger and Tomonaga, but Freeman Dyson later showed that the two approaches were equivalent. Renormalization, the need to attach a physical meaning at certain divergences appearing in the theory through integrals, has subsequently become one of the fundamental aspects of quantum field theory and has come to be seen as a criterion for a theory's general acceptability. Even though renormalization works very well in practice, Feynman was never entirely comfortable with its mathematical validity, even referring to renormalization as a "shell game" and "hocus pocus".

QED has served as the model and template for all subsequent quantum field theories. One such subsequent theory is quantum chromodynamics, which began in the early 1960s and attained its present form in the 1970s work by H. David Politzer, Sidney Coleman, David Gross and Frank Wilczek. Building on the pioneering work of Schwinger, Gerald Guralnik, Dick Hagen, and Tom Kibble, Peter Higgs, Jeffrey Goldstone, and others, Sheldon Lee Glashow, Steven Weinberg and Abdus Salam independently showed how the weak nuclear force and quantum electrodynamics could be merged into a single electroweak force.

Near the end of his life, Richard Feynman gave a series of lectures on QED intended for the lay public. These lectures were transcribed and published as Feynman (1985), "", a classic non-mathematical exposition of QED from the point of view articulated below.

The key components of Feynman's presentation of QED are three basic actions.
These actions are represented in the form of visual shorthand by the three basic elements of Feynman diagrams: a wavy line for the photon, a straight line for the electron and a junction of two straight lines and a wavy one for a vertex representing emission or absorption of a photon by an electron. These can all be seen in the adjacent diagram.

As well as the visual shorthand for the actions Feynman introduces another kind of shorthand for the numerical quantities called probability amplitudes. The probability is the square of the absolute value of total probability amplitude, formula_1. If a photon moves from one place and time formula_2 to another place and time formula_3, the associated quantity is written in Feynman's shorthand as formula_4. The similar quantity for an electron moving from formula_5 to formula_6 is written formula_7. The quantity that tells us about the probability amplitude for the emission or absorption of a photon he calls "j". This is related to, but not the same as, the measured electron charge "e".

QED is based on the assumption that complex interactions of many electrons and photons can be represented by fitting together a suitable collection of the above three building blocks and then using the probability amplitudes to calculate the probability of any such complex interaction. It turns out that the basic idea of QED can be communicated while assuming that the square of the total of the probability amplitudes mentioned above ("P"("A" to "B"), "E"("C" to "D") and "j") acts just like our everyday probability (a simplification made in Feynman's book). Later on, this will be corrected to include specifically quantum-style mathematics, following Feynman.

The basic rules of probability amplitudes that will be used are:

Suppose, we start with one electron at a certain place and time (this place and time being given the arbitrary label "A") and a photon at another place and time (given the label "B"). A typical question from a physical standpoint is: "What is the probability of finding an electron at "C" (another place and a later time) and a photon at "D" (yet another place and time)?". The simplest process to achieve this end is for the electron to move from "A" to "C" (an elementary action) and for the photon to move from "B" to "D" (another elementary action). From a knowledge of the probability amplitudes of each of these sub-processes – "E"("A" to "C") and "P"("B" to "D") – we would expect to calculate the probability amplitude of both happening together by multiplying them, using rule b) above. This gives a simple estimated overall probability amplitude, which is squared to give an estimated probability.

But there are other ways in which the end result could come about. The electron might move to a place and time "E", where it absorbs the photon; then move on before emitting another photon at "F"; then move on to "C", where it is detected, while the new photon moves on to "D". The probability of this complex process can again be calculated by knowing the probability amplitudes of each of the individual actions: three electron actions, two photon actions and two vertexes – one emission and one absorption. We would expect to find the total probability amplitude by multiplying the probability amplitudes of each of the actions, for any chosen positions of "E" and "F". We then, using rule a) above, have to add up all these probability amplitudes for all the alternatives for "E" and "F". (This is not elementary in practice and involves integration.) But there is another possibility, which is that the electron first moves to "G", where it emits a photon, which goes on to "D", while the electron moves on to "H", where it absorbs the first photon, before moving on to "C". Again, we can calculate the probability amplitude of these possibilities (for all points "G" and "H"). We then have a better estimation for the total probability amplitude by adding the probability amplitudes of these two possibilities to our original simple estimate. Incidentally, the name given to this process of a photon interacting with an electron in this way is Compton scattering.

There is an "infinite number" of other intermediate processes in which more and more photons are absorbed and/or emitted. For each of these possibilities, there is a Feynman diagram describing it. This implies a complex computation for the resulting probability amplitudes, but provided it is the case that the more complicated the diagram, the less it contributes to the result, it is only a matter of time and effort to find as accurate an answer as one wants to the original question. This is the basic approach of QED. To calculate the probability of "any" interactive process between electrons and photons, it is a matter of first noting, with Feynman diagrams, all the possible ways in which the process can be constructed from the three basic elements. Each diagram involves some calculation involving definite rules to find the associated probability amplitude.

That basic scaffolding remains when one moves to a quantum description, but some conceptual changes are needed. One is that whereas we might expect in our everyday life that there would be some constraints on the points to which a particle can move, that is "not" true in full quantum electrodynamics. There is a possibility of an electron at "A", or a photon at "B", moving as a basic action to "any other place and time in the universe". That includes places that could only be reached at speeds greater than that of light and also "earlier times". (An electron moving backwards in time can be viewed as a positron moving forward in time.)

Quantum mechanics introduces an important change in the way probabilities are computed. Probabilities are still represented by the usual real numbers we use for probabilities in our everyday world, but probabilities are computed as the square of probability amplitudes, which are complex numbers.

Feynman avoids exposing the reader to the mathematics of complex numbers by using a simple but accurate representation of them as arrows on a piece of paper or screen. (These must not be confused with the arrows of Feynman diagrams, which are simplified representations in two dimensions of a relationship between points in three dimensions of space and one of time.) The amplitude arrows are fundamental to the description of the world given by quantum theory. They are related to our everyday ideas of probability by the simple rule that the probability of an event is the "square" of the length of the corresponding amplitude arrow. So, for a given process, if two probability amplitudes, v and w, are involved, the probability of the process will be given either by

or

The rules as regards adding or multiplying, however, are the same as above. But where you would expect to add or multiply probabilities, instead you add or multiply probability amplitudes that now are complex numbers.

Addition and multiplication are common operations in the theory of complex numbers and are given in the figures. The sum is found as follows. Let the start of the second arrow be at the end of the first. The sum is then a third arrow that goes directly from the beginning of the first to the end of the second. The product of two arrows is an arrow whose length is the product of the two lengths. The direction of the product is found by adding the angles that each of the two have been turned through relative to a reference direction: that gives the angle that the product is turned relative to the reference direction.

That change, from probabilities to probability amplitudes, complicates the mathematics without changing the basic approach. But that change is still not quite enough because it fails to take into account the fact that both photons and electrons can be polarized, which is to say that their orientations in space and time have to be taken into account. Therefore, "P"("A" to "B") consists of 16 complex numbers, or probability amplitude arrows. There are also some minor changes to do with the quantity "j", which may have to be rotated by a multiple of 90° for some polarizations, which is only of interest for the detailed bookkeeping.

Associated with the fact that the electron can be polarized is another small necessary detail, which is connected with the fact that an electron is a fermion and obeys Fermi–Dirac statistics. The basic rule is that if we have the probability amplitude for a given complex process involving more than one electron, then when we include (as we always must) the complementary Feynman diagram in which we exchange two electron events, the resulting amplitude is the reverse – the negative – of the first. The simplest case would be two electrons starting at "A" and "B" ending at "C" and "D". The amplitude would be calculated as the "difference", , where we would expect, from our everyday idea of probabilities, that it would be a sum.

Finally, one has to compute "P"("A" to "B") and "E"("C" to "D") corresponding to the probability amplitudes for the photon and the electron respectively. These are essentially the solutions of the Dirac equation, which describe the behavior of the electron's probability amplitude and the Maxwell's equations, which describes the behavior of the photon's probability amplitude. These are called Feynman propagators. The translation to a notation commonly used in the standard literature is as follows:

where a shorthand symbol such as formula_11 stands for the four real numbers that give the time and position in three dimensions of the point labeled "A".

A problem arose historically which held up progress for twenty years: although we start with the assumption of three basic "simple" actions, the rules of the game say that if we want to calculate the probability amplitude for an electron to get from "A" to "B", we must take into account "all" the possible ways: all possible Feynman diagrams with those endpoints. Thus there will be a way in which the electron travels to "C", emits a photon there and then absorbs it again at "D" before moving on to "B". Or it could do this kind of thing twice, or more. In short, we have a fractal-like situation in which if we look closely at a line, it breaks up into a collection of "simple" lines, each of which, if looked at closely, are in turn composed of "simple" lines, and so on "ad infinitum". This is a challenging situation to handle. If adding that detail only altered things slightly, then it would not have been too bad, but disaster struck when it was found that the simple correction mentioned above led to "infinite" probability amplitudes. In time this problem was "fixed" by the technique of renormalization. However, Feynman himself remained unhappy about it, calling it a "dippy process".

Within the above framework physicists were then able to calculate to a high degree of accuracy some of the properties of electrons, such as the anomalous magnetic dipole moment. However, as Feynman points out, it fails to explain why particles such as the electron have the masses they do. "There is no theory that adequately explains these numbers. We use the numbers in all our theories, but we don't understand them – what they are, or where they come from. I believe that from a fundamental point of view, this is a very interesting and serious problem."

Mathematically, QED is an abelian gauge theory with the symmetry group U(1). The gauge field, which mediates the interaction between the charged spin-1/2 fields, is the electromagnetic field.
The QED Lagrangian for a spin-1/2 field interacting with the electromagnetic field is given in natural units by the real part of
where

Substituting the definition of "D" into the Lagrangian gives

From this Lagrangian, the equations of motion for the "ψ" and "A" fields can be obtained.

Using the field-theoretic Euler–Lagrange equation for "ψ",

The derivatives of the Lagrangian concerning "ψ" are

Inserting these into () results in

with Hermitian conjugate

Bringing the middle term to the right-hand side yields

The left-hand side is like the original Dirac equation, and the right-hand side is the interaction with the electromagnetic field.

Using the Euler–Lagrange equation for the "A" field,

the derivatives this time are

Substituting back into () leads to

Now, if we impose the Lorenz gauge condition

the equations reduce to

which is a wave equation for the four-potential, the QED version of the classical Maxwell equations in the Lorenz gauge. (The square represents the D'Alembert operator, formula_28.)

This theory can be straightforwardly quantized by treating bosonic and fermionic sectors as free. This permits us to build a set of asymptotic states that can be used to start computation of the probability amplitudes for different processes. In order to do so, we have to compute an evolution operator, which for a given initial state formula_29 will give a final state formula_30 in such a way to have

This technique is also known as the S-matrix. The evolution operator is obtained in the interaction picture, where time evolution is given by the interaction Hamiltonian, which is the integral over space of the second term in the Lagrangian density given above:

and so, one has

where "T" is the time-ordering operator. This evolution operator only has meaning as a series, and what we get here is a perturbation series with the fine-structure constant as the development parameter. This series is called the Dyson series.

Despite the conceptual clarity of this Feynman approach to QED, almost no early textbooks follow him in their presentation. When performing calculations, it is much easier to work with the Fourier transforms of the propagators. Experimental tests of quantum electrodynamics are typically scattering experiments. In scattering theory, particles momenta rather than their positions are considered, and it is convenient to think of particles as being created or annihilated when they interact. Feynman diagrams then "look" the same, but the lines have different interpretations. The electron line represents an electron with a given energy and momentum, with a similar interpretation of the photon line. A vertex diagram represents the annihilation of one electron and the creation of another together with the absorption or creation of a photon, each having specified energies and momenta.

Using Wick's theorem on the terms of the Dyson series, all the terms of the S-matrix for quantum electrodynamics can be computed through the technique of Feynman diagrams. In this case, rules for drawing are the following

To these rules we must add a further one for closed loops that implies an integration on momenta formula_34, since these internal ("virtual") particles are not constrained to any specific energy–momentum, even that usually required by special relativity (see Propagator for details).

From them, computations of probability amplitudes are straightforwardly given. An example is Compton scattering, with an electron and a photon undergoing elastic scattering. Feynman diagrams are in this case

and so we are able to get the corresponding amplitude at the first order of a perturbation series for the S-matrix:

from which we can compute the cross section for this scattering.

The predictive success of quantum electrodynamics largely rests on the use of perturbation theory, expressed in Feynman diagrams. However, quantum electrodynamics also leads to predictions beyond perturbation theory. In the presence of very strong electric fields, it predicts that electrons and positrons will be spontaneously produced, so causing the decay of the field. This process, called the Schwinger effect, cannot be understood in terms of any finite number of Feynman diagrams and hence is described as nonperturbative. Mathematically, it can be derived by a semiclassical approximation to the path integral of quantum electrodynamics.

Higher-order terms can be straightforwardly computed for the evolution operator, but these terms display diagrams containing the following simpler ones

that, being closed loops, imply the presence of diverging integrals having no mathematical meaning. To overcome this difficulty, a technique called renormalization has been devised, producing finite results in very close agreement with experiments. A criterion for the theory being meaningful after renormalization is that the number of diverging diagrams is finite. In this case, the theory is said to be "renormalizable". The reason for this is that to get observables renormalized, one needs a finite number of constants to maintain the predictive value of the theory untouched. This is exactly the case of quantum electrodynamics displaying just three diverging diagrams. This procedure gives observables in very close agreement with experiment as seen e.g. for electron gyromagnetic ratio.

Renormalizability has become an essential criterion for a quantum field theory to be considered as a viable one. All the theories describing fundamental interactions, except gravitation, whose quantum counterpart is only conjectural and presently under very active research, are renormalizable theories.

An argument by Freeman Dyson shows that the radius of convergence of the perturbation series in QED is zero. The basic argument goes as follows: if the coupling constant were negative, this would be equivalent to the Coulomb force constant being negative. This would "reverse" the electromagnetic interaction so that "like" charges would "attract" and "unlike" charges would "repel". This would render the vacuum unstable against decay into a cluster of electrons on one side of the universe and a cluster of positrons on the other side of the universe. Because the theory is "sick" for any negative value of the coupling constant, the series does not converge but are at best an asymptotic series.

From a modern perspective, we say that QED is not well defined as a quantum field theory to arbitrarily high energy. The coupling constant runs to infinity at finite energy, signalling a Landau pole. The problem is essentially that QED appears to suffer from quantum triviality issues. This is one of the motivations for embedding QED within a Grand Unified Theory.




</doc>
<doc id="25270" url="https://en.wikipedia.org/wiki?curid=25270" title="Quine (computing)">
Quine (computing)

A quine is a computer program which takes no input and produces a copy of its own source code as its only output. The standard terms for these programs in the computability theory and computer science literature are "self-replicating programs", "self-reproducing programs", and "self-copying programs".

A quine is a fixed point of an execution environment, when the execution environment is viewed as a function transforming programs into their outputs. Quines are possible in any Turing complete programming language, as a direct consequence of Kleene's recursion theorem. For amusement, programmers sometimes attempt to develop the shortest possible quine in any given programming language.

The name "quine" was coined by Douglas Hofstadter, in his popular science book "Gödel, Escher, Bach", in honor of philosopher Willard Van Orman Quine (1908–2000), who made an extensive study of indirect self-reference, and in particular for the following paradox-producing expression, known as Quine's paradox:
"Yields falsehood when preceded by its quotation" yields falsehood when preceded by its quotation.

The idea of self-reproducing automata came from the dawn of computing, if not before. John von Neumann theorized about them in the 1940s. Later, Paul Bratley and Jean Millo's article "Computer Recreations: Self-Reproducing Automata" discussed them in 1972.
Bratley first became interested in self-reproducing programs after seeing the first known such program written in Atlas Autocode at Edinburgh in the 1960s by the University of Edinburgh lecturer and researcher Hamish Dewar.

The "download source" requirement of the Affero General Public License is based on the idea of a quine.

In general, the method used to create a quine in any programming language is to have, within the program, two pieces: (a) code used to do the actual printing and (b) data that represents the textual form of the code. The code functions by using the data to print the code (which makes sense since the data represents the textual form of the code), but it also uses the data, processed in a simple way, to print the textual representation of the data itself.

Here a very 3 smalls codes in Python3 (those lines are not from the program's code but from the output):
a='a=%s%s%s;print(a%%(chr(39),a,chr(39)))';print(a%(chr(39),a,chr(39)))
b='b={}{}{};print(b.format(chr(39),b,chr(39)))';print(b.format(chr(39),b,chr(39)))
c='c=%r;print(c%%c)';print(c%c)

The following Java code demonstrates the basic structure of a quine.
public class Quine
The source code contains a string array of itself, which is output twice, once inside quotation marks.

This code was adapted from an original post from c2.com, where the author, Jason Wilson, posted it as a minimalistic version of a Quine, without Java comments.

Some programming languages have the ability to evaluate a string as a program. Quines can take advantage of this feature. For example, this Ruby quine:

eval s="print 'eval s=';p s"

In many functional languages, including Scheme and other Lisps, and interactive languages such as APL, numbers are self-evaluating. In TI-BASIC, if the last line of a program is value returning, the returned value is displayed on the screen. Therefore, in such languages a program containing a single digit results in a 1-byte quine. Since such code does not "construct" itself, this is often considered cheating.
1
In some languages, particularly scripting languages but also C, an empty source file is a fixed point of the language, being a valid program that produces no output. Such an empty program, submitted as "the world's smallest self reproducing program", once won the "worst abuse of the rules" prize in the International Obfuscated C Code Contest. The program was not actually compiled, but used codice_1 to copy the file into another file, which could be executed to print nothing.

Other questionable techniques include making use of compiler messages; for example, in the GW-BASIC environment, entering "Syntax Error" will cause the interpreter to respond with "Syntax Error".

Quines, per definition, cannot receive "any" form of input, including reading a file, which means a quine is considered to be "cheating" if it looks at its own source code. The following shell script is not a quine:

cat $0

The quine concept can be extended to multiple levels of recursion, originating "ouroboros programs", or quine-relays. This should not be confused with Multiquines.

This Java program outputs the source for a C++ program that outputs the original Java code.
Such programs have been produced with various cycle lengths:


David Madore, creator of Unlambda, describes multiquines as follows:
"A multiquine is a set of r different programs (in r different languages — without this condition we could take them all equal to a single quine), each of which is able to print any of the r programs (including itself) according to the command line argument it is passed. (Note that cheating is not allowed: the command line arguments must not be too long — passing the full text of a program is considered cheating)."
A multiquine consisting of 2 languages (or biquine) would be a program which:

A biquine could then be seen as a set of two programs, both of which are able to print either of the two, depending on the command line argument supplied.

Theoretically, there is no limit on the number of languages in a multiquine,
a 5-part multiquine (or pentaquine) has been produced with Python, Perl, C, NewLISP, and F#
and there is also a 25-language multiquine.

A radiation-hardened quine is a quine that can have any single character removed and still produce the original program with no missing character. Of necessity, such quines are much more convoluted than ordinary quines, as is seen by the following example in Ruby:
eval='eval$q=%q(puts %q(10210/#{1 1 if 1==21}}/.i rescue##/

1 1"[13,213].max_by{|s|s.size}#"##").gsub(/\d/){["=\47eval$q=%q(#$q)#\47##\47

exit)#'##'

instance_eval='eval$q=%q(puts %q(10210/#{1 1 if 1==21}}/.i rescue##/

1 1"[13,213].max_by{|s|s.size}#"##").gsub(/\d/){["=\47eval$q=%q(#$q)#\47##\47

exit)#'##'

/#{eval eval if eval==instance_eval}}/.i rescue##/

eval eval"[eval||=9,instance_eval||=9].max_by{|s|s.size}#"##"



</doc>
<doc id="25271" url="https://en.wikipedia.org/wiki?curid=25271" title="Field of fractions">
Field of fractions

In abstract algebra, the field of fractions of an integral domain is the smallest field in which it can be embedded. The elements of the field of fractions of the integral domain formula_1 are equivalence classes (see the construction below) written as formula_2 with formula_3 and formula_4 in formula_1 and formula_6. The field of fractions of formula_1 is sometimes denoted by formula_8 or formula_9.

Mathematicians refer to this construction as the field of fractions, fraction field, field of quotients, or quotient field. All four are in common usage. The expression "quotient field" may sometimes run the risk of confusion with the quotient of a ring by an ideal, which is a quite different concept.


Let formula_1 be any integral domain. For formula_17 with formula_18, the fraction formula_19 denotes the equivalence class of pairs formula_20, where formula_20 is equivalent to formula_22 if and only if formula_23.
The "field of fractions" formula_8 is defined as the set of all such fractions formula_19.
The sum of formula_19 and formula_29 is defined as formula_30, and the product of formula_19 and formula_29 is defined as formula_33 (one checks that these are well defined).

The embedding of formula_1 in formula_8 maps each formula_36 in formula_1 to the fraction formula_38 for any nonzero formula_39 (the equivalence class is independent of the choice formula_40). This is modelled on the identity formula_41.

The field of fractions of formula_1 is characterised by the following universal property: if formula_43 is an injective ring homomorphism from formula_1 into a field formula_45, then there exists a unique ring homomorphism formula_46 which extends formula_47.

There is a categorical interpretation of this construction. Let formula_48 be the category of integral domains and injective ring maps. The functor from formula_48 to the category of fields which takes every integral domain to its fraction field and every homomorphism to the induced map on fields (which exists by the universal property) is the left adjoint of the forgetful functor from the category of fields to formula_48.

A multiplicative identity is not required for the role of the integral domain; this construction can be applied to any nonzero commutative rng formula_1 with no nonzero zero divisors. The embedding is given by formula_52 for any nonzero formula_53.

For any commutative ring formula_1 and any multiplicative set formula_55 in formula_1, the localization formula_55formula_1 is the commutative ring consisting of fractions formula_59 with formula_60 and formula_61,
where now formula_62 is equivalent to formula_63 if and only if there exists formula_64 such that formula_65.
Two special cases of this are notable:



</doc>
<doc id="25272" url="https://en.wikipedia.org/wiki?curid=25272" title="Quadratic reciprocity">
Quadratic reciprocity

In number theory, the law of quadratic reciprocity is a theorem about modular arithmetic that gives conditions for the solvability of quadratic equations modulo prime numbers. Due to its subtlety, it has many formulations, but the most standard statement is: 
.</math>

This law allows the easy calculation of any Legendre symbol, making it possible to determine whether there is an integer solution for any quadratic equation of the form formula_1 for "p" an odd prime; that is, to determine the "perfect squares" mod "p". However, this is a non-constructive result: it gives no help at all for "finding" a specific solution; for this, one uses quadratic residues.

The theorem was conjectured by Euler and Legendre and first proved by Gauss. He refers to it as the "fundamental theorem" in the "Disquisitiones Arithmeticae" and his papers, writing

Privately he referred to it as the "golden theorem." He published six proofs, and two more were found in his posthumous papers. There are now over 240 published proofs. The shortest known proof is included below, together with short proofs of the law's supplements (the Legendre symbols of -1 and 2).

Since Gauss, generalizing the reciprocity law has been a leading problem in mathematics, and has been crucial to the development of much of the machinery of modern algebra, number theory, and algebraic geometry, culminating in Artin reciprocity, class field theory, and the Langlands program. 

Quadratic reciprocity arises from certain subtle factorization patterns involving perfect square numbers. In this section, we give examples which lead to the general case. 

Consider the polynomial formula_2 and its values for formula_3 The prime factorizations of these values are given as follows:

The prime factors formula_4 dividing formula_5 are formula_6, and every prime whose final digit is formula_7 or formula_8; no primes ending in formula_9 or formula_10 ever appear. Now, formula_4 is a prime factor of some formula_12 whenever formula_13, i.e. whenever formula_14, i.e. whenever 5 is a quadratic residue modulo formula_4. This happens for formula_16 and those primes with formula_17, and note that the latter numbers formula_18 and formula_19 are precisely the quadratic residues modulo formula_20. Therefore, except for formula_21, we have that formula_20 is a quadratic residue modulo formula_4 iff formula_4 is a quadratic residue modulo formula_20.

The law of quadratic reciprocity gives a similar characterization of prime divisors of formula_26 for any prime "q", which leads to a characterization for any integer formula_27.

Let "p" be an odd prime. A number modulo "p" is a quadratic residue whenever it is congruent to a square (mod "p"); otherwise it is a quadratic non-residue. ("Quadratic" can be dropped if it is clear from the context.) Here we exclude zero as a special case. Then as a consequence of the fact that the multiplicative group of a finite field of order "p" is cyclic of order "p-1", the following statements hold:


For the avoidance of doubt, these statements do "not" hold if the modulus is not prime. 
For example, there are only 2 quadratic residues (1 and 4) in the multiplicative group modulo 15. 
Moreover although 7 and 8 are quadratic non-residues, their product 7x8 = 11 is also a quadratic non-residue, in contrast to the prime case. 

Quadratic residues are entries in the following table: 

This table is complete for odd primes less than 50. To check whether a number "m" is a quadratic residue mod one of these primes "p", find "a" ≡ "m" (mod "p") and 0 ≤ "a" < "p". If "a" is in row "p", then "m" is a residue (mod "p"); if "a" is not in row "p" of the table, then "m" is a nonresidue (mod "p").

The quadratic reciprocity law is the statement that certain patterns found in the table are true in general.

Trivially 1 is a quadratic residue for all primes. The question becomes more interesting for −1. Examining the table, we find −1 in rows 5, 13, 17, 29, 37, and 41 but not in rows 3, 7, 11, 19, 23, 31, 43 or 47. The former set of primes are all congruent to 1 modulo 4, and the latter are congruent to 3 modulo 4.

Examining the table, we find 2 in rows 7, 17, 23, 31, 41, and 47, but not in rows 3, 5, 11, 13, 19, 29, 37, or 43. The former primes are all ≡ ±1 (mod 8), and the latter are all ≡ ±3 (mod 8). This leads to

−2 is in rows 3, 11, 17, 19, 41, 43, but not in rows 5, 7, 13, 23, 29, 31, 37, or 47. The former are ≡ 1 or ≡ 3 (mod 8), and the latter are ≡ 5, 7 (mod 8).

3 is in rows 11, 13, 23, 37, and 47, but not in rows 5, 7, 17, 19, 29, 31, 41, or 43. The former are ≡ ±1 (mod 12) and the latter are all ≡ ±5 (mod 12).

−3 is in rows 7, 13, 19, 31, 37, and 43 but not in rows 5, 11, 17, 23, 29, 41, or 47. The former are ≡ 1 (mod 3) and the latter ≡ 2 (mod 3).

Since the only residue (mod 3) is 1, we see that −3 is a quadratic residue modulo every prime which is a residue modulo 3.

5 is in rows 11, 19, 29, 31, and 41 but not in rows 3, 7, 13, 17, 23, 37, 43, or 47. The former are ≡ ±1 (mod 5) and the latter are ≡ ±2 (mod 5).

Since the only residues (mod 5) are ±1, we see that 5 is a quadratic residue modulo every prime which is a residue modulo 5.

−5 is in rows 3, 7, 23, 29, 41, 43, and 47 but not in rows 11, 13, 17, 19, 31, or 37. The former are ≡ 1, 3, 7, 9 (mod 20) and the latter are ≡ 11, 13, 17, 19 (mod 20).

The observations about −3 and 5 continue to hold: −7 is a residue modulo "p" if and only if "p" is a residue modulo 7, −11 is a residue modulo "p" if and only if "p" is a residue modulo 11, 13 is a residue (mod "p") if and only if "p" is a residue modulo 13, etc. The more complicated-looking rules for the quadratic characters of 3 and −5, which depend upon congruences modulo 12 and 20 respectively, are simply the ones for −3 and 5 working with the first supplement.

The generalization of the rules for −3 and 5 is Gauss's statement of quadratic reciprocity.

Another way to organize the data is to see which primes are residues mod which other primes, as illustrated in the following table. The entry in row "p" column "q" is R if "q" is a quadratic residue (mod "p"); if it is a nonresidue the entry is N.

If the row, or the column, or both, are ≡ 1 (mod 4) the entry is blue or green; if both row and column are ≡ 3 (mod 4), it is yellow or orange.

The blue and green entries are symmetric around the diagonal: The entry for row "p", column "q" is R (resp N) if and only if the entry at row "q", column "p", is R (resp N).

The yellow and orange ones, on the other hand, are antisymmetric: The entry for row "p", column "q" is R (resp N) if and only if the entry at row "q", column "p", is N (resp R).

The reciprocity law states that these patterns hold for all "p" and "q".

Quadratic Reciprocity (Gauss's statement). If formula_32 then the congruence formula_33 is solvable if and only if formula_34 is solvable. If formula_35 then the congruence formula_33 is solvable if and only if formula_37 is solvable.

Quadratic Reciprocity (combined statement). Define formula_38. Then the congruence formula_33 is solvable if and only if formula_40 is solvable.

Quadratic Reciprocity (Legendre's statement). If "p" or "q" are congruent to 1 modulo 4, then: formula_34 is solvable if and only if formula_33 is solvable. If "p" and "q" are congruent to 3 modulo 4, then: formula_34 is solvable if and only if formula_33 is not solvable.

The last is immediately equivalent to the modern form stated in the introduction above. It is a simple exercise to prove that Legendre's and Gauss's statements are equivalent – it requires no more than the first supplement and the facts about multiplying residues and nonresidues.

The following proof, from The American Mathematical Monthly , is apparently the shortest one known. 

Let formula_45, where formula_46 and formula_47 is the Legendre symbol. Note that for an odd formula_48 and any formula_49, 
In particular, substituting formula_51 and formula_52 a nonresidue, we get formula_53, and setting formula_54, we get formula_55; and by similar reasoning, formula_56. Furthermore, formula_57, and, recalling that formula_58, formula_59 Therefore, for odd formula_48 we have formula_61 formula_62 Since formula_63, by induction for odd formula_48 formula_65. Therefore, by Euler's criterion, for an odd prime formula_27, formula_67. Now, the formula_27 cyclic shifts of a given formula_27-tuple formula_70 are distinct unless all formula_71 are equal, since the period of its repeated single-position cyclic shift divides formula_27, and so is formula_27 or 1. When they are distinct, their total contribution to the sum defining formula_74 is formula_75, which is divisible by formula_27. Therefore, modulo formula_27 (we take formula_78), formula_79 So formula_80 and formula_81 are congruent to formula_74, and thus to each other, modulo formula_27 – but they both are numbers of the form formula_84, so they are equal, which is the law of quadratic reciprocity.

The value of the Legendre symbol of formula_85 (used in the proof above) follows directly from Euler's criterion: formula_86 by Euler's criterion, but both sides of this congruence are numbers of the form formula_84, so they must be equal. 

Whether formula_88 is a quadratic residue can be concluded if we know the number of solutions of the equation formula_89 with formula_90 , which can be solved by standard methods. Namely, all its solutions where formula_91 can be grouped into octuplets of the form formula_92, and what is left are four solutions of the form formula_93 and possibly four additional solutions where formula_94 and formula_95, which exist precisely if formula_88 is a quadratic residue. That is, formula_88 is a quadratic residue precisely if the number of solutions of this equation is divisible by formula_98. And this equation can be solved in just the same way here as over the rational numbers: substitute formula_99, where we demand that formula_100 (leaving out the two solutions formula_101), then the original equation transforms into formula_102. Here formula_103 can have any value that does not make the denominator zero - for which there are formula_104 possibilities (i.e. formula_88 if formula_85 is a residue, formula_107 if not) - and also does not make formula_52 zero, which excludes one more option, formula_109. Thus there are formula_110 possibilities for formula_103, and so together with the two excluded solutions there are overall formula_112 solutions of the original equation. Therefore, formula_88 is a residue modulo formula_4 if and only if formula_98 divides formula_116. This is a reformulation of the condition stated above.

The theorem was formulated in many ways before its modern form: Euler and Legendre did not have Gauss's congruence notation, nor did Gauss have the Legendre symbol.

In this article "p" and "q" always refer to distinct positive odd primes, and "x" and "y" to unspecified integers.

Fermat proved (or claimed to have proved) a number of theorems about expressing a prime by a quadratic form:

He did not state the law of quadratic reciprocity, although the cases −1, ±2, and ±3 are easy deductions from these and other of his theorems.

He also claimed to have a proof that if the prime number "p" ends with 7, (in base 10) and the prime number "q" ends in 3, and "p" ≡ "q" ≡ 3 (mod 4), then

Euler conjectured, and Lagrange proved, that

Proving these and other statements of Fermat was one of the things that led mathematicians to the reciprocity theorem.

Translated into modern notation, Euler stated that for distinct odd primes "p" and "q":


This is equivalent to quadratic reciprocity.

He could not prove it, but he did prove the second supplement.

Fermat proved that if "p" is a prime number and "a" is an integer, 

Thus if "p" does not divide "a", using the non-obvious fact (see for example Ireland and Rosen below) that the residues modulo "p" form a field and therefore in particular the multiplicative group is cyclic, hence there can be at most two solutions to a quadratic equation: 

Legendre lets "a" and "A" represent positive primes ≡ 1 (mod 4) and "b" and "B" positive primes ≡ 3 (mod 4), and sets out a table of eight theorems that together are equivalent to quadratic reciprocity:

He says that since expressions of the form

will come up so often he will abbreviate them as:

This is now known as the Legendre symbol, and an equivalent definition is used today: for all integers "a" and all odd primes "p"

He notes that these can be combined:

A number of proofs, especially those based on Gauss's Lemma, explicitly calculate this formula.

Legendre's attempt to prove reciprocity is based on a theorem of his:

Example. Theorem I is handled by letting "a" ≡ 1 and "b" ≡ 3 (mod 4) be primes and assuming that formula_130 and, contrary the theorem, that formula_131 Then formula_132 has a solution, and taking congruences (mod 4) leads to a contradiction.

This technique doesn't work for Theorem VIII. Let "b" ≡ "B" ≡ 3 (mod 4), and assume

Then if there is another prime "p" ≡ 1 (mod 4) such that

the solvability of formula_135 leads to a contradiction (mod 4). But Legendre was unable to prove there has to be such a prime "p"; he was later able to show that all that is required is:

but he couldn't prove that either. Hilbert symbol (below) discusses how techniques based on the existence of solutions to formula_137 can be made to work.

Gauss first proves the supplementary laws. He sets the basis for induction by proving the theorem for ±3 and ±5. Noting that it is easier to state for −3 and +5 than it is for +3 or −5, he states the general theorem in the form:

Introducing the notation "a" R "b" (resp. "a" N "b") to mean "a" is a quadratic residue (resp. nonresidue) (mod "b"), and letting "a", "a"′, etc. represent positive primes ≡ 1 (mod 4) and "b", "b"′, etc. positive primes ≡ 3 (mod 4), he breaks it out into the same 8 cases as Legendre:

In the next Article he generalizes this to what are basically the rules for the Jacobi symbol (below). Letting "A", "A"′, etc. represent any (prime or composite) positive numbers ≡ 1 (mod 4) and "B", "B"′, etc. positive numbers ≡ 3 (mod 4):

All of these cases take the form "if a prime is a residue (mod a composite), then the composite is a residue or nonresidue (mod the prime), depending on the congruences (mod 4)". He proves that these follow from cases 1) - 8).

Gauss needed, and was able to prove, a lemma similar to the one Legendre needed:

The proof of quadratic reciprocity uses complete induction.

These can be combined:

A number of proofs of the theorem, especially those based on Gauss sums derive this formula. or the splitting of primes in algebraic number fields,

Note that the statements in this section are equivalent to quadratic reciprocity: if, for example, Euler's version is assumed, the Legendre-Gauss version can be deduced from it, and vice versa.

This can be proven using Gauss's lemma.

Gauss's fourth proof consists of proving this theorem (by comparing two formulas for the value of Gauss sums) and then restricting it to two primes. He then gives an example: Let "a" = 3, "b" = 5, "c" = 7, and "d" = 11. Three of these, 3, 7, and 11 ≡ 3 (mod 4), so "m" ≡ 3 (mod 4). 5×7×11 R 3; 3×7×11 R 5; 3×5×11 R 7;  and  3×5×7 N 11, so there are an odd number of nonresidues.

The Jacobi symbol is a generalization of the Legendre symbol; the main difference is that the bottom number has to be positive and odd, but does not have to be prime. If it is prime, the two symbols agree. It obeys the same rules of manipulation as the Legendre symbol. In particular

and if both numbers are positive and odd (this is sometimes called "Jacobi's reciprocity law"):

However, if the Jacobi symbol is 1 but the denominator is not a prime, it does not necessarily follow that the numerator is a quadratic residue of the denominator. Gauss's cases 9) - 14) above can be expressed in terms of Jacobi symbols:

and since "p" is prime the left hand side is a Legendre symbol, and we know whether "M" is a residue modulo "p" or not.

The formulas listed in the preceding section are true for Jacobi symbols as long as the symbols are defined. Euler's formula may be written

Example.

2 is a residue modulo the primes 7, 23 and 31:

But 2 is not a quadratic residue modulo 5, so it can't be one modulo 15. This is related to the problem Legendre had: if formula_155 then "a" is a non-residue modulo every prime in the arithmetic progression "m" + 4"a", "m" + 8"a", ..., if there "are" any primes in this series, but that wasn't proved until decades after Legendre.

Eisenstein's formula requires relative primality conditions (which are true if the numbers are prime)

The quadratic reciprocity law can be formulated in terms of the Hilbert symbol formula_159 where "a" and "b" are any two nonzero rational numbers and "v" runs over all the non-trivial absolute values of the rationals (the archimedean one and the "p"-adic absolute values for primes "p"). The Hilbert symbol formula_159 is 1 or −1. It is defined to be 1 if and only if the equation formula_161 has a solution in the completion of the rationals at "v" other than formula_162. The Hilbert reciprocity law states that formula_159, for fixed "a" and "b" and varying "v", is 1 for all but finitely many "v" and the product of formula_159 over all "v" is 1. (This formally resembles the residue theorem from complex analysis.)

The proof of Hilbert reciprocity reduces to checking a few special cases, and the non-trivial cases turn out to be equivalent to the main law and the two supplementary laws of quadratic reciprocity for the Legendre symbol. There is no kind of reciprocity in the Hilbert reciprocity law; its name simply indicates the historical source of the result in quadratic reciprocity. Unlike quadratic reciprocity, which requires sign conditions (namely positivity of the primes involved) and a special treatment of the prime 2, the Hilbert reciprocity law treats all absolute values of the rationals on an equal footing. Therefore, it is a more natural way of expressing quadratic reciprocity with a view towards generalization: the Hilbert reciprocity law extends with very few changes to all global fields and this extension can rightly be considered a generalization of quadratic reciprocity to all global fields.

The early proofs of quadratic reciprocity are relatively unilluminating. The situation changed when Gauss used Gauss sums to show that quadratic fields are subfields of cyclotomic fields, and implicitly deduced quadratic reciprocity from a reciprocity theorem for cyclotomic fields. His proof was cast in modern form by later algebraic number theorists. This proof served as a template for class field theory, which can be viewed as a vast generalization of quadratic reciprocity.

Robert Langlands formulated the Langlands program, which gives a conjectural vast generalization of class field theory. He wrote:

There are also quadratic reciprocity laws in rings other than the integers.

In his second monograph on quartic reciprocity Gauss stated quadratic reciprocity for the ring formula_165 of Gaussian integers, saying that it is a corollary of the biquadratic law in formula_166 but did not provide a proof of either theorem. Dirichlet showed that the law in formula_165 can be deduced from the law for formula_168 without using biquadratic reciprocity.

For an odd Gaussian prime formula_169 and a Gaussian integer formula_170 relatively prime to formula_171 define the quadratic character for formula_172 by:

Let formula_174 be distinct Gaussian primes where "a" and "c" are odd and "b" and "d" are even. Then

Consider the following third root of unity:

The ring of Eisenstein integers is formula_177 For an Eisenstein prime formula_178 and an Eisenstein integer formula_170 with formula_180 define the quadratic character for formula_181 by the formula

Let λ = "a" + "bω" and μ = "c" + "dω" be distinct Eisenstein primes where "a" and "c" are not divisible by 3 and "b" and "d" are divisible by 3. Eisenstein proved

The above laws are special cases of more general laws that hold for the ring of integers in any imaginary quadratic number field. Let "k" be an imaginary quadratic number field with ring of integers formula_184 For a prime ideal formula_185 with odd norm formula_186 and formula_187 define the quadratic character for formula_188 as

for an arbitrary ideal formula_190 factored into prime ideals formula_191 define

and for formula_193 define

Let formula_195 i.e. formula_196 is an integral basis for formula_184 For formula_198 with odd norm formula_199 define (ordinary) integers "a", "b", "c", "d" by the equations,

and a function

If "m" = "Nμ" and "n" = "Nν" are both odd, Herglotz proved

Also, if

Then

Let "F" be a finite field with "q" = "p" elements, where "p" is an odd prime number and "n" is positive, and let "F"["x"] be the ring of polynomials in one variable with coefficients in "F". If formula_205 and "f" is irreducible, monic, and has positive degree, define the quadratic character for "F"["x"] in the usual manner:

If formula_207 is a product of monic irreducibles let

Dedekind proved that if formula_205 are monic and have positive degrees,

The attempt to generalize quadratic reciprocity for powers higher than the second was one of the main goals that led 19th century mathematicians, including Carl Friedrich Gauss, Peter Gustav Lejeune Dirichlet, Carl Gustav Jakob Jacobi, Gotthold Eisenstein, Richard Dedekind, Ernst Kummer, and David Hilbert to the study of general algebraic number fields and their rings of integers; specifically Kummer invented ideals in order to state and prove higher reciprocity laws.

The ninth in the list of 23 unsolved problems which David Hilbert proposed to the Congress of Mathematicians in 1900 asked for the 
"Proof of the most general reciprocity law [f]or an arbitrary number field". In 1923 Emil Artin, building upon work by Philipp Furtwängler, Teiji Takagi, Helmut Hasse and others, discovered a general theorem for which all known reciprocity laws are special cases; he proved it in 1927.

The links below provide more detailed discussions of these theorems.

The "Disquisitiones Arithmeticae" has been translated (from Latin) into English and German. The German edition includes all of Gauss's papers on number theory: all the proofs of quadratic reciprocity, the determination of the sign of the Gauss sum, the investigations into biquadratic reciprocity, and unpublished notes. Footnotes referencing the "Disquisitiones Arithmeticae" are of the form "Gauss, DA, Art. "n"".

The two monographs Gauss published on biquadratic reciprocity have consecutively numbered sections: the first contains §§ 1–23 and the second §§ 24–76. Footnotes referencing these are of the form "Gauss, BQ, § "n"". 

These are in Gauss's "Werke", Vol II, pp. 65–92 and 93–148. German translations are in pp. 511–533 and 534–586 of "Untersuchungen über höhere Arithmetik."

Every textbook on elementary number theory (and quite a few on algebraic number theory) has a proof of quadratic reciprocity. Two are especially noteworthy:

Franz Lemmermeyer's "Reciprocity Laws: From Euler to Eisenstein" has "many" proofs (some in exercises) of both quadratic and higher-power reciprocity laws and a discussion of their history. Its immense bibliography includes literature citations for 196 different published proofs for the quadratic reciprocity law.

Kenneth Ireland and Michael Rosen's "A Classical Introduction to Modern Number Theory" also has many proofs of quadratic reciprocity (and many exercises), and covers the cubic and biquadratic cases as well. Exercise 13.26 (p. 202) says it all



</doc>
<doc id="25274" url="https://en.wikipedia.org/wiki?curid=25274" title="Quantum information">
Quantum information

In physics and computer science, quantum information is the information of the state of a quantum system. It is the basic entity of study in quantum information theory, and can be manipulated using quantum information processing techniques. Quantum information refers to both the technical definition in terms of Von Neumann entropy and the general computational term.

Quantum information, like classical information, can be processed using digital computers, transmitted from one location to another, manipulated with algorithms, and analyzed with computer science and mathematics. Recently, the field quantum computing has become an active research area because of the possibility to disrupt modern computation, communication, and cryptography.

Quantum information differs strongly from classical information, epitomized by the bit, in many striking and unfamiliar ways. While the fundamental unit of classical information is the bit, the most basic unit of quantum information is the qubit. Classical information is measured using Shannon entropy, while the quantum mechanical analogue is Von Neumann entropy. Given a statistical ensemble of quantum mechanical systems with the density matrix formula_1, it is given by formula_2 Many of the same entropy measures in classical information theory can also be generalized to the quantum case, such as Holevo entropy and the conditional quantum entropy.

Unlike classical digital states (which are discrete), a qubit is continuous-valued, describable by a direction on the Bloch sphere. Despite being continuously valued in this way, a qubit is the "smallest" possible unit of quantum information, as despite the qubit state being continuously-valued, it is impossible to measure the value precisely. Three famous theorems describe the limits on manipulation of quantum information. 

These theorems prove that quantum information within the universe is conserved. They open up possibilities in quantum information processing.

The state of a qubit contains all of its information. This state is frequently expressed as a vector on the Bloch sphere. This state can be changed by applying linear transformations or quantum gates to them. These unitary transformations are described as rotations on the Bloch Sphere. While classical gates correspond to the familiar operations of Boolean logic, quantum gates are physical unitary operators.


The study of all of the above topics and differences comprises quantum information theory.

Quantum mechanics is the study of how microscopic physical systems change dynamically in nature. In the field of quantum information theory, the quantum systems studied are abstracted away from any real world counterpart. A qubit might for instance physically be a photon in a linear optical quantum computer, an ion in a trapped ion quantum computer, or it might be a large collection of atoms as in a superconducting quantum computer. Regardless of the physical implementation, the limits and features of qubits implied by quantum information theory hold as all these systems are all mathematically described by the same apparatus of density matrices over the complex numbers. Another important difference with quantum mechanics is that, while quantum mechanics often studies infinite-dimensional systems such as a harmonic oscillator, quantum information theory concerns both with continuous-variable systems and finite-dimensional systems

Many journals publish research in quantum information science, although only a few are dedicated to this area. Among these are 




</doc>
<doc id="25275" url="https://en.wikipedia.org/wiki?curid=25275" title="Quinolone">
Quinolone

Quinolone may refer to:


</doc>
<doc id="25277" url="https://en.wikipedia.org/wiki?curid=25277" title="Quarterback">
Quarterback

The quarterback (commonly abbreviated "QB"), colloquially known as the "signal caller", is a position in gridiron football. Quarterbacks are members of the offensive team and line up directly behind the offensive line. In modern American football, the quarterback is usually considered the leader of the offensive team, and is often responsible for calling the play in the huddle. The quarterback also touches the ball on almost every offensive play, and is the offensive player that almost always throws forward passes. When the QB is tackled behind the line of scrimmage, it is called a sack.

In modern American football, the quarterback is usually the leader of the offense, and their successes and failures can have a significant impact on the fortunes of his team. Accordingly, the quarterback is among the most glorified, scrutinized and highest-paid positions in team sports. "Bleacher Report" describes the signing of a starting quarterback as a Catch-22, where "NFL teams cannot maintain success without excellent quarterback play. But excellent quarterback play is usually so expensive that it prevents NFL teams from maintaining success"; as a star quarterback's high salary may prevent the signing of other expensive star players as the team has to stay under the hard salary cap.

The quarterback touches the ball on almost every offensive play. Prior to each play, the quarterback will usually tell the rest of his team which play the team will run. After the team is lined up, the center will pass the ball back to the quarterback (a process called the snap). Usually on a running play, the quarterback will then hand or pitch the ball backwards to a halfback or fullback. On a passing play, the quarterback is almost always the player responsible for trying to throw the ball downfield to an eligible receiver. Additionally, the quarterback will often run with the football himself, which could be part of a designed play like the option run or quarterback sneak, or it could be an effort to avoid being sacked by the defense.

Depending on the offensive scheme by his team, the quarterback's role can vary. In systems like the triple option the quarterback will only pass the ball a few times per game, if at all, while the pass-heavy spread offense as run by schools like Texas Tech requires quarterbacks to throw the ball in most plays. The passing game is emphasized heavily in the Canadian Football League (CFL), where there are only three downs as opposed to the four downs used in American football, a larger field of play and an extra eligible receiver. Different skillsets are required of the quarterback depending upon the offensive system. Quarterbacks that perform well in a pass-heavy spread offensive system, a popular offensive scheme in the NCAA and NFHS, rarely perform well in the National Football League (NFL), as the fundamentals of the pro-style offense used in the NFL are very different from those in the spread system, while quarterbacks in Canadian football need to be able to throw the ball often and accurately. In general, quarterbacks need to have physical skills such as arm strength, mobility, and quick throwing motion, in addition to intangibles such as competitiveness, leadership, intelligence, and downfield vision.

In the NFL, quarterbacks are required to wear a uniform number between 1 and 19. In the National Collegiate Athletic Association (NCAA) and National Federation of State High School Associations (NFHS), quarterbacks are required to wear a uniform number between 1 and 49; in the NFHS, the quarterback can also wear a number between 80 and 89. In the CFL, the quarterback can wear any number from 0 to 49 and 70 to 99. Because of their numbering, quarterbacks are eligible receivers in the NCAA, NFHS, and CFL; in the NFL, quarterbacks are eligible receivers if they are not lined up directly under center.

Often compared to captains of other team sports, before the implementation of NFL team captains in 2007, the starting quarterback is usually the "de facto" team leader and well-respected player on and off the field. Since 2007, when the NFL allowed teams to designate several captains to serve as on-field leaders, the starting quarterback has usually been one of the team captains as the leader of the team's offense.

In the NFL, while the starting quarterback has no other responsibility or authority, he may, depending on the league or individual team, have various informal duties, such as participation in pre-game ceremonies, the coin toss, or other events outside the game. For instance the starting quarterback is the first player (and third person after the team owner and head coach) to be presented with the Lamar Hunt Trophy/George Halas Trophy (after winning the AFC/NFC Conference title) and the Vince Lombardi Trophy (after a Super Bowl victory). The starting quarterback of the victorious Super Bowl team is often chosen for the "I'm going to Disney World!" campaign (which includes a trip to Walt Disney World for them and their families), whether they are the Super Bowl MVP or not; examples include Joe Montana (XXIII), Trent Dilfer (XXXV), Peyton Manning (50), Tom Brady and Julian Edelman (LIII). Dilfer was chosen even though teammate Ray Lewis was the MVP of Super Bowl XXXV, due to the bad publicity from Lewis' murder trial the prior year.

Being able to rely on a quarterback is vital to team morale. San Diego Chargers safety Rodney Harrison called the 1998 season a "nightmare" because of poor play by Ryan Leaf and Craig Whelihan and, from the rookie Leaf, obnoxious behavior toward teammates. Although their 1999 season replacements Jim Harbaugh and Erik Kramer were not stars, linebacker Junior Seau said "you can't imagine the security we feel as teammates knowing we have two quarterbacks who have performed in this league and know how to handle themselves as players and as leaders".

Commentators have noted the "disproportionate importance" of the quarterback, describing it as the "most glorified -- and scrutinized -- position" in team sports. It is believed that "there is no other position in sports that 'dictates the terms' of a game the way quarterback does, whether that impact is positive or negative, as "Everybody feeds off of what the quarterback can and cannot do...Defensively, offensively, everybody reacts to what threats or non-threats the quarterback has. Everything else is secondary". "An argument can be made that quarterback is the most influential position in team sports, considering he touches the ball on virtually every offensive play of a far shorter season than baseball, basketball or hockey -- a season in which every game is vitally important". Most consistently successful NFL teams (for instance, multiple Super Bowl appearances within a short period of time) have been centered around a single starting quarterback; the one exception was the Washington Redskins under head coach Joe Gibbs who won three Super Bowls with three different starting quarterbacks from 1982 to 1991.

On a team's defense, the middle linebacker is regarded as "quarterback of the defense" and is often the defensive leader, since he must be as smart as he is athletic. The middle linebacker (MLB), sometimes known as the "Mike", is the only inside linebacker in the 4–3 scheme.

Compared to other positions in gridiron football, the backup quarterback gets considerably much less playing time than the starting quarterback. A backup quarterback, besides being dressed for a game in case of an injury to the starter, may also have additional roles such as a holder on placekicks or a punter, and often helping to prepare the defense. Backup quarterbacks typically have the career of a journeyman quarterback and have short stints with multiple teams, a notable exception being Frank Reich who backed up Jim Kelly for nine years at the Buffalo Bills. A capable backup quarterback, however, may threaten the starting quarterback's place in the team (see Platooning quarterbacks below); Aaron Rodgers was drafted by the Green Bay Packers as the eventual successor to Brett Favre, though Rodgers served in a backup role for a few years in order to develop sufficiently for the team to give him the starting job.

A quarterback controversy results when a team has two capable quarterbacks competing for the starting position. Dallas Cowboys head coach Tom Landry alternated Roger Staubach and Craig Morton on each play, sending in the quarterbacks with the play call from the sideline; Morton started in Super Bowl V which his team lost, while Staubach started the Super Bowl VI next year and won. Although Morton played most of the 1972 season due to Staubach's injury, Staubach took back the starting job when he rallied the Cowboys in a come-from-behind win in the playoffs and Morton was subsequently traded; Staubach and Morton faced each other in Super Bowl XII. Another notable quarterback controversy involved the San Francisco 49ers' who had three capable starters; Joe Montana, Steve Young, and Steve Bono. Montana suffered a season-ending injury that cost him the 1991 NFL season and was supplanted by Young. Young was injured midway though the season, but Bono held the starting job (despite Young's recovery) until Bono's own injury let Young reclaim it. Montana also missed most of the 1992 NFL season making only one appearance, then was traded away at his request where he took over as the starter for the Kansas City Chiefs; upon retirement he was succeeded by Bono as the Chiefs' starting quarterback.

In addition to their main role, quarterbacks are occasionally used in other roles. Most teams utilize a backup quarterback as their holder on placekicks. A benefit of using quarterbacks as holders is that it would be easier to pull off a fake field goal attempt, but many coaches prefer to use punters as holders because a punter will have far more time in practice sessions to work with the kicker than any quarterback would. In the Wildcat, a formation where a halfback lines up behind the center and the quarterback lines up out wide, the quarterback can be used as a receiving target or a blocker. A more rare use for a quarterback is to punt the ball himself, a play known as a quick kick. Denver Broncos quarterback John Elway was known to perform quick kicks occasionally, typically when the Broncos were facing a third-and-long situation. Philadelphia Eagles quarterback Randall Cunningham, an All-America punter in college, was also known to punt the ball occasionally, and was assigned as the team's default punter for certain situations, such as when the team was backed up inside their own five-yard line.

As Roger Staubach's back-up, Dallas Cowboys quarterback Danny White was also the team's punter, opening strategic possibilities for coach Tom Landry. Ascending the starting role upon Staubach's retirement, White held his position as the team's punter for several seasons—a double duty he performed to All-American standard at Arizona State University. White also had two touchdown receptions as a Dallas Cowboy, both from the halfback option.

If quarterbacks are uncomfortable with the formation the defense is using, they may call an audible change to their play. For example, if a quarterback receives the call to execute a running play, but he notices that the defense is ready to blitz—that is, to send additional defensive backs across the line of scrimmage in an attempt to tackle the quarterback or hurt his ability to pass—the quarterback may want to change the play. To do this, the quarterback yells a special code, like "Blue 42," or "Texas 29," which tells the offense to switch to a specific play or formation, but it all depends on the quarterback's judgment of the defense's alignment.

Quarterbacks can also "spike" (throw the football at the ground) to stop the official game clock. For example, if a team is down by a field goal with only seconds remaining, a quarterback may spike the ball to prevent the game clock from running out. This usually allows the field goal unit to come onto the field, or attempt a final "Hail Mary pass". However, if a team is winning, a quarterback can keep the clock running by kneeling after the snap. This is normally done when the opposing team has no timeouts and there is little time left in the game, as it allows a team to burn up the remaining time on the clock without risking a turnover or injury.

A dual-threat quarterback possesses the skills and physique to run with the ball if necessary. With the rise of several blitz-heavy defensive schemes and increasingly faster defensive players, the importance of a mobile quarterback has been redefined. While arm power, accuracy, and pocket presence – the ability to successfully operate from within the "pocket" formed by his blockers – are still the most important quarterback virtues, the ability to elude or run past defenders creates an additional threat that allows greater flexibility in a team's passing and running game.

Dual-threat quarterbacks have historically been more prolific at the college level. Typically, a quarterback with exceptional quickness is used in an option offense, which allows the quarterback to hand the ball off, run it himself, or pitch it to the running back following him at a distance of three yards outside and one yard behind. This type of offense forces defenders to commit to the running back up the middle, the quarterback around the end, or the running back trailing the quarterback. It is then that the quarterback has the "option" to identify which match-up is most favorable to the offense as the play unfolds and exploit that defensive weakness. In the college game, many schools employ several plays that are designed for the quarterback to run with the ball. This is much less common in professional football, except for a quarterback sneak, but there is still an emphasis on being mobile enough to escape a heavy pass rush. Historically, high-profile dual-threat quarterbacks in the NFL were uncommon, Steve Young and John Elway being among the notable exceptions, leading their teams to three and five Super Bowl appearances respectively; and Michael Vick, whose rushing ability was a rarity in the early 2000s, although he never led his team to a Super Bowl. In recent years, quarterbacks with dual-threat capabilities have become more popular. Current NFL quarterbacks considered to be dual-threats include Russell Wilson, Lamar Jackson and Josh Allen.

Some teams employ a strategy which involves the use of more than one quarterback during the course of a game. This is more common at lower levels of football, such as high school or small college, but rare in major college or professional football.

There are four circumstances in which a two-quarterback system may be used.

The first is when a team is in the process of determining which quarterback will eventually be the starter, and may choose to use each quarterback for part of the game in order to compare the performances. For instance, the Seattle Seahawks' Pete Carroll used the pre-season games in 2012 to select Russell Wilson as the starting quarterback over Matt Flynn and Tarvaris Jackson.

The second is a starter–reliever system, in which the starting quarterback splits the regular season playing time with the backup quarterback, although the former will start playoff games. This strategy is rare, and was last seen in the NFL in the "WoodStrock" combination of Don Strock and David Woodley, which took the Miami Dolphins to the Epic in Miami in 1982 and Super Bowl XVII the following year. The starter-reliever system is distinct from a one-off situation in which a starter is benched in favor of the back-up because the switch is part of the game plan (usually if the starter is playing poorly for that game), and the expectation is that the two players will assume the same roles game after game.

The third is if a coach decides that the team has two quarterbacks who are equally effective and proceeds to rotate the quarterbacks at predetermined intervals, such as after each quarter or after each series. Southern California high school football team Corona Centennial operated this model during the 2014 football season, rotating quarterbacks after every series. In a game against the Chicago Bears in the seventh week of the 1971 season, Dallas Cowboys head coach Tom Landry alternated Roger Staubach and Craig Morton on each play, sending in the quarterbacks with the play call from the sideline.

The fourth, still occasionally seen in major-college football, is the use of different quarterbacks in different game or down/distance situations. Generally this involves a running quarterback and a passing quarterback in an option or wishbone offense. In Canadian football, quarterback sneaks or other runs in short-yardage situations tend to be successful as a result of the distance between the offensive and defensive lines being one yard. Drew Tate, a quarterback for the Calgary Stampeders, was primarily used in short-yardage situations and led the CFL in rushing touchdowns during the 2014 season with ten scores as the backup to Bo Levi Mitchell. This strategy had all but disappeared from professional American football, but returned to some extent with the advent of the "wildcat" offense. There is a great debate within football circles as to the effectiveness of the so-called "two-quarterback system". Many coaches and media personnel remain skeptical of the model. 
Teams such as USC (Southern California), OSU (Oklahoma State), Northwestern, and smaller West Georgia have utilized the two-quarterback system; West Georgia, for example, uses the system due to the skill sets of its quarterbacks. Teams like these use this situation because of the advantages it gives them against defenses of the other team, so that the defense is unable to adjust to their game plan.

The quarterback position dates to the late 1800s, when American Ivy League schools playing a form of rugby union imported from the United Kingdom began to put their own spin on the game. Walter Camp, a prominent athlete and rugby player at Yale University, pushed through a change in rules at a meeting in 1880 that established a line of scrimmage and allowed for the football to be snapped to a quarterback. The change was meant to allow for teams to strategize their play more thoroughly and retain possession more easily than was possible in the chaos of a scrummage in rugby. In Camp's formulation, the "quarter-back" was the person who received a ball snapped back with another player's foot. Originally he was not allowed to run forward of the line of scrimmage:

The quarterback in this context was often called the "blocking back" as their duties usually involved blocking after the initial handoff. The "fullback" was the furthest back behind the line of scrimmage. The "halfback" was halfway between the fullback and the line of scrimmage, and the "quarter-back" was halfway between the halfback and the line of scrimmage. Hence, he was called a "quarter-back" by Walter Camp.

The requirement to stay behind the line of scrimmage was soon rescinded, but it was later re-imposed in six-man football. The exchange between the person snapping the ball (typically the center) and the quarterback was initially an awkward one because it involved a kick. At first, centers gave the ball a small boot, and then picked it up and handed it to the quarterback. By 1889, Yale center Bert Hanson was bouncing the ball on the ground to the quarterback between his legs. The following year, a rule change officially made snapping the ball using the hands between the legs legal. Several years later, Amos Alonzo Stagg at the University of Chicago invented the lift-up snap: the center passed the ball off the ground and between his legs to a standing quarterback. A similar set of changes were later adopted in Canadian football as part of the Burnside rules, a set of rules proposed by John Meldrum "Thrift" Burnside, the captain of the University of Toronto's football team.

The change from a scrummage to a "scrimmage" made it easier for teams to decide what plays they would run before the snap. At first, the captains of college teams were put in charge of play-calling, indicating with shouted codes which players would run with the ball and how the men on the line were supposed to block. Yale later used visual signals, including adjustments of the captain's knit hat, to call plays. Centers could also signal plays based on the alignment of the ball before the snap. In 1888, however, Princeton University began to have its quarterback call plays using number signals. That system caught on, and quarterbacks began to act as directors and organizers of offensive play.

Early on, quarterbacks were used in a variety of formations. Harvard's team put seven men on the line of scrimmage, with three halfbacks who alternated at quarterback and a lone fullback. Princeton put six men on the line and had one designated quarterback, while Yale used seven linemen, one quarterback and two halfbacks who lined up on either side of the fullback. This was the origin of the T-formation, an offensive set that remained in use for many decades afterward and gained popularity in professional football starting in the 1930s.

In 1906, the forward pass was legalized in American football; Canadian football did not adopt the forward pass until 1929. Despite the legalization of the forward pass, the most popular formations of the early 20th century focused mostly on the rushing game. The single-wing formation, a run-oriented offensive set, was invented by football coach Glenn "Pop" Warner around the year 1908. In the single-wing, the quarterback was positioned behind the line of scrimmage and was flanked by a tailback, fullback and wingback. He served largely as a blocking back; the tailback typically took the snap, either running forward with the ball or making a lateral pass to one of the other players in the backfield. The quarterback's job was usually to make blocks upfield to help the tailback or fullback gain yards. Passing plays were rare in the single-wing, an unbalanced power formation where four linemen lined up to one side of the center and two lined up to the other. The tailback was the focus of the offense, and was often a triple-threat man who would either pass, run or kick the ball.

Offensive play-calling continued to focus on rushing up through the 1920s, when professional leagues began to challenge the popularity of college football. In the early days of the professional National Football League (NFL), which was founded in 1920, games were largely low-scoring affairs. Two-thirds of all games in the 1920s were shutouts, and quarterbacks/tailbacks usually passed only out of desperation. In addition to a reluctance to risk turnovers by passing, various rules existed that limited the effectiveness of the forward pass: passers were required to drop back five yards behind the line of scrimmage before they could attempt a pass, and incomplete passes in the end zone resulted in a change of possession and a touchback. Additionally, the rules required the ball to be snapped from the location on the field where it was ruled dead; if a play ended with a player going out of bounds, the center had to snap the ball from the sideline, an awkward place to start a play.

Despite these constraints, player-coach Curly Lambeau of the Green Bay Packers, along with several other NFL figures of his era, was a consistent proponent of the forward pass. The Packers found success in the 1920s and 1930s using variations on the single-wing that emphasized the passing game. Packers quarterback Red Dunn and New York Giants and Brooklyn Dodgers quarterback Benny Friedman were the leading passers of their era, but passing remained a relative rarity among other teams; between 1920 and 1932, there were three times as many running plays as there were passing plays.

Early NFL quarterbacks typically were responsible for calling the team's offensive plays with signals before the snap. The use of the huddle to call plays originated with Stagg in 1896, but only began to be used regularly in college games in 1921. In the NFL, players were typically assigned numbers, as were the gaps between offensive linemen. One player, usually the quarterback, would call signals indicating which player was to run the ball and which gap he would run toward. Play-calling or any other kind of coaching from the sidelines was not permitted during this period, leaving the quarterback to devise the offensive strategy (often, the quarterback doubled as head coach during this era). Substitutions were limited, and quarterbacks often played on both offense and defense. 

The period between 1933 and 1945 was marked by numerous changes for the quarterback position. The rule requiring a quarterback/tailback to be five yards behind the line of scrimmage to pass was abolished. Hash marks were added to the field that established a limited zone between which the ball was placed before snaps, making offensive formations more flexible. Additionally, incomplete passes in the end zone were no longer counted as turnovers and touchbacks.

The single-wing continued to be in wide use throughout this, and a number of forward-passing tailbacks became stars, including Sammy Baugh of the Washington Redskins. In 1939, University of Chicago head football coach Clark Shaughnessy made modifications to the T-formation, a formation that put the quarterback behind the center and had him receive the snap directly. Shaughnessy altered the formation by having the linemen be spaced further apart, and he began having players go in motion behind the line of scrimmage before the snap to confuse defenses. These changes were picked up by Chicago Bears coach George Halas, a close friend of Shaughnessy, and they quickly caught on in the professional ranks. Utilizing the T-formation and led by quarterback Sid Luckman, the Bears reached the NFL championship game in 1940 and beat the Redskins by a score of 73–0. The blowout led other teams across the league to adopt variations on the T-formation, including the Philadelphia Eagles, Cleveland Rams and Detroit Lions. Baugh and the Redskins converted to the T-formation and continued to succeed.

Thanks in part to the emergence of the T-formation and changes in the rulebooks to liberalize the passing game, passing from the quarterback position became more common in the 1940s and as teams switched to the T-formation, passing tailbacks, such as Sammy Baugh, would line up as quarterbacks instead. Over the course of the decade, passing yards began to exceed rushing yards for the first time in the history of football. The Cleveland Browns of the late 1940s in the All-America Football Conference (AAFC), a professional league created to challenge the NFL, were one of the teams of that era that relied most on passing. Quarterback Otto Graham helped the Browns win four AAFC championships in the late 1940s in head coach Paul Brown's T-formation offense, which emphasized precision timing passes. Cleveland, along with several other AAFC teams, was absorbed by the NFL in 1950 after the dissolution of the AAFC that same year. By the end of the 1940s, all NFL teams aside from the Pittsburgh Steelers used the T-formation as their primary offensive formation.

As late as the 1960s, running plays occurred more frequently than passes. NFL quarterback Milt Plum later stated that during his career (1957-1969) passes typically only occurred on third downs and sometimes on first downs. Quarterbacks only increased in importance as rules changed to favor passing and higher scoring and as football gained popularity on television after the 1958 NFL Championship Game, often referred to as "The Greatest Game Ever Played". Early modern offenses evolved around the quarterback as a passing threat, boosted by rules changes in 1978 and 1979 that made it a penalty for defensive backs to interfere with receivers downfield and allowed offensive linemen to pass-block using their arms and open hands; the rules had limited them to blocking with their hands held to their chests. Average passing yards per game rose from 283.3 in 1977 to 408.7 in 1979.
The NFL continues to be a pass-heavy league, in part due to further rule changes that prescribed harsher penalties for hitting the quarterback and for hitting defenseless receivers as they awaited passes. Passing in wide-open offenses has also been an emphasis at the high school and college levels, and professional coaches have devised schemes to fit the talents of new generations of quarterbacks.

While quarterbacks and team captains usually called plays in football's early years, today coaches often decide which plays the offense will run. Some teams use an offensive coordinator, an assistant coach whose duties include offensive game-planning and often play-calling. In the NFL, coaches are allowed to communicate with quarterbacks and call plays using audio equipment built into the player's helmet. Quarterbacks are allowed to hear, but not talk to, their coaches until there are fifteen seconds left on the play clock. Once the quarterback receives the call, he may relay it to other players via signals or in a huddle.

Dallas Cowboys head coach Tom Landry was an early advocate of taking play calling out of the quarterback's hands. Although this remained a common practice in the NFL through the 1970s, fewer QBs were doing it by the 1980s and even Hall of Famers like Joe Montana did not call their own plays. Buffalo Bills QB Jim Kelly was one of the last to regularly call plays. Peyton Manning, formerly of the Indianapolis Colts and Denver Broncos, was the best modern example of a quarterback who called his own plays, primary using an uptempo, no-huddle-based attack. Manning had almost complete control over the offense. Former Baltimore Ravens quarterback Joe Flacco retained a high degree of control over the offense as well, particularly when running a no-huddle scheme, as does Ben Roethlisberger of the Pittsburgh Steelers.

During the 2013 season, 67 percent of NFL players were African American (blacks make up 13 percent of the US population), yet only 17 percent of quarterbacks were; 82 percent of quarterbacks were white, with just one percent of quarterbacks from other races. In 2017, the New York Giants benched longtime starter Eli Manning in favor of Geno Smith, who was declared the starter. The Giants were the last team to have never fielded a black starting QB during an NFL season.

Since the inception of the game, only two quarterbacks with known black ancestry have led their team to a Super Bowl victory: Doug Williams in 1988 and Russell Wilson, who is multiracial, in 2014.

Some black quarterbacks claim to have experienced bias towards or against them due to their race. Despite his ability to both pass and run effectively, current Houston Texans signal-caller Deshaun Watson despises being called a dual-threat quarterback because he believes the term is often used to stereotype black quarterbacks.

Achievements:

Diversity:

Strategy and related positions:


</doc>
<doc id="25278" url="https://en.wikipedia.org/wiki?curid=25278" title="Quadrilateral">
Quadrilateral

In Euclidean plane geometry, a quadrilateral is a polygon with four edges (or sides) and four vertices or corners. Sometimes, the term quadrangle is used, by analogy with triangle, and sometimes tetragon for consistency with pentagon (5-sided), hexagon (6-sided) and so on.

The word "quadrilateral" is derived from the Latin words "quadri", a variant of four, and "latus", meaning "side".

Quadrilaterals are simple (not self-intersecting) or complex (self-intersecting), also called crossed. Simple quadrilaterals are either convex or concave.

The interior angles of a simple (and planar) quadrilateral "ABCD" add up to 360 degrees of arc, that is

This is a special case of the "n"-gon interior angle sum formula ("n" − 2) × 180°.

All non-self-crossing quadrilaterals tile the plane by repeated rotation around the midpoints of their edges.

Any quadrilateral that is not self-intersecting is a simple quadrilateral.

In a convex quadrilateral, all interior angles are less than 180° and the two diagonals both lie inside the quadrilateral.



In a concave quadrilateral, one interior angle is bigger than 180° and one of the two diagonals lies outside the quadrilateral.

A self-intersecting quadrilateral is called variously a cross-quadrilateral, crossed quadrilateral, butterfly quadrilateral or bow-tie quadrilateral. In a crossed quadrilateral, the four "interior" angles on either side of the crossing (two acute and two reflex, all on the left or all on the right as the figure is traced out) add up to 720°.


The two diagonals of a convex quadrilateral are the line segments that connect opposite vertices.

The two bimedians of a convex quadrilateral are the line segments that connect the midpoints of opposite sides. They intersect at the "vertex centroid" of the quadrilateral (see Remarkable points below).

The four maltitudes of a convex quadrilateral are the perpendiculars to a side through the midpoint of the opposite side.

There are various general formulas for the area "K" of a convex quadrilateral "ABCD" with sides .

The area can be expressed in trigonometric terms as

where the lengths of the diagonals are "p" and "q" and the angle between them is "θ". In the case of an orthodiagonal quadrilateral (e.g. rhombus, square, and kite), this formula reduces to formula_3 since "θ" is 90°.

The area can be also expressed in terms of bimedians as
where the lengths of the bimedians are "m" and "n" and the angle between them is "φ".

Bretschneider's formula expresses the area in terms of the sides and two opposite angles:

where the sides in sequence are "a", "b", "c", "d", where "s" is the semiperimeter, and "A" and "C" are two (in fact, any two) opposite angles. This reduces to Brahmagupta's formula for the area of a cyclic quadrilateral when .

Another area formula in terms of the sides and angles, with angle "C" being between sides "b" and "c", and "A" being between sides "a" and "d", is

In the case of a cyclic quadrilateral, the latter formula becomes formula_7

In a parallelogram, where both pairs of opposite sides and angles are equal, this formula reduces to formula_8

Alternatively, we can write the area in terms of the sides and the intersection angle "θ" of the diagonals, so long as this angle is not 90°:

In the case of a parallelogram, the latter formula becomes formula_10

Another area formula including the sides "a", "b", "c", "d" is
where "x" is the distance between the midpoints of the diagonals and "φ" is the angle between the bimedians.

The last trigonometric area formula including the sides "a", "b", "c", "d" and the angle "α" between "a" and "b" is: 
which can also be used for the area of a concave quadrilateral (having the concave part opposite to angle "α") just changing the first sign + to - .

The following two formulas express the area in terms of the sides "a", "b", "c", "d", the semiperimeter "s", and the diagonals "p", "q":

The first reduces to Brahmagupta's formula in the cyclic quadrilateral case, since then "pq" = "ac" + "bd".

The area can also be expressed in terms of the bimedians "m", "n" and the diagonals "p", "q":

In fact, any three of the four values "m", "n", "p", and "q" suffice for determination of the area, since in any quadrilateral the four values are related by formula_17 The corresponding expressions are:

if the lengths of two bimedians and one diagonal are given, and

if the lengths of two diagonals and one bimedian are given.

The area of a quadrilateral "ABCD" can be calculated using vectors. Let vectors AC and BD form the diagonals from "A" to "C" and from "B" to "D". The area of the quadrilateral is then

which is half the magnitude of the cross product of vectors AC and BD. In two-dimensional Euclidean space, expressing vector AC as a free vector in Cartesian space equal to (x","y") and BD as (x","y"), this can be rewritten as:

In the following table it is listed if the diagonals in some of the most basic quadrilaterals bisect each other, if their diagonals are perpendicular, and if their diagonals have equal length. The list applies to the most general cases, and excludes named subsets.

"Note 1: The most general trapezoids and isosceles trapezoids do not have perpendicular diagonals, but there are infinite numbers of (non-similar) trapezoids and isosceles trapezoids that do have perpendicular diagonals and are not any other named quadrilateral."

"Note 2: In a kite, one diagonal bisects the other. The most general kite has unequal diagonals, but there is an infinite number of (non-similar) kites in which the diagonals are equal in length (and the kites are not any other named quadrilateral)."

The lengths of the diagonals in a convex quadrilateral "ABCD" can be calculated using the law of cosines on each triangle formed by one diagonal and two sides of the quadrilateral. Thus

and

Other, more symmetric formulas for the lengths of the diagonals, are

and

In any convex quadrilateral "ABCD", the sum of the squares of the four sides is equal to the sum of the squares of the two diagonals plus four times the square of the line segment connecting the midpoints of the diagonals. Thus

where "x" is the distance between the midpoints of the diagonals. This is sometimes known as Euler's quadrilateral theorem and is a generalization of the parallelogram law.

The German mathematician Carl Anton Bretschneider derived in 1842 the following generalization of Ptolemy's theorem, regarding the product of the diagonals in a convex quadrilateral

This relation can be considered to be a law of cosines for a quadrilateral. In a cyclic quadrilateral, where "A" + "C" = 180°, it reduces to "pq = ac + bd". Since cos ("A" + "C") ≥ −1, it also gives a proof of Ptolemy's inequality.

If "X" and "Y" are the feet of the normals from "B" and "D" to the diagonal "AC" = "p" in a convex quadrilateral "ABCD" with sides "a" = "AB", "b" = "BC", "c" = "CD", "d" = "DA", then

In a convex quadrilateral "ABCD" with sides "a" = "AB", "b" = "BC", "c" = "CD", "d" = "DA", and where the diagonals intersect at "E",

where "e" = "AE", "f" = "BE", "g" = "CE", and "h" = "DE".

The shape and size of a convex quadrilateral are fully determined by the lengths of its sides in sequence and of one diagonal between two specified vertices. The two diagonals "p, q" and the four side lengths "a, b, c, d" of a quadrilateral are related by the Cayley-Menger determinant, as follows:

The internal angle bisectors of a convex quadrilateral either form a cyclic quadrilateral (that is, the four intersection points of adjacent angle bisectors are concyclic) or they are concurrent. In the latter case the quadrilateral is a tangential quadrilateral.

In quadrilateral "ABCD", if the angle bisectors of "A" and "C" meet on diagonal "BD", then the angle bisectors of "B" and "D" meet on diagonal "AC".

The bimedians of a quadrilateral are the line segments connecting the midpoints of the opposite sides. The intersection of the bimedians is the centroid of the vertices of the quadrilateral.

The midpoints of the sides of any quadrilateral (convex, concave or crossed) are the vertices of a parallelogram called the Varignon parallelogram. It has the following properties:

The two bimedians in a quadrilateral and the line segment joining the midpoints of the diagonals in that quadrilateral are concurrent and are all bisected by their point of intersection.

In a convex quadrilateral with sides "a", "b", "c" and "d", the length of the bimedian that connects the midpoints of the sides "a" and "c" is

where "p" and "q" are the length of the diagonals. The length of the bimedian that connects the midpoints of the sides "b" and "d" is

Hence

This is also a corollary to the parallelogram law applied in the Varignon parallelogram.

The lengths of the bimedians can also be expressed in terms of two opposite sides and the distance "x" between the midpoints of the diagonals. This is possible when using Euler's quadrilateral theorem in the above formulas. Whence

and

Note that the two opposite sides in these formulas are not the two that the bimedian connects.

In a convex quadrilateral, there is the following dual connection between the bimedians and the diagonals:

The four angles of a simple quadrilateral "ABCD" satisfy the following identities:

and

Also,

In the last two formulas, no angle is allowed to be a right angle, since tan 90° is not defined.

If a convex quadrilateral has the consecutive sides "a", "b", "c", "d" and the diagonals "p", "q", then its area "K" satisfies

From Bretschneider's formula it directly follows that the area of a quadrilateral satisfies

with equality if and only if the quadrilateral is cyclic or degenerate such that one side is equal to the sum of the other three (it has collapsed into a line segment, so the area is zero).

The area of any quadrilateral also satisfies the inequality

Denoting the perimeter as "L", we have

with equality only in the case of a square.

The area of a convex quadrilateral also satisfies

for diagonal lengths "p" and "q", with equality if and only if the diagonals are perpendicular.

Let "a", "b", "c", "d" be the lengths of the sides of a convex quadrilateral "ABCD" with the area "K" and diagonals "AC = p", "BD = q". Then

Let "a", "b", "c", "d" be the lengths of the sides of a convex quadrilateral "ABCD" with the area "K", then the following inequality holds:

A corollary to Euler's quadrilateral theorem is the inequality

where equality holds if and only if the quadrilateral is a parallelogram.

Euler also generalized Ptolemy's theorem, which is an equality in a cyclic quadrilateral, into an inequality for a convex quadrilateral. It states that

where there is equality if and only if the quadrilateral is cyclic. This is often called Ptolemy's inequality.

In any convex quadrilateral the bimedians "m, n" and the diagonals "p, q" are related by the inequality

with equality holding if and only if the diagonals are equal. This follows directly from the quadrilateral identity formula_52

The sides "a", "b", "c", and "d" of any quadrilateral satisfy

and

Among all quadrilaterals with a given perimeter, the one with the largest area is the square. This is called the "isoperimetric theorem for quadrilaterals". It is a direct consequence of the area inequality

where "K" is the area of a convex quadrilateral with perimeter "L". Equality holds if and only if the quadrilateral is a square. The dual theorem states that of all quadrilaterals with a given area, the square has the shortest perimeter.

The quadrilateral with given side lengths that has the maximum area is the cyclic quadrilateral.

Of all convex quadrilaterals with given diagonals, the orthodiagonal quadrilateral has the largest area. This is a direct consequence of the fact that the area of a convex quadrilateral satisfies

where "θ" is the angle between the diagonals "p" and "q". Equality holds if and only if "θ" = 90°.

If "P" is an interior point in a convex quadrilateral "ABCD", then

From this inequality it follows that the point inside a quadrilateral that minimizes the sum of distances to the vertices is the intersection of the diagonals. Hence that point is the Fermat point of a convex quadrilateral.

The centre of a quadrilateral can be defined in several different ways. The "vertex centroid" comes from considering the quadrilateral as being empty but having equal masses at its vertices. The "side centroid" comes from considering the sides to have constant mass per unit length. The usual centre, called just centroid (centre of area) comes from considering the surface of the quadrilateral as having constant density. These three points are in general not all the same point.

The "vertex centroid" is the intersection of the two bimedians. As with any polygon, the "x" and "y" coordinates of the vertex centroid are the arithmetic means of the "x" and "y" coordinates of the vertices.

The "area centroid" of quadrilateral "ABCD" can be constructed in the following way. Let "G", "G", "G", "G" be the centroids of triangles "BCD", "ACD", "ABD", "ABC" respectively. Then the "area centroid" is the intersection of the lines "GG" and "GG".

In a general convex quadrilateral "ABCD", there are no natural analogies to the circumcenter and orthocenter of a triangle. But two such points can be constructed in the following way. Let "O", "O", "O", "O" be the circumcenters of triangles "BCD", "ACD", "ABD", "ABC" respectively; and denote by "H", "H", "H", "H" the orthocenters in the same triangles. Then the intersection of the lines "OO" and "OO" is called the quasicircumcenter, and the intersection of the lines "HH" and "HH" is called the "quasiorthocenter" of the convex quadrilateral. These points can be used to define an Euler line of a quadrilateral. In a convex quadrilateral, the quasiorthocenter "H", the "area centroid" "G", and the quasicircumcenter "O" are collinear in this order, and "HG" = 2"GO".

There can also be defined a "quasinine-point center" "E" as the intersection of the lines "EE" and "EE", where "E", "E", "E", "E" are the nine-point centers of triangles "BCD", "ACD", "ABD", "ABC" respectively. Then "E" is the midpoint of "OH".

Another remarkable line in a convex non-parallelogram quadrilateral is the Newton line, which connects the midpoints of the diagonals, the segment connecting these points being bisected by the vertex centroid. One more interesting line (in some sense dual to the Newton's one) is the line connecting the point of intersection of diagonals with the vertex centroid. The line is remarkable by the fact that it contains the (area) centroid. The vertex centroid divides the segment connecting the intersection of diagonals and the (area) centroid in the ratio 3:1.

For any quadrilateral "ABCD" with points "P" and "Q" the intersections of "AD" and "BC" and "AB" and "CD", respectively, the circles "(PAB), (PCD), (QAD)," and "(QBC)" pass through a common point "M", called a Miquel point.


A hierarchical taxonomy of quadrilaterals is illustrated by the figure to the right. Lower classes are special cases of higher classes they are connected to. Note that "trapezoid" here is referring to the North American definition (the British equivalent is a trapezium). Inclusive definitions are used throughout.

A non-planar quadrilateral is called a skew quadrilateral. Formulas to compute its dihedral angles from the edge lengths and the angle between two adjacent edges were derived for work on the properties of molecules such as cyclobutane that contain a "puckered" ring of four atoms. Historically the term gauche quadrilateral was also used to mean a skew quadrilateral. A skew quadrilateral together with its diagonals form a (possibly non-regular) tetrahedron, and conversely every skew quadrilateral comes from a tetrahedron where a pair of opposite edges is removed.




</doc>
<doc id="25280" url="https://en.wikipedia.org/wiki?curid=25280" title="Quantum teleportation">
Quantum teleportation

Quantum teleportation is a process in which quantum information (e.g. the exact state of an atom or photon) can be transmitted (exactly, in principle) from one location to another, with the help of classical communication and previously shared quantum entanglement between the sending and receiving location. Because it depends on classical communication, which can proceed no faster than the speed of light, it cannot be used for faster-than-light transport or communication of classical bits. While it has proven possible to teleport one or more qubits of information between two (entangled) quanta, this has not yet been achieved between anything larger than molecules.

Although the name is inspired by the teleportation commonly used in fiction, quantum teleportation is limited to the transfer of information rather than matter itself. Quantum teleportation is not a form of transportation, but of communication: it provides a way of transporting a qubit from one location to another without having to move a physical particle along with it.

The term was coined by physicist Charles Bennett. The seminal paper first expounding the idea of quantum teleportation was published by C. H. Bennett, G. Brassard, C. Crépeau, R. Jozsa, A. Peres, and W. K. Wootters in 1993. Quantum teleportation was first realized in single photons, later being demonstrated in various material systems such as atoms, ions, electrons and superconducting circuits. The latest reported record distance for quantum teleportation is by the group of Jian-Wei Pan using the Micius satellite for space-based quantum teleportation.

In matters relating to quantum or classical information theory, it is convenient to work with the simplest possible unit of information, the two-state system. In classical information, this is a bit, commonly represented using one or zero (or true or false). The quantum analog of a bit is a quantum bit, or qubit. Qubits encode a type of information, called quantum information, which differs sharply from "classical" information. For example, quantum information can be neither copied (the no-cloning theorem) nor destroyed (the no-deleting theorem).

Quantum teleportation provides a mechanism of moving a qubit from one location to another, without having to physically transport the underlying particle to which that qubit is normally attached. Much like the invention of the telegraph allowed classical bits to be transported at high speed across continents, quantum teleportation holds the promise that one day, qubits could be moved likewise. the quantum states of single photons, photon modes, single atoms, atomic ensembles, defect centers in solids, single electrons, and superconducting circuits have been employed as information bearers.

The movement of qubits does not require the movement of "things" any more than communication over the internet does: no quantum object needs to be transported, but it is necessary to communicate two classical bits per teleported qubit from the sender to the receiver. The actual teleportation protocol requires that an entangled quantum state or Bell state be created, and its two parts shared between two locations (the source and destination, or Alice and Bob). In essence, a certain kind of quantum channel between two sites must be established first, before a qubit can be moved. Teleportation also requires a classical information channel to be established, as two classical bits must be transmitted to accompany each qubit. The reason for this is that the results of the measurements must be communicated, and this must be done over ordinary classical communication channels. The need for such classical channels may, at first, seem disappointing; however, this is not unlike ordinary communications, which requires wires, radios or lasers. What's more, Bell states are most easily shared using photons from lasers, and so teleportation could be done, in principle, through open space, i.e., without the need to send the light through cables or optical fibers.

The quantum states of single atoms have been teleported. Quantum states can be encoded in various degrees of freedom of atoms. For example, qubits can be encoded in the degrees of freedom of electrons surrounding the atomic nucleus or in the degrees of freedom of the nucleus itself. It is inaccurate to say "an atom has been teleported". It is the quantum state of an atom that is teleported. Thus, performing this kind of teleportation requires a stock of atoms at the receiving site, available for having qubits imprinted on them. The importance of teleporting the nuclear state is unclear: the nuclear state does affect the atom, e.g. in hyperfine splitting, but whether such state would need to be teleported in some futuristic "practical" application is debatable.

An important aspect of quantum information theory is entanglement, which imposes statistical correlations between otherwise distinct physical systems by creating or placing two or more separate particles into a single, shared quantum state. These correlations hold even when measurements are chosen and performed independently, out of causal contact from one another, as verified in Bell test experiments. Thus, an observation resulting from a measurement choice made at one point in spacetime seems to instantaneously affect outcomes in another region, even though light hasn't yet had time to travel the distance; a conclusion seemingly at odds with special relativity (EPR paradox). However such correlations can never be used to transmit any information faster than the speed of light, a statement encapsulated in the no-communication theorem. Thus, teleportation, as a whole, can never be superluminal, as a qubit cannot be reconstructed until the accompanying classical information arrives.

Understanding quantum teleportation requires a good grounding in finite-dimensional linear algebra, Hilbert spaces and projection matrixes. A qubit is described using a two-dimensional complex number-valued vector space (a Hilbert space), which are the primary basis for the formal manipulations given below. A working knowledge of quantum mechanics is not absolutely required to understand the mathematics of quantum teleportation, although without such acquaintance, the deeper meaning of the equations may remain quite mysterious.

The prerequisites for quantum teleportation are a qubit that is to be teleported, a conventional communication channel capable of transmitting two classical bits (i.e., one of four states), and means of generating an entangled EPR pair of qubits, transporting each of these to two different locations, A and B, performing a Bell measurement on one of the EPR pair qubits, and manipulating the quantum state of the other pair. The protocol is then as follows:


It is worth to notice that the above protocol assumes that the qubits are individually addressable, that means the qubits are distinguishable and physically labeled. However, there can be situations where two identical qubits are indistinguishable due to the spatial overlap of their wave functions. Under this condition, the qubits cannot be individually controlled or measured. Nevertheless, a teleportation protocol analogous to that described above can still be (conditionally) implemented by exploiting two independently-prepared qubits, with no need of an initial EPR pair. This can be made by addressing the internal degrees of freedom of the qubits (e.g., spins or polarizations) by spatially localized measurements performed in separated regions A and B shared by the wave functions of the two indistinguishable qubits.

Work in 1998 verified the initial predictions, and the distance of teleportation was increased in August 2004 to 600 meters, using optical fiber. Subsequently, the record distance for quantum teleportation has been gradually increased to , then to , and is now , set in open air experiments in the Canary Islands, done between the two astronomical observatories of the Instituto de Astrofísica de Canarias. There has been a recent record set () using superconducting nanowire detectors that reached the distance of over optical fiber. For material systems, the record distance is .

A variant of teleportation called "open-destination" teleportation, with receivers located at multiple locations, was demonstrated in 2004 using five-photon entanglement. Teleportation of a composite state of two single qubits has also been realized. In April 2011, experimenters reported that they had demonstrated teleportation of wave packets of light up to a bandwidth of 10 MHz while preserving strongly nonclassical superposition states. In August 2013, the achievement of "fully deterministic" quantum teleportation, using a hybrid technique, was reported. On 29 May 2014, scientists announced a reliable way of transferring data by quantum teleportation. Quantum teleportation of data had been done before but with highly unreliable methods. On 26 February 2015, scientists at the University of Science and Technology of China in Hefei, led by Chao-yang Lu and Jian-Wei Pan carried out the first experiment teleporting multiple degrees of freedom of a quantum particle. They managed to teleport the quantum information from ensemble of rubidium atoms to another ensemble of rubidium atoms over a distance of using entangled photons. In 2016, researchers demonstrated quantum teleportation with two independent sources which are separated by in Hefei optical fiber network. In September 2016, researchers at the University of Calgary demonstrated quantum teleportation over the Calgary metropolitan fiber network over a distance of .

Researchers have also successfully used quantum teleportation to transmit information between clouds of gas atoms, notable because the clouds of gas are macroscopic atomic ensembles.

In 2018, physicists at Yale demonstrated a deterministic teleported CNOT operation between logically encoded qubits.

There are a variety of ways in which the teleportation protocol can be written mathematically. Some are very compact but abstract, and some are verbose but straightforward and concrete. The presentation below is of the latter form: verbose, but has the benefit of showing each quantum state simply and directly. Later sections review more compact notations.

The teleportation protocol begins with a quantum state or qubit formula_4, in Alice's possession, that she wants to convey to Bob. This qubit can be written generally, in bra–ket notation, as:

The subscript "C" above is used only to distinguish this state from "A" and "B", below.

Next, the protocol requires that Alice and Bob share a maximally entangled state. This state is fixed in advance, by mutual agreement between Alice and Bob, and can be any one of the four Bell states shown. It does not matter which one.

In the following, assume that Alice and Bob share the state formula_10
Alice obtains one of the particles in the pair, with the other going to Bob. (This is implemented by preparing the particles together and shooting them to Alice and Bob from a common source.) The subscripts "A" and "B" in the entangled state refer to Alice's or Bob's particle.

At this point, Alice has two particles ("C", the one she wants to teleport, and "A", one of the entangled pair), and Bob has one particle, "B". In the total system, the state of these three particles is given by

Alice will then make a local measurement in the Bell basis (i.e. the four Bell states) on the two particles in her possession. To make the result of her measurement clear, it is best to write the state of Alice's two qubits as superpositions of the Bell basis. This is done by using the following general identities, which are easily verified:

and

One applies these identities with "A" and "C" subscripts. The total three particle state, of "A", "B" and "C" together, thus becomes the following four-term superposition:

The above is just a change of basis on Alice's part of the system. No operation has been performed and the three particles are still in the same total state. The actual teleportation occurs when Alice measures her two qubits A,C, in the Bell basis

Experimentally, this measurement may be achieved via a series of laser pulses directed at the two particles. Given the above expression, evidently the result of Alice's (local) measurement is that the three-particle state would collapse to one of the following four states (with equal probability of obtaining each):


Alice's two particles are now entangled to each other, in one of the four Bell states, and the entanglement originally shared between Alice's and Bob's particles is now broken. Bob's particle takes on one of the four superposition states shown above. Note how Bob's qubit is now in a state that resembles the state to be teleported. The four possible states for Bob's qubit are unitary images of the state to be teleported.

The result of Alice's Bell measurement tells her which of the above four states the system is in. She can now send her result to Bob through a classical channel. Two classical bits can communicate which of the four results she obtained.

After Bob receives the message from Alice, he will know which of the four states his particle is in. Using this information, he performs a unitary operation on his particle to transform it to the desired state formula_22:


to recover the state.


to his qubit.


Teleportation is thus achieved. The above-mentioned three gates correspond to rotations of π radians (180°) about appropriate axes (X, Y and Z) in the Bloch sphere picture of a qubit.

Some remarks:

Alice's state in qubit 2 is transferred to Bob's qubit 0 using a priorly entangled pair of qubits between Alice and Bob, qubits 1 and 0.

There are a variety of different notations in use that describe the teleportation protocol. One common one is by using the notation of quantum gates. In the above derivation, the unitary transformation that is the change of basis (from the standard product basis into the Bell basis) can be written using quantum gates. Direct calculation shows that this gate is given by

where "H" is the one qubit Walsh-Hadamard gate and formula_31 is the Controlled NOT gate.

Teleportation can be applied not just to pure states, but also mixed states, that can be regarded as the state of a single subsystem of an entangled pair. The so-called entanglement swapping is a simple and illustrative example.

If Alice has a particle which is entangled with a particle owned by Bob, and Bob teleports it to Carol, then afterwards, Alice's particle is entangled with Carol's.

A more symmetric way to describe the situation is the following: Alice has one particle, Bob two, and Carol one. Alice's particle and Bob's first particle are entangled, and so are Bob's second and Carol's particle:

Now, if Bob does a projective measurement on his two particles in the Bell state basis and communicates the results to Carol, as per the teleportation scheme described above, the state of Bob's first particle can be teleported to Carol's. Although Alice and Carol never interacted with each other, their particles are now entangled.

A detailed diagrammatic derivation of entanglement swapping has been given by Bob Coecke, presented in terms of categorical quantum mechanics.

The basic teleportation protocol for a qubit described above has been generalized in several directions, in particular regarding the dimension of the system teleported and the number of parties involved (either as sender, controller, or receiver).

A generalization to formula_32-level systems (so-called qudits) is straight forward and was already discussed in the original paper by Bennett "et al.": the maximally entangled state of two qubits has to be replaced by a maximally entangled state of two qudits and the Bell measurement by a measurement defined by a maximally entangled orthonormal basis. All possible such generalizations were discussed by Werner in 2001. 
The generalization to infinite-dimensional so-called continuous-variable systems was proposed in and led to the first teleportation experiment that worked unconditionally.

The use of multipartite entangled states instead of a bipartite maximally entangled state allows for several new features: either the sender can teleport information to several receivers either sending the same state to all of them (which allows to reduce the amount of entanglement needed for the process) or teleporting multipartite states or sending a single state in such a way that the receiving parties need to cooperate to extract the information. A different way of viewing the latter setting is that some of the parties can control whether the others can teleport.

In general, mixed states ρ may be transported, and a linear transformation ω applied during teleportation, thus allowing data processing of quantum information. This is one of the foundational building blocks of quantum information processing. This is demonstrated below.

A general teleportation scheme can be described as follows. Three quantum systems are involved. System 1 is the (unknown) state "ρ" to be teleported by Alice. Systems 2 and 3 are in a maximally entangled state "ω" that are distributed to Alice and Bob, respectively. The total system is then in the state

A successful teleportation process is a LOCC quantum channel Φ that satisfies

where Tr is the partial trace operation with respect systems 1 and 2, and formula_35 denotes the composition of maps. This describes the channel in the Schrödinger picture.

Taking adjoint maps in the Heisenberg picture, the success condition becomes

for all observable "O" on Bob's system. The tensor factor in formula_37 is formula_38 while that of formula_39 is formula_40.

The proposed channel Φ can be described more explicitly. To begin teleportation, Alice performs a local measurement on the two subsystems (1 and 2) in her possession. Assume the local measurement have "effects"

If the measurement registers the "i"-th outcome, the overall state collapses to

The tensor factor in formula_43 is formula_38 while that of formula_39 is formula_40. Bob then applies a corresponding local operation Ψ"" on system 3. On the combined system, this is described by

where "Id" is the identity map on the composite system formula_48.

Therefore, the channel Φ is defined by

Notice Φ satisfies the definition of LOCC. As stated above, the teleportation is said to be successful if, for all observable "O" on Bob's system, the equality

holds. The left hand side of the equation is:

where Ψ"*" is the adjoint of Ψ"" in the Heisenberg picture. Assuming all objects are finite dimensional, this becomes

The success criterion for teleportation has the expression

A local explanation of quantum teleportation is put forward by David Deutsch and Patrick Hayden, with respect to the many-worlds interpretation of quantum mechanics. Their paper asserts that the two bits that Alice sends Bob contain "locally inaccessible information" resulting in the teleportation of the quantum state. "The ability of quantum information to flow through a classical channel "[…]", surviving decoherence, is "[…]" the basis of quantum teleportation."





</doc>
<doc id="25284" url="https://en.wikipedia.org/wiki?curid=25284" title="Qubit">
Qubit

In quantum computing, a qubit () or quantum bit (sometimes qbit) is the basic unit of quantum information—the quantum version of the classical binary bit physically realized with a two-state device. A qubit is a two-state (or two-level) quantum-mechanical system, one of the simplest quantum systems displaying the peculiarity of quantum mechanics. Examples include: the spin of the electron in which the two levels can be taken as spin up and spin down; or the polarization of a single photon in which the two states can be taken to be the vertical polarization and the horizontal polarization. In a classical system, a bit would have to be in one state or the other. However, quantum mechanics allows the qubit to be in a coherent superposition of both states simultaneously, a property which is fundamental to quantum mechanics and quantum computing.

The coining of the term "qubit" is attributed to Benjamin Schumacher. In the acknowledgments of his 1995 paper, Schumacher states that the term "qubit" was created in jest during a conversation with William Wootters. The paper describes a way of compressing states emitted by a quantum source of information so that they require fewer physical resources to store. This procedure is now known as Schumacher compression.

A binary digit, characterized as 0 and 1, is used to represent information in classical computers. A binary digit can represent up to one bit of Shannon information, where a bit is the basic unit of information.
However, in this article, the word bit is synonymous with binary digit.

In classical computer technologies, a "processed" bit is implemented by one of two levels of low DC voltage, and whilst switching from one of these two levels to the other, a so-called forbidden zone must be passed as fast as possible, as electrical voltage cannot change from one level to another "instantaneously".

There are two possible outcomes for the measurement of a qubit—usually taken to have the value "0" and "1", like a bit or binary digit. However, whereas the state of a bit can only be either 0 or 1, the general state of a qubit according to quantum mechanics can be a coherent superposition of both. Moreover, whereas a measurement of a classical bit would not disturb its state, a measurement of a qubit would destroy its coherence and irrevocably disturb the superposition state. It is possible to fully encode one bit in one qubit. However, a qubit can hold more information, e.g. up to two bits using superdense coding.

For a system of "n" components, a complete description of its state in classical physics requires only "n" bits, whereas in quantum physics it requires 2−1 complex numbers.

In quantum mechanics, the general quantum state of a qubit can be represented by a linear superposition of its two orthonormal basis states (or basis vectors). These vectors are usually denoted as
formula_1
and
formula_2. They are written in the conventional Dirac—or "bra–ket"—notation; the formula_3 and formula_4 are pronounced "ket 0" and "ket 1", respectively. These two orthonormal basis states, <math>\

In a paper entitled "Solid-state quantum memory using the P nuclear spin", published in the October 23, 2008, issue of the journal "Nature", a team of scientists from the U.K. and U.S. reported the first relatively long (1.75 seconds) and coherent transfer of a superposition state in an electron spin "processing" qubit to a nuclear spin "memory" qubit. This event can be considered the first relatively consistent quantum data storage, a vital step towards the development of quantum computing. Recently, a modification of similar systems (using charged rather than neutral donors) has dramatically extended this time, to 3 hours at very low temperatures and 39 minutes at room temperature. Room temperature preparation of a qubit based on electron spins instead of nuclear spin was also demonstrated by a team of scientists from Switzerland and Australia.




</doc>
<doc id="25286" url="https://en.wikipedia.org/wiki?curid=25286" title="Quechuan languages">
Quechuan languages

Quechua (, ; ), usually called ("people's language") in Quechuan languages, is an indigenous language family spoken by the Quechua peoples, primarily living in the Peruvian Andes and highlands of South America. Derived from a common ancestral language, it is the most widely spoken language family of indigenous peoples of the Americas, with a total of probably some 8–10 million speakers. Approximately 25% (7.7 million) of Peruvians speak a Quechuan language.
It is perhaps most widely known for being the main language family of the Inca Empire. The Spaniards encouraged its use, so Quechua ultimately survived and variants are still widely spoken today, being the co-official language of many regions and the second most spoken language in Peru.

Quechua had already expanded across wide ranges of the central Andes long before the expansion of the Inca Empire. The Inca were one among many peoples in present-day Peru who already spoke a form of Quechua. In the Cusco region, Quechua was influenced by neighboring languages such as Aymara, which caused it to develop as distinct. In similar ways, diverse dialects developed in different areas, borrowing from local languages, when the Inca Empire ruled and imposed Quechua as the official language.

After the Spanish conquest of Peru in the 16th century, Quechua continued to be used widely by the indigenous peoples as the "common language". It was officially recognized by the Spanish administration and many Spanish learned it in order to communicate with local peoples. Clergy of the Catholic Church adopted Quechua to use as the language of evangelization. Given its use by the Catholic missionaries, the range of Quechua continued to expand in some areas.

In the late 18th century, colonial officials ended administrative and religious use of Quechua, banning it from public use in Peru after the Túpac Amaru II rebellion of indigenous peoples. The Crown banned even "loyal" pro-Catholic texts in Quechua, such as Garcilaso de la Vega's "Comentarios Reales."

Despite a brief revival of the language immediately after the Latin American nations achieved independence in the 19th century, the prestige of Quechua had decreased sharply. Gradually its use declined so that it was spoken mostly by indigenous people in the more isolated and conservative rural areas. Nevertheless, in the 21st century, Quechua language speakers number 8 to 10 million people across South America, the most speakers of any indigenous language.

The oldest written records of the language are by missionary Domingo de Santo Tomás, who arrived in Peru in 1538 and learned the language from 1540. He published his "Grammatica o arte de la lengua general de los indios de los reynos del Perú" (Grammar or Art of the General Language of the Indians of the Royalty of Peru) in 1560.

As result of Inca expansion into Central Chile there were bilingual Quechua-Mapudungu Mapuche in Central Chile at the time of the Spanish arrival. It has been argued that Mapuche, Quechua and Spanish coexisted in Central Chile, with significant biligualism, during the 17th century. Quechua is the indigenous language that has influenced Chilean Spanish the most.

In 2016 the first thesis defense done in Quechua in Europe was done by Peruvian Carmen Escalante Gutiérrez at Pablo de Olavide University. A Peruvian student, Roxana Quispe Collantes of the University of San Marcos, completed and defended the first thesis in the language group in 2019; it concerned the works of poet and it was also the first non-Spanish native language thesis done at that university.

In 1975, Peru became the first country to recognize Quechua as one of its official languages. Ecuador conferred official status on the language in its 2006 constitution, and in 2009, Bolivia adopted a new constitution that recognized Quechua and several other indigenous languages as official languages of the country.

The major obstacle to the usage and teaching of Quechuan languages is the lack of written materials in the languages, such as books, newspapers, software, and magazines. The Bible has been translated into Quechua and is distributed by certain missionary groups. Quechua, along with Aymara and minor indigenous languages, remains essentially a spoken language.

In recent years, Quechua has been introduced in intercultural bilingual education (IBE) in Bolivia, Ecuador and Peru. Even in these areas, the governments are reaching only a part of the Quechua-speaking populations. Some indigenous people in each of the countries are having their children study in Spanish for the purposes of social advancement.

Radio Nacional del Perú broadcasts news and agrarian programs in Quechua for periods in the mornings.

Quechua and Spanish are now heavily intermixed in much of the Andean region, with many hundreds of Spanish loanwords in Quechua. Similarly, Quechua phrases and words are commonly used by Spanish speakers. In southern rural Bolivia, for instance, many Quechua words such as "wawa" (infant), "misi" (cat), "waska" (strap or thrashing), are as commonly used as their Spanish counterparts, even in entirely Spanish-speaking areas. Quechua has also had a profound influence on other native languages of the Americas, such as Mapuche.

The number of speakers given varies widely according to the sources. The total in "Ethnologue" 16 is 10 million, mostly based on figures published 1987–2002, but with a few dating from the 1960s. The figure for Imbabura Highland Quechua in "Ethnologue", for example, is 300,000, an estimate from 1977.

The missionary organization FEDEPI, on the other hand, estimated one million Imbabura dialect speakers (published 2006). Census figures are also problematic, due to under-reporting. The 2001 Ecuador census reports only 500,000 Quechua speakers, compared to the estimate in most linguistic sources of more than 2 million. The censuses of Peru (2007) and Bolivia (2001) are thought to be more reliable.


Additionally, there are an unknown number of speakers in emigrant communities, including Queens, New York, and Paterson, New Jersey, in the United States.

There are significant differences among the varieties of Quechua spoken in the central Peruvian highlands and the peripheral varieties of Ecuador, as well as those of southern Peru and Bolivia. They can be labeled Quechua I (or Quechua B, central) and Quechua II (or Quechua A, peripheral). Within the two groups, there are few sharp boundaries, making them dialect continua.

However, there is a secondary division in Quechua II between the grammatically simplified northern varieties of Ecuador, Quechua II-B, known there as Kichwa, and the generally more conservative varieties of the southern highlands, Quechua II-C, which include the old Inca capital of Cusco. The closeness is at least in part because of the influence of Cusco Quechua on the Ecuadorean varieties in the Inca Empire. Because Northern nobles were required to educate their children in Cusco, this was maintained as the prestige dialect in the north.

Speakers from different points within any of the three regions can generally understand one another reasonably well. There are nonetheless significant local-level differences across each. (Wanka Quechua, in particular, has several very distinctive characteristics that make the variety more difficult to understand, even for other Central Quechua speakers.) Speakers from different major regions, particularly Central or Southern Quechua, are not able to communicate effectively.

The lack of mutual intelligibility among the dialects is the basic criterion that defines Quechua not as a single language, but as a language family. The complex and progressive nature of how speech varies across the dialect continua makes it nearly impossible to differentiate discrete varieties; "Ethnologue" lists 45 varieties which are then divided into two groups; Central and Peripheral. Due to the non-intelligibility among the two groups, they are all classified as separate languages.

As a reference point, the overall degree of diversity across the family is a little less than that of the Romance or Germanic families, and more of the order of Slavic or Arabic. The greatest diversity is within Central Quechua, or Quechua I, which is believed to lie close to the homeland of the ancestral Proto-Quechua language.

Alfredo Torero devised the traditional classification, the three divisions above, plus a fourth, a northern or Peruvian branch. The latter causes complications in the classification, however, as the northern dialects (Cajamarca–Cañaris, Pacaraos, and Yauyos–Chincha) have features of both Quechua I and Quechua II, and so are difficult to assign to either.

Torero classifies them as the following:

Willem Adelaar adheres to the Quechua I / Quechua II (central/peripheral) bifurcation. But, partially following later modifications by Torero, he reassigns part of Quechua II-A to Quechua I:

Landerman (1991) does not believe a truly genetic classification is possible and divides Quechua II so that the family has four geographical–typological branches: Northern, North Peruvian, Central, and Southern. He includes Chachapoyas and Lamas in North Peruvian Quechua so Ecuadorian is synonymous with Northern Quechua.

Quechua I (Central Quechua, "Waywash") is spoken in Peru's central highlands, from the Ancash Region to Huancayo. It is the most diverse branch of Quechua, to the extent that its divisions are commonly considered different languages.

Quechua II (Peripheral Quechua, "Wamp'una" "Traveler")

This is a sampling of words in several Quechuan languages:

Quechua shares a large amount of vocabulary, and some striking structural parallels, with Aymara, and the two families have sometimes been grouped together as a "Quechumaran family". That hypothesis is generally rejected by specialists, however. The parallels are better explained by mutual influence and borrowing through intensive and long-term contact. Many Quechua–Aymara cognates are close, often closer than intra-Quechua cognates, and there is little relationship in the affixal system. 

The Puquina language of the Tiwanaku Empire is a possible source for some of the shared vocabulary between Quechua and Aymara.

A number of Quechua loanwords have entered English (also in French with minor orthographic adaptations) via Spanish, including "coca", "condor", "guano", "jerky", "llama" ("lama" in French), "pampa", "poncho", "puma", "quinine", "quinoa", "vicuña" ("vigogne" in French), and, possibly, "gaucho". The word "lagniappe" comes from the Quechuan word "yapay" ("to increase; to add"), via Spanish then Louisiana French, with the French or Spanish article "la" in front of it, "la ñapa" in Louisiana French or Creole, or "la yapa" in Spanish.

The influence on Latin American Spanish includes such borrowings as "papa" for "potato", "chuchaqui" for "hangover" in Ecuador, and diverse borrowings for "altitude sickness", in Bolivia from Quechuan "suruqch'i" to Bolivian "sorojchi", in Ecuador and Peru "soroche".

A rare instance of a Quechua word being taken into general Spanish use is given by "carpa" for "tent" (Quechua "karpa").

In Bolivia, particularly, Quechua words are used extensively even by non-Quechua speakers. These include wawa (baby, infant), ch'aki (hangover), misi (cat), juk'ucho (mouse), q'omer uchu (green pepper), jacu ("lets go"), chhiri and chhurco (curly haired), among many others. Quechua grammar also enters Bolivian Spanish, such as the use of the suffix -ri. In Bolivian Quechua, -ri is added to verbs to signify an action is performed with affection or, in the imperative, as a rough equivalent to please. In Bolivia -ri is often included in the Spanish imperative to imply "please" or to soften commands. For example, the standard "pásame" (pass me), becomes pasarime.

Quechua has borrowed a large number of Spanish words, such as "piru" (from "pero", but), "bwenu" (from "bueno", good), iskwila (from "escuela," school), waka (from "vaca," cow) and "burru" (from "burro", donkey).

At first, Spaniards referred to the language of the Inca empire as the "lengua general", the "general language". The name "quichua" was first used in 1560 by Domingo de Santo Tomás in his "Grammatica o arte de la lengua general de los indios de los reynos del Perú". It is not known what name the native speakers gave to their language before colonial times and whether it was Spaniards who called it "quechua".

There are two possible etymologies of Quechua as the name of the language. There is a possibility that the name Quechua was derived from "*qiĉ.wa", the native word which originally meant the "temperate valley" altitude ecological zone in the Andes (suitable for maize cultivation) and to its inhabitants.

Alternatively, Pedro Cieza de León and Inca Garcilaso de la Vega, the early Spanish chroniclers, mention the existence of a people called Quichua in the present Apurímac Region, and it could be inferred that their name was given to the entire language.

The Hispanicised spellings "Quechua" and "Quichua" have been used in Peru and Bolivia since the 17th century, especially after the Third Council of Lima. Today, the various local pronunciations of "Quechua Simi" include , , , and .

Another name that native speakers give to their own language is "runa simi", "language of man/people"; it also seems to have emerged during the colonial period.

The description below applies to Cusco Quechua; there are significant differences in other varieties of Quechua.

Quechua only has three vowel phonemes: and , as in Aymara (including Jaqaru). Monolingual speakers pronounce them as respectively, but Spanish realizations may also be found. When the vowels appear adjacent to uvular consonants (, , and ), they are rendered more like , and , respectively.

Voicing is not phonemic in the Quechua native vocabulary of the modern Cusco variety.

About 30% of the modern Quechua vocabulary is borrowed from Spanish, and some Spanish sounds (such as , , , ) may have become phonemic even among monolingual Quechua-speakers.

Cusco Quechua, North- and South-Bolivian Quechua are the only varieties of Quechua to have glottalized consonants, and they, along with certain kinds of Ecuadorian Kichwa, are the only varieties with aspirated consonants. Because reflexes of a given Proto-Quechua word may have different stops in neighboring dialects (Proto-Quechua "čaki" 'foot' becomes "č'aki" and "čaka" 'bridge' becomes "čaka"), they are thought to be innovations in Quechua from Aymara, borrowed independently after branching off from Proto-Quechua.

Gemination of the tap results in a trill .

Stress is penultimate in most dialects of Quechua. In some varieties, factors such as apocope of word-final vowels may cause exceptional final stress.

Quechua has been written using the Roman alphabet since the Spanish conquest of the Inca Empire. However, written Quechua is rarely used by Quechua speakers because of the lack of printed material in Quechua.

Until the 20th century, Quechua was written with a Spanish-based orthography, for example "Inca, Huayna Cápac, Collasuyo, Mama Ocllo, Viracocha, quipu, tambo, condor". This orthography is the most familiar to Spanish speakers and so has been used for most borrowings into English, which essentially always happen through Spanish.

In 1975, the Peruvian government of Juan Velasco Alvarado adopted a new orthography for Quechua. This is the system preferred by the Academia Mayor de la Lengua Quechua, which results in the following spellings of the examples listed above: "Inka, Wayna Qhapaq, Qollasuyu, Mama Oqllo, Wiraqocha, khipu, tampu, kuntur". This orthography has the following features:

In 1985, a variation of this system was adopted by the Peruvian government that uses the Quechuan three-vowel system, resulting in the following spellings: "Inka, Wayna Qhapaq, Qullasuyu, Mama Uqllu, Wiraqucha, khipu, tampu, kuntur".

The different orthographies are still highly controversial in Peru. Advocates of the traditional system believe that the new orthographies look too foreign and believe that it makes Quechua harder to learn for people who have first been exposed to written Spanish. Those who prefer the new system maintain that it better matches the phonology of Quechua, and they point to studies showing that teaching the five-vowel system to children later causes reading difficulties in Spanish.

For more on this, see Quechuan and Aymaran spelling shift.

Writers differ in the treatment of Spanish loanwords. These are sometimes adapted to the modern orthography and sometimes left as in Spanish. For instance, "I am Roberto" could be written "Robertom kani" or "Ruwirtum kani". (The "-m" is not part of the name; it is an evidential suffix, showing how the information is known: firsthand, in this case.)

The Peruvian linguist Rodolfo Cerrón Palomino has proposed an orthographic norm for all of Southern Quechua: this Standard Quechua ("el Quechua estándar" or "Hanan Runasimi") conservatively integrates features of the two widespread dialects Ayacucho Quechua and Cusco Quechua. For instance:

The Spanish-based orthography is now in conflict with Peruvian law. According to article 20 of the decree "Decreto Supremo No 004-2016-MC", which approves regulations relative to Law 29735, published in the official newspaper El Peruano on July 22, 2016, adequate spellings of the toponyms in the normalized alphabets of the indigenous languages must progressively be proposed, with the aim of standardizing the spellings used by the National Geographic Institute "(Instituto Geográfico Nacional, IGN)" The IGN implements the necessary changes on the official maps of Peru.

Quechua is an agglutinating language. Words are built up from basic roots followed by several suffixes each of which carry one meaning. All varieties of Quechua are very regular agglutinative languages, as opposed to isolating or fusional ones [Thompson]. Their normal sentence order is SOV (subject–object–verb). Their large number of suffixes changes both the overall meaning of words and their subtle shades of meaning. Notable grammatical features include bipersonal conjugation (verbs agree with both subject and object), evidentiality (indication of the source and veracity of knowledge), a set of topic particles, and suffixes indicating who benefits from an action and the speaker's attitude toward it, but some languages and varieties may lack some of the characteristics.

In Quechua, there are seven pronouns. First-person plural pronouns (equivalent to "we") may be inclusive or exclusive; which mean, respectively, that the addressee ("you") is and is not part of the "we". Quechua also adds the suffix "-kuna" to the second and third person singular pronouns "qam" and "pay" to create the plural forms, "qam-kuna" and "pay-kuna".

Adjectives in Quechua are always placed before nouns. They lack gender and number and are not declined to agree with substantives.


Noun roots accept suffixes that indicate person (defining of possession, not identity), number, and case. In general, the personal suffix precedes that of number. In the Santiago del Estero variety, however, the order is reversed. From variety to variety, suffixes may change.

Adverbs can be formed by adding "-ta" or, in some cases, "-lla" to an adjective: "allin – allinta" ("good – well"), "utqay – utqaylla" ("quick – quickly"). They are also formed by adding suffixes to demonstratives: "chay" ("that") – "chaypi" ("there"), "kay" ("this") – "kayman" ("hither").

There are several original adverbs. For Europeans, it is striking that the adverb "qhipa" means both "behind" and "future" and "ñawpa" means "ahead, in front" and "past". Local and temporal concepts of adverbs in Quechua (as well as in Aymara) are associated to each other reversely, compared to European languages. For the speakers of Quechua, we are moving backwards into the future (we cannot see it: it is unknown), facing the past (we can see it: it is remembered).

The infinitive forms (unconjugated) have the suffix "-y" ("much'a"= "kiss"; "much'a-y" = "to kiss"). These are the endings for the indicative:
The suffixes shown in the table above usually indicate the subject; the person of the object is also indicated by a suffix ("-a-" for first person and "-su-" for second person), which precedes the suffixes in the table. In such cases, the plural suffixes from the table ("-chik" and "-ku") can be used to express the number of the object rather than the subject.

Various suffixes are added to the stem to change the meaning. For example, "-chi" is a causative and "-ku" is a reflexive (example: "wañuy" = "to die"; "wañuchiy" = to kill "wañuchikuy" = "to commit suicide"); "-naku" is used for mutual action (example: "marq'ay"= "to hug"; "marq'anakuy"= "to hug each other"), and "-chka" is a progressive, used for an ongoing action (e.g., "mikhuy" = "to eat"; "mikhuchkay" = "to be eating").

Particles are indeclinable: they do not accept suffixes. They are relatively rare, but the most common are "arí" ("yes") and "mana" ("no"), although "mana" can take some suffixes, such as "-n"/"-m" ("manan"/"manam"), "-raq" ("manaraq", not yet) and "-chu" ("manachu?", or not?), to intensify the meaning. Other particles are "yaw" ("hey", "hi"), and certain loan words from Spanish, such as "piru" (from Spanish "pero" "but") and "sinuqa" (from "sino" "rather").

The Quechuan languages have three different morphemes that mark evidentiality. Evidentiality refers to a morpheme whose primary purpose is to indicate the source of information. In Quechuan languages, evidentiality is a three-term system: there are three evidential morphemes that mark varying levels of source information. The markers can apply to first, second, and third persons. The chart below depicts an example of these morphemes from Wanka Quechua:
The parentheses around the vowels indicate that the vowel can be dropped in when following an open vowel. For the sake of cohesiveness, the above forms are used to discuss the evidential morphemes. There are dialectal variations to the forms. The variations will be presented in the following descriptions.

The following sentences provide examples of the three evidentials and further discuss the meaning behind each of them.

Regional variations: In Cusco Quechua, the direct evidential presents itself as "–mi" and "–n".

The evidential "–mi" indicates that the speaker has a "strong personal conviction the veracity of the circumstance expressed." It has the basis of direct personal experience.

Wanka Quechua

I saw them with my own eyes.

In Quechuan languages, not specified by the source, the inference morpheme appears as "–ch(i), -ch(a), -chr(a)".

The "–chr(a)" evidential indicates that the utterance is an inference or form of conjecture. That inference relays the speaker's non-commitment to the truth-value of the statement. It also appears in cases such as acquiescence, irony, interrogative constructions, and first person inferences. These uses constitute nonprototypical use and will be discussed later in the "changes in meaning and other uses" section.

Wanka Quechua

I think they will probably come back.

Regional variations: It can appear as "–sh(i)" or "–s(i)" depending on the dialect.

With the use of this morpheme, the speaker "serves as a conduit through which information from another source passes." The information being related is hearsay or revelatory in nature. It also works to express the uncertainty of the speaker regarding the situation. However, it also appears in other constructions that are discussed in the "changes in meaning" section.

Wanka Quechua

(I was told) Shanti borrowed it.

Hintz discusses an interesting case of evidential behavior found in the Sihaus dialect of Ancash Quechua. The author postulates that instead of three single evidential markers, that Quechuan language contains three pairs of evidential markers.

The evidential morphemes have been referred to as markers or morphemes. The literature seems to differ on whether or not the evidential morphemes are acting as affixes or clitics, in some cases, such as Wanka Quechua, enclitics. Lefebvre and Muysken (1998) discuss this issue in terms of case but remark the line between affix and clitic is not clear. Both terms are used interchangeably throughout these sections.

Evidentials in the Quechuan languages are "second position enclitics", which usually attach to the first constituent in the sentence, as shown in this example.

Once, there were an old man and an old woman.

They can, however, also occur on a focused constituent.

It is now that Pedro is building the house.

Sometimes, the affix is described as attaching to the focus, particularly in the Tarma dialect of Yaru Quechua, but this does not hold true for all varieties of Quechua. In Huanuco Quechua, the evidentials may follow any number of topics, marked by the topic marker "–qa", and the element with the evidential must precede the main verb or be the main verb.

However, there are exceptions to that rule, and the more topics there are in a sentence, the more likely the sentence is to deviate from the usual pattern.

When she (the witch) reached the peak, God had already taken the child up into heaven.

Evidentials can be used to relay different meanings depending on the context and perform other functions. The following examples are restricted to Wanka Quechua.

The direct evidential, -mi

The direct evidential appears in wh-questions and yes/no questions. By considering the direct evidential in terms of prototypical semantics, it seems somewhat counterintuitive to have a direct evidential, basically an evidential that confirms the speaker's certainty about a topic, in a question. However, if one focuses less on the structure and more on the situation, some sense can be made. The speaker is asking the addressee for information so the speaker assumes the speaker knows the answer. That assumption is where the direct evidential comes into play. The speaker holds a certain amount of certainty that the addressee will know the answer. The speaker interprets the addressee as being in "direct relation" to the proposed content; the situation is the same as when, in regular sentences, the speaker assumes direct relation to the proposed information.

When did he come back from Huancayo?

The direct evidential affix is also seen in yes/no questions, similar to the situation with wh-questions. Floyd describes yes/no questions as being "characterized as instructions to the addressee to assert one of the propositions of a disjunction." Once again, the burden of direct evidence is being placed on the addressee, not on the speaker. The question marker in Wanka Quechua, "-chun", is derived from the negative –chu marker and the direct evidential (realized as –n in some dialects).

Is he going to Tarma?

While "–chr(a)" is usually used in an inferential context, it has some non-prototypical uses.

"Mild Exhortation"
In these constructions the evidential works to reaffirm and encourage the addressee's actions or thoughts.

Yes, tell them, "I've gone farther."

This example comes from a conversation between husband and wife, discussing the reactions of their family and friends after they have been gone for a while. The husband says he plans to stretch the truth and tell them about distant places to which he has gone, and his wife (in the example above) echoes and encourages his thoughts.

"Acquiescence"
With these, the evidential is used to highlight the speaker's assessment of inevitability of an event and acceptance of it. There is a sense of resistance, diminished enthusiasm, and disinclination in these constructions.

I suppose I'll pay you then.

This example comes from a discourse where a woman demands compensation from the man (the speaker in the example) whose pigs ruined her potatoes. He denies the pigs as being his but finally realizes he may be responsible and produces the above example.

"Interrogative"
Somewhat similar to the "–mi" evidential, the inferential evidential can be found in content questions. However, the salient difference between the uses of the evidentials in questions is that in the "–m(i)" marked questions, an answer is expected. That is not the case with "–chr(a)" marked questions.

I wonder what we will give our families when we arrive.

"Irony"
Irony in language can be a somewhat complicated topic in how it functions differently in languages, and by its semantic nature, it is already somewhat vague. For these purposes, it is suffice to say that when irony takes place in Wanka Quechua, the "–chr(a)" marker is used.
(I suppose) That's how you learn [that is the way in which you will learn].

This example comes from discourse between a father and daughter about her refusal to attend school. It can be interpreted as a genuine statement (perhaps one can learn by resisting school) or as an ironic statement (that is an absurd idea).

Aside from being used to express hearsay and revelation, this affix also has other uses.

"Folktales, myths, and legends"

Because folktales, myths, and legends are, in essence, reported speech, it follows that the hearsay marker would be used with them. Many of these types of stories are passed down through generations, furthering this aspect of reported speech. A difference between simple hearsay and folktales can be seen in the frequency of the "–sh(i)" marker. In normal conversation using reported speech, the marker is used less, to avoid redundancy.

"Riddles"

Riddles are somewhat similar to myths and folktales in that their nature is to be passed by word of mouth.

In certain grammatical structures, the evidential marker does not appear at all. In all Quechuan languages the evidential will not appear in a dependent clause. Sadly, no example was given to depict this omission.
Omissions occur in Quechua. The sentence is understood to have the same evidentiality as the other sentences in the context. Quechuan speakers vary as to how much they omit evidentials, but they occur only in connected speech.

An interesting contrast to omission of evidentials is overuse of evidentials. If a speaker uses evidentials too much with no reason, competence is brought into question. For example, the overuse of –m(i) could lead others to believe that the speaker is not a native speaker or, in some extreme cases, that one is mentally ill.

By using evidentials, the Quechua culture has certain assumptions about the information being relayed. Those who do not abide by the cultural customs should not be trusted. A passage from Weber (1986) summarizes them nicely below:

Evidentials also show that being precise and stating the source of one's information is extremely important in the language and the culture. Failure to use them correctly can lead to diminished standing in the community. Speakers are aware of the evidentials and even use proverbs to teach children the importance of being precise and truthful. Precision and information source are of the utmost importance. They are a powerful and resourceful method of human communication.

Although the body of literature in Quechua is not as sizable as its historical and current prominence would suggest, it is nevertheless not negligible.

As in the case of the pre-Columbian Mesoamerica, there are a number of surviving Andean documents in the local language that were written down in Latin characters after the European conquest, but they express, to a great extent, the culture of pre-Conquest times. That type of Quechua literature is somewhat scantier, but nevertheless significant. It includes the so-called Huarochirí Manuscript (1598), describing the mythology and religion of the valley of Huarochirí as well as Quechua poems quoted within the Spanish-language texts of some chronicles dealing with the pre-Conquest period. There are a number of anonymous or signed Quechua dramas dating from the post-conquest period (starting from the 17th century), some of which deal with the Inca era, while most are on religious topics and of European inspiration. The most famous dramas is "Ollantay" and the plays describing the death of Atahualpa. For example, Juan de Espinosa Medrano wrote several dramas in the language. Poems in Quechua were also composed during the colonial period.

There is at least one Quechuan version of the Bible.

Dramas and poems continued to be written in the 19th and especially in 20th centuries as well; in addition, in the 20th century and more recently, more prose has been published. However, few literary forms were made present in the 19th century as European influences limited literary criticism. While some of that literature consists of original compositions (poems and dramas), the bulk of 20th century Quechua literature consists of traditional folk stories and oral narratives. Johnny Payne has translated two sets of Quechua oral short stories, one into Spanish and the other into English.

Demetrio Túpac Yupanqui wrote a Quechuan version of "Don Quixote", under the title "Yachay sapa wiraqucha dun Qvixote Manchamantan".

A news broadcast in Quechua, "Ñuqanchik" (all of us), began in Peru in 2016.

Many Andean musicians write and sing in their native languages, including Quechua and Aymara. Notable musical groups are Los Kjarkas, Kala Marka, J'acha Mallku, Savia Andina, Wayna Picchu, Wara, Alborada, Uchpa and many others.




</doc>
<doc id="25291" url="https://en.wikipedia.org/wiki?curid=25291" title="Protein quaternary structure">
Protein quaternary structure

Protein quaternary structure is the number and arrangement of multiple folded protein subunits in a multi-subunit complex. It includes organisations from simple dimers to large homooligomers and complexes with defined or variable numbers of subunits. It can also refer to biomolecular complexes of proteins with nucleic acids and other cofactors.

Many proteins are actually assemblies of multiple polypeptide chains. The quaternary structure refers to the number and arrangement of the protein subunits with respect to one another. Examples of proteins with quaternary structure include hemoglobin, DNA polymerase, and ion channels.

Enzymes composed of subunits with diverse functions are sometimes called holoenzymes, in which some parts may be known as regulatory subunits and the functional core is known as the catalytic subunit. Other assemblies referred to instead as multiprotein complexes also possess quaternary structure. Examples include nucleosomes and microtubules. Changes in quaternary structure can occur through conformational changes within individual subunits or through reorientation of the subunits relative to each other. It is through such changes, which underlie cooperativity and allostery in "multimeric" enzymes, that many proteins undergo regulation and perform their physiological function.

The above definition follows a classical approach to biochemistry, established at times when the distinction between a protein and a functional, proteinaceous unit was difficult to elucidate. More recently, people refer to protein–protein interaction when discussing quaternary structure of proteins and consider all assemblies of proteins as protein complexes.

The number of subunits in an oligomeric complex is described using names that end in -mer (Greek for "part, subunit"). Formal and Greco-Latinate names are generally used for the first ten types and can be used for up to twenty subunits, whereas higher order complexes are usually described by the number of subunits, followed by -meric.

Although complexes higher than octamers are rarely observed for most proteins, there are some important exceptions. Viral capsids are often composed of multiples of 60 proteins. Several molecular machines are also found in the cell, such as the proteasome (four heptameric rings = 28 subunits), the transcription complex and the spliceosome. The ribosome is probably the largest molecular machine, and is composed of many RNA and protein molecules.

In some cases, proteins form complexes that then assemble into even larger complexes. In such cases, one uses the nomenclature, e.g., "dimer of dimers" or "trimer of dimers", to suggest that the complex might dissociate into smaller sub-complexes before dissociating into monomers.

Protein quaternary structure can be determined using a variety of experimental techniques that require a sample of protein in a variety of experimental conditions. The experiments often provide an estimate of the mass of the native protein and, together with knowledge of the masses and/or stoichiometry of the subunits, allow the quaternary structure to be predicted with a given accuracy. It is not always possible to obtain a precise determination of the subunit composition for a variety of reasons.

The number of subunits in a protein complex can often be determined by measuring the hydrodynamic molecular volume or mass of the intact complex, which requires native solution conditions. For "folded" proteins, the mass can be inferred from its volume using the partial specific volume of 0.73 ml/g. However, volume measurements are less certain than mass measurements, since "unfolded" proteins appear to have a much larger volume than folded proteins; additional experiments are required to determine whether a protein is unfolded or has formed an oligomer.

Some bioinformatics methods were developed for predicting the quaternary structural attributes of proteins based on their sequence information by using various modes of pseudo amino acid composition (see, e.g., refs.).




Methods that measure the mass or volume under unfolding conditions (such as 
MALDI-TOF mass spectrometry and SDS-PAGE) are generally not useful, since non-native conditions usually cause the complex to dissociate into monomers. However, these may sometimes be applicable; for example, the experimenter may apply SDS-PAGE after first treating the intact complex with chemical cross-link reagents.

Proteins are capable of forming very tight complexes. For example, ribonuclease inhibitor binds to ribonuclease A with a roughly 20 fM dissociation constant. Other proteins have evolved to bind specifically to unusual moieties on another protein, e.g., biotin groups (avidin), phosphorylated tyrosines (SH2 domains) or proline-rich segments (SH3 domains). Protein-protein interactions can be engineered to favor certain oligomerization states.




</doc>
<doc id="25292" url="https://en.wikipedia.org/wiki?curid=25292" title="Quest for Glory">
Quest for Glory

Quest for Glory is a series of hybrid adventure/role-playing video games, which were designed by Corey and Lori Ann Cole. The series was created in the Sierra Creative Interpreter, a toolset developed at Sierra specifically to assist with adventure game development. The series combines humor, puzzle elements, themes and characters borrowed from various legends, puns, and memorable characters, creating a 5-part series in the Sierra stable.

The series was originally titled "Hero's Quest". However, Sierra failed to trademark the name. The Milton Bradley Company successfully trademarked an electronic version of their unrelated joint Games Workshop board game, "HeroQuest", which forced Sierra to change the series' title to "Quest for Glory". This decision meant that all future games in the series (as well as newer releases of "Hero's Quest I") used the new name.

Lori Cole pitched Quest for Glory to Sierra as a: "rich, narrative-driven, role-playing experience".

The series consisted of five games, each of which followed directly upon the events of the last. New games frequently referred to previous entries in the series, often in the form of cameos by recurring characters. The objective of the series is to transform the player character from an average adventurer to a hero by completing non-linear quests.

The game also was revolutionary in its character import system. This allowed players to import their individual character, including the skills and wealth s/he had acquired, from one game to the next.

Hybrids by their gameplay and themes, the games feature serious stories leavened with humor throughout. There are real dangers to face, and true heroic feats to perform, but silly details and overtones creep in (when the drama of adventuring does not force them out). Cheap word play is particularly frequent, to the point that the second game's ending refers to itself as the hero's "latest set of adventures and miserable puns."

The games have recurring story elements. For example, each installment in the series requires the player to create a dispel potion.

The games include a number of easter eggs, including a number of allusions to other Sierra games. For example, if a player types "pick nose" in the first game, (or clicks the lockpick icon on the player in the new version), if their lock-picking skill is high enough, the game responds: "Success! You now have an open nose". If the skill is too low, the player could insert the lock pick too far, killing himself. Another example is Dr. Cranium, an allusion to "The Castle of Dr. Brain", in the fourth game.

Each game draws its inspiration from a different culture and mythology: (in order, Germanic/fairy tale; Middle Eastern/Arabian Nights; Egyptian/African; Slavic folklore; and finally Greco-Mediterranean) with the hero facing increasingly powerful opponents with help from characters who become more familiar from game to game.

Each game varies somewhat from the tradition it is derived from; for example, Baba Yaga, a character borrowed from Slavic folklore, appears in the first game which is based on German mythology. The second game, which uses Middle Eastern folklore, introduces several Arab and African-themed characters who reappear in the third game based on Egyptian mythology. Characters from every game and genre in the series reappear in the fourth and fifth games. In addition to deviating from the player's expectations of the culture represented in each game, the series also includes a number of intentional anachronisms, such as the pizza-loving, mad scientists in the later games.

Many CRPG enthusiasts consider the "Quest for Glory" series to be among the best in the genre, and the series is lauded for its non-linearity. The games are notable for blending the mechanics of adventure video games and roleplaying video games, their unique tone which combines pathos and humour, and the game systems which were ahead of their time, such as day-night cycles, non-playable characters which adhered to their own schedules within the games, and character improvement through both skill practice and point investiture. The website Polygon and the Kotaku blog have characterised the game as a precursor to modern day RPGs. Fraser Brown of the Destructoid blog considers the games: "one of the greatest adventure series of all time".

Rowan Kaizer of the blog Engadget credits the games' hybrid adventure and roleplaying systems for the series' success. "The binary succeed/fail form of adventure game puzzles tended to either make those games too easy or too hard," he wrote, "But most puzzles in "Quest For Glory" involved some kind of skill check for your hero. This meant that you could succeed at most challenges by practicing or exploring, instead of getting stuck on bizarre item-combination puzzles".

The first four games are hybrid Adventure/Role playing video games with real-time combat, while the fifth game switches to the Action/RPG genre.

The gameplay standards established in earlier Sierra adventure games are enhanced by the player's ability to choose his character's career path from among the three traditional role-playing game backgrounds: fighter, magic-user/wizard and thief. Further variation is added by the ability to customize the Hero's abilities, including the option of selecting skills normally reserved for another character class, leading to unique combinations often referred to as "hybrid characters". During the second or third games, a character can be initiated as a Paladin by performing honorable actions, changing his class and abilities, and receiving a unique sword. This applies when the character is exported into later games. Any character that finishes any game in the series (except "Dragon Fire", the last in the series) can be exported to a more recent game ("Shadows of Darkness" has a glitch which allows one to import characters from the same game), keeping the character's statistics and parts of its inventory. If the character received the paladin sword, he would keep the magic sword (Soulforge or Piotyr's sword) and special paladin magic abilities. A character imported into a later game in the series from any other game can be assigned any character class, including Paladin.

Each career path has its own strengths and weaknesses, and scenarios unique to the class because of the skills associated with it. Each class also has its own distinct way to solve various in-game puzzles, which encourage replay: some puzzles have up to four different solutions. For instance, if a door is closed, instead of lockpicking or casting an open spell, the fighter can simply knock down the door. The magic user and the thief are both non-confrontational characters, as they lack the close range ability of the fighter, but are better able to attack from a distance, using daggers or spells. An example of these separate paths can be seen early in the first game. A gold ring belonging to the healer rests in a nest on top of a tree; fighters might make it fall by hurling rocks, thieves may want to climb the tree, while a magic user can simply cast the fetch spell to retrieve the nest, and then, while the fighter and magic user return the ring for a reward, the thief can choose between returning or selling the same ring in the thieves' guild (which is not available for those not possessing the "thieving" skills). It is also possible to build, over the course of several games, a character that has points in every skill in the game, and can therefore perform nearly every task.

Each character class features special abilities unique to that class, as well as a shared set of attributes which can be developed by performing tasks and completing quests. In general, for a particular game the maximum value which can be reached for an ability is 100*[the number of that game]. "Quest for Glory V" allows stat bonuses which can push an attribute over the maximum and lets certain classes raise certain attributes beyond the normal limits. "Quest for Glory V" also features special kinds of equipment which lower some stats while raising others. At the beginning of each game, the player may assign points to certain attributes, and certain classes only have specific attributes enabled, although skills can be added for an extra cost.

General attributes influence all characters' classes and how they interact with objects and other people in the game; high values in strength allows movement of heavier objects and communication helps with bargaining goods with sellers. These attributes are changed by performing actions related to the skill; climbing a tree eventually increases the skill value in climb, running increases vitality, and so on. There are also complementing skills which are only of associated with some classes; parry (the ability to block a blow with the sword), for instance, is mainly used by fighters and paladins, lock picking and sneaking thief's hobby, and the ability to cast magic spells is usually associated with magic user.

Vital statistics are depleted by performing some actions. Health, (determined by strength and vitality), determines the hit points of the character, which decreases when the player is attacked or harms himself. Stamina, (based on agility and vitality), limits the number of actions (exercise, fighting, running, etc.) the character is able to perform before needing rest or risking injury. Mana is only required by characters with skill in magic, and is calculated according to the character's intelligence and magic attributes.

Puzzle and Experience points only show the development of the player and his progress in the game, though in the first game also affects the kind of random encounters a player faces, as some monsters only appear after a certain level of experience is reached.

In the valley barony of Spielburg, the evil ogress Baba Yaga has cursed the land and the baron who tried to drive her off. His children have disappeared, while the land is ravaged by monsters and brigands. The Valley of Spielburg is in need of a Hero able to solve these problems.

The original game was released in 1989 while a VGA remake was released in 1992.

"Quest for Glory II: Trial by Fire" takes place in the land of Shapeir, in the world of Gloriana. Directly following from the events of the first game, the newly proclaimed Hero of Spielburg travels by flying carpet with his friends Abdulla Doo, Shameen and Shema to the desert city of Shapeir. The city is threatened by magical elementals, while the Emir Arus al-Din of Shapeir's sister city Raseir is missing and his city fallen under tyranny.

"Quest for Glory II" is the only game in the series not to have originated or have been remade beyond the EGA graphics engine by Sierra, but AGD Interactive released a VGA fan remake of the game using the Adventure Game Studio engine on August 24, 2008.

Rakeesh the Paladin brings the Hero (and Prince of Shapeir) along with Uhura and her son Simba to his homeland, the town of Tarna in a jungle and savannah country called Fricana that resembles central African ecosystems.

Tarna is on the brink of war; the Simbani, the tribe of Uhura, are ready to do battle with the Leopardmen. Each tribe has stolen a sacred relic from the other, and both refuse to return it until the other side does. The Hero must prevent the war then thwart a demon who may be loosed upon the world.

Drawn without warning from his victory in Fricana, the Hero arrives without equipment or explanation in the middle of the hazardous Dark One Caves in the distant land of Mordavia. While struggling to survive in this land plagued with undead, the Hero must prevent a dark power from summoning eternal darkness into the world.

Erasmus introduces the player character, the Hero, to the Greece-like kingdom of Silmaria, whose king was recently assassinated. Thus, the traditional Rites of Rulership are due to commence, and the victor will be crowned king. The Hero enters the contest with the assistance of Erasmus, Rakeesh, and many old friends from previous entries in the series. The Hero competes against competitors, including the Silmarian guard Kokeeno Pookameeso, the warlord Magnum Opus, the hulking Gort, and the warrior Elsa Von Spielburg.


Originally, the series was to be a tetralogy, consisting of 4 games, with the following themes and cycles: the 4 cardinal directions, the 4 classical elements, the 4 seasons and 4 different mythologies.

This is what the creators originally had in mind:
However, when "" was designed, it was thought that it would be too difficult for the hero to go straight from Shapeir to Mordavia and defeat the Dark One. To solve the problem, a new game, "", was inserted into the canon, and resulting in a renumbering of the series. Evidence for this can be found in the end of "": the player is told that the next game will be "" and a fanged vampiric moon is shown, to hint at the next game's theme.

The developers discussed this in the Fall 1992 issue of Sierra's "InterAction" magazine, and an online chat room:
Somewhere between finishing "Trial by Fire" and cranking up the design process for "Shadows of Darkness", the husband-and-wife team realized a fifth chapter would have to be added to bridge the games. That chapter became "Wages of War".

The concept of seasons in the games represents the maturation of the Hero as he moves from story to story. It's a critical component in a series that – from the very beginning – was designed to be a defined quartet of stories, representing an overall saga with a distinct beginning, middle, and end.

In the first episode, the player is a new graduate of the Famous Adventurer's Correspondence School, ready to venture out into the springtime of his career and build a rep. It's a light-hearted, exhilarating journey into the unknown that can be replayed three times with three distinct outlooks at puzzle-solving.

In the second chapter – "Trial by Fire" – the Hero enters the summer of his experience, facing more difficult challenges with more highly developed skills. While the episode is more serious and dangerous than its predecessor, it retains the enchanting mixture of fantasy, challenge, and humor that made the first game a hit with so many fans.

Of all the reasons Lori and Corey found for creating a bridge between "Trial by Fire" and "Shadows of Darkenss", the most compelling was the feeling that the Hero character simply hadn't matured enough to face the very grim challenges awaiting him in Transylvania.

Along with the Hero, several recurring characters appear and re-appear throughout the series including: Rakeesh Sah Tarna, Baba Yaga, Abdullah Doo, Elsa von Spielburg, the evil Ad Avis, and others.

The fictional world in which the Quest for Glory series takes place includes the town of Spielburg (based on German folklore), the desert city of Shapeir (based on the Arabia of "One Thousand and One Nights"), the jungle city of Tarna (based on African mythology, especially Egypt), the hamlet of Mordavia (based on Slavic mythology) and Silmaria (based on Greek mythology). Adventures, monsters and story of the games are usually drawn from legends of the respective mythology on which a title is based, although there are several cross-over exceptions, like the Eastern European Baba Yaga also appearing in the first game, which is distinctly German.


</doc>
<doc id="25293" url="https://en.wikipedia.org/wiki?curid=25293" title="Quango">
Quango

A quango or QUANGO (less often QuANGO or QANGO) is a quasi-autonomous non-governmental organisation. It is typically an organisation to which a government has devolved power, but which is still partly controlled and/or financed by government bodies. As its name suggests, a quango is a hybrid form of organization, with elements of both non-government organizations (NGOs) and public sector bodies. The concept is most often applied in the United Kingdom and, to a lesser degree, Australia, Canada, Ireland, New Zealand, the United States, and other English-speaking countries. 

In the UK, the term quango covers different "arm's-length" government bodies, including "non-departmental public bodies", non-ministerial government departments, and executive agencies. One UK example is the Forestry Commission, which is a non-ministerial government department responsible for forestry in England. The term has spawned the derivative quangocrat; the Taxpayers' Alliance faulted a majority of them for not making declarations of political activity.

The term "quasi-autonomous non-governmental organisation" was created in 1967 by Alan Pifer of the US-based Carnegie Foundation, in an essay on the independence and accountability of public-funded bodies that are incorporated in the private sector. This essay got the attention of David Howell, a Conservative M.P. in Britain, who then organized an Anglo-American project with Pifer, to examine the pros and cons of such enterprises. The lengthy term was shortened to the acronym (later lowercased quango) by a British participant to the joint project, Anthony Barker, during one of the conferences on the subject.

It describes an ostensibly non-governmental organisation performing governmental functions, often in receipt of funding or other support from government, while mainstream NGOs mostly get their donations or funds from the public and other organisations that support their cause. Numerous quangos were created from the 1980s onwards. Examples in the United Kingdom include those engaged in the regulation of various commercial and service sectors, such as the Water Services Regulation Authority.

An essential feature of a quango in the original definition was that it should not be a formal part of the state structure. The term was then extended to apply to a range of organisations, such as executive agencies providing (from 1988) health, education and other services. Particularly in the UK, this occurred in a polemical atmosphere in which it was alleged that proliferation of such bodies was undesirable and should be reversed (see below). This spawned the related acronym "qualgo", a 'quasi-autonomous "local" government organisation'.

The less contentious term non-departmental public body (NDPB) is often employed to identify numerous organisations with devolved governmental responsibilities. The UK government's definition in 1997 of a non-departmental public body or quango was:
"The Times" has accused quangos of bureaucratic waste and excess. In 2005, Dan Lewis, author of "The Essential Guide to Quangos", claimed that the UK had 529 quangos, many of which were useless and duplicated the work of others.

Quangos are filled with appointed members. This means, unlike governmental bodies, members of quangos do not need to seek re-election. This is seen as a major criticism in liberal democracy as members of quangos have not been legitimised by the electorate, but have governmental power and influence. They also do not have the same level of accountability as elected officials, worsened by the lack of media coverage of their work.

In 2006, there were 832 quangos in Ireland - 482 at national and 350 at local level - with a total of 5,784 individual appointees and a combined annual budget of €13 billion.

The Irish majority party, Fine Gael, had promised to eliminate 145 quangos should they be the governing party in the 2016 election. Since coming to power they have reduced the overall number of quangos by 17. This reduction also included agencies which the former government had already planned to remove.

Despite a 'commitment' from the 1979 Conservative party to curb the growth of unelected bodies, their numbers grew rapidly through their time in power throughout the 80s. This was criticised across the UK as those appointed were often connected with, or were sympathetic to the Conservative party. This was only an issue for the opposing Labour party until they came in to power and appointed their own biased members.

The Cabinet Office 2009 report on non-departmental public bodies found that there are 766 NDPBs sponsored by the UK government.
The number has been falling: there were 790 in 2008 and 827 in 2007. The number of NDPBs has fallen by over 10% since 1997. Staffing and expenditure of NDPBs have increased. They employed 111,000 people in 2009 and spent £46.5 billion, of which £38.4 billion was directly funded by the Government.

Since the coalition government of Conservatives and Liberal Democrats was formed in May 2010, numerous NDPBs have been abolished under Conservative plans to reduce the overall budget deficit by reducing the size of the public sector. As of the end of July 2010, the government had abolished at least 80 NDPBs and warned many others that they faced mergers or deep cuts. In September 2010, "The Telegraph" published a leaked Cabinet Office list suggesting that a further 94 could be abolished, while four would be privatised and 129 merged. In August 2012, Cabinet Office minister Francis Maude said the government was on course to abolish 204 public bodies by 2015, and said this would create a net saving of at least £2.6 billion.

Use of the term quango is less common and therefore more controversial in the United States due to their commitment to limited government and electoral accountability. However, Paul Krugman has stated that the US Federal Reserve is, effectively, "what the British call a quango... Its complex structure divides power between the federal government and the private banks that are its members, and in effect gives substantial autonomy to a governing board of long-term appointees."

Two other U.S.-based organizations that might be described as quangos are the Internet Corporation for Assigned Names and Numbers (ICANN) and the National Center for Missing and Exploited Children (NCMEC).




</doc>
<doc id="25295" url="https://en.wikipedia.org/wiki?curid=25295" title="Quiver">
Quiver

A quiver is a container for holding arrows, bolts, darts, or javelins. It can be carried on an archer's body, the bow, or the ground, depending on the type of shooting and the archer's personal preference. Quivers were traditionally made of leather, wood, furs, and other natural materials, but are now often made of metal or plastic.

The English word quiver has its origins in Old French, written as quivre, cuevre or coivre . 

The most common style of quiver is a flat or cylindrical container suspended from the belt. They are found across many cultures from North America to China. Many variations of this type exist, such as being canted forwards or backwards, and being carried on the dominant hand side, off-hand side, or the small of the back. Some variants enclose almost the entire arrow, while minimalist "pocket quivers" consist of little more than a small stiff pouch that only covers the first few inches. The Bayeux Tapestry shows that most bowmen in medieval Europe used belt quivers.

Back quivers are secured to the archer's back by leather straps, with the nock ends protruding above the dominant hand's shoulder. Arrows can be drawn over the shoulder rapidly by the nock. This style of quiver was used by native peoples of North America and Africa, and was also commonly depicted in bas-reliefs from ancient Assyria. While popular in cinema and 20th century art for depictions of medieval European characters (such as Robin Hood), this style of quiver was rarely used in medieval Europe.

A ground quiver is used for both target shooting or warfare when the archer is shooting from a fixed location. They can be simply stakes in the ground with a ring at the top to hold the arrows, or more elaborate designs that hold the arrows within reach without the archer having to lean down to draw.

A modern invention, the bow quiver attaches directly to the bow's limbs and holds the arrows steady with a clip of some kind. They are popular with compound bow hunters as it allows one piece of equipment to be carried in the field without encumbering the hunter's body.

A style used by medieval English Longbowmen and several other cultures, an arrow bag is a simple drawstring cloth sack with a leather spacer at the top to keep the arrows divided. When not in use, the drawstring could be closed, completely covering the arrows so as to protect them from rain and dirt. Some had straps or rope sewn to them for carrying, but many either were tucked into the belt or set on the ground before battle to allow easier access.

Yebira refers to a variety of quiver designs. The Yazutsu is a different type, used in Kyudo. Arrows are removed from it before shooting, and held in the hand, so it is mainly used to transport and protect arrows.



Dr. Brian Marin, author of Ancient Warfare| Concordia Press| page 137


</doc>
<doc id="25296" url="https://en.wikipedia.org/wiki?curid=25296" title="Quid">
Quid

Quid may refer to:





</doc>
<doc id="25297" url="https://en.wikipedia.org/wiki?curid=25297" title="Quinine">
Quinine

Quinine is a medication used to treat malaria and babesiosis. This includes the treatment of malaria due to "Plasmodium falciparum" that is resistant to chloroquine when artesunate is not available. While used for restless legs syndrome, it is not recommended for this purpose due to the risk of side effects. It can be taken by mouth or used intravenously. Malaria resistance to quinine occurs in certain areas of the world. Quinine is also the ingredient in tonic water that gives it its bitter taste.
Common side effects include headache, ringing in the ears, trouble seeing, and sweating. More severe side effects include deafness, low blood platelets, and an irregular heartbeat. Use can make one more prone to sunburn. While it is unclear if use during pregnancy causes harm to the baby, use to treat malaria during pregnancy is still recommended. Quinine is an alkaloid, a naturally occurring chemical compound. How it works as a medicine is not entirely clear.
Quinine was first isolated in 1820 from the bark of a cinchona tree. Bark extracts have been used to treat malaria since at least 1632. It is on the World Health Organization's List of Essential Medicines, the safest and most effective medicines needed in a health system. The wholesale price in the developing world is about US$1.70 to $3.40 per course of treatment. In the United States a course of treatment is more than $200.

As of 2006, it is no longer recommended by the World Health Organization (WHO) as a first-line treatment for malaria, and it should be used only when artemisinins are not available. Quinine is also used to treat lupus and arthritis.

Quinine was frequently prescribed as an off-label treatment for leg cramps at night, but this has become less common due to a warning from the US Food and Drug Administration (FDA) that this practice is associated with life-threatening side effects.

Quinine is a basic amine and is usually provided as a salt. Various existing preparations include the hydrochloride, dihydrochloride, sulfate, bisulfate and gluconate. In the United States, quinine sulfate is commercially available in 324-mg tablets under the brand name Qualaquin.

All quinine salts may be given orally or intravenously (IV); quinine gluconate may also be given intramuscularly (IM) or rectally (PR). The main problem with the rectal route is that the dose can be expelled before it is completely absorbed; in practice, this is corrected by giving a further half dose. No injectable preparation of quinine is licensed in the US; quinidine is used instead.

Quinine is a flavor component of tonic water and bitter lemon drink mixers. On the soda gun behind many bars, tonic water is designated by the letter "Q" representing quinine.

According to tradition, the bitter taste of anti-malarial quinine tonic led British colonials in India to mix it with gin, thus creating the gin and tonic cocktail, which is still popular today. Nowadays, the amount of quinine in tonic is much lower and drinking it against malaria is ineffective. In the US, quinine is listed as an ingredient in some Diet Snapple flavors, including Cranberry-Raspberry.

In France, quinine is an ingredient of an known as , or "Cap Corse," and the wine-based Dubonnet. In Spain, quinine ("Peruvian bark") is sometimes blended into sweet Malaga wine, which is then called "Malaga Quina". In Italy, the traditional flavoured wine Barolo Chinato is infused with quinine and local herbs and is served as a . In Canada and Italy, quinine is an ingredient in the carbonated chinotto beverages Brio and San Pellegrino. In Scotland, the company A.G. Barr uses quinine as an ingredient in the carbonated and caffeinated beverage Irn-Bru. In Uruguay and Argentina, quinine is an ingredient of a PepsiCo tonic water named Paso de los Toros. In Denmark, it is used as an ingredient in the carbonated sports drink Faxe Kondi made by Royal Unibrew.

As a flavoring agent in beverages, quinine is limited to less than 83 parts per million in the United States, and 100 in the European Union.

Quinine (and quinidine) are used as the chiral moiety for the ligands used in Sharpless asymmetric dihydroxylation as well as for numerous other chiral catalyst backbones. Because of its relatively constant and well-known fluorescence quantum yield, quinine is used in photochemistry as a common fluorescence standard.

Because of the narrow difference between its therapeutic and toxic effects, quinine is a common cause of drug-induced disorders, including thrombocytopenia and thrombotic microangiopathy. Even from minor levels occurring in common beverages, quinine can have severe adverse effects involving multiple organ systems, among which are immune system effects and fever, hypotension, hemolytic anemia, acute kidney injury, liver toxicity, and blindness. In people with atrial fibrillation, conduction defects, or heart block, quinine can cause heart arrhythmias, and should be avoided.

Quinine can cause hemolysis in G6PD deficiency (an inherited deficiency), but this risk is small and the physician should not hesitate to use quinine in people with G6PD deficiency when there is no alternative.

Quinine can cause unpredictable serious and life-threatening blood and cardiovascular reactions including low platelet count and hemolytic-uremic syndrome/thrombotic thrombocytopenic purpura (HUS/TTP), long QT syndrome and other serious cardiac arrhythmias including torsades de pointes, blackwater fever, disseminated intravascular coagulation, leukopenia, and neutropenia. Some people who have developed TTP due to quinine have gone on to develop kidney failure. It can also cause serious hypersensitivity reactions include anaphylactic shock, urticaria, serious skin rashes, including Stevens-Johnson syndrome and toxic epidermal necrolysis, angioedema, facial edema, bronchospasm, granulomatous hepatitis, and itchiness.

The most common adverse effects involve a group of symptoms called cinchonism, which can include headache, vasodilation and sweating, nausea, tinnitus, hearing impairment, vertigo or dizziness, blurred vision, and disturbance in color perception. More severe cinchonism includes vomiting, diarrhea, abdominal pain, deafness, blindness, and disturbances in heart rhythms. Cinchonism is much less common when quinine is given by mouth, but oral quinine is not well tolerated (quinine is exceedingly bitter and many patients will vomit after ingesting quinine tablets): Other drugs, such as Fansidar (sulfadoxine with pyrimethamine) or Malarone (proguanil with atovaquone), are often used when oral therapy is required. Quinine ethyl carbonate is tasteless and odourless, but is available commercially only in Japan. Blood glucose, electrolyte and cardiac monitoring are not necessary when quinine is given by mouth.

Quinine is theorized to be toxic to the malarial pathogen, "Plasmodium falciparum", by interfering with the parasite's ability to dissolve and metabolize hemoglobin. As with other quinoline antimalarial drugs, the mechanism of action of quinine has not been fully resolved. The most widely accepted hypothesis of its action is based on the well-studied and closely related quinoline drug, chloroquine. This model involves the inhibition of hemozoin biocrystallization in Heme Detoxification pathway, which facilitates the aggregation of cytotoxic heme. Free cytotoxic heme accumulates in the parasites, causing their deaths. Quinine may target malaria's purine nucleoside phosphorylase enzyme.

The UV absorption of quinine peaks around 350 nm (in UVA). Fluorescent emission peaks at around 460 nm (bright blue/cyan hue). Quinine is highly fluorescent (quantum yield ~0.58) in 0.1 M sulfuric acid solution.

Cinchona trees remain the only economically practical source of quinine. However, under wartime pressure, research towards its synthetic production was undertaken. A formal chemical synthesis was accomplished in 1944 by American chemists R.B. Woodward and W.E. Doering. Since then, several more efficient quinine total syntheses have been achieved, but none of them can compete in economic terms with isolation of the alkaloid from natural sources. The first synthetic organic dye, mauveine, was discovered by William Henry Perkin in 1856 while he was attempting to synthesize quinine.

In the first step of quinine biosynthesis, the enzyme strictosidine synthase catalyzes a stereoselective Pictet–Spengler reaction between tryptamine and secologanin to yield strictosidine. Suitable modification of strictosidine leads to an aldehyde. Hydrolysis and decarboxylation would initially remove one carbon from the iridoid portion and produce corynantheal. Then the tryptamine side-chain were cleaved adjacent to the nitrogen, and this nitrogen was then bonded to the acetaldehyde function to yield cinchonaminal. Ring opening in the indole heterocyclic ring could generate new amine and keto functions. The new quinoline heterocycle would then be formed by combining this amine with the aldehyde produced in the tryptamine side-chain cleavage, giving cinchonidinone. For the last step, hydroxylation and methylation gives quinine.

Quinine was used as a muscle relaxant by the Quechua, who are indigenous to Peru, Bolivia and Ecuador, to halt shivering due to low temperatures. The Quechuas would mix the ground bark of cinchona trees with sweetened water to offset the bark's bitter taste, thus producing tonic water.

The Jesuit were the first to bring cinchona to Europe. The Spanish were aware of the medicinal properties of cinchona bark by the 1570s or earlier: Nicolás Monardes (1571) and Juan Fragoso (1572) both described a tree that was subsequently identified as the cinchona tree and whose bark was used to produce a drink to treat diarrhea. Quinine has been used in unextracted form by Europeans since at least the early 17th century. It was first used to treat malaria in Rome in 1631. A popular story of how it was brought to Europe by the Countess of Chinchon was debunked by medical historian Alec Haggis around 1941. During the 17th century, malaria was endemic to the swamps and marshes surrounding the city of Rome. Malaria was responsible for the deaths of several popes, many cardinals and countless common Roman citizens. Most of the priests trained in Rome had seen malaria victims and were familiar with the shivering brought on by the febrile phase of the disease. The Jesuit brother Agostino Salumbrino (1564–1642), an apothecary by training who lived in Lima, observed the Quechua using the bark of the cinchona tree for that purpose. While its effect in treating malaria (and malaria-induced shivering) was unrelated to its effect in controlling shivering from rigors, it was a successful medicine against malaria. At the first opportunity, Salumbrino sent a small quantity to Rome for testing as a malaria treatment. In the years that followed, cinchona bark, known as Jesuit's bark or Peruvian bark, became one of the most valuable commodities shipped from Peru to Europe. When King Charles II was cured of malaria at the end of the 17th Century with quinine, it became popular in London. It remained the antimalarial drug of choice until the 1940s, when other drugs took over.

The form of quinine most effective in treating malaria was found by Charles Marie de La Condamine in 1737. In 1820, French researchers Pierre Joseph Pelletier and Joseph Bienaimé Caventou first isolated quinine from the bark of a tree in the genus "Cinchona" – probably "Cinchona officinalis" – and subsequently named the substance. The name was derived from the original Quechua (Inca) word for the cinchona tree bark, "quina" or "quina-quina", which means "bark of bark" or "holy bark". Prior to 1820, the bark was first dried, ground to a fine powder, and then mixed into a liquid (commonly wine) which was then drunk. Large-scale use of quinine as a malaria prophylaxis started around 1850. In 1853 Paul Briquet published a brief history and discussion of the literature on "quinquina".

Quinine played a significant role in the colonization of Africa by Europeans. Quinine had been said to be the prime reason Africa ceased to be known as the "white man's grave". A historian has stated, "it was quinine's efficacy that gave colonists fresh opportunities to swarm into the Gold Coast, Nigeria and other parts of west Africa".

To maintain their monopoly on cinchona bark, Peru and surrounding countries began outlawing the export of cinchona seeds and saplings in the early 19th century. The Dutch government persisted in its attempts to smuggle the seeds, and in the late 19th century the Dutch grew the plants in Indonesian plantations. Soon they became the main suppliers of the plant, and in 1913 they set up the Kina Bureau, a cartel of cinchona producers charged with controlling price and production. By the 1930s Dutch plantations in Java were producing 22 million pounds of cinchona bark, or 97% of the world's quinine production. U.S. attempts to prosecute the Kina Bureau proved unsuccessful. During World War II, Allied powers were cut off from their supply of quinine when Germany conquered the Netherlands, and Japan controlled the Philippines and Indonesia. The US had obtained four million cinchona seeds from the Philippines and began operating cinchona plantations in Costa Rica. Additionally, they began harvesting wild cinchona bark during the Cinchona Missions. Such supplies came too late. Tens of thousands of US troops in Africa and the South Pacific died due to the lack of quinine. Despite controlling the supply, the Japanese did not make effective use of quinine, and thousands of Japanese troops in the southwest Pacific died as a result. Quinine remained the antimalarial drug of choice until after World War II, when other drugs, such as chloroquine, that have fewer side effects largely replaced it.

"Bromo Quinine" were brand name cold tablets containing quinine, manufactured by Grove Laboratories. They were first marketed in 1889 and available until at least the 1960s.

The bark of "Remijia" contains 0.5–2% of quinine. The bark is cheaper than bark of "Cinchona". As it has an intense taste, it is used for making tonic water.

From 1969, to 1992, the US Food and Drug Administration (FDA) received 157 reports of health problems related to quinine use, including 23 which had resulted in death. In 1994, the FDA banned the marketing of over-the-counter quinine as a treatment for nocturnal leg cramps. Pfizer Pharmaceuticals had been selling the brand name Legatrin for this purpose. Also sold as a Softgel (by SmithKlineBeecham) as Q-vel. Doctors may still prescribe quinine, but the FDA has ordered firms to stop marketing unapproved drug products containing quinine. The FDA is also cautioning consumers about off-label use of quinine to treat leg cramps. Quinine is approved for treatment of malaria, but was also commonly prescribed to treat leg cramps and similar conditions. Because malaria is life-threatening, the risks associated with quinine use are considered acceptable when used to treat that affliction.

Though Legatrin was banned by the FDA for the treatment of leg cramps, the drug manufacturer URL Mutual has branded a quinine-containing drug named Qualaquin. It is marketed as a treatment for malaria and is sold in the United States only by prescription. In 2004, the CDC reported only 1,347 confirmed cases of malaria in the United States.

Quinine is sometimes detected as a cutting agent in street drugs such as cocaine and heroin.

Quinine is used as a treatment for "Cryptocaryon irritans" (commonly referred to as white spot, crypto or marine ich) infection of marine aquarium fish.




</doc>
<doc id="25298" url="https://en.wikipedia.org/wiki?curid=25298" title="Quincy">
Quincy

Quincy may refer to:











</doc>
<doc id="25301" url="https://en.wikipedia.org/wiki?curid=25301" title="Quimby">
Quimby

Quimby may refer to:




</doc>
<doc id="25302" url="https://en.wikipedia.org/wiki?curid=25302" title="Quail">
Quail

Quail is a collective name for several genera of mid-sized birds generally placed in the order Galliformes. 
Old World quail are placed in the family Phasianidae, and New World quail are placed in the family Odontophoridae. The species of buttonquail are named for their superficial resemblance to quail, and form the family Turnicidae in the order Charadriiformes. The king quail, an Old World quail, often is sold in the pet trade, and within this trade is commonly, though mistakenly, referred to as a "button quail". Many of the common larger species are farm-raised for table food or egg consumption, and are hunted on game farms or in the wild, where they may be released to supplement the wild population, or extend into areas outside their natural range. In 2007, 40 million quail were produced in the U.S. 

The collective noun for a group of quail is a flock, covey, or bevy.


Quail that have fed on hemlock (e.g., during migration) may induce acute renal failure due to accumulation of toxic substances from the hemlock in the meat; this problem is referred to as "coturnism".




</doc>
<doc id="25303" url="https://en.wikipedia.org/wiki?curid=25303" title="Quagmire (disambiguation)">
Quagmire (disambiguation)

A quagmire is a wetland type, dominated by living, peat-forming plants.

Quagmire may also refer to:






</doc>
<doc id="25305" url="https://en.wikipedia.org/wiki?curid=25305" title="Crossbow bolt">
Crossbow bolt

A quarrel or bolt is the arrow used in a crossbow. The name "quarrel" is derived from the French word "carré", meaning square, referring to their typically square heads. Although their lengths vary, bolts are typically shorter than traditional arrows.


</doc>
<doc id="25308" url="https://en.wikipedia.org/wiki?curid=25308" title="Quasispecies model">
Quasispecies model

The quasispecies model is a description of the process of the Darwinian evolution of certain self-replicating entities within the framework of physical chemistry. A quasispecies is a large group or "cloud" of related genotypes that exist in an environment of high mutation rate (at stationary state), where a large fraction of offspring are expected to contain one or more mutations relative to the parent. This is in contrast to a species, which from an evolutionary perspective is a more-or-less stable single genotype, most of the offspring of which will be genetically accurate copies.

It is useful mainly in providing a qualitative understanding of the evolutionary processes of self-replicating macromolecules such as RNA or DNA or simple asexual organisms such as bacteria or viruses (see also viral quasispecies), and is helpful in explaining something of the early stages of the origin of life. Quantitative predictions based on this model are difficult because the parameters that serve as its input are impossible to obtain from actual biological systems. The quasispecies model was put forward by Manfred Eigen and Peter Schuster based on initial work done by Eigen.

When evolutionary biologists describe competition between species, they generally assume that each species is a single genotype whose descendants are mostly accurate copies. (Such genotypes are said to have a high reproductive "fidelity".) In evolutionary terms, we are interested in the behavior and fitness of that one species or genotype over time.

Some organisms or genotypes, however, may exist in circumstances of low fidelity, where most descendants contain one or more mutations. A group of such genotypes is constantly changing, so discussions of which single genotype is the most fit become meaningless. Importantly, if many closely related genotypes are only one mutation away from each other, then genotypes in the group can mutate back and forth into each other. For example, with one mutation per generation, a child of the sequence AGGT could be AGTT, and a grandchild could be AGGT again. Thus we can envision a "cloud" of related genotypes that is rapidly mutating, with sequences going back and forth among different points in the cloud. Though the proper definition is mathematical, that cloud, roughly speaking, is a quasispecies.

Quasispecies behavior exists for large numbers of individuals existing at a certain (high) range of mutation rates.

In a species, though reproduction may be mostly accurate, periodic mutations will give rise to one or more competing genotypes. If a mutation results in greater replication and survival, the mutant genotype may out-compete the parent genotype and come to dominate the species. Thus, the individual genotypes (or species) may be seen as the units on which selection acts and biologists will often speak of a single genotype's fitness.

In a quasispecies, however, mutations are ubiquitous and so the fitness of an individual genotype becomes meaningless: if one particular mutation generates a boost in reproductive success, it can't amount to much because that genotype's offspring are unlikely to be accurate copies with the same properties. Instead, what matters is the "connectedness" of the cloud. For example, the sequence AGGT has 12 (3+3+3+3) possible single point mutants AGGA, AGGG, and so on. If 10 of those mutants are viable genotypes that may reproduce (and some of whose offspring or grandchildren may mutate back into AGGT again), we would consider that sequence a well-connected node in the cloud. If instead only two of those mutants are viable, the rest being lethal mutations, then that sequence is poorly connected and most of its descendants will not reproduce. The analog of fitness for a quasispecies is the tendency of nearby relatives within the cloud to be well-connected, meaning that more of the mutant descendants will be viable and give rise to further descendants within the cloud.

When the fitness of a single genotype becomes meaningless because of the high rate of mutations, the cloud as a whole or quasispecies becomes the natural unit of selection.

Quasispecies represents the evolution of high-mutation-rate viruses such as HIV and sometimes single genes or molecules within the genomes of other organisms. Quasispecies models have also been proposed by Jose Fontanari and Emmanuel David Tannenbaum to model the evolution of sexual reproduction. Quasispecies was also shown in compositional replicators (based on the Gard model for abiogenesis) and was also suggested to be applicable to describe cell's replication, which amongst other things requires the maintenance and evolution of the internal composition of the parent and bud.

The model rests on four assumptions:

In the quasispecies model, mutations occur through errors made in the process of copying already existing sequences. Further, selection arises because different types of sequences tend to replicate at different rates, which leads to the suppression of sequences that replicate more slowly in favor of sequences that replicate faster. However, the quasispecies model does not predict the ultimate extinction of all but the fastest replicating sequence. Although the sequences that replicate more slowly cannot sustain their abundance level by themselves, they are constantly replenished as sequences that replicate faster mutate into them. At equilibrium, removal of slowly replicating sequences due to decay or outflow is balanced by replenishing, so that even relatively slowly replicating sequences can remain present in finite abundance.

Due to the ongoing production of mutant sequences, selection does not act on single sequences, but on mutational "clouds" of closely related sequences, referred to as "quasispecies". In other words, the evolutionary success of a particular sequence depends not only on its own replication rate, but also on the replication rates of the mutant sequences it produces, and on the replication rates of the sequences of which it is a mutant. As a consequence, the sequence that replicates fastest may even disappear completely in selection-mutation equilibrium, in favor of more slowly replicating sequences that are part of a quasispecies with a higher average growth rate. Mutational clouds as predicted by the quasispecies model have been observed in RNA viruses and in "in vitro" RNA replication.

The mutation rate and the general fitness of the molecular sequences and their neighbors is crucial to the formation of a quasispecies. If the mutation rate is zero, there is no exchange by mutation, and each sequence is its own species. If the mutation rate is too high, exceeding what is known as the error threshold, the quasispecies will break down and be dispersed over the entire range of available sequences.

A simple mathematical model for a quasispecies is as follows: let there be formula_1 possible sequences and let there be formula_2 organisms with sequence "i". Let's say that each of these organisms asexually gives rise to formula_3 offspring. Some are duplicates of their parent, having sequence "i", but some are mutant and have some other sequence. Let the mutation rate formula_4 correspond to the probability that a "j" type parent will produce an "i" type organism. Then the expected fraction of offspring generated by "j" type organisms that would be "i" type organisms is formula_5,

where formula_6.

Then the total number of "i"-type organisms after the first round of reproduction, given as formula_7, is

Sometimes a death rate term formula_9 is included so that:

where formula_11 is equal to 1 when i=j and is zero otherwise. Note that the "n-th" generation can be found by just taking the "n-th" power of W substituting it in place of W in the above formula.

This is just a system of linear equations. The usual way to solve such a system is to first diagonalize the W matrix. Its diagonal entries will be eigenvalues corresponding to certain linear combinations of certain subsets of sequences which will be eigenvectors of the W matrix. These subsets of sequences are the quasispecies. Assuming that the matrix W is a primitive matrix (irreducible and aperiodic), then after very many generations only the eigenvector with the largest eigenvalue will prevail, and it is this quasispecies that will eventually dominate. The components of this eigenvector give the relative abundance of each sequence at equilibrium.

W being primitive means that for some integer formula_12, that the formula_13 power of W is > 0, i.e. all the entries are positive. If W is primitive then each type can, through a sequence of mutations (i.e. powers of W) mutate into all the other types after some number of generations. W is not primitive if it is periodic, where the population can perpetually cycle through different disjoint sets of compositions, or if it is reducible, where the dominant species (or quasispecies) that develops can depend on the initial population, as is the case in the simple example given below.

The quasispecies formulae may be expressed as a set of linear differential equations. If we consider the difference between the new state formula_7 and the old state formula_2 to be the state change over one moment of time, then we can state that the time derivative of formula_2 is given by this difference, formula_17 we can write:

The quasispecies equations are usually expressed in terms of concentrations formula_19 where

The above equations for the quasispecies then become for the discrete version:

or, for the continuum version:

The quasispecies concept can be illustrated by a simple system consisting of 4 sequences. Sequences [0,0], [0,1], [1,0], and [1,1] are numbered 1, 2, 3, and 4, respectively. Let's say the [0,0] sequence never mutates and always produces a single offspring. Let's say the other 3 sequences all produce, on average, formula_24 replicas of themselves, and formula_25 of each of the other two types, where formula_26. The W matrix is then:

The diagonalized matrix is:

And the eigenvectors corresponding to these eigenvalues are:

Only the eigenvalue formula_29 is more than unity. For the n-th generation, the corresponding eigenvalue will be formula_30 and so will increase without bound as time goes by. This eigenvalue corresponds to the eigenvector [0,1,1,1], which represents the quasispecies consisting of sequences 2, 3, and 4, which will be present in equal numbers after a very long time. Since all population numbers must be positive, the first two quasispecies are not legitimate. The third quasispecies consists of only the non-mutating sequence 1. It's seen that even though sequence 1 is the most fit in the sense that it reproduces more of itself than any other sequence, the quasispecies consisting of the other three sequences will eventually dominate (assuming that the initial population was not homogeneous of the sequence 1 type).



</doc>
<doc id="25310" url="https://en.wikipedia.org/wiki?curid=25310" title="Qing dynasty">
Qing dynasty

The Qing dynasty, officially the Great Qing (), was the last imperial dynasty of China. It was established in 1636, and ruled China proper from 1644 to 1911. It was preceded by the Ming dynasty and succeeded by the Republic of China. The Qing multi-cultural empire lasted for almost three centuries and formed the territorial base for modern China. It was the fifth largest empire in world history. The dynasty was founded by the Manchu Aisin Gioro clan in Manchuria. In the late sixteenth century, Nurhaci, originally a Ming vassal, began organizing "Banners", military-social units that included Manchu, Han, and Mongol elements. Nurhaci united Manchu clans and officially proclaimed the Later Jin dynasty in 1616. His son Hong Taiji began driving Ming forces out of the Liaodong Peninsula and declared a new dynasty, the Qing, in 1636.

As Ming control disintegrated, peasant rebels led by Li Zicheng conquered the capital, Beijing, in 1644. Ming general Wu Sangui refused to serve them, but opened the Shanhai Pass to the Banner Armies led by the regent Prince Dorgon, who defeated the rebels and seized the capital. Dorgon served as Prince Regent under the Shunzhi Emperor and implemented policies of rule. Resistance from the Ming loyalists in the south and the Revolt of the Three Feudatories led by Wu Sangui delayed complete conquest until 1683 under the Kangxi Emperor (1661–1722). The Manchu conquest of China killed over 25 million people. The Ten Great Campaigns of the Qianlong Emperor from the 1750s to the 1790s extended Qing control into Inner Asia. During the peak of the Qing dynasty, the empire ruled over the entirety of today's Mainland China, Hainan, Taiwan, Mongolia, Outer Manchuria and Outer Northwest China. 
The early Qing rulers maintained their Manchu customs, and while their title was Emperor, they used "Bogd khaan" when dealing with the Mongols and they were patrons of Tibetan Buddhism. They governed using Confucian styles and institutions of bureaucratic government and retained the imperial examinations to recruit Han Chinese to work under or in parallel with Manchus. They also adapted the ideals of the Chinese tributary system in asserting superiority over peripheral countries such as Korea and Vietnam, while annexing neighboring territories such as Tibet and Mongolia.

The dynasty reached its high point in the late 18th century, then gradually declined in the face of challenges from abroad, internal revolts, population growth, disruption of the economy, corruption, and the reluctance of ruling elites to change their mindsets. The population rose to some 400 million, but taxes and government revenues were fixed at a low rate, leading to fiscal crisis. Following the Opium Wars, European powers led by Great Britain imposed "unequal treaties", free trade, extraterritoriality and treaty ports under foreign control. The Taiping Rebellion (1850–1864) and the Dungan Revolt (1862–1877) in Central Asia led to the deaths of some 20 million people, due to famine, disease, and war. In spite of these disasters, in the Tongzhi Restoration of the 1860s, Han Chinese elites rallied to the defense of the Confucian order and the Manchu rulers. The initial gains in the Self-Strengthening Movement were lost in the First Sino-Japanese War of 1895, in which the Qing lost its influence over Korea and the possession of Taiwan. New Armies were organized, but the ambitious Hundred Days' Reform of 1898 was turned back in a coup by the conservative Empress Dowager Cixi (1835–1908), who was the dominant voice in the national government (with one interruption) after 1861. When the Scramble for Concessions by foreign powers triggered the violently anti-foreign "Boxers" in 1900, with many foreigners and Christians killed, the foreign powers invaded China. Cixi sided with the Boxers and was decisively defeated by the eight invading powers, leading to the flight of the Imperial Court to Xi'an.

After agreeing to sign the Boxer Protocol, the government initiated unprecedented fiscal and administrative reforms, including elections, a new legal code, and abolition of the examination system. Sun Yat-sen and other revolutionaries competed with constitutional monarchists such as Kang Youwei and Liang Qichao to transform the Qing Empire into a modern nation. After the deaths of Cixi and the Guangxu Emperor in 1908, the hardline Manchu court alienated reformers and local elites alike by obstructing social reform. The Wuchang Uprising on 11 October 1911, led to the Xinhai Revolution. General Yuan Shikai negotiated the abdication of Puyi, the last emperor, on 12 February 1912.

Nurhaci declared himself the "Bright Khan" of the "Later Jin" (lit. "gold") state in honor both of the 12th–13th century Jurchen Jin dynasty and of his Aisin Gioro clan ("Aisin" being Manchu for the Chinese ("jīn", "gold")). His son Hong Taiji renamed the dynasty "Great Qing" in 1636. There are competing explanations on the meaning of "Qīng" (lit. "clear" or "pure"). The name may have been selected in reaction to the name of the Ming dynasty (), which consists of the Chinese characters for "sun" () and "moon" (), both associated with the fire element of the Chinese zodiacal system. The character "Qīng" () is composed of "water" () and "azure" (), both associated with the water element. This association would justify the Qing conquest as defeat of fire by water. The water imagery of the new name may also have had Buddhist overtones of perspicacity and enlightenment and connections with the Bodhisattva Manjusri. The Manchu name "daicing", which sounds like a phonetic rendering of "Dà Qīng" or "Dai Ching", may in fact have been derived from a Mongolian word ", дайчин" that means "warrior". "Daicing gurun" may therefore have meant "warrior state", a pun that was only intelligible to Manchu and Mongol people. In the later part of the dynasty, however, even the Manchus themselves had forgotten this possible meaning.

After conquering "China proper", the Manchus identified their state as "China" (中國, "Zhōngguó"; "Middle Kingdom"), and referred to it as "Dulimbai Gurun" in Manchu ("Dulimbai" means "central" or "middle," "gurun" means "nation" or "state"). The emperors equated the lands of the Qing state (including present-day Northeast China, Xinjiang, Mongolia, Tibet and other areas) as "China" in both the Chinese and Manchu languages, defining China as a multi-ethnic state, and rejecting the idea that "China" only meant Han areas. The Qing emperors proclaimed that both Han and non-Han peoples were part of "China". They used both "China" and "Qing" to refer to their state in official documents, international treaties (as the Qing was known internationally as "China" or the "Chinese Empire") and foreign affairs, and "Chinese language" (Manchu: "Dulimbai gurun i bithe") included Chinese, Manchu, and Mongol languages, and "Chinese people" (中國之人 "Zhōngguó zhī rén"; Manchu: "Dulimbai gurun i niyalma") referred to all subjects of the empire. In the Chinese-language versions of its treaties and its maps of the world, the Qing government used "Qing" and "China" interchangeably.

The Qing dynasty was founded not by Han Chinese, who constitute the majority of the Chinese population, but by a sedentary farming people known as the Jurchen, a Tungusic people who lived around the region now comprising the Chinese provinces of Jilin and Heilongjiang. The Manchus are sometimes mistaken for a nomadic people, which they were not.

What was to become the Manchu state was founded by Nurhaci, the chieftain of a minor Jurchen tribethe Aisin Gioroin Jianzhou in the early 17th century. Nurhaci may have spent time in a Chinese household in his youth, and became fluent in Chinese as well as Mongol, and read the Chinese novels Romance of the Three Kingdoms and Water Margin. Originally a vassal of the Ming emperors, Nurhaci embarked on an intertribal feud in 1582 that escalated into a campaign to unify the nearby tribes. By 1616, he had sufficiently consolidated Jianzhou so as to be able to proclaim himself Khan of the Great Jin in reference to the previous Jurchen dynasty.

Two years later, Nurhaci announced the "Seven Grievances" and openly renounced the sovereignty of Ming overlordship in order to complete the unification of those Jurchen tribes still allied with the Ming emperor. After a series of successful battles, he relocated his capital from Hetu Ala to successively bigger captured Ming cities in Liaodong: first Liaoyang in 1621, then Shenyang (Mukden) in 1625.

When the Jurchens were reorganized by Nurhaci into the Eight Banners, many Manchu clans were artificially created as a group of unrelated people founded a new Manchu clan (mukun) using a geographic origin name such as a toponym for their hala (clan name). The irregularities over Jurchen and Manchu clan origin led to the Qing trying to document and systematize the creation of histories for Manchu clans, including manufacturing an entire legend around the origin of the Aisin Gioro clan by taking mythology from the northeast.

Relocating his court from Jianzhou to Liaodong provided Nurhaci access to more resources; it also brought him in close contact with the Khorchin Mongol domains on the plains of Mongolia. Although by this time the once-united Mongol nation had long since fragmented into individual and hostile tribes, these tribes still presented a serious security threat to the Ming borders. Nurhaci's policy towards the Khorchins was to seek their friendship and cooperation against the Ming, securing his western border from a powerful potential enemy.

Furthermore, the Khorchin proved a useful ally in the war, lending the Jurchens their expertise as cavalry archers. To guarantee this new alliance, Nurhaci initiated a policy of inter-marriages between the Jurchen and Khorchin nobilities, while those who resisted were met with military action. This is a typical example of Nurhaci's initiatives that eventually became official Qing government policy. During most of the Qing period, the Mongols gave military assistance to the Manchus.

Some other important contributions by Nurhaci include ordering the creation of a written Manchu script, based on Mongolian script, after the earlier Jurchen script was forgotten (it had been derived from Khitan and Chinese). Nurhaci also created the civil and military administrative system that eventually evolved into the Eight Banners, the defining element of Manchu identity and the foundation for transforming the loosely knitted Jurchen tribes into a nation.

There were too few ethnic Manchus to conquer China proper, so they gained strength by defeating and absorbing Mongols. More importantly, they added Han Chinese to the Eight Banners. The Manchus had to create an entire "Jiu Han jun" (Old Han Army) due to the massive number of Han Chinese soldiers who were absorbed into the Eight Banners by both capture and defection. Ming artillery was responsible for many victories against the Manchus, so the Manchus established an artillery corps made out of Han Chinese soldiers in 1641, and the swelling of Han Chinese numbers in the Eight Banners led in 1642 to all Eight Han Banners being created. Armies of defected Ming Han Chinese conquered southern China for the Qing.

Han Chinese played a massive role in the Qing conquest of China. Han Chinese Generals who defected to the Manchu were often given women from the Imperial Aisin Gioro family in marriage while the ordinary soldiers who surrendered were often given non-royal Manchu women as wives. Jurchen (Manchu) women married Han Chinese in Liaodong. Manchu Aisin Gioro princesses were also given in marriage to Han Chinese officials' sons.

The unbroken series of Nurhaci's military successes ended in January 1626 when he was defeated by Yuan Chonghuan while laying siege to Ningyuan. He died a few months later and was succeeded by his eighth son, Hong Taiji, who emerged after a short political struggle amongst other contenders to be the new Khan. Although Hong Taiji was an experienced leader and the commander of two Banners at the time of his succession, his reign did not start well on the military front. The Jurchens suffered yet another defeat in 1627 at the hands of Yuan Chonghuan. As before, this defeat was, in part, due to the Ming's newly acquired Portuguese cannons.

To redress the technological and numerical disparity, Hong Taiji in 1634 created his own artillery corps, the "ujen cooha" (Chinese: ) from among his existing Han troops who cast their own cannons in the European design with the help of defector Chinese metallurgists. One of the defining events of Hong Taiji's reign was the official adoption of the name "Manchu" for the united Jurchen people in November 1635. In 1635, the Manchus' Mongol allies were fully incorporated into a separate Banner hierarchy under direct Manchu command. Hong Taiji conquered the territory north of Shanhai Pass by Ming dynasty and Ligdan Khan in Inner Mongolia. In April 1636, Mongol nobility of Inner Mongolia, Manchu nobility and the Han mandarin held the Kurultai in Shenyang, recommended khan of Later Jin to be the emperor of Great Qing empire. One of the Yuan Dynasty's jade seal has also dedicated to the emperor (Bogd Setsen Khan) by nobility. When he was said to be presented with the imperial seal of the Yuan dynasty after the defeat of the last Khagan of the Mongols, Hong Taiji renamed his state from "Great Jin" to "Great Qing" and elevated his position from Khan to Emperor, suggesting imperial ambitions beyond unifying the Manchu territories. Hong Taiji then proceeded in 1636 to invade Korea again.

The change of the name from Jurchen to Manchu was made to hide the fact that the ancestors of the Manchus, the Jianzhou Jurchens, were ruled by the Chinese. The Qing dynasty carefully hid the original editions of the books of ""Qing Taizu Wu Huangdi Shilu"" and the ""Manzhou Shilu Tu"" (Taizu Shilu Tu) in the Qing palace, forbidden from public view because they showed that the Manchu Aisin Gioro family had been ruled by the Ming dynasty and followed many Manchu customs that seemed "uncivilized" in later eyes. In the Ming period, the Koreans of Joseon referred to the Jurchen inhabited lands north of the Korean peninsula, above the rivers Yalu and Tumen to be part of Ming China, as the "superior country" (sangguk) which they called Ming China. The Qing deliberately excluded references and information that showed the Jurchens (Manchus) as subservient to the Ming dynasty, from the History of Ming to hide their former subservient relationship to the Ming. The Veritable Records of Ming were not used to source content on Jurchens during Ming rule in the History of Ming because of this.

After the Second Manchu invasion of Korea, Joseon Korea was forced to give several of their royal princesses as concubines to the Qing Manchu regent Prince Dorgon. In 1650, Dorgon married the Korean Princess Uisun.

This was followed by the creation of the first two Han Banners in 1637 (increasing to eight in 1642). Together these military reforms enabled Hong Taiji to resoundingly defeat Ming forces in a series of battles from 1640 to 1642 for the territories of Songshan and Jinzhou. This final victory resulted in the surrender of many of the Ming dynasty's most battle-hardened troops, the death of Yuan Chonghuan at the hands of the Chongzhen Emperor (who thought Yuan had betrayed him), and the complete and permanent withdrawal of the remaining Ming forces north of the Great Wall.
Meanwhile, Hong Taiji set up a rudimentary bureaucratic system based on the Ming model. He established six boards or executive level ministries in 1631 to oversee finance, personnel, rites, military, punishments, and public works. However, these administrative organs had very little role initially, and it was not until the eve of completing the conquest ten years later that they fulfilled their government roles.

Hong Taiji's bureaucracy was staffed with many Han Chinese, including many newly surrendered Ming officials. The Manchus' continued dominance was ensured by an ethnic quota for top bureaucratic appointments. Hong Taiji's reign also saw a fundamental change of policy towards his Han Chinese subjects. Nurhaci had treated Han in Liaodong differently according to how much grain they had: those with less than 5 to 7 sin were treated badly, while those with more than that amount were rewarded with property. Due to a revolt by Han in Liaodong in 1623, Nurhaci, who previously gave concessions to conquered Han subjects in Liaodong, turned against them and ordered that they no longer be trusted. He enacted discriminatory policies and killings against them, while ordering that Han who assimilated to the Jurchen (in Jilin) before 1619 be treated equally, as Jurchens were, and not like the conquered Han in Liaodong. Hong Taiji recognized that the Manchus needed to attract Han Chinese, explaining to reluctant Manchus why he needed to treat the Ming defector General Hong Chengchou leniently. Hong Taiji instead incorporated them into the Jurchen "nation" as full (if not first-class) citizens, obligated to provide military service. By 1648, less than one-sixth of the bannermen were of Manchu ancestry. This change of policy not only increased Hong Taiji's manpower and reduced his military dependence on banners not under his personal control, it also greatly encouraged other Han Chinese subjects of the Ming dynasty to surrender and accept Jurchen rule when they were defeated militarily. Through these and other measures Hong Taiji was able to centralize power unto the office of the Khan, which in the long run prevented the Jurchen federation from fragmenting after his death.

Hong Taiji died suddenly in September 1643. As the Jurchens had traditionally "elected" their leader through a council of nobles, the Qing state did not have a clear succession system. The leading contenders for power were Hong Taiji's oldest son Hooge and Hong Taiji's half brother Dorgon. A compromise installed Hong Taiji's five-year-old son, Fulin, as the Shunzhi Emperor, with Dorgon as regent and de facto leader of the Manchu nation.

Meanwhile, Ming government officials fought against each other, against fiscal collapse, and against a series of peasant rebellions. They were unable to capitalise on the Manchu succession dispute and the presence of a minor as emperor. In April 1644, the capital, Beijing, was sacked by a coalition of rebel forces led by Li Zicheng, a former minor Ming official, who established a short-lived Shun dynasty. The last Ming ruler, the Chongzhen Emperor, committed suicide when the city fell to the rebels, marking the official end of the dynasty.

Li Zicheng then led a collection of rebel forces numbering some 200,000 to confront Wu Sangui, the general commanding the Ming garrison at Shanhai Pass, a key pass of the Great Wall, located fifty miles northeast of Beijing, which defended the capital. Wu Sangui, caught between a rebel army twice his size and an enemy he had fought for years, cast his lot with the foreign but familiar Manchus. Wu Sangui may have been influenced by Li Zicheng's mistreatment of wealthy and cultured officials, including Li's own family; it was said that Li took Wu's concubine Chen Yuanyuan for himself. Wu and Dorgon allied in the name of avenging the death of the Chongzhen Emperor. Together, the two former enemies met and defeated Li Zicheng's rebel forces in battle on May 27, 1644.

The newly allied armies captured Beijing on 6 June. The Shunzhi Emperor was invested as the "Son of Heaven" on 30 October. The Manchus, who had positioned themselves as political heirs to the Ming emperor by defeating Li Zicheng, completed the symbolic transition by holding a formal funeral for the Chongzhen Emperor. However, conquering the rest of China Proper took another seventeen years of battling Ming loyalists, pretenders and rebels. The last Ming pretender, Prince Gui, sought refuge with the King of Burma, Pindale Min, but was turned over to a Qing expeditionary army commanded by Wu Sangui, who had him brought back to Yunnan province and executed in early 1662.

The Qing had taken shrewd advantage of Ming civilian government discrimination against the military and encouraged the Ming military to defect by spreading the message that the Manchus valued their skills. Banners made up of Han Chinese who defected before 1644 were classed among the Eight Banners, giving them social and legal privileges in addition to being acculturated to Manchu traditions. Han defectors swelled the ranks of the Eight Banners so greatly that ethnic Manchus became a minority—only 16% in 1648, with Han Bannermen dominating at 75% and Mongol Bannermen making up the rest. Gunpowder weapons like muskets and artillery were wielded by the Chinese Banners. Normally, Han Chinese defector troops were deployed as the vanguard, while Manchu Bannermen acted as reserve forces or in the rear and were used predominantly for quick strikes with maximum impact, so as to minimize ethnic Manchu losses.

This multi-ethnic force conquered China for the Qing. The three Liaodong Han Bannermen officers who played key roles in the conquest of southern China were Shang Kexi, Geng Zhongming, and Kong Youde, who governed southern China autonomously as viceroys for the Qing after the conquest. Han Chinese Bannermen made up the majority of governors in the early Qing, and they governed and administered China after the conquest, stabilizing Qing rule. Han Bannermen dominated the post of governor-general in the time of the Shunzhi and Kangxi Emperors, and also the post of governor, largely excluding ordinary Han civilians from these posts.

To promote ethnic harmony, a 1648 decree allowed Han Chinese civilian men to marry Manchu women from the Banners with the permission of the Board of Revenue if they were registered daughters of officials or commoners, or with the permission of their banner company captain if they were unregistered commoners. Later in the dynasty the policies allowing intermarriage were done away with.

The southern cadet branch of Confucius' descendants who held the title "Wujing boshi" (Doctor of the Five Classics) and 65th generation descendant in the northern branch who held the title Duke Yansheng both had their titles confirmed by the Shunzhi Emperor upon the Qing entry into Beijing on 31 October. The Kong's title of Duke was maintained in later reigns.

The first seven years of the Shunzhi Emperor's reign were dominated by the regent prince Dorgon. Because of his own political insecurity, Dorgon followed Hong Taiji's example by ruling in the name of the emperor at the expense of rival Manchu princes, many of whom he demoted or imprisoned under one pretext or another. Although the period of his regency was relatively short, Dorgon's precedents and example cast a long shadow over the dynasty.

First, the Manchus had entered "South of the Wall" because Dorgon responded decisively to Wu Sangui's appeal. Then, after capturing Beijing, instead of sacking the city as the rebels had done, Dorgon insisted, over the protests of other Manchu princes, on making it the dynastic capital and reappointing most Ming officials. Choosing Beijing as the capital had not been a straightforward decision, since no major Chinese dynasty had directly taken over its immediate predecessor's capital. Keeping the Ming capital and bureaucracy intact helped quickly stabilize the regime and sped up the conquest of the rest of the country. Dorgon then drastically reduced the influence of the eunuchs, a major force in the Ming bureaucracy, and directed Manchu women not to bind their feet in the Chinese style.

However, not all of Dorgon's policies were equally popular or as easy to implement. The controversial July 1645 edict (the "haircutting order") forced adult Han Chinese men to shave the front of their heads and comb the remaining hair into the queue hairstyle which was worn by Manchu men, on pain of death. The popular description of the order was: "To keep the hair, you lose the head; To keep your head, you cut the hair." To the Manchus, this policy was a test of loyalty and an aid in distinguishing friend from foe. For the Han Chinese, however, it was a humiliating reminder of Qing authority that challenged traditional Confucian values. The "Classic of Filial Piety" ("Xiaojing") held that "a person's body and hair, being gifts from one's parents, are not to be damaged". Under the Ming dynasty, adult men did not cut their hair but instead wore it in the form of a top-knot. The order triggered strong resistance to Qing rule in Jiangnan and massive killing of Han Chinese. It was Han Chinese defectors who carried out massacres against people refusing to wear the queue. Li Chengdong, a Han Chinese general who had served the Ming but surrendered to the Qing, ordered his Han troops to carry out three separate massacres in the city of Jiading within a month, resulting in tens of thousands of deaths. At the end of the third massacre, there was hardly a living person left in this city. Jiangyin also held out against about 10,000 Han Chinese Qing troops for 83 days. When the city wall was finally breached on 9 October 1645, the Han Chinese Qing army led by the Han Chinese Ming defector Liu Liangzuo (劉良佐), who had been ordered to "fill the city with corpses before you sheathe your swords", massacred the entire population, killing between 74,000 and 100,000 people. The queue was the only aspect of Manchu culture which the Qing forced on the common Han population. The Qing required people serving as officials to wear Manchu clothing, but allowed non-official Han civilians to continue wearing Hanfu (Han clothing).

On 31 December 1650, Dorgon suddenly died during a hunting expedition, marking the official start of the Shunzhi Emperor's personal rule. Because the emperor was only 12 years old at that time, most decisions were made on his behalf by his mother, Empress Dowager Xiaozhuang, who turned out to be a skilled political operator.

Although his support had been essential to Shunzhi's ascent, Dorgon had centralised so much power in his hands as to become a direct threat to the throne. So much so that upon his death he was bestowed the extraordinary posthumous title of Emperor Yi (), the only instance in Qing history in which a Manchu "prince of the blood" () was so honored. Two months into Shunzhi's personal rule, however, Dorgon was not only stripped of his titles, but his corpse was disinterred and mutilated. to atone for multiple "crimes", one of which was persecuting to death Shunzhi's agnate eldest brother, Hooge. More importantly, Dorgon's symbolic fall from grace also led to the purge of his family and associates at court, thus reverting power back to the person of the emperor. After a promising start, Shunzhi's reign was cut short by his early death in 1661 at the age of twenty-four from smallpox. He was succeeded by his third son Xuanye, who reigned as the Kangxi Emperor.

The Manchus sent Han Bannermen to fight against Koxinga's Ming loyalists in Fujian. They removed the population from coastal areas in order to deprive Koxinga's Ming loyalists of resources. This led to a misunderstanding that Manchus were "afraid of water". Han Bannermen carried out the fighting and killing, casting doubt on the claim that fear of the water led to the coastal evacuation and ban on maritime activities. Even though a poem refers to the soldiers carrying out massacres in Fujian as "barbarians", both Han Green Standard Army and Han Bannermen were involved and carried out the worst slaughter. 400,000 Green Standard Army soldiers were used against the Three Feudatories in addition to the 200,000 Bannermen.

The sixty-one year reign of the Kangxi Emperor was the longest of any Chinese emperor. Kangxi's reign is also celebrated as the beginning of an era known as the "High Qing", during which the dynasty reached the zenith of its social, economic and military power. Kangxi's long reign started when he was eight years old upon the untimely demise of his father. To prevent a repeat of Dorgon's dictatorial monopolizing of power during the regency, the Shunzhi Emperor, on his deathbed, hastily appointed four senior cabinet ministers to govern on behalf of his young son. The four ministers – Sonin, Ebilun, Suksaha, and Oboi – were chosen for their long service, but also to counteract each other's influences. Most important, the four were not closely related to the imperial family and laid no claim to the throne. However, as time passed, through chance and machination, Oboi, the most junior of the four, achieved such political dominance as to be a potential threat. Even though Oboi's loyalty was never an issue, his personal arrogance and political conservatism led him into an escalating conflict with the young emperor. In 1669 Kangxi, through trickery, disarmed and imprisoned Oboi – a significant victory for a fifteen-year-old emperor over a wily politician and experienced commander.

The early Manchu rulers established two foundations of legitimacy that help to explain the stability of their dynasty. The first was the bureaucratic institutions and the neo-Confucian culture that they adopted from earlier dynasties. Manchu rulers and Han Chinese scholar-official elites gradually came to terms with each other. The examination system offered a path for ethnic Han to become officials. Imperial patronage of Kangxi Dictionary demonstrated respect for Confucian learning, while the Sacred Edict of 1670 effectively extolled Confucian family values. His attempts to discourage Chinese women from foot binding, however, were unsuccessful.

The second major source of stability was the Central Asian aspect of their Manchu identity, which allowed them to appeal to Mongol, Tibetan and Uighur constituents. The ways of the Qing legitimization were different for the Chinese, Mongolian and Tibetan peoples. This contradicted traditional Chinese worldview requiring acculturation of "barbarians". Qing emperors, on the contrary, sought to prevent this in regard to Mongols and Tibetans. The Qing used the title of Emperor (Huangdi) in Chinese, while among Mongols the Qing monarch was referred to as Bogda khan (wise Khan), and referred to as Gong Ma in Tibet. The Qianlong Emperor propagated the image of himself as a Buddhist sage ruler, a patron of Tibetan Buddhism. In the Manchu language, the Qing monarch was alternately referred to as either Huwangdi (Emperor) or Khan with no special distinction between the two usages. The Kangxi Emperor also welcomed to his court Jesuit missionaries, who had first come to China under the Ming. Missionaries including Tomás Pereira, Martino Martini, Johann Adam Schall von Bell, Ferdinand Verbiest and Antoine Thomas held significant positions as military weapons experts, mathematicians, cartographers, astronomers and advisers to the emperor. The relationship of trust was however lost in the later Chinese Rites controversy.

Yet controlling the "Mandate of Heaven" was a daunting task. The vastness of China's territory meant that there were only enough banner troops to garrison key cities forming the backbone of a defense network that relied heavily on surrendered Ming soldiers. In addition, three surrendered Ming generals were singled out for their contributions to the establishment of the Qing dynasty, ennobled as feudal princes (藩王), and given governorships over vast territories in Southern China. The chief of these was Wu Sangui, who was given the provinces of Yunnan and Guizhou, while generals Shang Kexi and Geng Jingzhong were given Guangdong and Fujian provinces respectively.

As the years went by, the three feudal lords and their extensive territories became increasingly autonomous. Finally, in 1673, Shang Kexi petitioned Kangxi for permission to retire to his hometown in Liaodong province and nominated his son as his successor. The young emperor granted his retirement, but denied the heredity of his fief. In reaction, the two other generals decided to petition for their own retirements to test Kangxi's resolve, thinking that he would not risk offending them. The move backfired as the young emperor called their bluff by accepting their requests and ordering that all three fiefdoms to be reverted to the crown.

Faced with the stripping of their powers, Wu Sangui, later joined by Geng Zhongming and by Shang Kexi's son Shang Zhixin, felt they had no choice but to revolt. The ensuing Revolt of the Three Feudatories lasted for eight years. Wu attempted, ultimately in vain, to fire the embers of south China Ming loyalty by restoring Ming customs, ordering that the resented queues be cut, and declaring himself emperor of a new dynasty. At the peak of the rebels' fortunes, they extended their control as far north as the Yangtze River, nearly establishing a divided China. Wu then hesitated to go further north, not being able to coordinate strategy with his allies, and Kangxi was able to unify his forces for a counterattack led by a new generation of Manchu generals. By 1681, the Qing government had established control over a ravaged southern China which took several decades to recover.

Manchu Generals and Bannermen were initially put to shame by the better performance of the Han Chinese Green Standard Army. Kangxi accordingly assigned generals Sun Sike, Wang Jinbao, and Zhao Liangdong to crush the rebels, since he thought that Han Chinese were superior to Bannermen at battling other Han people. Similarly, in north-western China against Wang Fuchen, the Qing used Han Chinese Green Standard Army soldiers and Han Chinese generals as the primary military forces. This choice was due to the rocky terrain, which favoured infantry troops over cavalry, to the desire to keep Bannermen in reserve, and, again, to the belief that Han troops were better at fighting other Han people. These Han generals achieved victory over the rebels. Also due to the mountainous terrain, Sichuan and southern Shaanxi were retaken by the Green Standard Army in 1680, with Manchus participating only in logistics and provisions. 400,000 Green Standard Army soldiers and 150,000 Bannermen served on the Qing side during the war. 213 Han Chinese Banner companies, and 527 companies of Mongol and Manchu Banners were mobilized by the Qing during the revolt. 400,000 Green Standard Army soldiers were used against the Three Feudatories besides 200,000 Bannermen.

The Qing forces were crushed by Wu from 1673–1674. The Qing had the support of the majority of Han Chinese soldiers and Han elite against the Three Feudatories, since they refused to join Wu Sangui in the revolt, while the Eight Banners and Manchu officers fared poorly against Wu Sangui, so the Qing responded with using a massive army of more than 900,000 Han Chinese (non-Banner) instead of the Eight Banners, to fight and crush the Three Feudatories. Wu Sangui's forces were crushed by the Green Standard Army, made out of defected Ming soldiers.

To extend and consolidate the dynasty's control in Central Asia, the Kangxi Emperor personally led a series of military campaigns against the Dzungars in Outer Mongolia. The Kangxi Emperor was able to successfully expel Galdan's invading forces from these regions, which were then incorporated into the empire. Galdan was eventually killed in the Dzungar–Qing War. In 1683, Qing forces received the surrender of Formosa (Taiwan) from Zheng Keshuang, grandson of Koxinga, who had conquered Taiwan from the Dutch colonists as a base against the Qing. Zheng Keshuang was awarded the title "Duke Haicheng" (海澄公) and was inducted into the Han Chinese Plain Red Banner of the Eight Banners when he moved to Beijing. Several Ming princes had accompanied Koxinga to Taiwan in 1661–1662, including the Prince of Ningjing Zhu Shugui and Prince Zhu Honghuan (朱弘桓), son of Zhu Yihai, where they lived in the Kingdom of Tungning. The Qing sent the 17 Ming princes still living on Taiwan in 1683 back to mainland China where they spent the rest of their lives in exile since their lives were spared from execution. Winning Taiwan freed Kangxi's forces for series of battles over Albazin, the far eastern outpost of the Tsardom of Russia. Zheng's former soldiers on Taiwan like the rattan shield troops were also inducted into the Eight Banners and used by the Qing against Russian Cossacks at Albazin. The 1689 Treaty of Nerchinsk was China's first formal treaty with a European power and kept the border peaceful for the better part of two centuries. After Galdan's death, his followers, as adherents to Tibetan Buddhism, attempted to control the choice of the next Dalai Lama. Kangxi dispatched two armies to Lhasa, the capital of Tibet, and installed a Dalai Lama sympathetic to the Qing.

By the end of the 17th century, China was at its greatest height of confidence and political control since the Ming dynasty.

The reigns of the Yongzheng Emperor (r. 1723–1735) and his son, the Qianlong Emperor (r. 1735–1796), marked the height of Qing power. During this period, the Qing Empire ruled over 13 million square kilometers of territory. Yet, as the historian Jonathan Spence puts it, the empire by the end of the Qianlong reign was "like the sun at midday". In the midst of "many glories", he writes, "signs of decay and even collapse were becoming apparent".

After the death of the Kangxi Emperor in the winter of 1722, his fourth son, Prince Yong (雍親王), became the Yongzheng Emperor. In the later years of Kangxi's reign, Yongzheng and his brothers had fought, and there were rumours that he had usurped the throne – most of the rumours held that Yongzheng's brother Yingzhen (Kangxi's 14th son) was the real successor of the Kangxi Emperor, and that Yongzheng and his confidant Keduo Long had tampered with the Kangxi's testament on the night when Kangxi died, though there was little evidence for these charges. In fact, his father had trusted him with delicate political issues and discussed state policy with him. When Yongzheng came to power at the age of 45, he felt a sense of urgency about the problems that had accumulated in his father's later years, and he did not need instruction on how to exercise power. In the words of one recent historian, he was "severe, suspicious, and jealous, but extremely capable and resourceful", and in the words of another, he turned out to be an "early modern state-maker of the first order".

Yongzheng moved rapidly. First, he promoted Confucian orthodoxy and reversed what he saw as his father's laxness by cracking down on unorthodox sects and by decapitating an anti-Manchu writer his father had pardoned. In 1723 he outlawed Christianity and expelled Christian missionaries, though some were allowed to remain in the capital. Next, he moved to control the government. He expanded his father's system of Palace Memorials, which brought frank and detailed reports on local conditions directly to the throne without being intercepted by the bureaucracy, and he created a small Grand Council of personal advisors, which eventually grew into the emperor's "de facto" cabinet for the rest of the dynasty. He shrewdly filled key positions with Manchu and Han Chinese officials who depended on his patronage. When he began to realize that the financial crisis was even greater than he had thought, Yongzheng rejected his father's lenient approach to local landowning elites and mounted a campaign to enforce collection of the land tax. The increased revenues were to be used for "money to nourish honesty" among local officials and for local irrigation, schools, roads, and charity. Although these reforms were effective in the north, in the south and lower Yangzi valley, where Kangxi had wooed the elites, there were long established networks of officials and landowners. Yongzheng dispatched experienced Manchu commissioners to penetrate the thickets of falsified land registers and coded account books, but they were met with tricks, passivity, and even violence. The fiscal crisis persisted.

Yongzheng also inherited diplomatic and strategic problems. A team made up entirely of Manchus drew up the Treaty of Kyakhta (1727) to solidify the diplomatic understanding with Russia. In exchange for territory and trading rights, the Qing would have a free hand dealing with the situation in Mongolia. Yongzheng then turned to that situation, where the Zunghars threatened to re-emerge, and to the southwest, where local Miao chieftains resisted Qing expansion. These campaigns drained the treasury but established the emperor's control of the military and military finance.

The Yongzheng Emperor died in 1735. His 24-year-old son, Prince Bao (寶親王), then became the Qianlong Emperor. Qianlong personally led military campaigns near Xinjiang and Mongolia, putting down revolts and uprisings in Sichuan and parts of southern China while expanding control over Tibet.

The Qianlong Emperor launched several ambitious cultural projects, including the compilation of the "Siku Quanshu", or "Complete Repository of the Four Branches of Literature". With a total of over 3,400 books, 79,000 chapters, and 36,304 volumes, the "Siku Quanshu" is the largest collection of books in Chinese history. Nevertheless, Qianlong used Literary Inquisition to silence opposition. The accusation of individuals began with the emperor's own interpretation of the true meaning of the corresponding words. If the emperor decided these were derogatory or cynical towards the dynasty, persecution would begin. Literary inquisition began with isolated cases at the time of Shunzhi and Kangxi, but became a pattern under Qianlong's rule, during which there were 53 cases of literary persecution.

Beneath outward prosperity and imperial confidence, the later years of Qianlong's reign were marked by rampant corruption and neglect. Heshen, the emperor's handsome young favorite, took advantage of the emperor's indulgence to become one of the most corrupt officials in the history of the dynasty. Qianlong's son, the Jiaqing Emperor (r. 1796–1820), eventually forced Heshen to commit suicide.

China also began suffering from mounting overpopulation during this period. Population growth was stagnant for the first half of the 17th century due to civil wars and epidemics, but prosperity and internal stability gradually reversed this trend. The introduction of new crops from the Americas such as the potato and peanut allowed an improved food supply as well, so that the total population of China during the 18th century ballooned from 100 million to 300 million people. Soon all available farmland was used up, forcing peasants to work ever-smaller and more intensely worked plots. The Qianlong Emperor once bemoaned the country's situation by remarking, "The population continues to grow, but the land does not." The only remaining part of the empire that had arable farmland was Manchuria, where the provinces of Jilin and Heilongjiang had been walled off as a Manchu homeland. The emperor decreed for the first time that Han Chinese civilians were forbidden to settle. Mongols were forbidden by the Qing from crossing the borders of their banners, even into other Mongol Banners, and from crossing into neidi (the Han Chinese 18 provinces) and were given serious punishments if they did in order to keep the Mongols divided against each other to benefit the Qing. Mongol pilgrims wanting to leave their banner's borders for religious reasons such as pilgrimage had to apply for passports to give them permission.

Select groups of Han Chinese bannermen were mass transferred into Manchu Banners by the Qing, changing their ethnicity from Han Chinese to Manchu. Han Chinese bannermen of Tai Nikan 台尼堪 (watchpost Chinese) and Fusi Nikan 抚顺尼堪 (Fushun Chinese) backgrounds into the Manchu banners in 1740 by order of the Qing Qianlong emperor. It was between 1618–1629 when the Han Chinese from Liaodong who later became the Fushun Nikan and Tai Nikan defected to the Jurchens (Manchus). These Han Chinese origin Manchu clans continue to use their original Han surnames and are marked as of Han origin on Qing lists of Manchu clans.

Despite officially prohibiting Han Chinese settlement on the Manchu and Mongol lands, by the 18th century the Qing decided to settle Han refugees from northern China who were suffering from famine, floods, and drought into Manchuria and Inner Mongolia. Han Chinese then streamed into Manchuria, both illegally and legally, over the Great Wall and Willow Palisade. As Manchu landlords desired Han Chinese to rent their land and grow grain, most Han Chinese migrants were not evicted. During the eighteenth century Han Chinese farmed 500,000 hectares of privately owned land in Manchuria and 203,583 hectares of lands that were part of courrier stations, noble estates, and Banner lands. In garrisons and towns in Manchuria Han Chinese made up 80% of the population.

In 1796, open rebellion broke out by the White Lotus Society against the Qing government. The White Lotus Rebellion continued for eight years, until 1804, and marked a turning point in the history of the Qing dynasty.

At the start of the dynasty, the Chinese empire continued to be the hegemonic power in East Asia. Although there was no formal ministry of foreign relations, the Lifan Yuan was responsible for relations with the Mongol and Tibetans in Central Asia, while the tributary system, a loose set of institutions and customs taken over from the Ming, in theory governed relations with East and Southeast Asian countries. The Treaty of Nerchinsk (1689) stabilized relations with Czarist Russia.

In the Jahriyya revolt sectarian violence between two suborders of the Naqshbandi Sufis, the Jahriyya Sufi Muslims and their rivals, the Khafiyya Sufi Muslims, led to a Jahriyya Sufi Muslim rebellion which the Qing dynasty in China crushed with the help of the Khafiyya Sufi Muslims. The Eight Trigrams uprising of 1813 broke out in 1813.

However, during the 18th century European empires gradually expanded across the world, as European states developed economies built on maritime trade. The dynasty was confronted with newly developing concepts of the international system and state to state relations. European trading posts expanded into territorial control in nearby India and on the islands that are now Indonesia. The Qing response, successful for a time, was to establish the Canton System in 1756, which restricted maritime trade to that city (modern-day Guangzhou) and gave monopoly trading rights to private Chinese merchants. The British East India Company and the Dutch East India Company had long before been granted similar monopoly rights by their governments.

In 1793, the British East India Company, with the support of the British government, sent a delegation to China under Lord George Macartney in order to open free trade and put relations on a basis of equality. The imperial court viewed trade as of secondary interest, whereas the British saw maritime trade as the key to their economy. The Qianlong Emperor told Macartney "the kings of the myriad nations come by land and sea with all sorts of precious things", and "consequently there is nothing we lack ..."

Demand in Europe for Chinese goods such as silk, tea, and ceramics could only be met if European companies funneled their limited supplies of silver into China. In the late 1700s, the governments of Britain and France were deeply concerned about the imbalance of trade and the drain of silver. To meet the growing Chinese demand for opium, the British East India Company greatly expanded its production in Bengal. Since China's economy was essentially self-sufficient, the country had little need to import goods or raw materials from the Europeans, so the usual way of payment was through silver. The Daoguang Emperor, concerned both over the outflow of silver and the damage that opium smoking was causing to his subjects, ordered Lin Zexu to end the opium trade. Lin confiscated the stocks of opium without compensation in 1839, leading Britain to send a military expedition the following year.

The First Opium War revealed the outdated state of the Chinese military. The Qing navy, composed entirely of wooden sailing junks, was severely outclassed by the modern tactics and firepower of the British Royal Navy. British soldiers, using advanced muskets and artillery, easily outmanoeuvred and outgunned Qing forces in ground battles. The Qing surrender in 1842 marked a decisive, humiliating blow to China. The Treaty of Nanjing, the first of the "unequal treaties", demanded war reparations, forced China to open up the Treaty Ports of Canton, Amoy, Fuchow, Ningpo and Shanghai to Western trade and missionaries, and to cede Hong Kong Island to Britain. It revealed weaknesses in the Qing government and provoked rebellions against the regime. In 1842, the Qing dynasty fought a war with the Sikh Empire (the last independent kingdom of India), resulting in a negotiated peace and a return to the "status quo ante bellum".

The Taiping Rebellion in the mid-19th century was the first major instance of anti-Manchu sentiment. Amid widespread social unrest and worsening famine, the rebellion not only posed the most serious threat towards Qing rulers, it has also been called the "bloodiest civil war of all time"; during its fourteen-year course from 1850 to 1864 between 20 and 30 million people died. Hong Xiuquan, a failed civil service candidate, in 1851 launched an uprising in Guizhou province, and established the Taiping Heavenly Kingdom with Hong himself as king. Hong announced that he had visions of God and that he was the brother of Jesus Christ. Slavery, concubinage, arranged marriage, opium smoking, footbinding, judicial torture, and the worship of idols were all banned. However, success led to internal feuds, defections and corruption. In addition, British and French troops, equipped with modern weapons, had come to the assistance of the Qing imperial army. It was not until 1864 that Qing armies under Zeng Guofan succeeded in crushing the revolt. After the outbreak of this rebellion, there were also revolts by the Muslims and Miao people of China against the Qing dynasty, most notably in the Miao Rebellion (1854–73) in Guizhou, the Panthay Rebellion (1856–1873) in Yunnan and the Dungan Revolt (1862–77) in the northwest.

The Western powers, largely unsatisfied with the Treaty of Nanjing, gave grudging support to the Qing government during the Taiping and Nian Rebellions. China's income fell sharply during the wars as vast areas of farmland were destroyed, millions of lives were lost, and countless armies were raised and equipped to fight the rebels. In 1854, Britain tried to re-negotiate the Treaty of Nanjing, inserting clauses allowing British commercial access to Chinese rivers and the creation of a permanent British embassy at Beijing.

In 1856, Qing authorities, in searching for a pirate, boarded a ship, the "Arrow", which the British claimed had been flying the British flag, an incident which led to the Second Opium War. In 1858, facing no other options, the Xianfeng Emperor agreed to the Treaty of Tientsin, which contained clauses deeply insulting to the Chinese, such as a demand that all official Chinese documents be written in English and a proviso granting British warships unlimited access to all navigable Chinese rivers.

Ratification of the treaty in the following year led to a resumption of hostilities. In 1860, with Anglo-French forces marching on Beijing, the emperor and his court fled the capital for the imperial hunting lodge at Rehe. Once in Beijing, the Anglo-French forces looted the Old Summer Palace and, in an act of revenge for the arrest of several Englishmen, burnt it to the ground. Prince Gong, a younger half-brother of the emperor, who had been left as his brother's proxy in the capital, was forced to sign the Convention of Beijing. The humiliated emperor died the following year at Rehe.

Yet the dynasty rallied. Chinese generals and officials such as Zuo Zongtang led the suppression of rebellions and stood behind the Manchus. When the Tongzhi Emperor came to the throne at the age of five in 1861, these officials rallied around him in what was called the Tongzhi Restoration. Their aim was to adopt Western military technology in order to preserve Confucian values. Zeng Guofan, in alliance with Prince Gong, sponsored the rise of younger officials such as Li Hongzhang, who put the dynasty back on its feet financially and instituted the Self-Strengthening Movement. The reformers then proceeded with institutional reforms, including China's first unified ministry of foreign affairs, the Zongli Yamen; allowing foreign diplomats to reside in the capital; establishment of the Imperial Maritime Customs Service; the formation of modernized armies, such as the Beiyang Army, as well as a navy; and the purchase from Europeans of armament factories.

The dynasty lost control of peripheral territories bit by bit. In return for promises of support against the British and the French, the Russian Empire took large chunks of territory in the Northeast in 1860. The period of cooperation between the reformers and the European powers ended with the Tientsin Massacre of 1870, which was incited by the murder of French nuns set off by the belligerence of local French diplomats. Starting with the Cochinchina Campaign in 1858, France expanded control of Indochina. By 1883, France was in full control of the region and had reached the Chinese border. The Sino-French War began with a surprise attack by the French on the Chinese southern fleet at Fuzhou. After that the Chinese declared war on the French. A French invasion of Taiwan was halted and the French were defeated on land in Tonkin at the Battle of Bang Bo. However Japan threatened to enter the war against China due to the Gapsin Coup and China chose to end the war with negotiations. The war ended in 1885 with the Treaty of Tientsin (1885) and the Chinese recognition of the French protectorate in Vietnam.

In 1884, pro-Japanese Koreans in Seoul led the Gapsin Coup. Tensions between China and Japan rose after China intervened to suppress the uprising. Japanese Prime Minister Itō Hirobumi and Li Hongzhang signed the Convention of Tientsin, an agreement to withdraw troops simultaneously, but the First Sino-Japanese War of 1895 was a military humiliation. The Treaty of Shimonoseki recognized Korean independence and ceded Taiwan and the Pescadores to Japan. The terms might have been harsher, but when a Japanese citizen attacked and wounded Li Hongzhang, an international outcry shamed the Japanese into revising them. The original agreement stipulated the cession of Liaodong Peninsula to Japan, but Russia, with its own designs on the territory, along with Germany and France, in what was known as the Triple Intervention, successfully put pressure on the Japanese to abandon the peninsula.

These years saw an evolution in the participation of Empress Dowager Cixi (Wade–Giles: Tz'u-Hsi) in state affairs. She entered the imperial palace in the 1850s as a concubine to the Xianfeng Emperor (r. 1850–1861) and came to power in 1861 after her five-year-old son, the Tongzhi Emperor ascended the throne. She, the Empress Dowager Ci'an (who had been Xianfeng's empress), and Prince Gong (a son of the Daoguang Emperor), staged a coup that ousted several regents for the boy emperor. Between 1861 and 1873, she and Ci'an served as regents, choosing the reign title "Tongzhi" (ruling together). Following the emperor's death in 1875, Cixi's nephew, the Guangxu Emperor, took the throne, in violation of the dynastic custom that the new emperor be of the next generation, and another regency began. In the spring of 1881, Ci'an suddenly died, aged only forty-three, leaving Cixi as sole regent.

From 1889, when Guangxu began to rule in his own right, to 1898, the Empress Dowager lived in semi-retirement, spending the majority of the year at the Summer Palace. On 1 November 1897, two German Roman Catholic missionaries were murdered in the southern part of Shandong province (the Juye Incident). Germany used the murders as a pretext for a naval occupation of Jiaozhou Bay. The occupation prompted a "scramble for concessions" in 1898, which included the German lease of Jiazhou Bay, the Russian acquisition of Liaodong, and the British lease of the New Territories of Hong Kong.

In the wake of these external defeats, the Guangxu Emperor initiated the Hundred Days' Reform of 1898. Newer, more radical advisers such as Kang Youwei were given positions of influence. The emperor issued a series of edicts and plans were made to reorganize the bureaucracy, restructure the school system, and appoint new officials. Opposition from the bureaucracy was immediate and intense. Although she had been involved in the initial reforms, the Empress Dowager stepped in to call them off, arrested and executed several reformers, and took over day-to-day control of policy. Yet many of the plans stayed in place, and the goals of reform were implanted.

Widespread drought in North China, combined with the imperialist designs of European powers and the instability of the Qing government, created conditions that led to the emergence of the Righteous and Harmonious Fists, or "Boxers." In 1900, local groups of Boxers proclaiming support for the Qing dynasty murdered foreign missionaries and large numbers of Chinese Christians, then converged on Beijing to besiege the Foreign Legation Quarter. A coalition of European, Japanese, and Russian armies (the Eight-Nation Alliance) then entered China without diplomatic notice, much less permission. Cixi declared war on all of these nations, only to lose control of Beijing after a short, but hard-fought campaign. She fled to Xi'an. The victorious allies drew up scores of demands on the Qing government, including compensation for their expenses in invading China and execution of complicit officials.

By the early 20th century, mass civil disorder had begun in China, and it was growing continuously. To overcome such problems, Empress Dowager Cixi issued an imperial edict in 1901 calling for reform proposals from the governors-general and governors and initiated the era of the dynasty's "New Policies", also known as the "Late Qing Reform". The edict paved the way for the most far-reaching reforms in terms of their social consequences, including the creation of a national education system and the abolition of the imperial examinations in 1905.

The Guangxu Emperor died on 14 November 1908, and on 15 November 1908, Cixi also died. Rumors held that she or Yuan Shikai ordered trusted eunuchs to poison the Guangxu Emperor, and an autopsy conducted nearly a century later confirmed lethal levels of arsenic in his corpse. Puyi, the oldest son of Zaifeng, Prince Chun, and nephew to the childless Guangxu Emperor, was appointed successor at the age of two, leaving Zaifeng with the regency. This was followed by the dismissal of General Yuan Shikai from his former positions of power. In April 1911 Zaifeng created a cabinet in which there were two vice-premiers. Nonetheless, this cabinet was also known by contemporaries as "The Royal Cabinet" because among the thirteen cabinet members, five were members of the imperial family or Aisin Gioro relatives. This brought a wide range of negative opinions from senior officials like Zhang Zhidong.
The Wuchang Uprising of 10 October 1911 was a success; by November 14 of the 15 provinces had rejected Qing rule. This led to the creation of a new central government, the Republic of China, in Nanjing with Sun Yat-sen as its provisional head. Many provinces soon began "separating" from Qing control. Seeing a desperate situation unfold, the Qing government brought Yuan Shikai back to military power. He took control of his Beiyang Army to crush the revolution in Wuhan at the Battle of Yangxia. After taking the position of Prime Minister and creating his own cabinet, Yuan Shikai went as far as to ask for the removal of Zaifeng from the regency. This removal later proceeded with directions from Empress Dowager Longyu. Yuan Shikai was now a dictator—the ruler of China and the Manchu dynasty had lost all power; it formally abdicated in early 1912.

Premier Yuan Shikai and his Beiyang commanders decided that going to war would be unreasonable and costly. Similarly, Sun Yat-sen wanted a republican constitutional reform, for the benefit of China's economy and populace. With permission from Empress Dowager Longyu, Yuan Shikai began negotiating with Sun Yat-sen, who decided that his goal had been achieved in forming a republic, and that therefore he could allow Yuan to step into the position of President of the Republic of China.

On 12 February 1912, after rounds of negotiations, Longyu issued an imperial edict bringing about the abdication of the child emperor Puyi. This brought an end to over 2,000 years of Imperial China and began an extended period of instability of warlord factionalism. The unorganized political and economic systems combined with a widespread criticism of Chinese culture led to questioning and doubt about the future. Some Qing loyalists organized themselves as "Royalist Party", and tried to use militant activism and open rebellions to restore the monarchy, but to no avail. In July 1917, there was an abortive attempt to restore the Qing dynasty led by Zhang Xun, which was quickly reversed by republican troops. In the 1930s, the Empire of Japan invaded Northeast China and founded Manchukuo in 1932, with Puyi as its emperor. After the invasion by the Soviet Union, Manchukuo fell in 1945.

The early Qing emperors adopted the bureaucratic structures and institutions from the preceding Ming dynasty but split rule between Han Chinese and Manchus, with some positions also given to Mongols. Like previous dynasties, the Qing recruited officials via the imperial examination system, until the system was abolished in 1905. The Qing divided the positions into civil and military positions, each having nine grades or ranks, each subdivided into a and b categories. Civil appointments ranged from an attendant to the emperor or a Grand Secretary in the Forbidden City (highest) to being a prefectural tax collector, deputy jail warden, deputy police commissioner, or tax examiner. Military appointments ranged from being a field marshal or chamberlain of the imperial bodyguard to a third class sergeant, corporal or a first or second class private.

The formal structure of the Qing government centered on the Emperor as the absolute ruler, who presided over six Boards (Ministries), each headed by two presidents and assisted by four vice presidents. In contrast to the Ming system, however, Qing ethnic policy dictated that appointments were split between Manchu noblemen and Han officials who had passed the highest levels of the state examinations. The Grand Secretariat, which had been an important policy-making body under the Ming, lost its importance during the Qing and evolved into an imperial chancery. The institutions which had been inherited from the Ming formed the core of the Qing "Outer Court," which handled routine matters and was located in the southern part of the Forbidden City.

In order not to let the routine administration take over the running of the empire, the Qing emperors made sure that all important matters were decided in the "Inner Court," which was dominated by the imperial family and Manchu nobility and which was located in the northern part of the Forbidden City. The core institution of the inner court was the Grand Council. It emerged in the 1720s under the reign of the Yongzheng Emperor as a body charged with handling Qing military campaigns against the Mongols, but it soon took over other military and administrative duties and served to centralize authority under the crown. The Grand Councillors served as a sort of privy council to the emperor.

The Six Ministries and their respective areas of responsibilities were as follows:

Board of Civil Appointments

Board of Revenue

Board of Rites

Board of War

Board of Punishments

Board of Works

From the early Qing, the central government was characterized by a system of dual appointments by which each position in the central government had a Manchu and a Han Chinese assigned to it. The Han Chinese appointee was required to do the substantive work and the Manchu to ensure Han loyalty to Qing rule. The distinction between Han Chinese and Manchus extended to their court costumes. During the Qianlong Emperor's reign, for example, members of his family were distinguished by garments with a small circular emblem on the back, whereas Han officials wore clothing with a square emblem.

In addition to the six boards, there was a Lifan Yuan unique to the Qing government. This institution was established to supervise the administration of Tibet and the Mongol lands. As the empire expanded, it took over administrative responsibility of all minority ethnic groups living in and around the empire, including early contacts with Russia – then seen as a tribute nation. The office had the status of a full ministry and was headed by officials of equal rank. However, appointees were at first restricted only to candidates of Manchu and Mongol ethnicity, until later open to Han Chinese as well.

Even though the Board of Rites and Lifan Yuan performed some duties of a foreign office, they fell short of developing into a professional foreign service. It was not until 1861 – a year after losing the Second Opium War to the Anglo-French coalition – that the Qing government bowed to foreign pressure and created a proper foreign affairs office known as the Zongli Yamen. The office was originally intended to be temporary and was staffed by officials seconded from the Grand Council. However, as dealings with foreigners became increasingly complicated and frequent, the office grew in size and importance, aided by revenue from customs duties which came under its direct jurisdiction.

There was also another government institution called Imperial Household Department which was unique to the Qing dynasty. It was established before the fall of the Ming, but it became mature only after 1661, following the death of the Shunzhi Emperor and the accession of his son, the Kangxi Emperor. The department's original purpose was to manage the internal affairs of the imperial family and the activities of the inner palace (in which tasks it largely replaced eunuchs), but it also played an important role in Qing relations with Tibet and Mongolia, engaged in trading activities (jade, ginseng, salt, furs, etc.), managed textile factories in the Jiangnan region, and even published books. Relations with the Salt Superintendents and salt merchants, such as those at Yangzhou, were particularly lucrative, especially since they were direct, and did not go through absorptive layers of bureaucracy. The department was manned by "booi", or "bondservants," from the Upper Three Banners. By the 19th century, it managed the activities of at least 56 subagencies.

Qing China reached its largest extent during the 18th century, when it ruled China proper (eighteen provinces) as well as the areas of present-day Northeast China, Inner Mongolia, Outer Mongolia, Xinjiang and Tibet, at approximately 13 million km in size. There were originally 18 provinces, all of which in China proper, but later this number was increased to 22, with Manchuria and Xinjiang being divided or turned into provinces. Taiwan, originally part of Fujian province, became a province of its own in the 19th century, but was ceded to the Empire of Japan following the First Sino-Japanese War by the end of the century. In addition, many surrounding countries, such as Korea (Joseon dynasty), Vietnam frequently paid tribute to China during much of this period. The Katoor dynasty of Afghanistan also paid tribute to the Qing dynasty of China until the mid-19th century. During the Qing dynasty the Chinese claimed suzerainty over the Taghdumbash Pamir in the south-west of Taxkorgan Tajik Autonomous County but permitted the Mir of Hunza to administer the region in return for a tribute. Until 1937 the inhabitants paid tribute to the Mir of Hunza, who exercised control over the pastures. Khanate of Kokand were forced to submit as protectorate and pay tribute to the Qing dynasty in China between 1774 and 1798.




The Qing organization of provinces was based on the fifteen administrative units set up by the Ming dynasty, later made into eighteen provinces by splitting for example, Huguang into Hubei and Hunan provinces. The provincial bureaucracy continued the Yuan and Ming practice of three parallel lines, civil, military, and censorate, or surveillance. Each province was administered by a governor (, "xunfu") and a provincial military commander (, "tidu"). Below the province were prefectures (, "fu") operating under a prefect (, "zhīfǔ"), followed by subprefectures under a subprefect. The lowest unit was the county, overseen by a county magistrate. The eighteen provinces are also known as "China proper". The position of viceroy or governor-general (, "zongdu") was the highest rank in the provincial administration. There were eight regional viceroys in China proper, each usually took charge of two or three provinces. The Viceroy of Zhili, who was responsible for the area surrounding the capital Beijing, is usually considered as the most honorable and powerful viceroy among the eight.


By the mid-18th century, the Qing had successfully put outer regions such as Inner and Outer Mongolia, Tibet and Xinjiang under its control. Imperial commissioners and garrisons were sent to Mongolia and Tibet to oversee their affairs. These territories were also under supervision of a central government institution called Lifan Yuan. Qinghai was also put under direct control of the Qing court. Xinjiang, also known as Chinese Turkestan, was subdivided into the regions north and south of the Tian Shan mountains, also known today as Dzungaria and Tarim Basin respectively, but the post of Ili General was established in 1762 to exercise unified military and administrative jurisdiction over both regions. Dzungaria was fully opened to Han migration by the Qianlong Emperor from the beginning. Han migrants were at first forbidden from permanently settling in the Tarim Basin but were the ban was lifted after the invasion by Jahangir Khoja in the 1820s. Likewise, Manchuria was also governed by military generals until its division into provinces, though some areas of Xinjiang and Northeast China were lost to the Russian Empire in the mid-19th century. Manchuria was originally separated from China proper by the Inner Willow Palisade, a ditch and embankment planted with willows intended to restrict the movement of the Han Chinese, as the area was off-limits to civilian Han Chinese until the government started colonizing the area, especially since the 1860s.

With respect to these outer regions, the Qing maintained imperial control, with the emperor acting as Mongol khan, patron of Tibetan Buddhism and protector of Muslims. However, Qing policy changed with the establishment of Xinjiang province in 1884. During The Great Game era, taking advantage of the Dungan revolt in northwest China, Yaqub Beg invaded Xinjiang from Central Asia with support from the British Empire, and made himself the ruler of the kingdom of Kashgaria. The Qing court sent forces to defeat Yaqub Beg and Xinjiang was reconquered, and then the political system of China proper was formally applied onto Xinjiang. The Kumul Khanate, which was incorporated into the Qing empire as a vassal after helping Qing defeat the Zunghars in 1757, maintained its status after Xinjiang turned into a province through the end of the dynasty in the Xinhai Revolution up until 1930. In the early 20th century, Britain sent an expedition force to Tibet and forced Tibetans to sign a treaty. The Qing court responded by asserting Chinese sovereignty over Tibet, resulting in the 1906 Anglo-Chinese Convention signed between Britain and China. The British agreed not to annex Tibetan territory or to interfere in the administration of Tibet, while China engaged not to permit any other foreign state to interfere with the territory or internal administration of Tibet. Furthermore, similar to Xinjiang which was converted into a province earlier, the Qing government also turned Manchuria into three provinces in the early 20th century, officially known as the "Three Northeast Provinces", and established the post of Viceroy of the Three Northeast Provinces to oversee these provinces, making the total number of regional viceroys to nine.

The early Qing military was rooted in the Eight Banners first developed by Nurhaci to organize Jurchen society beyond petty clan affiliations. There were eight banners in all, differentiated by color. The yellow, bordered yellow, and white banners were known as the "Upper Three Banners" and were under the direct command of the emperor. Only Manchus belonging to the Upper Three Banners, and selected Han Chinese who had passed the highest level of martial exams could serve as the emperor's personal bodyguards. The remaining Banners were known as the "Lower Five Banners". They were commanded by hereditary Manchu princes descended from Nurhachi's immediate family, known informally as the "Iron cap princes". Together they formed the ruling council of the Manchu nation as well as high command of the army. Nurhachi's son Hong Taiji expanded the system to include mirrored Mongol and Han Banners. After capturing Beijing in 1644, the relatively small Banner armies were further augmented by the Green Standard Army, made up of those Ming troops who had surrendered to the Qing, which eventually outnumbered Banner troops three to one. They maintained their Ming era organization and were led by a mix of Banner and Green Standard officers.

Banner Armies were organized along ethnic lines, namely Manchu and Mongol, but included non-Manchu bondservants registered under the household of their Manchu masters. The years leading up to the conquest increased the number of Han Chinese under Manchu rule, leading Hong Taiji to create the , and around the time of the Qing takeover of Beijing, their numbers rapidly swelled. Han Bannermen held high status and power in the early Qing period, especially immediately after the conquest during Shunzhi and Kangxi's reign where they dominated Governor-Generalships and Governorships across China at the expense of both Manchu Bannermen and Han civilians. Han also numerically dominated the Banners up until the mid 18th century. European visitors in Beijing called them "Tartarized Chinese" or "Tartarified Chinese".

The Qianlong Emperor, concerned about maintaining Manchu identity, re-emphasized Manchu ethnicity, ancestry, language, and culture in the Eight Banners and started a mass discharge of Han Bannermen from the Eight Banners, either asking them to voluntarily resign from the Banner rolls or striking their names off. This led to a change from Han majority to a Manchu majority within the Banner system,
and previous Han Bannermen garrisons in southern China such as at Fuzhou, Zhenjiang, Guangzhou, were replaced by Manchu Bannermen in the purge, which started in 1754. The turnover by Qianlong most heavily impacted Han banner garrisons stationed in the provinces while it less impacted Han Bannermen in Beijing, leaving a larger proportion of remaining Han Bannermen in Beijing than the provinces. Han Bannermen's status was decreased from that point on with Manchu Banners gaining higher status. Han Bannermen numbered 75% in 1648 Shunzhi's reign, 72% in 1723 Yongzheng's reign, but decreased to 43% in 1796 during the first year of Jiaqing's reign, which was after Qianlong's purge. The mass discharge was known as the . Qianlong directed most of his ire at those Han Bannermen descended from defectors who joined the Qing after the Qing passed through the Great Wall at Shanhai Pass in 1644, deeming their ancestors as traitors to the Ming and therefore untrustworthy, while retaining Han Bannermen who were descended from defectors who joined the Qing before 1644 in Liaodong and marched through Shanhai pass, also known as those who "followed the Dragon through the pass" ().

After a century of peace the Manchu Banner troops lost their fighting edge. Before the conquest, the Manchu banner had been a "citizen" army whose members were farmers and herders obligated to provide military service in times of war. The decision to turn the banner troops into a professional force whose every need was met by the state brought wealth, corruption, and decline as a fighting force. The Green Standard Army declined in a similar way.

Early during the Taiping Rebellion, Qing forces suffered a series of disastrous defeats culminating in the loss of the regional capital city of Nanjing in 1853. Shortly thereafter, a Taiping expeditionary force penetrated as far north as the suburbs of Tianjin, the imperial heartlands. In desperation the Qing court ordered a Chinese official, Zeng Guofan, to organize regional and village militias into an emergency army called tuanlian. Zeng Guofan's strategy was to rely on local gentry to raise a new type of military organization from those provinces that the Taiping rebels directly threatened. This new force became known as the Xiang Army, named after the Hunan region where it was raised. The Xiang Army was a hybrid of local militia and a standing army. It was given professional training, but was paid for out of regional coffers and funds its commanders – mostly members of the Chinese gentry – could muster. The Xiang Army and its successor, the Huai Army, created by Zeng Guofan's colleague and protégée Li Hongzhang, were collectively called the "Yong Ying" (Brave Camp).

Zeng Guofan had no prior military experience. Being a classically educated official, he took his blueprint for the Xiang Army from the Ming general Qi Jiguang, who, because of the weakness of regular Ming troops, had decided to form his own "private" army to repel raiding Japanese pirates in the mid-16th century. Qi Jiguang's doctrine was based on Neo-Confucian ideas of binding troops' loyalty to their immediate superiors and also to the regions in which they were raised. Zeng Guofan's original intention for the Xiang Army was simply to eradicate the Taiping rebels. However, the success of the Yongying system led to its becoming a permanent regional force within the Qing military, which in the long run created problems for the beleaguered central government.

First, the Yongying system signaled the end of Manchu dominance in Qing military establishment. Although the Banners and Green Standard armies lingered on as a drain on resources, henceforth the Yongying corps became the Qing government's de facto first-line troops. Second, the Yongying corps were financed through provincial coffers and were led by regional commanders, weakening central government's grip on the whole country. Finally, the nature of Yongying command structure fostered nepotism and cronyism amongst its commanders, who laid the seeds of regional warlordism in the first half of the 20th century.

By the late 19th century, the most conservative elements within the Qing court could no longer ignore China's military weakness. In 1860, during the Second Opium War, the capital Beijing was captured and the Summer Palace sacked by a relatively small Anglo-French coalition force numbering 25,000. The advent of modern weaponry resulting from the European Industrial Revolution had rendered China's traditionally trained and equipped army and navy obsolete. The government attempts to modernize during the Self-Strengthening Movement were initially successful, but yielded few lasting results because of the central government's lack of funds, lack of political will, and unwillingness to depart from tradition.

Losing the First Sino-Japanese War of 1894–1895 was a watershed. Japan, a country long regarded by the Chinese as little more than an upstart nation of pirates, annihilated the Qing government's modernized Beiyang Fleet, then deemed to be the strongest naval force in Asia. The Japanese victory occurred a mere three decades after the Meiji Restoration set a feudal Japan on course to emulate the Western nations in their economic and technological achievements. Finally, in December 1894, the Qing government took concrete steps to reform military institutions and to re-train selected units in Westernized drills, tactics and weaponry. These units were collectively called the New Army. The most successful of these was the Beiyang Army under the overall supervision and control of a former Huai Army commander, General Yuan Shikai, who used his position to build networks of loyal officers and eventually become President of the Republic of China.

The most significant facts of early and mid-Qing social history was growth in population, population density, and mobility. The population in 1700, according to widely accepted estimates, was roughly 150 million, about what it had been under the late Ming a century before, then doubled over the next century, and reached a height of 450 million on the eve of the Taiping Rebellion in 1850.

One reason for this growth was the spread of New World crops like peanuts, sweet potatoes, and potatoes, which helped to sustain the people during shortages of harvest for crops such as rice or wheat.  These crops could be grown under harder conditions, and thus cheaper as well, which led to them becoming staples for poorer farmers, decreasing the number of deaths from malnutrition. Diseases such as smallpox, widespread in the seventeenth century, was brought under control by an increase in inoculations. In addition, infant deaths were also greatly decreased due to improvements in birthing techniques and childcare performed by doctors and midwives and through an increase in medical books available to the public. Government campaigns decrease the incidence of infanticide. Unlike Europe, where populatation growth in this period was greatest in the cities, in China the growth in cities and the lower Yangzi was low. The greatest growth was in the borderlands and the highlands, where farmers could clear large tracts of marshlands and forests.
The population was also remarkably mobile, perhaps more so than at any time in Chinese history. Indeed, the Qing government did far more to encourage mobility than to discourage it. Millions of Han Chinese migrated to Yunnan and Guizhou in the 18th century, then also to Taiwan. After the conquests of the 1750s and 1760s, the court organized agricultural colonies in Xinjiang. Migration might be permanent, for resettlement, or the migrant (in theory at least) might regard the move as a temporary sojourn. The latter included an increasingly large and mobile workforce. Local-origin-based merchant groups also moved freely. It also included the organized movement of Qing subjects overseas, largely to Southeastern Asia, in search of trade and other economic opportunities.

According to statute, Qing society was divided into relatively closed estates, of which in most general terms there were five. Apart from the estates of the officials, the comparatively minuscule aristocracy, and the degree-holding literati, there also existed a major division among ordinary Chinese between commoners and people with inferior status. They were divided into two categories: one of them, the good "commoner" people, the other "mean" people who were seen as debased and servile. The majority of the population belonged to the first category and were described as "liangmin", a legal term meaning good people, as opposed to "jianmin" meaning the mean (or ignoble) people. Qing law explicitly stated that the traditional four occupational groups of scholars, farmers, artisans and merchants were "good", or having a status of commoners. On the other hand, slaves or bondservants, entertainers (including prostitutes and actors), tattooed criminals, and those low-level employees of government officials were the "mean people". Mean people were considered legally inferior to commoners and suffered unequal treatments, forbidden to take the imperial examination. Furthermore, such people were usually not allowed to marry with free commoners and were even often required to acknowledge their abasement in society through actions such as bowing. However, throughout the Qing dynasty, the emperor and his court, as well as the bureaucracy, worked towards reducing the distinctions between the debased and free but did not completely succeed even at the end of its era in merging the two classifications together.

Although there had been no powerful hereditary aristocracy since the Song dynasty, the gentry ("shenshi"), like their British counterparts, enjoyed imperial privileges and managed local affairs. The status of this scholar-official was defined by passing at least the first level of civil service examinations and holding a degree, which qualified him to hold imperial office, although he might not actually do so. The gentry member could legally wear gentry robes and could talk to other officials as equals. Officials who had served for one or two terms could then retire to enjoy the glory of their status. Informally, the gentry then presided over local society and could use their connections to influence the magistrate, acquire land, and maintain large households. The gentry thus included not only the males holding degrees but also their wives, descendants, some of their relatives.

The Qing gentry were defined as much by their refined lifestyle as by their legal status. They lived more refined and comfortable lives than the commoners and used sedan-chairs to travel any significant distance. They were usually highly literate and often showed off their learning. They commonly collected objects such as scholars' stones,
porcelain or pieces of art for their beauty, which set them off from less cultivated commoners.

In Qing society, women did not enjoy the same rights as men. The Confucian moral system, which was built by and thus favored men, restrained their rights, and they were often seen as a type of "merchandise" that could be traded away by their family. Once a woman married, she essentially became the property of her husband's family, and could not divorce her husband except under very specific circumstances, such as severe physical harm or an attempt to sell her into prostitution. Men, on the other hand, could divorce their wives for trivial matters such as excessive talkativeness. Furthermore, women were extremely restricted in owning property and inheritance and were essentially confined to their homes and stripped of social interaction and mobility. Mothers often bound their young daughters' feet, a practice that was seen as a standard of feminine beauty and a necessity to be marriageable, but was also a way to restrict a woman's physical movement in society.

By early Qing, the romanticized courtesan culture, which had been much more popular in the late-Ming with men who had sought a model of a refinement and literacy that was missing from their marriage partners, had mostly disappeared. Such a decline was the result of the Qing's reinforced defense of fundamental Confucian family values as well as an attempt to put a stop to the cultural revolution that was happening at the time. The court thus began to rain down heavily on such practices as prostitution, pornography, rape, and homosexuality. However, by the time of the Qianlong emperor, red-light districts had once again become capitals of tasteful and trending courtesanship. In economically diverse port cities such as Tianjin, Chongqing, and Hankou, the sex trade became a large business, which helped supply a fine hierarchy of prostitutes to all classes of men. Shanghai, which had been rapidly growing in the late nineteenth century, became a city where prostitutes of different ranks whom male patrons fawned over and gossiped about, as some became recognized as national entities of femininity.

Another rising phenomenon, especially during the eighteenth century, was the cult of widow chastity. The fact that many young women were betrothed during early adolescence coupled with the high rate of early mortality resulted in a significant number of young widows. This resulted in a problem, as most women had already moved into their husband's household and upon her husband's death essentially became a burden who could never fulfill her original duty of producing a male heir. Widow chastity began to be seen as a form of devout filiality for other relationships including loyalty to the emperor, which resulted in the Qing court's attempt to reward those families who resisted selling off their unneeded daughters-in-law in order to underline such women's virtue. However, this system began to decline when families who attempted to "abuse" the system appeared for social competition and authorities speculated that some families coerced their young widows to commit suicide at the time of their husband's death to obtain more honors. Such corruption showed a lack of respect for human life, and was thus greatly disapproved of by the officials who then chose to reward the families more sparingly.

One of the main reasons for a shift in gender roles was the unprecedentedly high incidence of men leaving their homes to travel, which in turn gave women more freedom for action. Wives of such men often became the ones to run the household, especially in financial matters. Elite women also began to pursue different fashionable activities, such as writing poetry, and a new frenzy of female sociability appeared. Women started to leave their households to attend local opera performances and temple festivals and some even began to form little societies to visit famous sacred sites with other restless women, which helped to shape a new view of the conventional societal norms on how women should behave.

Patrilineal kinship had compelling power socially and culturally; local lineages became the building blocks of society. A person's success or failure depended, people believed, on guidance from a father, from which the family's success and prosperity also grew. The patrilineage kinship structure, that is, descent through the male line, was often translated as "clan" in earlier scholarship. By the Qing, the patrilineage had become the primary organizational device in society. This change began during the Song dynasty when the civil service examination became a means for gaining status versus nobility and inheritance of status. Elite families began to shift their marital practices, identity and loyalty. Instead of intermarrying within aristocratic elites of the same social status, they tended to form marital alliances with nearby families of the same or higher wealth, and established the local people's interests as first and foremost which helped to form intermarried townships. The Neo-Confucian ideology particular Cheng-Zhu thinking adopted by the Qing placed emphasis on patrilineal families and genealogy in society. The emperors exhorted families to compile genealogies in order to strengthen local society.

Inner Mongols and Khalkha Mongols in the Qing rarely knew their ancestors beyond four generations and Mongol tribal society was not organized among patrilineal clans, contrary to what was commonly thought, but included unrelated people at the base unit of organization. The Qing tried but failed to promote the Chinese Neo-Confucian ideology of organizing society along patrimonial clans among the Mongols.

Qing lineages claimed to be based on biological descent but they were often purposefully crafted. When a member of a lineage gained office or became wealthy, he might look back to identify a "founding ancestor", sometimes using considerable creativity in selecting a prestigious local figure. Once such a person had been chosen, a Chinese character was assigned to be used in the given name of each male in each succeeding generation. A written genealogy was compiled to record the lineage's history, biographies of respected ancestors, a chart of all the family members of each generation, rules for the members to follow, and often copies of title contracts for collective property as well. Lastly, an ancestral hall was built to serve as the lineage's headquarters and a place for annual ancestral sacrifice. Such worship was intended to ensure that the ancestors remain content and benevolent spirits ("shen") who would keep watch over and protect the family. Later observers felt that the ancestral cult focused on the family and lineage, rather than on more public matters such as community and nation.

Catholic missionaries—mostly Jesuits—had arrived in the Ming dynasty. By 1701 there were 117 Catholic missionaries, and at most 300,000 converts out of hundreds of millions. There were many persecutions and reverses in the 18th century and by 1800 there was little help from the main supporters in France, Spain and Portugal. The impact on Chinese society was hard to see, apart from some contributions to mathematics, astronomy and the calendar. By the 1840s China was again becoming a major destination for Protestant and Catholic missionaries from Europe and the United States. They encountered significant opposition from local elites, who were committed to Confucianism and resented Western ethical systems. Missionaries were often seen as part of Western imperialism. The educated gentry were afraid for their own power. The mandarins claim to power lay in the knowledge of the Chinese classics—all government officials had to pass extremely difficult tests on Confucianism. The elite currently in power feared this might be replaced by the Bible, scientific training and Western education. Indeed, the examination system was abolished in the early 20th century by reformers who admired Western models of modernization. According to Paul Cohen, from 1860 to 1900:

Catholic missionaries in the 19th century arrived primarily from France. While they arrived somewhat later than the Protestants, their congregations grew at a faster rate. By 1900 there were about 1400 Catholic priests and nuns in China serving nearly 1 million Catholics. Over 3000 Protestant missionaries were active among the 250,000 Christians in China. Missionaries, like all foreigners, enjoyed extraterritorial legal rights. The main goal was conversions, but they made relatively few. They were much more successful in setting up schools, as well as hospitals and dispensaries. They usually avoided Chinese politics, but were opponents of foot-binding and opium. Western governments could protect them in the treaty ports, but outside those limited areas they were at the mercy of local government officials and threats were common. Chinese elites often associated missionary activity with the imperialistic exploitation of China, and with promoting "new technology and ideas that threatened their positions." Historian John K. Fairbank says, "To most Chinese, Christian missionaries seem to be the ideological arm of foreign aggression... To the scholar-gentry, missionaries were foreign subversives, whose immoral conduct and teachings were backed by gunboats. Conservative patriots hated and feared these alien, intruders." The missionaries and their converts were a prime target of attack and murder by Boxers in 1900.

Medical missions in China by the late 19th century laid the foundations for modern medicine in China. Western medical missionaries established the first modern clinics and hospitals, provided the first training for nurses, and opened the first medical schools in China. By 1901, China was the most popular destination for medical missionaries. The 150 foreign physicians operated 128 hospitals and 245 dispensaries, treating 1.7 million patients. In 1894, male medical missionaries comprised 14 percent of all missionaries; women doctors were four percent. Modern medical education in China started in the early 20th century at hospitals run by international missionaries. They began establishing nurse training schools in China in the late 1880s, but nursing of sick men by female nurses was rejected by local traditions, so the number of Chinese students was small until the practice became accepted in the 1930s. There was also a level of distrust on the part of traditional evangelical missionaries who thought hospitals were diverting needed resources away from the primary goal of conversions.

By the end of the 17th century, the Chinese economy had recovered from the devastation caused by the wars in which the Ming dynasty were overthrown, and the resulting breakdown of order. In the following century, markets continued to expand as in the late Ming period, but with more trade between regions, a greater dependence on overseas markets and a greatly increased population. By the end of the 18th century the population had risen to 300 million from approximately 150 million during the late Ming dynasty. The dramatic rise in population was due to several reasons, including the long period of peace and stability in the 18th century and the import of new crops China received from the Americas, including peanuts, sweet potatoes and maize. New species of rice from Southeast Asia led to a huge increase in production. Merchant guilds proliferated in all of the growing Chinese cities and often acquired great social and even political influence. Rich merchants with official connections built up huge fortunes and patronized literature, theater and the arts. Textile and handicraft production boomed.

The government broadened land ownership by returning land that had been sold to large landowners in the late Ming period by families unable to pay the land tax. To give people more incentives to participate in the market, they reduced the tax burden in comparison with the late Ming, and replaced the corvée system with a head tax used to hire laborers. The administration of the Grand Canal was made more efficient, and transport opened to private merchants. A system of monitoring grain prices eliminated severe shortages, and enabled the price of rice to rise slowly and smoothly through the 18th century. Wary of the power of wealthy merchants, Qing rulers limited their trading licenses and usually refused them permission to open new mines, except in poor areas. These restrictions on domestic resource exploration, as well as on foreign trade, are held by some scholars as a cause of the Great Divergence, by which the Western world overtook China economically.

During the Ming–Qing period (1368–1911) the biggest development in the Chinese economy was its transition from a command to a market economy, the latter becoming increasingly more pervasive throughout the Qing's rule. From roughly 1550 to 1800 China proper experienced a second commercial revolution, developing naturally from the first commercial revolution of the Song period which saw the emergence of long-distance inter-regional trade of luxury goods. During the second commercial revolution, for the first time, a large percentage of farming households began producing crops for sale in the local and national markets rather than for their own consumption or barter in the traditional economy. Surplus crops were placed onto the national market for sale, integrating farmers into the commercial economy from the ground up. This naturally led to regions specializing in certain cash-crops for export as China's economy became increasingly reliant on inter-regional trade of bulk staple goods such as cotton, grain, beans, vegetable oils, forest products, animal products, and fertilizer.

Perhaps the most important factor in the development of the second commercial revolution was the mass influx of silver that entered into the country from foreign trade. After the Spanish conquered the Philippines in the 1570s they mined for silver around the New World, greatly expanding the circulating supply of silver. Foreign trade stimulated the ubiquity of the silver standard, after the re-opening of the southeast coast, which had been closed in the late 17th century, foreign trade was quickly re-established, and was expanding at 4% per annum throughout the latter part of the 18th century. China continued to export tea, silk and manufactures, creating a large, favorable trade balance with the West. The resulting inflow of silver expanded the money supply, facilitating the growth of competitive and stable markets. During the mid-Ming China had gradually shifted to silver as the standard currency for large scale transactions and by the late Kangxi reign the assessment and collection of the land tax was done in silver. By standardizing the collection of the land tax in silver, landlords followed suit and began only accepting rent payments in silver rather than in crops themselves, which in turn incentivized farmers to produce crops for sale in local and national markets rather than for their own personal consumption or barter. Unlike the copper coins, "qian" or cash, used mainly for smaller peasant transactions, silver was not properly minted into a coin but rather was traded in designated units of weight: the "liang" or "tael", which equaled roughly 1.3 ounces of silver. Since it was never properly minted, a third-party had to be brought in to assess the weight and purity of the silver, resulting in an extra "meltage fee" added on to the price of transaction. Furthermore, since the "meltage fee" was unregulated until the reign of the Yongzheng emperor it was the source of much corruption at each level of the bureaucracy. The Yongzheng emperor cracked down on the corrupt "meltage fees," legalizing and regulating them so that they could be collected as a tax, "returning meltage fees to the public coffer." From this newly increased public coffer, the Yongzheng emperor increased the salaries of the officials who collected them, further legitimizing silver as the standard currency of the Qing economy.

The second commercial revolution also had a profound effect on the dispersion of the Qing populace. Up until the late Ming there existed a stark contrast between the rural countryside and city metropoles and very few mid-sized cities existed. This was due to the fact that extraction of surplus crops from the countryside was traditionally done by the state and not commercial organizations. However, as commercialization expanded exponentially in the late-Ming and early-Qing, mid-sized cities began popping up to direct the flow of domestic, commercial trade. Some towns of this nature had such a large volume of trade and merchants flowing through them that they developed into full-fledged market-towns. Some of these more active market-towns even developed into small-cities and became home to the new rising merchant-class. The proliferation of these mid-sized cities was only made possible by advancements in long-distance transportation and methods of communication. As more and more Chinese-citizens were travelling the country conducting trade they increasingly found themselves in a far-away place needing a place to stay, in response the market saw the expansion of guild halls to house these merchants.

A key distinguishing feature of the Qing economy was the emergence of guild halls around the nation. As inter-regional trade and travel became ever more common during the Qing, guild halls dedicated to facilitating commerce, "huiguan", gained prominence around the urban landscape. The location where two merchants would meet to exchange commodities was usually mediated by a third-party broker who served a variety of roles for the market and local citizenry including bringing together buyers and sellers, guaranteeing the good faith of both parties, standardizing the weights, measurements, and procedures of the two parties, collecting tax for the government, and operating inns and warehouses. It was these broker's and their places of commerce that were expanded during the Qing into full-fledged trade guilds, which, among other things, issued regulatory codes and price schedules, and provided a place for travelling merchants to stay and conduct their business. The first recorded trade guild set up to facilitate inter-regional commerce was in Hankou in 1656. Along with the "huiguan" trade guilds, guild halls dedicated to more specific professions, "gongsuo", began to appear and to control commercial craft or artisanal industries such as carpentry, weaving, banking, and medicine. By the nineteenth century guild halls had much more impact on the local communities than simply facilitating trade, they transformed urban areas into cosmopolitan, multi-cultural hubs, staged theatre performances open to general public, developed real estate by pooling funds together in the style of a trust, and some even facilitated the development of social services such as maintaining streets, water supply, and sewage facilities.

In 1685 the Kangxi emperor legalized private maritime trade along the coast, establishing a series of customs stations in major port cities. The customs station at Canton became by far the most active in foreign trade and by the late Kangxi reign more than forty mercantile houses specializing in trade with the West had appeared. The Yongzheng emperor made a parent corporation comprising those forty individual houses in 1725 known as the Cohong system. Firmly established by 1757, the Canton Cohong was an association of thirteen business firms that had been awarded exclusive rights to conduct trade with Western merchants in Canton. Until its abolition after the Opium War in 1842, the Canton Cohong system was the only permitted avenue of Western trade into China, and thus became a booming hub of international trade by the early eighteenth century. By the eighteenth century the most significant export China had was tea. British demand for tea increased exponentially up until they figured out how to grow it for themselves in the hills of northern India in the 1880s. By the end of the eighteenth century tea exports going through the Canton Cohong system amounted to one-tenth of the revenue from taxes collected from the British and nearly the entire revenue of the British East India Company and until the early nineteenth century tea comprised ninety percent of exports leaving Canton.

Chinese scholars, court academies, and local officials carried on late Ming dynasty strengths in astronomy, mathematics, and geography, as well as technologies in ceramics, metallurgy, water transport, printing. Contrary to stereotypes in some Western writing, 16th and 17th century Qing dynasty officials and literati eagerly explored the technology and science introduced by Jesuit missionaries. Manchu leaders employed Jesuits to use cannon and gunpowder to great effect in the conquest of China, and the court sponsored their research in astronomy. The aim of these efforts, however, was to reform and improve inherited science and technology, not to replace it.

Scientific knowledge advanced during the Qing, but there was not a change in the way this knowledge was organized or the way scientific evidence was defined or its truth tested. The powerful official Ruan Yuan at the end of the eighteenth and early nineteenth centuries, for instance, supported a community of scientists and compiled the "Chouren zhuan" (畴人传; Biographies of mathematical scientists), a collection of biographies that eventually included nearly 700 Chinese and over 200 Western scientists. His attempt to reconcile Chinese and the Western science introduced by the Jesuits by arguing that both had originated in ancient China did not succeed, but he did show that science could be conceived and practiced separately from humanistic scholarship. Those who studied the physical universe shared their findings with each other and identified themselves as men of science, but they did not have a separate and independent professional role with its own training and advancement. They were still literati.

The Opium Wars, however, demonstrated the power of steam engine and military technology that had only recently been put into practice in the West. During the Self-Strengthening Movement of the 1860s and 1870s Confucian officials in several coastal provinces established an industrial base in military technology. The introduction of railroads into China raised questions that were more political than technological. A British company built the twelve-mile Shanghai—Woosung line in 1876, obtaining the land under false pretenses, and it was soon torn up. Court officials feared local public opinion and that railways would help invaders, harm farmlands, and obstruct feng shui. To keep development in Chinese hands, the Qing government borrowed 34 billion taels of silver from foreign lenders for railway construction between 1894 and 1911. As late as 1900, only 292 miles were in operation, with 4000 more miles in the planning stage. Finally, 5,200 miles of railway were completed. The British and French After 1905 were finally able to open lines to Burma and Vietnam.

Protestant missionaries by the 1830s translated and printed Western science and medical textbooks. The textbooks found homes in the rapidly enlarging network of missionary schools, and universities. The textbooks opened learning open possibilities for the small number of Chinese students interested in science, and a very small number interested in technology. After 1900, Japan had a greater role in bringing modern science and technology to Chinese audiences but even then they reached chiefly the children of the rich landowning gentry, who seldom engaged in industrial careers.

Under the Qing, inherited forms of art flourished and innovations occurred at many levels and in many types. High levels of literacy, a successful publishing industry, prosperous cities, and the Confucian emphasis on cultivation all fed a lively and creative set of cultural fields.

By the end of the nineteenth century, national artistic and cultural worlds had begun to come to terms with the cosmopolitan culture of the West and Japan. The decision to stay within old forms or welcome Western models was now a conscious choice rather than an unchallenged acceptance of tradition. Classically trained Confucian scholars such as Liang Qichao and Wang Guowei read widely and broke aesthetic and critical ground later cultivated in the New Culture Movement.

The Qing emperors were generally adept at poetry and often skilled in painting, and offered their patronage to Confucian culture. The Kangxi and Qianlong Emperors, for instance, embraced Chinese traditions both to control them and to proclaim their own legitimacy. The Kangxi Emperor sponsored the "Peiwen Yunfu", a rhyme dictionary published in 1711, and the "Kangxi Dictionary" published in 1716, which remains to this day an authoritative reference. The Qianlong Emperor sponsored the largest collection of writings in Chinese history, the "Siku Quanshu," completed in 1782. Court painters made new versions of the Song masterpiece, Zhang Zeduan's "Along the River During the Qingming Festival" whose depiction of a prosperous and happy realm demonstrated the beneficence of the emperor. The emperors undertook tours of the south and commissioned monumental scrolls to depict the grandeur of the occasion. Imperial patronage also encouraged the industrial production of ceramics and Chinese export porcelain. Peking glassware became popular after European glass making processes were introduced by Jesuits to Beijing.

Yet the most impressive aesthetic works were done among the scholars and urban elite. Calligraphy and painting remained a central interest to both court painters and scholar-gentry who considered the Four Arts part of their cultural identity and social standing. The painting of the early years of the dynasty included such painters as the orthodox Four Wangs and the individualists Bada Shanren (1626–1705) and Shitao (1641–1707). The nineteenth century saw such innovations as the Shanghai School and the Lingnan School which used the technical skills of tradition to set the stage for modern painting.

Traditional learning flourished, especially among Ming loyalists such as Dai Zhen and Gu Yanwu, but scholars in the school of evidential learning made innovations in skeptical textual scholarship. Scholar-bureaucrats, including Lin Zexu and Wei Yuan, developed a school of practical statecraft which rooted bureaucratic reform and restructuring in classical philosophy.

Philosophy and literature grew to new heights in the Qing period. Poetry continued as a mark of the cultivated gentleman, but women wrote in larger and larger numbers and came from all walks of life. The poetry of the Qing dynasty is a lively field of research, being studied (along with the poetry of the Ming dynasty) for its association with Chinese opera, developmental trends of Classical Chinese poetry, the transition to a greater role for vernacular language, and for poetry by women in Chinese culture. The Qing dynasty was a period of much literary collection and criticism, and many of the modern popular versions of Classical Chinese poems were transmitted through Qing dynasty anthologies, such as the Quan Tangshi and the "Three Hundred Tang Poems". Pu Songling brought the short story form to a new level in his "Strange Stories from a Chinese Studio", published in the mid-18th century, and Shen Fu demonstrated the charm of the informal memoir in "Six Chapters of a Floating Life", written in the early 19th century but published only in 1877. The art of the novel reached a pinnacle in Cao Xueqin's "Dream of the Red Chamber", but its combination of social commentary and psychological insight were echoed in highly skilled novels such as Wu Jingzi's "Rulin waishi" (1750) and Li Ruzhen's "Flowers in the Mirror" (1827).

In drama, Kong Shangren's Kunqu opera "The Peach Blossom Fan", completed in 1699, portrayed the tragic downfall of the Ming dynasty in romantic terms. The most prestigious form became the so-called Peking opera, though local and folk opera were also widely popular.

Cuisine aroused a cultural pride in the richness of a long and varied past. The gentleman gourmet, such as Yuan Mei, applied aesthetic standards to the art of cooking, eating, and appreciation of tea at a time when New World crops and products entered everyday life. Yuan's "Suiyuan Shidan" expounded culinary aesthetics and theory, along with a range of recipes. The Manchu Han Imperial Feast originated at the court. Although this banquet was probably never common, it reflected an appreciation of Manchu culinary customs. Nevertheless, culinary traditionalists such as Yuan Mei lambasted the opulence of the Manchu Han Feast. Yuan wrote that the feast was caused in part by the "vulgar habits of bad chefs" and that "displays this trite are useful only for welcoming new relations through one's gates or when the boss comes to visit". (皆惡廚陋習。只可用之於新親上門，上司入境)

After 1911, writers, historians and scholars in China and abroad generally deprecated the failures of the late imperial system. However, in the 21st century, a favorable view has emerged in popular culture. Building pride in Chinese history, nationalists have portrayed Imperial China as benevolent, strong and more advanced than the West. They blame ugly wars and diplomatic controversies on imperialist exploitation by Western nations and Japan. Although officially still communist and Maoist, in practice China's rulers have used this grassroots settlement to proclaim that their current policies are restoring China's historical glory. Chairman Xi Jinping has sought parity between Beijing and Washington and promised to restore China to its historical glory.

The New Qing History is a historiographical school that gained prominence in the United States in the mid-1990s by offering a wide-ranging revision of history of the Qing dynasty emphasizing the Manchus who ran it had a private agenda emphasizing multiculturalism, not Chinese nationalism. Earlier historians had emphasized the power of Han Chinese to “sinicize” their conquerors, that is, to assimilate and make them Chinese in their thought and institutions. In the 1980s and early 1990s, American scholars began to learn Manchu and took advantage of newly opened Chinese- and Manchu-language documents in the archives. This research found that the Manchu rulers were savvy in manipulating their subjects and from the 1630s through at least the 18th century, emperors developed a sense of Manchu identity and used Central Asian models of rule as much as they did Confucian ones. According to the new school the Manchu ruling class regarded "China" as only a part, although a very important part, of a much wider empire that extended into the Inner Asian territories of Mongolia, Tibet, the Manchuria and Xinjiang.

Some scholars, led by Ping-ti Ho criticize the new approach for exaggerating the Manchu character of the dynasty, and some in China accuse the American historians in the group of imposing American concerns with race and identity or even of imperialist misunderstanding to weaken China. Still others in China agree that this scholarship has opened new vistas for the study of Qing history.

The use of "New Qing History" as an approach has no bearing on the "New Qing History", a multi-volume history of the Qing dynasty that was authorized by the Chinese State Council in 2003.



</doc>
<doc id="25312" url="https://en.wikipedia.org/wiki?curid=25312" title="Quantum gravity">
Quantum gravity

Quantum gravity (QG) is a field of theoretical physics that seeks to describe gravity according to the principles of quantum mechanics, and where quantum effects cannot be ignored, such as near compact astrophysical objects where the effects of gravity are strong.

The current understanding of gravity is based on Albert Einstein's general theory of relativity, which is formulated within the framework of classical physics. On the other hand, the other three fundamental forces of physics are described within the framework of quantum mechanics and quantum field theory, radically different formalisms for describing physical phenomena. It is sometimes argued that a quantum mechanical description of gravity is necessary on the grounds that one cannot consistently couple a classical system to a quantum one.

While a quantum theory of gravity may be needed to reconcile general relativity with the principles of quantum mechanics, difficulties arise when applying the usual prescriptions of quantum field theory to the force of gravity via hypothesised graviton bosons. The problem is that the theory one gets in this way is not renormalizable — it predicts infinite values for some observable properties, such as the mass of particles, and therefore cannot be used to make meaningful physical predictions of those properties. As a result, theorists have taken up more radical approaches to the problem of quantum gravity, the most popular approaches being string theory and loop quantum gravity. Although some quantum gravity theories, such as string theory, try to unify gravity with the other fundamental forces, others, such as loop quantum gravity, make no such attempt; instead, they make an effort to quantize the gravitational field while it is kept separate from the other forces.

Strictly speaking, the aim of quantum gravity is only to describe the quantum behavior of the gravitational field and should not be confused with the objective of unifying all fundamental interactions into a single mathematical framework. A quantum field theory of gravity that is unified with a grand unified theory is sometimes referred to as a theory of everything (TOE). While any substantial improvement into the present understanding of gravity would aid further work towards unification, the study of quantum gravity is a field in its own right with various branches having different approaches to unification.

One of the difficulties of formulating a quantum gravity theory is that quantum gravitational effects only appear at length scales near the Planck scale, around 10 meter, a scale far smaller, and equivalently far larger in energy, than those currently accessible by high energy particle accelerators. Therefore, physicists lack experimental data which could distinguish between the competing theories which have been proposed and thus thought experiment approaches are suggested as a testing tool for these theories.

Much of the difficulty in meshing these theories at all energy scales comes from the different assumptions that these theories make on how the universe works. General relativity models gravity as curvature of spacetime: in the slogan of John Archibald Wheeler, "Spacetime tells matter how to move; matter tells spacetime how to curve." On the other hand, quantum field theory is typically formulated in the "flat" spacetime used in special relativity. No theory has yet proven successful in describing the general situation where the dynamics of matter, modeled with quantum mechanics, affect the curvature of spacetime. If one attempts to treat gravity as simply another quantum field, the resulting theory is not renormalizable. Even in the simpler case where the curvature of spacetime is fixed "a priori," developing quantum field theory becomes more mathematically challenging, and many ideas physicists use in quantum field theory on flat spacetime are no longer applicable.

It is widely hoped that a theory of quantum gravity would allow us to understand problems of very high energy and very small dimensions of space, such as the behavior of black holes, and the origin of the universe.

At present, one of the deepest problems in theoretical physics is harmonizing the theory of general relativity, which describes gravitation and applies to large-scale structures (stars, planets, galaxies), with quantum mechanics, which describes the other three fundamental forces acting on the atomic scale. This problem must be put in the proper context, however. In particular, contrary to the popular claim that quantum mechanics and general relativity are fundamentally incompatible, one can demonstrate that the structure of general relativity essentially follows inevitably from the quantum mechanics of interacting theoretical spin-2 massless particles (called gravitons).

No concrete proof of gravitons exists, but quantized theories of matter may necessitate their existence. The observation that all fundamental forces except gravity have one or more known messenger particles leads researchers to believe that at least one must exist. This hypothetical particle is known as the "graviton". The predicted find would result in the classification of the graviton as a force particle similar to the photon of the electromagnetic interaction. Many of the accepted notions of a unified theory of physics since the 1970s assume, and to some degree depend upon, the existence of the graviton. These include string theory, superstring theory, and M-theory. Detection of gravitons would validate these various lines of research to unify quantum mechanics and relativity theory.

The Weinberg–Witten theorem places some constraints on theories in which the graviton is a composite particle.

The dilaton made its first appearance in Kaluza–Klein theory, a five-dimensional theory that combined gravitation and electromagnetism. It appears in string theory. However, it's become central to the lower-dimensional many-bodied gravity problem based on the field theoretic approach of Roman Jackiw. The impetus arose from the fact that complete analytical solutions for the metric of a covariant "N"-body system have proven elusive in general relativity. To simplify the problem, the number of dimensions was lowered to "1+1" - one spatial dimension and one temporal dimension. This model problem, known as "R=T" theory, as opposed to the general "G=T" theory, was amenable to exact solutions in terms of a generalization of the Lambert W function. Also, the field equation governing the dilaton, derived from differential geometry, as the Schrödinger equation could be amenable to quantization.

This combines gravity, quantization, and even the electromagnetic interaction, promising ingredients of a fundamental physical theory. This outcome revealed a previously unknown and already existing natural link between general relativity and quantum mechanics. There lacks clarity in the generalization of this theory to "3+1" dimensions. However, a recent derivation in "3+1" dimensions under the right coordinate conditions yields a formulation similar to the earlier "1+1", a dilaton field governed by the logarithmic Schrödinger equation that is seen in condensed matter physics and superfluids. The field equations are amenable to such a generalization, as shown with the inclusion of a one-graviton process, and yield the correct Newtonian limit in "d" dimensions, but only with a dilaton. Furthermore, some speculate on the view of the apparent resemblance between the dilaton and the Higgs boson. However, there needs more experimentation to resolve the relationship between these two particles. Finally, since this theory can combine gravitational, electromagnetic, and quantum effects, their coupling could potentially lead to a means of testing the theory through cosmology and experimentation.

General relativity, like electromagnetism, is a classical field theory. One might expect that, as with electromagnetism, the gravitational force should also have a corresponding quantum field theory.

However, gravity is perturbatively nonrenormalizable. For a quantum field theory to be well defined according to this understanding of the subject, it must be asymptotically free or asymptotically safe. The theory must be characterized by a choice of "finitely many" parameters, which could, in principle, be set by experiment. For example, in quantum electrodynamics these parameters are the charge and mass of the electron, as measured at a particular energy scale.

On the other hand, in quantizing gravity there are, in perturbation theory, "infinitely many independent parameters" (counterterm coefficients) needed to define the theory. For a given choice of those parameters, one could make sense of the theory, but since it is impossible to conduct infinite experiments to fix the values of every parameter, it has been argued that one does not, in perturbation theory, have a meaningful physical theory. At low energies, the logic of the renormalization group tells us that, despite the unknown choices of these infinitely many parameters, quantum gravity will reduce to the usual Einstein theory of general relativity. On the other hand, if we could probe very high energies where quantum effects take over, then "every one" of the infinitely many unknown parameters would begin to matter, and we could make no predictions at all.

It is conceivable that, in the correct theory of quantum gravity, the infinitely many unknown parameters will reduce to a finite number that can then be measured. One possibility is that normal perturbation theory is not a reliable guide to the renormalizability of the theory, and that there really "is" a UV fixed point for gravity. Since this is a question of non-perturbative quantum field theory, it is difficult to find a reliable answer, but some people still pursue this option. Another possibility is that there are new, undiscovered symmetry principles that constrain the parameters and reduce them to a finite set. This is the route taken by string theory, where all of the excitations of the string essentially manifest themselves as new symmetries.

In an effective field theory, all but the first few of the infinite set of parameters in a nonrenormalizable theory are suppressed by huge energy scales and hence can be neglected when computing low-energy effects. Thus, at least in the low-energy regime, the model is a predictive quantum field theory. Furthermore, many theorists argue that the Standard Model should be regarded as an effective field theory itself, with "nonrenormalizable" interactions suppressed by large energy scales and whose effects have consequently not been observed experimentally.

By treating general relativity as an effective field theory, one can actually make legitimate predictions for quantum gravity, at least for low-energy phenomena. An example is the well-known calculation of the tiny first-order quantum-mechanical correction to the classical Newtonian gravitational potential between two masses.

A fundamental lesson of general relativity is that there is no fixed spacetime background, as found in Newtonian mechanics and special relativity; the spacetime geometry is dynamic. While easy to grasp in principle, this is the hardest idea to understand about general relativity, and its consequences are profound and not fully explored, even at the classical level. To a certain extent, general relativity can be seen to be a relational theory, in which the only physically relevant information is the relationship between different events in space-time.

On the other hand, quantum mechanics has depended since its inception on a fixed background (non-dynamic) structure. In the case of quantum mechanics, it is time that is given and not dynamic, just as in Newtonian classical mechanics. In relativistic quantum field theory, just as in classical field theory, Minkowski spacetime is the fixed background of the theory.

String theory can be seen as a generalization of quantum field theory where instead of point particles, string-like objects propagate in a fixed spacetime background, although the interactions among closed strings give rise to space-time in a dynamical way.
Although string theory had its origins in the study of quark confinement and not of quantum gravity, it was soon discovered that the string spectrum contains the graviton, and that "condensation" of certain vibration modes of strings is equivalent to a modification of the original background. In this sense, string perturbation theory exhibits exactly the features one would expect of a perturbation theory that may exhibit a strong dependence on asymptotics (as seen, for example, in the AdS/CFT correspondence) which is a weak form of background dependence.

Loop quantum gravity is the fruit of an effort to formulate a background-independent quantum theory.

Topological quantum field theory provided an example of background-independent quantum theory, but with no local degrees of freedom, and only finitely many degrees of freedom globally. This is inadequate to describe gravity in 3+1 dimensions, which has local degrees of freedom according to general relativity. In 2+1 dimensions, however, gravity is a topological field theory, and it has been successfully quantized in several different ways, including spin networks.

Quantum field theory on curved (non-Minkowskian) backgrounds, while not a full quantum theory of gravity, has shown many promising early results. In an analogous way to the development of quantum electrodynamics in the early part of the 20th century (when physicists considered quantum mechanics in classical electromagnetic fields), the consideration of quantum field theory on a curved background has led to predictions such as black hole radiation.

Phenomena such as the Unruh effect, in which particles exist in certain accelerating frames but not in stationary ones, do not pose any difficulty when considered on a curved background (the Unruh effect occurs even in flat Minkowskian backgrounds). The vacuum state is the state with the least energy (and may or may not contain particles).
See Quantum field theory in curved spacetime for a more complete discussion.

A conceptual difficulty in combining quantum mechanics with general relativity arises from the contrasting role of time within these two frameworks. In quantum theories time acts as an independent background through which states evolve, with the Hamiltonian operator acting as the generator of infinitesimal translations of quantum states through time. In contrast, general relativity treats time as a dynamical variable which interacts directly with matter and moreover requires the Hamiltonian constraint to vanish, removing any possibility of employing a notion of time similar to that in quantum theory.

There are a number of proposed quantum gravity theories. Currently, there is still no complete and consistent quantum theory of gravity, and the candidate models still need to overcome major formal and conceptual problems. They also face the common problem that, as yet, there is no way to put quantum gravity predictions to experimental tests, although there is hope for this to change as future data from cosmological observations and particle physics experiments becomes available.

One suggested starting point is ordinary quantum field theories which are successful in describing the other three basic fundamental forces in the context of the standard model of elementary particle physics. However, while this leads to an acceptable effective (quantum) field theory of gravity at low energies, gravity turns out to be much more problematic at higher energies. For ordinary field theories such as quantum electrodynamics, a technique known as renormalization is an integral part of deriving predictions which take into account higher-energy contributions, but gravity turns out to be nonrenormalizable: at high energies, applying the recipes of ordinary quantum field theory yields models that are devoid of all predictive power.

One attempt to overcome these limitations is to replace ordinary quantum field theory, which is based on the classical concept of a point particle, with a quantum theory of one-dimensional extended objects: string theory. At the energies reached in current experiments, these strings are indistinguishable from point-like particles, but, crucially, different modes of oscillation of one and the same type of fundamental string appear as particles with different (electric and other) charges. In this way, string theory promises to be a unified description of all particles and interactions. The theory is successful in that one mode will always correspond to a graviton, the messenger particle of gravity; however, the price of this success are unusual features such as six extra dimensions of space in addition to the usual three for space and one for time.

In what is called the , it was conjectured that both string theory and a unification of general relativity and supersymmetry known as supergravity form part of a hypothesized eleven-dimensional model known as M-theory, which would constitute a uniquely defined and consistent theory of quantum gravity. As presently understood, however, string theory admits a very large number (10 by some estimates) of consistent vacua, comprising the so-called "string landscape". Sorting through this large family of solutions remains a major challenge.

Loop quantum gravity seriously considers general relativity's insight that spacetime is a dynamical field and is therefore a quantum object. Its second idea is that the quantum discreteness that determines the particle-like behavior of other field theories (for instance, the photons of the electromagnetic field) also affects the structure of space.

The main result of loop quantum gravity is the derivation of a granular structure of space at the Planck length. This is derived from following considerations: In the case of electromagnetism, the quantum operator representing the energy of each frequency of the field has a discrete spectrum. Thus the energy of each frequency is quantized, and the quanta are the photons. In the case of gravity, the operators representing the area and the volume of each surface or space region likewise have discrete spectrum. Thus area and volume of any portion of space are also quantized, where the quanta are elementary quanta of space. It follows, then, that spacetime has an elementary quantum granular structure at the Planck scale, which cuts off the ultraviolet infinities of quantum field theory.

The quantum state of spacetime is described in the theory by means of a mathematical structure called spin networks. Spin networks were initially introduced by Roger Penrose in abstract form, and later shown by Carlo Rovelli and Lee Smolin to derive naturally from a non-perturbative quantization of general relativity. Spin networks do not represent quantum states of a field in spacetime: they represent directly quantum states of spacetime.

The theory is based on the reformulation of general relativity known as Ashtekar variables, which represent geometric gravity using mathematical analogues of electric and magnetic fields.
In the quantum theory, space is represented by a network structure called a spin network, evolving over time in discrete steps.

The dynamics of the theory is today constructed in several versions. One version starts with the canonical quantization of general relativity. The analogue of the Schrödinger equation is a Wheeler–DeWitt equation, which can be defined within the theory.
In the covariant, or spinfoam formulation of the theory, the quantum dynamics is obtained via a sum over discrete versions of spacetime, called spinfoams. These represent histories of spin networks.

There are a number of other approaches to quantum gravity. The approaches differ depending on which features of general relativity and quantum theory are accepted unchanged, and which features are modified. Examples include:

As was emphasized above, quantum gravitational effects are extremely weak and therefore difficult to test. For this reason, the possibility of experimentally testing quantum gravity had not received much attention prior to the late 1990s. However, in the past decade, physicists have realized that evidence for quantum gravitational effects can guide the development of the theory. Since theoretical development has been slow, the field of phenomenological quantum gravity, which studies the possibility of experimental tests, has obtained increased attention.

The most widely pursued possibilities for quantum gravity phenomenology include violations of Lorentz invariance, imprints of quantum gravitational effects in the cosmic microwave background (in particular its polarization), and decoherence induced by fluctuations in the space-time foam.

ESA's INTEGRAL satellite measured polarization of photons of different wavelengths and was able to place a limit in the granularity of space that is less than 10⁻⁴⁸m or 13 orders of magnitude below the Planck scale .

The BICEP2 experiment detected what was initially thought to be primordial B-mode polarization caused by gravitational waves in the early universe. Had the signal in fact been primordial in origin, it could have been an indication of quantum gravitational effects, but it soon transpired that the polarization was due to interstellar dust interference.

As explained above, quantum gravitational effects are extremely weak and therefore difficult to test. For this reason, thought experiments are becoming an important theoretical tool.
An important aspect of quantum gravity relates to the question of coupling of spin and spacetime.
While spin and spacetime are expected to be coupled, the precise nature of this coupling is currently unknown. In particular and most importantly, it is not known how quantum spin sources gravity and what is the correct characterization of the spacetime of a single spin-half particle.
To analyze this question, thought experiments in the context of quantum information, have been suggested.
This work shows that, in order to avoid violation of relativistic causality, the measurable spacetime around a spin-half particle's (rest frame) must be spherically symmetric - i.e., either spacetime is spherically symmetric, or somehow measurements of the spacetime (e.g., time-dilation measurements) should create some sort of back action that affects and changes the quantum spin.




</doc>
<doc id="25315" url="https://en.wikipedia.org/wiki?curid=25315" title="Quality of service">
Quality of service

Quality of service (QoS) is the description or measurement of the overall performance of a service, such as a telephony or computer network or a cloud computing service, particularly the performance seen by the users of the network. To quantitatively measure quality of service, several related aspects of the network service are often considered, such as packet loss, bit rate, throughput, transmission delay, availability, jitter, etc.

In the field of computer networking and other packet-switched telecommunication networks, quality of service refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priority to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow.

Quality of service is particularly important for the transport of traffic with special requirements. In particular, developers have introduced Voice over IP technology to allow computer networks to become as useful as telephone networks for audio conversations, as well as supporting new applications with even stricter network performance requirements.

In the field of telephony, quality of service was defined by the ITU in 1994. Quality of service comprises requirements on all the aspects of a connection, such as service response time, loss, signal-to-noise ratio, crosstalk, echo, interrupts, frequency response, loudness levels, and so on. A subset of telephony QoS is grade of service (GoS) requirements, which comprises aspects of a connection relating to capacity and coverage of a network, for example guaranteed maximum blocking probability and outage probability.

In the field of computer networking and other packet-switched telecommunication networks, teletraffic engineering refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priority to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow. For example, a required bit rate, delay, delay variation, packet loss or bit error rates may be guaranteed. Quality of service is important for real-time streaming multimedia applications such as voice over IP, multiplayer online games and IPTV, since these often require fixed bit rate and are delay sensitive. Quality of service is especially important in networks where the capacity is a limited resource, for example in cellular data communication.

A network or protocol that supports QoS may agree on a traffic contract with the application software and reserve capacity in the network nodes, for example during a session establishment phase. During the session it may monitor the achieved level of performance, for example the data rate and delay, and dynamically control scheduling priorities in the network nodes. It may release the reserved capacity during a tear down phase.

A best-effort network or service does not support quality of service. An alternative to complex QoS control mechanisms is to provide high quality communication over a best-effort network by over-provisioning the capacity so that it is sufficient for the expected peak traffic load. The resulting absence of network congestion reduces or eliminates the need for QoS mechanisms.

QoS is sometimes used as a quality measure, with many alternative definitions, rather than referring to the ability to reserve resources. Quality of service sometimes refers to the level of quality of service, i.e. the guaranteed service quality. High QoS is often confused with a high level of performance, for example high bit rate, low latency and low bit error rate.

QoS is sometimes used in application layer services such as telephony and streaming video to describe a metric that reflects or predicts the subjectively experienced quality. In this context, QoS is the acceptable cumulative effect on subscriber satisfaction of all imperfections affecting the service. Other terms with similar meaning are the quality of experience (QoE), mean opinion score (MOS), perceptual speech quality measure (PSQM) and perceptual evaluation of video quality (PEVQ). See also Subjective video quality.

A number of attempts for layer 2 technologies that add QoS tags to the data have gained popularity in the past. Examples are frame relay, asynchronous transfer mode (ATM) and multiprotocol label switching (MPLS) (a technique between layer 2 and 3). Despite these network technologies remaining in use today, this kind of network lost attention after the advent of Ethernet networks. Today Ethernet is, by far, the most popular layer 2 technology. Conventional Internet routers and LAN switches operate on a best effort basis. This equipment is less expensive, less complex and faster and thus more popular than earlier more complex technologies that provide QoS mechanisms. 

Ethernet optionally uses 802.1p to signal the priority of a frame. 

There were four "type of service" bits and three "precedence" bits originally provided in each IP packet header, but they were not generally respected. These bits were later re-defined as Differentiated services code points (DSCP).

With the advent of IPTV and IP telephony, QoS mechanisms are increasingly available to the end user.

In packet-switched networks, quality of service is affected by various factors, which can be divided into human and technical factors. Human factors include: stability of service quality, availability of service, waiting times and user information. Technical factors include: reliability, scalability, effectiveness, maintainability and network congestion.

Many things can happen to packets as they travel from origin to destination, resulting in the following problems as seen from the point of view of the sender and receiver:


A defined quality of service may be desired or required for certain types of network traffic, for example:

These types of service are called "inelastic", meaning that they require a certain minimum bit rate and a certain maximum latency to function. By contrast, "elastic" applications can take advantage of however much or little bandwidth is available. Bulk file transfer applications that rely on TCP are generally elastic.

Circuit switched networks, especially those intended for voice transmission, such as Asynchronous Transfer Mode (ATM) or GSM, have QoS in the core protocol, resources are reserved at each step on the network for the call as it is set up, there is no need for additional procedures to achieve required performance. Shorter data units and built-in QoS were some of the unique selling points of ATM for applications such as video on demand.

When the expense of mechanisms to provide QoS is justified, network customers and providers can enter into a contractual agreement termed a service-level agreement (SLA) which specifies guarantees for the ability of a connection to give guaranteed performance in terms of throughput or latency based on mutually agreed measures.

An alternative to complex QoS control mechanisms is to provide high quality communication by generously over-provisioning a network so that capacity is based on peak traffic load estimates. This approach is simple for networks with predictable peak loads. This calculation may need to appreciate demanding applications that can compensate for variations in bandwidth and delay with large receive buffers, which is often possible for example in video streaming. 

Over-provisioning can be of limited use in the face of transport protocols (such as TCP) that over time increase the amount of data placed on the network until all available bandwidth is consumed and packets are dropped. Such greedy protocols tend to increase latency and packet loss for all users.

The amount of over-provisioning in interior links required to replace QoS depends on the number of users and their traffic demands. This limits usability of over-provisioning. Newer more bandwidth intensive applications and the addition of more users results in the loss of over-provisioned networks. This then requires a physical update of the relevant network links which is an expensive process. Thus over-provisioning cannot be blindly assumed on the Internet.

Commercial VoIP services are often competitive with traditional telephone service in terms of call quality even without QoS mechanisms in use on the user's connection to their ISP and the VoIP provider's connection to a different ISP. Under high load conditions, however, VoIP may degrade to cell-phone quality or worse. The mathematics of packet traffic indicate that network requires just 60% more raw capacity under conservative assumptions.

Unlike single-owner networks, the Internet is a series of exchange points interconnecting private networks. Hence the Internet's core is owned and managed by a number of different network service providers, not a single entity. Its behavior is much more unpredictable.

There are two principal approaches to QoS in modern packet-switched IP networks, a parameterized system based on an exchange of application requirements with the network, and a prioritized system where each packet identifies a desired service level to the network.

Early work used the integrated services (IntServ) philosophy of reserving network resources. In this model, applications used RSVP to request and reserve resources through a network. While IntServ mechanisms do work, it was realized that in a broadband network typical of a larger service provider, Core routers would be required to accept, maintain, and tear down thousands or possibly tens of thousands of reservations. It was believed that this approach would not scale with the growth of the Internet, and in any event was antithetical to the end-to-end principle, the notion of designing networks so that core routers do little more than simply switch packets at the highest possible rates.

Under DiffServ, packets are marked either by the traffic sources themselves or by the edge devices where the traffic enters the network. In response to these markings, routers and switches use various queuing strategies to tailor performance to requirements. At the IP layer, DSCP markings use the 6 bit DS field in the IP packet header. At the MAC layer, VLAN IEEE 802.1Q can be used to carry 3 bit of essentially the same information. Routers and switches supporting DiffServ configure their network scheduler to use multiple queues for packets awaiting transmission from bandwidth constrained (e.g., wide area) interfaces. Router vendors provide different capabilities for configuring this behavior, to include the number of queues supported, the relative priorities of queues, and bandwidth reserved for each queue.

In practice, when a packet must be forwarded from an interface with queuing, packets requiring low jitter (e.g., VoIP or videoconferencing) are given priority over packets in other queues. Typically, some bandwidth is allocated by default to network control packets (such as Internet Control Message Protocol and routing protocols), while best effort traffic might simply be given whatever bandwidth is left over.

At the Media Access Control (MAC) layer, VLAN IEEE 802.1Q and IEEE 802.1p can be used to distinguish between Ethernet frames and classify them. Queueing theory models have been developed on performance analysis and QoS for MAC layer protocols.

Cisco IOS NetFlow and the Cisco Class Based QoS (CBQoS) Management Information Base (MIB) are marketed by Cisco Systems.
One compelling example of the need for QoS on the Internet relates to congestive collapse. The Internet relies on congestion avoidance protocols, as built into Transmission Control Protocol (TCP), to reduce traffic under conditions that would otherwise lead to "meltdown". QoS applications, such as VoIP and IPTV, require largely constant bitrates and low latency, therefore they cannot use TCP and cannot otherwise reduce their traffic rate to help prevent congestion. QoS contracts limit traffic that can be offered to the Internet and thereby enforce traffic shaping that can prevent it from becoming overloaded, and are hence an indispensable part of the Internet's ability to handle a mix of real-time and non-real-time traffic without meltdown.


End-to-end quality of service can require a method of coordinating resource allocation between one autonomous system and another.
The Internet Engineering Task Force (IETF) defined the Resource Reservation Protocol (RSVP) for bandwidth reservation as a proposed standard in 1997.
RSVP is an end-to-end bandwidth reservation protocol. The traffic engineering version, RSVP-TE, is used in many networks to establish traffic-engineered Multiprotocol Label Switching (MPLS) label-switched paths.
The IETF also defined Next Steps in Signaling (NSIS) with QoS signalling as a target. NSIS is a development and simplification of RSVP.

Research consortia such as "end-to-end quality of service support over heterogeneous networks" (EuQoS, from 2004 through 2007) and fora such as the IPsphere Forum developed more mechanisms for handshaking QoS invocation from one domain to the next. IPsphere defined the Service Structuring Stratum (SSS) signaling bus in order to establish, invoke and (attempt to) assure network services.
EuQoS conducted experiments to integrate Session Initiation Protocol, Next Steps in Signaling and IPsphere's SSS with an estimated cost of about 15.6 million Euro and published a book.

A research project Multi Service Access Everywhere (MUSE) defined another QoS concept in a first phase from January 2004 through February 2006, and a second phase from January 2006 through 2007.
Another research project named PlaNetS was proposed for European funding circa 2005.
A broader European project called "Architecture and design for the future Internet" known as 4WARD had a budget estimated at 23.4 million Euro and was funded from January 2008 through June 2010.
It included a "Quality of Service Theme" and published a book.
Another European project, called WIDENS (Wireless Deployable Network System) , proposed a bandwidth reservation approach for mobile wireless multirate adhoc networks.

Strong cryptography network protocols such as Secure Sockets Layer, I2P, and virtual private networks obscure the data transferred using them. As all electronic commerce on the Internet requires the use of such strong cryptography protocols, unilaterally downgrading the performance of encrypted traffic creates an unacceptable hazard for customers. Yet, encrypted traffic is otherwise unable to undergo deep packet inspection for QoS.

Protocols like ICA and RDP may encapsulate other traffic (e.g. printing, video streaming) with varying requirements that can make optimization difficult.

The Internet2 project found, in 2001, that the QoS protocols were probably not deployable inside its Abilene Network with equipment available at that time.
Equipment available at the time relied on software to implement QoS. The group also predicted that “logistical, financial, and organizational barriers will block the way toward any bandwidth guarantees” by protocol modifications aimed at QoS.
They believed that the economics would encourage network providers to deliberately erode the quality of best effort traffic as a way to push customers to higher priced QoS services. Instead they proposed over-provisioning of capacity as more cost-effective at the time.

The Abilene network study was the basis for the testimony of Gary Bachula to the US Senate Commerce Committee's hearing on Network Neutrality in early 2006. He expressed the opinion that adding more bandwidth was more effective than any of the various schemes for accomplishing QoS they examined.

Bachula's testimony has been cited by proponents of a law banning quality of service as proof that no legitimate purpose is served by such an offering. This argument is dependent on the assumption that over-provisioning isn't a form of QoS and that it is always possible. Cost and other factors affect the ability of carriers to build and maintain permanently over-provisioned networks.

Mobile cellular service providers may offer mobile QoS to customers just as the fixed line PSTN services providers and Internet Service Providers (ISP) may offer QoS. QoS mechanisms are always provided for circuit switched services, and are essential for non-elastic services, for example streaming multimedia.

Mobility adds complication to the QoS mechanisms, for several reasons:

Quality of service in the field of telephony was first defined in 1994 in the ITU-T Recommendation E.800. This definition is very broad, listing 6 primary components: Support, Operability, Accessibility, Retainability, Integrity and Security.
A 1995 recommendation X.902 included a definition is the OSI reference model.
In 1998 the ITU published a document discussing QoS in the field of data networking. X.641 offers a means of developing or enhancing standards related to QoS and provide concepts and terminology that will assist in maintaining the consistency of related standards.

Some QoS-related IETF Request For Comments (RFC)s are , and ; both these are discussed above. The IETF has also published two RFCs giving background on QoS: , and .

The IETF has also published as an informative or "best practices" document about the practical aspects of designing a QoS solution for a DiffServ network. They try to identify which types of applications are commonly run over an IP network to group them into traffic classes, study what treatment do each of these classes need from the network, and suggest which of the QoS mechanisms commonly available in routers can be used to implement those treatments.





</doc>
<doc id="25316" url="https://en.wikipedia.org/wiki?curid=25316" title="Quadrature amplitude modulation">
Quadrature amplitude modulation

Quadrature amplitude modulation (QAM) is the name of a family of digital modulation methods and a related family of analog modulation methods widely used in modern telecommunications to transmit information. It conveys two analog message signals, or two digital bit streams, by changing ("modulating") the amplitudes of two carrier waves, using the amplitude-shift keying (ASK) digital modulation scheme or amplitude modulation (AM) analog modulation scheme. The two carrier waves of the same frequency are out of phase with each other by 90°, a condition known as orthogonality or quadrature. The transmitted signal is created by adding the two carrier waves together. At the receiver, the two waves can be coherently separated (demodulated) because of their orthogonality property. Another key property is that the modulations are low-frequency/low-bandwidth waveforms compared to the carrier frequency, which is known as the narrowband assumption.

Phase modulation (analog PM) and phase-shift keying (digital PSK) can be regarded as a special case of QAM, where the amplitude of the transmitted signal is a constant, but its phase varies. This can also be extended to frequency modulation (FM) and frequency-shift keying (FSK), for these can be regarded as a special case of phase modulation.

QAM is used extensively as a modulation scheme for digital telecommunication systems, such as in 802.11 Wi-Fi standards. Arbitrarily high spectral efficiencies can be achieved with QAM by setting a suitable constellation size, limited only by the noise level and linearity of the communications channel.  QAM is being used in optical fiber systems as bit rates increase; QAM16 and QAM64 can be optically emulated with a 3-path interferometer.

In a QAM signal, one carrier lags the other by 90°, and its amplitude modulation is customarily referred to as the in-phase component, denoted by The other modulating function is the quadrature component, So the composite waveform is mathematically modeled as:

where is the carrier frequency.  At the receiver, a coherent demodulator multiplies the received signal separately with both a cosine and sine signal to produce the received estimates of and . For example:

Using standard trigonometric identities, we can write this as:

Low-pass filtering removes the high frequency terms (containing ), leaving only the term. This filtered signal is unaffected by showing that the in-phase component can be received independently of the quadrature component.  Similarly, we can multiply by a sine wave and then low-pass filter to extract 

The addition of two sinusoids is a linear operation that creates no new frequency components. So the bandwidth of the composite signal is comparable to the bandwidth of the DSB (Double-Sideband) components. Effectively, the spectral redundancy of DSB enables a doubling of the information capacity using this technique. This comes at the expense of demodulation complexity. In particular, a DSB signal has zero-crossings at a regular frequency, which makes it easy to recover the phase of the carrier sinusoid. It is said to be self-clocking. But the sender and receiver of a quadrature-modulated signal must share a clock or otherwise send a clock signal. If the clock phases drift apart, the demodulated "I" and "Q" signals bleed into each other, yielding crosstalk. In this context, the clock signal is called a "phase reference". Clock synchronization is typically achieved by transmitting a burst subcarrier or a pilot signal. The phase reference for NTSC, for example, is included within its colorburst signal.

Analog QAM is used in:

In the frequency domain, QAM has a similar spectral pattern to DSB-SC modulation. Applying Euler's formula to the sinusoids in , the positive-frequency portion of (or analytic representation) is:

where formula_5 denotes the Fourier transform, and and are the transforms of and This result represents the sum of two DSB-SC signals with the same center frequency. The factor of represents the 90° phase shift that enables their individual demodulations.

As in many digital modulation schemes, the constellation diagram is useful for QAM. In QAM, the constellation points are usually arranged in a square grid with equal vertical and horizontal spacing, although other configurations are possible (e.g. Cross-QAM). Since in digital telecommunications the data is usually binary, the number of points in the grid is usually a power of 2 (2, 4, 8, …). Since QAM is usually square, some of these are rare—the most common forms are 16-QAM, 64-QAM and 256-QAM. By moving to a higher-order constellation, it is possible to transmit more bits per symbol. However, if the mean energy of the constellation is to remain the same (by way of making a fair comparison), the points must be closer together and are thus more susceptible to noise and other corruption; this results in a higher bit error rate and so higher-order QAM can deliver more data less reliably than lower-order QAM, for constant mean constellation energy. Using higher-order QAM without increasing the bit error rate requires a higher signal-to-noise ratio (SNR) by increasing signal energy, reducing noise, or both.

If data-rates beyond those offered by 8-PSK are required, it is more usual to move to QAM since it achieves a greater distance between adjacent points in the I-Q plane by distributing the points more evenly. The complicating factor is that the points are no longer all the same amplitude and so the demodulator must now correctly detect both phase and amplitude, rather than just phase.

64-QAM and 256-QAM are often used in digital cable television and cable modem applications. In the United States, 64-QAM and 256-QAM are the mandated modulation schemes for digital cable (see QAM tuner) as standardised by the SCTE in the standard ANSI/SCTE 07 2013. Note that many marketing people will refer to these as QAM-64 and QAM-256. In the UK, 64-QAM is used for digital terrestrial television (Freeview) whilst 256-QAM is used for Freeview-HD.

Communication systems designed to achieve very high levels of spectral efficiency usually employ very dense QAM constellations. For example, current Homeplug AV2 500-Mbit/s powerline Ethernet devices use 1024-QAM and 4096-QAM, as well as future devices using ITU-T G.hn standard for networking over existing home wiring (coaxial cable, phone lines and power lines); 4096-QAM provides 12 bits/symbol. Another example is ADSL technology for copper twisted pairs, whose constellation size goes up to 32768-QAM (in ADSL terminology this is referred to as bit-loading, or bit per tone, 32768-QAM being equivalent to 15 bits per tone).

Ultra-high capacity Microwave Backhaul Systems also use 1024-QAM. With 1024-QAM, adaptive coding and modulation (ACM) and XPIC, vendors can obtain gigabit capacity in a single 56 MHz channel.

In moving to a higher order QAM constellation (higher data rate and mode) in hostile RF/microwave QAM application environments, such as in broadcasting or telecommunications, multipath interference typically increases. There is a spreading of the spots in the constellation, decreasing the separation between adjacent states, making it difficult for the receiver to decode the signal appropriately. In other words, there is reduced noise immunity. There are several test parameter measurements which help determine an optimal QAM mode for a specific operating environment. The following three are most significant:





</doc>
<doc id="25317" url="https://en.wikipedia.org/wiki?curid=25317" title="QAM (disambiguation)">
QAM (disambiguation)

QAM stands for Quadrature amplitude modulation

QAM may also refer to:



</doc>
<doc id="25319" url="https://en.wikipedia.org/wiki?curid=25319" title="Quetzalcoatlus">
Quetzalcoatlus

Quetzalcoatlus northropi is a pterosaur known from the Late Cretaceous of North America (Maastrichtian stage) and one of the biggest known flying animals of all time. It is a member of the family Azhdarchidae, a family of advanced toothless pterosaurs with unusually long, stiffened necks. Its name comes from the Aztec feathered serpent god, Quetzalcoatl.

When it was first named as a new species in 1975, scientists estimated that the largest "Quetzalcoatlus" fossils came from an individual with a wingspan as large as . Choosing the middle of three extrapolations from the proportions of other pterosaurs gave an estimate of 11 m, 15.5 m, and 21 m, respectively (36 ft, 50.85 ft, 68.9 ft). In 1981, further advanced studies lowered these estimates to .

More recent estimates based on greater knowledge of azhdarchid proportions place its wingspan at . Remains found in Texas in 1971 indicate that this reptile had a minimum wingspan of about . Generalized height in a bipedal stance, based on its wingspan, would have been at least high at the shoulder.

Weight estimates for giant azhdarchids are extremely problematic because no existing species share a similar size or body plan, and in consequence, published results vary widely. Generalized weight, based on some studies that have historically found extremely low weight estimates for "Quetzalcoatlus", was as low as for a individual. A majority of estimates published since the 2000s have been substantially higher, around .

Skull material (from smaller specimens, possibly a related species) shows that "Quetzalcoatlus" had a very sharp and pointed beak. That is contrary to some earlier reconstructions that showed a blunter snout, based on the inadvertent inclusion of jaw material from another pterosaur species, possibly a tapejarid or a form related to "Tupuxuara". A skull crest was also present but its exact form and size are still unknown.

The first "Quetzalcoatlus" fossils were discovered in Texas, United States, from the Maastrichtian Javelina Formation at Big Bend National Park (dated to around 68 million years ago) in 1971 by Douglas A. Lawson, a geology graduate student from the Jackson School of Geosciences at the University of Texas at Austin. The specimen consisted of a partial wing (in pterosaurs composed of the forearms and elongated fourth finger), from an individual later estimated at over in wingspan.

Lawson discovered a second site of the same age, about from the first, where between 1972 and 1974 he and Professor Wann Langston Jr. of the Texas Memorial Museum unearthed three fragmentary skeletons of much smaller individuals. Lawson in 1975 announced the find in an article in "Science". That same year, in a subsequent letter to the same journal, he made the original large specimen, TMM 41450-3, the holotype of a new genus and species, Quetzalcoatlus northropi. The genus name refers to the Aztec feathered serpent god, Quetzalcoatl. The specific name honors John Knudsen Northrop, the founder of Northrop, who drove the development of large tailless flying wing aircraft designs resembling "Quetzalcoatlus".
At first it was assumed that the smaller specimens were juvenile or subadult forms of the larger type. Later, when more remains were found, it was realized they could have been a separate species. This possible second species from Texas was provisionally referred to as a "Quetzalcoatlus" sp. by Alexander Kellner and Langston in 1996, indicating that its status was too uncertain to give it a full new species name. The smaller specimens are more complete than the "Q. northropi" holotype, and include four partial skulls, though they are much less massive, with an estimated wingspan of .

The holotype specimen of "Q. northropi" has yet to be properly described and diagnosed, and the current status of the genus "Quetzalcoatlus" has been identified as problematic. Mark Witton and colleagues (2010) noted that the type species of the genus—the fragmentary wing bones comprising "Q. northropi"—represent elements which are typically considered undiagnostic to generic or specific level, and that this complicates interpretations of azhdarchid taxonomy. For instance, Witton et al. (2010) suggested that the "Q. northropi" type material is of generalised enough morphology to be near identical to that of other giant azhdarchids, such as the overlapping elements of the contemporary Romanian giant azhdarchid" Hatzegopteryx". This being the case, and assuming "Q. northropi" can be distinguished from other pterosaurs (i.e., if it is not a "nomen dubium"), perhaps "Hatzegopteryx" should be regarded as a European occurrence of "Quetzalcoatlus". However, Witton "et al." also noted that the skull material of "Hatzegopteryx" and "Q." sp. differ enough that they cannot be regarded as the same animal, but that the significance of this cannot be ascertained given uncertainty over the relationships of "Quetzalcoatlus" specimens. These issues can only be resolved by "Q. northropi" being demonstrated as a valid taxon and its relationships with "Q". sp. being investigated. An additional complication to these discussions are the likelihood that huge pterosaurs such as "Q. northropi" could have made long, transcontinental flights, suggesting that locations as disparate as North America and Europe could have shared giant azhdarchid species.

An azhdarchid neck vertebra, discovered in 2002 from the Maastrichtian age Hell Creek Formation, may also belong to "Quetzalcoatlus". The specimen (BMR P2002.2) was recovered accidentally when it was included in a field jacket prepared to transport part of a "Tyrannosaurus" specimen. Despite this association with the remains of a large carnivorous dinosaur, the vertebra shows no evidence that it was chewed on by the dinosaur. The bone came from an individual azhdarchid pterosaur estimated to have had a wingspan of .

Below is a cladogram showing the phylogenetic placement of "Quetzalcoatlus" within Neoazhdarchia from Andres and Myers (2013).

"Quetzalcoatlus" was abundant in Texas during the Lancian in a fauna dominated by "Alamosaurus". The "Alamosaurus"-"Quetzalcoatlus" association probably represents semi-arid inland plains. "Quetzalcoatlus" had precursors in North America and its apparent rise to widespreadness may represent the expansion of its preferred habitat rather than an immigration event, as some experts have suggested.

There have been a number of different ideas proposed about the lifestyle of "Quetzalcoatlus". Because the area of the fossil site was four hundred kilometers removed from the coastline and there were no indications of large rivers or deep lakes nearby at the end of the Cretaceous, Lawson in 1975 rejected a fish-eating lifestyle, instead suggesting that "Quetzalcoatlus" scavenged like the marabou stork (which will scavenge, but is more of a terrestrial predator of small animals), but then on the carcasses of titanosaur sauropods such as "Alamosaurus". Lawson had found the remains of the giant pterosaur while searching for the bones of this dinosaur, which formed an important part of its ecosystem.

In 1996, Lehman and Langston rejected the scavenging hypothesis, pointing out that the lower jaw bent so strongly downwards that even when it closed completely a gap of over five centimeters remained between it and the upper jaw, very different from the hooked beaks of specialized scavenging birds. They suggested that with its long neck vertebrae and long toothless jaws "Quetzalcoatlus" fed like modern-day skimmers, catching fish during flight while cleaving the waves with its beak. While this skim-feeding view became widely accepted, it was not subjected to scientific research until 2007 when a study showed that for such large pterosaurs it was not a viable method because the energy costs would be too high due to excessive drag. In 2008 pterosaur workers Mark Witton and Darren Naish published an examination of possible feeding habits and ecology of azhdarchids. Witton and Naish noted that most azhdarchid remains are found in inland deposits far from seas or other large bodies of water required for skimming. Additionally, the beak, jaw, and neck anatomy are unlike those of any known skimming animal. Rather, they concluded that azhdarchids were more likely terrestrial stalkers, similar to modern storks, and probably hunted small vertebrates on land or in small streams. Though "Quetzalcoatlus", like other pterosaurs, was a quadruped when on the ground, "Quetzalcoatlus" and other azhdarchids have fore and hind limb proportions more similar to modern running ungulate mammals than to their smaller cousins, implying that they were uniquely suited to a terrestrial lifestyle.

The nature of flight in "Quetzalcoatlus" and other giant azhdarchids was poorly understood until serious biomechanical studies were conducted in the 21st century. One early (1984) experiment by Paul MacCready used practical aerodynamics to test the flight of "Quetzalcoatlus". MacCready constructed a model flying machine or ornithopter with a simple computer functioning as an autopilot. The model successfully flew with a combination of soaring and wing flapping; the model was based on a then-current weight estimate of around , far lower than more modern estimates of over . The method of flight in these pterosaurs depends largely on weight, which has been controversial, and widely differing masses have been favored by different scientists. Some researchers have suggested that these animals employed slow, soaring flight, while others have concluded that their flight was fast and dynamic. In 2010, Donald Henderson argued that the mass of "Q. northropi" had been underestimated, even the highest estimates, and that it was too massive to have achieved powered flight. He estimated it in his 2010 paper as . Henderson argued that it may have been flightless.

Other flight capability estimates have disagreed with Henderson's research, suggesting instead an animal superbly adapted to long-range, extended flight. In 2010, Mike Habib, a professor of biomechanics at Chatham University, and Mark Witton, a British paleontologist, undertook further investigation into the claims of flightlessness in large pterosaurs. After factoring wingspan, body weight, and aerodynamics, computer modelling led the two researchers to conclude that "Q. northropi" was capable of flight up to for 7 to 10 days at altitudes of . Habib further suggested a maximum flight range of for "Q. northropi". Henderson's work was also further criticized by Witton and Habib in another study, which pointed out that although Henderson used excellent mass estimations, they were based on outdated pterosaur models, which caused Henderson's mass estimations to be more than double what Habib used in his estimations, and that anatomical study of "Q. northropi" and other big pterosaur forelimbs showed a higher degree of robustness than would be expected if they were purely quadrupedal. This study proposed that large pterosaurs most likely utilized a short burst of powered flight to then transition to thermal soaring.

In June 2010, several life-sized models of "Q. northropi" were put on display on London's South Bank as the centerpiece exhibit for the Royal Society's 350th-anniversary exhibition. The models, which included both flying and standing individuals with wingspans of , were intended to help build public interest in science. The models were created by scientists from the University of Portsmouth and engineers from Griffon Hoverwork. The display featured the most accurate pterosaur models constructed at the time; these models took into account the latest evidence based on skeletal and trace fossils from related pterosaurs.

In 1985, the US Defense Advanced Research Projects Agency (DARPA) and AeroVironment used "Quetzalcoatlus northropi" as the basis for an experimental ornithopter unmanned aerial vehicle (UAV). They produced a half-scale model weighing , with a wingspan of . Coincidentally, Douglas A. Lawson, who discovered "Q. northropi" in Texas in 1971, named it after John "Jack" Northrop, a developer of tailless flying wing aircraft in the 1940s. The replica of "Q. northropi" incorporates a "flight control system/autopilot which processes pilot commands and sensor inputs, implements several feedback loops, and delivers command signals to its various servo-actuators". It is on exhibit at the National Air and Space Museum.




</doc>
<doc id="25320" url="https://en.wikipedia.org/wiki?curid=25320" title="Quedlinburg">
Quedlinburg

Quedlinburg () is a town situated just north of the Harz mountains, in the district of Harz in the west of Saxony-Anhalt, Germany. In 1994, the castle, church and old town were added to the UNESCO World Heritage List.

Quedlinburg has a population of more than 24,000. The town was the capital of the district of Quedlinburg until 2007, when the district was dissolved. Several locations in the town are designated stops along a scenic holiday route, the Romanesque Road.

The town of Quedlinburg is known to have existed since at least the early 9th century, when there was a settlement known as "Gross Orden" on the eastern bank of the River Bode. It was first mentioned as a town in 922 as part of a donation by King Henry the Fowler ("Heinrich der Vogler"). The records of this donation were held by the abbey of Corvey.

According to legend, Henry had been offered the German crown at Quedlinburg in 919 by Franconian nobles, giving rise to the town being called the "cradle of the German Reich".

After Henry's death in 936, his widow Saint Matilda founded a religious community for women ("Frauenstift") on the castle hill, where daughters of the higher nobility were educated. The main task of this collegiate foundation, Quedlinburg Abbey, was to pray for the memory of King Henry and the rulers who came after him. The "Annals of Quedlinburg" were also compiled there. The first abbess was Matilda, a granddaughter of King Henry and St. Matilda.

The Quedlinburg castle complex, founded by King Henry I and built up by Emperor Otto I in 936, was an imperial "Pfalz" of the Saxon emperors. The "Pfalz", including the male convent, was in the valley, where today the Roman Catholic Church of "St. Wiperti" is situated, while the women's convent was located on the castle hill.

In 973, shortly before the death of Emperor Otto I, a "Reichstag" (Imperial Convention) was held at the imperial court in which Mieszko, duke of Polans, and Boleslav, duke of Bohemia, as well as numerous other nobles from as far away as Byzantium and Bulgaria, gathered to pay homage to the emperor. On the occasion, Otto the Great introduced his new daughter-in-law Theophanu, a Byzantine princess whose marriage to Otto II brought hope for recognition and continued peace between the rulers of the Eastern and Western empires.

In 994, Otto III granted the right of market, tax, and coining, and established the first market place to the north of the castle hill.

The town became a member of the Hanseatic League in 1426. Quedlinburg Abbey frequently disputed the independence of the town, which sought the aid of the Bishopric of Halberstadt. In 1477, Abbess Hedwig, aided by her brothers Ernest and Albert, broke the resistance of the town and expelled the bishop's forces. Quedlinburg was forced to leave the Hanseatic League and was subsequently protected by the Electorate of Saxony. Both town and abbey converted to Lutheranism in 1539 during the Protestant Reformation.

In 1697, Elector Frederick Augustus I of Saxony sold his rights to Quedlinburg to Elector Frederick III of Brandenburg for 240,000 thalers. Quedlinburg Abbey contested Brandenburg-Prussia's claims throughout the 18th century, however. The abbey was secularized in 1802 during the German Mediatisation, and Quedlinburg passed to the Kingdom of Prussia as part of the Principality of Quedlinburg. Part of the Napoleonic Kingdom of Westphalia from 1807–13, it was included within the new Prussian Province of Saxony in 1815. In all this time, ladies ruled Quedlinburg as abbesses without "taking the veil"; they were free to marry. The last of these ladies was a Swedish princess, an early fighter for women's rights, Sofia Albertina.

During the Nazi regime, the memory of Henry I became a sort of cult, as Heinrich Himmler saw himself as the reincarnation of the "most German of all German" rulers. The collegiate church and castle were to be turned into a shrine for Nazi Germany. The Nazi Party tried to create a new religion. The cathedral was closed from 1938 and during the war. The local crematory was kept busy burning the victims of the Langenstein-Zwieberge concentration camp. Georg Ay was local party chief from 1931 until the end of the war. Liberation in 1945 brought back the Protestant bishop and the church bells, and the Nazi-style eagle was taken down from the tower.

During the last months of World War II, the United States military had occupied Quedlinburg. In the 1980s, upon the death of one of the US military men, the theft of medieval art from Quedlinburg came to light.

Quedlinburg was administered within Bezirk Halle while part of the Communist East Germany from 1949 to 1990. It became part of the state of Saxony-Anhalt upon German reunification in 1990.

During Quedlinburg's Communist era, restoration specialists from Poland were called in during the 1980s to carry out repairs on the old architecture. Today, Quedlinburg is a center of restoration of "Fachwerk" houses.

The town is located north of the Harz mountains, about 123 m above NHN. The nearest mountains reach 181 m above NHN. The largest part of the town is located in the western part of the Bode river valley. This river comes from the Harz mountains and flows into the river Saale, a tributary of the river Elbe. The municipal area of Quedlinburg is . Before the incorporation of the two (previously independent) municipalities of Gernrode and Bad Suderode in January 2014 it was only .

Quedlinburg has an oceanic climate (Cfb) resulting from prevailing westerlies, blowing from the high-pressure area in the central Atlantic towards Scandinavia. Snowfall occurs almost every winter. January and February are the coldest months of the year, with an average temperature of 0.5 °C and 1.5 °C. July and August are the hottest months, with an average temperature of 17 °C (63 °F) and 18 °C (64 °F). The average annual precipitation is close to 438 mm with rain occurring usually from May to September. This precipitation is one of the lowest in Germany, which has an annual average close to 700 mm. In August 2010, Quedlinburg was the driest place in Germany, with only 72,4 l/m.

The mayor is Frank Ruch (CDU).

Quedlinburg is twinned with:

In the centre of the town are a wide selection of half-timbered buildings from at least five different centuries (including a 14th-century structure, one of Germany's oldest), while around the outer fringes of the old town are examples of "Jugendstil" buildings, dating from the late 19th and early 20th centuries.

Since December 1994, the old town of Quedlinburg and the castle mount with the "Stiftskirche" (collegiate church) are listed as one of UNESCO's World Heritage Sites. Quedlinburg is one of the best-preserved medieval and Renaissance towns in Europe, having escaped major damage in World War II.

In 2006, the Selke valley branch of the Harz Narrow Gauge Railways was extended to Quedlinburg from Gernrode, giving access to the historic steam narrow gauge railway, Alexisbad and the high Harz plateau.

The castle and "Stiftskirche St. Servatius" still dominate the town like in the early Middle Ages. The church is a prime example of German Romanesque style. The treasure of the church, containing ancient Christian religious artifacts and books, was stolen by an American soldier but brought back to Quedlinburg in 1993 and is again on display here.

The former "Stiftskirche St. Wiperti" was established in 936 when the "Kanonikerstift St. Wigpertus" (of male canons) was moved from the castle hill to make way for what became Quedlinburg Abbey. The church was built at the location of the first Ottonian Royal palace at Quedlinburg. Around 1020, a three-aisled crypt was added to the basilica. The crypt, which survived all later alterations to the church, is also a designated stop on the Romanesque Road today.

The nearest airports to Quedlinburg are Hannover, northwest, and Leipzig/Halle Airport, southeast. Much closer, but only served by a few airlines, is Magdeburg-Cochstedt. An airfield is located at Ballenstedt-Assmussstedt for general aviation.

Regional trains operated by Deutsche Bahn and the private Transdev company run on the standard-gauge Magdeburg–Thale line connecting Quedlinburg station with Magdeburg, Thale, and Halberstadt.

In 2006, the Selke Valley branch of the Harz Narrow Gauge Railways was extended into Quedlinburg from Gernrode, giving access via the historic steam-operated narrow-gauge railway to Alexisbad and the High Harz plateau.

Quedlinburg is connected by regional buses to the surrounding villages and small towns. Additionally, there are long distance buses to Berlin.







</doc>
<doc id="25321" url="https://en.wikipedia.org/wiki?curid=25321" title="Quantization">
Quantization

Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set (such as the integers). The terms "quantization" and "discretization" are often denotatively synonymous but not always connotatively interchangeable.





</doc>
<doc id="25322" url="https://en.wikipedia.org/wiki?curid=25322" title="Quantum theory">
Quantum theory

Quantum theory may refer to:





</doc>
<doc id="25323" url="https://en.wikipedia.org/wiki?curid=25323" title="QRP operation">
QRP operation

In amateur radio, QRP operation refers to transmitting at reduced power while attempting to maximize one's effective range. QRP operation is a specialized pursuit within the hobby that was first popularized in the early 1920s. QRP operators generally limit their transmitted RF output power to 5 watts or less for CW operation and 10 watts or less for SSB operation. Reliable two-way communication at such low power levels can be challenging due to changing radio propagation and the difficulty of receiving the relatively weak transmitted signals. QRP enthusiasts may employ optimized antenna systems, enhanced operating skills, and a variety of special modes, in order to maximize their ability to make and maintain radio contact. Since the late 1960s, commercial transceivers specially designed for QRP operation have evolved from vacuum tube to solid state technology. A number of organizations dedicated to QRP operation exist, and aficionados participate in various contests designed to test their skill in making long distance contacts at low power levels. 

The term QRP derives from the standard Q code used in radio communications, where "QRP" and "QRP?" are used to request, "Reduce power", and ask "Should I reduce power?" respectively. The opposite of QRP is QRO, or increased power operation.

Most amateur transceivers are capable of transmitting approximately 100 watts, but in some parts of the world, such as the U.S., amateurs can transmit up to 1,500 watts. QRP enthusiasts contend that this is not always necessary, and doing so wastes power, increases the likelihood of causing interference to nearby televisions, radios, and telephones and, for United States' amateurs, is incompatible with FCC Part 97 rule, which states that one must use "the minimum power necessary to carry out the desired communications". QRP can also be used for emergency communications during disaster recovery.

The practice of operating with low power was popularized as early as 1924, with a variety of reports, editorials and articles published in U.S. amateur radio magazines and journals that encouraged amateurs to lower power output, both for purposes of experimentation, and for improving operating conditions by reducing interference.

There is not complete agreement on what constitutes QRP power. Most amateur organizations agree that for CW, AM, FM, and data modes, the transmitter output power should be 5 watts (or less). The maximum output power for SSB (single sideband) is not always agreed upon. Some believe that the power should be no more than 10 watts peak envelope power (PEP), while others strongly hold that the power limit should be 5 watts. QRPers are known to use even less than five watts, sometimes operating with as little as 100 milliwatts or even less. Extremely low power—1 watt and below—is often referred to by hobbyists as QRPp.

Communicating using QRP can be difficult since the QRPer must face the same challenges of radio propagation faced by amateurs using higher power levels, but with the inherent disadvantages associated with having a weaker signal on the receiving end, all other things being equal. QRP aficionados try to make up for this through more efficient antenna systems and enhanced operating skills.

QRP enthusiasts may use special modes that employ technology and software designed to enhance reception of the relatively weak transmitted signals resulting from low power levels.


Many of the larger, more powerful commercial transceivers permit the operator to lower their output level to QRP levels. Commercial transceivers specially designed to operate at or near QRP power levels have been commercially available since the late 1960s. In 1969 the American manufacturer Ten-Tec produced the Powermite-1, one of Ten-Tec's first assembled transceivers, and featured modular construction. All stages of the transceiver were on individual circuit boards: the transmitter was capable of about one or two watts of RF, and the receiver was a direct-conversion unit, similar to that found in the Heathkit HW-7 and HW-8 lines, which introduced many amateurs to QRP'ing and led to the popularity of the mode.. Enthusiasts operate QRP radios on the HF bands in portable modes, usually carrying the radios in backpacks, with whip antennas. Some QRPers prefer to construct their equipment from kits, published plans, or homebrew it from scratch. Many popular designs are based on the NE612 mixer IC, i.e. the K1, K2, ATS series and the Softrock SDR.

Notable amateur radio organizations dedicated to QRP include QRP Amateur Radio Club International (QRPARCI), American QRP Club, G-QRP Club based in the United Kingdom, and The Adventure Radio Society emphasizing portable QRP operation. Major QRP gatherings are held yearly at hamfests such as Dayton Hamvention, Pacificon, and Frederichshafen.

There are specific operating awards, contests, clubs, and conventions devoted to QRP enthusiasts. In the United States, the November Sweepstakes, June and September VHF QSO Parties, January VHF Sweepstakes, and the ARRL International DX Contest, as well as many major international contests have designated special QRP categories. For example, during the annual ARRL's Field Day contest, making a QSO (ham-to-ham contact) using "QRP battery power" is worth five times as many points as a contact made by conventional means. The QRP ARCI club sponsors 12 contests during the year specifically for QRP operators.

Typical awards include the QRP ARCI club's "thousand-miles-per-watt" award, available to anyone presenting evidence of a qualifying contact. QRP ARCI also offers special awards for achieving the ARRL's Worked All States, Worked All Continents, and DX Century Club awards under QRP conditions. Other QRP clubs also offer similar versions of these awards, as well as general QRP operating achievement awards.



</doc>
<doc id="25327" url="https://en.wikipedia.org/wiki?curid=25327" title="QCD (disambiguation)">
QCD (disambiguation)

QCD, or Quantum chromodynamics, is the theory of the strong interaction between quarks and gluons.

QCD may also refer to:


</doc>
<doc id="25328" url="https://en.wikipedia.org/wiki?curid=25328" title="Quicksilver">
Quicksilver

Quicksilver may refer to:













</doc>
<doc id="25330" url="https://en.wikipedia.org/wiki?curid=25330" title="Quartet">
Quartet

In music, a quartet or quartette (, , , , ) is an ensemble of four singers or instrumental performers; or a musical composition for four voices or instruments.

In Classical music, one of the most common combinations of four instruments in chamber music is the string quartet. String quartets most often consist of two violins, a viola, and a cello. The particular choice and number of instruments derives from the registers of the human voice: soprano, alto, tenor and bass. In the string quartet, two violins play the soprano and alto vocal registers, the viola plays the tenor register and the cello plays the bass register.

Composers of notable string quartets include Joseph Haydn (68 compositions), Wolfgang Amadeus Mozart (23), Ludwig van Beethoven (16), Franz Schubert (15), Felix Mendelssohn (6), Johannes Brahms (3), Antonín Dvořák (14), Alexander Borodin (2), Béla Bartók (6), Elizabeth Maconchy (13), Darius Milhaud (18), Heitor Villa-Lobos (17), and Dmitri Shostakovich (15). The Italian composer Luigi Boccherini (1743–1805), wrote 91 string quartets.

Less often, string quartets are written for other combinations of the standard string ensemble. These include quartets for one violin, two violas, and one cello, notably by Carl Stamitz (6 compositions) and others; and for one violin, one viola, and two cellos, by Johann Georg Albrechtsberger and others.

Another common standard classical quartet is the piano quartet, consisting of violin, viola, cello, and piano. Romantic composers Beethoven, Brahms, and Mendelssohn each wrote three important compositions in this form, and Mozart, Dvořák, and Gabriel Fauré each wrote two.

Wind quartets are scored either the same as a string quartet with the wind instrument replacing the first violin (i.e. scored for wind, violin, viola and cello) or are groups of four wind instruments. Among the latter, the SATB format woodwind quartet of flute, oboe, clarinet, and bassoon is relatively common.

An example of a wind quartet featuring four of the same types of wind instruments is the saxophone quartet, consisting of soprano saxophone, alto saxophone, tenor saxophone and baritone saxophone or (SATB). Often a second alto may be substituted for the soprano part (AATB) or a bass saxophone may be substituted for the baritone.

Compositions for four singers have been written for quartets a cappella; accompanied by instruments, such as a piano; and accompanied by larger vocal forces, such as a choir. Brahms and Schubert wrote numerous pieces for four voices that were once popular in private salons, although they are seldom performed today. Vocal quartets also feature within larger classical compositions, such as opera, choral works, and symphonic compositions. The final movement of Beethoven's Ninth Symphony and the Verdi Requiem are two examples of renowned concert works that include vocal quartets.

Typically, a vocal quartet is composed of:

The baroque quartet is a form of music composition similar to the trio sonata, but with four music parts performed by three solo melodic instruments and basso continuo. The solo instruments could be strings or wind instruments.

Examples of baroque quartets are Telemann's Paris quartets.

Quartets are popular in jazz and jazz fusion music. Jazz quartet ensembles are often composed of a horn, classically clarinet (or saxophone, trumpet, etc.), a chordal instrument (e.g., electric guitar, piano, Hammond organ, etc.), a bass instrument (e.g., double bass, tuba or bass guitar) and a drum kit. This configuration is sometimes modified by using a second horn replacing the chordal instrument, such as a trumpet and saxophone with string bass and drum kit, or by using two chordal instruments (e.g., piano and electric guitar).

In 20th century Western popular music, the term "vocal quartet" usually refers to ensembles of four singers of the same gender. This is particularly common for barbershop quartets and Gospel quartets.

Some well-known female US vocal quartets include The Carter Sisters; The Forester Sisters; The Chiffons; The Chordettes; The Lennon Sisters; and En Vogue. Some well-known male US vocal quartets include The Oak Ridge Boys; The Statler Brothers; The Ames Brothers; The Chi-Lites; Crosby Stills Nash & Young; The Dixie Hummingbirds; The Four Aces; Four Freshmen; The Four Seasons; The Four Tops; The Cathedral Quartet; Ernie Haase and Signature Sound; The Golden Gate Quartet; The Hilltoppers; The Jordanaires; and Mills Brothers. The only known U.S. drag quartet is The Kinsey Sicks. Some mixed-gender vocal quartets include The Pied Pipers; The Mamas & the Papas; The Merry Macs; and The Weavers.

The quartet lineup also is very common in pop and rock music. A standard quartet formation in pop and rock music is an ensemble consisting of two electric guitars, a bass guitar, and a drum kit. This configuration is sometimes modified by using a keyboard instrument (e.g., organ, piano, synthesizer) or a soloing instrument (e.g., saxophone) in place of the second electric guitar.

A Russian folk-instrument quartet commonly consists of a bayan, a prima balalaika, a prima or alto domra, and a contrabass balalaika (e.g., Quartet Moskovskaya Balalaika). Configurations without a bayan include a prima domra, a prima balalaika, an alto domra, and a bass balalaika (Quartet Skaz); or two prima domras, a prima balalaika, and a bass balalaika. 



</doc>
<doc id="25336" url="https://en.wikipedia.org/wiki?curid=25336" title="Quantum entanglement">
Quantum entanglement

Quantum entanglement is a label for the observed physical phenomenon that occurs when a pair or group of particles is generated, interact, or share spatial proximity in a way such that the quantum state of each particle of the pair or group cannot be described independently of the state of the others, even when the particles are separated by a large distance. The topic of quantum entanglement is at the heart of the disparity between classical and quantum physics.

Measurements of physical properties such as position, momentum, spin, and polarization performed on entangled particles are found to be perfectly correlated. For example, if a pair of entangled particles is generated such that their total spin is known to be zero, and one particle is found to have clockwise spin on a first axis, then the spin of the other particle, measured on the same axis, will be found to be counterclockwise. However, this behavior gives rise to seemingly paradoxical effects: any measurement of a property of a particle results in an irreversible wave function collapse of that particle and will change the original quantum state. In the case of entangled particles, such a measurement will affect the entangled system as a whole.

Such phenomena were the subject of a 1935 paper by Albert Einstein, Boris Podolsky, and Nathan Rosen, and several papers by Erwin Schrödinger shortly thereafter, describing what came to be known as the EPR paradox. Einstein and others considered such behavior to be impossible, as it violated the local realism view of causality (Einstein referring to it as "spooky action at a distance") and argued that the accepted formulation of quantum mechanics must therefore be incomplete.

Later, however, the counterintuitive predictions of quantum mechanics were verified experimentally in tests in which polarization or spin of entangled particles were measured at separate locations, statistically violating Bell's inequality. In earlier tests it couldn't be absolutely ruled out that the test result at one point could have been subtly transmitted to the remote point, affecting the outcome at the second location. However so-called "loophole-free" Bell tests have been performed in which the locations were separated such that communications at the speed of light would have taken longer—in one case 10,000 times longer—than the interval between the measurements.

According to "some" interpretations of quantum mechanics, the effect of one measurement occurs instantly. Other interpretations which don't recognize wavefunction collapse dispute that there is any "effect" at all. However, all interpretations agree that entanglement produces "correlation" between the measurements and that the mutual information between the entangled particles can be exploited, but that any "transmission" of information at faster-than-light speeds is impossible.

Quantum entanglement has been demonstrated experimentally with photons, neutrinos, electrons, molecules as large as buckyballs, and even small diamonds. On 13 July 2019, scientists from the University of Glasgow reported taking the first ever photo of a strong form of quantum entanglement known as Bell entanglement. The utilization of entanglement in communication, computation and quantum radar is a very active area of research and development.

The counterintuitive predictions of quantum mechanics about strongly correlated systems were first discussed by Albert Einstein in 1935, in a joint paper with Boris Podolsky and Nathan Rosen. 
In this study, the three formulated the Einstein–Podolsky–Rosen paradox (EPR paradox), a thought experiment that attempted to show that quantum mechanical theory was incomplete. They wrote: "We are thus forced to conclude that the quantum-mechanical description of physical reality given by wave functions is not complete."

However, the three scientists did not coin the word "entanglement", nor did they generalize the special properties of the state they considered. Following the EPR paper, Erwin Schrödinger wrote a letter to Einstein in German in which he used the word "Verschränkung" (translated by himself as "entanglement") "to describe the correlations between two particles that interact and then separate, as in the EPR experiment."

Schrödinger shortly thereafter published a seminal paper defining and discussing the notion of "entanglement." In the paper, he recognized the importance of the concept, and stated: "I would not call [entanglement] "one" but rather "the" characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought."

Like Einstein, Schrödinger was dissatisfied with the concept of entanglement, because it seemed to violate the speed limit on the transmission of information implicit in the theory of relativity. Einstein later famously derided entanglement as ""spukhafte Fernwirkung"" or "spooky action at a distance."

The EPR paper generated significant interest among physicists, which inspired much discussion about the foundations of quantum mechanics (perhaps most famously Bohm's interpretation of quantum mechanics), but produced relatively little other published work. Despite the interest, the weak point in EPR's argument was not discovered until 1964, when John Stewart Bell proved that one of their key assumptions, the principle of locality, as applied to the kind of hidden variables interpretation hoped for by EPR, was mathematically inconsistent with the predictions of quantum theory.

Specifically, Bell demonstrated an upper limit, seen in Bell's inequality, regarding the strength of correlations that can be produced in any theory obeying local realism, and showed that quantum theory predicts violations of this limit for certain entangled systems. His inequality is experimentally testable, and there have been numerous relevant experiments, starting with the pioneering work of Stuart Freedman and John Clauser in 1972 and Alain Aspect's experiments in 1982. An early experimental breakthrough was due to Carl Kocher, who already in 1967 presented an apparatus in which two photons successively emitted from a calcium atom were shown to be entangled – the first case of entangled visible light. The two photons passed diametrically positioned parallel polarizers with higher probability than classically predicted but with correlations in quantitative agreement with quantum mechanical calculations. He also showed that the correlation varied only upon (as cosine square of) the angle between the polarizer settings and decreased exponentially with time lag between emitted photons. Kocher’s apparatus, equipped with better polarizers, was used by Freedman and Clauser who could confirm the cosine square dependence and use it to demonstrate a violation of Bell’s inequality for a set of fixed angles. All these experiments have shown agreement with quantum mechanics rather than the principle of local realism.

For decades, each had left open at least one loophole by which it was possible to question the validity of the results. However, in 2015 an experiment was performed that simultaneously closed both the detection and locality loopholes, and was heralded as "loophole-free"; this experiment ruled out a large class of local realism theories with certainty. Alain Aspect notes that the "setting-independence loophole" – which he refers to as "far-fetched", yet, a "residual loophole" that "cannot be ignored" – has yet to be closed, and the free-will / "superdeterminism" loophole is unclosable; saying "no experiment, as ideal as it is, can be said to be totally loophole-free."

A minority opinion holds that although quantum mechanics is correct, there is no superluminal instantaneous action-at-a-distance between entangled particles once the particles are separated.

Bell's work raised the possibility of using these super-strong correlations as a resource for communication. It led to the 1984 discovery of quantum key distribution protocols, most famously BB84 by Charles H. Bennett and Gilles Brassard and E91 by Artur Ekert. Although BB84 does not use entanglement, Ekert's protocol uses the violation of a Bell's inequality as a proof of security.

In October 2018, physicists reported that quantum behavior can be explained with classical physics for a single particle, but not for multiple particles as in quantum entanglement and related nonlocality phenomena.

In July 2019 physicists reported, for the first time, capturing an image of quantum entanglement.

An entangled system is defined to be one whose quantum state cannot be factored as a product of states of its local constituents; that is to say, they are not individual particles but are an inseparable whole. In entanglement, one constituent cannot be fully described without considering the other(s). The state of a composite system is always expressible as a sum, or superposition, of products of states of local constituents; it is entangled if this sum necessarily has more than one term.

Quantum systems can become entangled through various types of interactions. For some ways in which entanglement may be achieved for experimental purposes, see the section below on methods. Entanglement is broken when the entangled particles decohere through interaction with the environment; for example, when a measurement is made.

As an example of entanglement: a subatomic particle decays into an entangled pair of other particles. The decay events obey the various conservation laws, and as a result, the measurement outcomes of one daughter particle must be highly correlated with the measurement outcomes of the other daughter particle (so that the total momenta, angular momenta, energy, and so forth remains roughly the same before and after this process). For instance, a spin-zero particle could decay into a pair of spin-½ particles. Since the total spin before and after this decay must be zero (conservation of angular momentum), whenever the first particle is measured to be spin up on some axis, the other, when measured on the same axis, is always found to be spin down. (This is called the spin anti-correlated case; and if the prior probabilities for measuring each spin are equal, the pair is said to be in the singlet state.)

The special property of entanglement can be better observed if we separate the said two particles. Let's put one of them in the White House in Washington and the other in Buckingham Palace (think about this as a thought experiment, not an actual one). Now, if we measure a particular characteristic of one of these particles (say, for example, spin), get a result, and then measure the other particle using the same criterion (spin along the same axis), we find that the result of the measurement of the second particle will match (in a complementary sense) the result of the measurement of the first particle, in that they will be opposite in their values.

The above result may or may not be perceived as surprising. A classical system would display the same property, and a hidden variable theory (see below) would certainly be required to do so, based on conservation of angular momentum in classical and quantum mechanics alike. The difference is that a classical system has definite values for all the observables all along, while the quantum system does not. In a sense to be discussed below, the quantum system considered here seems to acquire a probability distribution for the outcome of a measurement of the spin along any axis of the other particle upon measurement of the first particle. This probability distribution is in general different from what it would be without measurement of the first particle. This may certainly be perceived as surprising in the case of spatially separated entangled particles.

The paradox is that a measurement made on either of the particles apparently collapses the state of the entire entangled system—and does so instantaneously, before any information about the measurement result could have been communicated to the other particle (assuming that information cannot travel faster than light) and hence assured the "proper" outcome of the measurement of the other part of the entangled pair. In the Copenhagen interpretation, the result of a spin measurement on one of the particles is a collapse into a state in which each particle has a definite spin (either up or down) along the axis of measurement. The outcome is taken to be random, with each possibility having a probability of 50%. However, if both spins are measured along the same axis, they are found to be anti-correlated. This means that the random outcome of the measurement made on one particle seems to have been transmitted to the other, so that it can make the "right choice" when it too is measured.

The distance and timing of the measurements can be chosen so as to make the interval between the two measurements spacelike, hence, any causal effect connecting the events would have to travel faster than light. According to the principles of special relativity, it is not possible for any information to travel between two such measuring events. It is not even possible to say which of the measurements came first. For two spacelike separated events and there are inertial frames in which is first and others in which is first. Therefore, the correlation between the two measurements cannot be explained as one measurement determining the other: different observers would disagree about the role of cause and effect.

A possible resolution to the paradox is to assume that quantum theory is incomplete, and the result of measurements depends on predetermined "hidden variables". The state of the particles being measured contains some hidden variables, whose values effectively determine, right from the moment of separation, what the outcomes of the spin measurements are going to be. This would mean that each particle carries all the required information with it, and nothing needs to be transmitted from one particle to the other at the time of measurement. Einstein and others (see the previous section) originally believed this was the only way out of the paradox, and the accepted quantum mechanical description (with a random measurement outcome) must be incomplete.

The hidden variables theory fails, however, when measurements of the spin of entangled particles along different axes are considered. If a large number of pairs of such measurements are made (on a large number of pairs of entangled particles), then statistically, if the local realist or hidden variables view were correct, the results would always satisfy Bell's inequality. A number of experiments have shown in practice that Bell's inequality is not satisfied. However, prior to 2015, all of these had loophole problems that were considered the most important by the community of physicists. When measurements of the entangled particles are made in moving relativistic reference frames, in which each measurement (in its own relativistic time frame) occurs before the other, the measurement results remain correlated.

The fundamental issue about measuring spin along different axes is that these measurements cannot have definite values at the same time―they are incompatible in the sense that these measurements' maximum simultaneous precision is constrained by the uncertainty principle. This is contrary to what is found in classical physics, where any number of properties can be measured simultaneously with arbitrary accuracy. It has been proven mathematically that compatible measurements cannot show Bell-inequality-violating correlations, and thus entanglement is a fundamentally non-classical phenomenon.
In experiments in 2012 and 2013, polarization correlation was created between photons that never coexisted in time. The authors claimed that this result was achieved by entanglement swapping between two pairs of entangled photons after measuring the polarization of one photon of the early pair, and that it proves that quantum non-locality applies not only to space but also to time.

In three independent experiments in 2013 it was shown that classically-communicated separable quantum states can be used to carry entangled states. The first loophole-free Bell test was held in TU Delft in 2015 confirming the violation of Bell inequality.

In August 2014, Brazilian researcher Gabriela Barreto Lemos and team were able to "take pictures" of objects using photons that had not interacted with the subjects, but were entangled with photons that did interact with such objects. Lemos, from the University of Vienna, is confident that this new quantum imaging technique could find application where low light imaging is imperative, in fields like biological or medical imaging.

In 2015, Markus Greiner's group at Harvard performed a direct measurement of Renyi entanglement in a system of ultracold bosonic atoms.

From 2016 various companies like IBM, Microsoft etc. have successfully created quantum computers and allowed developers and tech enthusiasts to openly experiment with concepts of quantum mechanics including quantum entanglement.

There have been suggestions to look at the concept of time as an emergent phenomenon that is a side effect of quantum entanglement. 
In other words, time is an entanglement phenomenon, which places all equal clock readings (of correctly prepared clocks, or of any objects usable as clocks) into the same history. This was first fully theorized by Don Page and William Wootters in 1983. 
The Wheeler–DeWitt equation that combines general relativity and quantum mechanics – by leaving out time altogether – was introduced in the 1960s and it was taken up again in 1983, when Page and Wootters made a solution based on quantum entanglement. Page and Wootters argued that entanglement can be used to measure time.

In 2013, at the Istituto Nazionale di Ricerca Metrologica (INRIM) in Turin, Italy, researchers performed the first experimental test of Page and Wootters' ideas. Their result has been interpreted to confirm that time is an emergent phenomenon for internal observers but absent for external observers of the universe just as the Wheeler-DeWitt equation predicts.

Physicist Seth Lloyd says that quantum uncertainty gives rise to entanglement, the putative source of the arrow of time. According to Lloyd; "The arrow of time is an arrow of increasing correlations." The approach to entanglement would be from the perspective of the causal arrow of time, with the assumption that the cause of the measurement of one particle determines the effect of the result of the other particle's measurement.

Based on AdS/CFT correspondence, Mark Van Raamsdonk suggested that spacetime arises as an emergent phenomenon of the quantum degrees of freedom that are entangled and live in the boundary of the space-time. Induced gravity can emerge from the entanglement first law.

In the media and popular science, quantum non-locality is often portrayed as being equivalent to entanglement. While this is true for pure bipartite quantum states, in general entanglement is only necessary for non-local correlations, but there exist mixed entangled states that do not produce such correlations. A well-known example is the Werner states that are entangled for certain values of formula_1, but can always be described using local hidden variables. Moreover, it was shown that, for arbitrary numbers of parties, there exist states that are genuinely entangled but admit a local model. 
The mentioned proofs about the existence of local models assume that there is only one copy of the quantum state available at a time. If the parties are allowed to perform local measurements on many copies of such states, then many apparently local states (e.g., the qubit Werner states) can no longer be described by a local model. This is, in particular, true for all distillable states. However, it remains an open question whether all entangled states become non-local given sufficiently many copies.

In short, entanglement of a state shared by two parties is necessary but not sufficient for that state to be non-local. It is important to recognize that entanglement is more commonly viewed as an algebraic concept, noted for being a prerequisite to non-locality as well as to quantum teleportation and to superdense coding, whereas non-locality is defined according to experimental statistics and is much more involved with the foundations and interpretations of quantum mechanics.

The following subsections are for those with a good working knowledge of the formal, mathematical description of quantum mechanics, including familiarity with the formalism and theoretical framework developed in the articles: bra–ket notation and mathematical formulation of quantum mechanics.

Consider two noninteracting systems and , with respective Hilbert spaces and . The Hilbert space of the composite system is the tensor product

If the first system is in state formula_3 and the second in state formula_4, the state of the composite system is

States of the composite system that can be represented in this form are called separable states, or product states.

Not all states are separable states (and thus product states). Fix a basis formula_6 for and a basis formula_7 for . The most general state in is of the form

This state is separable if there exist vectors formula_9 so that formula_10 yielding formula_11 and formula_12 It is inseparable if for any vectors formula_13 at least for one pair of coordinates formula_14 we have formula_15 If a state is inseparable, it is called an 'entangled state'.

For example, given two basis vectors formula_16 of and two basis vectors formula_17 of , the following is an entangled state:

If the composite system is in this state, it is impossible to attribute to either system or system a definite pure state. Another way to say this is that while the von Neumann entropy of the whole state is zero (as it is for any pure state), the entropy of the subsystems is greater than zero. In this sense, the systems are "entangled". This has specific empirical ramifications for interferometry. The above example is one of four Bell states, which are (maximally) entangled pure states (pure states of the space, but which cannot be separated into pure states of each and ).

Now suppose Alice is an observer for system , and Bob is an observer for system . If in the entangled state given above Alice makes a measurement in the formula_19 eigenbasis of , there are two possible outcomes, occurring with equal probability:


If the former occurs, then any subsequent measurement performed by Bob, in the same basis, will always return 1. If the latter occurs, (Alice measures 1) then Bob's measurement will return 0 with certainty. Thus, system has been altered by Alice performing a local measurement on system . This remains true even if the systems and are spatially separated. This is the foundation of the EPR paradox.

The outcome of Alice's measurement is random. Alice cannot decide which state to collapse the composite system into, and therefore cannot transmit information to Bob by acting on her system. Causality is thus preserved, in this particular scheme. For the general argument, see no-communication theorem.

As mentioned above, a state of a quantum system is given by a unit vector in a Hilbert space. More generally, if one has less information about the system, then one calls it an 'ensemble' and describes it by a density matrix, which is a positive-semidefinite matrix, or a trace class when the state space is infinite-dimensional, and has trace 1. Again, by the spectral theorem, such a matrix takes the general form:

where the "w" are positive-valued probabilities (they sum up to 1), the vectors are unit vectors, and in the infinite-dimensional case, we would take the closure of such states in the trace norm. We can interpret as representing an ensemble where is the proportion of the ensemble whose states are formula_23. When a mixed state has rank 1, it therefore describes a 'pure ensemble'. When there is less than total information about the state of a quantum system we need density matrices to represent the state.

Experimentally, a mixed ensemble might be realized as follows. Consider a "black box" apparatus that spits electrons towards an observer. The electrons' Hilbert spaces are identical. The apparatus might produce electrons that are all in the same state; in this case, the electrons received by the observer are then a pure ensemble. However, the apparatus could produce electrons in different states. For example, it could produce two populations of electrons: one with state formula_24 with spins aligned in the positive direction, and the other with state formula_25 with spins aligned in the negative direction. Generally, this is a mixed ensemble, as there can be any number of populations, each corresponding to a different state.

Following the definition above, for a bipartite composite system, mixed states are just density matrices on . That is, it has the general form

where the "w" are positively valued probabilities, formula_27, and the vectors are unit vectors. This is self-adjoint and positive and has trace 1.

Extending the definition of separability from the pure case, we say that a mixed state is separable if it can be written as

where the are positively valued probabilities and the formula_29's and formula_30's are themselves mixed states (density operators) on the subsystems and respectively. In other words, a state is separable if it is a probability distribution over uncorrelated states, or product states. By writing the density matrices as sums of pure ensembles and expanding, we may assume without loss of generality that formula_29 and formula_30 are themselves pure ensembles. A state is then said to be entangled if it is not separable.

In general, finding out whether or not a mixed state is entangled is considered difficult. The general bipartite case has been shown to be NP-hard. For the and cases, a necessary and sufficient criterion for separability is given by the famous Positive Partial Transpose (PPT) condition.

The idea of a reduced density matrix was introduced by Paul Dirac in 1930. Consider as above systems and each with a Hilbert space . Let the state of the composite system be

As indicated above, in general there is no way to associate a pure state to the component system . However, it still is possible to associate a density matrix. Let

which is the projection operator onto this state. The state of is the partial trace of over the basis of system :

For example, the reduced density matrix of for the entangled state

discussed above is

This demonstrates that, as expected, the reduced density matrix for an entangled pure ensemble is a mixed ensemble. Also not surprisingly, the density matrix of for the pure product state formula_38 discussed above is

In general, a bipartite pure state ρ is entangled if and only if its reduced states are mixed rather than pure.

Reduced density matrices were explicitly calculated in different spin chains with unique ground state. An example is the one-dimensional AKLT spin chain: the ground state can be divided into a block and an environment. The reduced density matrix of the block is proportional to a projector to a degenerate ground state of another Hamiltonian.

The reduced density matrix also was evaluated for XY spin chains, where it has full rank. It was proved that in the thermodynamic limit, the spectrum of the reduced density matrix of a large block of spins is an exact geometric sequence in this case.

In quantum information theory, entangled states are considered a 'resource', i.e., something costly to produce and that allows to implement valuable transformations. The setting in which this perspective is most evident is that of "distant labs", i.e., two quantum systems labeled "A" and "B" on each of which arbitrary quantum operations can be performed, but which do not interact with each other quantum mechanically. The only interaction allowed is the exchange of classical information, which combined with the most general local quantum operations gives rise to the class of operations called LOCC (local operations and classical communication). These operations do not allow the production of entangled states between the systems A and B. But if A and B are provided with a supply of entangled states, then these, together with LOCC operations can enable a larger class of transformations. For example, an interaction between a qubit of A and a qubit of B can be realized by first teleporting A's qubit to B, then letting it interact with B's qubit (which is now a LOCC operation, since both qubits are in B's lab) and then teleporting the qubit back to A. Two maximally entangled states of two qubits are used up in this process. Thus entangled states are a resource that enables the realization of quantum interactions (or of quantum channels) in a setting where only LOCC are available, but they are consumed in the process. There are other applications where entanglement can be seen as a resource, e.g., private communication or distinguishing quantum states.

Not all quantum states are equally valuable as a resource. To quantify this value, different entanglement measures (see below) can be used, that assign a numerical value to each quantum state. However, it is often interesting to settle for a coarser way to compare quantum states. This gives rise to different classification schemes. Most entanglement classes are defined based on whether states can be converted to other states using LOCC or a subclass of these operations. The smaller the set of allowed operations, the finer the classification. Important examples are:

A different entanglement classification is based on what the quantum correlations present in a state allow A and B to do: one distinguishes three subsets of entangled states: (1) the "non-local states", which produce correlations that cannot be explained by a local hidden variable model and thus violate a Bell inequality, (2) the "steerable states" that contain sufficient correlations for A to modify ("steer") by local measurements the conditional reduced state of B in such a way, that A can prove to B that the state they possess is indeed entangled, and finally (3) those entangled states that are neither non-local nor steerable. All three sets are non-empty.

In this section, the entropy of a mixed state is discussed as well as how it can be viewed as a measure of quantum entanglement.

In classical information theory , the Shannon entropy, is associated to a probability distribution,formula_50, in the following way:

Since a mixed state is a probability distribution over an ensemble, this leads naturally to the definition of the von Neumann entropy:

In general, one uses the Borel functional calculus to calculate a non-polynomial function such as . If the nonnegative operator acts on a finite-dimensional Hilbert space and has eigenvalues formula_53, turns out to be nothing more than the operator with the same eigenvectors, but the eigenvalues formula_54. The Shannon entropy is then:

Since an event of probability 0 should not contribute to the entropy, and given that

the convention is adopted. This extends to the infinite-dimensional case as well: if has spectral resolution

assume the same convention when calculating

As in statistical mechanics, the more uncertainty (number of microstates) the system should possess, the larger the entropy. For example, the entropy of any pure state is zero, which is unsurprising since there is no uncertainty about a system in a pure state. The entropy of any of the two subsystems of the entangled state discussed above is (which can be shown to be the maximum entropy for mixed states).

Entropy provides one tool that can be used to quantify entanglement, although other entanglement measures exist. If the overall system is pure, the entropy of one subsystem can be used to measure its degree of entanglement with the other subsystems.

For bipartite pure states, the von Neumann entropy of reduced states is the unique measure of entanglement in the sense that it is the only function on the family of states that satisfies certain axioms required of an entanglement measure.

It is a classical result that the Shannon entropy achieves its maximum at, and only at, the uniform probability distribution {1/"n"...,1/"n"}. Therefore, a bipartite pure state is said to be a maximally entangled state if the reduced state of is the diagonal matrix

For mixed states, the reduced von Neumann entropy is not the only reasonable entanglement measure.

As an aside, the information-theoretic definition is closely related to entropy in the sense of statistical mechanics (comparing the two definitions in the present context, it is customary to set the Boltzmann constant ). For example, by properties of the Borel functional calculus, we see that for any unitary operator ,

Indeed, without this property, the von Neumann entropy would not be well-defined.

In particular, could be the time evolution operator of the system, i.e.,

where is the Hamiltonian of the system. Here the entropy is unchanged.

The reversibility of a process is associated with the resulting entropy change, i.e., a process is reversible if, and only if, it leaves the entropy of the system invariant. Therefore, the march of the arrow of time towards thermodynamic equilibrium is simply the growing spread of quantum entanglement.
This provides a connection between quantum information theory and thermodynamics.

Rényi entropy also can be used as a measure of entanglement.

Entanglement measures quantify the amount of entanglement in a (often viewed as a bipartite) quantum state. As aforementioned, entanglement entropy is the standard measure of entanglement for pure states (but no longer a measure of entanglement for mixed states). For mixed states, there are some entanglement measures in the literature and no single one is standard.

Most (but not all) of these entanglement measures reduce for pure states to entanglement entropy, and are difficult (NP-hard) to compute.

The Reeh-Schlieder theorem of quantum field theory is sometimes seen as an analogue of quantum entanglement.

Entanglement has many applications in quantum information theory. With the aid of entanglement, otherwise impossible tasks may be achieved.

Among the best-known applications of entanglement are superdense coding and quantum teleportation.

Most researchers believe that entanglement is necessary to realize quantum computing (although this is disputed by some).

Entanglement is used in some protocols of quantum cryptography. This is because the "shared noise" of entanglement makes for an excellent one-time pad. Moreover, since measurement of either member of an entangled pair destroys the entanglement they share, entanglement-based quantum cryptography allows the sender and receiver to more easily detect the presence of an interceptor.

In interferometry, entanglement is necessary for surpassing the standard quantum limit and achieving the Heisenberg limit.

There are several canonical entangled states that appear often in theory and experiments.

For two qubits, the Bell states are

These four pure states are all maximally entangled (according to the entropy of entanglement) and form an orthonormal basis (linear algebra) of the Hilbert space of the two qubits. They play a fundamental role in Bell's theorem.

For M>2 qubits, the GHZ state is

which reduces to the Bell state formula_65 for formula_66. The traditional GHZ state was defined for formula_67. GHZ states are occasionally extended to qudits, i.e., systems of "d" rather than 2 dimensions.

Also for M>2 qubits, there are spin squeezed states. Spin squeezed states are a class of squeezed coherent states satisfying certain restrictions on the uncertainty of spin measurements, and are necessarily entangled. Spin squeezed states are good candidates for enhancing precision measurements using quantum entanglement.

For two bosonic modes, a NOON state is

This is like the Bell state formula_69 except the basis kets 0 and 1 have been replaced with "the "N" photons are in one mode" and "the "N" photons are in the other mode".

Finally, there also exist twin Fock states for bosonic modes, which can be created by feeding a Fock state into two arms leading to a beam splitter. They are the sum of multiple of NOON states, and can used to achieve the Heisenberg limit.

For the appropriately chosen measure of entanglement, Bell, GHZ, and NOON states are maximally entangled while spin squeezed and twin Fock states are only partially entangled. The partially entangled states are generally easier to prepare experimentally.

Entanglement is usually created by direct interactions between subatomic particles. These interactions can take numerous forms. One of the most commonly used methods is spontaneous parametric down-conversion to generate a pair of photons entangled in polarisation. Other methods include the use of a fiber coupler to confine and mix photons, photons emitted from decay cascade of the bi-exciton in a quantum dot, the use of the Hong–Ou–Mandel effect, etc., In the earliest tests of Bell's theorem, the entangled particles were generated using atomic cascades.

It is also possible to create entanglement between quantum systems that never directly interacted, through the use of entanglement swapping. Two independently-prepared, identical particles may also be entangled if their wave functions merely spatially overlap, at least partially.

A density matrix ρ is called separable if it can be written as a convex sum of product states, namely

formula_70

with formula_71 probabilities. By definition, a state is entangled if it is not separable.

For 2-Qubit and Qubit-Qutrit systems (2 × 2 and 2 × 3 respectively) the simple Peres–Horodecki criterion provides both a necessary and a sufficient criterion for separability, and thus -inadvertently- for detecting entanglement. However, for the general case, the criterion is merely a necessary one for separability, as the problem becomes NP-hard when generalized. Other separability criteria include (but not limited to) the range criterion, reduction criterion, and those based on uncertainty relations. See Ref. for a review of separability criteria in discrete variable systems.

A numerical approach to the problem is suggested by Jon Magne Leinaas, Jan Myrheim and Eirik Ovrum in their paper "Geometrical aspects of entanglement". Leinaas et al. offer a numerical approach, iteratively refining an estimated separable state towards the target state to be tested, and checking if the target state can indeed be reached. An implementation of the algorithm (including a built-in Peres-Horodecki criterion testing) is "StateSeparator" web-app.

In continuous variable systems, the Peres-Horodecki criterion also applies. Specifically, Simon formulated a particular version of the Peres-Horodecki criterion in terms of the second-order moments of canonical operators and showed that it is necessary and sufficient for formula_72-mode Gaussian states (see Ref. for a seemingly different but essentially equivalent approach). It was later found that Simon's condition is also necessary and sufficient for formula_73-mode Gaussian states, but no longer sufficient for formula_74-mode Gaussian states. Simon's condition can be generalized by taking into account the higher order moments of canonical operators or by using entropic measures.

In 2016 China launched the world’s first quantum communications satellite. The $100m Quantum Experiments at Space Scale (QUESS) mission was launched on Aug 16, 2016, from the Jiuquan Satellite Launch Center in northern China at 01:40 local time.

For the next two years, the craft – nicknamed "Micius" after the ancient Chinese philosopher – will demonstrate the feasibility of quantum
communication between Earth and space, and test quantum entanglement over unprecedented distances.

In the June 16, 2017, issue of "Science", Yin et al. report setting a new quantum entanglement distance record of 1,203 km, demonstrating the survival of a two-photon pair and a violation of a Bell inequality, reaching a CHSH valuation of 2.37 ± 0.09, under strict Einstein locality conditions, from the Micius satellite to bases in Lijian, Yunnan and Delingha, Quinhai, increasing the efficiency of transmission over prior fiberoptic experiments by an order of magnitude.

The electron shells of multi-electron atoms always consist of entangled electrons. The correct ionization energy can be calculated only by consideration of electron entanglement.

It has been suggested that in the process of photosynthesis, entanglement is involved in the transfer of energy between light-harvesting complexes and photosynthetic reaction centers where the kinetic energy is harvested in the form of chemical energy. Without such a process, the efficient conversion of optical energy into chemical energy cannot be explained. Using femtosecond spectroscopy, the coherence of entanglement in the Fenna-Matthews-Olson complex was measured over hundreds of femtoseconds (a relatively long time in this regard) providing support to this theory.

In October 2018, physicists reported producing quantum entanglement using living organisms, particularly between living bacteria and quantized light.




</doc>
<doc id="25343" url="https://en.wikipedia.org/wiki?curid=25343" title="Quasi-War">
Quasi-War

The Quasi-War () was an undeclared war fought almost entirely at sea between the United States and France from 1798 to 1800, which broke out during the beginning of John Adams's presidency. After the French Monarchy was abolished in September 1792, the United States refused to continue repaying its large debt to France, which had supported the U.S. during its own War for Independence. The U.S. claimed that the debt had been owed to a previous regime. France was also outraged over the Jay Treaty and that the United States was actively trading with Britain, with whom France was at war. In response, France authorized privateers to conduct attacks on American shipping, seizing numerous merchant ships and ultimately leading the U.S. to retaliate.

The war was called "quasi" because it was undeclared. It involved two years of hostilities at sea, in which both navies and privateers attacked the other's shipping in the West Indies. Many of the battles involved famous naval officers such as Stephen Decatur, Silas Talbot and William Bainbridge. The unexpected fighting ability of the newly re-established U.S. Navy, which concentrated on attacking the French West Indian privateers, together with the growing weaknesses and final overthrow of the ruling French Directory, led the French foreign minister, Talleyrand, to reopen negotiations with the U.S. At the same time, Adams feuded with Alexander Hamilton over control of the Adams administration. Adams took sudden and unexpected action, rejecting the anti-French hawks in his own party and offering peace to France. In 1800 he sent William Vans Murray to France to negotiate peace; the Federalists cried betrayal. Hostilities ended with the signing of the Convention of 1800.

When the United States won its independence it no longer had Britain's protection and therefore had the task of protecting its own ships and interests at sea. There were few American ships capable of defending the American coastline while trying to protect its merchant ships at sea. The Kingdom of France was a crucial ally of the United States in the American Revolutionary War. In March 1778, France signed a treaty of alliance with the rebelling colonists against Great Britain and had loaned the new Republic large sums of money. However, Louis XVI of France was deposed in September 1792. The monarchy was abolished. 

In 1794 the U.S. government reached an agreement with Great Britain in the Jay Treaty, which was ratified the following year. It resolved several points of contention between the United States and Britain that had lingered since the end of the American Revolution. The treaty encouraged bilateral trade, and enabled expanded trade between the United States and Britain, stimulating the American economy. From 1794 to 1801, the value of American exports nearly tripled, from US$33 million to US$94 million. But the Jeffersonian Democratic-Republicans, who were pro-France, always denigrated the Jay Treaty. 

The United States declared neutrality in the conflict between Great Britain and revolutionary France, and U.S. legislation was being passed for a trade deal with Great Britain. When the U.S. refused to continue repaying its debt, saying that the debt was owed to the previous government, not to the French First Republic, French outrage led to a series of responses. First, France authorized privateers to seize U.S. ships trading with Great Britain, and taking them back to port as prizes to be sold. Next, the French government refused to receive Charles Cotesworth Pinckney, the new U.S. Minister, when he arrived in Paris in December 1796, severing diplomatic relations. In President John Adams's annual message to Congress at the close of 1797, he reported on France's refusal to negotiate a settlement and spoke of the need "to place our country in a suitable posture of defense". Adams offered Washington a commission as lieutenant general on July 4, 1798, and as commander-in-chief of the armies raised for service in that conflict. In April 1798, President Adams informed Congress of the "XYZ Affair", in which French agents demanded a large bribe before engaging in substantive negotiations with United States diplomats.

Meanwhile, French privateers inflicted substantial losses on U.S. shipping. On 21 February 1797, Secretary of State Timothy Pickering told Congress that during the previous eleven months, France had seized 316 U.S. merchant ships. French marauders cruised the length of the Atlantic seaboard virtually unopposed. The United States government had nothing to combat them, as it had abolished the navy at the end of the Revolutionary War, and its last warship was sold in 1785. The United States had only a flotilla of small Revenue-Marine cutters and a few neglected coastal forts.

Increased depredations by French privateers led to the government in 1798 establishing the Department of Navy and the U.S. Marine Corps to defend the expanding U.S. merchant fleet. Benjamin Stoddert was appointed as Secretary of Navy. Congress authorized the president to acquire, arm, and man not more than twelve ships of up to twenty-two guns each. Several merchantmen were immediately purchased and refitted as ships of war.

Congress rescinded the treaties with France on 7 July 1798, and two days later Congress passed authorization for the U.S. to attack French warships in U.S. waters.
On 16 July Congress appropriated funds "to build and equip the three remaining frigates begun under the Act of 1794": 
, launched at Portsmouth, New Hampshire, on 15 August 1799; , launched at Gosport Shipyard, Virginia, on 2 December 1799; and , launched at New York, New York, on 10 April 1800. To make the most effective use of his limited resources, Secretary Stoddert established a policy that U.S. forces would be concentrated on attacks against French forces in the Caribbean, where France still had colonies, though at times he had to grant merchant ships' requests for escorts.

The U.S. Navy now operated with a battle fleet of about 25 vessels, which patrolled the southern coast of the United States and throughout the Caribbean hunting down French privateers. Captain Thomas Truxtun's focus on crew training paid dividends when the frigate captured the French Navy's frigate "L'Insurgente" and severely damaged the frigate "La Vengeance". French privateers generally resisted, as did , which was captured on 7 July 1798, by outside Egg Harbor, New Jersey. 

By 1 July 1799, under the command of Stephen Decatur, had been refitted and repaired and embarked on its mission to patrol the south Atlantic coast and West Indies in search of French ships which were preying on American merchant vessels. 

In April, 1800 Silas Talbot investigated an increase in merchant ship traffic near Puerto Plata, Santo Domingo, and discovered that the French privateer "Sandwich" had taken refuge there. On 8 May the squadron captured the sloop "Sally", and Talbot devised a plan to capture "Sandwich" by using the familiarity of "Sally" to allow the Americans access to the harbor. First Lieutenant Isaac Hull led 90 sailors and Marines into Puerto Plata without challenge on 11 May, capturing "Sandwich" and spiking the guns of the nearby Spanish fort. 

The U.S. Navy lost only one ship to the French, . She was the captured privateer "La Croyable", recently purchased by the U.S. Navy. "Retaliation" departed Norfolk on 28 October 1798, with and , and cruised in the West Indies protecting U.S. commerce. On 20 November 1798, the French frigates "L'Insurgente" and "Volontaire" overtook "Retaliation" while her consorts were away; commanding officer Lieutenant William Bainbridge surrendered the out-gunned schooner. 

Bainbridge was allowed to remain on board "Retaliation", and after ten days of detention was allowed to go ashore to Guadaloupe and negotiate terms of prisoner exchange with French General Desferneaux. The Governor promised to free officers and crew if Bainbridge, acting as a U.S. representative, would agree to declare Guadaloupe as neutral during the remainder of the war, with the hopes of commercial trade with the United States. Bainbridge, however, protesting the inhumane treatment of U.S. prisoners, maintained that his authority extended no further than to arrange for their exchange. Negotiations ultimately failed and Bainbridge was threatened with imprisonment if he did not comply with the wishes of the governor. Bainbridge, with his commitment to duty as a naval officer, again refused to co-operate. The governor, after further deliberations, and with earnest designs of forming his own cartel for purposes of trade with the United States, finally agreed to the release of prisoners and prepared a dispatch for Bainbridge to present to President Adams, assuring him of the neutrality of Guadaloupe. He released "Retaliation" to the command of Bainbridge with the stipulation that if their arrangement was not honored, Bainbridge and all released prisoners would be put to death if captured again. Bainbridge sailed for the United States and presented the Guadaloupe Governor's offer. Adams presented the offer to Congress, which accepted it, resulting in the passage of the "Retaliation Act" allowing the United States to capture and punish any French citizens aboard any French vessels. Bainbridge was ultimately promoted to the rank of Master and Commander and assigned to "Norfolk" for immediate service.

"Montezuma" and "Norfolk" escaped after Bainbridge convinced the senior French commander that those U.S. warships were too powerful for his frigates, and he should abandon the chase. The French renamed "Retaliation" as "Magicienne", but on 28 June fired a broadside and forced her to haul down her colors, and took the former privateer back into U.S. control.

Revenue cutters in the service of the U.S. Revenue-Marine (the predecessor to the U.S. Coast Guard), also took part in the conflict. The cutter USRC "Pickering", commanded by Edward Preble, made two cruises to the West Indies and captured ten prizes. Preble turned command of "Pickering" over to Benjamin Hillar, who captured the much larger and more heavily armed French privateer "lEgypte Conquise" after a nine-hour battle. In September 1800, Hillar, "Pickering", and her entire crew were lost at sea in a storm. Preble next commanded the frigate , which he sailed around Cape Horn into the Pacific to protect U.S. merchantmen in the East Indies. He recaptured several U.S. ships that had been seized by French privateers.

U.S. naval losses may have been light, but the French had successfully seized many U.S. merchant ships by the war's end in 1800 – more than 2,000, according to one source.

Although they were fighting the same enemy, the Royal Navy and the United States Navy did not cooperate operationally or share operational plans. There were no mutual understandings about deployment between their forces. However, the British sold naval stores and munitions to the U.S. government, and the two navies shared a signal system so they could recognise the other's warships at sea and allowed their merchantmen to join each other's convoys for safety.

By late 1800, the United States Navy and the Royal Navy, combined with a more conciliatory diplomatic stance by the government of First Consul Napoleon Bonaparte, had reduced the activity of the French privateers and warships. The Convention of 1800, signed on 30 September, ended the Quasi-War. It affirmed the rights of Americans as neutrals upon the sea and abrogated the alliance with France of 1778. However, it failed to provide compensation for the $20,000,000 "French Spoliation Claims" of the United States. The agreement between the two nations implicitly ensured that the United States would remain neutral toward France in the wars of Napoleon and ended the "entangling" French alliance. This alliance had been viable only between 1778 and 1783.







</doc>
<doc id="25345" url="https://en.wikipedia.org/wiki?curid=25345" title="Quality management system">
Quality management system

A quality management system (QMS) is a collection of business processes focused on consistently meeting customer requirements and enhancing their satisfaction. It is aligned with an organization's purpose and strategic direction (ISO9001:2015). It is expressed as the organizational goals and aspirations, policies, processes, documented information and resources needed to implement and maintain it. Early quality management systems emphasized predictable outcomes of an industrial product production line, using simple statistics and random sampling. By the 20th century, labor inputs were typically the most costly inputs in most industrialized societies, so focus shifted to team cooperation and dynamics, especially the early signaling of problems via a continual improvement cycle. In the 21st century, QMS has tended to converge with sustainability and transparency initiatives, as both investor and customer satisfaction and perceived quality is increasingly tied to these factors. Of QMS regimes, the ISO 9000 family of standards is probably the most widely implemented worldwide – the ISO 19011 audit regime applies to both, and deals with quality and sustainability and their integration.

Other QMS, e.g. Natural Step, focus on sustainability issues and assume that other quality problems will be reduced as result of the systematic thinking, transparency, documentation and diagnostic discipline.

The term "Quality Management System" and the initialism "QMS" were invented in 1991 by Ken Croucher, a British management consultant working on designing and implementing a generic model of a QMS within the IT industry.


The concept of a quality as we think of it now first emerged from the Industrial Revolution. Previously goods had been made from start to finish by the same person or team of people, with handcrafting and tweaking the product to meet 'quality criteria'. Mass production brought huge teams of people together to work on specific stages of production where one person would not necessarily complete a product from start to finish. In the late 19th century pioneers such as Frederick Winslow Taylor and Henry Ford recognized the limitations of the methods being used in mass production at the time and the subsequent varying quality of output. Birland established Quality Departments to oversee the quality of production and rectifying of errors, and Ford emphasized standardization of design and component standards to ensure a standard product was produced. Management of quality was the responsibility of the Quality department and was implemented by Inspection of product output to 'catch' defects.

Application of statistical control came later as a result of World War production methods, which were advanced by the work done of W. Edwards Deming, a statistician, after whom the Deming Prize for quality is named. Joseph M. Juran focused more on managing for quality. The first edition of Juran's Quality Control Handbook was published in 1951. He also developed the "Juran's trilogy", an approach to cross-functional management that is composed of three managerial processes: quality planning, quality control, and quality improvement. These functions all play a vital role when evaluating quality.

Quality, as a profession and the managerial process associated with the quality function, was introduced during the second half of the 20th century and has evolved since then. Over this period, few other disciplines have seen as many changes as the quality profession.

The quality profession grew from simple control to engineering, to systems engineering. Quality control activities were predominant in the 1940s, 1950s, and 1960s. The 1970s were an era of quality engineering and the 1990s saw quality systems as an emerging field. Like medicine, accounting, and engineering, quality has achieved status as a recognized profession

As Lee and Dale (1998) state, there are many organizations that are striving to assess the methods and ways in which their overall productivity, the quality of their products and services and the required operations to achieve them are done.

The two primary, state of the art, guidelines for medical device manufacturer QMS and related services today are the ISO 13485 standards and the US FDA 21 CFR 820 regulations. The two have a great deal of similarity, and many manufacturers adopt QMS that is compliant with both guidelines.

ISO 13485 are harmonized with the European Union medical devices directive (93/42/EEC) as well as the IVD and AIMD directives. The ISO standard is also incorporated in regulations for other jurisdictions such as Japan (JPAL) and Canada (CMDCAS).

Quality System requirements for medical devices have been internationally recognized as a way to assure product safety and efficacy and customer satisfaction since at least 1983 and were instituted as requirements in a final rule published on October 7, 1996. The U.S. Food and Drug Administration (FDA) had documented design defects in medical devices that contributed to recalls from 1983 to 1989 that would have been prevented if Quality Systems had been in place. The rule is promulgated at 21 CFR 820.

According to current Good Manufacturing Practice (GMP), medical device manufacturers have the responsibility to use good judgment when developing their quality system and apply those sections of the FDA Quality System (QS) Regulation that are applicable to their specific products and operations, in Part 820 of the QS regulation. As with GMP, operating within this flexibility, it is the responsibility of each manufacturer to establish requirements for each type or family of devices that will result in devices that are safe and effective, and to establish methods and procedures to design, produce, and distribute devices that meet the quality system requirements.

The FDA has identified in the QS regulation the 7 essential subsystems of a quality system. These subsystems include:

all overseen by management and quality audits.

Because the QS regulation covers a broad spectrum of devices and production processes, it allows some leeway in the details of quality system elements. It is left to manufacturers to determine the necessity for, or extent of, some quality elements and to develop and implement procedures tailored to their particular processes and devices. For example, if it is impossible to mix up labels at a manufacturer because there is only one label to each product, then there is no necessity for the manufacturer to comply with all of the GMP requirements under device labeling.

Drug manufactures are regulated under a different section of the Code of Federal Regulations:

The International Organization for Standardization's ISO 9001:2015 series describes standards for a QMS addressing the principles and processes surrounding the design, development, and delivery of a general product or service. Organizations can participate in a continuing certification process to ISO 9001:2008 to demonstrate their compliance with the standard, which includes a requirement for continual (i.e. planned) improvement of the QMS, as well as more foundational QMS components such as failure mode and effects analysis (FMEA).

(ISO 9000:2005 provides information on the fundamentals and vocabulary used in quality management systems. ISO 9004:2009 provides guidance on quality management approach for the sustained success of an organization. Neither of these standards can be used for certification purposes as they provide guidance, not requirements).

The Baldrige Performance Excellence Program educates organizations in improving their performance and administers the Malcolm Baldrige National Quality Award. The Baldrige Award recognizes U.S. organizations for performance excellence based on the Baldrige Criteria for Performance Excellence. The Criteria address critical aspects of management that contribute to performance excellence: leadership; strategy; customers; measurement, analysis, and knowledge management; workforce; operations; and results.

The European Foundation for Quality Management's EFQM Excellence Model supports an award scheme similar to the Baldrige Award for European companies.

In Canada, the National Quality Institute presents the 'Canada Awards for Excellence' on an annual basis to organizations that have displayed outstanding performance in the areas of Quality and Workplace Wellness, and have met the Institute's criteria with documented overall achievements and results.

The European Quality in Social Service (EQUASS) is a sector-specific quality system designed for the social services sector and addresses quality principles that are specific to service delivery to vulnerable groups, such as empowerment, rights, and person-centredness. 

The Alliance for Performance Excellence is a network of state and local organizations that use the Baldrige Criteria for Performance Excellence at the grassroots level to improve the performance of local organizations and economies. browsers can find Alliance members in their state and get the latest news and events from the Baldrige community.

A QMS process is an element of an organizational QMS. The ISO 9001:2000 standard requires organizations seeking compliance or certification to define the processes which form the QMS and the sequence and interaction of these processes. Butterworth-Heinemann and other publishers have offered several books which provide step-by-step guides to those seeking the quality certifications of their products 

Examples of such processes include:


ISO9001 requires that the performance of these processes be measured, analyzed and continually improved, and the results of this form an input into the management review process.




</doc>
<doc id="25346" url="https://en.wikipedia.org/wiki?curid=25346" title="Québécois (word)">
Québécois (word)

Québécois (pronounced ); feminine: Québécoise (pronounced ), ' (fem.: '), or (fem.: ) is a word used primarily to refer to a native or inhabitant of the Canadian province of Quebec that speaks French as a mother tongue; sometimes, it is used more generally to refer to any native or inhabitant of Quebec. It can refer to French spoken in Quebec. It may also be used, with an upper- or lower-case initial, as an adjective relating to Quebec, or to the French culture of Quebec. A resident or native of Quebec is often referred to in English as a Quebecer or Quebecker. In French, Québécois or Québécoise usually refers to any native or resident of Quebec. Its use became more prominent in the 1960s as French Canadians from Quebec increasingly self-identified as Québécois.

The name "Quebec" comes from a Mi'kmaq word "k'webeq" meaning "where the waters get narrow" and originally referred to the area around Quebec City, where the Saint Lawrence River narrows to a cliff-lined gap. French explorer Samuel de Champlain chose this name in 1608 for the colonial outpost he would use as the administrative seat for the French colony of Canada and New France. The Province of Quebec was first founded as a British colony in the Royal Proclamation of 1763 after the Treaty of Paris formally transferred the French colony of New France to Britain after the Seven Years' War. Quebec City remained the capital. In 1774, Guy Carleton obtained from the British Government the Quebec Act, which gave Canadiens most of the territory they held before 1763; the right of religion; and their right of language and culture. The British Government did this to in order to keep their loyalty, in the face of a growing menace of independence from the 13 original British colonies.

The term became more common in English as "Québécois" largely replacing "French Canadian" as an expression of cultural and national identity among French Canadians living in Quebec during the Quiet Revolution of the 1960s. The predominant French Canadian nationalism and identity of previous generations was based on the protection of the French language, the Roman Catholic Church, and Church-run institutions across Canada and in parts of the United States. In contrast, the modern Québécois identity is secular and based on a social democratic ideal of an active Quebec government promoting the French language and French-speaking culture in the arts, education, and business within the Province of Quebec. Politically, this resulted in a push towards more autonomy for Quebec and an internal debate on Quebec independence and identity that continues to this day. The emphasis on the French language and Quebec autonomy means that French-speakers across Canada now self-identify more specifically with provincial or regional identity-tags, such as "acadienne", or "franco-canadienne", "franco-manitobaine", "franco-ontarienne" or "fransaskoise". Terms such as Franco-Ontarian, acadian and Franco-Manitoban are still predominant. Francophones and anglophones use many terms when discussing issues of francophone linguistic and cultural identity in English.

The political shift towards a new Quebec nationalism in the 1960s led to Québécois increasingly referring to provincial institutions as being national. This was reflected in the change of the provincial "Legislative Assembly" to "National Assembly" in 1968. Nationalism reached an apex the 1970s and 1990s, with contentious constitutional debates resulting in close to half of all of French-speaking Québécois seeking recognition of nation status through tight referendums on Quebec sovereignty in 1980 and 1995. Having lost both referendums, the sovereigntist Parti Québécois government renewed the push for recognition as a nation through symbolic motions that gained the support of all parties in the National Assembly. They affirmed the right to determine the independent status of Quebec. They also renamed the area around Quebec City the "Capitale-Nationale" (national capital) region and renamed provincial parks "Parcs Nationaux" (national parks). In opposition in October 2003, the Parti Québécois tabled a motion that was unanimously adopted in the National Assembly affirming that the Quebec people formed a nation. Bloc Québécois leader Gilles Duceppe scheduled a similar motion in the House of Commons for November 23, 2006, that would have recognized "Quebecers as a nation". Conservative Prime Minister Stephen Harper tabled the "Québécois nation motion" the day before the Bloc Québécois resolution came to a vote. The English version changed the word "Quebecer" to "Québécois" and added "within a united Canada" at the end of the Bloc motion.

The "Québécois nation" was recognized by the House of Commons of Canada on November 27, 2006. The Prime Minister specified that the motion used the ""cultural"" and ""sociological"" as opposed to the ""legal"" sense of the word ""nation"". According to Harper, the motion was of a symbolic political nature, representing no constitutional change, no recognition of Quebec sovereignty, and no legal change in its political relations within the federation. The Prime Minister has further elaborated, stating that the motion's definition of Québécois relies on personal decisions to self-identify as Québécois, and therefore is a personal choice.

Despite near-universal support in the House of Commons, several important dissenters criticized the motion. Intergovernmental Affairs minister Michael Chong resigned from his position and abstained from voting, arguing that this motion was too ambiguous and had the potential of recognizing a destructive ethnic nationalism in Canada. Liberals were the most divided on the issue and represented 15 of the 16 votes against the motion. Liberal MP Ken Dryden summarized the view of many of these dissenters, maintaining that it was a game of semantics that cheapened issues of national identity. A survey by Leger Marketing in November 2006 showed that Canadians were deeply divided on this issue. When asked if Québécois are a nation, only 53 per cent of Canadians agreed, 47 per cent disagreed, with 33 per cent strongly disagreeing; 78 per cent of French-speaking Canadians agreed that Québécois are a nation, compared with 38 per cent of English-speaking Canadians. As well, 78 per cent of 1,000 Québécois polled thought that Québécois should be recognized as a nation.

The Québécois self-identify as an ethnic group in both the English and French versions of the Canadian census and in demographic studies of ethnicity in Canada.

In the 2016 census, 74,575 chose Québécois as one of multiple responses with 119,985 choosing it as a single response (194,555 as a combined response).

In the 2001 Census of Canada, 98,670 Canadians, or just over 1% of the population of Quebec identified "Québécois" as their ethnicity, ranking "Québécois" as the 37th most common response. These results were based on a question on residents in each household in Canada: ""To which ethnic or cultural group(s) did this person's ancestors belong?"", along with a list of sample choices ("Québécois" did not appear among the various sample choices). The ethnicity""Canadien"" or Canadian, did appear as an example on the questionnaire, and was selected by 4.9 million people or 68.2% of the Quebec population.

In the more detailed "Ethnic Diversity Survey",
Québécois was the most common ethnic identity in Quebec, reported by 37% of
Quebec's population aged 15 years and older, either as their only identity or alongside
other identities. The survey, based on interviews, asked the following questions: ""1) I would now like to ask you about your ethnic ancestry, heritage or background. What were the ethnic or cultural origins of your ancestors? 2) In addition to "Canadian", what were the other ethnic or cultural origins of your ancestors on first coming to North America?"" This survey did not list possible choices of ancestry and permitted multiple answers.
In census ethnic surveys, French-speaking Canadians identify their ethnicity most often as French, "Canadien", "Québécois", or French Canadian, with the latter three referred to by Jantzen (2005) as "French New World" ancestries because they originate in Canada. Jantzen (2005) distinguishes the English "Canadian", meaning "someone whose family has been in Canada for multiple generations", and the French "Canadien", used to refer to descendants of the original settlers of New France in the 17th and 18th centuries.

Those reporting "French New World" ancestries overwhelmingly had ancestors that went back at least 4 generations in Canada: specifically, 90% of "Québécois" traced their ancestry back this far. Fourth generation Canadiens and Québécois showed considerable attachment to their ethno-cultural group, with 70% and 61% respectively reporting a strong sense of belonging.

The generational profile and strength of identity of French New World ancestries contrast with those of British or Canadian ancestries, which represent the largest ethnic identities in Canada. Although deeply rooted Canadians express a deep attachment to their ethnic identity, most English-speaking Canadians of British ancestry generally cannot trace their ancestry as far back in Canada as French-speakers. As a result, their identification with their ethnicity is weaker tending to have a more broad based cultural identification: for example, only 50% of third generation "Canadians" strongly identify as such, bringing down the overall average. The survey report notes that 80% of Canadians whose families had been in Canada for three or more generations reported "Canadian and provincial or regional ethnic identities". These identities include "Québécois" (37% of Quebec population), "Acadian" (6% of Atlantic provinces) and "Newfoundlander" (38% of Newfoundland and Labrador).

English expressions employing the term may imply specific reference to francophones; such as "Québécois music", "a Québécois rocker" or "Québécois literature".

The dictionary "Le Petit Robert", published in France, states that the adjective "québécois", in addition to its territorial meaning, may refer specifically to francophone or French Canadian culture in Quebec. The dictionary gives as examples "cinéma québécois" and "littérature québécoise".

However, an ethnic or linguistic sense is absent from "Le Petit Larousse, also published in France, as well as from French dictionaries published in Canada such as "Le Dictionnaire québécois d'aujourd'hui" and "Le Dictionnaire du français Plus", which indicate instead "Québécois francophone" "francophone Quebecer" in the linguistic sense.

The online dictionary "Grand dictionnaire terminologique" of the Office québécois de la langue française mentions only a territorial meaning for "Québécois".

Newspaper editor Lysiane Gagnon has referred to an ethnic sense of the word "Québécois" in both English and French.

French expressions employing "Québécois" often appear in both French and English.





</doc>
<doc id="25348" url="https://en.wikipedia.org/wiki?curid=25348" title="Quantico, Virginia">
Quantico, Virginia

Quantico (formerly Potomac) is a town in Prince William County, Virginia, United States. The population was 480 at the 2010 census. Quantico is bordered by the Potomac River to the east and the Quantico Creek to the north. The word Quantico is a derivation of the name of a Doeg village recorded by English colonists as "Pamacocack".

Quantico is surrounded on its remaining two sides by one of the largest U.S. Marine Corps bases, Marine Corps Base Quantico. The base is the site of the Marine Corps Combat Development Command and HMX-1 (the presidential helicopter squadron), Officer Candidates School, and The Basic School. The United States Drug Enforcement Administration's training academy, the FBI Academy, the FBI Laboratory, the Naval Criminal Investigative Service, the United States Army Criminal Investigation Command, and the Air Force Office of Special Investigations headquarters are on the base. A replica of the United States Marine Corps War Memorial stands at one of the entrances to the base.

According to the United States Census Bureau, the town has a total area of , of which, of it is land and none of the area is covered with water.

Quantico has a humid subtropical climate (Köppen climate classification "Cfa").

As of the census of 2000, there were 561 people, 295 households, and 107 families living in the town. The population density was . The racial makeup was 61.32% White, 20.32% African American, 10.16% Asian, 0.36% Native American, 2.32% from other races, and 5.53% from two or more races. Hispanic or Latino of any race were 5.53% of the population. The median income for a household in the town was $26,250. About 22.4% of families and 21.4% of the population were below the poverty line, including 39.4% of those under the age of 18.

There are no significant highways passing through Quantico. All road vehicles must pass through MCB Quantico in order to reach the town. Therefore, all vehicle drivers must present a valid driver’s license to the military security officer stationed at the gate, and may be required to state their destination and reason for visiting. More thorough searches and checks may also be undertaken, according to the discretion and authority of base security.

Amtrak and Virginia Railway Express trains stop at the Quantico station.





</doc>
<doc id="25349" url="https://en.wikipedia.org/wiki?curid=25349" title="QSIG">
QSIG

QSIG is an ISDN based signaling protocol for signaling between private branch exchanges (PBXs) in a private integrated services network (PISN). It makes use of the connection-level Q.931 protocol and the application-level ROSE protocol. ISDN "proper" functions as the physical link layer.

QSIG was originally developed by Ecma International, adopted by ETSI and is defined by a set of ISO standard documents, so is not owned by any company. This allows interoperability between communications platforms provided by disparate vendors. 

QSIG has two layers, called BC (basic call) and GF (generic function). QSIG BC describes how to set up calls between PBXs. QSIG GF provides supplementary services for large-scale corporate, educational, and government networks, such as line identification, call intrusion and call forwarding. Thus for a large or very distributed company that requires multiple PBXs, users can receive the same services across the network and be unaware of the switch that their telephone is connected to. This greatly eases the problems of management of large networks.

QSIG will likely never rival each vendor's private network protocols, but it does provide an option for a higher level of integration than that of the traditional choices.

Note: This list is not complete. See the "source" after the list for more information.

Source : ECMA - list of standards (search the list for PISN to find all QSIG related standards at ECMA)

QSIG basically uses ROSE to invoke specific supplementary service at the remote PINX. These ROSE operations are coded in a Q.931 FACILITY info element. Here a list of QSIG opcodes:

Source : European Telecommunications Standards Institute (ETSI)

Source : International Telecommunications Union (ITU)



</doc>
<doc id="25350" url="https://en.wikipedia.org/wiki?curid=25350" title="Quasicrystal">
Quasicrystal

A quasiperiodic crystal, or quasicrystal, is a structure that is ordered but not periodic. A quasicrystalline pattern can continuously fill all available space, but it lacks translational symmetry. While crystals, according to the classical crystallographic restriction theorem, can possess only two-, three-, four-, and six-fold rotational symmetries, the Bragg diffraction pattern of quasicrystals shows sharp peaks with other symmetry orders — for instance, five-fold.

Aperiodic tilings were discovered by mathematicians in the early 1960s, and, some twenty years later, they were found to apply to the study of natural quasicrystals. The discovery of these aperiodic forms in nature has produced a paradigm shift in the fields of crystallography. Quasicrystals had been investigated and observed earlier, but, until the 1980s, they were disregarded in favor of the prevailing views about the atomic structure of matter. In 2009, after a dedicated search, a mineralogical finding, icosahedrite, offered evidence for the existence of natural quasicrystals.

Roughly, an ordering is non-periodic if it lacks translational symmetry, which means that a shifted copy will never match exactly with its original. The more precise mathematical definition is that there is never translational symmetry in more than "n" – 1 linearly independent directions, where "n" is the dimension of the space filled, e.g., the three-dimensional tiling displayed in a quasicrystal may have translational symmetry in two directions. Symmetrical diffraction patterns result from the existence of an indefinitely large number of elements with a regular spacing, a property loosely described as long-range order. Experimentally, the aperiodicity is revealed in the unusual symmetry of the diffraction pattern, that is, symmetry of orders other than two, three, four, or six. 
In 1982 materials scientist Dan Shechtman observed that certain aluminium-manganese alloys produced the unusual diffractograms which today are seen as revelatory of quasicrystal structures. Due to fear of the scientific community's reaction, it took him two years to publish the results for which he was awarded the Nobel Prize in Chemistry in 2011.
On 25 October 2018, Luca Bindi and Paul Steinhardt were awarded the Aspen Institute 2018 Prize for collaboration and scientific research between Italy and the United States.

In 1961, Hao Wang asked whether determining if a set of tiles admits a tiling of the plane is an algorithmically unsolvable problem or not. He conjectured that it is solvable, relying on the hypothesis that every set of tiles that can tile the plane can do it "periodically" (hence, it would suffice to try to tile bigger and bigger patterns until obtaining one that tiles periodically). Nevertheless, two years later, his student Robert Berger constructed a set of some 20,000 square tiles (now called "Wang tiles") that can tile the plane but not in a periodic fashion. As further aperiodic sets of tiles were discovered, sets with fewer and fewer shapes were found. In 1976 Roger Penrose discovered a set of just two tiles, now referred to as Penrose tiles, that produced only non-periodic tilings of the plane. These tilings displayed instances of fivefold symmetry. One year later Alan Mackay showed experimentally that the diffraction pattern from the Penrose tiling had a two-dimensional Fourier transform consisting of sharp 'delta' peaks arranged in a fivefold symmetric pattern. Around the same time, Robert Ammann created a set of aperiodic tiles that produced eightfold symmetry.

Mathematically, quasicrystals have been shown to be derivable from a general method that treats them as projections of a higher-dimensional lattice. Just as circles, ellipses, and hyperbolic curves in the plane can be obtained as sections from a three-dimensional double cone, so too various (aperiodic or periodic) arrangements in two and three dimensions can be obtained from postulated hyperlattices with four or more dimensions. Icosahedral quasicrystals in three dimensions were projected from a six-dimensional hypercubic lattice by Peter Kramer and Roberto Neri in 1984. The tiling is formed by two tiles with rhombohedral shape.

Shechtman first observed ten-fold electron diffraction patterns in 1982, as described in his notebook. The observation was made during a routine investigation, by electron microscopy, of a rapidly cooled alloy of aluminium and manganese prepared at the US National Bureau of Standards (later NIST).

In the summer of the same year Shechtman visited Ilan Blech and related his observation to him. Blech responded that such diffractions had been seen before. Around that time, Shechtman also related his finding to John Cahn of NIST who did not offer any explanation and challenged him to solve the observation. Shechtman quoted Cahn as saying: "Danny, this material is telling us something and I challenge you to find out what it is".

The observation of the ten-fold diffraction pattern lay unexplained for two years until the spring of 1984, when Blech asked Shechtman to show him his results again. A quick study of Shechtman's results showed that the common explanation for a ten-fold symmetrical diffraction pattern, the existence of twins, was ruled out by his experiments. Since periodicity and twins were ruled out, Blech, unaware of the two-dimensional tiling work, was looking for another possibility: a completely new structure containing cells connected to each other by defined angles and distances but without translational periodicity. Blech decided to use a computer simulation to calculate the diffraction intensity from a cluster of such a material without long-range translational order but still not random. He termed this new structure multiple polyhedral.

The idea of a new structure was the necessary paradigm shift to break the impasse. The "Eureka moment" came when the computer simulation showed sharp ten-fold diffraction patterns, similar to the observed ones, emanating from the three-dimensional structure devoid of periodicity. The multiple polyhedral structure was termed later by many researchers as icosahedral glass but in effect it embraces "any arrangement of polyhedra connected with definite angles and distances" (this general definition includes tiling, for example).

Shechtman accepted Blech's discovery of a new type of material and it gave him the courage to publish his experimental observation. Shechtman and Blech jointly wrote a paper entitled "The Microstructure of Rapidly Solidified AlMn" and sent it for publication around June 1984 to the "Journal of Applied Physics" (JAP). The JAP editor promptly rejected the paper as being better fit for a metallurgical readership. As a result, the same paper was re-submitted for publication to the "Metallurgical Transactions A", where it was accepted. Although not noted in the body of the published text, the published paper was slightly revised prior to publication.

Meanwhile, on seeing the draft of the Shechtman–Blech paper in the summer of 1984, John Cahn suggested that Shechtman's experimental results merit a fast publication in a more appropriate scientific journal. Shechtman agreed and, in hindsight, called this fast publication "a winning move”. This paper, published in the "Physical Review Letters" (PRL), repeated Shechtman's observation and used the same illustrations as the original Shechtman–Blech paper in the "Metallurgical Transactions A". The PRL paper, the first to appear in print, caused considerable excitement in the scientific community.

Next year Ishimasa "et al." reported twelvefold symmetry in Ni-Cr particles. Soon, eightfold diffraction patterns were recorded in V-Ni-Si and Cr-Ni-Si alloys. Over the years, hundreds of quasicrystals with various compositions and different symmetries have been discovered. The first quasicrystalline materials were thermodynamically unstable—when heated, they formed regular crystals. However, in 1987, the first of many stable quasicrystals were discovered, making it possible to produce large samples for study and opening the door to potential applications. 
Almost concurrently Paul Steinhardt (Princeton University) hypothesized the possibility to find a quasicrystal in nature, developing a method of recognition, published on Physical Review Letters in 2001, inviting all the mineralogical collections of the world to identify any badly cataloged crystals. In 2007 Steinhardt received a reply by Luca Bindi (University of Florence) that stated to have found an almost perfect match crystal in Florence Mineralogical Collection with quasicrystal characteristics, originally coming from Khatyrka. So in 2008 the crystal samples were sent to Princeton University for other tests and on 2009 New Year's Eve Steinhardt obtained the smoking gun, the final evidence, communicating the great discovery to Luca Bindi. After other studies was stated that the found quasicrystal was extraterrestrial and 4,57 mld old. In 2011 Bindi, Steinhardt and a team of specialists did an expedition in the desolate lands around the Kathyrka river, in Chukotka Autonomous Okrug managing to find other natural quasicrystal samples. This natural quasicrystal exhibits high crystalline quality, equalling the best artificial examples. The natural quasicrystal phase, with a composition of AlCuFe, was named icosahedrite and it was approved by the International Mineralogical Association in 2010. Furthermore, analysis indicates it may be meteoritic in origin, possibly delivered from a carbonaceous chondrite asteroid.
A further study of Khatyrka meteorites revealed micron-sized grains of another natural quasicrystal, which has a ten-fold symmetry and a chemical formula of AlNiFe. This quasicrystal is stable in a narrow temperature range, from 1120 to 1200 K at ambient pressure, which suggests that natural quasicrystals are formed by rapid quenching of a meteorite heated during an impact-induced shock.
In 1972 de Wolf and van Aalst reported that the diffraction pattern produced by a crystal of sodium carbonate cannot be labeled with three indices but needed one more, which implied that the underlying structure had four dimensions in reciprocal space. Other puzzling cases have been reported, but until the concept of quasicrystal came to be established, they were explained away or denied. However, at the end of the 1980s the idea became acceptable, and in 1992 the International Union of Crystallography altered its definition of a crystal, broadening it as a result of Shechtman's findings, reducing it to the ability to produce a clear-cut diffraction pattern and acknowledging the possibility of the ordering to be either periodic or aperiodic. Now, the symmetries compatible with translations are defined as "crystallographic", leaving room for other "non-crystallographic" symmetries. Therefore, aperiodic or quasiperiodic structures can be divided into two main classes: those with crystallographic point-group symmetry, to which the incommensurately modulated structures and composite structures belong, and those with non-crystallographic point-group symmetry, to which quasicrystal structures belong.

Originally, the new form of matter was dubbed "Shechtmanite". The term "quasicrystal" was first used in print by Steinhardt and Levine shortly after Shechtman's paper was published.
The adjective "quasicrystalline" had already been in use, but now it came to be applied to any pattern with unusual symmetry. 'Quasiperiodical' structures were claimed to be observed in some decorative tilings devised by medieval Islamic architects. For example, Girih tiles in a medieval Islamic mosque in Isfahan, Iran, are arranged in a two-dimensional quasicrystalline pattern. These claims have, however, been under some debate.

Shechtman was awarded the Nobel Prize in Chemistry in 2011 for his work on quasicrystals. "His discovery of quasicrystals revealed a new principle for packing of atoms and molecules," stated the Nobel Committee and pointed that "this led to a paradigm shift within chemistry." In 2014, Post of Israel issued a stamp dedicated to quasicrystals and the 2011 Nobel Prize.

Earlier in 2009, it was found that thin-film quasicrystals can be formed by self-assembly of uniformly shaped, nano-sized molecular units at an air-liquid interface. It was later demonstrated that those units can be not only inorganic, but also organic.

In 2018, chemists from Brown University announced the successful creation of a self-constructing lattice structure based on a strangely shaped quantum dot. While single-component quasicrystal lattices have been previously predicted mathematically and in computer simulations, they had not been demonstrated prior to this.

There are several ways to mathematically define quasicrystalline patterns. One definition, the "cut and project" construction, is based on the work of Harald Bohr (mathematician brother of Niels Bohr). The concept of an almost periodic function (also called a quasiperiodic function) was studied by Bohr, including work of Bohl and Escanglon.
He introduced the notion of a superspace. Bohr showed that quasiperiodic functions arise as restrictions of high-dimensional periodic functions to an irrational slice (an intersection with one or more hyperplanes), and discussed their Fourier point spectrum. These functions are not exactly periodic, but they are arbitrarily close in some sense, as well as being a projection of an exactly periodic function.

In order that the quasicrystal itself be aperiodic, this slice must avoid any lattice plane of the higher-dimensional lattice. De Bruijn showed that Penrose tilings can be viewed as two-dimensional slices of five-dimensional hypercubic structures. Equivalently, the Fourier transform of such a quasicrystal is nonzero only at a dense set of points spanned by integer multiples of a finite set of basis vectors (the projections of the primitive reciprocal lattice vectors of the higher-dimensional lattice).
The intuitive considerations obtained from simple model aperiodic tilings are formally expressed in the concepts of Meyer and Delone sets. The mathematical counterpart of physical diffraction is the Fourier transform and the qualitative description of a diffraction picture as 'clear cut' or 'sharp' means that singularities are present in the Fourier spectrum. There are different methods to construct model quasicrystals. These are the same methods that produce aperiodic tilings with the additional constraint for the diffractive property. Thus, for a substitution tiling the eigenvalues of the substitution matrix should be Pisot numbers. The aperiodic structures obtained by the cut-and-project method are made diffractive by choosing a suitable orientation for the construction; this is a geometric approach that has also a great appeal for physicists.

Classical theory of crystals reduces crystals to point lattices where each point is the center of mass of one of the identical units of the crystal. The structure of crystals can be analyzed by defining an associated group. Quasicrystals, on the other hand, are composed of more than one type of unit, so, instead of lattices, quasilattices must be used. Instead of groups, groupoids, the mathematical generalization of groups in category theory, is the appropriate tool for studying quasicrystals.

Using mathematics for construction and analysis of quasicrystal structures is a difficult task for most experimentalists. Computer modeling, based on the existing theories of quasicrystals, however, greatly facilitated this task. Advanced programs have been developed allowing one to construct, visualize and analyze quasicrystal structures and their diffraction patterns.

Study of quasicrystals may shed light on the most basic notions related to the quantum critical point observed in heavy fermion metals. Experimental measurements on the gold-aluminium-ytterbium quasicrystal have revealed a quantum critical point defining the divergence of the magnetic susceptibility as temperature tends to zero. It is suggested that the electronic system of some quasicrystals is located at a quantum critical point without tuning, while quasicrystals exhibit the typical scaling behaviour of their thermodynamic properties and belong to the well-known family of heavy fermion metals.

Since the original discovery by Dan Shechtman, hundreds of quasicrystals have been reported and confirmed. Undoubtedly, the quasicrystals are no longer a unique form of solid; they exist
universally in many metallic alloys and some polymers. Quasicrystals are found most often in aluminium alloys (Al-Li-Cu, Al-Mn-Si, Al-Ni-Co, Al-Pd-Mn, Al-Cu-Fe, Al-Cu-V, etc.), but numerous other compositions are also known (Cd-Yb, Ti-Zr-Ni, Zn-Mg-Ho, Zn-Mg-Sc, In-Ag-Yb, Pd-U-Si, etc.).

Two types of quasicrystals are known. The first type, polygonal (dihedral) quasicrystals, have an axis of 8, 10, or 12-fold local symmetry (octagonal, decagonal, or dodecagonal quasicrystals, respectively). They are periodic along this axis and quasiperiodic in planes normal to it. The second type, icosahedral quasicrystals, are aperiodic in all directions.

Quasicrystals fall into three groups of different thermal stability:

Except for the Al–Li–Cu system, all the stable quasicrystals are almost free of defects and disorder, as evidenced by X-ray and electron diffraction revealing peak widths as sharp as those of perfect crystals such as Si. Diffraction patterns exhibit fivefold, threefold, and twofold symmetries, and reflections are arranged quasiperiodically in three dimensions.

The origin of the stabilization mechanism is different for the stable and metastable quasicrystals. Nevertheless, there is a common feature observed in most quasicrystal-forming liquid alloys or their undercooled liquids: a local icosahedral order. The icosahedral order is in equilibrium in the "liquid state" for the stable quasicrystals, whereas the icosahedral order prevails in the "undercooled liquid state" for the metastable quasicrystals.

A nanoscale icosahedral phase was formed in Zr-, Cu- and Hf-based bulk metallic glasses alloyed with noble metals.

Most quasicrystals have ceramic-like properties including high thermal and electrical resistance, hardness and brittleness, resistance to corrosion, and non-stick
properties. Many metallic quasicrystalline substances are impractical for most applications due to their thermal instability; the Al-Cu-Fe ternary system and the Al-Cu-Fe-Cr and Al-Co-Fe-Cr quaternary systems, thermally stable up to 700 °C, are notable exceptions.

Quasicrystalline substances have potential applications in several forms.

Metallic quasicrystalline coatings can be applied by plasma-coating or magnetron sputtering. A problem that must be resolved is the tendency for cracking due to the materials' extreme brittleness. The cracking could be suppressed by reducing sample dimensions or coating thickness. Recent studies show typically brittle quasicrystals can exhibit remarkable ductility of over 50% strains at room temperature and sub-micrometer scales (<500 nm).

An application was the use of low-friction Al-Cu-Fe-Cr quasicrystals as a coating for frying pans. Food did not stick to it as much as to stainless steel making the pan moderately non-stick and easy to clean; heat transfer and durability were better than PTFE non-stick cookware and the pan was free from perfluorooctanoic acid (PFOA); the surface was very hard, claimed to be ten times harder than stainless steel, and not harmed by metal utensils or cleaning in a dishwasher; and the pan could withstand temperatures of without harm. However, cooking with a lot of salt would etch the quasicrystalline coating used, and the pans were eventually withdrawn from production. Shechtman had one of these pans.

The Nobel citation said that quasicrystals, while brittle, could reinforce steel "like armor". When Shechtman was asked about potential applications of quasicrystals he said that a precipitation-hardened stainless steel is produced that is strengthened by small quasicrystalline particles. It does not corrode and is extremely strong, suitable for razor blades and surgery instruments. The small quasicrystalline particles impede the motion of dislocation in the material.

Quasicrystals were also being used to develop heat insulation, LEDs, diesel engines, and new materials that convert heat to electricity. Shechtman suggested new applications taking advantage of the low coefficient of friction and the hardness of some quasicrystalline materials, for example embedding particles in plastic to make strong, hard-wearing, low-friction plastic gears. The low heat conductivity of some quasicrystals makes them good for heat insulating coatings.

Other potential applications include selective solar absorbers for power conversion, broad-wavelength reflectors, and bone repair and prostheses applications where biocompatibility, low friction and corrosion resistance are required. Magnetron sputtering can be readily applied to other stable quasicrystalline alloys such as Al-Pd-Mn.

While saying that the discovery of icosahedrite, the first quasicrystal found in nature, was important, Shechtman saw no practical applications.




</doc>
<doc id="25381" url="https://en.wikipedia.org/wiki?curid=25381" title="Recreation">
Recreation

Recreation is an activity of leisure, leisure being discretionary time. The "need to do something for recreation" is an essential element of human biology and psychology. Recreational activities are often done for enjoyment, amusement, or pleasure and are considered to be "fun".

The term "recreation" appears to have been used in English first in the late 14th century, first in the sense of "refreshment or curing of a sick person", and derived turn from Latin ("re": "again", "creare": "to create, bring forth, beget").

Humans spend their time in activities of daily living, work, sleep, social duties, and leisure, the latter time being free from prior commitments to physiologic or social needs, a prerequisite of recreation. Leisure has increased with increased longevity and, for many, with decreased hours spent for physical and economic survival, yet others argue that time pressure has increased for modern people, as they are committed to too many tasks. Other factors that account for an increased role of recreation are affluence, population trends, and increased commercialization of recreational offerings. While one perception is that leisure is just "spare time", time not consumed by the necessities of living, another holds that leisure is a force that allows individuals to consider and reflect on the values and realities that are missed in the activities of daily life, thus being an essential element of personal development and civilization. This direction of thought has even been extended to the view that leisure is the purpose of work, and a reward in itself, and "leisure life" reflects the values and character of a nation. Leisure is considered a human right under the Universal Declaration of Human Rights.

Recreation is difficult to separate from the general concept of play, which is usually the term for children's recreational activity. Children may playfully imitate activities that reflect the realities of adult life. It has been proposed that play or recreational activities are outlets of or expression of excess energy, channeling it into socially acceptable activities that fulfill individual as well as societal needs, without need for compulsion, and providing satisfaction and pleasure for the participant. A traditional view holds that work is supported by recreation, recreation being useful to "recharge the battery" so that work performance is improved. Work, an activity generally performed out of economic necessity and useful for society and organized within the economic framework, however can also be pleasurable and may be self-imposed thus blurring the distinction to recreation. Many activities may be work for one person and recreation for another, or, at an individual level, over time recreational activity may become work, and vice versa. Thus, for a musician, playing an instrument may be at one time a profession, and at another a recreation. Similarly, it may be difficult to separate education from recreation as in the case of recreational mathematics.

Recreation is an essential part of human life and finds many different forms which are shaped naturally by individual interests but also by the surrounding social construction. Recreational activities can be communal or solitary, active or passive, outdoors or indoors, healthy or harmful, and useful for society or detrimental. A significant section of recreational activities are designated as hobbies which are activities done for pleasure on a regular basis. A list of typical activities could be almost endless including most human activities, a few examples being reading, playing or listening to music, watching movies or TV, gardening, fine dining, hunting, sports, studies, and travel. Some recreational activities - such as gambling, recreational drug use, or delinquent activities - may violate societal norms and laws.

Public space such as parks and beaches are essential venues for many recreational activities. Tourism has recognized that many visitors are specifically attracted by recreational offerings. In support of recreational activities government has taken an important role in their creation, maintenance, and organization, and whole industries have developed merchandise or services. Recreation-related business is an important factor in the economy; it has been estimated that the outdoor recreation sector alone contributes $730 billion annually to the U.S. economy and generates 6.5 million jobs.

Research has shown that practicing creative leisure activities is interrelated with the emotional creativity.

A recreation center is a place for recreational activities usually administered by a municipal government agency. Swimming, basketball, weightlifting, volleyball and kids' play areas are very common.

Many recreational activities are organized, typically by public institutions, voluntary group-work agencies, private groups supported by membership fees, and commercial enterprises. Examples of each of these are the National Park Service, the YMCA, the Kiwanis, and Walt Disney World.

Recreation has many health benefits, and, accordingly, Therapeutic Recreation has been developed to take advantage of this effect. The National Council for Therapeutic Recreation Certification (NCTRC) is the nationally recognized credentialing organization for the profession of Therapeutic Recreation. Professionals in the field of Therapeutic Recreation who are certified by the NCTRC are called "Certified Therapeutic Recreation Specialists". The job title "Recreation Therapist" is identified in the U.S. Dept of Labor's Occupation Outlook. Such therapy is applied in rehabilitation, psychiatric facilities for youth and adults, and in the care of the elderly, the disabled, or people with chronic diseases. Recreational physical activity is important to reduce obesity, and the risk of osteoporosis and of cancer, most significantly in men that of colon and prostate, and in women that of the breast; however, not all malignancies are reduced as outdoor recreation has been linked to a higher risk of melanoma. Extreme adventure recreation naturally carries its own hazards.

A recreation specialist would be expected to meet the recreational needs of a community or assigned interest group. Educational institutions offer courses that lead to a degree as a Bachelor of Arts in recreation management. People with such degrees often work in parks and recreation centers in towns, on community projects and activities. Networking with instructors, budgeting, and evaluation of continuing programs are common job duties.

In the United States, most states have a professional organization for continuing education and certification in recreation management. The National Recreation and Park Association administers a certification program called the CPRP (Certified Park and Recreation Professional) that is considered a national standard for professional recreation specialist practices.

Since the beginning of the 2000s, there are more and more online booking / ticketing platforms for recreational activities that emerged. Many of them leveraged the ever-growing prevalence of internet, mobile devices and e-payments to build comprehensive online booking solutions. The first successful batch includes tourist recreation activities platform like TripAdvisor that went public. More examples of recreational activities booking platform includes Klook and KKDay that came to the market after 2010s. For recreational activities within the home city of people, there are bigger breakthrough in China like DianPing, Reubird and FunNow. The emergence of these platforms infers the rising needs for recreation and entertainment from the growing urban citizens worldwide.



</doc>
<doc id="25382" url="https://en.wikipedia.org/wiki?curid=25382" title="Recession">
Recession

In economics, a recession is a business cycle contraction when there is a general decline in economic activity. Recessions generally occur when there is a widespread drop in spending (an adverse demand shock). This may be triggered by various events, such as a financial crisis, an external trade shock, an adverse supply shock or the bursting of an economic bubble. In the United States, it is defined as "a significant decline in economic activity spread across the market, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales". In the United Kingdom, it is defined as a negative economic growth for two consecutive quarters.

Governments usually respond to recessions by adopting expansionary macroeconomic policies, such as increasing money supply or increasing government spending and decreasing taxation

In a 1974 "The New York Times" article, Commissioner of the Bureau of Labor Statistics Julius Shiskin suggested several rules of thumb for defining a recession, one of which was two consecutive quarters of negative GDP growth. In time, the other rules of thumb were forgotten. Some economists prefer a definition of a 1.5-2 percentage points rise in unemployment within 12 months.

In the United States, the Business Cycle Dating Committee of the National Bureau of Economic Research (NBER) is generally seen as the authority for dating US recessions. The NBER, a private economic research organization, defines an economic recession as: "a significant decline in economic activity spread across the economy, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales". Almost universally, academics, economists, policy makers, and businesses refer to the determination by the NBER for the precise dating of a recession's onset and end.

In the United Kingdom, recessions are generally defined as two consecutive quarters of negative economic growth, as measured by the seasonal adjusted quarter-on-quarter figures for real GDP, with the same definition being used for all other member states of the European Union.

A recession has many attributes that can occur simultaneously and includes declines in component measures of economic activity (GDP) such as consumption, investment, government spending, and net export activity. These summary measures reflect underlying drivers such as employment levels and skills, household savings rates, corporate investment decisions, interest rates, demographics, and government policies.

Economist Richard C. Koo wrote that under ideal conditions, a country's economy should have the household sector as net savers and the corporate sector as net borrowers, with the government budget nearly balanced and net exports near zero. When these relationships become imbalanced, recession can develop within the country or create pressure for recession in another country. Policy responses are often designed to drive the economy back towards this ideal state of balance.

A severe (GDP down by 10%) or prolonged (three or four years) recession is referred to as an economic depression, although some argue that their causes and cures can be different. As an informal shorthand, economists sometimes refer to different recession shapes, such as V-shaped, U-shaped, L-shaped and W-shaped recessions.

The type and shape of recessions are distinctive. In the US, v-shaped, or short-and-sharp contractions followed by rapid and sustained recovery, occurred in 1954 and 1990–91; U-shaped (prolonged slump) in 1974–75, and W-shaped, or double-dip recessions in 1949 and 1980–82. Japan’s 1993–94 recession was U-shaped and its 8-out-of-9 quarters of contraction in 1997–99 can be described as L-shaped. Korea, Hong Kong and South-east Asia experienced U-shaped recessions in 1997–98, although Thailand’s eight consecutive quarters of decline should be termed L-shaped.

Recessions have psychological and confidence aspects. For example, if companies expect economic activity to slow, they may reduce employment levels and save money rather than invest. Such expectations can create a self-reinforcing downward cycle, bringing about or worsening a recession. Consumer confidence is one measure used to evaluate economic sentiment. The term animal spirits has been used to describe the psychological factors underlying economic activity. Economist Robert J. Shiller wrote that the term "...refers also to the sense of trust we have in each other, our sense of fairness in economic dealings, and our sense of the extent of corruption and bad faith. When animal spirits are on ebb, consumers do not want to spend and businesses do not want to make capital expenditures or hire people."

High levels of indebtedness or the bursting of a real estate or financial asset price bubble can cause what is called a "balance sheet recession". This is when large numbers of consumers or corporations pay down debt (i.e., save) rather than spend or invest, which slows the economy. The term balance sheet derives from an accounting identity that holds that assets must always equal the sum of liabilities plus equity. If asset prices fall below the value of the debt incurred to purchase them, then the equity must be negative, meaning the consumer or corporation is insolvent. Economist Paul Krugman wrote in 2014 that "the best working hypothesis seems to be that the financial crisis was only one manifestation of a broader problem of excessive debt—that it was a so-called "balance sheet recession". In Krugman's view, such crises require debt reduction strategies combined with higher government spending to offset declines from the private sector as it pays down its debt.

For example, economist Richard Koo wrote that Japan's "Great Recession" that began in 1990 was a "balance sheet recession". It was triggered by a collapse in land and stock prices, which caused Japanese firms to have negative equity, meaning their assets were worth less than their liabilities. Despite zero interest rates and expansion of the money supply to encourage borrowing, Japanese corporations in aggregate opted to pay down their debts from their own business earnings rather than borrow to invest as firms typically do. Corporate investment, a key demand component of GDP, fell enormously (22% of GDP) between 1990 and its peak decline in 2003. Japanese firms overall became net savers after 1998, as opposed to borrowers. Koo argues that it was massive fiscal stimulus (borrowing and spending by the government) that offset this decline and enabled Japan to maintain its level of GDP. In his view, this avoided a U.S. type Great Depression, in which U.S. GDP fell by 46%. He argued that monetary policy was ineffective because there was limited demand for funds while firms paid down their liabilities. In a balance sheet recession, GDP declines by the amount of debt repayment and un-borrowed individual savings, leaving government stimulus spending as the primary remedy.

Krugman discussed the balance sheet recession concept during 2010, agreeing with Koo's situation assessment and view that sustained deficit spending when faced with a balance sheet recession would be appropriate. However, Krugman argued that monetary policy could also affect savings behavior, as inflation or credible promises of future inflation (generating negative real interest rates) would encourage less savings. In other words, people would tend to spend more rather than save if they believe inflation is on the horizon. In more technical terms, Krugman argues that the private sector savings curve is elastic even during a balance sheet recession (responsive to changes in real interest rates) disagreeing with Koo's view that it is inelastic (non-responsive to changes in real interest rates).

A July 2012 survey of balance sheet recession research reported that consumer demand and employment are affected by household leverage levels. Both durable and non-durable goods consumption declined as households moved from low to high leverage with the decline in property values experienced during the subprime mortgage crisis. Further, reduced consumption due to higher household leverage can account for a significant decline in employment levels. Policies that help reduce mortgage debt or household leverage could therefore have stimulative effects.

A liquidity trap is a Keynesian theory that a situation can develop in which interest rates reach near zero (zero interest-rate policy) yet do not effectively stimulate the economy. In theory, near-zero interest rates should encourage firms and consumers to borrow and spend. However, if too many individuals or corporations focus on saving or paying down debt rather than spending, lower interest rates have less effect on investment and consumption behavior; the lower interest rates are like "pushing on a string". Economist Paul Krugman described the U.S. 2009 recession and Japan's lost decade as liquidity traps. One remedy to a liquidity trap is expanding the money supply via quantitative easing or other techniques in which money is effectively printed to purchase assets, thereby creating inflationary expectations that cause savers to begin spending again. Government stimulus spending and mercantilist policies to stimulate exports and reduce imports are other techniques to stimulate demand. He estimated in March 2010 that developed countries representing 70% of the world's GDP were caught in a liquidity trap.

Behavior that may be optimal for an individual (e.g., saving more during adverse economic conditions) can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. Too many consumers attempting to save (or pay down debt) simultaneously is called the paradox of thrift and can cause or deepen a recession. Economist Hyman Minsky also described a "paradox of deleveraging" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets.

During April 2009, U.S. Federal Reserve Vice Chair Janet Yellen discussed these paradoxes: "Once this massive credit crunch hit, it didn’t take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And, financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole."

The U.S. Conference Board’s Present Situation Index year-over-year change turns negative by more than 15 points before a recession.

The U.S. Conference Board Leading Economic Indicator year-over-year change turns negative before a recession.

When the CFNAI Diffusion Index drops below the value of -0.35, then there is an increased probability of the beginning a recession. Usually the signal happens in the three months of the recession. The CFNAI Diffusion Index signal tends to happen about one month before a related signal by the CFNAI-MA3 (3-month moving average) drops below the -0.7 level. The CFNAI-MA3 correctly identified the 7 recessions between March 1967–August 2019, while triggering only 2 false alarms.
Except for the above, there are no known completely reliable predictors, but the following are considered possible predictors.

Analysis by Prakash Loungani of the International Monetary Fund found that only two of the sixty recessions around the world during the 1990s had been predicted by a consensus of economists one year earlier, while there were zero consensus predictions one year earlier for the 49 recessions during 2009.

Most mainstream economists believe that recessions are caused by inadequate aggregate demand in the economy, and favor the use of expansionary macroeconomic policy during recessions. Strategies favored for moving an economy out of a recession vary depending on which economic school the policymakers follow. Monetarists would favor the use of expansionary monetary policy, while Keynesian economists may advocate increased government spending to spark economic growth. Supply-side economists may suggest tax cuts to promote business capital investment. When interest rates reach the boundary of an interest rate of zero percent (zero interest-rate policy) conventional monetary policy can no longer be used and government must use other measures to stimulate recovery. Keynesians argue that fiscal policy—tax cuts or increased government spending—works when monetary policy fails. Spending is more effective because of its larger multiplier but tax cuts take effect faster.

For example, Paul Krugman wrote in December 2010 that significant, sustained government spending was necessary because indebted households were paying down debts and unable to carry the U.S. economy as they had previously: "The root of our current troubles lies in the debt American families ran up during the Bush-era housing bubble...highly indebted Americans not only can’t spend the way they used to, they’re having to pay down the debts they ran up in the bubble years. This would be fine if someone else were taking up the slack. But what’s actually happening is that some people are spending much less while nobody is spending more — and this translates into a depressed economy and high unemployment. What the government should be doing in this situation is spending more while the private sector is spending less, supporting employment while those debts are paid down. And this government spending needs to be sustained..."

Some recessions have been anticipated by stock market declines. In "Stocks for the Long Run", Siegel mentions that since 1948, ten recessions were preceded by a stock market decline, by a lead time of 0 to 13 months (average 5.7 months), while ten stock market declines of greater than 10% in the Dow Jones Industrial Average were not followed by a recession.

The real-estate market also usually weakens before a recession. However real-estate declines can last much longer than recessions.

Since the business cycle is very hard to predict, Siegel argues that it is not possible to take advantage of economic cycles for timing investments. Even the National Bureau of Economic Research (NBER) takes a few months to determine if a peak or trough has occurred in the US.

During an economic decline, high yield stocks such as fast-moving consumer goods, pharmaceuticals, and tobacco tend to hold up better. However, when the economy starts to recover and the bottom of the market has passed, growth stocks tend to recover faster. There is significant disagreement about how health care and utilities tend to recover. Diversifying one's portfolio into international stocks may provide some safety; however, economies that are closely correlated with that of the U.S. may also be affected by a recession in the U.S.

There is a view termed the "halfway rule" according to which investors start discounting an economic recovery about halfway through a recession. In the 16 U.S. recessions since 1919, the average length has been 13 months, although the recent recessions have been shorter. Thus, if the 2008 recession had followed the average, the downturn in the stock market would have bottomed around November 2008. The actual US stock market bottom of the 2008 recession was in March 2009.

Generally an administration gets credit or blame for the state of economy during its time. This has caused disagreements about on how it actually started. In an economic cycle, a downturn can be considered a consequence of an expansion reaching an unsustainable state, and is corrected by a brief decline. Thus it is not easy to isolate the causes of specific phases of the cycle.

The 1981 recession is thought to have been caused by the tight-money policy adopted by Paul Volcker, chairman of the Federal Reserve Board, before Ronald Reagan took office. Reagan supported that policy. Economist Walter Heller, chairman of the Council of Economic Advisers in the 1960s, said that "I call it a Reagan-Volcker-Carter recession." The resulting taming of inflation did, however, set the stage for a robust growth period during Reagan's presidency. .

Economists usually teach that to some degree recession is unavoidable, and its causes are not well understood. Consequently, modern government administrations attempt to take steps, also not agreed upon, to soften a recession.

Unemployment is particularly high during a recession. Many economists working within the neoclassical paradigm argue that there is a natural rate of unemployment which, when subtracted from the actual rate of unemployment, can be used to calculate the negative GDP gap during a recession. In other words, unemployment never reaches 0 percent, and thus is not a negative indicator of the health of an economy unless above the "natural rate," in which case it corresponds directly to a loss in gross domestic product, or GDP.

The full impact of a recession on employment may not be felt for several quarters. Research in Britain shows that low-skilled, low-educated workers and the young are most vulnerable to unemployment in a downturn. After recessions in Britain in the 1980s and 1990s, it took five years for unemployment to fall back to its original levels. Many companies often expect employment discrimination claims to rise during a recession.

Productivity tends to fall in the early stages of a recession, then rises again as weaker firms close. The variation in profitability between firms rises sharply. Recessions have also provided opportunities for anti-competitive mergers, with a negative impact on the wider economy: the suspension of competition policy in the United States in the 1930s may have extended the Great Depression.

The living standards of people dependent on wages and salaries are not more affected by recessions than those who rely on fixed incomes or welfare benefits. The loss of a job is known to have a negative impact on the stability of families, and individuals' health and well-being. Fixed income benefits receive small cuts which make it tougher to survive.

According to the International Monetary Fund (IMF), "Global recessions seem to occur over a cycle lasting between eight and 10 years." The IMF takes many factors into account when defining a global recession. Until April 2009, IMF several times communicated to the press, that a global annual real GDP growth of 3.0 percent or less in their view was "...equivalent to a global recession".
By this measure, six periods since 1970 qualify: 1974–1975, 1980–1983, 1990–1993, 1998, 2001–2002, and 2008–2009. During what IMF in April 2002 termed the past three global recessions of the last three decades, global per capita output growth was zero or negative, and IMF argued—at that time—that because of the opposite being found for 2001, the economic state in this year by itself did not qualify as a "global recession".

In April 2009, IMF had changed their Global recession definition to:
By this new definition, a total of four global recessions took place since World War II: 1975, 1982, 1991 and 2009. All of them only lasted one year, although the third would have lasted three years (1991–93) if IMF as criteria had used the normal exchange rate weighted percapita real World GDP rather than the purchase power parity weighted percapita real World GDP.

The worst recession Australia has ever suffered happened in the beginning of the 1930s. As a result of late 1920s profit issues in agriculture and cutbacks, 1931-1932 saw Australia’s biggest recession in its entire history. It fared better than other nations, that underwent depressions, but their poor economic states influenced Australia’s as well, that depended on them for export, as well as foreign investments. The nation also benefited from bigger productivity in manufacturing, facilitated by trade protection, which also helped with feeling the effects less.

Due to a credit squeeze, the economy had gone into a brief recession in 1961
Australia was facing a rising level of inflation in 1973, caused partially by the oil crisis happening in that same year, which brought inflation at a 13% increase. Economic recession hit by the middle of the year 1974, with no change in policy enacted by the government as a measure to counter the economic situation of the country. Consequently, the unemployment level rose and the trade deficit increased significantly.

Another recession – the most recent one to date – came in the 1990s, at the beginning of the decade. It was the result of a major stock collapse in 1987, in October, referred to now as Black Monday. Although the collapse was larger than the one in 1929, the global economy recovered quickly, but North America still suffered a decline in lumbering savings and loans, which led to a crisis. The recession wasn’t limited to only America, but it also affected partnering nations, such as Australia. The unemployment level increased to 10.8%, employment declined by 3.4% and the GDP also decreased as much as 1.7%. Inflation, however, was successfully reduced.

The most recent recession to affect the United Kingdom was the late-2000s recession.

According to economists, since 1854, the U.S. has encountered 32 cycles of expansions and contractions, with an average of 17 months of contraction and 38 months of expansion. However, since 1980 there have been only eight periods of negative economic growth over one fiscal quarter or more, and four periods considered recessions:

For the past three recessions, the NBER decision has approximately conformed with the definition involving two consecutive quarters of decline. While the 2001 recession did not involve two consecutive quarters of decline, it was preceded by two quarters of alternating decline and weak growth.

Official economic data shows that a substantial number of nations were in recession as of early 2009. The US entered a recession at the end of 2007, and 2008 saw many other nations follow suit. The US recession of 2007 ended in June 2009 as the nation entered the current economic recovery. The timeline of the Great Recession details the many elements of this period.

The United States housing market correction (a consequence of the United States housing bubble) and subprime mortgage crisis significantly contributed to a recession.

The 2007–2009 recession saw private consumption fall for the first time in nearly 20 years. This indicated the depth and severity of the recession. With consumer confidence so low, economic recovery took a long time. Consumers in the U.S. were hit hard by the Great Recession, with the value of their houses dropping and their pension savings decimated on the stock market.

U.S. employers shed 63,000 jobs in February 2008, the most in five years. Former Federal Reserve chairman Alan Greenspan said on 6 April 2008 that "There is more than a 50 percent chance the United States could go into recession." On 1 October, the Bureau of Economic Analysis reported that an additional 156,000 jobs had been lost in September. On 29 April 2008, Moody's declared that nine US states were in a recession. In November 2008, employers eliminated 533,000 jobs, the largest single-month loss in 34 years. In 2008, an estimated 2.6 million U.S. jobs were eliminated.

The unemployment rate in the U.S. grew to 8.5 percent in March 2009, and there were 5.1 million job losses by March 2009 since the recession began in December 2007. That was about five million more people unemployed compared to just a year prior, which was the largest annual jump in the number of unemployed persons since the 1940s.

Although the US Economy grew in the first quarter by 1%, by June 2008 some analysts stated that due to a protracted credit crisis and "...rampant inflation in commodities such as oil, food, and steel," the country was nonetheless in a recession. The third quarter of 2008 brought on a GDP retraction of 0.5% the biggest decline since 2001. The 6.4% decline in spending during Q3 on non-durable goods, like clothing and food, was the largest since 1950.

A 17 November 2008 report from the Federal Reserve Bank of Philadelphia based on the survey of 51 forecasters, suggested that the recession started in April 2008 and would last 14 months. They project real GDP declining at an annual rate of 2.9% in the fourth quarter and 1.1% in the first quarter of 2009. These forecasts represent significant downward revisions from the forecasts of three months ago.

A 1 December 2008 report from the National Bureau of Economic Research stated that the U.S. had been in a recession since December 2007 (when economic activity peaked), based on a number of measures including job losses, declines in personal income, and declines in real GDP. By July 2009 a growing number of economists believed that the recession may have ended. The National Bureau of Economic Research announced on 20 September 2010 that the 2008/2009 recession ended in June 2009, making it the longest recession since World War II.



</doc>
<doc id="25385" url="https://en.wikipedia.org/wiki?curid=25385" title="RSA (cryptosystem)">
RSA (cryptosystem)

RSA (Rivest–Shamir–Adleman) is one of the first public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and it is different from the decryption key which is kept secret (private). In RSA, this asymmetry is based on the practical difficulty of the factorization of the product of two large prime numbers, the "factoring problem". The acronym RSA is made of the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who first publicly described the algorithm in 1977. Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), had developed an equivalent system in 1973, but this was not declassified until 1997.

A user of RSA creates and then publishes a public key based on two large prime numbers, along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but only someone with knowledge of the prime numbers can decode the message.
Breaking RSA encryption is known as the RSA problem. Whether it is as difficult as the factoring problem remains an open question. There are currently no published methods to defeat the system if a large enough key is used.

RSA is a relatively slow algorithm, and because of this, it is less commonly used to directly encrypt user data. More often, RSA passes encrypted shared keys for symmetric key cryptography which in turn can perform bulk encryption-decryption operations at much higher speed.

The idea of an asymmetric public-private key cryptosystem is attributed to Whitfield Diffie and Martin Hellman, who published this concept in 1976. They also introduced digital signatures and attempted to apply number theory. Their formulation used a shared-secret-key created from exponentiation of some number, modulo a prime number. However, they left open the problem of realizing a one-way function, possibly because the difficulty of factoring was not well-studied at the time.

Ron Rivest, Adi Shamir, and Leonard Adleman at the Massachusetts Institute of Technology, made several attempts over the course of a year to create a one-way function that was hard to invert. Rivest and Shamir, as computer scientists, proposed many potential functions, while Adleman, as a mathematician, was responsible for finding their weaknesses. They tried many approaches including "knapsack-based" and "permutation polynomials". For a time, they thought what they wanted to achieve was impossible due to contradictory requirements. In April 1977, they spent Passover at the house of a student and drank a good deal of Manischewitz wine before returning to their homes at around midnight. Rivest, unable to sleep, lay on the couch with a math textbook and started thinking about their one-way function. He spent the rest of the night formalizing his idea, and he had much of the paper ready by daybreak. The algorithm is now known as RSA – the initials of their surnames in same order as their paper.

Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), described an equivalent system in an internal document in 1973. However, given the relatively expensive computers needed to implement it at the time, RSA was considered to be mostly a curiosity and, as far as is publicly known, was never deployed. His discovery, however, was not revealed until 1997 due to its top-secret classification.

Kid-RSA (KRSA) is a simplified public-key cipher published in 1997, designed for educational purposes. Some people feel that learning Kid-RSA gives insight into RSA and other public-key ciphers, analogous to simplified DES.

MIT was granted for a "Cryptographic communications system and method" that used the algorithm, on September 20, 1983. Though the patent was going to expire on September 21, 2000 (the term of patent was 17 years at the time), the algorithm was released to the public domain by RSA Security on September 6, 2000, two weeks earlier. Since a detailed description of the algorithm had been published in the Mathematical Games column in the August 1977 issue of Scientific American, prior to the December 1977 filing date of the patent application, regulations in much of the rest of the world precluded patents elsewhere and only the US patent was granted. Had Cocks's work been publicly known, a patent in the United States would not have been legal either.

From the DWPI's abstract of the patent,
The RSA algorithm involves four steps: key generation, key distribution, encryption and decryption.

A basic principle behind RSA is the observation that it is practical to find three very large positive integers , and such that with modular exponentiation for all integers (with ):

and that even knowing and or even it can be extremely difficult to find . The triple bar (≡) here denotes modular congruence.

In addition, for some operations it is convenient that the order of the two exponentiations can be changed and that this relation also implies:

RSA involves a "public key" and a "private key." The public key can be known by everyone, and it is used for encrypting messages. The intention is that messages encrypted with the public key can only be decrypted in a reasonable amount of time by using the private key. The public key is represented by the integers and ; and, the private key, by the integer (although is also used during the decryption process. Thus, it might be considered to be a part of the private key, too). represents the message (previously prepared with a certain technique explained below).

The keys for the RSA algorithm are generated in the following way:
The "public key" consists of the modulus "n" and the public (or encryption) exponent "e". The "private key" consists of the private (or decryption) exponent "d", which must be kept secret. "p", "q", and "λ"("n") must also be kept secret because they can be used to calculate "d". In fact, they can all be discarded after "d" has been computed.

In the original RSA paper, the Euler totient function is used instead of "λ"("n") for calculating the private exponent "d". Since "φ"("n") is always divisible by "λ"("n") the algorithm works as well. That the Euler totient function can be used can also be seen as a consequence of the Lagrange's theorem applied to the multiplicative group of integers modulo pq. Thus any "d" satisfying also satisfies . However, computing "d" modulo "φ"("n") will sometimes yield a result that is larger than necessary (i.e. ). Most of the implementations of RSA will accept exponents generated using either method (if they use the private exponent "d" at all, rather than using the optimized decryption method based on the Chinese remainder theorem described below), but some standards like FIPS 186-4 may require that . Any "oversized" private exponents not meeting that criterion may always be reduced modulo "λ"("n") to obtain a smaller equivalent exponent.

Since any common factors of and are present in the factorisation of = = , it is recommended that and have only very small common factors, if any besides the necessary 2.

Note: The authors of the original RSA paper carry out the key generation by choosing "d" and then computing "e" as the modular multiplicative inverse of "d" modulo "φ"("n"), whereas most current implementations of RSA, such as those following PKCS#1, do the reverse (choose "e" and compute "d"). Since the chosen key can be small whereas the computed key normally is not, the RSA paper's algorithm optimizes decryption compared to encryption, while the modern algorithm optimizes encryption instead.

Suppose that Bob wants to send information to Alice. If they decide to use RSA, Bob must know Alice's public key to encrypt the message and Alice must use her private key to decrypt the message.
To enable Bob to send his encrypted messages, Alice transmits her public key to Bob via a reliable, but not necessarily secret, route. Alice's private key is never distributed.

After Bob obtains Alice's public key, he can send a message to Alice.

To do it, he first turns (strictly speaking, the un-padded plaintext) into an integer (strictly speaking, the padded plaintext), such that by using an agreed-upon reversible protocol known as a padding scheme. He then computes the ciphertext , using Alice's public key , corresponding to

This can be done reasonably quickly, even for very large numbers, using modular exponentiation. Bob then transmits to Alice.
Alice can recover from by using her private key exponent by computing

Given , she can recover the original message by reversing the padding scheme.

Here is an example of RSA encryption and decryption. The parameters used here are artificially small, but one can also .


The public key is (, ). For a padded plaintext message "m", the encryption function is

The private key is (, ). For an encrypted ciphertext "c", the decryption function is

For instance, in order to encrypt , we calculate

To decrypt , we calculate

Both of these calculations can be computed efficiently using the square-and-multiply algorithm for modular exponentiation. In real-life situations the primes selected would be much larger; in our example it would be trivial to factor "n", 3233 (obtained from the freely available public key) back to the primes "p" and "q". "e", also from the public key, is then inverted to get "d", thus acquiring the private key.

Practical implementations use the Chinese remainder theorem to speed up the calculation using modulus of factors (mod "pq" using mod "p" and mod "q").

The values "d", "d" and "q", which are part of the private key are computed as follows:

Here is how "d", "d" and "q" are used for efficient decryption. (Encryption is efficient by choice of a suitable "d" and "e" pair)

A working example in JavaScript using BigInteger.js. This code should not be used in production, as codice_1 uses codice_2, which is not a cryptographically secure pseudorandom number generator.
'use strict';
const RSA = {};
RSA.generate = function(keysize) {

};
RSA.encrypt = function(m, n, e) {

RSA.decrypt = function(c, d, n) {

Suppose Alice uses Bob's public key to send him an encrypted message. In the message, she can claim to be Alice but Bob has no way of verifying that the message was actually from Alice since anyone can use Bob's public key to send him encrypted messages. In order to verify the origin of a message, RSA can also be used to sign a message.

Suppose Alice wishes to send a signed message to Bob. She can use her own private key to do so. She produces a hash value of the message, raises it to the power of "d" (modulo "n") (as she does when decrypting a message), and attaches it as a "signature" to the message. When Bob receives the signed message, he uses the same hash algorithm in conjunction with Alice's public key. He raises the signature to the power of "e" (modulo "n") (as he does when encrypting a message), and compares the resulting hash value with the message's actual hash value. If the two agree, he knows that the author of the message was in possession of Alice's private key, and that the message has not been tampered with since.

This works because multiplication is commutative so

Thus, the keys may be swapped without loss of generality, that is a private key of a key pair may be used either to:

There are a number of attacks against plain RSA as described below.


To avoid these problems, practical RSA implementations typically embed some form of structured, randomized padding into the value "m" before encrypting it. This padding ensures that "m" does not fall into the range of insecure plaintexts, and that a given message, once padded, will encrypt to one of a large number of different possible ciphertexts.

Standards such as PKCS#1 have been carefully designed to securely pad messages prior to RSA encryption. Because these schemes pad the plaintext "m" with some number of additional bits, the size of the un-padded message "M" must be somewhat smaller. RSA padding schemes must be carefully designed so as to prevent sophisticated attacks which may be facilitated by a predictable message structure. Early versions of the PKCS#1 standard (up to version 1.5) used a construction that appears to make RSA semantically secure. However, at Crypto 1998, Bleichenbacher showed that this version is vulnerable to a practical adaptive chosen ciphertext attack. Furthermore, at Eurocrypt 2000, Coron et al. showed that for some types of messages, this padding does not provide a high enough level of security. Later versions of the standard include Optimal Asymmetric Encryption Padding (OAEP), which prevents these attacks. As such, OAEP should be used in any new application, and PKCS#1 v1.5 padding should be replaced wherever possible. The PKCS#1 standard also incorporates processing schemes designed to provide additional security for RSA signatures, e.g. the Probabilistic Signature Scheme for RSA (RSA-PSS).

Secure padding schemes such as RSA-PSS are as essential for the security of message signing as they are for message encryption. Two US patents on PSS were granted (USPTO 6266771 and USPTO 70360140); however, these patents expired on 24 July 2009 and 25 April 2010, respectively. Use of PSS no longer seems to be encumbered by patents. Note that using different RSA key-pairs for encryption and signing is potentially more secure.

For efficiency many popular crypto libraries (like OpenSSL, Java and .NET) use the following optimization for decryption and signing based on the Chinese remainder theorem. The following values are precomputed and stored as part of the private key:

These values allow the recipient to compute the exponentiation more efficiently as follows:

This is more efficient than computing exponentiation by squaring even though two modular exponentiations have to be computed. The reason is that these two modular exponentiations both use a smaller exponent and a smaller modulus.

The security of the RSA cryptosystem is based on two mathematical problems: the problem of factoring large numbers and the RSA problem. Full decryption of an RSA ciphertext is thought to be infeasible on the assumption that both of these problems are hard, i.e., no efficient algorithm exists for solving them. Providing security against "partial" decryption may require the addition of a secure padding scheme.

The RSA problem is defined as the task of taking "e"th roots modulo a composite "n": recovering a value "m" such that , where is an RSA public key and "c" is an RSA ciphertext. Currently the most promising approach to solving the RSA problem is to factor the modulus "n". With the ability to recover prime factors, an attacker can compute the secret exponent "d" from a public key , then decrypt "c" using the standard procedure. To accomplish this, an attacker factors "n" into "p" and "q", and computes which allows the determination of "d" from "e". No polynomial-time method for factoring large integers on a classical computer has yet been found, but it has not been proven that none exists. "See integer factorization for a discussion of this problem".

Multiple polynomial quadratic sieve (MPQS) can be used to factor the public modulus "n". The time taken to factor 128-bit and 256-bit "n" on a desktop computer are respectively 2 seconds and 35 minutes.
A tool called YAFU can be used to optimize this process. It took about 5720s to factor "320bit-N" on the same computer.

In 2009, Benjamin Moody factored an RSA-512 bit key in 73 days using only public software (GGNFS) and his desktop computer (a dual-core Athlon64 with a 1,900 MHz cpu.). Just less than five gigabytes of disk storage was required and about 2.5 gigabytes of RAM for the sieving process. The first RSA-512 factorization in 1999 required the equivalent of 8,400 MIPS years, over an elapsed time of about seven months.

Rivest, Shamir, and Adleman noted that Miller has shown that – assuming the truth of the Extended Riemann Hypothesis – finding "d" from "n" and "e" is as hard as factoring "n" into "p" and "q" (up to a polynomial time difference). However, Rivest, Shamir, and Adleman noted, in section IX/D of their paper, that they had not found a proof that inverting RSA is equally as hard as factoring.

, the largest factored RSA number was 795 bits long (240 decimal digits, see RSA-240). Its factorization, by a state-of-the-art distributed implementation, took around 900 CPU years. No larger RSA key is known publicly to have been factored. In practice, RSA keys are typically 1024 to 4096 bits long. Some experts believe that 1024-bit keys may become breakable in the near future or may already be breakable by a sufficiently well-funded attacker, though this is disputable. Few people see any way that 4096-bit keys could be broken in the foreseeable future. Therefore, it is generally presumed that RSA is secure if "n" is sufficiently large. If "n" is 300 bits or shorter, it can be factored in a few hours in a personal computer, using software already freely available. Keys of 512 bits have been shown to be practically breakable in 1999 when RSA-155 was factored by using several hundred computers, and these are now factored in a few weeks using common hardware. Exploits using 512-bit code-signing certificates that may have been factored were reported in 2011. A theoretical hardware device named TWIRL, described by Shamir and Tromer in 2003, called into question the security of 1024 bit keys. It is currently recommended that "n" be at least 2048 bits long.

In 1994, Peter Shor showed that a quantum computer – if one could ever be practically created for the purpose – would be able to factor in polynomial time, breaking RSA; see Shor's algorithm.

Finding the large primes "p" and "q" is usually done by testing random numbers of the right size with probabilistic primality tests that quickly eliminate virtually all of the nonprimes.

The numbers "p" and "q" should not be "too close", lest the Fermat factorization for "n" be successful. If "p" − "q" is less than 2"n" ("n" = "p" * "q", which for even small 1024-bit values of "n" is ) solving for "p" and "q" is trivial. Furthermore, if either "p" − 1 or "q" − 1 has only small prime factors, "n" can be factored quickly by Pollard's p − 1 algorithm, and such values of "p" or "q" should hence be discarded.

It is important that the private exponent "d" be large enough. Michael J. Wiener showed that if "p" is between "q" and 2"q" (which is quite typical) and , then "d" can be computed efficiently from "n" and "e".

There is no known attack against small public exponents such as , provided that the proper padding is used. Coppersmith's Attack has many applications in attacking RSA specifically if the public exponent "e" is small and if the encrypted message is short and not padded. 65537 is a commonly used value for "e"; this value can be regarded as a compromise between avoiding potential small exponent attacks and still allowing efficient encryptions (or signature verification). The NIST Special Publication on Computer Security (SP 800-78 Rev 1 of August 2007) does not allow public exponents "e" smaller than 65537, but does not state a reason for this restriction.

In October 2017 a team of researchers from Masaryk University announced the ROCA vulnerability, which affects RSA keys generated by an algorithm embodied in a library from Infineon. Large number of smart cards and TPMs were shown to be affected. Vulnerable RSA keys are easily identified using a test program the team released.

A cryptographically strong random number generator, which has been properly seeded with adequate entropy, must be used to generate the primes "p" and "q". An analysis comparing millions of public keys gathered from the Internet was carried out in early 2012 by Arjen K. Lenstra, James P. Hughes, Maxime Augier, Joppe W. Bos, Thorsten Kleinjung and Christophe Wachter. They were able to factor 0.2% of the keys using only Euclid's algorithm.

They exploited a weakness unique to cryptosystems based on integer factorization. If is one public key and is another, then if by chance (but "q" is not equal to "q"′), then a simple computation of factors both "n" and "n"′, totally compromising both keys. Lenstra et al. note that this problem can be minimized by using a strong random seed of bit-length twice the intended security level, or by employing a deterministic function to choose "q" given "p", instead of choosing "p" and "q" independently.

Nadia Heninger was part of a group that did a similar experiment. They used an idea of Daniel J. Bernstein to compute the GCD of each RSA key "n" against the product of all the other keys "n"′ they had found (a 729 million digit number), instead of computing each gcd("n","n"′) separately, thereby achieving a very significant speedup since after one large division the GCD problem is of normal size.

Heninger says in her blog that the bad keys occurred almost entirely in embedded applications, including "firewalls, routers, VPN devices, remote server administration devices, printers, projectors, and VOIP phones" from over 30 manufacturers. Heninger explains that the one-shared-prime problem uncovered by the two groups results from situations where the pseudorandom number generator is poorly seeded initially and then reseeded between the generation of the first and second primes. Using seeds of sufficiently high entropy obtained from key stroke timings or electronic diode noise or atmospheric noise from a radio receiver tuned between stations should solve the problem.

Strong random number generation is important throughout every phase of public key cryptography. For instance, if a weak generator is used for the symmetric keys that are being distributed by RSA, then an eavesdropper could bypass RSA and guess the symmetric keys directly.

Kocher described a new attack on RSA in 1995: if the attacker Eve knows Alice's hardware in sufficient detail and is able to measure the decryption times for several known ciphertexts, Eve can deduce the decryption key "d" quickly. This attack can also be applied against the RSA signature scheme. In 2003, Boneh and Brumley demonstrated a more practical attack capable of recovering RSA factorizations over a network connection (e.g., from a Secure Sockets Layer (SSL)-enabled webserver) This attack takes advantage of information leaked by the Chinese remainder theorem optimization used by many RSA implementations.

One way to thwart these attacks is to ensure that the decryption operation takes a constant amount of time for every ciphertext. However, this approach can significantly reduce performance. Instead, most RSA implementations use an alternate technique known as cryptographic blinding. RSA blinding makes use of the multiplicative property of RSA. Instead of computing , Alice first chooses a secret random value "r" and computes . The result of this computation after applying Euler's Theorem is and so the effect of "r" can be removed by multiplying by its inverse. A new value of "r" is chosen for each ciphertext. With blinding applied, the decryption time is no longer correlated to the value of the input ciphertext and so the timing attack fails.

In 1998, Daniel Bleichenbacher described the first practical adaptive chosen ciphertext attack, against RSA-encrypted messages using the PKCS #1 v1 padding scheme (a padding scheme randomizes and adds structure to an RSA-encrypted message, so it is possible to determine whether a decrypted message is valid). Due to flaws with the PKCS #1 scheme, Bleichenbacher was able to mount a practical attack against RSA implementations of the Secure Socket Layer protocol, and to recover session keys. As a result of this work, cryptographers now recommend the use of provably secure padding schemes such as Optimal Asymmetric Encryption Padding, and RSA Laboratories has released new versions of PKCS #1 that are not vulnerable to these attacks.

A side-channel attack using branch prediction analysis (BPA) has been described. Many processors use a branch predictor to determine whether a conditional branch in the instruction flow of a program is likely to be taken or not. Often these processors also implement simultaneous multithreading (SMT). Branch prediction analysis attacks use a spy process to discover (statistically) the private key when processed with these processors.

Simple Branch Prediction Analysis (SBPA) claims to improve BPA in a non-statistical way. In their paper, "On the Power of Simple Branch Prediction Analysis", the authors of SBPA (Onur Aciicmez and Cetin Kaya Koc) claim to have discovered 508 out of 512 bits of an RSA key in 10 iterations.

A power fault attack on RSA implementations has been described in 2010. The author recovered the key by varying the CPU power voltage outside limits; this caused multiple power faults on the server.

The generated primes can be attacked by rainbow tables because the random numbers are fixed and finite sets.

Below are some cryptography libraries that provide support for RSA:






</doc>
<doc id="25389" url="https://en.wikipedia.org/wiki?curid=25389" title="Robert A. Heinlein">
Robert A. Heinlein

Robert Anson Heinlein (; July 7, 1907 – May 8, 1988) was an American science-fiction author, aeronautical engineer, and retired Naval officer. Sometimes called the "dean of science fiction writers", he was among the first to emphasize scientific accuracy in his fiction, and was thus a pioneer of the subgenre of hard science fiction. His published works, both fiction and non-fiction, express admiration for competence and emphasize the value of critical thinking. His work continues to have an influence on the science-fiction genre, and on modern culture more generally.

Heinlein became one of the first American science-fiction writers to break into mainstream magazines such as "The Saturday Evening Post" in the late 1940s. He was one of the best-selling science-fiction novelists for many decades, and he, Isaac Asimov, and Arthur C. Clarke are often considered the "Big Three" of English-language science fiction authors. Notable Heinlein works include "Stranger in a Strange Land", "Starship Troopers" (which helped mold the space marine and mecha archetypes) and "The Moon Is a Harsh Mistress". His work sometimes had controversial aspects, such as plural marriage in "The Moon is a Harsh Mistress", militarism in "Starship Troopers" and technologically competent women characters that were strong and independent, yet often stereotypically feminine – such as "Friday".

A writer also of numerous science-fiction short stories, Heinlein was one of a group of writers who came to prominence under the editorship (1937–1971) of John W. Campbell at "Astounding Science Fiction" magazine, though Heinlein denied that Campbell influenced his writing to any great degree.

Heinlein used his science fiction as a way to explore provocative social and political ideas, and to speculate how progress in science and engineering might shape the future of politics, race, religion, and sex. Within the framework of his science-fiction stories, Heinlein repeatedly addressed certain social themes: the importance of individual liberty and self-reliance, the nature of sexual relationships, the obligation individuals owe to their societies, the influence of organized religion on culture and government, and the tendency of society to repress nonconformist thought. He also speculated on the influence of space travel on human cultural practices.

Heinlein was named the first Science Fiction Writers Grand Master in 1974. Four of his novels won Hugo Awards. In addition, fifty years after publication, seven of his works were awarded "Retro Hugos"—awards given retrospectively for works that were published before the Hugo Awards came into existence. In his fiction, Heinlein coined terms that have become part of the English language, including "grok", "waldo", and "speculative fiction", as well as popularizing existing terms like "TANSTAAFL", "pay it forward", and "space marine". He also anticipated mechanical computer-aided design with "Drafting Dan" and described a modern version of a waterbed in his novel "Beyond This Horizon". Although he never patented nor built one, his description in several stories caused the US patent office to refuse a patent attempt by Charles Hall, who created and marketed the waterbed we know today. In the first chapter of the novel "Space Cadet" he anticipated the cell-phone, 35 years before Motorola invented the technology. Several of Heinlein's works have been adapted for film and television.

Heinlein, born on July 7, 1907 to Rex Ivar Heinlein (an accountant) and Bam Lyle Heinlein, in Butler, Missouri, was the third of seven children. He was a 6th-generation German-American: a family tradition had it that Heinleins fought in every American war starting with the War of Independence.

He spent his childhood in Kansas City, Missouri.
The outlook and values of this time and place (in his own words, "The Bible Belt") had a definite influence on his fiction, especially in his later works, as he drew heavily upon his childhood in establishing the setting and cultural atmosphere in works like "Time Enough for Love" and "To Sail Beyond the Sunset". The 1910 appearance of Halley's Comet inspired the young child's life-long interest in astronomy.

When Heinlein graduated from Central High School in Kansas City in 1924 he aspired to a career as an officer in the US Navy. However, he was initially prevented from attending the United States Naval Academy at Annapolis because his older brother Rex was a student there, and regulations discouraged multiple family-members from attending the Academy simultaneously. He instead matriculated at Kansas City Community College and began vigorously petitioning Missouri Senator James A. Reed for an appointment to the Naval Academy. In part due to the influence of the Pendergast machine, the Naval Academy admitted him in June 1925.

Heinlein's experience in the Navy exerted a strong influence on his character and writing. In 1929, he graduated from the Naval Academy with the equivalent of a Bachelor of Arts degree in Engineering, ranking fifth in his class academically but with a class standing of 20th of 243 due to disciplinary demerits. Shortly after graduation, he was commissioned as an ensign by the U. S. Navy. He advanced to lieutenant, junior grade while serving aboard the new aircraft carrier in 1931, where he worked in radio communications, then in its earlier phases, with the carrier's aircraft. The captain of this carrier was Ernest J. King, who served as the Chief of Naval Operations and Commander-in-Chief, U.S. Fleet during World War II. Heinlein was frequently interviewed during his later years by military historians who asked him about Captain King and his service as the commander of the U.S. Navy's first modern aircraft carrier. Heinlein also served as gunnery officer aboard the destroyer in 1933 and 1934, reaching the rank of lieutenant. His brother, Lawrence Heinlein, served in the U.S. Army, the U.S. Air Force, and the Missouri National Guard, reaching the rank of major general in the National Guard.

In 1929, Heinlein married Elinor Curry of Kansas City. However, their marriage only lasted about a year. His second marriage in 1932 to Leslyn MacDonald (1904–1981) lasted for 15 years. MacDonald was, according to the testimony of Heinlein's Navy friend, Rear Admiral Cal Laning, "astonishingly intelligent, widely read, and extremely liberal, though a registered Republican," while Isaac Asimov later recalled that Heinlein was, at the time, "a flaming liberal". "(See section: Politics of Robert Heinlein.)"

At the Philadelphia Naval Shipyard Heinlein met and befriended a chemical engineer named Virginia "Ginny" Gerstenfeld. After the war, her engagement having fallen through, she moved to UCLA for doctoral studies in chemistry and made contact again.

As his second wife's alcoholism gradually spun out of control, Heinlein moved out and the couple filed for divorce. Heinlein's friendship with Virginia turned into a relationship and on October 21, 1948 — shortly after the decree nisi came through — they married in the town of Raton, New Mexico, shortly after setting up housekeeping in Colorado. They remained married until Heinlein's death.

As Heinlein's increasing success as a writer resolved their initial financial woes, they had a house custom built with various innovative features, later described in an article in "Popular Mechanics". In 1965, after various chronic health problems of Virginia's were traced back to altitude sickness, they moved to Santa Cruz, California, which is at sea level. They built a new residence in the adjacent village of Bonny Doon, California. Robert and Virginia designed and built their California house themselves, which is in a circular shape.

Ginny undoubtedly served as a model for many of his intelligent, fiercely independent female characters. She was a chemist and rocket test engineer, and held a higher rank in the Navy than Heinlein himself. She was also an accomplished college athlete, earning four letters. In 1953–1954, the Heinleins voyaged around the world (mostly via ocean liners and cargo liners, as Ginny detested flying), which Heinlein described in "Tramp Royale", and which also provided background material for science fiction novels set aboard spaceships on long voyages, such as "Podkayne of Mars", "Friday" and "", the latter initially being set on a cruise much as detailed in "Tramp Royale". Ginny acted as the first reader of his manuscripts. Isaac Asimov believed that Heinlein made a swing to the right politically at the same time he married Ginny.

In 1934, Heinlein was discharged from the Navy due to pulmonary tuberculosis. During a lengthy hospitalization, and inspired by his own experience while bed-ridden, he developed a design for a waterbed.

After his discharge, Heinlein attended a few weeks of graduate classes in mathematics and physics at the University of California at Los Angeles (UCLA), but he soon quit either because of his health or from a desire to enter politics.

Heinlein supported himself at several occupations, including real estate sales and silver mining, but for some years found money in short supply. Heinlein was active in Upton Sinclair's socialist End Poverty in California movement in the early 1930s. When Sinclair gained the Democratic nomination for Governor of California in 1934, Heinlein worked actively in the campaign. Heinlein himself ran for the California State Assembly in 1938, but was unsuccessful.

While not destitute after the campaign — he had a small disability pension from the Navy — Heinlein turned to writing to pay off his mortgage. His first published story, "Life-Line", was printed in the August 1939 issue of "Astounding Science Fiction". Originally written for a contest, he sold it to "Astounding" for significantly more than the contest's first-prize payoff. Another Future History story, "Misfit", followed in November. Some saw Heinlein's talent and stardom from his first story, and he was quickly acknowledged as a leader of the new movement toward "social" science fiction. In California he hosted the Mañana Literary Society, a 1940–41 series of informal gatherings of new authors. He was the guest of honor at Denvention, the 1941 Worldcon, held in Denver. During World War II, he did aeronautical engineering for the U.S. Navy, also recruiting Isaac Asimov and L. Sprague de Camp to work at the Philadelphia Naval Shipyard in Pennsylvania. While at the Philadelphia Naval Shipyards, Asimov, Heinlein, and de Camp brainstormed unconventional approaches to kamikaze attacks, such as using sound to detect approaching planes.

As the war wound down in 1945, Heinlein began to re-evaluate his career. The atomic bombings of Hiroshima and Nagasaki, along with the outbreak of the Cold War, galvanized him to write nonfiction on political topics. In addition, he wanted to break into better-paying markets. He published four influential short stories for "The Saturday Evening Post" magazine, leading off, in February 1947, with "The Green Hills of Earth". That made him the first science fiction writer to break out of the "pulp ghetto". In 1950, the movie "Destination Moon" — the documentary-like film for which he had written the story and scenario, co-written the script, and invented many of the effects — won an Academy Award for special effects. Also, he embarked on a series of juvenile novels for the Charles Scribner's Sons publishing company that went from 1947 through 1959, at the rate of one book each autumn, in time for Christmas presents to teenagers. He also wrote for "Boys' Life" in 1952.

Heinlein had used topical materials throughout his juvenile series beginning in 1947, but in 1958 he interrupted work on "The Heretic" (the working title of "Stranger in a Strange Land") to write and publish a book exploring ideas of civic virtue, initially serialized as "Starship Soldiers". In 1959, his novel (now entitled "Starship Troopers") was considered by the editors and owners of Scribner's to be too controversial for one of its prestige lines, and it was rejected. Heinlein found another publisher (Putnam), feeling himself released from the constraints of writing novels for children. He had told an interviewer that he did not want to do stories that merely added to categories defined by other works. Rather he wanted to do his own work, stating that: "I want to do my own stuff, my own way". He would go on to write a series of challenging books that redrew the boundaries of science fiction, including "Stranger in a Strange Land" (1961) and "The Moon Is a Harsh Mistress" (1966).

Beginning in 1970, Heinlein had a series of health crises, broken by strenuous periods of activity in his hobby of stonemasonry: in a private correspondence, he referred to that as his "usual and favorite occupation between books". The decade began with a life-threatening attack of peritonitis, recovery from which required more than two years, and treatment of which required multiple transfusions of Heinlein's rare blood type, A2 negative. As soon as he was well enough to write again, he began work on "Time Enough for Love" (1973), which introduced many of the themes found in his later fiction.

In the mid-1970s, Heinlein wrote two articles for the "Britannica Compton Yearbook". He and Ginny crisscrossed the country helping to reorganize blood donation in the United States in an effort to assist the system which had saved his life. At science fiction conventions to receive his autograph, fans would be asked to co-sign with Heinlein a beautifully embellished pledge form he supplied stating that the recipient agrees that they will donate blood. He was the guest of honor at the Worldcon in 1976 for the third time at MidAmeriCon in Kansas City, Missouri. At that Worldcon, Heinlein hosted a blood drive and donors' reception to thank all those who had helped save lives.

Beginning in 1977 and including an episode while vacationing in Tahiti in early 1978, he had episodes of reversible neurologic dysfunction due to transient ischemic attacks. Over the next few months, he became more and more exhausted, and his health again began to decline. The problem was determined to be a blocked carotid artery, and he had one of the earliest known carotid bypass operations to correct it. Heinlein and Virginia had been smokers, and smoking appears often in his fiction, as do fictitious strikable self-lighting cigarettes.

In 1980 Robert Heinlein was a member of the Citizens Advisory Council on National Space Policy, chaired by Jerry Pournelle, which met at the home of SF writer Larry Niven to write space policy papers for the incoming Reagan Administration. Members included such aerospace industry leaders as former astronaut Buzz Aldrin, General Daniel O. Graham, aerospace engineer Max Hunter and North American Rockwell VP for Space Shuttle development George Merrick. Policy recommendations from the Council included ballistic missile defense concepts which were later transformed into what was called the Strategic Defense Initiative, or "Star Wars" as derided by Senator Ted Kennedy. Heinlein assisted with Council contribution to the Reagan "Star Wars" speech of Spring 1983.

Asked to appear before a Joint Committee of the United States Congress that year, he testified on his belief that spin-offs from space technology were benefiting the infirm and the elderly. Heinlein's surgical treatment re-energized him, and he wrote five novels from 1980 until he died in his sleep from emphysema and heart failure on May 8, 1988.

At that time, he had been putting together the early notes for another "World as Myth" novel. Several of his other works have been published posthumously. Based on an outline and notes created by Heinlein in 1955, Spider Robinson has written the novel "Variable Star". Heinlein's posthumously published nonfiction includes a selection of correspondence and notes edited into a somewhat autobiographical examination of his career, published in 1989 under the title "Grumbles from the Grave" by his wife, Virginia; his book on practical politics written in 1946 published as "Take Back Your Government"; and a travelogue of their first around-the-world tour in 1954, "Tramp Royale". The novels "Podkayne of Mars" and "Red Planet", which were edited against his wishes in their original release, have been reissued in restored editions. "Stranger In a Strange Land" was originally published in a shorter form, but both the long and short versions are now simultaneously available in print.

Heinlein's archive is housed by the Special Collections department of McHenry Library at the University of California at Santa Cruz. The collection includes manuscript drafts, correspondence, photographs and artifacts. A substantial portion of the archive has been digitized and it is available online through the Robert A. and Virginia Heinlein Archives.

Heinlein published 32 novels, 59 short stories, and 16 collections during his life. Four films, two television series, several episodes of a radio series, and a board game have been derived more or less directly from his work. He wrote a screenplay for one of the films. Heinlein edited an anthology of other writers' SF short stories.

Three nonfiction books and two poems have been published posthumously. "For Us, The Living: A Comedy of Customs" was published posthumously in 2003; "Variable Star", written by Spider Robinson based on an extensive outline by Heinlein, was published in September 2006. Four collections have been published posthumously.

Over the course of his career Heinlein wrote three somewhat overlapping series.

Heinlein began his career as a writer of stories for "Astounding Science Fiction" magazine, which was edited by John Campbell. The science fiction writer Frederik Pohl has described Heinlein as "that greatest of Campbell-era sf writers". Isaac Asimov said that, from the time of his first story, the science fiction world accepted that Heinlein was the best science fiction writer in existence, adding that he would hold this title through his lifetime.

Alexei and Cory Panshin noted that Heinlein's impact was immediately felt. In 1940, the year after selling 'Life-Line' to Campbell, he wrote three short novels, four novelettes, and seven short stories. They went on to say that "No one ever dominated the science fiction field as Bob did in the first few years of his career." Alexei expresses awe in Heinlein's ability to show readers a world so drastically different from the one we live in now, yet have so many similarities. He says that "We find ourselves not only in a world other than our own, but identifying with a living, breathing individual who is operating within its context, and thinking and acting according to its terms."

The first novel that Heinlein wrote, "" (1939), did not see print during his lifetime, but Robert James tracked down the manuscript and it was published in 2003. Though some regard it as a failure as a novel, considering it little more than a disguised lecture on Heinlein's social theories, some readers took a very different view. In a review of it, John Clute wrote: I'm not about to suggest that if Heinlein had been able to publish [such works] openly in the pages of "Astounding" in 1939, SF would have gotten the future right; I would suggest, however, that if Heinlein, and his colleagues, had been able to publish adult SF in "Astounding" and its fellow journals, then SF might not have done such a grotesquely poor job of prefiguring something of the flavor of actually living here at the onset of 2004.

"For Us, the Living" was intriguing as a window into the development of Heinlein's radical ideas about man as a social animal, including his interest in free love. The root of many themes found in his later stories can be found in this book. It also contained a large amount of material that could be considered background for his other novels. This included a detailed description of the protagonist's treatment to avoid being banned to Coventry (a lawless land in the Heinlein mythos where unrepentant law-breakers are exiled).
It appears that Heinlein at least attempted to live in a manner consistent with these ideals, even in the 1930s, and had an open relationship in his marriage to his second wife, Leslyn. He was also a nudist; nudism and body taboos are frequently discussed in his work. At the height of the Cold War, he built a bomb shelter under his house, like the one featured in "Farnham's Freehold".

After "For Us, The Living", Heinlein began selling (to magazines) first short stories, then novels, set in a Future History, complete with a time line of significant political, cultural, and technological changes. A chart of the future history was published in the May 1941 issue of "Astounding". Over time, Heinlein wrote many novels and short stories that deviated freely from the Future History on some points, while maintaining consistency in some other areas. The Future History was eventually overtaken by actual events. These discrepancies were explained, after a fashion, in his later World as Myth stories.

The 1972 collection "Myths and Modern Man" noted 

Heinlein's first novel published as a book, "Rocket Ship Galileo", was initially rejected because going to the moon was considered too far-fetched, but he soon found a publisher, Scribner's, that began publishing a Heinlein juvenile once a year for the Christmas season. Eight of these books were illustrated by Clifford Geary in a distinctive white-on-black scratchboard style. Some representative novels of this type are "Have Space Suit—Will Travel", "Farmer in the Sky", and "Starman Jones". Many of these were first published in serial form under other titles, e.g., "Farmer in the Sky" was published as "Satellite Scout" in the Boy Scout magazine "Boys' Life". There has been speculation that Heinlein's intense obsession with his privacy was due at least in part to the apparent contradiction between his unconventional private life and his career as an author of books for children. However, "For Us, The Living" explicitly discusses the political importance Heinlein attached to privacy as a matter of principle thus negating this line of reasoning.

The novels that Heinlein wrote for a young audience are commonly called "the Heinlein juveniles", and they feature a mixture of adolescent and adult themes. Many of the issues that he takes on in these books have to do with the kinds of problems that adolescents experience. His protagonists are usually intelligent teenagers who have to make their way in the adult society they see around them. On the surface, they are simple tales of adventure, achievement, and dealing with stupid teachers and jealous peers. Heinlein was a vocal proponent of the notion that juvenile readers were far more sophisticated and able to handle more complex or difficult themes than most people realized. His juvenile stories often had a maturity to them that made them readable for adults. "Red Planet", for example, portrays some subversive themes, including a revolution in which young students are involved; his editor demanded substantial changes in this book's discussion of topics such as the use of weapons by children and the misidentified sex of the Martian character. Heinlein was always aware of the editorial limitations put in place by the editors of his novels and stories, and while he observed those restrictions on the surface, was often successful in introducing ideas not often seen in other authors' juvenile SF.

In 1957, James Blish wrote that one reason for Heinlein's success "has been the high grade of machinery which goes, today as always, into his story-telling. Heinlein seems to have known from the beginning, as if instinctively, technical lessons about fiction which other writers must learn the hard way (or often enough, never learn). He does not always operate the machinery to the best advantage, but he always seems to be aware of it."

Heinlein decisively ended his juvenile novels with "Starship Troopers" (1959), a controversial work and his personal riposte to leftists calling for President Dwight D. Eisenhower to stop nuclear testing in 1958. "The 'Patrick Henry' ad shocked 'em", he wrote many years later. ""Starship Troopers" outraged 'em." "Starship Troopers" is a coming-of-age story about duty, citizenship, and the role of the military in society. The book portrays a society in which suffrage is earned by demonstrated willingness to place society's interests before one's own, at least for a short time and often under onerous circumstances, in government service; in the case of the protagonist, this was military service.

Later, in "Expanded Universe", Heinlein said that it was his intention in the novel that service could include positions outside strictly military functions such as teachers, police officers, and other government positions. This is presented in the novel as an outgrowth of the failure of unearned suffrage government and as a very successful arrangement. In addition, the franchise was only awarded after leaving the assigned service; thus those serving their terms—in the military, or any other service—were excluded from exercising any franchise. Career military were completely disenfranchised until retirement.

The name "Starship Troopers" was licensed for an unrelated, B movie script called "Bug Hunt at Outpost Nine", which was then retitled to benefit from the book's credibility. The resulting film, entitled "Starship Troopers" (1997), which was written by Ed Neumeier and directed by Paul Verhoeven, had little relationship to the book, beyond the inclusion of character names, the depiction of space marines, and the concept of suffrage earned by military service. Fans of Heinlein were critical of the movie, which they considered a betrayal of Heinlein's philosophy, presenting the society in which the story takes place as fascist.

Likewise, the powered armor technology that is not only central to the book, but became a standard subgenre of science fiction thereafter, is completely absent in the movie, where the characters use World War II-technology weapons and wear light combat gear little more advanced than that. In Verhoeven's movie of the same name, there is no battle armor. Verhoeven commented that he had tried to read the book after he had bought the rights to it, in order to add it to his existing movie. However he read only the first two chapters, finding it too boring to continue. He thought it was a bad book and asked Ed Neumeier to tell him the story because he couldn't read it.

From about 1961 ("Stranger in a Strange Land") to 1973 ("Time Enough for Love"), Heinlein explored some of his most important themes, such as individualism, libertarianism, and free expression of physical and emotional love. Three novels from this period, "Stranger in a Strange Land", "The Moon Is a Harsh Mistress", and "Time Enough for Love", won the Libertarian Futurist Society's Prometheus Hall of Fame Award, designed to honor classic libertarian fiction. Jeff Riggenbach described "The Moon Is a Harsh Mistress" as "unquestionably one of the three or four most influential libertarian novels of the last century".

Heinlein did not publish "Stranger in a Strange Land" until some time after it was written, and the themes of free love and radical individualism are prominently featured in his long-unpublished first novel, "For Us, The Living: A Comedy of Customs".

"The Moon Is a Harsh Mistress" tells of a war of independence waged by the Lunar penal colonies, with significant comments from a major character, Professor La Paz, regarding the threat posed by government to individual freedom.

Although Heinlein had previously written a few short stories in the fantasy genre, during this period he wrote his first fantasy novel, "Glory Road". In "Stranger in a Strange Land" and "I Will Fear No Evil", he began to mix hard science with fantasy, mysticism, and satire of organized religion. Critics William H. Patterson, Jr., and Andrew Thornton believe that this is simply an expression of Heinlein's longstanding philosophical opposition to positivism. Heinlein stated that he was influenced by James Branch Cabell in taking this new literary direction. The penultimate novel of this period, "I Will Fear No Evil", is according to critic James Gifford "almost universally regarded as a literary failure" and he attributes its shortcomings to Heinlein's near-death from peritonitis.

After a seven-year hiatus brought on by poor health, Heinlein produced five new novels in the period from 1980 ("The Number of the Beast") to 1987 ("To Sail Beyond the Sunset"). These books have a thread of common characters and time and place. They most explicitly communicated Heinlein's philosophies and beliefs, and many long, didactic passages of dialog and exposition deal with government, sex, and religion. These novels are controversial among his readers and one critic, David Langford, has written about them very negatively. Heinlein's four Hugo awards were all for books written before this period.

Most of the novels from this period are recognized by critics as forming an offshoot from the Future History series, and referred to by the term World as Myth.

The tendency toward authorial self-reference begun in "Stranger in a Strange Land" and "Time Enough for Love" becomes even more evident in novels such as "The Cat Who Walks Through Walls", whose first-person protagonist is a disabled military veteran who becomes a writer, and finds love with a female character.

The 1982 novel "Friday", a more conventional adventure story (borrowing a character and backstory from the earlier short story "Gulf", also containing suggestions of connection to "The Puppet Masters") continued a Heinlein theme of expecting what he saw as the continued disintegration of Earth's society, to the point where the title character is strongly encouraged to seek a new life off-planet. It concludes with a traditional Heinlein note, as in "The Moon Is a Harsh Mistress" or "Time Enough for Love", that freedom is to be found on the frontiers.

The 1984 novel "" is a sharp satire of organized religion. Heinlein himself was agnostic.

Several Heinlein works have been published since his death, including the aforementioned "" as well as 1989's "Grumbles from the Grave", a collection of letters between Heinlein and his editors and agent; 1992's "Tramp Royale", a travelogue of a southern hemisphere tour the Heinleins took in the 1950s; "Take Back Your Government", a how-to book about participatory democracy written in 1946; and a tribute volume called "Requiem: Collected Works and Tributes to the Grand Master", containing some additional short works previously unpublished in book form. "Off the Main Sequence", published in 2005, includes three short stories never before collected in any Heinlein book (Heinlein called them "stinkeroos").

Spider Robinson, a colleague, friend, and admirer of Heinlein, wrote "Variable Star", based on an outline and notes for a juvenile novel that Heinlein prepared in 1955. The novel was published as a collaboration, with Heinlein's name above Robinson's on the cover, in 2006.

A complete collection of Heinlein's published work has been published by the Heinlein Prize Trust as the "Virginia Edition", after his wife. See the Complete Works section of Robert A. Heinlein bibliography for details.

On February 1, 2019, Phoenix Pick announced that through a collaboration with the Heinlein Prize Trust, a reconstruction of the full text of an unpublished Heinlein novel had been produced. The reconstructed novel, tentatively entitled "The Pursuit of the Pankera: A Parallel Novel about Parallel Universes", is an alternative version of "The Number of the Beast", with the first one-third of "The Pursuit of the Pankera" mostly the same as the first one-third of "The Number of the Beast" but the remainder of "The Pursuit of the Pankera" deviating entirely from "The Number of the Beast", with a completely different story-line. The newly reconstructed novel pays homage to Edgar Rice Burroughs and E. E. “Doc” Smith. It is currently being edited by Patrick Lobrutto. Some reviewers describe the newly-reconstructed novel as more in line with the style of a traditional Heinlein novel than was 'The Number of the Beast.' Both "The Pursuit of the Pankera" and a new edition of "The Number of the Beast" are planned to be published in the fourth quarter of 2019. The new edition of the latter will share the subtitle of "The Pursuit of the Pankera", hence it will be titled "The Number of the Beast: A Parallel Novel about Parallel Universes"

The primary influence on Heinlein's writing style may have been Rudyard Kipling. Kipling is the first known modern example of "indirect exposition", a writing technique for which Heinlein later became famous. In his famous text on "On the Writing of Speculative Fiction", Heinlein quotes Kipling:

"Stranger in a Strange Land" originated as a modernized version of Kipling's "The Jungle Book", his wife suggesting that the child be raised by Martians instead of wolves. Likewise, "Citizen of the Galaxy" can be seen as a reboot of Kipling's novel "Kim".

The "Starship Troopers" idea of needing to serve in the military in order to vote, can be found in Kipling's "The Army of a Dream":

Poul Anderson once said of Kipling's science fiction story "As Easy as A.B.C.", "a wonderful science fiction yarn, showing the same eye for detail that would later distinguish the work of Robert Heinlein".

Heinlein described himself as also being influenced by George Bernard Shaw, having read most of his plays. Shaw is an example of an earlier author who used the competent man, a favorite Heinlein archetype. He denied, though, any direct influence of "Back to Methuselah" on "Methuselah's Children".

Heinlein's books probe a range of ideas about a range of topics such as sex, race, politics, and the military. Many were seen as radical or as ahead of their time in their social criticism. His books have inspired considerable debate about the specifics, and the evolution, of Heinlein's own opinions, and have earned him both lavish praise and a degree of criticism. He has also been accused of contradicting himself on various philosophical questions.

Brian Doherty cites William Patterson, saying that the best way to gain an understanding of Heinlein is as a "full-service iconoclast, the unique individual who decides that things do not have to be, and won't continue, as they are". He says this vision is "at the heart of Heinlein, science fiction, libertarianism, and America. Heinlein imagined how everything about the human world, from our sexual mores to our religion to our automobiles to our government to our plans for cultural survival, might be flawed, even fatally so."

The critic Elizabeth Anne Hull, for her part, has praised Heinlein for his interest in exploring fundamental life questions, especially questions about "political power—our responsibilities to one another" and about "personal freedom, particularly sexual freedom".

Edward R. Murrow hosted a series on CBS Radio called This I Believe, which solicited an Entry from Heinlein that is probably the most enduring and popular of the title: Our Noble, Essential Decency. In it, Heinlein broke with the normal trends, stating that he believed in his neighbors (some of whom he named and described), community, and towns across America that share the same sense of good will and intentions as his own, going on to apply this same philosophy to the US, and humanity in general.

Heinlein's political positions shifted throughout his life. Heinlein's early political leanings were liberal. In 1934, he worked actively for the Democratic campaign of Upton Sinclair for Governor of California. After Sinclair lost, Heinlein became an anti-Communist Democratic activist. He made an unsuccessful bid for a California State Assembly seat in 1938. Heinlein's first novel, "For Us, The Living" (written 1939), consists largely of speeches advocating the Social Credit system, and the early story "Misfit" (1939) deals with an organization—"The Cosmic Construction Corps"—that seems to be Franklin D. Roosevelt's Civilian Conservation Corps translated into outer space.

Of this time in his life, Heinlein later said:

Heinlein's fiction of the 1940s and 1950s, however, began to espouse conservative views. After 1945, he came to believe that a strong world government was the only way to avoid mutual nuclear annihilation. His 1949 novel "Space Cadet" describes a future scenario where a military-controlled global government enforces world peace. Heinlein ceased considering himself a Democrat in 1954.

The Heinleins formed the small "Patrick Henry League" in 1958, and they worked in the 1964 Barry Goldwater Presidential campaign.

That ad was entitled Who Are the Heirs of Patrick Henry?. It started with the famous Henry quotation: "Is life so dear, or peace so sweet, as to be purchased at the price of chains and slavery? Forbid it, Almighty God! I know not what course others may take, but as for me, give me liberty, or give me death!!". It then went on to admit that there was some risk to nuclear testing (albeit less than the "willfully distorted" claims of the test ban advocates), and risk of nuclear war, but that "The alternative is surrender. We accept the risks." Heinlein was among those who in 1968 signed a pro-Vietnam War ad in "Galaxy Science Fiction". In his essay "Starship Stormtroopers", Michael Moorcock posits that Heinlein was a fascist who fetishized violence and militarism.

Heinlein always considered himself a libertarian; in a letter to Judith Merril in 1967 (never sent) he said, "As for libertarian, I've been one all my life, a radical one. You might use the term 'philosophical anarchist' or 'autarchist' about me, but 'libertarian' is easier to define and fits well enough."

"Stranger in a Strange Land" was embraced by the hippie counterculture, and libertarians have found inspiration in "The Moon Is a Harsh Mistress". Both groups found resonance with his themes of personal freedom in both thought and action.

Heinlein grew up in the era of racial segregation in the United States and wrote some of his most influential fiction at the height of the civil rights movement. He explicitly made the case for using his fiction not only to predict the future but to educate his readers about the value of racial equality and the importance of racial tolerance. His early novels were very much ahead of their time both in their explicit rejection of racism and in their inclusion of protagonists of color—in the context of science fiction before the 1960s, the mere existence of characters of color was a remarkable novelty, with green occurring more often than brown. For example, his 1948 novel "Space Cadet" explicitly uses aliens as a metaphor for minorities. In his novel "Star Beast", the "de facto" foreign minister of the Terran government is an undersecretary, a Mr. Kiku, who is from Africa. Heinlein explicitly states his skin is "ebony black", and that Kiku is in an arranged marriage that is happy.

In a number of his stories, Heinlein challenges his readers' possible racial preconceptions by introducing a strong, sympathetic character, only to reveal much later that he or she is of African or other ancestry; in several cases, the covers of the books show characters as being light-skinned, when in fact the text states, or at least implies, that they are dark-skinned or of African ancestry. Heinlein repeatedly denounced racism in his non-fiction works, including numerous examples in "Expanded Universe".

Heinlein reveals in "Starship Troopers" that the novel's protagonist and narrator, Johnny Rico, the formerly disaffected scion of a wealthy family, is Filipino, actually named "Juan Rico" and speaks Tagalog in addition to English.

Race was a central theme in some of Heinlein's fiction. The most prominent and controversial example is "Farnham's Freehold", which casts a white family into a future in which white people are the slaves of cannibalistic black rulers. In the 1941 novel "Sixth Column" (also known as "The Day After Tomorrow"), a white resistance movement in the United States defends itself against an invasion by an Asian fascist state (the "Pan-Asians") using a "super-science" technology that allows ray weapons to be tuned to specific races. The book is sprinkled with racist slurs against Asian people, and black and Hispanic people are not mentioned at all. The idea for the story was pushed on Heinlein by editor John W. Campbell, and Heinlein wrote later that he had "had to re-slant it to remove racist aspects of the original story line" and that he did not "consider it to be an artistic success". However, the novel prompted a heated debate in the scientific community regarding the plausibility of developing ethnic bioweapons.

In keeping with his belief in individualism, his work for adults—and sometimes even his work for juveniles—often portrays both the oppressors and the oppressed with considerable ambiguity. Heinlein believed that individualism was incompatible with ignorance. He believed that an appropriate level of adult competence was achieved through a wide-ranging education, whether this occurred in a classroom or not. In his juvenile novels, more than once a character looks with disdain at a student's choice of classwork, saying, "Why didn't you study something useful?" In "Time Enough for Love", Lazarus Long gives a long list of capabilities that anyone should have, concluding, "Specialization is for insects." The ability of the individual to create himself is explored in stories such as "I Will Fear No Evil", "—All You Zombies—", and "By His Bootstraps".

Heinlein claimed to have written "Starship Troopers" in response to "calls for the unilateral ending of nuclear testing by the United States". Heinlein suggests in the book that the Bugs are a good example of Communism being something that humans cannot successfully adhere to, since humans are strongly defined individuals, whereas the Bugs, being a collective, can all contribute to the whole without consideration of individual desire.

For Heinlein, personal liberation included sexual liberation, and free love was a major subject of his writing starting in 1939, with "For Us, The Living". During his early period, Heinlein's writing for younger readers needed to take account of both editorial perceptions of sexuality in his novels, and potential perceptions among the buying public; as critic William H. Patterson has put it, his dilemma was "to sort out what was really objectionable from what was only excessive over-sensitivity to imaginary librarians".

By his middle period, sexual freedom and the elimination of sexual jealousy became a major theme; for instance, in "Stranger in a Strange Land" (1961), the progressively minded but sexually conservative reporter, Ben Caxton, acts as a dramatic foil for the less parochial characters, Jubal Harshaw and Valentine Michael Smith (Mike). Another of the main characters, Jill, is homophobic.

According to Gary Westfahl, "Heinlein is a problematic case for feminists; on the one hand, his works often feature strong female characters and vigorous statements that women are equal to or even superior to men; but these characters and statements often reflect hopelessly stereotypical attitudes about typical female attributes. It is disconcerting, for example, that in "Expanded Universe" Heinlein calls for a society where all lawyers and politicians are women, essentially on the grounds that they possess a mysterious feminine practicality that men cannot duplicate."

In books written as early as 1956, Heinlein dealt with incest and the sexual nature of children. Many of his books including "Time for the Stars", "Glory Road", "Time Enough for Love", and "The Number of the Beast" dealt explicitly or implicitly with incest, sexual feelings and relations between adults and children, or both. The treatment of these themes include the romantic relationship and eventual marriage, once the girl becomes an adult via time-travel, of a 30-year-old engineer and an 11-year-old girl in "The Door into Summer" or the more overt intra-familial incest in "To Sail Beyond the Sunset" and "Farnham's Freehold". Heinlein often posed situations where the nominal purpose of sexual taboos was irrelevant to a particular situation, due to future advances in technology. For example, in "Time Enough for Love" Heinlein describes a brother and sister (Joe and Llita) who were mirror twins, being complementary diploids with entirely disjoint genomes, and thus not at increased risk for unfavorable gene duplication due to consanguinity. In this instance, Llita and Joe were props used to explore the concept of incest, where the usual objection to incest — heightened risk of genetic defect in their children — was not a consideration. Peers such as L. Sprague de Camp and Damon Knight have commented critically on Heinlein's portrayal of incest and pedophilia in a lighthearted and even approving manner. However, Heinlein's intent seems more to provoke the reader and to question sexual mores than to promote any particular sexual agenda.

In "To Sail Beyond the Sunset", Heinlein has the main character, Maureen, state that the purpose of metaphysics is to ask questions: "Why are we here?" "Where are we going after we die?" (and so on); and that you are not allowed to answer the questions. "Asking" the questions is the point of metaphysics, but "answering" them is not, because once you answer this kind of question, you cross the line into religion. Maureen does not state a reason for this; she simply remarks that such questions are "beautiful" but lack answers. Maureen's son/lover Lazarus Long makes a related remark in "Time Enough for Love". In order for us to answer the "big questions" about the universe, Lazarus states at one point, it would be necessary to stand "outside" the universe.

During the 1930s and 1940s, Heinlein was deeply interested in Alfred Korzybski's general semantics and attended a number of seminars on the subject. His views on epistemology seem to have flowed from that interest, and his fictional characters continue to express Korzybskian views to the very end of his writing career. Many of his stories, such as "Gulf", "If This Goes On—", and "Stranger in a Strange Land", depend strongly on the premise, related to the well-known Sapir–Whorf hypothesis, that by using a correctly designed language, one can change or improve oneself mentally, or even realize untapped potential (as in the case of Joe in "Gulf" – whose last name may be Greene, Gilead or Briggs).

When Ayn Rand's novel "The Fountainhead" was published, Heinlein was very favorably impressed, as quoted in "Grumbles ..." and mentioned John Galt—the hero in Rand's "Atlas Shrugged"—as a heroic archetype in "The Moon Is a Harsh Mistress". He was also strongly affected by the religious philosopher P. D. Ouspensky. Freudianism and psychoanalysis were at the height of their influence during the peak of Heinlein's career, and stories such as "Time for the Stars" indulged in psychological theorizing.

However, he was skeptical about Freudianism, especially after a struggle with an editor who insisted on reading Freudian sexual symbolism into his juvenile novels. Heinlein was fascinated by the social credit movement in the 1930s. This is shown in "Beyond This Horizon" and in his 1938 novel "", which was finally published in 2003, long after his death.

The term "pay it forward", though it was already in occasional use as a quotation, was popularized by Robert A. Heinlein in his book "Between Planets", published in 1951:

He referred to this in a number of other stories, although sometimes just saying to pay a debt back by helping others, as in one of his last works, Job, a Comedy of Justice.

Heinlein was a mentor to Ray Bradbury, giving him help and quite possibly passing on the concept, made famous by the publication of a letter from him to Heinlein thanking him. In Bradbury's novel "Dandelion Wine", published in 1957, when the main character Douglas Spaulding is reflecting on his life being saved by Mr. Jonas, the Junkman:

Bradbury has also advised that writers he has helped thank him by helping other writers.

Heinlein both preached and practiced this philosophy; now the Heinlein Society, a humanitarian organization founded in his name, does so, attributing the philosophy to its various efforts, including Heinlein for Heroes, the Heinlein Society Scholarship Program, and Heinlein Society blood drives.
Author Spider Robinson made repeated reference to the doctrine, attributing it to his spiritual mentor Heinlein.

Heinlein is usually identified, along with Isaac Asimov and Arthur C. Clarke, as one of the three masters of science fiction to arise in the so-called Golden Age of science fiction, associated with John W. Campbell and his magazine "Astounding".
In the 1950s he was a leader in bringing science fiction out of the low-paying and less prestigious "pulp ghetto". Most of his works, including short stories, have been continuously in print in many languages since their initial appearance and are still available as new paperbacks decades after his death.

He was at the top of his form during, and himself helped to initiate, the trend toward social science fiction, which went along with a general maturing of the genre away from space opera to a more literary approach touching on such adult issues as politics and human sexuality. In reaction to this trend, hard science fiction began to be distinguished as a separate subgenre, but paradoxically Heinlein is also considered a seminal figure in hard science fiction, due to his extensive knowledge of engineering and the careful scientific research demonstrated in his stories. Heinlein himself stated—with obvious pride—that in the days before pocket calculators, he and his wife Virginia once worked for several days on a mathematical equation describing an Earth-Mars rocket orbit, which was then subsumed in a single sentence of the novel "Space Cadet".

Heinlein is often credited with bringing serious writing techniques to the genre of science fiction.

For example, when writing about fictional worlds, previous authors were often limited by the reader's existing knowledge of a typical "space opera" setting, leading to a relatively low creativity level: The same starships, death rays, and horrifying rubbery aliens becoming ubiquitous. This was necessary unless the author was willing to go into long expositions about the setting of the story, at a time when the word count was at a premium in SF.

But Heinlein utilized a technique called "indirect exposition", perhaps first introduced by Rudyard Kipling in his own science fiction venture, the Aerial Board of Control stories. Kipling had picked this up during his time in India, using it to avoid bogging down his stories set in India with explanations for his English readers. This technique — mentioning details in a way that lets the reader infer more about the universe than is actually spelled out became a trademark rhetorical technique of both Heinlein and generation of writers influenced by him. Heinlein was significantly influenced by Kipling beyond this, for example quoting him in On the Writing of Speculative Fiction.
Likewise, Heinlein's name is often associated with the competent hero, a character archetype who, though he or she may have flaws and limitations, is a strong, accomplished person able to overcome any soluble problem set in their path. They tend to feel confident overall, have a broad life experience and set of skills, and not give up when the going gets tough. This style influenced not only the writing style of a generation of authors, but even their personal character. Harlan Ellison once said, "Very early in life when I read Robert Heinlein I got the thread that runs through his stories—the notion of the competent man ... I've always held that as my ideal. I've tried to be a very competent man."

When fellow writers, or fans, wrote Heinlein asking for writing advice, he famously gave out his own list of rules for becoming a successful writer:

About which he said:
Heinlein later published an entire article, "On the Writing of Speculative Fiction", which included his rules, and from which the above quote is taken. When he says "anything said above them", he refers to his other guidelines. For example, he describes most stories as fitting into one of a handful of basic categories:


In the article, Heinlein credits L. Ron Hubbard as having identified "The Man-Who-Learned-Better".

Heinlein has had a pervasive influence on other science fiction writers. In a 1953 poll of leading science fiction authors, he was cited more frequently as an influence than any other modern writer. Critic James Gifford writes that "Although many other writers have exceeded Heinlein's output, few can claim to match his broad and seminal influence. Scores of science fiction writers from the prewar Golden Age through the present day loudly and enthusiastically credit Heinlein for blazing the trails of their own careers, and shaping their styles and stories."

Heinlein gave Larry Niven and Jerry Pournelle extensive advice on a draft manuscript of "The Mote in God's Eye". He contributed a cover blurb "Possibly the finest science fiction novel I have ever read." Writer David Gerrold, responsible for creating the tribbles in "Star Trek", also credited Heinlein as the inspiration for his "Dingilliad" series of novels. Gregory Benford refers to his novel "Jupiter Project" as a Heinlein tribute. Similarly, Charles Stross says his Hugo Award-nominated novel "Saturn's Children" is "a space opera and late-period Robert A. Heinlein tribute", referring to Heinlein's "Friday". The theme and plot of Kameron Hurley's novel, "The Light Brigade" clearly echo those of Heinlein's "Starship Troopers".

Even outside the science fiction community, several words and phrases coined or adopted by Heinlein have passed into common English usage:

In 1962, Oberon Zell-Ravenheart (then still using his birth name, Tim Zell) founded the Church of All Worlds, a Neopagan religious organization modeled in many ways (including its name) after the treatment of religion in the novel "Stranger in a Strange Land". This spiritual path included several ideas from the book, including non-mainstream family structures, social libertarianism, water-sharing rituals, an acceptance of all religious paths by a single tradition, and the use of several terms such as "grok", "Thou art God", and "Never Thirst". Though Heinlein was neither a member nor a promoter of the Church, there was a frequent exchange of correspondence between Zell and Heinlein, and he was a paid subscriber to their magazine, "Green Egg". This Church still exists as a 501(C)(3) religious organization incorporated in California, with membership worldwide, and it remains an active part of the neopagan community today. Zell-Ravenheart went on to coin the term polyamory in the 1980s, another movement that includes Heinlein concepts among its roots.

Heinlein was influential in making space exploration seem to the public more like a practical possibility. His stories in publications such as "The Saturday Evening Post" took a matter-of-fact approach to their outer-space setting, rather than the "gee whiz" tone that had previously been common. The documentary-like film "Destination Moon" advocated a Space Race with an unspecified foreign power almost a decade before such an idea became commonplace, and was promoted by an unprecedented publicity campaign in print publications. Many of the astronauts and others working in the U.S. space program grew up on a diet of the Heinlein juveniles, best evidenced by the naming of a crater on Mars after him, and a tribute interspersed by the Apollo 15 astronauts into their radio conversations while on the moon.

Heinlein was also a guest commentator for Walter Cronkite during Neil Armstrong and Buzz Aldrin's Apollo 11 moon landing. He remarked to Cronkite during the landing that, "This is the greatest event in human history, up to this time. This is—today is New Year's Day of the Year One." Businessman and entrepreneur Elon Musk says that Heinlein's books have helped inspire his career.

The Heinlein Society was founded by Virginia Heinlein on behalf of her husband, to "pay forward" the legacy of the writer to future generations of "Heinlein's Children". The foundation has programs to:

The Heinlein society also established the Robert A. Heinlein Award in 2003 "for outstanding published works in science fiction and technical writings to inspire the human exploration of space".


In his lifetime, Heinlein received four Hugo Awards, for "Double Star", "Starship Troopers", "Stranger in a Strange Land", and "The Moon Is a Harsh Mistress", and was nominated for four Nebula Awards, for "Stranger in a Strange Land", "Friday", "Time Enough for Love", and "Job: A Comedy of Justice". He was also given seven Retro-Hugos: two for best novel: "Beyond This Horizon" and "Farmer in the Sky"; Three for best novella: :"If This Goes On ...", "Waldo", and "The Man Who Sold the Moon"; one for best novelette: "The Roads Must Roll"; and one for best dramatic presentation: "Destination Moon".

The Science Fiction Writers of America named Heinlein its first Grand Master in 1974, presented 1975. Officers and past presidents of the Association select a living writer for lifetime achievement (now annually and including fantasy literature).

Main-belt asteroid 6312 Robheinlein (1990 RH4), discovered on September 14, 1990 by H. E. Holt, at Palomar was named after him.

There is no lunar feature named explicitly for Heinlein, but in 1994 the International Astronomical Union named Heinlein crater on Mars in his honor.

The Science Fiction and Fantasy Hall of Fame inducted Heinlein in 1998, its third class of two deceased and two living writers and editors.

In 2001 the United States Naval Academy created the Robert A. Heinlein Chair In Aerospace Engineering.

In 2016, after an intensive online campaign to win a vote for the opening, Heinlein was inducted into the Hall of Famous Missourians. His bronze bust, created by Kansas City sculptor E. Spencer Schubert, is on permanent display in the Missouri State Capitol in Jefferson City.

The Libertarian Futurist Society has honored five of Heinlein's novels and two short stories with their Hall of Fame award. The first two were given during his lifetime for "The Moon Is a Harsh Mistress" and "Stranger in a Strange Land". Five more were awarded posthumously for "Red Planet", "Methuselah's Children", "Time Enough for Love", and the short stories "Requiem" and "Coventry".







</doc>
<doc id="25391" url="https://en.wikipedia.org/wiki?curid=25391" title="Russia">
Russia

Russia (), officially the Russian Federation, is a European country located in Eastern Europe with a vast expanse of territory that stretches across Northern Asia. At , it is, by far the largest country in the world by area, covering more than one-eighth of the Earth's inhabited land area, spanning eleven time zones, and bordering 16 sovereign nations. The territory of Russia extends from the Baltic Sea in the west to the Pacific Ocean in the east, and from the Arctic Ocean in the north to the Black Sea and the Caucasus in the south. About 146.8 million people live in the country's 85 federal subjects (including the disputed Crimea and Sevastapol) as of 2019, making Russia the ninth most populous nation in the world and the most populous nation in Europe. Russia's capital and largest city is Moscow; other major urban areas include Saint Petersburg, Novosibirsk, Yekaterinburg, Nizhny Novgorod, Kazan and Chelyabinsk.

The East Slavs emerged as a recognizable group in Europe between the 3rd and 8th centuries AD. Founded and ruled by a Varangian warrior elite and their descendants, the medieval state of Rus arose in the 9th century. In 988 it adopted Orthodox Christianity from the Byzantine Empire, beginning the synthesis of Byzantine and Slavic cultures that defined Russian culture for the next millennium. Rus ultimately disintegrated into a number of smaller states, until it was finally reunified by the Grand Duchy of Moscow in the 15th century. By the 18th century, the nation had greatly expanded through conquest, annexation, and exploration to become the Russian Empire, which was the third largest empire in history, stretching from Poland on the west to Alaska on the east. Following the Russian Revolution, the Russian Soviet Federative Socialist Republic (Russian SFSR) became the largest and leading constituent of the Union of Soviet Socialist Republics (USSR/Soviet Union), the world's first constitutionally socialist state. The Soviet Union played a decisive role in the Allied victory in World War II, and emerged as a recognized superpower and rival to the United States during the Cold War. The Soviet era saw some of the most significant technological achievements of the 20th century, including the world's first human-made satellite and the launching of the first humans in space. Following the dissolution of the Soviet Union in 1991, the Russian SFSR reconstituted itself as the Russian Federation and is recognized as the continuing legal personality and a successor of the USSR.

The political system of Russia is governed as a federal semi-presidential republic since 1993. Vladimir Putin has dominated Russia's political system since 2000, serving as either president or prime minister. His government is often characterized as authoritarian, where Russia has experienced democratic backsliding and a worsening human rights record under his leadership. It is ranked 137th out of 180 countries in the 2019 Corruption Perceptions Index and 24 of 29 in the 2018 Nations in Transit Report by Freedom House, while being ranked 134th in the 2019 Democracy Index.

Russia's economy ranks as the eleventh largest by nominal GDP and sixth largest by purchasing power parity in 2019. Russia's extensive mineral and energy resources are the largest such reserves in the world, making it one of the leading producers of oil and natural gas globally. The country is one of the five recognized nuclear weapons states and possesses the largest stockpile of weapons of mass destruction. Russia is a great power as well as a regional power and has been characterised as a potential superpower. Its military has been ranked as the world's second most powerful. It is a permanent member of the United Nations Security Council and an active global partner of ASEAN, as well as a member of the Shanghai Cooperation Organisation (SCO), the G20, the Council of Europe, the Asia-Pacific Economic Cooperation (APEC), the Organization for Security and Co-operation in Europe (OSCE), the International Investment Bank (IIB) and the World Trade Organization (WTO), as well as being the leading member of the Commonwealth of Independent States (CIS), the Collective Security Treaty Organization (CSTO) and a member of the Eurasian Economic Union (EAEU).

The name "Russia" is derived from Rus', a medieval state populated mostly by the East Slavs. However, this proper name became more prominent in the later history, and the country typically was called by its inhabitants "Русская Земля" (russkaja zemlja), which can be translated as "Russian Land" or "Land of Rus'". In order to distinguish this state from other states derived from it, it is denoted as "Kievan Rus'" by modern historiography. The name "Rus" itself comes from the early medieval Rus' people, Swedish merchants and warriors who relocated from across the Baltic Sea and founded a state centered on Novgorod that later became Kievan Rus.

An old Latin version of the name Rus' was Ruthenia, mostly applied to the western and southern regions of Rus' that were adjacent to Catholic Europe. The current name of the country, Россия (Rossija), comes from the Byzantine Greek designation of the Rus', Ρωσσία "Rossía"—spelled Ρωσία ("Rosía" ) in Modern Greek.

The standard way to refer to citizens of Russia is "Russians" in English and "rossiyane" () in Russian. There are two Russian words which are commonly translated into English as "Russians". One is "русские" ("russkiye"), which most often means "ethnic Russians". Another is "россияне" ("rossiyane"), which means "citizens of Russia, regardless of ethnicity". Translations into other languages often do not distinguish these two groups.

Nomadic pastoralism developed in the Pontic-Caspian steppe beginning in the Chalcolithic.

In classical antiquity, the Pontic Steppe was known as Scythia. Beginning in the 8th century BC, Ancient Greek traders brought their civilization to the trade emporiums in Tanais and Phanagoria. Ancient Greek explorers, most notably Pytheas, even went as far as modern day Kaliningrad, on the Baltic Sea. Romans settled on the western part of the Caspian Sea, where their empire stretched towards the east. In the 3rd to 4th centuries AD a semi-legendary Gothic kingdom of Oium existed in Southern Russia until it was overrun by Huns. Between the 3rd and 6th centuries AD, the Bosporan Kingdom, a Hellenistic polity which succeeded the Greek colonies, was also overwhelmed by nomadic invasions led by warlike tribes, such as the Huns and Eurasian Avars. A Turkic people, the Khazars, ruled the lower Volga basin steppes between the Caspian and Black Seas until the 10th century.

The ancestors of modern Russians are the Slavic tribes, whose original home is thought by some scholars to have been the wooded areas of the Pinsk Marshes. The East Slavs gradually settled Western Russia in two waves: one moving from Kiev toward present-day Suzdal and Murom and another from Polotsk toward Novgorod and Rostov. From the 7th century onwards, the East Slavs constituted the bulk of the population in Western Russia and assimilated the native Finno-Ugric peoples, including the Merya, the Muromians, and the Meshchera.

The establishment of the first East Slavic states in the 9th century coincided with the arrival of Varangians, the traders, warriors and settlers from the Baltic Sea region. Primarily they were Vikings of Scandinavian origin, who ventured along the waterways extending from the eastern Baltic to the Black and Caspian Seas. According to the "Primary Chronicle", a Varangian from Rus' people, named Rurik, was elected ruler of Novgorod in 862. In 882, his successor Oleg ventured south and conquered Kiev, which had been previously paying tribute to the Khazars. Oleg, Rurik's son Igor and Igor's son Sviatoslav subsequently subdued all local East Slavic tribes to Kievan rule, destroyed the Khazar khaganate and launched several military expeditions to Byzantium and Persia.

In the 10th to 11th centuries Kievan Rus' became one of the largest and most prosperous states in Europe. The reigns of Vladimir the Great (980–1015) and his son Yaroslav the Wise (1019–1054) constitute the Golden Age of Kiev, which saw the acceptance of Orthodox Christianity from Byzantium and the creation of the first East Slavic written legal code, the "Russkaya Pravda".

In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Kipchaks and the Pechenegs, caused a massive migration of Slavic populations to the safer, heavily forested regions of the north, particularly to the area known as Zalesye.
The age of feudalism and decentralization was marked by constant in-fighting between members of the Rurik Dynasty that ruled Kievan Rus' collectively. Kiev's dominance waned, to the benefit of Vladimir-Suzdal in the north-east, Novgorod Republic in the north-west and Galicia-Volhynia in the south-west.

Ultimately Kievan Rus' disintegrated, with the final blow being the Mongol invasion of 1237–40 that resulted in the destruction of Kiev and the death of about half the population of Rus'. The invading Mongol elite, together with their conquered Turkic subjects (Cumans, Kipchaks, Bulgars), became known as Tatars, forming the state of the Golden Horde, which pillaged the Russian principalities; the Mongols ruled the Cuman-Kipchak confederation and Volga Bulgaria (modern-day southern and central expanses of Russia) for over two centuries.

Galicia-Volhynia was eventually assimilated by the Kingdom of Poland, while the Mongol-dominated Vladimir-Suzdal and Novgorod Republic, two regions on the periphery of Kiev, established the basis for the modern Russian nation. The Novgorod together with Pskov retained some degree of autonomy during the time of the Mongol yoke and were largely spared the atrocities that affected the rest of the country. Led by Prince Alexander Nevsky, Novgorodians repelled the invading Swedes in the Battle of the Neva in 1240, as well as the Germanic crusaders in the Battle of the Ice in 1242, breaking their attempts to colonize the Northern Rus'.

The most powerful state to eventually arise after the destruction of Kievan Rus' was the Grand Duchy of Moscow ("Muscovy" in the Western chronicles), initially a part of Vladimir-Suzdal. While still under the domain of the Mongol-Tatars and with their connivance, Moscow began to assert its influence in the Central Rus' in the early 14th century, gradually becoming the leading force in the process of the Rus' lands' reunification and expansion of Russia. Moscow's last rival, the Novgorod Republic, prospered as the chief fur trade center and the easternmost port of the Hanseatic League.

Times remained difficult, with frequent Mongol-Tatar raids. Agriculture suffered from the beginning of the Little Ice Age. As in the rest of Europe, plague was a frequent occurrence between 1350 and 1490. However, because of the lower population density and better hygiene—widespread practicing of banya, a wet steam bath—the death rate from plague was not as severe as in Western Europe, and population numbers recovered by 1500.

Led by Prince Dmitry Donskoy of Moscow and helped by the Russian Orthodox Church, the united army of Russian principalities inflicted a milestone defeat on the Mongol-Tatars in the Battle of Kulikovo in 1380. Moscow gradually absorbed the surrounding principalities, including formerly strong rivals such as Tver and Novgorod.

Ivan III ("the Great") finally threw off the control of the Golden Horde and consolidated the whole of Central and Northern Rus' under Moscow's dominion. He was also the first to take the title "Grand Duke of all the Russias". After the fall of Constantinople in 1453, Moscow claimed succession to the legacy of the Eastern Roman Empire. Ivan III married Sophia Palaiologina, the niece of the last Byzantine emperor Constantine XI, and made the Byzantine double-headed eagle his own, and eventually Russia's, coat-of-arms.

In development of the Third Rome ideas, the Grand Duke Ivan IV (the "Terrible") was officially crowned first "Tsar" ("Caesar") of Russia in 1547. The "Tsar" promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (Zemsky Sobor) and introduced local self-management into the rural regions.

During his long reign, Ivan the Terrible nearly doubled the already large Russian territory by annexing the three Tatar khanates (parts of the disintegrated Golden Horde): Kazan and Astrakhan along the Volga River, and the Siberian Khanate in southwestern Siberia. Thus, by the end of the 16th century Russia was transformed into a multiethnic, multidenominational and transcontinental state.

However, the Tsardom was weakened by the long and unsuccessful Livonian War against the coalition of Poland, Lithuania, and Sweden for access to the Baltic coast and sea trade. At the same time, the Tatars of the Crimean Khanate, the only remaining successor to the Golden Horde, continued to raid Southern Russia. In an effort to restore the Volga khanates, Crimeans and their Ottoman allies invaded central Russia and were even able to burn down parts of Moscow in 1571. But in the next year the large invading army was thoroughly defeated by Russians in the Battle of Molodi, forever eliminating the threat of an Ottoman–Crimean expansion into Russia. The slave raids of Crimeans, however, did not cease until the late 17th century though the construction of new fortification lines across Southern Russia, such as the Great Abatis Line, constantly narrowed the area accessible to incursions.

The death of Ivan's sons marked the end of the ancient Rurik Dynasty in 1598, and in combination with the famine of 1601–03 led to civil war, the rule of pretenders, and foreign intervention during the Time of Troubles in the early 17th century. The Polish–Lithuanian Commonwealth occupied parts of Russia, including Moscow. In 1612, the Poles were forced to retreat by the Russian volunteer corps, led by two national heroes, merchant Kuzma Minin and Prince Dmitry Pozharsky. The Romanov Dynasty acceded to the throne in 1613 by the decision of Zemsky Sobor, and the country started its gradual recovery from the crisis.

Russia continued its territorial growth through the 17th century, which was the age of Cossacks. Cossacks were warriors organized into military communities, resembling pirates and pioneers of the New World. In 1648, the peasants of Ukraine joined the Zaporozhian Cossacks in rebellion against Poland-Lithuania during the Khmelnytsky Uprising in reaction to the social and religious oppression they had been suffering under Polish rule. In 1654, the Ukrainian leader, Bohdan Khmelnytsky, offered to place Ukraine under the protection of the Russian Tsar, Aleksey I. Aleksey's acceptance of this offer led to another Russo-Polish War. Finally, Ukraine was split along the Dnieper River, leaving the western part, right-bank Ukraine, under Polish rule and the eastern part (Left-bank Ukraine and Kiev) under Russian rule. Later, in 1670–71, the Don Cossacks led by Stenka Razin initiated a major uprising in the Volga Region, but the Tsar's troops were successful in defeating the rebels.

In the east, the rapid Russian exploration and colonisation of the huge territories of Siberia was led mostly by Cossacks hunting for valuable furs and ivory. Russian explorers pushed eastward primarily along the Siberian River Routes, and by the mid-17th century there were Russian settlements in Eastern Siberia, on the Chukchi Peninsula, along the Amur River, and on the Pacific coast. In 1648, the Bering Strait between Asia and North America was passed for the first time by Fedot Popov and Semyon Dezhnyov.

Under Peter the Great, Russia was proclaimed an Empire in 1721 and became recognized as a world power. Ruling from 1682 to 1725, Peter defeated Sweden in the Great Northern War, forcing it to cede West Karelia and Ingria (two regions lost by Russia in the Time of Troubles), as well as Estland and Livland, securing Russia's access to the sea and sea trade. On the Baltic Sea, Peter founded a new capital called Saint Petersburg, later known as Russia's "window to Europe". Peter the Great's reforms brought considerable Western European cultural influences to Russia.

The reign of Peter I's daughter Elizabeth in 1741–62 saw Russia's participation in the Seven Years' War (1756–63). During this conflict Russia annexed East Prussia for a while and even took Berlin. However, upon Elizabeth's death, all these conquests were returned to the Kingdom of Prussia by pro-Prussian Peter III of Russia.

Catherine II ("the Great"), who ruled in 1762–96, presided over the Age of Russian Enlightenment. She extended Russian political control over the Polish-Lithuanian Commonwealth and incorporated most of its territories into Russia during the Partitions of Poland, pushing the Russian frontier westward into Central Europe. In the south, after successful Russo-Turkish Wars against Ottoman Turkey, Catherine advanced Russia's boundary to the Black Sea, defeating the Crimean Khanate. As a result of victories over Qajar Iran through the Russo-Persian Wars, by the first half of the 19th century Russia also made significant territorial gains in Transcaucasia and the North Caucasus, forcing the former to irrevocably cede what is nowadays Georgia, Dagestan, Azerbaijan and Armenia to Russia. This continued with Alexander I's (1801–25) wresting of Finland from the weakened kingdom of Sweden in 1809 and of Bessarabia from the Ottomans in 1812. At the same time, Russians colonized Alaska and even founded settlements in California, such as Fort Ross.
In 1803–1806, the first Russian circumnavigation was made, later followed by other notable Russian sea exploration voyages. In 1820, a Russian expedition discovered the continent of Antarctica.

In alliances with various European countries, Russia fought against Napoleon's France. The French invasion of Russia at the height of Napoleon's power in 1812 reached Moscow, but eventually failed miserably as the obstinate resistance in combination with the bitterly cold Russian winter led to a disastrous defeat of invaders, in which more than 95% of the pan-European Grande Armée perished. Led by Mikhail Kutuzov and Barclay de Tolly, the Russian army ousted Napoleon from the country and drove through Europe in the war of the Sixth Coalition, finally entering Paris. Alexander I headed Russia's delegation at the Congress of Vienna that defined the map of post-Napoleonic Europe.

The officers of the Napoleonic Wars brought ideas of liberalism back to Russia with them and attempted to curtail the tsar's powers during the abortive Decembrist revolt of 1825. At the end of the conservative reign of Nicolas I (1825–55), a zenith period of Russia's power and influence in Europe was disrupted by defeat in the Crimean War. Between 1847 and 1851, about one million people died of Asiatic cholera.

Nicholas's successor Alexander II (1855–81) enacted significant changes in the country, including the emancipation reform of 1861. These "Great Reforms" spurred industrialization and modernized the Russian army, which had successfully liberated Bulgaria from Ottoman rule in the 1877–78 Russo-Turkish War.

The late 19th century saw the rise of various socialist movements in Russia. Alexander II was killed in 1881 by revolutionary terrorists, and the reign of his son
Alexander III (1881–94) was less liberal but more peaceful. The last Russian Emperor, Nicholas II (1894–1917), was unable to prevent the events of the Russian Revolution of 1905, triggered by the unsuccessful Russo-Japanese War and the demonstration incident known as Bloody Sunday. The uprising was put down, but the government was forced to concede major reforms (Russian Constitution of 1906), including granting the freedoms of speech and assembly, the legalization of political parties, and the creation of an elected legislative body, the State Duma of the Russian Empire. The Stolypin agrarian reform led to a massive peasant migration and settlement into Siberia. More than four million settlers arrived in that region between 1906 and 1914.

In 1914, Russia entered World War I in response to Austria-Hungary's declaration of war on Russia's ally Serbia, and fought across multiple fronts while isolated from its Triple Entente allies. In 1916, the Brusilov Offensive of the Russian Army almost completely destroyed the military of Austria-Hungary. However, the already-existing public distrust of the regime was deepened by the rising costs of war, high casualties, and rumors of corruption and treason. All this formed the climate for the Russian Revolution of 1917, carried out in two major acts.

The February Revolution forced Nicholas II to abdicate; he and his family were imprisoned and later executed in Yekaterinburg during the Russian Civil War. The monarchy was replaced by a shaky coalition of political parties that declared itself the Provisional Government. On 1 September (14), 1917, upon a decree of the Provisional Government, the Russian Republic was proclaimed. On 6 January (19), 1918, the Russian Constituent Assembly declared Russia a democratic federal republic (thus ratifying the Provisional Government's decision). The next day the Constituent Assembly was dissolved by the All-Russian Central Executive Committee.
An alternative socialist establishment co-existed, the Petrograd Soviet, wielding power through the democratically elected councils of workers and peasants, called "Soviets". The rule of the new authorities only aggravated the crisis in the country, instead of resolving it. Eventually, the October Revolution, led by Bolshevik leader Vladimir Lenin, overthrew the Provisional Government and gave full governing power to the Soviets, leading to the creation of the world's first socialist state.

Following the October Revolution, a civil war broke out between the anti-Communist White movement and the new Soviet regime with its Red Army. Bolshevist Russia lost its Ukrainian, Polish, Baltic, and Finnish territories by signing the Treaty of Brest-Litovsk that concluded hostilities with the Central Powers of World War I. The Allied powers launched an unsuccessful military intervention in support of anti-Communist forces. In the meantime both the Bolsheviks and White movement carried out campaigns of deportations and executions against each other, known respectively as the Red Terror and White Terror. By the end of the civil war, Russia's economy and infrastructure were heavily damaged. There were an estimated 7–12 million casualties during the war, mostly civilians. Millions became White émigrés, and the Russian famine of 1921–22 claimed up to five million victims.

The Russian Soviet Federative Socialist Republic (called "Russian Socialist Federative Soviet Republic" at the time), together with the Ukrainian, Byelorussian, and Transcaucasian Soviet Socialist Republics, formed the Union of Soviet Socialist Republics (USSR), or Soviet Union, on 30 December 1922. Out of the 15 republics that would make up the USSR, the largest in size and over half of the total USSR population was the Russian SFSR, which came to dominate the union for its entire 69-year history.

Following Lenin's death in 1924, a troika was designated to govern the Soviet Union. However, Joseph Stalin, an elected General Secretary of the Communist Party, managed to suppress all opposition groups within the party and consolidate power in his hands. Leon Trotsky, the main proponent of world revolution, was exiled from the Soviet Union in 1929, and Stalin's idea of Socialism in One Country became the primary line. The continued internal struggle in the Bolshevik party culminated in the Great Purge, a period of mass repressions in 1937–38, during which hundreds of thousands of people were executed, including original party members and military leaders accused of coup d'état plots.

Under Stalin's leadership, the government launched a command economy, industrialization of the largely rural country, and collectivization of its agriculture. During this period of rapid economic and social change, millions of people were sent to penal labor camps, including many political convicts for their opposition to Stalin's rule; millions were deported and exiled to remote areas of the Soviet Union. The transitional disorganisation of the country's agriculture, combined with the harsh state policies and a drought, led to the Soviet famine of 1932–1933, which killed between 2 and 3 million people in the Russian SFSR. The Soviet Union made the costly transformation from a largely agrarian economy to a major industrial powerhouse in a short span of time.

Under the doctrine of state atheism in the Soviet Union, there was a "government-sponsored program of forced conversion to atheism" conducted by Communists. The communist regime targeted religions based on State interests, and while most organized religions were never outlawed, religious property was confiscated, believers were harassed, and religion was ridiculed while atheism was propagated in schools. In 1925 the government founded the League of Militant Atheists to intensify the persecution. Accordingly, although personal expressions of religious faith were not explicitly banned, a strong sense of social stigma was imposed on them by the official structures and mass media and it was generally considered unacceptable for members of certain professions (teachers, state bureaucrats, soldiers) to be openly religious. As for the Russian Orthodox Church, Soviet authorities sought to control it and, in times of national crisis, to exploit it for the regime's own purposes; but their ultimate goal was to eliminate it. During the first five years of Soviet power, the Bolsheviks executed 28 Russian Orthodox bishops and over 1,200 Russian Orthodox priests. Many others were imprisoned or exiled. Believers were harassed and persecuted. Most seminaries were closed, and the publication of most religious material was prohibited. By 1941 only 500 churches remained open out of about 54,000 in existence prior to World War I.

The Appeasement policy of Great Britain and France towards Adolf Hitler's annexation of Austria and Czechoslovakia did not stem an increase in the power of Nazi Germany. Around the same time, the Third Reich allied with the Empire of Japan, a rival of the USSR in the Far East and an open enemy of the USSR in the Soviet–Japanese Border Wars in 1938–39.

In August 1939, the Soviet government decided to improve relations with Germany by concluding the Molotov–Ribbentrop Pact, pledging non-aggression between the two countries and dividing Eastern Europe into their respective spheres of influence. When Germany launched the Invasion of Poland, the Soviets followed weeks later with their own invasion of the country, claiming the eastern half of Poland while avoiding war with the Allied Powers. As the other European powers were busy fighting in World War II, the USSR was able to build up its military and occupy the Hertza region and Northern Bukovina as a result of the Winter War, the occupation of the Baltic states and Soviet occupation of Bessarabia and Northern Bukovina.

On 22 June 1941, Nazi Germany broke the non-aggression treaty and invaded the Soviet Union with the largest and most powerful invasion force in human history, opening the largest theater of World War II. The Nazi Hunger Plan foresaw the "extinction of industry as well as a great part of the population". Nearly 3 million Soviet POWs in German captivity were murdered in just eight months of 1941–42. Although the German army had considerable early success, their attack was halted in the Battle of Moscow. Subsequently, the Germans were dealt major defeats first at the Battle of Stalingrad in the winter of 1942–43, and then in the Battle of Kursk in the summer of 1943. Another German failure was the Siege of Leningrad, in which the city was fully blockaded on land between 1941 and 1944 by German and Finnish forces, and suffered starvation and more than a million deaths, but never surrendered. Under Stalin's administration and the leadership of such commanders as Georgy Zhukov and Konstantin Rokossovsky, Soviet forces took Eastern Europe in 1944–45 and captured Berlin in May 1945. In August 1945 the Soviet Army ousted the Japanese from China's Manchukuo and North Korea, contributing to the allied victory over Japan.

The 1941–45 period of World War II is known in Russia as the "Great Patriotic War". The Soviet Union together with the United States, the United Kingdom and China were considered as the Big Four of Allied powers in World War II and later became the Four Policemen which was the foundation of the United Nations Security Council. During this war, which included many of the most lethal battle operations in human history, Soviet civilian and military death were about 27 million, accounting for about a third of all World War II casualties. The full demographic loss to the Soviet peoples was even greater. The Soviet economy and infrastructure suffered massive devastation which caused the Soviet famine of 1946–47, but the Soviet Union emerged as an acknowledged military superpower on the continent.

The Soviet rear was also badly damaged by the German invasion. Luftwaffe bombed the cities of the Soviet Union from the air. Gorky suffered the most from the bombing. This city was the main industrial center of the USSR and was located near the Moscow Defence Zone. The bombing of the Volga capital destroyed the largest automobile plant GAZ. This plant supplied tanks for the front. Whole residential areas and other large factories of the city were destroyed. From 1941 to 1943, German pilots bombed different areas of the city. This bombardment is comparable to the London Blitz. Some damage remained until this time.

After the war, Eastern and Central Europe including East Germany and part of Austria was occupied by Red Army according to the Potsdam Conference. Dependent socialist governments were installed in the Eastern Bloc satellite states. Becoming the world's second nuclear weapons power, the USSR established the Warsaw Pact alliance and entered into a struggle for global dominance, known as the Cold War, with the United States and NATO. The Soviet Union supported revolutionary movements across the world, including the newly formed People's Republic of China, the Democratic People's Republic of Korea and, later on, the Republic of Cuba. Significant amounts of Soviet resources were allocated in aid to the other socialist states.

After Stalin's death and a short period of collective rule, the new leader Nikita Khrushchev denounced the cult of personality of Stalin and launched the policy of de-Stalinization. The penal labor system was reformed and many prisoners were released and rehabilitated (many of them posthumously). The general easement of repressive policies became known later as the Khrushchev Thaw. At the same time, tensions with the United States heightened when the two rivals clashed over the deployment of the United States Jupiter missiles in Turkey and Soviet missiles in Cuba.

In 1957, the Soviet Union launched the world's first artificial satellite, "Sputnik 1", thus starting the Space Age. Russia's cosmonaut Yuri Gagarin became the first human to orbit the Earth, aboard the "Vostok 1" manned spacecraft on 12 April 1961.

Following the ousting of Khrushchev in 1964, another period of collective rule ensued, until Leonid Brezhnev became the leader. The era of the 1970s and the early 1980s was later designated as the Era of Stagnation, a period when economic growth slowed and social policies became static. The 1965 Kosygin reform aimed for partial decentralization of the Soviet economy and shifted the emphasis from heavy industry and weapons to light industry and consumer goods but was stifled by the conservative Communist leadership.

In 1979, after a Communist-led revolution in Afghanistan, Soviet forces entered that country. The occupation drained economic resources and dragged on without achieving meaningful political results. Ultimately, the Soviet Army was withdrawn from Afghanistan in 1989 due to international opposition, persistent anti-Soviet guerrilla warfare, and a lack of support by Soviet citizens.
From 1985 onwards, the last Soviet leader Mikhail Gorbachev, who sought to enact liberal reforms in the Soviet system, introduced the policies of "glasnost" (openness) and "perestroika" (restructuring) in an attempt to end the period of economic stagnation and to democratize the government. This, however, led to the rise of strong nationalist and separatist movements. Prior to 1991, the Soviet economy was the second largest in the world, but during its last years it was afflicted by shortages of goods in grocery stores, huge budget deficits, and explosive growth in the money supply leading to inflation.

By 1991, economic and political turmoil began to boil over, as the Baltic states chose to secede from the Soviet Union. On 17 March, a referendum was held, in which the vast majority of participating citizens voted in favour of changing the Soviet Union into a renewed federation. In August 1991, a coup d'état attempt by members of Gorbachev's government, directed against Gorbachev and aimed at preserving the Soviet Union, instead led to the end of the Communist Party of the Soviet Union. On 25 December 1991, the USSR was dissolved into 15 post-Soviet states.

In June 1991, Boris Yeltsin became the first directly elected President in Russian history when he was elected President of the Russian Soviet Federative Socialist Republic, which became the independent Russian Federation in December of that year. The economic and political collapse of USSR led to a deep and prolonged depression, characterized by a 50% decline in both GDP and industrial output between 1990 and 1995, although some of the recorded declines may have been a result of an upward bias in Soviet-era economic data. During and after the disintegration of the Soviet Union, wide-ranging reforms including privatization and market and trade liberalization were undertaken, including radical changes along the lines of "shock therapy" as recommended by the United States and the International Monetary Fund.

The privatization largely shifted control of enterprises from state agencies to individuals with inside connections in the government. Many of the newly rich moved billions in cash and assets outside of the country in an enormous capital flight. The depression of the economy led to the collapse of social services; the birth rate plummeted while the death rate skyrocketed. Millions plunged into poverty, from a level of 1.5% in the late Soviet era to 39–49% by mid-1993. The 1990s saw extreme corruption and lawlessness, the rise of criminal gangs and violent crime.

The 1990s were plagued by armed conflicts in the North Caucasus, both local ethnic skirmishes and separatist Islamist insurrections. From the time Chechen separatists declared independence in the early 1990s, an intermittent guerrilla war has been fought between the rebel groups and the Russian military. Terrorist attacks against civilians carried out by separatists, most notably the Moscow theater hostage crisis and Beslan school siege, caused hundreds of deaths and drew worldwide attention.

Russia took up the responsibility for settling the USSR's external debts, even though its population made up just half of the population of the USSR at the time of its dissolution. In 1992, most consumer price controls were eliminated, causing extreme inflation and significantly devaluing the Ruble. With a devalued Ruble, the Russian government struggled to pay back its debts to internal debtors, as well as international institutions like the International Monetary Fund. Despite significant attempts at economic restructuring, Russia's debt outpaced GDP growth. High budget deficits coupled with increasing capital flight and inability to pay back debts caused the 1998 Russian financial crisis and resulted in a further GDP decline.

On 31 December 1999, President Yeltsin unexpectedly resigned, handing the post to the recently appointed Prime Minister, Vladimir Putin, who then won the 2000 presidential election. Putin suppressed the Chechen insurgency although sporadic violence still occurs throughout the Northern Caucasus. High oil prices and the initially weak currency followed by increasing domestic demand, consumption, and investments helped the economy grow at an average of 7% per year from 1998 to 2008, improving the standard of living and increasing Russia's influence on the world stage. Following the world economic crisis of 2008 and a subsequent drop in oil prices, Russia's economy stagnated and poverty again started to rise until 2017 when, after the prolonged recession, Russia's economy began to grow again, supported by stronger global growth, higher oil prices, and solid macro fundamentals. While many reforms made during the Putin presidency have been generally criticized by Western nations as undemocratic, Putin's leadership over the return of order, stability, and progress has won him widespread admiration in Russia.

On 2 March 2008, Dmitry Medvedev was elected President of Russia while Putin became Prime Minister. Putin returned to the presidency following the 2012 presidential elections, and Medvedev was appointed Prime Minister. This quick succession in leadership change was coined "tandemocracy" by outside media. Some critics claimed that the leadership change was superficial, and that Putin remained as the decision making force in the Russian government. Within the context of the ongoing Russia–Ukraine gas dispute in early January 2009, Nikolai Petrov, an analyst with the Carnegie Moscow Center said: "What we see right now is the dominant role of Putin. We see him as a real head of state. ... This is not surprising. We are still living in Putin's Russia." Some Russian political analysts and commentators viewed the political power as truly tandem between Medvedev and Putin. Prior to the 2008 election, political scientists Gleb Pavlovsky and Stanislav Belkovsky discussed the future configuration of power. According to Mr. Pavlovsky, people would be very suited with the option of the union of Putin and Medvedev "similar to the two Consuls of Rome". Belkovsky called Medvedev "President of a dream", referring to the early 1990s when people ostensibly dreamed of the time they "would live without the stranglehold of ubiquitous ideology, and a common person would become the head of the state".

In 2014, after President Viktor Yanukovych of Ukraine fled as a result of a revolution, Putin requested and received authorization from the Russian Parliament to deploy Russian troops to Ukraine. Following a Crimean referendum in which separation was favored by a large majority of voters, the Russian leadership announced the accession of Crimea into the Russian Federation, though this and the referendum that preceded it were not accepted internationally. On 27 March the United Nations General Assembly voted in favor of a non-binding resolution opposing the Russian annexation of Crimea by a vote of 100 member states in favor, 11 against and 58 abstentions. The annexation of Crimea lead to sanctions by Western countries, in which the Russian government responded with its own against a number of countries.

In September 2015, Russia started military intervention in the Syrian Civil War, consisting of air strikes against militant groups of the Islamic State, al-Nusra Front (al-Qaeda in the Levant), and the Army of Conquest.

According to the Constitution of Russia, the country is an asymmetric federation and semi-presidential republic, wherein the President is the head of state and the Prime Minister is the head of government. The Russian Federation is fundamentally structured as a multi-party representative democracy, with the federal government composed of three branches:

The president is elected by popular vote for a six-year term (eligible for a second term, but not for a third consecutive term). Ministries of the government are composed of the Premier and his deputies, ministers, and selected other individuals; all are appointed by the President on the recommendation of the Prime Minister (whereas the appointment of the latter requires the consent of the State Duma). Leading political parties in Russia include United Russia, the Communist Party, the Liberal Democratic Party, and A Just Russia. In 2017, Russia was ranked as 135th of 167 countries in the Democracy Index, compiled by The Economist Intelligence Unit, while the World Justice Project, , ranked Russia 80th of 99 countries surveyed in terms of rule of law.

The Russian Federation is recognized in international law as a successor state of the former Soviet Union. Russia continues to implement the international commitments of the USSR, and has assumed the USSR's permanent seat in the UN Security Council, membership in other international organisations, the rights and obligations under international treaties, and property and debts. Russia has a multifaceted foreign policy. , it maintains diplomatic relations with 191 countries and has 144 embassies. The foreign policy is determined by the President and implemented by the Ministry of Foreign Affairs of Russia.

Although it is the successor state to a former superpower, Russia is commonly accepted to be a great power. Russia is one of five permanent members of the UN Security Council. The country participates in the Quartet on the Middle East and the Six-party talks with North Korea. Russia is a member of the Council of Europe, OSCE, and APEC. Russia usually takes a leading role in regional organisations such as the CIS, EurAsEC, CSTO, and the SCO. Russia became the 39th member state of the Council of Europe in 1996. In 1998, Russia ratified the European Convention on Human Rights. The legal basis for EU relations with Russia is the Partnership and Cooperation Agreement, which came into force in 1997. The Agreement recalls the parties' shared respect for democracy and human rights, political and economic freedom and commitment to international peace and security. In May 2003, the EU and Russia agreed to reinforce their cooperation on the basis of common values and shared interests. Former President Vladimir Putin had advocated a strategic partnership with close integration in various dimensions including establishment of EU-Russia Common Spaces. From the dissolution of the Soviet Union, Russia has initially developed a friendlier relationship with the United States and NATO, however today, the trilateral relationship has significantly deteriorated due to several issues and conflicts between Russia and the Western countries. The NATO-Russia Council was established in 2002 to allow the United States, Russia and the 27 allies in NATO to work together as equal partners to pursue opportunities for joint collaboration.
Russia maintains strong and positive relations with other SCO and BRICS countries. In recent years, the country has significantly strengthened bilateral ties with the People's Republic of China by signing the Treaty of Friendship as well as building the Trans-Siberian oil pipeline and gas pipeline from Siberia to China, and has since formed a special relationship with China. India is the largest customer of Russian military equipment and the two countries share extensive defense and strategic relations.

An important aspect of Russia's relations with the West is the criticism of Russia's political system and human rights management (including LGBT rights, media freedom, and reports about killed journalists) by Western governments, the mass media and the leading democracy and human rights watchdogs. In particular, such organisations as Amnesty International and Human Rights Watch consider Russia to have not enough democratic attributes and to allow few political rights and civil liberties to its citizens. Freedom House, an international organisation funded by the United States, ranks Russia as "not free", citing "carefully engineered elections" and "absence" of debate. Russian authorities dismiss these claims and especially criticise Freedom House. The Russian Ministry of Foreign Affairs has called the 2006 "Freedom in the World" report "prefabricated", stating that the human rights issues have been turned into a political weapon in particular by the United States. The ministry also claims that such organisations as Freedom House and Human Rights Watch use the same scheme of voluntary extrapolation of "isolated facts that of course can be found in any country" into "dominant tendencies".

The Russian military is divided into the Ground Forces, Navy, and Air Force. There are also three independent arms of service: Strategic Missile Troops, Aerospace Defence Forces, and the Airborne Troops. , the military comprised over one million active duty personnel, the fifth largest in the world. Additionally, there are over 2.5 million reservists, with the total number of reserve troops possibly being as high as 20 million. It is mandatory for all male citizens aged 18–27 to be drafted for a year of service in Armed Forces.

Russia has the largest stockpile of nuclear weapons in the world, the second largest fleet of ballistic missile submarines, and the only modern strategic bomber force outside the United States. More than 90% of world's 14,000 nuclear weapons are owned by Russia and the United States. Russia's tank force is the largest in the world, while its surface navy and air force are among the largest.

The country has a large and fully indigenous arms industry, producing most of its own military equipment with only a few types of weapons imported. It has been one of the world's top supplier of arms since 2001, accounting for around 30% of worldwide weapons sales and exporting weapons to about 80 countries. The Stockholm International Peace Research Institute, SIPRI, found that Russia was the second biggest exporter of arms in 2010–14, increasing their exports by 37 per cent from the period 2005–2009. SIPRI estimated in 2020 that Russia is the third biggest exporters of arms, only behind the US and China. In 2010–14, Russia delivered weapons to 56 states and to rebel forces in eastern Ukraine.

The Russian government's official 2014 military budget is about 2.49 trillion rubles (approximately US$69.3 billion), the third largest in the world behind the US and China. The official budget is set to rise to 3.03 trillion rubles (approximately US$83.7 billion) in 2015, and 3.36 trillion rubles (approximately US$93.9 billion) in 2016. However, unofficial estimates put the budget significantly higher, for example the Stockholm International Peace Research Institute (SIPRI) 2013 Military Expenditure Database estimated Russia's military expenditure in 2012 at US$90.749 billion. This estimate is an increase of more than US$18 billion on SIPRI's estimate of the Russian military budget for 2011 (US$71.9 billion). , Russia's military budget is higher than any other European nation.

According to the Constitution, the country comprises eighty-five federal subjects, including the disputed Republic of Crimea and federal city of Sevastopol. In 1993, when the Constitution was adopted, there were eighty-nine federal subjects listed, but later some of them were merged. These subjects have equal representation—two delegates each—in the Federation Council. However, they differ in the degree of autonomy they enjoy.

Federal subjects are grouped into eight federal districts, each administered by an envoy appointed by the President of Russia. Unlike the federal subjects, the federal districts are not a subnational level of government, but are a level of administration of the federal government. Federal districts' envoys serve as liaisons between the federal subjects and the federal government and are primarily responsible for overseeing the compliance of the federal subjects with the federal laws.

Russia is the largest country in the world; its total area is . This makes it larger than the continents of Oceania, Europe and Antarctica. It lies between latitudes 41° and 82° N, and longitudes 19° E and 169° W.

Russia's territorial expansion was achieved largely in the late 16th century under the Cossack Yermak Timofeyevich during the reign of Ivan the Terrible, at a time when competing city-states in the western regions of Russia had banded together to form one country. Yermak mustered an army and pushed eastward where he conquered nearly all the lands once belonging to the Mongols, defeating their ruler, Khan Kuchum.

Russia has a wide natural resource base, including major deposits of timber, petroleum, natural gas, coal, ores and other mineral resources.

The two most widely separated points in Russia are about apart along a geodesic line. These points are: a long Vistula Spit the boundary with Poland separating the Gdańsk Bay from the Vistula Lagoon and the most southeastern point of the Kuril Islands. The points which are farthest separated in longitude are apart along a geodesic line. These points are: in the west, the same spit on the boundary with Poland, and in the east, the Big Diomede Island. The Russian Federation spans 11 time zones.

Most of Russia consists of vast stretches of plains that are predominantly steppe to the south and heavily forested to the north, with tundra along the northern coast. Russia possesses 10% of the world's arable land. Mountain ranges are found along the southern borders, such as the Caucasus (containing Mount Elbrus, which at is the highest point in both Russia and Europe) and the Altai (containing Mount Belukha, which at the is the highest point of Siberia outside of the Russian Far East); and in the eastern parts, such as the Verkhoyansk Range or the volcanoes of Kamchatka Peninsula (containing Klyuchevskaya Sopka, which at the is the highest active volcano in Eurasia as well as the highest point of Asian Russia). The Ural Mountains, rich in mineral resources, form a north-south range that divides Europe and Asia.

Russia has an extensive coastline of over along the Arctic and Pacific Oceans, as well as along the Baltic Sea, Sea of Azov, Black Sea and Caspian Sea. The Barents Sea, White Sea, Kara Sea, Laptev Sea, East Siberian Sea, Chukchi Sea, Bering Sea, Sea of Okhotsk, and the Sea of Japan are linked to Russia via the Arctic and Pacific. Russia's major islands and archipelagos include Novaya Zemlya, the Franz Josef Land, the Severnaya Zemlya, the New Siberian Islands, Wrangel Island, the Kuril Islands, and Sakhalin. The Diomede Islands (one controlled by Russia, the other by the United States) are just apart, and Kunashir Island is about from Hokkaido, Japan.

Russia has thousands of rivers and inland bodies of water, providing it with one of the world's largest surface water resources. Its lakes contain approximately one-quarter of the world's liquid fresh water. The largest and most prominent of Russia's bodies of fresh water is Lake Baikal, the world's deepest, purest, oldest and most capacious fresh water lake. Baikal alone contains over one-fifth of the world's fresh surface water. Other major lakes include Ladoga and Onega, two of the largest lakes in Europe. Russia is second only to Brazil in volume of the total renewable water resources. Of the country's 100,000 rivers, the Volga is the most famous, not only because it is the longest river in Europe, but also because of its major role in Russian history. The Siberian rivers Ob, Yenisey, Lena and Amur are among the longest rivers in the world.

The enormous size of Russia and the remoteness of many areas from the sea result in the dominance of the humid continental climate, which is prevalent in all parts of the country except for the tundra and the extreme southwest. Mountains in the south obstruct the flow of warm air masses from the Indian Ocean, while the plain of the west and north makes the country open to Arctic and Atlantic influences.

Most of Northern European Russia and Siberia has a subarctic climate, with extremely severe winters in the inner regions of Northeast Siberia (mostly the Sakha Republic, where the Northern Pole of Cold is located with the record low temperature of ), and more moderate winters elsewhere. Both the strip of land along the shore of the Arctic Ocean and the Russian Arctic islands have a polar climate.

The coastal part of Krasnodar Krai on the Black Sea, most notably in Sochi, possesses a humid subtropical climate with mild and wet winters. In many regions of East Siberia and the Far East, winter is dry compared to summer; other parts of the country experience more even precipitation across seasons. Winter precipitation in most parts of the country usually falls as snow. The region along the Lower Volga and Caspian Sea coast, as well as some areas of southernmost Siberia, possesses a semi-arid climate.
Throughout much of the territory there are only two distinct seasons—winter and summer—as spring and autumn are usually brief periods of change between extremely low and extremely high temperatures. The coldest month is January (February on the coastline); the warmest is usually July. Great ranges of temperature are typical. In winter, temperatures get colder both from south to north and from west to east. Summers can be quite hot, even in Siberia. The continental interiors are the driest areas.

From north to south the East European Plain, also known as Russian Plain, is clad sequentially in Arctic tundra, coniferous forest (taiga), mixed and broad-leaf forests, grassland (steppe), and semi-desert (fringing the Caspian Sea), as the changes in vegetation reflect the changes in climate. Siberia supports a similar sequence but is largely taiga. Russia has the world's largest forest reserves, known as "the lungs of Europe", second only to the Amazon Rainforest in the amount of carbon dioxide it absorbs.

There are 266 mammal species and 780 bird species in Russia. A total of 415 animal species have been included in the Red Data Book of the Russian Federation as of 1997 and are now protected. There are 28 UNESCO World Heritage Sites in Russia, 40 UNESCO biosphere reserves, 41 national parks and 101 nature reserves. Russia still has many ecosystems which are still untouched by man— mainly in the northern areas taiga and in subarctic tundra of Siberia. Over time Russia has been having improvement and application of environmental legislation, development and implementation of various federal and regional strategies and programmes,and study, inventory and protection of rare and endangered plants, animals, and other organisms, and including them in the Red Data Book of the Russian Federation.

Russia has an upper-middle income mixed economy<ref name="https://datahelpdesk.worldbank.org">, "World Bank"</ref> with enormous natural resources, particularly oil and natural gas. It has the 11th largest economy in the world by nominal GDP and the 6th largest by purchasing power parity (PPP). Since the turn of the 21st century, higher domestic consumption and greater political stability have bolstered economic growth in Russia. The country ended 2008 with its ninth straight year of growth, but growth has slowed with the decline in the price of oil and gas. Real GDP per capita, PPP (current international) was 19,840 in 2010. Growth was primarily driven by non-traded services and goods for the domestic market, as opposed to oil or mineral extraction and exports. The average nominal salary in Russia was $967 per month in early 2013, up from $80 in 2000. In May 2016 the average nominal monthly wages fell below $450 per month, and tax on the income of individuals is payable at the rate of 13% on most incomes. Approximately 19.2 million of Russians lived below the national poverty line in 2016, significantly up from 16.1 million in 2015. Unemployment in Russia was 5.4% in 2014, down from about 12.4% in 1999. Officially, about 20–25% of the Russian population is categorized as middle class; however some economists and sociologists think this figure is inflated and the real fraction is about 7%. After the United States, the European Union and other countries imposed economic sanctions after the annexation of Crimea and a collapse in oil prices, the proportion of middle-class could decrease drastically. The economic development of the country has been uneven geographically with the Moscow region contributing a very large share of the country's GDP.
Oil, natural gas, metals, and timber account for more than 80% of Russian exports abroad. Since 2003, the exports of natural resources started decreasing in economic importance as the internal market strengthened considerably. the oil-and-gas sector accounted for 16% of GDP, 52% of federal budget revenues and over 80% of total exports. Oil export earnings allowed Russia to increase its foreign reserves from $12 billion in 1999 to $597.3 billion on 1 August 2008. , foreign reserves in Russia fell to US$332 Billion. The macroeconomic policy under Finance Minister Alexei Kudrin was prudent and sound, with excess income being stored in the Stabilization Fund of Russia. In 2006, Russia repaid most of its formerly massive debts, leaving it with one of the lowest foreign debts among major economies. The Stabilization Fund helped Russia to come out of the global financial crisis in a much better state than many experts had expected.

A simpler, more streamlined tax code adopted in 2001 reduced the tax burden on people and dramatically increased state revenue. Russia has a flat tax rate of 13%. This ranks it as the country with the second most attractive personal tax system for single managers in the world after the United Arab Emirates. According to Bloomberg, Russia is considered well ahead of most other resource-rich countries in its economic development, with a long tradition of education, science, and industry. The country has a higher proportion of higher education graduates than any other country in Eurasia.

Inequality of household income and wealth has also been noted, with Credit Suisse finding Russian wealth distribution so much more extreme than other countries studied it "deserves to be placed in a separate category."
Another problem is modernisation of infrastructure, ageing and inadequate after years of being neglected in the 1990s; the government has said $1 trillion will be invested in development of infrastructure by 2020. In December 2011, Russia was approved as a member of the World Trade Organisation after 18 years of dialogue, allowing it a greater access to overseas markets. Some analysts estimate that WTO membership could bring the Russian economy a bounce of up to 3% annually. Russia ranks as the second-most corrupt country in Europe (after Ukraine), according to the Corruption Perceptions Index. The Norwegian-Russian Chamber of Commerce also states that "[c]orruption is one of the biggest problems both Russian and international companies have to deal with." Corruption in Russia is perceived as a significant problem impacting all aspects of life, including public administration, law enforcement, healthcare and education. The phenomenon of corruption is strongly established in the historical model of public governance in Russia and attributed to general weakness of rule of law in Russia. According to Transparency International's Corruption Perceptions Index, Russia ranked 138th with a score of 28 out of 100 in 2018.

The Russian central bank announced plans in 2013 to free float the Russian ruble in 2015. According to a stress test conducted by the central bank Russian financial system would be able to handle a currency decline of 25%–30% without major central bank interference. However, the Russian economy began stagnating in late 2013 and in combination with the War in Donbass is in danger of entering stagflation, slow growth and high inflation. The recent decline in the Russian ruble has increased the costs for Russian companies to make interest payments on debt issued in U.S. dollar or other foreign currencies that have strengthened against the ruble; thus it costs Russian companies more of their ruble-denominated revenue to repay their debt holders in dollars or other foreign currencies. , the ruble was devalued more than 50 percent since July 2014. Moreover, after bringing inflation down to 3.6% in 2012, the lowest rate since gaining independence from the Soviet Union, inflation in Russia jumped to nearly 7.5% in 2014, causing the central bank to increase its lending rate to 8% from 5.5% in 2013. In an October 2014 article in "Bloomberg Business Week", it was reported that Russia had significantly started shifting its economy towards China in response to increasing financial tensions following its annexation of Crimea and subsequent Western economic sanctions.

In recent years, Russia has frequently been described in the media as an energy superpower. The country has the world's largest natural gas reserves, the 8th largest oil reserves, and the second largest coal reserves. Russia is the world's leading natural gas exporter and second largest natural gas producer, while also the largest oil exporter and the largest oil producer.

Russia is the third largest electricity producer in the world and the 5th largest renewable energy producer, the latter because of the well-developed hydroelectricity production in the country. Large cascades of hydropower plants are built in European Russia along big rivers like the Volga. The Asian part of Russia also features a number of major hydropower stations; however, the gigantic hydroelectric potential of Siberia and the Russian Far East largely remains unexploited.

Russia was the first country to develop civilian nuclear power and to construct the world's first nuclear power plant. Currently the country is the 4th largest nuclear energy producer, with all nuclear power in Russia being managed by Rosatom State Corporation. The sector is rapidly developing, with an aim of increasing the total share of nuclear energy from current 16.9% to 23% by 2020. The Russian government plans to allocate 127 billion rubles ($5.42 billion) to a federal program dedicated to the next generation of nuclear energy technology. About 1 trillion rubles ($42.7 billion) is to be allocated from the federal budget to nuclear power and industry development before 2015.

In May 2014 on a two-day trip to Shanghai, President Putin signed a deal on behalf of Gazprom for the Russian energy giant to supply China with 38 billion cubic meters of natural gas per year. Construction of a pipeline to facilitate the deal was agreed whereby Russia would contribute $55bn to the cost, and China $22bn, in what Putin described as "the world's biggest construction project for the next four years." The natural gas would begin to flow sometime between 2018 and 2020 and would continue for 30 years at an ultimate cost to China of $400bn.

Russia recorded a trade surplus of US$130.1 billion in 2017. Russia's Trade Balance recorded a surplus of US$19.7 billion in October 2018, compared with a surplus of US$10.1 billion in October 2017.

The European Union is Russia's largest trading partner and Russia is the EU's fourth largest trading partner. 75% of foreign direct investment (FDI) stocks in Russia come from the EU.

Reuters reported that U.S. companies "generated more than $90 billion in revenue from Russia in 2017." According to the AALEP, "there are almost 3,000 American companies in Russia, and the U.S. is also the leader in terms of foreign companies in Special Economic Zones, with 11 projects."

Russia's total area of cultivated land is estimated at , the fourth largest in the world. From 1999 to 2009, Russia's agriculture grew steadily, and the country turned from a grain importer to the third largest grain exporter after the EU and the United States. The production of meat has grown from 6,813,000 tonnes in 1999 to 9,331,000 tonnes in 2008, and continues to grow.

The 2014 devaluation of the rouble and imposition of sanctions spurred domestic production, and in 2016 Russia exceeded Soviet grain production levels, and became the world's largest exporter of wheat.

This restoration of agriculture was supported by a credit policy of the government, helping both individual farmers and large privatized corporate farms that once were Soviet kolkhozes and which still own the significant share of agricultural land. While large farms concentrate mainly on grain production and husbandry products, small private household plots produce most of the country's potatoes, vegetables and fruits.

Since Russia borders three oceans (the Atlantic, Arctic, and Pacific), Russian fishing fleets are a major world fish supplier. Russia captured 3,191,068 tons of fish in 2005. Both exports and imports of fish and sea products grew significantly in recent years, reaching $2,415 and $2,036 million, respectively, in 2008.

Sprawling from the Baltic Sea to the Pacific Ocean, Russia has more than a fifth of the world's forests, which makes it the largest forest country in the world. However, according to a 2012 study by the Food and Agriculture Organization of the United Nations and the Government of the Russian Federation, the considerable potential of Russian forests is underutilized and Russia's share of the global trade in forest products is less than four percent.

Railway transport in Russia is mostly under the control of the state-run Russian Railways monopoly. The company accounts for over 3.6% of Russia's GDP and handles 39% of the total freight traffic (including pipelines) and more than 42% of passenger traffic. The total length of common-used railway tracks exceeds , second only to the United States. Over of tracks are electrified, which is the largest number in the world, and additionally there are more than of industrial non-common carrier lines. Railways in Russia, unlike in the most of the world, use broad gauge of , with the exception of on Sakhalin island using narrow gauge of . The most renowned railway in Russia is Trans-Siberian ("Transsib"), spanning a record seven time zones and serving the longest single continuous services in the world, Moscow-Vladivostok (), Moscow–Pyongyang () and Kiev–Vladivostok ().

Much of Russia's inland waterways, which total, are made up of natural rivers or lakes. In the European part of the country the network of channels connects the basins of major rivers. Russia's capital, Moscow, is sometimes called "the port of the five seas", because of its waterway connections to the Baltic, White, Caspian, Azov and Black Seas.
Major sea ports of Russia include Rostov-on-Don on the Azov Sea, Novorossiysk on the Black Sea, Astrakhan and Makhachkala on the Caspian, Kaliningrad and St Petersburg on the Baltic, Arkhangelsk on the White Sea, Murmansk on the Barents Sea, Petropavlovsk-Kamchatsky and Vladivostok on the Pacific Ocean. In 2008 the country owned 1,448 merchant marine ships. The world's only fleet of nuclear-powered icebreakers advances the economic exploitation of the Arctic continental shelf of Russia and the development of sea trade through the Northern Sea Route between Europe and East Asia.

By total length of pipelines Russia is second only to the United States. Currently many new pipeline projects are being realized, including Nord Stream and South Stream natural gas pipelines to Europe, and the Eastern Siberia – Pacific Ocean oil pipeline (ESPO) to the Russian Far East and China.

Russia has 1,216 airports, the busiest being Sheremetyevo, Domodedovo, and Vnukovo in Moscow, and Pulkovo in St. Petersburg.

Typically, major Russian cities have well-developed systems of public transport, with the most common varieties of exploited vehicles being bus, trolleybus and tram. Seven Russian cities, namely Moscow, Saint Petersburg, Nizhny Novgorod, Novosibirsk, Samara, Yekaterinburg, and Kazan, have underground metros, while Volgograd features a metrotram. The total length of metros in Russia is . Moscow Metro and Saint Petersburg Metro are the oldest in Russia, opened in 1935 and 1955 respectively. These two are among the fastest and busiest metro systems in the world, and some of them are famous for rich decorations and unique designs of their stations, which is a common tradition in Russian metros and railways.

Science and technology in Russia blossomed since the Age of Enlightenment, when Peter the Great founded the Russian Academy of Sciences and Saint Petersburg State University, and polymath Mikhail Lomonosov established the Moscow State University, paving the way for a strong native tradition in learning and innovation. In the 19th and 20th centuries the country produced a large number of notable scientists and inventors.

The Russian physics school began with Lomonosov who proposed the law of conservation of matter preceding the energy conservation law. Russian discoveries and inventions in physics include the electric arc, electrodynamical Lenz's law, space groups of crystals, photoelectric cell, superfluidity, Cherenkov radiation, electron paramagnetic resonance, heterotransistors and 3D holography. Lasers and masers were co-invented by Nikolai Basov and Alexander Prokhorov, while the idea of tokamak for controlled nuclear fusion was introduced by Igor Tamm, Andrei Sakharov and Lev Artsimovich, leading eventually the modern international ITER project, where Russia is a party.

Since the time of Nikolay Lobachevsky (the "Copernicus of Geometry" who pioneered the non-Euclidean geometry) and a prominent tutor Pafnuty Chebyshev, the Russian mathematical school became one of the most influential in the world. Chebyshev's students included Aleksandr Lyapunov, who founded the modern stability theory, and Andrey Markov who invented the Markov chains. In the 20th century Soviet mathematicians, such as Andrey Kolmogorov, Israel Gelfand, and Sergey Sobolev, made major contributions to various areas of mathematics. Nine Soviet/Russian mathematicians were awarded with the Fields Medal, a most prestigious award in mathematics. Recently Grigori Perelman was offered the first ever Clay Millennium Prize Problems Award for his final proof of the Poincaré conjecture in 2002.

Russian chemist Dmitry Mendeleev invented the Periodic table, the main framework of modern chemistry. Aleksandr Butlerov was one of the creators of the theory of chemical structure, playing a central role in organic chemistry. Russian biologists include Dmitry Ivanovsky who discovered viruses, Ivan Pavlov who was the first to experiment with the classical conditioning, and Ilya Mechnikov who was a pioneer researcher of the immune system and probiotics.

Many Russian scientists and inventors were émigrés, like Igor Sikorsky, who built the first airliners and modern-type helicopters; Vladimir Zworykin, often called the father of television; chemist Ilya Prigogine, noted for his work on dissipative structures and complex systems; Nobel Prize-winning economists Simon Kuznets and Wassily Leontief; physicist Georgiy Gamov (an author of the Big Bang theory) and social scientist Pitirim Sorokin. Many foreigners worked in Russia for a long time, like Leonard Euler and Alfred Nobel.

Russian inventions include arc welding by Nikolay Benardos, further developed by Nikolay Slavyanov, Konstantin Khrenov and other Russian engineers. Gleb Kotelnikov invented the knapsack parachute, while Evgeniy Chertovsky introduced the pressure suit. Alexander Lodygin and Pavel Yablochkov were pioneers of electric lighting, and Mikhail Dolivo-Dobrovolsky introduced the first three-phase electric power systems, widely used today. Sergei Lebedev invented the first commercially viable and mass-produced type of synthetic rubber. The first ternary computer, "Setun", was developed by Nikolay Brusentsov.

In the 20th century a number of prominent Soviet aerospace engineers, inspired by the fundamental works of Nikolai Zhukovsky, Sergei Chaplygin and others, designed many hundreds of models of military and civilian aircraft and founded a number of "KBs" ("Construction Bureaus") that now constitute the bulk of Russian United Aircraft Corporation. Famous Russian aircraft include the civilian Tu-series, Su and MiG fighter aircraft, Ka and Mi-series helicopters; many Russian aircraft models are on the list of most produced aircraft in history.

Famous Russian battle tanks include T34, the most heavily produced tank design of World War II, and further tanks of T-series, including the most produced tank in history, T54/55. The AK47 and AK74 by Mikhail Kalashnikov constitute the most widely used type of assault rifle throughout the world—so much so that more AK-type rifles have been manufactured than all other assault rifles combined.

With all these achievements, however, since the late Soviet era Russia was lagging behind the West in a number of technologies, mostly those related to energy conservation and consumer goods production. The crisis of the 1990s led to the drastic reduction of the state support for science and a brain drain migration from Russia.

In the 2000s, on the wave of a new economic boom, the situation in the Russian science and technology has improved, and the government launched a campaign aimed into modernisation and innovation. Russian President Dmitry Medvedev formulated top priorities for the country's technological development:

Currently Russia has completed the GLONASS satellite navigation system. The country is developing its own fifth-generation jet fighter and constructing the first serial mobile nuclear plant in the world.

Russian achievements in the field of space technology and space exploration are traced back to Konstantin Tsiolkovsky, the father of theoretical astronautics. His works had inspired leading Soviet rocket engineers, such as Sergey Korolyov, Valentin Glushko, and many others who contributed to the success of the Soviet space program in the early stages of the Space Race and beyond.

In 1957 the first Earth-orbiting artificial satellite, "Sputnik 1", was launched; in 1961 the first human trip into space was successfully made by Yuri Gagarin. Many other Soviet and Russian space exploration records ensued, including the first spacewalk performed by Alexei Leonov, Luna 9 was the first spacecraft to land on the Moon, Zond 5 brought the first Earthlings (two tortoises and other life forms) to circumnavigate the Moon, Venera 7 was the first to land on another planet (Venus), Mars 3 then the first to land on Mars, the first space exploration rover "Lunokhod 1", and the first space station "Salyut 1" and "Mir".

After the collapse of the Soviet Union, some government-funded space exploration programs, including the Buran space shuttle program, were cancelled or delayed, while participation of the Russian space industry in commercial activities and international cooperation intensified.
Nowadays Russia is the largest satellite launcher. After the United States Space Shuttle program ended in 2011, Soyuz rockets became the only provider of transport for astronauts at the International Space Station.

Luna-Glob is a Russian Moon exploration programme, with first planned mission launch in 2021. Roscosmos is also developing the Orel spacecraft, to replace the aging Soyuz, it could also conduct mission to lunar orbit as early as 2026. In February 2019, it was announced that Russia is intending to conduct its first crewed mission to land on the Moon in 2031.

In Russia, approximately 70 per cent of drinking water comes from surface water and 30 per cent from groundwater. In 2004, water supply systems had a total capacity of 90 million cubic metres a day. The average residential water use was 248 litres per capita per day. One fourth of the world's fresh surface and groundwater is located in Russia. The water utilities sector is one of the largest industries in Russia serving the entire Russian population.

Lake Baikal is famous for its record depth and clear waters. It contains 20% of the world's liquid fresh water. However, as water pollution gets worse, the lake is going to be a swamp instead of a freshwater lake soon.

There are many different estimates of the actual cost of corruption. According to official government statistics from Rosstat, the "shadow economy" occupied only 15% of Russia's GDP in 2011, and this included unreported salaries (to avoid taxes and social payments) and other types of tax evasion. According to Rosstat's estimates, corruption in 2011 amounted to only 3.5 to 7% of GDP. In comparison, some independent experts maintain that corruption consumes as much of 25% of Russia's GDP. A World Bank report puts this figure at 48%. There is also an interesting shift in the main focus of bribery: whereas previously officials took bribes to shut their eyes to legal infractions, they now take them simply to perform their duties. Many experts admit that in recent years corruption in Russia has become a business. In the 1990s, businessmen had to pay different criminal groups to provide a ""krysha"" (literally, a "roof", i.e., protection). Nowadays, this "protective" function is performed by officials. Corrupt hierarchies characterize different sectors of the economy, including education.

In the end, the Russian population pays for this corruption. For example, some experts believe that the rapid increases in tariffs for housing, water, gas and electricity, which significantly outpace the rate of inflation, are a direct result of high volumes of corruption at the highest levels. In the recent years the reaction to corruption has changed: starting from Putin's second term, very few corruption cases have been the subject of outrage. Putin's system is remarkable for its ubiquitous and open merging of the civil service and business, as well as its use of relatives, friends, and acquaintances to benefit from budgetary expenditures and take over state property. Corporate, property, and land raiding is commonplace.

On 26 March 2017, protests against alleged corruption in the federal Russian government took place simultaneously in many cities across the country. They were triggered by the lack of proper response from the Russian authorities to the published investigative film "He Is Not Dimon To You", which has garnered more than 20 million views on YouTube.

In the 2018 results of the Corruption Perceptions Index by Transparency International, Russia ranked 138th out of 180 countries with a score of 28 out of 100, tying with Guinea, Iran, Lebanon, Mexico and Papua New Guinea.

Ethnic Russians comprise 81% of the country's population. The Russian Federation is also home to several sizable minorities. In all, 160 different other ethnic groups and indigenous peoples live within its borders. Although Russia, by population is the 9th largest in the world, its density is low because of the country's enormous size. Population is densest in European Russia; (which houses about 77% of the country's total population), Near the Ural Mountains, and in southwest Siberia. 73% of the population lives in urban areas while 27% in rural ones. The results of the 2010 Census show a total population of 142,856,536.

Russia's population peaked at 148,689,000 in 1991, just before the dissolution of the Soviet Union. It began to experience a rapid decline starting in the mid-1990s. The decline has slowed to near stagnation in recent years because of reduced death rates, increased birth rates and increased immigration.

In 2009, Russia recorded annual population growth for the first time in fifteen years, with total growth of 10,500. The number of Russian emigrants steadily declined from 359,000 in 2000 to 32,000 in 2009. According to the UN, Russia's immigrant population is the third largest in the world, numbering 11.6 million. Ukraine, Uzbekistan, Tajikistan, Azerbaijan, Moldova and Kazakhstan were the leading countries of origin for immigrants to Russia. There are about 3 million Ukrainians living in Russia. 196,000 migrants arrived to the Russian Federation in 2016, mostly from the ex-Soviet states. Russia is home to approximately 116 million ethnic Russians and about 20 million ethnic Russians live outside Russia in the former republics of the Soviet Union, mostly in Ukraine and Kazakhstan.

The 2010 census recorded 81% of the population as ethnically Russian, and 19% as other ethnicities: 3.7% Tatars; 1.4% Ukrainians; 1.1% Bashkirs; 1% Chuvashes; 11.8% others and unspecified. According to the Census, 84.93% of the Russian population belongs to European ethnic groups (Slavic, Germanic, Finnic, Greek, and others). This is a decline from the 2002, when they constituted for more than 86% of the population.

Russia's birth rate is higher than that of most European countries (13.3 births per 1000 people in 2014 compared to the European Union average of 10.1 per 1000), but its death rate is also substantially higher (in 2014, Russia's death rate was 13.1 per 1000 people compared to the EU average of 9.7 per 1000). The Russian Ministry of Health and Social Affairs predicted that by 2011 the death rate would equal the birth rate because of increase in fertility and decline in mortality. The government is implementing a number of programs designed to increase the birth rate and attract more migrants. Monthly government child-assistance payments were doubled to US$55, and a one-time payment of US$9,200 was offered to women who had a second child since 2007.

In 2018 the average total fertility rate (TFR) across Russia was 1.61 children per woman., below the replacement rate of 2.1, it remains considerably below the high of 7.44 children born per woman in 1908. In 2018 the median age of the Russian population was 39.8 years.

In 2006, in a bid to compensate for the country's demographic decline, the Russian government started simplifying immigration laws and launched a state program "for providing assistance to voluntary immigration of ethnic Russians from former Soviet republics". In 2009 Russia experienced its highest birth rate since the dissolution of the Soviet Union. In 2012, the birth rate increased again. Russia recorded 1,896,263 births, the highest number since 1990, and even exceeding annual births during the period 1967–1969, with a TFR of about 1.7, the highest since 1991. (Source: Vital statistics table below)

In August 2012, as the country saw its first demographic growth since the 1990s, President Putin declared that Russia's population could reach 146 million by 2025, mainly as a result of immigration.

Russia is a multi-national state with over 170 ethnic groups designated as nationalities; the populations of these groups vary enormously, from millions (e.g., Russians and Tatars) to under 10,000 (e.g., Samis and Eskimo).

Russia's 160 ethnic groups speak some 100 languages. According to the 2002 Census, 142.6 million people speak Russian, followed by Tatar with 5.3 million and Ukrainian with 1.8 million speakers. Russian is the only official state language, but the Constitution gives the individual republics the right to establish their own state languages in addition to Russian.

Despite its wide distribution, the Russian language is homogeneous throughout the country. Russian is the most geographically widespread language of Eurasia, as well as the most widely spoken Slavic language. It belongs to the Indo-European language family and is one of the living members of the East Slavic languages, the others being Belarusian and Ukrainian (and possibly Rusyn). Written examples of Old East Slavic ("Old Russian") are attested from the 10th century onwards.

Russian is the second-most used language on the Internet after English, one of two official languages aboard the International Space Station and is one of the six official languages of the UN.

35 languages are officially recognized in Russia in various regions by local governments.

Russians have practised Orthodox Christianity since the 10th century. According to the historical traditions of the Orthodox Church, Christianity was first brought to the territory of modern Belarus, Russia and Ukraine by Saint Andrew, the first Apostle of Jesus Christ. Following the "Primary Chronicle", the definitive Christianization of Kievan Rus' dates from the year 988 (the year is disputed), when Vladimir the Great was baptized in Chersonesus and proceeded to baptize his family and people in Kiev. The latter events are traditionally referred to as the "baptism of Rus'" (, ) in Russian and Ukrainian literature. Much of the Russian population, like other Slavic peoples, preserved for centuries a double belief ("dvoeverie") in both indigenous religion and Orthodox Christianity.

At the time of the 1917 Revolution, the Russian Orthodox Church was deeply integrated into the autocratic state, enjoying official status. This was a significant factor that contributed to the Bolshevik attitude to religion and the steps they took to control it. Moreover, the Bolsheviks including many people with non-Russian and/or non-Christian background, such as Vladimir Lenin, Leon Trotsky, Grigory Zinoviev, Lev Kamenev, and Grigori Sokolnikov, who were, at best, indifferent towards Christianity, and at worst hostile to it. The ideas of German philosopher Karl Marx were synthesised with Lenin's own political thought to form the Communist Party.

Thus the USSR became one of the first communist states to proclaim, as an ideological objective, the elimination of religion and its replacement with universal atheism. The communist government ridiculed religions and their believers, and propagated atheism in schools. The confiscation of religious assets was often based on accusations of illegal accumulation of wealth. State atheism in the Soviet Union was known in Russian as "gosateizm", and was based on the ideology of Marxism–Leninism, which consistently advocated has consistently advocated the control, suppression, and elimination of religion. Within about a year of the revolution, the state expropriated all church property, including the churches themselves, and in the period from 1922 to 1926, 28 Russian Orthodox bishops and more than 1,200 priests were killed. Many more were persecuted.

After the collapse of the Soviet Union there has been a renewal of religions in Russia, and among Slavs various movements have emerged besides Christianity, including Rodnovery (Slavic Native Faith), Assianism, and other ethnic Paganisms, Roerichism, Ringing Cedars' Anastasianism, Hinduism, Siberian shamanism or Tengrism, and other religions.

In 2012 the research organization Sreda, in cooperation with the 2010 census and the Ministry of Justice, published the Arena Atlas, a detailed enumeration of religious populations and nationalities in Russia, based on a large-sample country-wide survey. The results showed that 46.8% of Russians declared themselves Christians—including 41% Russian Orthodox, 1.5% simply Orthodox or members of non-Russian Orthodox churches, 4.1% unaffiliated Christians, and less than 1% for both Old Believers, Catholics, and Protestants—while 25% were spiritual but not religious, 13% were atheists, 6.5% were Muslims, 1.2% were followers of "traditional religions honoring gods and ancestors" (including Rodnovery, Tengrism and other ethnic religions), and 0.5% were Buddhists, 0.1% were religious Jews and 0.1% were Hindus.

According to various reports, the proportion of not religious people in Russia is between 16% and 48% of the population. According to recent studies, the proportion of atheists has significantly decreased over the decades after the dissolution of the Soviet Union.

Orthodox Christianity, Islam, Buddhism, and Paganism (either preserved or revived) are recognised by law as Russia's traditional religions, marking the country's "historical heritage".

An estimated 95% of the registered Orthodox parishes belong to the Russian Orthodox Church while there are a number of smaller Orthodox churches. However, the vast majority of Orthodox believers do not attend church on a regular basis. Easter is the most popular religious holiday in Russia, celebrated by a large segment of the Russian population, including large numbers of those who are non-religious. More than three-quarters of the Russian population celebrate Easter by making traditional Easter cakes, coloured eggs and paskha.

Islam is the second largest religion in Russia after Russian Orthodoxy. It is the traditional or predominant religion amongst some Caucasian ethnicities (notably the Chechens, the Ingush and the Circassians), and amongst some Turkic peoples (notably the Tatars and the Bashkirs).

Buddhism is traditional in three republics of Russia: Buryatia, Tuva, and Kalmykia, the latter being the only region in Europe where Buddhism is the most practiced religion.

In cultural and social affairs, Vladimir Putin has collaborated closely with the Russian Orthodox Church. Patriarch Kirill of Moscow, head of the Church, endorsed his election in 2012. Steven Myers reports, "The church, once heavily repressed, had emerged from the Soviet collapse as one of the most respected institutions... Now Kiril led the faithful directly into an alliance with the state." Baptist minister Mark Woods provides specific examples of how the Church under Patriarch Kirill of Moscow has backed the expansion of Russian power into Crimea and eastern Ukraine.

The Holy Synod of the Russian Orthodox Church, at its session on 15 October 2018, severed ties with the Ecumenical Patriarchate of Constantinople. The decision was taken in response to the move made by the Patriarchate of Constantinople a few days prior that effectively ended the Moscow Patriarchate's jurisdiction over Ukraine and promised autocephaly to Ukraine.

On 26 April 2017, for the first time, the U.S. Commission on International Religious Freedom classified Russia as one of the world's worst violators of religious liberty, recommending in its 2017 annual report that the U.S. government deem Russia a "country of particular concern" under the International Religious Freedom Act and negotiate for religious liberty. The report states, "—it is the sole state to have not only continually intensified its repression of religious freedom since USCIRF commenced monitoring it, but also to have expanded its repressive policies...ranging from administrative harassment to arbitrary imprisonment to extrajudicial killing, are implemented in a fashion that is systematic, ongoing, and egregious." On 4 April 2017 UN Special Rapporteur on Freedom of Opinion and Expression David Kaye, UN Special Rapporteur on Freedoms of Peaceful Assembly and Association Maina Kiai, and UN Special Rapporteur on Freedom of Religion and Belief Ahmed Shaheed condemned Russia's treatment of Jehovah's Witnesses. Many other countries and international organizations have spoken out on Russia's religious abuses.

The Russian Constitution guarantees free, universal health care for all its citizens. In practice, however, free health care is partially restricted because of mandatory registration. While Russia has more physicians, hospitals, and health care workers than almost any other country in the world on a per capita basis, since the dissolution of the Soviet Union the health of the Russian population has declined considerably as a result of social, economic, and lifestyle changes; the trend has been reversed only in the recent years, with average life expectancy having increased 5.2 years for males and 3.1 years for females between 2006 and 2014.

Due to the ongoing Russian financial crisis since 2014, major cuts in health spending have resulted in a decline in the quality of service of the state healthcare system. About 40% of basic medical facilities have fewer staff than they are supposed to have, with others being closed down. Waiting times for treatment have increased, and patients have been forced to pay for more services that were previously free.

, the average life expectancy in Russia was 65.29 years for males and 76.49 years for females. The biggest factor contributing to the relatively low life expectancy for males is a high mortality rate among working-age males. Deaths mostly occur from preventable causes, including alcohol poisoning, smoking, traffic accidents and violent crime. As a result, Russia has one of the world's most female-biased sex ratios, with 0.859 males to every female.

Russia has the most college-level or higher graduates in terms of percentage of population in the world, at 54%. Russia has a free education system, which is guaranteed for all citizens by the Constitution, however entry to subsidized higher education is highly competitive. As a result of great emphasis on science and technology in education, Russian medical, mathematical, scientific, and aerospace research is generally of a high order.

Since 1990, the 11-year school education has been introduced. Education in state-owned secondary schools is free. University level education is free, with exceptions. A substantial share of students is enrolled for full pay (many state institutions started to open commercial positions in the last years).

The oldest and largest Russian universities are Moscow State University and Saint Petersburg State University. In the 2000s, in order to create higher education and research institutions of comparable scale in Russian regions, the government launched a program of establishing "federal universities", mostly by merging existing large regional universities and research institutes and providing them with a special funding. These new institutions include the Southern Federal University, Siberian Federal University, Kazan Volga Federal University, North-Eastern Federal University, and Far Eastern Federal University.
According to the 2018 QS World University Rankings, the highest-ranking Russian educational institution is Moscow State University, rated 95th in the world.

There are over 160 different ethnic groups and indigenous peoples in Russia. The country's vast cultural diversity spans ethnic Russians with their Slavic Orthodox traditions, Tatars and Bashkirs with their Turkic Muslim culture, Buddhist nomadic Buryats and Kalmyks, Shamanistic peoples of the Extreme North and Siberia, highlanders of the Northern Caucasus, and Finno-Ugric peoples of the Russian North West and Volga Region.

Handicraft, like Dymkovo toy, khokhloma, gzhel and palekh miniature represent an important aspect of Russian folk culture. Ethnic Russian clothes include kaftan, kosovorotka and ushanka for men, sarafan and kokoshnik for women, with lapti and valenki as common shoes. The clothes of Cossacks from Southern Russia include burka and papaha, which they share with the peoples of the Northern Caucasus.

Russian cuisine widely uses fish, caviar, poultry, mushrooms, berries, and honey. Crops of rye, wheat, barley, and millet provide the ingredients for various breads, pancakes and cereals, as well as for kvass, beer and vodka drinks. Black bread is rather popular in Russia, compared to the rest of the world. Flavourful soups and stews include shchi, borsch, ukha, solyanka and okroshka. Smetana (a heavy sour cream) is often added to soups and salads. Pirozhki, blini and syrniki are native types of pancakes. Chicken Kiev, pelmeni and shashlyk are popular meat dishes, the last two being of Tatar and Caucasus origin respectively. Other meat dishes include stuffed cabbage rolls "(golubtsy)" usually filled with meat. Salads include Olivier salad, vinegret and dressed herring.

Russia's large number of ethnic groups have distinctive traditions regarding folk music. Typical ethnic Russian musical instruments are gusli, balalaika, zhaleika, and garmoshka. Folk music had a significant influence on Russian classical composers, and in modern times it is a source of inspiration for a number of popular folk bands, like Melnitsa. Russian folk songs, as well as patriotic Soviet songs, constitute the bulk of the repertoire of the world-renowned Red Army choir and other popular ensembles.

Russians have many traditions, including the washing in banya, a hot steam bath somewhat similar to sauna. Old Russian folklore takes its roots in the pagan Slavic religion. Many Russian fairy tales and epic bylinas were adapted for animation films, or for feature movies by the prominent directors like Aleksandr Ptushko ("Ilya Muromets", "Sadko") and Aleksandr Rou ("Morozko", "Vasilisa the Beautiful"). Russian poets, including Pyotr Yershov and Leonid Filatov, made a number of well-known poetical interpretations of the classical fairy tales, and in some cases, like that of Alexander Pushkin, also created fully original fairy tale poems of great popularity.

Since the Christianization of Kievan Rus' for several ages Russian architecture was influenced predominantly by the Byzantine architecture. Apart from fortifications (kremlins), the main stone buildings of ancient Rus' were Orthodox churches with their many domes, often gilded or brightly painted.

Aristotle Fioravanti and other Italian architects brought Renaissance trends into Russia since the late 15th century, while the 16th century saw the development of unique tent-like churches culminating in Saint Basil's Cathedral. By that time the onion dome design was also fully developed. In the 17th century, the "fiery style" of ornamentation flourished in Moscow and Yaroslavl, gradually paving the way for the Naryshkin baroque of the 1690s. After the reforms of Peter the Great the change of architectural styles in Russia generally followed that in the Western Europe.

The 18th-century taste for rococo architecture led to the ornate works of Bartolomeo Rastrelli and his followers. The reigns of Catherine the Great and her grandson Alexander I saw the flourishing of Neoclassical architecture, most notably in the capital city of Saint Petersburg. The second half of the 19th century was dominated by the Neo-Byzantine and Russian Revival styles. Prevalent styles of the 20th century were the Art Nouveau, Constructivism, and the Stalin Empire style.

With the change in values imposed by communist ideology, the tradition of preservation was broken. Independent preservation societies, even those that defended only secular landmarks such as Moscow-based OIRU were disbanded by the end of the 1920s. A new anti-religious campaign, launched in 1929, coincided with collectivization of peasants; destruction of churches in the cities peaked around 1932. A number of churches were demolished, including the Cathedral of Christ the Saviour in Moscow. In Moscow alone losses of 1917–2006 are estimated at over 640 notable buildings (including 150 to 200 listed buildings, out of a total inventory of 3,500) – some disappeared completely, others were replaced with concrete replicas.

In 1955, a new Soviet leader, Nikita Khrushchev, condemned the "excesses" of the former academic architecture, and the late Soviet era was dominated by plain functionalism in architecture. This helped somewhat to resolve the housing problem, but created a large quantity of buildings of low architectural quality, much in contrast with the previous bright styles. In 1959 Nikita Khrushchev launched his anti-religious campaign. By 1964 over 10 thousand churches out of 20 thousand were shut down (mostly in rural areas) and many were demolished. Of 58 monasteries and convents operating in 1959, only sixteen remained by 1964; of Moscow's fifty churches operating in 1959, thirty were closed and six demolished.

Early Russian painting is represented in icons and vibrant frescos, the two genres inherited from Byzantium. As Moscow rose to power, Theophanes the Greek, Dionisius and Andrei Rublev became vital names associated with a distinctly Russian art.

The Russian Academy of Arts was created in 1757 and gave Russian artists an international role and status. Ivan Argunov, Dmitry Levitzky, Vladimir Borovikovsky and other 18th-century academicians mostly focused on portrait painting. In the early 19th century, when neoclassicism and romantism flourished, mythological and Biblical themes inspired many prominent paintings, notably by Karl Briullov and Alexander Ivanov.

In the mid-19th century the "Peredvizhniki" ("Wanderers") group of artists broke with the Academy and initiated a school of art liberated from academic restrictions. These were mostly realist painters who captured Russian identity in landscapes of wide rivers, forests, and birch clearings, as well as vigorous genre scenes and robust portraits of their contemporaries. Some artists focused on depicting dramatic moments in Russian history, while others turned to social criticism, showing the conditions of the poor and caricaturing authority; critical realism flourished under the reign of Alexander II. Leading realists include Ivan Shishkin, Arkhip Kuindzhi, Ivan Kramskoi, Vasily Polenov, Isaac Levitan, Vasily Surikov, Viktor Vasnetsov, Ilya Repin, and Boris Kustodiev.

The turn of the 20th century saw the rise of symbolist painting, represented by Mikhail Vrubel, Kuzma Petrov-Vodkin, and Nicholas Roerich.

The Russian avant-garde was a large, influential wave of modernist art that flourished in Russia from approximately 1890 to 1930. The term covers many separate, but inextricably related art movements that occurred at the time, namely neo-primitivism, suprematism, constructivism, rayonism, and Russian Futurism. Notable artists from this era include El Lissitzky, Kazimir Malevich, Wassily Kandinsky, and Marc Chagall. Since the 1930s the revolutionary ideas of the avant-garde clashed with the newly emerged conservative direction of socialist realism.

Soviet art produced works that were furiously patriotic and anti-fascist during and after the Great Patriotic War. Multiple war memorials, marked by a great restrained solemnity, were built throughout the country. Soviet artists often combined innovation with socialist realism, notably the sculptors Vera Mukhina, Yevgeny Vuchetich and Ernst Neizvestny.

Music in 19th-century Russia was defined by the tension between classical composer Mikhail Glinka along with other members of The Mighty Handful, who embraced Russian national identity and added religious and folk elements to their compositions, and the Russian Musical Society led by composers Anton and Nikolay Rubinsteins, which was musically conservative. The later tradition of Pyotr Ilyich Tchaikovsky, one of the greatest composers of the Romantic era, was continued into the 20th century by Sergei Rachmaninoff. World-renowned composers of the 20th century include Alexander Scriabin, Igor Stravinsky, Sergei Prokofiev, Dmitri Shostakovich and Alfred Schnittke.

Russian conservatories have turned out generations of famous soloists. Among the best known are violinists Jascha Heifetz, David Oistrakh, Leonid Kogan, Gidon Kremer, and Maxim Vengerov; cellists Mstislav Rostropovich, Natalia Gutman; pianists Vladimir Horowitz, Sviatoslav Richter, Emil Gilels, Vladimir Sofronitsky and Evgeny Kissin; and vocalists Fyodor Shalyapin, Mark Reizen, Elena Obraztsova, Tamara Sinyavskaya, Nina Dorliak, Galina Vishnevskaya, Anna Netrebko and Dmitry Hvorostovsky.

During the early 20th century, Russian ballet dancers Anna Pavlova and Vaslav Nijinsky rose to fame, and impresario Sergei Diaghilev and his Ballets Russes' travels abroad profoundly influenced the development of dance worldwide. Soviet ballet preserved the perfected 19th-century traditions, and the Soviet Union's choreography schools produced many internationally famous stars, including Galina Ulanova, Maya Plisetskaya, Rudolf Nureyev, and Mikhail Baryshnikov. The Bolshoi Ballet in Moscow and the Mariinsky Ballet in St Petersburg remain famous throughout the world.

Modern Russian rock music takes its roots both in the Western rock and roll and heavy metal, and in traditions of the Russian bards of the Soviet era, such as Vladimir Vysotsky and Bulat Okudzhava. Popular Russian rock groups include Mashina Vremeni, DDT, Aquarium, Alisa, Kino, Kipelov, Nautilus Pompilius, Aria, Grazhdanskaya Oborona, Splean, and Korol i Shut. Russian pop music developed from what was known in the Soviet times as "estrada" into full-fledged industry, with some performers gaining wide international recognition, such as t.A.T.u., Nu Virgos and Vitas.

In the 18th century, during the era of Russian Enlightenment, the development of Russian literature was boosted by the works of Mikhail Lomonosov and Denis Fonvizin. By the early 19th century a modern national tradition had emerged, producing some of the greatest writers in Russian history. This period, known also as the Golden Age of Russian Poetry, began with Alexander Pushkin, who is considered the founder of the modern Russian literary language and often described as the "Russian Shakespeare". It continued with the poetry of Mikhail Lermontov and Nikolay Nekrasov, dramas of Alexander Ostrovsky and Anton Chekhov, and the prose of Nikolai Gogol and Ivan Turgenev. Leo Tolstoy and Fyodor Dostoyevsky have been described by literary critics as the greatest novelists of all time.

By the 1880s, the age of the great novelists was over, and short fiction and poetry became the dominant genres. The next several decades became known as the Silver Age of Russian Poetry, when the previously dominant literary realism was replaced by symbolism. Leading authors of this era include such poets as Valery Bryusov, Vyacheslav Ivanov, Alexander Blok, Nikolay Gumilev and Anna Akhmatova, and novelists Leonid Andreyev, Ivan Bunin, and Maxim Gorky.

Russian philosophy blossomed in the 19th century, when it was defined initially by the opposition of Westernizers, who advocated Western political and economical models, and Slavophiles, who insisted on developing Russia as a unique civilization. The latter group includes Nikolai Danilevsky and Konstantin Leontiev, the founders of eurasianism. In its further development Russian philosophy was always marked by a deep connection to literature and interest in creativity, society, politics and nationalism; Russian cosmism and religious philosophy were other major areas. Notable philosophers of the late 19th and the early 20th centuries include Vladimir Solovyev, Sergei Bulgakov, and Vladimir Vernadsky.

Following the Russian Revolution of 1917 many prominent writers and philosophers left the country, including Bunin, Vladimir Nabokov and Nikolay Berdyayev, while a new generation of talented authors joined together in an effort to create a distinctive working-class culture appropriate for the new Soviet state. In the 1930s censorship over literature was tightened in line with the policy of socialist realism. In the late 1950s restrictions on literature were eased, and by the 1970s and 1980s, writers were increasingly ignoring official guidelines. Leading authors of the Soviet era include novelists Yevgeny Zamyatin (emigrated), Ilf and Petrov, Mikhail Bulgakov (censored) and Mikhail Sholokhov, and poets Vladimir Mayakovsky, Yevgeny Yevtushenko, and Andrey Voznesensky.

The Soviet Union was also a major producer of science fiction, written by authors like Arkady and Boris Strugatsky, Kir Bulychov, Alexander Belayev and Ivan Yefremov. Traditions of Russian science fiction and fantasy are continued today by numerous writers.

Russian and later Soviet cinema was a hotbed of invention in the period immediately following 1917, resulting in world-renowned films such as "The Battleship Potemkin" by Sergei Eisenstein. Eisenstein was a student of filmmaker and theorist Lev Kuleshov, who developed the Soviet montage theory of film editing at the world's first film school, the All-Union Institute of Cinematography. Dziga Vertov, whose "kino-glaz" ("film-eye") theory—that the camera, like the human eye, is best used to explore real life—had a huge impact on the development of documentary film making and cinema realism. The subsequent state policy of socialist realism somewhat limited creativity; however, many Soviet films in this style were artistically successful, including "Chapaev", "The Cranes Are Flying", and "Ballad of a Soldier".

The 1960s and 1970s saw a greater variety of artistic styles in Soviet cinema. Eldar Ryazanov's and Leonid Gaidai's comedies of that time were immensely popular, with many of the catch phrases still in use today. In 1961–68 Sergey Bondarchuk directed an Oscar-winning film adaptation of Leo Tolstoy's epic "War and Peace", which was the most expensive film made in the Soviet Union. In 1969, Vladimir Motyl's "White Sun of the Desert" was released, a very popular film in a genre of ostern; the film is traditionally watched by cosmonauts before any trip into space.

Russian animation dates back to late Russian Empire times. During the Soviet era, Soyuzmultfilm studio was the largest animation producer. Soviet animators developed a great variety of pioneering techniques and aesthetic styles, with prominent directors including Ivan Ivanov-Vano, Fyodor Khitruk and Aleksandr Tatarsky. Many Soviet cartoon heroes such as the Russian-style Winnie-the-Pooh, cute little Cheburashka, Wolf and Hare from "Nu, Pogodi!", are iconic images in Russia and many surrounding countries.

The late 1980s and 1990s were a period of crisis in Russian cinema and animation. Although Russian filmmakers became free to express themselves, state subsidies were drastically reduced, resulting in fewer films produced. The early years of the 21st century have brought increased viewership and subsequent prosperity to the industry on the back of the economic revival. Production levels are already higher than in Britain and Germany. Russia's total box-office revenue in 2007 was $565 million, up 37% from the previous year. In 2002 the "Russian Ark" became the first feature film ever to be shot in a single take. The traditions of Soviet animation were developed recently by such directors as Aleksandr Petrov and studios like Melnitsa Animation.

While there were few stations or channels in the Soviet time, in the past two decades many new state and privately owned radio stations and TV channels have appeared. In 2005 a state-run English language Russia Today TV started broadcasting, and its Arabic version Rusiya Al-Yaum was launched in 2007. Censorship and Media freedom in Russia has always been a main theme of Russian media.

Soviet and later Russian athletes have always been in the top four for the number of gold medals collected at the Summer Olympics. Soviet gymnasts, track-and-field athletes, weightlifters, wrestlers, boxers, fencers, shooters, cross country skiers, biathletes, speed skaters and figure skaters were consistently among the best in the world, along with Soviet basketball, handball, volleyball and ice hockey players. The 1980 Summer Olympics were held in Moscow while the 2014 Winter Olympics were hosted in Sochi.

Although ice hockey was only introduced during the Soviet era, the Soviet Union national team managed to win gold at almost all the Olympics and World Championships they contested. Russian players Valery Kharlamov, Sergei Makarov, Vyacheslav Fetisov and Vladislav Tretiak hold four of six positions in the IIHF "Team of the Century". Russia has not won the Olympic ice hockey tournament since the Unified Team won gold in 1992. Russia won the 1993, 2008, 2009, 2012 and the 2014 IIHF World Championships.

The Kontinental Hockey League (KHL) was founded in 2008 as a successor to the Russian Superleague. It is ranked the top hockey league in Europe , and the second-best in the world. It is an international professional ice hockey league in Eurasia and consists of 29 teams, of which 21 are based in Russia and 7 more are located in Latvia, Kazakhstan, Belarus, Finland, Slovakia, Croatia and China. KHL is on the 4th place by attendance in Europe.

Bandy, also known as Russian hockey, is another traditionally popular ice sport. The Soviet Union won all the Bandy World Championships for men between 1957–79 and some thereafter too. After the dissolution of the Soviet Union, Russia has continuously been one of the most successful teams, winning many world championships.

Association football is one of the most popular sports in modern Russia. The Soviet national team became the first European Champions by winning Euro 1960. Appearing in four FIFA World Cups from 1958 to 1970, Lev Yashin is regarded as one of the greatest goalkeepers in the history of football, and was chosen on the FIFA World Cup Dream Team. The Soviet national team reached the finals of Euro 1988. In 1956 and 1988, the Soviet Union won gold at the Olympic football tournament. Russian clubs CSKA Moscow and Zenit St Petersburg won the UEFA Cup in 2005 and 2008. The Russian national football team reached the semi-finals of Euro 2008, losing only to the eventual champions Spain. Russia was the host nation for the 2018 FIFA World Cup. The matches were held from 14 June to 15 July 2018 in the stadiums of 11 host cities located in the European part of the country and in the Ural region. This was the first football World Cup ever held in Eastern Europe, and the first held in Europe since 2006. Russia will also host games of Euro 2020.
In 2007, the Russian national basketball team won the European Basketball Championship. The Russian basketball club PBC CSKA Moscow is one of the top teams in Europe, winning the Euroleague in 2006 and 2008.

Larisa Latynina, who currently holds the record for the most gold Olympic medals won by a woman, established the USSR as the dominant force in gymnastics for many years. Today, Russia is the leading nation in rhythmic gymnastics with Yevgeniya Kanayeva. Double 50 m and 100 m freestyle Olympic gold medalist Alexander Popov is widely considered the greatest sprint swimmer in history. Russian synchronized swimming is the best in the world, with almost all gold medals at Olympics and World Championships having been swept by Russians in recent decades. Figure skating is another popular sport in Russia, especially pair skating and ice dancing. With the exception of 2010 and 2018 a Soviet or Russian pair has won gold at every Winter Olympics since 1964.

Since the end of the Soviet era, tennis has grown in popularity and Russia has produced a number of famous players, including Maria Sharapova. In martial arts, Russia produced the sport Sambo and renowned fighters, like Fedor Emelianenko. Chess is a widely popular pastime in Russia; from 1927, Russian grandmasters have held the world chess championship almost continuously.

The 2014 Winter Olympics were held in Sochi in the south of Russia. In 2016 the McLaren Report found evidence of widespread state-sponsored doping and an institutional conspiracy to cover up Russian competitors' positive drug tests. 25 athletes are disqualified, 11 medals are stripped.

Formula One is also becoming increasingly popular in Russia. In 2010 Vitaly Petrov of Vyborg became the first Russian to drive in Formula One, and was soon followed by a second – Daniil Kvyat, from Ufa – in 2014. There had only been two Russian Grands Prix (in 1913 and 1914), but the Russian Grand Prix returned as part of the Formula One season in 2014, as part of a six-year deal.
Russia has the most Olympic medals stripped for doping violations (51), the most of any country, four times the number of the runner-up, and more than a third of the global total, and 129 athletes caught doping at the Olympics, also the most of any country. From 2011 to 2015, more than a thousand Russian competitors in various sports, including summer, winter, and Paralympic sports, benefited from a state-sponsored cover-up, with no indication that the program has ceased since then.

There are seven public holidays in Russia, except those always celebrated on Sunday. Russian New Year traditions resemble those of the Western Christmas, with New Year Trees and gifts, and Ded Moroz (Father Frost) playing the same role as Santa Claus. Orthodox Christmas falls on 7 January, because the Russian Orthodox Church still follows the Julian calendar, and all Orthodox holidays are 13 days after Western ones. Two other major Christian holidays are Easter and Trinity Sunday. Kurban Bayram and Uraza Bayram are celebrated by Russian Muslims.

Further Russian public holidays include Defender of the Fatherland Day (23 February), which honors Russian men, especially those serving in the army; International Women's Day (8 March), which combines the traditions of Mother's Day and Valentine's Day; Spring and Labor Day (1 May); Victory Day (9 May); Russia Day (12 June); and Unity Day (4 November), commemorating the popular uprising which expelled the Polish occupation force from Moscow in 1612.

Victory Day is the second most popular holiday in Russia; it commemorates the victory over Nazi Germany and its allies in the Great Patriotic War. A huge military parade, hosted by the President of Russia, is annually organised in Moscow on Red Square. Similar parades take place in all major Russian cities and cities with the status "Hero city" or "City of Military Glory".

Popular non-public holidays include Old New Year (the New Year according to the Julian Calendar on 14 January), Tatiana Day (students holiday on 25 January), Maslenitsa (a pre-Christian spring holiday a week before the Great Lent), Cosmonautics Day (in tribute to the first human trip into space), Ivan Kupala Day (another pre-Christian holiday on 7 July) and Peter and Fevronia Day (which takes place on 8 July and is the Russian analogue of Valentine's Day, focusing, however, on family love and fidelity).
State symbols of Russia include the Byzantine double-headed eagle, combined with St. George of Moscow in the Russian coat of arms. The Russian flag dates from the late Tsardom of Russia period and has been widely used since the time of the Russian Empire. The Russian anthem shares its music with the Soviet Anthem, though not the lyrics. The imperial motto "God is with us" and the Soviet motto "Proletarians of all countries, unite!" are now obsolete and no new motto has replaced them. The hammer and sickle and the full Soviet coat of arms are still widely seen in Russian cities as a part of old architectural decorations. The Soviet Red Stars are also encountered, often on military equipment and war memorials. The Red Banner continues to be honored, especially the Banner of Victory of 1945.

The Matryoshka doll is a recognizable symbol of Russia, and the towers of Moscow Kremlin and Saint Basil's Cathedral in Moscow are Russia's main architectural icons. Cheburashka is a mascot of the Russian national Olympic team. St. Mary, St. Nicholas, St. Andrew, St. George, St. Alexander Nevsky, St. Sergius of Radonezh and St. Seraphim of Sarov are Russia's patron saints. Chamomile is the national flower, while birch is the national tree. The Russian bear is an animal symbol and Mother Russia a national personification of Russia, though the bear image has a Western origin and Russians themselves have accepted it only fairly recently.

Tourism in Russia has seen rapid growth since the late Soviet period, first domestic tourism and then international tourism, fueled by the rich cultural heritage and great natural variety of the country. Major tourist routes in Russia include a journey around the Golden Ring theme route of ancient cities, cruises on the big rivers like the Volga, and long journeys on the famous Trans-Siberian Railway. In 2013, Russia was visited by 28.4 million tourists; it is the ninth most visited country in the world and the seventh most visited in Europe. The number of Western visitors dropped in 2014.
The most visited destinations in Russia are Moscow and Saint Petersburg, the current and former capitals of the country. Recognized as World Cities, they feature such world-renowned museums as the Tretyakov Gallery and the Hermitage, famous theaters like Bolshoi and Mariinsky, ornate churches like Saint Basil's Cathedral, Cathedral of Christ the Saviour, Saint Isaac's Cathedral and Church of the Savior on Blood, impressive fortifications like the Kremlin and Peter and Paul Fortress, beautiful squares and streets like Red Square, Palace Square, Tverskaya Street, Nevsky Prospect, and Arbat Street. Rich palaces and parks are found in the former imperial residences in suburbs of Moscow (Kolomenskoye, Tsaritsyno) and St Petersburg (Peterhof, Strelna, Oranienbaum, Gatchina, Pavlovsk and Tsarskoye Selo). Moscow displays Soviet architecture at its best, along with modern skyscrapers, while St Petersburg, nicknamed "Venice of the North", boasts of its classical architecture, many rivers, canals and bridges.

Kazan, the capital of Tatarstan, shows a mix of Christian Russian and Muslim Tatar cultures. The city has registered a brand "The Third Capital of Russia", though a number of other major cities compete for this status, including Novosibirsk, Yekaterinburg and Nizhny Novgorod.

The warm subtropical Black Sea coast of Russia is the site for a number of popular sea resorts, like Sochi, the follow-up host of the 2014 Winter Olympics. The mountains of the Northern Caucasus contain popular ski resorts such as Dombay. The most famous natural destination in Russia is Lake Baikal, "the Blue Eye of Siberia". This unique lake, the oldest and deepest in the world, has crystal-clear waters and is surrounded by taiga-covered mountains. Other popular natural destinations include Kamchatka with its volcanoes and geysers, Karelia with its lakes and granite rocks, the snowy Altai Mountains, and the wild steppes of Tuva.





</doc>
