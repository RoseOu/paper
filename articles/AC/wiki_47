<doc id="29234" url="https://en.wikipedia.org/wiki?curid=29234" title="Spear">
Spear

A spear is a pole weapon consisting of a shaft, usually of wood, with a pointed head. The head may be simply the sharpened end of the shaft itself, as is the case with fire hardened spears, or it may be made of a more durable material fastened to the shaft, such as flint, obsidian, iron, steel or bronze. The most common design for hunting or combat spears since ancient times has incorporated a metal spearhead shaped like a triangle, lozenge, or leaf. The heads of fishing spears usually feature barbs or serrated edges.

The word "spear" comes from the Old English "spere", from the Proto-Germanic "speri", from a Proto-Indo-European root "*sper-" "spear, pole".
Spears can be divided into two broad categories: those designed for thrusting in melee combat and those designed for throwing (usually referred to as javelins).

The spear has been used throughout human history both as a hunting and fishing tool and as a weapon. Along with the axe, knife and club, it is one of the earliest and most important tools developed by early humans. As a weapon, it may be wielded with either one hand or two. It was used in virtually every conflict up until the modern era, where even then it continues on in the form of the fixed bayonet, and is probably the most commonly used weapon in history.

Spear manufacture and use is not confined to humans. It is also practiced by the western chimpanzee. Chimpanzees near Kédougou, Senegal have been observed to create spears by breaking straight limbs off trees, stripping them of their bark and side branches, and sharpening one end with their teeth. They then used the weapons to hunt galagos sleeping in hollows.

Archaeological evidence found in present-day Germany documents that wooden spears have been used for hunting since at least 400,000 years ago, and a 2012 study from the site of Kathu Pan in South Africa suggests that hominids, possibly "Homo heidelbergensis", may have developed the technology of hafted stone-tipped spears in Africa about 500,000 years ago. Wood does not preserve well, however, and Craig Stanford, a primatologist and professor of anthropology at the University of Southern California, has suggested that the discovery of spear use by chimpanzees probably means that early humans used wooden spears as well, perhaps, five million years ago.

Neanderthals were constructing stone spear heads from as early as 300,000 BP and by 250,000 years ago, wooden spears were made with fire-hardened points.

From circa 200,000 BCE onwards, Middle Paleolithic humans began to make complex stone blades with flaked edges which were used as spear heads. These stone heads could be fixed to the spear shaft by gum or resin or by bindings made of animal sinew, leather strips or vegetable matter. During this period, a clear difference remained between spears designed to be thrown and those designed to be used in hand-to-hand combat. By the Magdalenian period (c. 15,000–9500 BCE), spear-throwers similar to the later atlatl were in use.

The spear is the main weapon of the warriors of Homer's Iliad. The use of both a single thrusting spear and two throwing spears are mentioned. It has been suggested that two styles of combat are being described; an early style, with thrusting spears, dating to the Mycenaean period in which the Iliad is set, and, anachronistically, a later style, with throwing spears, from Homer's own Archaic period.

In the 7th century BCE, the Greeks evolved a new close-order infantry formation, the phalanx. The key to this formation was the hoplite, who was equipped with a large, circular, bronze-faced shield (aspis) and a spear with an iron head and bronze butt-spike (doru). The hoplite phalanx dominated warfare among the Greek City States from the 7th into the 4th century BCE.

The 4th century saw major changes. One was the greater use of peltasts, light infantry armed with spear and javelins. The other was the development of the sarissa, a two-handed pike in length, by the Macedonians under Phillip of Macedon and Alexander the Great. The pike phalanx, supported by peltasts and cavalry, became the dominant mode of warfare among the Greeks from the late 4th century onward until Greek military systems were supplanted by the Roman legions.

In the pre-Marian Roman armies, the first two lines of battle, the "hastati" and "principes", often fought with a sword called a "gladius" and "pila", heavy javelins that were specifically designed to be thrown at an enemy to pierce and foul a target's shield. Originally the "principes" were armed with a short spear called a "hasta", but these gradually fell out of use, eventually being replaced by the gladius. The third line, the "triarii", continued to use the "hasta".

From the late 2nd century BCE, all legionaries were equipped with the "pilum". The "pilum" continued to be the standard legionary spear until the end of the 2nd century CE. "Auxilia", however, were equipped with a simple hasta and, perhaps, throwing spears. During the 3rd century CE, although the "pilum" continued to be used, legionaries usually were equipped with other forms of throwing and thrusting spear, similar to "auxilia" of the previous century. By the 4th century, the "pilum" had effectively disappeared from common use.

In the late period of the Roman Empire, the spear became more often used because of its anti-cavalry capacities as the barbarian invasions were often conducted by people with a developed culture of cavalry in warfare.

Muslim warriors used a spear that was called an "az-zaġāyah". Berbers pronounced it "zaġāya", but the English term, derived from the Old French via Berber, is "assegai". It is a pole weapon used for throwing or hurling, usually a light spear or javelin made of hard wood and pointed with a forged iron tip.The "az-zaġāyah" played an important role during the Islamic conquest as well as during later periods, well into the 20th century. A longer pole "az-zaġāyah" was being used as a hunting weapon from horseback. The "az-zaġāyah" was widely used. It existed in various forms in areas stretching from Southern Africa to the Indian subcontinent, although these places already had their own variants of the spear. This javelin was the weapon of choice during the "Fulani jihad" as well as during the Mahdist War in Sudan. It is still being used by Sikh "Nihang" in the Punjab as well as certain wandering Sufi ascetics "(Derwishes)".

After the fall of the Western Roman Empire, the spear and shield continued to be used by nearly all Western European cultures. Since a medieval spear required only a small amount of steel along the sharpened edges (most of the spear-tip was wrought iron), it was an economical weapon. Quick to manufacture, and needing less smithing skill than a sword, it remained the main weapon of the common soldier. The Vikings, for instance, although often portrayed with axe or sword in hand, were armed mostly with spears, as were their Anglo-Saxon, Irish, or continental contemporaries.

Broadly speaking, spears were either designed to be used in melee, or to be thrown. Within this simple classification, there was a remarkable range of types. For example, M. J. Swanton identified thirty different spearhead categories and sub-categories in early Saxon England. Most medieval spearheads were generally leaf-shaped. Notable types of early medieval spears include the "angon", a throwing spear with a long head similar to the Roman "pilum", used by the Franks and Anglo-Saxons, and the winged (or lugged) spear, which had two prominent wings at the base of the spearhead, either to prevent the spear penetrating too far into an enemy or to aid in spear fencing. Originally a Frankish weapon, the winged spear also was popular with the Vikings. It would become the ancestor of later medieval polearms, such as the partisan and spetum.

The thrusting spear also has the advantage of reach, being considerably longer than other weapon types. Exact spear lengths are hard to deduce as few spear shafts survive archaeologically but would seem to have been the norm. Some nations were noted for their long spears, including the Scots and the Flemish. Spears usually were used in tightly ordered formations, such as the shield wall or the schiltron. To resist cavalry, spear shafts could be planted against the ground. William Wallace drew up his schiltrons in a circle at the Battle of Falkirk in 1298 to deter charging cavalry; this was a widespread tactic sometimes known as the "crown" formation.

Throwing spears became rarer as the Middle Ages drew on, but survived in the hands of specialists such as the Catalan Almogavars. They were commonly used in Ireland until the end of the 16th century.

Spears began to lose fashion among the infantry during the 14th century, being replaced by pole weapons that combined the thrusting properties of the spear with the cutting properties of the axe, such as the halberd. Where spears were retained they grew in length, eventually evolving into pikes, which would be a dominant infantry weapon in the 16th and 17th centuries.

Cavalry spears were originally the same as infantry spears and were often used with two hands or held with one hand overhead. In the 12th century, after the adoption of stirrups and a high-cantled saddle, the spear became a decidedly more powerful weapon. A mounted knight would secure the lance by holding it with one hand and tucking it under the armpit (the "couched lance" technique) This allowed all the momentum of the horse and knight to be focused on the weapon's tip, whilst still retaining accuracy and control. This use of the spear spurred the development of the lance as a distinct weapon that was perfected in the medieval sport of jousting.

In the 14th century, tactical developments meant that knights and men-at-arms often fought on foot. This led to the practice of shortening the lance to about .) to make it more manageable. As dismounting became commonplace, specialist pole weapons such as the pollaxe were adopted by knights and this practice ceased.

Spears were used first as hunting weapons amongst the ancient Chinese. They became popular as infantry weapons during the Warring States and Qin era, when spearmen were used as especially highly disciplined soldiers in organized group attacks. When used in formation fighting, spearmen would line up their large rectangular or circular shields in a shieldwall manner. The Qin also employed long spears (more akin to a pike) in formations similar to Swiss pikemen in order to ward off cavalry. The Han Empire would use similar tactics as its Qin predecessors. Halberds, polearms, and dagger axes were also common weapons during this time.

Spears were also common weaponry for Warring States, Qin, and Han era cavalry units. During these eras, the spear would develop into a longer lance-like weapon used for cavalry charges.

There are many words in Chinese that would be classified as a spear in English. The "Mao" is the predecessor of the "Qiang". The first bronze "Mao" appeared in the Shang dynasty. This weapon was less prominent on the battlefield than the "ge" (dagger-axe). In some archaeological examples two tiny holes or ears can be found in the blade of the spearhead near the socket, these holes were presumably used to attach tassels, much like modern day wushu spears.

In the early Shang, the "Mao" appeared to have a relatively short shaft as well as a relatively narrow shaft as opposed to "Mao" in the later Shang and Western Zhou period. Some "Mao" from this era are heavily decorated as is evidenced by a Warring States period "Mao" from the Ba Shu area.

In the Han dynasty the "Mao" and the "Ji" (戟 "Ji" can be loosely defined as a halberd) rose to prominence in the military. Interesting to note is that the amount of iron Mao-heads found exceeds the number of bronze heads. By the end of the Han dynasty (Eastern Han) the process of replacement of the iron "Mao" had been completed and the bronze "Mao" had been rendered completely obsolete. After the Han dynasty toward the Sui and Tang dynasties the "Mao" used by cavalry were fitted with much longer shafts, as is mentioned above. During this era, the use of the "Shuo" (矟) was widespread among the footmen. The "Shuo" can be likened to a pike or simply a long spear.

After the Tang dynasty, the popularity of the "Mao" declined and was replaced by the "Qiang" (枪). The Tang dynasty divided the "Qiang" in four categories: "一曰漆枪， 二曰木枪， 三曰白杆枪， 四曰扑头枪。” Roughly translated the four categories are: Qi (a kind of wood) Spears, Wooden Spears, Bai Gan (A kind of wood) Spears and Pu Tou Qiang. The Qiang that were produced in the Song and Ming dynasties consisted of four major parts: Spearhead, Shaft, End Spike and Tassel. The types of Qiang that exist are many. Among the types there are cavalry Qiang that were the length of one "zhang" (eleven feet and nine inches or 3.58 m), Litte-Flower Spears (Xiao Hua Qiang 小花枪) that are the length of one person and their arm extended above his head, double hooked spears, single hooked spears, ringed spears and many more.

There is some confusion as to how to distinguish the "Qiang" from the "Mao", as they are obviously very similar. Some people say that a "Mao" is longer than a "Qiang", others say that the main difference is between the stiffness of the shaft, where the "Qiang" would be flexible and the "Mao" would be stiff. Scholars seem to lean toward the latter explanation more than the former. Because of the difference in the construction of the "Mao" and the "Qiang", the usage is also different, though there is no definitive answer as to what exactly the differences are between the "Mao" and the "Qiang".

Spears in the Indian society were used both in missile and non-missile form, both by cavalry and foot-soldiers. Mounted spear-fighting was practiced using with a ten-foot, ball-tipped wooden lance called a "bothati", the end of which was covered in dye so that hits may be confirmed. Spears were constructed from a variety of materials such as the "sang" made completely of steel, and the "ballam" which had a bamboo shaft. The Rajputs wielded a type of spear for infantrymen which had a club integrated into the spearhead, and a pointed butt end. Other spears had forked blades, several spear-points, and numerous other innovations. One particular spear unique to India was the "vita" or corded lance. Used by the Maratha army, it had a rope connecting the spear with the user's wrist, allowing the weapon to be thrown and pulled back. The "Vel" is a type of spear or lance, originated in Southern India, primarily used by Tamils.

The hoko spear was used in ancient Japan sometime between the Yayoi period and the Heian period, but it became unpopular as early samurai often acted as horseback archers. Medieval Japan employed spears again for infantrymen to use, but it was not until the 11th century in that samurai began to prefer spears over bows. Several polearms were used in the Japanese theatres; the naginata was a glaive-like weapon with a long, curved blade popularly among the samurai and the Buddhist warrior-monks, often used against cavalry; the yari was a longer polearm, with a straight-bladed spearhead, which became the weapon of choice of both the samurai and the ashigaru (footmen) during the Warring States Era; the horseback samurai used shorter yari for his single-armed combat; on the other hand, ashigaru infantries used long yari (similar with European pike) for their massed combat formation.

Filipino spears (sibat) were used as both a weapon and a tool throughout the Philippines. It is also called a "bangkaw" (after the Bankaw Revolt.), "sumbling" or "palupad" in the islands of Visayas and Mindanao. Sibat are typically made from rattan, either with a sharpened tip or a head made from metal. These heads may either be single-edged, double-edged or barbed. Styles vary according to function and origin. For example, a sibat designed for fishing may not be the same as those used for hunting.

The spear was used as the primary weapon in expeditions and battles against neighbouring island kingdoms and it became famous during the 1521 Battle of Mactan, where the chieftain Lapu Lapu of Cebu fought against Spanish forces led by Ferdinand Magellan who was subsequently killed.

As advanced metallurgy was largely unknown in pre-Columbian America outside of Western Mexico and South America, most weapons in Meso-America were made of wood or obsidian. This did not mean that they were less lethal, as obsidian may be sharpened to become many times sharper than steel. Meso-American spears varied greatly in shape and size. While the Aztecs preferred the sword-like macuahuitl for fighting, the advantage of a far-reaching thrusting weapon was recognised, and a large portion of the army would carry the tepoztopilli into battle. The tepoztopilli was a pole-arm, and to judge from depictions in various Aztec codices, it was roughly the height of a man, with a broad wooden head about twice the length of the users' palm or shorter, edged with razor-sharp obsidian blades which were deeply set in grooves carved into the head, and cemented in place with bitumen or plant resin as an adhesive. The tepoztopilli was able both to thrust and slash effectively.

Throwing spears also were used extensively in Meso-American warfare, usually with the help of an atlatl. Throwing spears were typically shorter and more stream-lined than the tepoztopilli, and some had obsidian edges for greater penetration.

Typically, most spears made by Native Americans were created with materials surrounded by their communities. Usually, the shaft of the spear was made with a wooden stick while the head of the spear was fashioned from arrowheads, pieces of metal such as copper, or a bone that had been sharpened. Spears were a preferred weapon by many since it was inexpensive to create, could more easily be taught to others, and could be made quickly and in large quantities.

Native Americans used the Buffalo Pound method to kill buffalo, which required a hunter to dress as a buffalo and lure one into a ravine where other hunters were hiding. Once the buffalo appeared, the other hunters would kill him with spears. A variation of this technique, called the Buffalo Jump, was when a runner would lead the animals towards a cliff. As the buffalo got close to the cliff, other members of the tribe would jump out from behind rocks or trees and scare the buffalo over the cliff. Other hunters would be waiting at the bottom of the cliff to spear the animal to death.

The development of both the long, two-handed pike and gunpowder in Renaissance Europe saw an ever-increasing focus on integrated infantry tactics. Those infantry not armed with these weapons carried variations on the pole-arm, including the halberd and the bill. Ultimately, the spear proper was rendered obsolete on the battlefield. Its last flowering was the half-pike or spontoon, a shortened version of the pike carried by officers and NCOs. While originally a weapon, this came to be seen more as a badge of office, or "leading staff" by which troops were directed. The half-pike, sometimes known as a boarding pike, was also used as a weapon on board ships until the 19th century.

At the start of the Renaissance, cavalry remained predominantly lance-armed; gendarmes with the heavy knightly lance and lighter cavalry with a variety of lighter lances. By the 1540s, however, pistol-armed cavalry called reiters were beginning to make their mark. Cavalry armed with pistols and other lighter firearms, along with a sword, had virtually replaced lance armed cavalry in Western Europe by the beginning of the 17th century.

One of the earliest forms of killing prey for humans, hunting game with a spear and spear fishing continues to this day as both a means of catching food and as a cultural activity. Some of the most common prey for early humans were mega fauna such as mammoths which were hunted with various kinds of spear. One theory for the Quaternary extinction event was that most of these animals were hunted to extinction by humans with spears. Even after the invention of other hunting weapons such as the bow the spear continued to be used, either as a projectile weapon or used in the hand as was common in boar hunting.


Spear hunting fell out of favour in most of Europe in the 18th century, but continued in Germany, enjoying a revival in the 1930s. Spear hunting is still practiced in the United States. Animals taken are primarily wild boar and deer, although trophy animals such as cats and big game as large as a Cape Buffalo are hunted with spears. Alligator are hunted in Florida with a type of harpoon.

Like many weapons, a spear may also be a symbol of power. In the Chinese martial arts community, the Chinese spear (Qiang 槍) is popularly known as the "king of weapons".

The Celts would symbolically destroy a dead warrior's spear either to prevent its use by another or as a sacrificial offering.

In classical Greek mythology Zeus' bolts of lightning may be interpreted as a symbolic spear. Some would carry that interpretation to the spear that frequently is associated with Athena, interpreting her spear as a symbolic connection to some of Zeus' power beyond the Aegis once he rose to replacing other deities in the pantheon. Athena was depicted with a spear prior to that change in myths, however. Chiron's wedding-gift to Peleus when he married the nymph Thetis in classical Greek mythology, was an ashen spear as the nature of ashwood with its straight grain made it an ideal choice of wood for a spear.

The Romans and their early enemies would force prisoners to walk underneath a 'yoke of spears', which humiliated them. The yoke would consist of three spears, two upright with a third tied between them at a height which made the prisoners stoop. It has been suggested that the arrangement has a magical origin, a way to trap evil spirits. The word "subjugate" has its origins in this practice (from Latin "sub" = under, "jugum" = yoke).
In Norse mythology, the god Odin's spear (named Gungnir) was made by the sons of Ivaldi. It had the special property that it never missed its mark. During the War with the Vanir, Odin symbolically threw Gungnir into the Vanir host. This practice of symbolically casting a spear into the enemy ranks at the start of a fight was sometimes used in historic clashes, to seek Odin's support in the coming battle. In Wagner's opera "Siegfried", the haft of Gungnir is said to be from the "World-Tree" Yggdrasil.

Other spears of religious significance are the Holy Lance and the Lúin of Celtchar, believed by some to have vast mystical powers.

Sir James George Frazer in "The Golden Bough" noted the phallic nature of the spear and suggested that in the Arthurian legends the spear or lance functioned as a symbol of male fertility, paired with the Grail (as a symbol of female fertility).

The Hindu god of war Murugan is worshipped by Tamils in the form of the spear called "Vel", which is his primary weapon.

The term "spear" is also used (in a somewhat archaic manner) to describe the male line of a family, as opposed to the distaff or female line.



Related weapons:


</doc>
<doc id="29236" url="https://en.wikipedia.org/wiki?curid=29236" title="Sigrid Undset">
Sigrid Undset

Sigrid Undset (20 May 1882 – 10 June 1949) was a Norwegian novelist who was awarded the Nobel Prize for Literature in 1928.

Undset was born in Kalundborg, Denmark, but her family moved to Norway when she was two years old. In 1924, she converted to Catholicism. She fled Norway for the United States in 1940 because of her opposition to Nazi Germany and the German invasion and occupation of Norway, but returned after World War II ended in 1945.

Her best-known work is "Kristin Lavransdatter", a trilogy about life in Norway in the Middle Ages, portrayed through the experiences of a woman from birth until death. Its three volumes were published between 1920 and 1922.

Sigrid Undset was born on 20 May 1882 in the small town of Kalundborg, Denmark, at the childhood home of her mother, Charlotte Undset (1855–1939, née Anna Maria Charlotte Gyth). Undset was the eldest of three daughters. She and her family moved to Norway when she was two.

She grew up in the Norwegian capital, Oslo (or Kristiania, as it was known until 1925). When she was only 11 years old, her father, the Norwegian archaeologist Ingvald Martin Undset (1853–1893), died at the age of 40 after a long illness.

The family's economic situation meant that Undset had to give up hope of a university education and after a one-year secretarial course she obtained work at the age of 16 as a secretary with an engineering company in Kristiania, a post she was to hold for 10 years.

She joined the Norwegian Authors' Union in 1907 and from 1933 through 1935 headed its Literary Council, eventually serving as the union's chairman from 1936 until 1940.

While employed at office work, Undset wrote and studied. She was 16 years old when she made her first attempt at writing a novel set in the Nordic Middle Ages. The manuscript, a historical novel set in medieval Denmark, was ready by the time she was 22. It was turned down by the publishing house.

Nonetheless, two years later, she completed another manuscript, much less voluminous than the first at only 80 pages. She had put aside the Middle Ages and had instead produced a realistic description of a woman with a middle-class background in contemporary Kristiania. This book was also refused by the publishers at first but it was subsequently accepted. The title was "Fru Marta Oulie", and the opening sentence (the words of the book's main character) scandalised readers: "I have been unfaithful to my husband".

Thus, at the age of 25, Undset made her literary debut with a short realistic novel on adultery, set against a contemporary background. It created a stir, and she found herself ranked as a promising young author in Norway. During the years up to 1919, Undset published a number of novels set in contemporary Kristiania. Her contemporary novels of the period 1907–1918 are about the city and its inhabitants. They are stories of working people, of trivial family destinies, of the relationship between parents and children. Her main subjects are women and their love. Or, as she herself put it—in her typically curt and ironic manner—"the immoral kind" (of love).

This realistic period culminated in the novels "Jenny" (1911) and "Vaaren" (Spring) (1914). The first is about a woman painter who, as a result of romantic crises, believes that she is wasting her life, and, in the end, commits suicide. The other tells of a woman who succeeds in saving both herself and her love from a serious matrimonial crisis, finally creating a secure family. These books placed Undset apart from the incipient women's emancipation movement in Europe.

Undset's books sold well from the start, and, after the publication of her third book, she left her office job and prepared to live on her income as a writer. Having been granted a writer's scholarship, she set out on a lengthy journey in Europe. After short stops in Denmark and Germany, she continued to Italy, arriving in Rome in December 1909, where she remained for nine months. Undset's parents had had a close relationship with Rome, and, during her stay there, she followed in their footsteps. The encounter with Southern Europe meant a great deal to her; she made friends within the circle of Scandinavian artists and writers in Rome.

In Rome, Undset met Anders Castus Svarstad, a Norwegian painter, whom she married almost three years later. She was 30; Svarstad was nine years older, married, and had a wife and three children in Norway. It was nearly three years before Svarstad got his divorce from his first wife.

Undset and Svarstad were married in 1912 and went to stay in London for six months. From London, they returned to Rome, where their first child was born in January 1913. A boy, he was named after his father. In the years up to 1919, she had another child, and the household also took in Svarstad's three children from his first marriage. These were difficult years: her second child, a girl, was mentally handicapped, as was one of Svarstad's sons by his first wife.

She continued writing, finishing her last realistic novels and collections of short stories. She also entered the public debate on topical themes: women's emancipation and other ethical and moral issues. She had considerable polemical gifts, and was critical of emancipation as it was developing, and of the moral and ethical decline she felt was threatening in the wake of the First World War.

In 1919, she moved to Lillehammer, a small town in the Gudbrand Valley in southeast Norway, taking her two children with her. She was then expecting her third child. The intention was that she should take a rest at Lillehammer and move back to Kristiania as soon as Svarstad had their new house in order. However, the marriage broke down and a divorce followed. In August 1919, she gave birth to her third child, at Lillehammer. She decided to make Lillehammer her home, and within two years, Bjerkebæk, a large house of traditional Norwegian timber architecture, was completed, along with a large fenced garden with views of the town and the villages around. Here she was able to retreat and concentrate on her writing.

After the birth of her third child, and with a secure roof over her head, Undset started a major project: "Kristin Lavransdatter". She was at home in the subject matter, having written a short novel at an earlier stage about a period in Norwegian history closer to the Pre-Christian era. She had also published a Norwegian retelling of the Arthurian legends. She had studied Old Norse manuscripts and Medieval chronicles and visited and examined Medieval churches and monasteries, both at home and abroad. She was now an authority on the period she was portraying and a very different person from the 22-year-old who had written her first novel about the Middle Ages. What had happened to her in the meantime has to do with more than history and literature; it has just as much to do with her development as a person. She had experienced love and passion. She had been in despair over a sick world in the throes of the bloodbath of the First World War. When she started on "Kristin Lavransdatter" in 1919, she knew what life was about.

It was only after the end of her marriage that Undset grew mature enough to write her masterpiece. In the years between 1920 and 1927, she first published the three-volume "Kristin", and then the 4-volume "Olav" (Audunssøn), swiftly translated into English as "The Master of Hestviken". Simultaneously with this creative process, she was engaged in trying to find meaning in her own life, finding the answer in God.

Undset experimented with modernist tropes such as stream of consciousness in her novel, although the original English translation by Charles Archer excised many of these passages. In 1997, the first volume of Tiina Nunnally's new translation of the work won the PEN/Faulkner Award for Fiction in the category of translation. The names of each volume were translated by Archer as "The Bridal Wreath", "The Mistress of Husaby", and "The Cross", and by Nunnally as "The Wreath", "The Wife", and "The Cross".

Both Undset's parents were atheists and, although, in accord with the norm of the day, she and her two younger sisters were baptised and with their mother regularly attended the local Lutheran church, the milieu in which they were raised was a thoroughly secular one. Undset spent much of her life as an agnostic, but marriage and the outbreak of the First World War were to change her attitudes. During those difficult years she experienced a crisis of faith, almost imperceptible at first, then increasingly strong. The crisis led her from clear agnostic skepticism, by way of painful uneasiness about the ethical decline of the age, towards Christianity.

In all her writing, one senses an observant eye for the mystery of life and for that which cannot be explained by reason or the human intellect. At the back of her sober, almost brutal realism, there is always an inkling of something unanswerable. At any rate, this crisis radically changed her views and ideology. Whereas she had once believed that man created God, she eventually came to believe that God created man.

However, she did not turn to the established Lutheran Church of Norway, where she had been nominally reared. She was received into the Catholic Church in November 1924, after thorough instruction from the Catholic priest in her local parish. She was 42 years old. She subsequently became a lay Dominican.

It is noteworthy that "The Master of Hestviken", written immediately after Undset's conversion, takes place in a historical period when Norway was Catholic, that it has very religious themes of the main character's relations with God and his deep feeling of sin, and that the Medieval Catholic Church is presented in a favorable light, with virtually all clergy and monks in the series being positive characters.

In Norway, Undset's conversion to Catholicism was not only considered sensational; it was scandalous. It was also noted abroad, where her name was becoming known through the international success of "Kristin Lavransdatter". At the time, there were very few practicing Catholics in Norway, which was an almost exclusively Lutheran country. Anti-Catholicism was widespread not only among the Lutheran clergy, but through large sections of the population. Likewise, there was just as much anti-Catholic scorn among the Norwegian intelligentsia, many of whom were adherents of socialism and communism The attacks against her faith and character were quite vicious at times, with the result that Undset's literary gifts were aroused in response. For many years, she participated in the public debate, going out of her way to defend the Catholic Church. In response, she was swiftly dubbed "The Mistress of Bjerkebæk" and "The Catholic Lady".

At the end of this creative eruption, Undset entered calmer waters. After 1929, she completed a series of novels set in contemporary Oslo, with a strong Catholic element. She selected her themes from the small Catholic community in Norway. But here also, the main theme is love. She also published a number of weighty historical works which put the history of Norway into a sober perspective. In addition, she translated several Icelandic sagas into Modern Norwegian and published a number of literary essays, mainly on English literature, of which a long essay on the Brontë sisters, and one on D. H. Lawrence, are especially worth mentioning.

In 1934, she published "Eleven Years Old", an autobiographical work. With a minimum of camouflage, it tells the story of her own childhood in Kristiania, of her home, rich in intellectual values and love, and of her sick father. It is one of the most fetching Norwegian books ever written about a little girl.

At the end of the 1930s, she commenced work on a new historical novel set in 18th century Scandinavia. Only the first volume, "Madame Dorthea", was published, in 1939. The Second World War broke out that same year and proceeded to break her, both as a writer and as a woman. She never completed her new novel. When Joseph Stalin's invasion of Finland touched off the Winter War, Undset supported the Finnish war effort by donating her Nobel Prize on 25 January 1940.

When Germany invaded Norway in April 1940, Undset was forced to flee. She had strongly criticised Hitler since the early 1930s, and, from an early date, her books were banned in Nazi Germany. She had no wish to become a target of the Gestapo and fled to neutral Sweden. Her eldest son, Second Lieutenant Anders Svarstad of the Norwegian Army, was killed in action at the age of 27, on 27 April 1940, in an engagement with German troops at Segalstad Bridge in Gausdal.

Undset's sick daughter had died shortly before the outbreak of the war. Bjerkebæk was requisitioned by the Wehrmacht, and used as officers' quarters throughout the Occupation of Norway.

In 1940, Undset and her younger son left neutral Sweden for the United States. There, she untiringly pleaded her occupied country's cause and that of Europe's Jews in writings, speeches and interviews. She lived in Brooklyn Heights, New York. She was active in St. Ansgar's Scandinavian Catholic League and wrote several articles for its bulletin. She also traveled to Florida, where she became close friends with novelist Marjorie Kinnan Rawlings.

Following the German execution of the Danish Lutheran pastor Kaj Munk on 4 January 1944, the Danish resistance newspaper "De frie Danske" printed condemning articles from influential Scandinavians, including Undset.

Undset returned to Norway after the liberation in 1945. She lived another four years but never published another word. Undset died at 67 in Lillehammer, Norway, where she had lived from 1919 through 1940. She was buried in the village of Mesnali, 15 kilometers east of Lillehammer, where also her daughter and the son who died in battle are remembered. The grave is recognizable by three black crosses.







</doc>
<doc id="29238" url="https://en.wikipedia.org/wiki?curid=29238" title="Systems theory">
Systems theory

Systems theory is the interdisciplinary study of systems. A system is a cohesive conglomeration of interrelated and interdependent parts which can be natural or human-made. Every system is bounded by space and time, influenced by its environment, defined by its structure and purpose, and expressed through its functioning. A system may be more than the sum of its parts if it expresses synergy or emergent behavior. 

Changing one part of a system may affect other parts or the whole system. It may be possible to predict these changes in patterns of behavior. For systems that learn and adapt, the growth and the degree of adaptation depend upon how well the system is engaged with its environment. Some systems support other systems, maintaining the other system to prevent failure. The goals of systems theory are to model a system's dynamics, constraints, conditions, and to elucidate principles (such as purpose, measure, methods, tools) that can be discerned and applied to other systems at every level of nesting, and in a wide range of fields for achieving optimized equifinality.

General systems theory is about developing broadly applicable concepts and principles, as opposed to concepts and principles specific to one domain of knowledge. It distinguishes dynamic or active systems from static or passive systems. Active systems are activity structures or components that interact in behaviours and processes. Passive systems are structures and components that are being processed. For example, a program is passive when it is a disc file and active when it runs in memory. The field is related to systems thinking, machine logic, and systems engineering.


The term "general systems theory" originates from Bertalanffy's general systems theory (GST). His ideas were adopted by others including Kenneth E. Boulding, William Ross Ashby and Anatol Rapoport working in mathematics, psychology, biology, game theory, and social network analysis.

In sociology, systems thinking started earlier, in the 19th century. Stichweh states: "... Since its beginnings the social sciences were an important part of the establishment of systems theory... the two most influential suggestions were the comprehensive sociological versions of systems theory which were proposed by Talcott Parsons since the 1950s and by Niklas Luhmann since the 1970s." References include Parsons' action theory and Luhmann's social systems theory.

Elements of systems thinking can also be seen in the work of James Clerk Maxwell, in particular control theory.

Systems theory is manifest in the work of practitioners in many disciplines, for example the works of biologist Ludwig von Bertalanffy, linguist Béla H. Bánáthy, sociologist Talcott Parsons, and in the study of ecological systems by Howard T. Odum, Eugene Odum and is Fritjof Capra's study of organizational theory, and in the study of management by Peter Senge, in interdisciplinary areas such as Human Resource Development in the works of Richard A. Swanson, and in the works of educators Debora Hammond and Alfonso Montuori. 

As a transdisciplinary, interdisciplinary, and multiperspectival endeavor, systems theory brings together principles and concepts from ontology, the philosophy of science, physics, computer science, biology and engineering as well as geography, sociology, political science, psychotherapy (especially family systems therapy), and economics. Systems theory promotes dialogue between autonomous areas of study as well as within systems science itself.

In this respect, with the possibility of misinterpretations, von Bertalanffy believed a general theory of systems "should be an important regulative device in science", to guard against superficial analogies that "are useless in science and harmful in their practical consequences". Others remain closer to the direct systems concepts developed by the original theorists. For example, Ilya Prigogine, of the Center for Complex Quantum Systems at the University of Texas, Austin, has studied emergent properties, suggesting that they offer analogues for living systems. The theories of autopoiesis of Francisco Varela and Humberto Maturana represent further developments in this field. Important names in contemporary systems science include Russell Ackoff, Ruzena Bajcsy, Béla H. Bánáthy, Gregory Bateson, Anthony Stafford Beer, Peter Checkland, Barbara Grosz, Brian Wilson, Robert L. Flood, Allenna Leonard, Radhika Nagpal, Fritjof Capra, Warren McCulloch, Kathleen Carley, Michael C. Jackson, Katia Sycara, and Edgar Morin among others.

With the modern foundations for a general theory of systems following World War I, Ervin Laszlo, in the preface for Bertalanffy's book: "Perspectives on General System Theory", points out that the translation of "general system theory" from German into English has "wrought a certain amount of havoc":

A system in this frame of reference can contain regularly interacting or interrelating groups of activities. For example, in noting the influence in organizational psychology as the field evolved from "an individually oriented industrial psychology to a systems and developmentally oriented organizational psychology", some theorists recognize that organizations have complex social systems; separating the parts from the whole reduces the overall effectiveness of organizations. This difference, from conventional models that center on individuals, structures, departments and units, separates in part from the whole, instead of recognizing the interdependence between groups of individuals, structures and processes that enable an organization to function. Laszlo explains that the new systems view of organized complexity went "one step beyond the Newtonian view of organized simplicity" which reduced the parts from the whole, or understood the whole without relation to the parts. The relationship between organisations and their environments can be seen as the foremost source of complexity and interdependence. In most cases, the whole has properties that cannot be known from analysis of the constituent elements in isolation. Béla H. Bánáthy, who argued—along with the founders of the systems society—that "the benefit of humankind" is the purpose of science, has made significant and far-reaching contributions to the area of systems theory. For the Primer Group at ISSS, Bánáthy defines a perspective that iterates this view:

Similar ideas are found in learning theories that developed from the same fundamental concepts, emphasising how understanding results from knowing concepts both in part and as a whole. In fact, Bertalanffy's organismic psychology paralleled the learning theory of Jean Piaget. Some consider interdisciplinary perspectives critical in breaking away from industrial age models and thinking, wherein history represents history and math represents math, while the arts and sciences specialization remain separate and many treat teaching as behaviorist conditioning. The contemporary work of Peter Senge provides detailed discussion of the commonplace critique of educational systems grounded in conventional assumptions about learning, including the problems with fragmented knowledge and lack of holistic learning from the "machine-age thinking" that became a "model of school separated from daily life". In this way some systems theorists attempt to provide alternatives to, and evolved ideation from orthodox theories which have grounds in classical assumptions, including individuals such as Max Weber and Émile Durkheim in sociology and Frederick Winslow Taylor in scientific management. The theorists sought holistic methods by developing systems concepts that could integrate with different areas.

Some may view the contradiction of reductionism in conventional theory (which has as its subject a single part) as simply an example of changing assumptions. The emphasis with systems theory shifts from parts to the organization of parts, recognizing interactions of the parts as not static and constant but dynamic processes. Some questioned the conventional closed systems with the development of open systems perspectives. The shift originated from absolute and universal authoritative principles and knowledge to relative and general conceptual and perceptual knowledge and still remains in the tradition of theorists that sought to provide means to organize human life. In other words, theorists rethought the preceding history of ideas; they did not lose them. Mechanistic thinking was particularly critiqued, especially the industrial-age mechanistic metaphor for the mind from interpretations of Newtonian mechanics by Enlightenment philosophers and later psychologists that laid the foundations of modern organizational theory and management by the late 19th century.

System dynamics is an approach to understanding the nonlinear behaviour of complex systems over time using stocks, flows, internal feedback loops, and time delays.

Systems biology is a movement that draws on several trends in bioscience research. Proponents describe systems biology as a biology-based inter-disciplinary study field that focuses on complex interactions in biological systems, claiming that it uses a new perspective (holism instead of reduction). Particularly from the year 2000 onwards, the biosciences use the term widely and in a variety of contexts. An often stated ambition of systems biology is the modelling and discovery of emergent properties which represents properties of a system whose theoretical description requires the only possible useful techniques to fall under the remit of systems biology. It is thought that Ludwig von Bertalanffy may have created the term systems biology in 1928.

Systems ecology is an interdisciplinary field of ecology, a subset of Earth system science, that takes a holistic approach to the study of ecological systems, especially ecosystems. Systems ecology can be seen as an application of general systems theory to ecology. Central to the systems ecology approach is the idea that an ecosystem is a complex system exhibiting emergent properties. Systems ecology focuses on interactions and transactions within and between biological and ecological systems, and is especially concerned with the way the functioning of ecosystems can be influenced by human interventions. It uses and extends concepts from thermodynamics and develops other macroscopic descriptions of complex systems.

Systems engineering is an interdisciplinary approach and means for enabling the realisation and deployment of successful systems. It can be viewed as the application of engineering techniques to the engineering of systems, as well as the application of a systems approach to engineering efforts. Systems engineering integrates other disciplines and specialty groups into a team effort, forming a structured development process that proceeds from concept to production to operation and disposal. Systems engineering considers both the business and the technical needs of all customers, with the goal of providing a quality product that meets the user's needs.

Systems psychology is a branch of psychology that studies human behaviour and experience in complex systems. It received inspiration from systems theory and systems thinking, as well as the basics of theoretical work from Roger Barker, Gregory Bateson, Humberto Maturana and others. It makes an approach in psychology in which groups and individuals receive consideration as systems in homeostasis. Systems psychology "includes the domain of engineering psychology, but in addition seems more concerned with societal systems and with the study of motivational, affective, cognitive and group behavior that holds the name engineering psychology." In systems psychology, "characteristics of organizational behaviour, for example individual needs, rewards, expectations, and attributes of the people interacting with the systems, considers this process in order to create an effective system".

Whether considering the first systems of written communication with Sumerian cuneiform to Mayan numerals, or the feats of engineering with the Egyptian pyramids, systems thinking can date back to antiquity. Differentiated from Western rationalist traditions of philosophy, C. West Churchman often identified with the I Ching as a systems approach sharing a frame of reference similar to pre-Socratic philosophy and Heraclitus. Von Bertalanffy traced systems concepts to the philosophy of G.W. Leibniz and Nicholas of Cusa's "coincidentia oppositorum". While modern systems can seem considerably more complicated, today's systems may embed themselves in history.

Figures like James Joule and Sadi Carnot represent an important step to introduce the "systems approach" into the (rationalist) hard sciences of the 19th century, also known as the energy transformation. Then, the thermodynamics of this century, by Rudolf Clausius, Josiah Gibbs and others, established the "system" reference model as a formal scientific object.

The Society for General Systems Research specifically catalyzed systems theory as an area of study, which developed following the World Wars from the work of Ludwig von Bertalanffy, Anatol Rapoport, Kenneth E. Boulding, William Ross Ashby, Margaret Mead, Gregory Bateson, C. West Churchman and others in the 1950s. Cognizant of advances in science that questioned classical assumptions in the organizational sciences, Bertalanffy's idea to develop a theory of systems began as early as the interwar period, publishing "An Outline for General Systems Theory" in the "British Journal for the Philosophy of Science", Vol 1, No. 2, by 1950. Where assumptions in Western science from Greek thought with Plato and Aristotle to Newton's "Principia" have historically influenced all areas from the hard to social sciences (see David Easton's seminal development of the "political system" as an analytical construct), the original theorists explored the implications of twentieth century advances in terms of systems.

People have studied subjects like complexity, self-organization, connectionism and adaptive systems in the 1940s and 1950s. In fields like cybernetics, researchers such as Norbert Wiener, William Ross Ashby, John von Neumann and Heinz von Foerster, examined complex systems mathematically. John von Neumann discovered cellular automata and self-reproducing systems, again with only pencil and paper. Aleksandr Lyapunov and Jules Henri Poincaré worked on the foundations of chaos theory without any computer at all. At the same time Howard T. Odum, known as a radiation ecologist, recognized that the study of general systems required a language that could depict energetics, thermodynamics and kinetics at any system scale. Odum developed a general system, or universal language, based on the circuit language of electronics, to fulfill this role, known as the Energy Systems Language. Between 1929-1951, Robert Maynard Hutchins at the University of Chicago had undertaken efforts to encourage innovation and interdisciplinary research in the social sciences, aided by the Ford Foundation with the interdisciplinary Division of the Social Sciences established in 1931. Numerous scholars had actively engaged in these ideas before (Tectology by Alexander Bogdanov, published in 1912-1917, is a remarkable example), but in 1937, von Bertalanffy presented the general theory of systems at a conference at the University of Chicago.

The systems view was based on several fundamental ideas. First, all phenomena can be viewed as a web of relationships among elements, or a system. Second, all systems, whether electrical, biological, or social, have common patterns, behaviors, and properties that the observer can analyze and use to develop greater insight into the behavior of complex phenomena and to move closer toward a unity of the sciences. System philosophy, methodology and application are complementary to this science. By 1956, theorists established the Society for General Systems Research, which they renamed the International Society for Systems Science in 1988. The Cold War affected the research project for systems theory in ways that sorely disappointed many of the seminal theorists. Some began to recognize that theories defined in association with systems theory had deviated from the initial General Systems Theory (GST) view. The economist Kenneth Boulding, an early researcher in systems theory, had concerns over the manipulation of systems concepts. Boulding concluded from the effects of the Cold War that abuses of power always prove consequential and that systems theory might address such issues. Since the end of the Cold War, a renewed interest in systems theory emerged, combined with efforts to strengthen an ethical view on the subject.

Many early systems theorists aimed at finding a general systems theory that could explain all systems in all fields of science. The term goes back to Bertalanffy's book titled "General System Theory: Foundations, Development, Applications" from 1968. He developed the "allgemeine Systemlehre" (general systems theory) first via lectures beginning in 1937 and then via publications beginning in 1946.

Von Bertalanffy's objective was to bring together under one heading the organismic science he had observed in his work as a biologist. His desire was to use the word "system" for those principles that are common to systems in general. In GST, he writes:

Ervin Laszlo in the preface of von Bertalanffy's book "Perspectives on General System Theory":

Ludwig von Bertalanffy outlines systems inquiry into three major domains: Philosophy, Science, and Technology. In his work with the Primer Group, Béla H. Bánáthy generalized the domains into four integratable domains of systemic inquiry:

These operate in a recursive relationship, he explained. Integrating Philosophy and Theory as Knowledge, and Method and Application as action, Systems Inquiry then is knowledgeable action.

Cybernetics is the study of the communication and control of regulatory feedback both in living and lifeless systems (organisms, organizations, machines), and in combinations of those. Its focus is how anything (digital, mechanical or biological) controls its behavior, processes information, reacts to information, and changes or can be changed to better accomplish those three primary tasks.

The terms "systems theory" and "cybernetics" have been widely used as synonyms. Some authors use the term "cybernetic" systems to denote a proper subset of the class of general systems, namely those systems that include feedback loops. However Gordon Pask's differences of eternal interacting actor loops (that produce finite products) makes general systems a proper subset of cybernetics. According to Jackson (2000), von Bertalanffy promoted an embryonic form of general system theory (GST) as early as the 1920s and 1930s but it was not until the early 1950s it became more widely known in scientific circles.

Threads of cybernetics began in the late 1800s that led toward the publishing of seminal works (e.g., Wiener's "Cybernetics" in 1948 and von Bertalanffy's "General Systems Theory" in 1968). Cybernetics arose more from engineering fields and GST from biology. If anything it appears that although the two probably mutually influenced each other, cybernetics had the greater influence. Von Bertalanffy (1969) specifically makes the point of distinguishing between the areas in noting the influence of cybernetics: "Systems theory is frequently identified with cybernetics and control theory. This again is incorrect. Cybernetics as the theory of control mechanisms in technology and nature is founded on the concepts of information and feedback, but as part of a general theory of systems;" then reiterates: "the model is of wide application but should not be identified with 'systems theory' in general", and that "warning is necessary against its incautious expansion to fields for which its concepts are not made." (17-23). Jackson (2000) also claims von Bertalanffy was informed by Alexander Bogdanov's three volume "Tectology" that was published in Russia between 1912 and 1917, and was translated into German in 1928. He also states it is clear to Gorelik (1975) that the "conceptual part" of general system theory (GST) had first been put in place by Bogdanov. The similar position is held by Mattessich (1978) and Capra (1996). Ludwig von Bertalanffy never even mentioned Bogdanov in his works, which Capra (1996) finds "surprising".

Cybernetics, catastrophe theory, chaos theory and complexity theory have the common goal to explain complex systems that consist of a large number of mutually interacting and interrelated parts in terms of those interactions. Cellular automata (CA), neural networks (NN), artificial intelligence (AI), and artificial life (ALife) are related fields, but they do not try to describe general (universal) complex (singular) systems. The best context to compare the different "C"-Theories about complex systems is historical, which emphasizes different tools and methodologies, from pure mathematics in the beginning to pure computer science now. Since the beginning of chaos theory when Edward Lorenz accidentally discovered a strange attractor with his computer, computers have become an indispensable source of information. One could not imagine the study of complex systems without the use of computers today.

Complex adaptive systems (CAS) are special cases of complex systems. They are "complex" in that they are diverse and composed of multiple, interconnected elements; they are "adaptive" in that they have the capacity to change and learn from experience. In contrast to control systems in which negative feedback dampens and reverses disequilibria, CAS are often subject to positive feedback, which magnifies and perpetuates changes, converting local irregularities into global features. Another mechanism, Dual-phase evolution arises when connections between elements repeatedly change, shifting the system between phases of variation and selection that reshape the system. Differently from Stafford Beer’s Management Cybernetics, Cultural Agency Theory (CAT) provides a modelling approach to explore predefined contexts and can be adapted to reflect those contexts.

The term "complex adaptive system" was coined at the interdisciplinary Santa Fe Institute (SFI), by John H. Holland, Murray Gell-Mann and others. An alternative conception of complex adaptive (and learning) systems, methodologically at the interface between natural and social science, has been presented by Kristo Ivanov in terms of hypersystems. This concept intends to offer a theoretical basis for understanding and implementing participation of "users", decisions makers, designers and affected actors, in the development or maintenance of self-learning systems.



Organizations


</doc>
<doc id="29240" url="https://en.wikipedia.org/wiki?curid=29240" title="Lists of stars">
Lists of stars

The following are lists of stars. These are astronomical objects that spend some portion of their existence generating energy through thermonuclear fusion.







The following is a list of particularly notable actual or hypothetical stars that have their own articles in Wikipedia, but are not included in the lists above.





</doc>
<doc id="29247" url="https://en.wikipedia.org/wiki?curid=29247" title="Sulfuric acid">
Sulfuric acid

Sulfuric acid (alternative spelling sulphuric acid), also known as vitriol, is a mineral acid composed of the elements sulfur, oxygen and hydrogen, with molecular formula HSO. It is a colorless, odorless, and viscous liquid that is soluble in water and is synthesized in reactions that are highly exothermic.

Its corrosiveness can be mainly ascribed to its strong acidic nature, and, if at a high concentration, its dehydrating and oxidizing properties. It is also hygroscopic, readily absorbing water vapor from the air. Upon contact, sulfuric acid can cause severe chemical burns and even secondary thermal burns; it is very dangerous even at lower concentrations.

Sulfuric acid is a very important commodity chemical, and a nation's sulfuric acid production is a good indicator of its industrial strength. It is widely produced with different methods, such as contact process, wet sulfuric acid process, lead chamber process and some other methods.

Sulfuric acid is also a key substance in the chemical industry. It is most commonly used in fertilizer manufacture, but is also important in mineral processing, oil refining, wastewater processing, and chemical synthesis. It has a wide range of end applications including in domestic acidic drain cleaners, as an electrolyte in lead-acid batteries, in dehydrating a compound, and in various cleaning agents.

Although nearly 100% sulfuric acid solutions can be made, the subsequent loss of at the boiling point brings the concentration to 98.3% acid. The 98.3% grade is more stable in storage, and is the usual form of what is described as "concentrated sulfuric acid". Other concentrations are used for different purposes. Some common concentrations are:
"Chamber acid" and "tower acid" were the two concentrations of sulfuric acid produced by the lead chamber process, chamber acid being the acid produced in the lead chamber itself (<70% to avoid contamination with nitrosylsulfuric acid) and tower acid being the acid recovered from the bottom of the Glover tower. They are now obsolete as commercial concentrations of sulfuric acid, although they may be prepared in the laboratory from concentrated sulfuric acid if needed. In particular, "10M" sulfuric acid (the modern equivalent of chamber acid, used in many titrations) is prepared by slowly adding 98% sulfuric acid to an equal volume of water, with good stirring: the temperature of the mixture can rise to 80 °C (176 °F) or higher.

Sulfuric acid reacts with its anhydride, , to form , called "pyrosulfuric acid", "fuming sulfuric acid", "Disulfuric acid" or "oleum" or, less commonly, "Nordhausen acid". Concentrations of oleum are either expressed in terms of % (called % oleum) or as % (the amount made if were added); common concentrations are 40% oleum (109% ) and 65% oleum (114.6% ). Pure is a solid with melting point of 36 °C.

Pure sulfuric acid has a vapor pressure of <0.001 mmHg at 25 °C and 1 mmHg at 145.8 °C, and 98% sulfuric acid has a <1 mmHg vapor pressure at 40 °C.

Pure sulfuric acid is a viscous clear liquid, like oil, and this explains the old name of the acid ('oil of vitriol').

Commercial sulfuric acid is sold in several different purity grades. Technical grade is impure and often colored, but is suitable for making fertilizer. Pure grades, such as United States Pharmacopeia (USP) grade, are used for making pharmaceuticals and dyestuffs. Analytical grades are also available.

Nine hydrates are known, but four of them were confirmed to be tetrahydrate (HSO·4HO), hemihexahydrate (HSO·HO) and octahydrate (HSO·8HO).

Anhydrous is a very polar liquid, having a dielectric constant of around 100. It has a high electrical conductivity, caused by dissociation through protonating itself, a process known as autoprotolysis.
The equilibrium constant for the autoprotolysis is

The comparable equilibrium constant for water, "K" is 10, a factor of 10 (10 billion) smaller.

In spite of the viscosity of the acid, the effective conductivities of the and ions are high due to an intramolecular proton-switch mechanism (analogous to the Grotthuss mechanism in water), making sulfuric acid a good conductor of electricity. It is also an excellent solvent for many reactions.

Because the hydration reaction of sulfuric acid is highly exothermic, dilution should always be performed by adding the acid to the water rather than the water to the acid. Because the reaction is in an equilibrium that favors the rapid protonation of water, addition of acid to the water ensures that the "acid" is the limiting reagent. This reaction is best thought of as the formation of hydronium ions:

Because the hydration of sulfuric acid is thermodynamically favorable and the affinity of it for water is sufficiently strong, sulfuric acid is an excellent dehydrating agent. Concentrated sulfuric acid has a very powerful dehydrating property, removing water (HO) from other chemical compounds including sugar and other carbohydrates and producing carbon, heat, and steam.

In the laboratory, this is often demonstrated by mixing table sugar (sucrose) into sulfuric acid. The sugar changes from white to dark brown and then to black as carbon is formed. A rigid column of black, porous carbon will emerge as well. The carbon will smell strongly of caramel due to the heat generated.

Similarly, mixing starch into concentrated sulfuric acid will give elemental carbon and water as absorbed by the sulfuric acid (which becomes slightly diluted). The effect of this can be seen when concentrated sulfuric acid is spilled on paper which is composed of cellulose; the cellulose reacts to give a burnt appearance, the carbon appears much as soot would in a fire.
Although less dramatic, the action of the acid on cotton, even in diluted form, will destroy the fabric.

The reaction with copper(II) sulfate can also demonstrate the dehydration property of sulfuric acid. The blue crystal is changed into white powder as water is removed.

As an acid, sulfuric acid reacts with most bases to give the corresponding sulfate. For example, the blue copper salt copper(II) sulfate, commonly used for electroplating and as a fungicide, is prepared by the reaction of copper(II) oxide with sulfuric acid:

Sulfuric acid can also be used to displace weaker acids from their salts. Reaction with sodium acetate, for example, displaces acetic acid, , and forms sodium bisulfate:

Similarly, reacting sulfuric acid with potassium nitrate can be used to produce nitric acid and a precipitate of potassium bisulfate. When combined with nitric acid, sulfuric acid acts both as an acid and a dehydrating agent, forming the nitronium ion , which is important in nitration reactions involving electrophilic aromatic substitution. This type of reaction, where protonation occurs on an oxygen atom, is important in many organic chemistry reactions, such as Fischer esterification and dehydration of alcohols.

When allowed to react with superacids, sulfuric acid can act as a base and be protonated, forming the [HSO] ion. Salt of [HSO] have been prepared using the following reaction in liquid HF:

The above reaction is thermodynamically favored due to the high bond enthalpy of the Si–F bond in the side product. Protonation using simply HF/SbF, however, have met with failure, as pure sulfuric acid undergoes self-ionization to give [HO] ions, which prevents the conversion of HSO to [HSO] by the HF/SbF system:

Dilute sulfuric acid reacts with metals via a single displacement reaction as with other typical acids, producing hydrogen gas and salts (the metal sulfate). It attacks reactive metals (metals at positions above copper in the reactivity series) such as iron, aluminium, zinc, manganese, magnesium, and nickel.

However, concentrated sulfuric acid is a strong oxidizing agent and does not react with metals in the same way as other typical acids. Sulfur dioxide, water and ions are evolved instead of the hydrogen and salts.

It can oxidize non-active metals such as tin and copper, depending upon the temperature.

Lead and tungsten, however, are resistant to sulfuric acid.

Hot concentrated sulfuric acid oxidizes non-metals such as carbon (as bituminous coal) and sulfur.

It reacts with sodium chloride, and gives hydrogen chloride gas and sodium bisulfate:

Benzene undergoes electrophilic aromatic substitution with sulfuric acid to give the corresponding sulfonic acids:

Pure sulfuric acid is not encountered naturally on Earth in anhydrous form, due to its great affinity for water. Dilute sulfuric acid is a constituent of acid rain, which is formed by atmospheric oxidation of sulfur dioxide in the presence of water – i.e., oxidation of sulfurous acid. Sulfur dioxide is the main byproduct produced when sulfur-containing fuels such as coal or oil are burned.

Sulfuric acid is formed naturally by the oxidation of sulfide minerals, such as iron sulfide. The resulting water can be highly acidic and is called acid mine drainage (AMD) or acid rock drainage (ARD). This acidic water is capable of dissolving metals present in sulfide ores, which results in brightly colored, toxic streams. The oxidation of pyrite (iron sulfide) by molecular oxygen produces iron(II), or :

The can be further oxidized to :

The produced can be precipitated as the hydroxide or hydrous iron oxide:

The iron(III) ion ("ferric iron") can also oxidize pyrite:

When iron(III) oxidation of pyrite occurs, the process can become rapid. pH values below zero have been measured in ARD produced by this process.

ARD can also produce sulfuric acid at a slower rate, so that the acid neutralizing capacity (ANC) of the aquifer can neutralize the produced acid. In such cases, the total dissolved solids (TDS) concentration of the water can be increased from the dissolution of minerals from the acid-neutralization reaction with the minerals.

Sulfuric acid is used as a defense by certain marine species, for example, the phaeophyte alga "Desmarestia munda" (order Desmarestiales) concentrates sulfuric acid in cell vacuoles.

In the stratosphere, the atmosphere's second layer that is generally between 10 and 50 km above Earth's surface, sulfuric acid is formed by the oxidation of volcanic sulfur dioxide by the hydroxyl radical:

Because sulfuric acid reaches supersaturation in the stratosphere, it can nucleate aerosol particles and provide a surface for aerosol growth via condensation and coagulation with other water-sulfuric acid aerosols. This results in the stratospheric aerosol layer.

Sulfuric acid is produced in the upper atmosphere of Venus by the Sun's photochemical action on carbon dioxide, sulfur dioxide, and water vapor. Ultraviolet photons of wavelengths less than 169 nm can photodissociate carbon dioxide into carbon monoxide and atomic oxygen. Atomic oxygen is highly reactive. When it reacts with sulfur dioxide, a trace component of the Venusian atmosphere, the result is sulfur trioxide, which can combine with water vapor, another trace component of Venus's atmosphere, to yield sulfuric acid. In the upper, cooler portions of Venus's atmosphere, sulfuric acid exists as a liquid, and thick sulfuric acid clouds completely obscure the planet's surface when viewed from above. The main cloud layer extends from 45–70 km above the planet's surface, with thinner hazes extending as low as 30 km and as high as 90 km above the surface. The permanent Venusian clouds produce a concentrated acid rain, as the clouds in the atmosphere of Earth produce water rain.

The atmosphere exhibits a sulfuric acid cycle. As sulfuric acid rain droplets fall down through the hotter layers of the atmosphere's temperature gradient, they are heated up and release water vapor, becoming more and more concentrated. When they reach temperatures above 300 °C, sulfuric acid begins to decompose into sulfur trioxide and water, both in the gas phase. Sulfur trioxide is highly reactive and dissociates into sulfur dioxide and atomic oxygen, which oxidizes traces of carbon monoxide to form carbon dioxide. Sulfur dioxide and water vapor rise on convection currents from the mid-level atmospheric layers to higher altitudes, where they will be transformed again into sulfuric acid, and the cycle repeats.

Infrared spectra taken by NASA's "Galileo" spacecraft show distinct absorptions on Jupiter's moon Europa that have been attributed to one or more sulfuric acid hydrates. Sulfuric acid in solution with water causes significant freezing-point depression of water's melting point, down to , and this would make the existence of liquid solutions beneath Europa's icy crust more likely. The interpretation of the spectra is somewhat controversial. Some planetary scientists prefer to assign the spectral features to the sulfate ion, perhaps as part of one or more minerals on Europa's surface.

Sulfuric acid is produced from sulfur, oxygen and water via the conventional contact process (DCDA) or the wet sulfuric acid process (WSA).

In the first step, sulfur is burned to produce sulfur dioxide.

This is then oxidized to sulfur trioxide using oxygen in the presence of a vanadium(V) oxide catalyst. This reaction is reversible and the formation of the sulfur trioxide is exothermic.

The sulfur trioxide is absorbed into 97–98% to form oleum (), also known as fuming sulfuric acid. The oleum is then diluted with water to form concentrated sulfuric acid.

Directly dissolving in water is not practical due to the highly exothermic nature of the reaction between sulfur trioxide and water. The reaction forms a corrosive aerosol that is very difficult to separate, instead of a liquid.

In the first step, sulfur is burned to produce sulfur dioxide:

or, alternatively, hydrogen sulfide () gas is incinerated to gas:
This is then oxidized to sulfur trioxide using oxygen with vanadium(V) oxide as catalyst.

The sulfur trioxide is hydrated into sulfuric acid :

The last step is the condensation of the sulfuric acid to liquid 97–98% :

Another method is the less well-known metabisulfite method, in which metabisulfite is placed at the bottom of a beaker, and 12.6 molar concentration hydrochloric acid is added. The resulting gas is bubbled through nitric acid, which will release brown/red vapors. The completion of the reaction is indicated by the ceasing of the fumes. This method does not produce an inseparable mist, which is quite convenient.

Sulfuric acid can be produced in the laboratory by burning sulfur in air and dissolving the gas produced in a hydrogen peroxide solution.

Or alternatively, with stirring, dissolving sulfur dioxide (any source) in an aqueous solution of a certain oxidizing metal salt such as copper (II) or iron (III) chloride:

There are also two other less well-known laboratory methods of producing sulfuric acid, albeit in dilute form and require some extra effort in purification. A solution of copper (II) sulfate can be electrolyzed with a copper cathode and platinum/graphite anode to give spongy copper at cathode and evolution of oxygen gas at anode, the solution diluted sulfuric acid that indicates completed reaction when it turns from blue to clear (production of hydrogen at cathode is another sign):

More costly, dangerous, and troublesome yet novel is the electrobromine method, which employs a mixture of sulfur, water, and hydrobromic acid as the electrolytic solution. The sulfur pushed to bottom of container under the acid solution, then copper cathode and platinum/graphite anode are used with cathode near surface and anode at bottom of electrolyte to apply the current. This may take longer and emits toxic bromine/sulfur bromide vapors, but reactant acid is recyclable, overall only sulfur and water converted to sulfuric acid (omitting losses of acid as vapors):

Prior to 1900, most sulfuric acid was manufactured by the lead chamber process. As late as 1940, up to 50% of sulfuric acid manufactured in the United States was produced by chamber process plants.

In early to mid nineteenth century "vitriol" plants existed, among other places, in Prestonpans in Scotland, Shropshire and the Lagan Valley in County Antrim Ireland where it was used as a bleach for linen. Early bleaching of linen was done using lactic acid from sour milk but this was a slow process and the use of vitriol sped up the bleaching process.

Sulfuric acid is a very important commodity chemical, and indeed, a nation's sulfuric acid production is a good indicator of its industrial strength. World production in the year 2004 was about 180 million tonnes, with the following geographic distribution: Asia 35%, North America (including Mexico) 24%, Africa 11%, Western Europe 10%, Eastern Europe and Russia 10%, Australia and Oceania 7%, South America 7%. Most of this amount (≈60%) is consumed for fertilizers, particularly superphosphates, ammonium phosphate and ammonium sulfates. About 20% is used in chemical industry for production of detergents, synthetic resins, dyestuffs, pharmaceuticals, petroleum catalysts, insecticides and antifreeze, as well as in various processes such as oil well acidicizing, aluminium reduction, paper sizing, water treatment. About 6% of uses are related to pigments and include paints, enamels, printing inks, coated fabrics and paper, and the rest is dispersed into a multitude of applications such as production of explosives, cellophane, acetate and viscose textiles, lubricants, non-ferrous metals, and batteries.

The major use for sulfuric acid is in the "wet method" for the production of phosphoric acid, used for manufacture of phosphate fertilizers. In this method, phosphate rock is used, and more than 100 million tonnes are processed annually. This raw material is shown below as fluorapatite, though the exact composition may vary. This is treated with 93% sulfuric acid to produce calcium sulfate, hydrogen fluoride (HF) and phosphoric acid. The HF is removed as hydrofluoric acid. The overall process can be represented as:

Ammonium sulfate, an important nitrogen fertilizer, is most commonly produced as a byproduct from coking plants supplying the iron and steel making plants. Reacting the ammonia produced in the thermal decomposition of coal with waste sulfuric acid allows the ammonia to be crystallized out as a salt (often brown because of iron contamination) and sold into the agro-chemicals industry.

Another important use for sulfuric acid is for the manufacture of aluminium sulfate, also known as paper maker's alum. This can react with small amounts of soap on paper pulp fibers to give gelatinous aluminium carboxylates, which help to coagulate the pulp fibers into a hard paper surface. It is also used for making aluminium hydroxide, which is used at water treatment plants to filter out impurities, as well as to improve the taste of the water. Aluminium sulfate is made by reacting bauxite with sulfuric acid:

Sulfuric acid is also important in the manufacture of dyestuffs solutions.

The sulfur–iodine cycle is a series of thermo-chemical processes used to obtain hydrogen. It consists of three chemical reactions whose net reactant is water and whose net products are hydrogen and oxygen. Step one of cycle is the Bunsen reaction.

The sulfur and iodine compounds are recovered and reused, hence the consideration of the process as a cycle. This process is endothermic and must occur at high temperatures, so energy in the form of heat has to be supplied.

The sulfur–iodine cycle has been proposed as a way to supply hydrogen for a hydrogen-based economy. It does not require hydrocarbons like current methods of steam reforming. But note that all of the available energy in the hydrogen so produced is supplied by the heat used to make it.

The sulfur–iodine cycle is currently being researched as a feasible method of obtaining hydrogen, but the concentrated, corrosive acid at high temperatures poses currently insurmountable safety hazards if the process were built on a large scale.

Sulfuric acid is used in large quantities by the iron and steelmaking industry to remove oxidation, rust, and scaling from rolled sheet and billets prior to sale to the automobile and major appliances industry. Used acid is often recycled using a spent acid regeneration (SAR) plant. These plants combust spent acid with natural gas, refinery gas, fuel oil or other fuel sources. This combustion process produces gaseous sulfur dioxide () and sulfur trioxide () which are then used to manufacture "new" sulfuric acid. SAR plants are common additions to metal smelting plants, oil refineries, and other industries where sulfuric acid is consumed in bulk, as operating a SAR plant is much cheaper than the recurring costs of spent acid disposal and new acid purchases.

Hydrogen peroxide () can be added to sulfuric acid to produce piranha solution, a powerful but very toxic cleaning solution with which substrate surfaces can be cleaned. Piranha solution is typically used in the microelectronics industry, and also in laboratory settings to clean glassware.

Sulfuric acid is used for a variety of other purposes in the chemical industry. For example, it is the usual acid catalyst for the conversion of cyclohexanone oxime to caprolactam, used for making nylon. It is used for making hydrochloric acid from salt via the Mannheim process. Much is used in petroleum refining, for example as a catalyst for the reaction of isobutane with isobutylene to give isooctane, a compound that raises the octane rating of gasoline (petrol). Sulfuric acid is also often used as a dehydrating or oxidising agent in industrial reactions, such as the dehydration of various sugars to form solid carbon.

Sulfuric acid acts as the electrolyte in lead–acid batteries (lead-acid accumulator):

At anode:

At cathode:

Overall:

Sulfuric acid at high concentrations is frequently the major ingredient in acidic drain cleaners which are used to remove grease, hair, tissue paper, etc. Similar to their alkaline versions, such drain openers can dissolve fats and proteins via hydrolysis. Moreover, as concentrated sulfuric acid has a strong dehydrating property, it can remove tissue paper via dehydrating process as well. Since the acid may react with water vigorously, such acidic drain openers should be added slowly into the pipe to be cleaned.

The study of vitriol, a category of glassy minerals from which the acid can be derived, began in ancient times. Sumerians had a list of types of vitriol that they classified according to the substances' color. Some of the earliest discussions on the origin and properties of vitriol is in the works of the Greek physician Dioscorides (first century AD) and the Roman naturalist Pliny the Elder (23–79 AD). Galen also discussed its medical use. Metallurgical uses for vitriolic substances were recorded in the Hellenistic alchemical works of Zosimos of Panopolis, in the treatise "Phisica et Mystica", and the Leyden papyrus X.

Medieval Islamic era alchemists, Jābir ibn Hayyān (c. 721 – c. 815 AD, also known as Geber), Razi (865 – 925 AD), and Jamal Din al-Watwat (d. 1318, wrote the book "Mabāhij al-fikar wa-manāhij al-'ibar"), included vitriol in their mineral classification lists. Ibn Sina focused on its medical uses and different varieties of vitriol. Muhammad ibn Zakariya al-Razi (854–925) is credited with being the first to produce sulfuric acid.

Sulfuric acid was called "oil of vitriol" by medieval European alchemists because it was prepared by roasting "green vitriol" (iron(II) sulfate) in an iron retort. There are references to it in the works of Vincent of Beauvais and in the "Compositum de Compositis" ascribed to Saint Albertus Magnus. A passage from Pseudo-Geber's "Summa Perfectionis" was long considered to be a recipe for sulfuric acid, but this was a misinterpretation.

In the seventeenth century, the German-Dutch chemist Johann Glauber prepared sulfuric acid by burning sulfur together with saltpeter (potassium nitrate, ), in the presence of steam. As saltpeter decomposes, it oxidizes the sulfur to , which combines with water to produce sulfuric acid. In 1736, Joshua Ward, a London pharmacist, used this method to begin the first large-scale production of sulfuric acid.

In 1746 in Birmingham, John Roebuck adapted this method to produce sulfuric acid in lead-lined chambers, which were stronger, less expensive, and could be made larger than the previously used glass containers. This process allowed the effective industrialization of sulfuric acid production. After several refinements, this method, called the lead chamber process or "chamber process", remained the standard for sulfuric acid production for almost two centuries.

Sulfuric acid created by John Roebuck's process approached a 65% concentration. Later refinements to the lead chamber process by French chemist Joseph Louis Gay-Lussac and British chemist John Glover improved concentration to 78%. However, the manufacture of some dyes and other chemical processes require a more concentrated product. Throughout the 18th century, this could only be made by dry distilling minerals in a technique similar to the original alchemical processes. Pyrite (iron disulfide, ) was heated in air to yield iron(II) sulfate, , which was oxidized by further heating in air to form iron(III) sulfate, Fe(SO), which, when heated to 480 °C, decomposed to iron(III) oxide and sulfur trioxide, which could be passed through water to yield sulfuric acid in any concentration. However, the expense of this process prevented the large-scale use of concentrated sulfuric acid.

In 1831, British vinegar merchant Peregrine Phillips patented the contact process, which was a far more economical process for producing sulfur trioxide and concentrated sulfuric acid. Today, nearly all of the world's sulfuric acid is produced using this method.

Sulfuric acid is capable of causing very severe burns, especially when it is at high concentrations. In common with other corrosive acids and alkali, it readily decomposes proteins and lipids through amide and ester hydrolysis upon contact with living tissues, such as skin and flesh. In addition, it exhibits a strong dehydrating property on carbohydrates, liberating extra heat and causing secondary thermal burns. Accordingly, it rapidly attacks the cornea and can induce permanent blindness if splashed onto eyes. If ingested, it damages internal organs irreversibly and may even be fatal. Protective equipment should hence always be used when handling it. Moreover, its strong oxidizing property makes it highly corrosive to many metals and may extend its destruction on other materials. Because of such reasons, damage posed by sulfuric acid is potentially more severe than that by other comparable strong acids, such as hydrochloric acid and nitric acid.
Sulfuric acid must be stored carefully in containers made of nonreactive material (such as glass). Solutions equal to or stronger than 1.5 M are labeled "CORROSIVE", while solutions greater than 0.5 M but less than 1.5 M are labeled "IRRITANT". However, even the normal laboratory "dilute" grade (approximately 1 M, 10%) will char paper if left in contact for a sufficient time.

The standard first aid treatment for acid spills on the skin is, as for other corrosive agents, irrigation with large quantities of water. Washing is continued for at least ten to fifteen minutes to cool the tissue surrounding the acid burn and to prevent secondary damage. Contaminated clothing is removed immediately and the underlying skin washed thoroughly.

Preparation of the diluted acid can be dangerous due to the heat released in the dilution process. To avoid splattering, the concentrated acid is usually added to water and not the other way around. Water has a higher heat capacity than the acid, and so a vessel of cold water will absorb heat as acid is added.

Also, because the acid is denser than water, it sinks to the bottom. Heat is generated at the interface between acid and water, which is at the bottom of the vessel. Acid will not boil, because of its higher boiling point. Warm water near the interface rises due to convection, which cools the interface, and prevents boiling of either acid or water.

In contrast, addition of water to concentrated sulfuric acid results in a thin layer of water on top of the acid. Heat generated in this thin layer of water can boil, leading to the dispersal of a sulfuric acid aerosol or worse, an explosion.

Preparation of solutions greater than 6 M (35%) in concentration is most dangerous, because the heat produced may be sufficient to boil the diluted acid: efficient mechanical stirring and external cooling (such as an ice bath) are essential.

Reaction rates double for about every 10-degree Celsius increase in temperature. Therefore, the reaction will become more violent as dilution proceeds, unless the mixture is given time to cool. Adding acid to warm water will cause a violent reaction.

On a laboratory scale, sulfuric acid can be diluted by pouring concentrated acid onto crushed ice made from de-ionized water. The ice melts in an endothermic process while dissolving the acid. The amount of heat needed to melt the ice in this process is greater than the amount of heat evolved by dissolving the acid so the solution remains cold. After all the ice has melted, further dilution can take place using water.

Sulfuric acid is non-flammable.

The main occupational risks posed by this acid are skin contact leading to burns (see above) and the inhalation of aerosols. Exposure to aerosols at high concentrations leads to immediate and severe irritation of the eyes, respiratory tract and mucous membranes: this ceases rapidly after exposure, although there is a risk of subsequent pulmonary edema if tissue damage has been more severe. At lower concentrations, the most commonly reported symptom of chronic exposure to sulfuric acid aerosols is erosion of the teeth, found in virtually all studies: indications of possible chronic damage to the respiratory tract are inconclusive as of 1997. Repeated occupational exposure to sulfuric acid mists may increase the chance of lung cancer by up to 64 percent. In the United States, the permissible exposure limit (PEL) for sulfuric acid is fixed at 1 mg/m: limits in other countries are similar. There have been reports of sulfuric acid ingestion leading to vitamin B12 deficiency with subacute combined degeneration. The spinal cord is most often affected in such cases, but the optic nerves may show demyelination, loss of axons and gliosis.

International commerce of sulfuric acid is controlled under the United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances, 1988, which lists sulfuric acid under Table II of the convention as a chemical frequently used in the illicit manufacture of narcotic drugs or psychotropic substances.




</doc>
<doc id="29248" url="https://en.wikipedia.org/wiki?curid=29248" title="Space colonization">
Space colonization

Space colonization (also called space settlement, or extraterrestrial colonization) is permanent human habitation off the planet Earth.

Many arguments have been made for and against space colonization. The two most common in favor of colonization are survival of human civilization and the biosphere in the event of a planetary-scale disaster (natural or man-made), and the availability of additional resources in space that could enable expansion of human society. The most common objections to colonization include concerns that the commodification of the cosmos may be likely to enhance the interests of the already powerful, including major economic and military institutions, and to exacerbate pre-existing detrimental processes such as wars, economic inequality, and environmental degradation.

No space colonies have been built so far. Currently, the building of a space colony would present a set of huge technological and economic challenges. Space settlements would have to provide for nearly all (or all) the material needs of hundreds or thousands of humans, in an environment out in space that is very hostile to human life. They would involve technologies, such as controlled ecological life support systems, that have yet to be developed in any meaningful way. They would also have to deal with the as-yet unknown issue of how humans would behave and thrive in such places long-term. Because of the present cost of sending anything from the surface of the Earth into orbit (around $1400 per kg, or $640 per-pound), to low Earth orbit by the Falcon Heavy vehicle, expected to further decrease), a space colony would currently be a massively expensive project.

There are yet no plans for building space colonies by any large-scale organization, either government or private. However, many proposals, speculations, and designs for space settlements have been made through the years, and a considerable number of space colonization advocates and groups are active. Several famous scientists, such as Freeman Dyson, have come out in favor of space settlement.

On the technological front, there is ongoing progress in making access to space cheaper (reusable launch systems could reach $20 per kg to orbit), and in creating automated manufacturing and construction techniques.

The primary argument calling for space colonization is the long-term survival of human civilization. By developing alternative locations off Earth, the planet's species, including humans, could live on in the event of natural or man-made disasters on our own planet.

On two occasions, theoretical physicist and cosmologist Stephen Hawking argued for space colonization as a means of saving humanity. In 2001, Hawking predicted that the human race would become extinct within the next thousand years, unless colonies could be established in space. In 2006, he stated that humanity faces two options: either we colonize space within the next two hundred years and build residential units on other planets, or we will face the prospect of long-term extinction.

In 2005, then NASA Administrator Michael Griffin identified space colonization as the ultimate goal of current spaceflight programs, saying:
Louis J. Halle, formerly of the United States Department of State, wrote in "Foreign Affairs" (Summer 1980) that the colonization of space will protect humanity in the event of global nuclear warfare. The physicist Paul Davies also supports the view that if a planetary catastrophe threatens the survival of the human species on Earth, a self-sufficient colony could "reverse-colonize" Earth and restore human civilization. The author and journalist William E. Burrows and the biochemist Robert Shapiro proposed a private project, the Alliance to Rescue Civilization, with the goal of establishing an off-Earth "backup" of human civilization.

Based on his Copernican principle, J. Richard Gott has estimated that the human race could survive for another 7.8 million years, but it is not likely to ever colonize other planets. However, he expressed a hope to be proven wrong, because "colonizing other worlds is our best chance to hedge our bets and improve the survival prospects of our species".

In a theoretical study from 2019, a group of researchers have pondered the long-term trajectory of human civilization. It is argued that due to Earth's finitude as well as the limited duration of our solar system, mankind's survival into the far future will very likely require extensive space colonization. This 'astronomical trajectory' of mankind, as it is termed, could come about in four steps: First step, plenty of space colonies could be established at various habitable locations — be it in outer space or on celestial bodies away from planet earth — and allowed to remain dependent on support from earth for a start. Second step, these colonies could gradually become self-sufficient, enabling them to survive if or when the mother civilization on earth fails or dies. Third step, the colonies could develop and expand their habitation by themselves on their space stations or celestial bodies, fx via terraforming. Fourth step, the colonies could self-replicate and establish new colonies further into space, a process that could then repeat itself and continue at an exponential rate throughout cosmos. However, this astronomical trajectory may not be a lasting one, as it will most likely be interrupted and eventually decline due to resource depletion or straining competition between various human factions, bringing about some 'star wars' scenario. In the very far future, mankind is expected to become extinct in any case, as no civilization — whether human or alien — will ever outlive the limited duration of cosmos itself.

Resources in space, both in materials and energy, are enormous. The Solar System alone has, according to different estimates, enough material and energy to support anywhere from several thousand to over a billion times that of the current Earth-based human population. Outside the Solar System, several hundred billion other planets in the Milky Way alone provide opportunities for both colonization and resource collection, though travel to any of them is impossible on any practical time-scale without interstellar travel by use of generation ships or revolutionary new methods of travel, such as faster-than-light (FTL).

Asteroid mining will also be a key player in space colonization. Water and materials to make structures and shielding can be easily found in asteroids. Instead of resupplying on Earth, mining and fuel stations need to be established on asteroids to facilitate better space travel. Optical mining is the term NASA uses to describe extracting materials from asteroids. NASA believes by using propellant derived from asteroids for exploration to the moon, Mars, and beyond will save $100 billion. If funding and technology come sooner than estimated, asteroid mining might be possible within a decade.

All these planets and other bodies offer a virtually endless supply of resources providing limitless growth potential. Harnessing these resources can lead to much economic development.

Expansion of humans and technological progress has usually resulted in some form of environmental devastation, and destruction of ecosystems and their accompanying wildlife. In the past, expansion has often come at the expense of displacing many indigenous peoples, the resulting treatment of these peoples ranging anywhere from encroachment to genocide. Because space has no known life, this need not be a consequence, as some space settlement advocates have pointed out.

Another argument for space colonization is to mitigate the negative effects of overpopulation.
If the resources of space were opened to use and viable life-supporting habitats were built, Earth would no longer define the limitations of growth. Although many of Earth's resources are non-renewable, off-planet colonies could satisfy the majority of the planet's resource requirements. With the availability of extraterrestrial resources, demand on terrestrial ones would decline.

Additional goals cite the innate human drive to explore and discover, a quality recognized at the core of progress and thriving civilizations.

Nick Bostrom has argued that from a utilitarian perspective, space colonization should be a chief goal as it would enable a very large population to live for a very long period of time (possibly billions of years), which would produce an enormous amount of utility (or happiness). He claims that it is more important to reduce existential risks to increase the probability of eventual colonization than to accelerate technological development so that space colonization could happen sooner. In his paper, he assumes that the created lives will have positive ethical value despite the problem of suffering.

In a 2001 interview with Freeman Dyson, J. Richard Gott and Sid Goldstein, they were asked for reasons why some humans should live in space. Their answers were:

Although some items of the infrastructure requirements above can already be easily produced on Earth and would therefore not be very valuable as trade items (oxygen, water, base metal ores, silicates, etc.), other high value items are more abundant, more easily produced, of higher quality, or can only be produced in space. These would provide (over the long-term) a very high return on the initial investment in space infrastructure.

Some of these high-value trade goods include precious metals, gemstones, power, solar cells, ball bearings, semi-conductors, and pharmaceuticals.

The mining and extraction of metals from a small asteroid the size of 3554 Amun or (6178) 1986 DA, both small near-Earth asteroids, would be 30 times as much metal as humans have mined throughout history. A metal asteroid this size would be worth approximately US$20 trillion at 2001 market prices

Space colonization is seen as a long-term goal of some national space programs. Since the advent of the 21st-century commercialization of space, which saw greater cooperation between NASA and the private sector, several private companies have announced plans toward the colonization of Mars. Among entrepreneurs leading the call for space colonization are Elon Musk, Dennis Tito and Bas Lansdorp.

The main impediments to commercial exploitation of these resources are the very high cost of initial investment, the very long period required for the expected return on those investments ("The Eros Project" plans a 50-year development), and the fact that the venture has never been carried out before—the high-risk nature of the investment.

Major governments and well-funded corporations have announced plans for new categories of activities: space tourism and hotels, prototype space-based solar-power satellites, heavy-lift boosters and asteroid mining—that create needs and capabilities for humans to be present in space.

Building colonies in space would require access to water, food, space, people, construction materials, energy, transportation, communications, life support, simulated gravity, radiation protection and capital investment. It is likely the colonies would be located near the necessary physical resources. The practice of space architecture seeks to transform spaceflight from a heroic test of human endurance to a normality within the bounds of comfortable experience. As is true of other frontier-opening endeavors, the capital investment necessary for space colonization would probably come from governments, an argument made by John Hickman and Neil deGrasse Tyson.

Colonies on the Moon, Mars, or asteroids could extract local materials. The Moon is deficient in volatiles such as argon, helium and compounds of carbon, hydrogen and nitrogen. The LCROSS impacter was targeted at the Cabeus crater which was chosen as having a high concentration of water for the Moon. A plume of material erupted in which some water was detected. Mission chief scientist Anthony Colaprete estimated that the Cabeus crater contains material with 1% water or possibly more. Water ice should also be in other permanently shadowed craters near the lunar poles. Although helium is present only in low concentrations on the Moon, where it is deposited into regolith by the solar wind, an estimated million tons of He-3 exists over all. It also has industrially significant oxygen, silicon, and metals such as iron, aluminum, and titanium.

Launching materials from Earth is expensive, so bulk materials for colonies could come from the Moon, a near-Earth object (NEO), Phobos, or Deimos. The benefits of using such sources include: a lower gravitational force, no atmospheric drag on cargo vessels, and no biosphere to damage. Many NEOs contain substantial amounts of metals. Underneath a drier outer crust (much like oil shale), some other NEOs are inactive comets which include billions of tons of water ice and kerogen hydrocarbons, as well as some nitrogen compounds.

Farther out, Jupiter's Trojan asteroids are thought to be rich in water ice and other volatiles.

Recycling of some raw materials would almost certainly be necessary.

Solar energy in orbit is abundant, reliable, and is commonly used to power satellites today. There is no night in free space, and no clouds or atmosphere to block sunlight. Light intensity obeys an inverse-square law. So the solar energy available at distance "d" from the Sun is "E" = 1367/"d" W/m, where "d" is measured in astronomical units (AU) and 1367 watts/m is the energy available at the distance of Earth's orbit from the Sun, 1 AU.

In the weightlessness and vacuum of space, high temperatures for industrial processes can easily be achieved in solar ovens with huge parabolic reflectors made of metallic foil with very lightweight support structures. Flat mirrors to reflect sunlight around radiation shields into living areas (to avoid line-of-sight access for cosmic rays, or to make the Sun's image appear to move across their "sky") or onto crops are even lighter and easier to build.

Large solar power photovoltaic cell arrays or thermal power plants would be needed to meet the electrical power needs of the settlers' use. In developed parts of Earth, electrical consumption can average 1 kilowatt/person (or roughly 10 megawatt-hours per person per year.) These power plants could be at a short distance from the main structures if wires are used to transmit the power, or much farther away with wireless power transmission.

A major export of the initial space settlement designs was anticipated to be large solar power satellites (SPS) that would use wireless power transmission (phase-locked microwave beams or lasers emitting wavelengths that special solar cells convert with high efficiency) to send power to locations on Earth, or to colonies on the Moon or other locations in space. For locations on Earth, this method of getting power is extremely benign, with zero emissions and far less ground area required per watt than for conventional solar panels. Once these satellites are primarily built from lunar or asteroid-derived materials, the price of SPS electricity could be lower than energy from fossil fuel or nuclear energy; replacing these would have significant benefits such as the elimination of greenhouse gases and nuclear waste from electricity generation.

Transmitting solar energy wirelessly from the Earth to the Moon and back is also an idea proposed for the benefit of space colonization and energy resources. Physicist Dr. David Criswell, who worked for NASA during the Apollo missions, came up with the idea of using power beams to transfer energy from space. These beams, microwaves with a wavelength of about 12 cm, will be almost untouched as they travel through the atmosphere. They can also be aimed at more industrial areas to keep away from humans or animal activities. This will allow for safer and more reliable methods of transferring solar energy.

In 2008, scientists were able to send a 20 watt microwave signal from a mountain in Maui to the island of Hawaii. Since then JAXA and Mitsubishi has teamed up on a $21 billion project in order to place satellites in orbit which could generate up to 1 gigawatt of energy. These are the next advancements being done today in order to make energy be transmitted wirelessly for space-based solar energy.

However, the value of SPS power delivered wirelessly to other locations in space will typically be far higher than to Earth. Otherwise, the means of generating the power would need to be included with these projects and pay the heavy penalty of Earth launch costs. Therefore, other than proposed demonstration projects for power delivered to Earth, the first priority for SPS electricity is likely to be locations in space, such as communications satellites, fuel depots or "orbital tugboat" boosters transferring cargo and passengers between low-Earth orbit (LEO) and other orbits such as geosynchronous orbit (GEO), lunar orbit or highly-eccentric Earth orbit (HEEO). The system will also rely on satellites and receiving stations on Earth to convert the energy into electricity. Because of this energy can be transmitted easily from dayside to nightside meaning power is reliable 24/7.

Nuclear power is sometimes proposed for colonies located on the Moon or on Mars, as the supply of solar energy is too discontinuous in these locations; the Moon has nights of two Earth weeks in duration. Mars has nights, relatively high gravity, and an atmosphere featuring large dust storms to cover and degrade solar panels. Also, Mars' greater distance from the Sun (1.5 astronomical units, AU) translates into "E/(1.5 = 2.25)" only ½–⅔ the solar energy of Earth orbit. Another method would be transmitting energy wirelessly to the lunar or Martian colonies from solar power satellites (SPSs) as described above; the difficulties of generating power in these locations make the relative advantages of SPSs much greater there than for power beamed to locations on Earth. In order to also be able to fulfill the requirements of a Moon base and energy to supply life support, maintenance, communications, and research, a combination of both nuclear and solar energy will be used in the first colonies.

For both solar thermal and nuclear power generation in airless environments, such as the Moon and space, and to a lesser extent the very thin Martian atmosphere, one of the main difficulties is dispersing the inevitable heat generated. This requires fairly large radiator areas.

In space settlements, a life support system must recycle or import all the nutrients without "crashing." The closest terrestrial analogue to space life support is possibly that of a nuclear submarine. Nuclear submarines use mechanical life support systems to support humans for months without surfacing, and this same basic technology could presumably be employed for space use. However, nuclear submarines run "open loop"—extracting oxygen from seawater, and typically dumping carbon dioxide overboard, although they recycle existing oxygen. Recycling of the carbon dioxide has been approached in the literature using the Sabatier process or the Bosch reaction.

Although a fully mechanistic life support system is conceivable, a closed ecological system is generally proposed for life support. The Biosphere 2 project in Arizona has shown that a complex, small, enclosed, man-made biosphere can support eight people for at least a year, although there were many problems. A year or so into the two-year mission oxygen had to be replenished, which strongly suggests that the mission failed.

The relationship between organisms, their habitat and the non-Earth environment can be:

A combination of the above technologies is also possible.

Cosmic rays and solar flares create a lethal radiation environment in space. In Earth orbit, the Van Allen belts make living above the Earth's atmosphere difficult. To protect life, settlements must be surrounded by sufficient mass to absorb most incoming radiation, unless magnetic or plasma radiation shields were developed.

Passive mass shielding of four metric tons per square meter of surface area will reduce radiation dosage to several mSv or less annually, well below the rate of some populated high natural background areas on Earth. This can be leftover material (slag) from processing lunar soil and asteroids into oxygen, metals, and other useful materials. However, it represents a significant obstacle to maneuvering vessels with such massive bulk (mobile spacecraft being particularly likely to use less massive active shielding). Inertia would necessitate powerful thrusters to start or stop rotation, or electric motors to spin two massive portions of a vessel in opposite senses. Shielding material can be stationary around a rotating interior.

Space manufacturing could enable self-replication. Some think it's the ultimate goal because it allows an exponential increase in colonies, while eliminating costs to and dependence on Earth. It could be argued that the establishment of such a colony would be Earth's first act of self-replication. Intermediate goals include colonies that expect only information from Earth (science, engineering, entertainment) and colonies that just require periodic supply of light weight objects, such as integrated circuits, medicines, genetic material and tools.

The monotony and loneliness that comes from a prolonged space mission can leave astronauts susceptible to cabin fever or having a psychotic break. Moreover, lack of sleep, fatigue, and work overload can affect an astronaut's ability to perform well in an environment such as space where every action is critical.

In 2002, the anthropologist John H. Moore estimated that a population of 150–180 would permit a stable society to exist for 60 to 80 generations—equivalent to 2000 years.

A much smaller initial population of as little as two women should be viable as long as human embryos are available from Earth. Use of a sperm bank from Earth also allows a smaller starting base with negligible inbreeding.

Researchers in conservation biology have tended to adopt the "50/500" rule of thumb initially advanced by Franklin and Soule. This rule says a short-term effective population size ("N") of 50 is needed to prevent an unacceptable rate of inbreeding, whereas a long‐term "N" of 500 is required to maintain overall genetic variability. The "N" = 50 prescription corresponds to an inbreeding rate of 1% per generation, approximately half the maximum rate tolerated by domestic animal breeders. The "N" = 500 value attempts to balance the rate of gain in genetic variation due to mutation with the rate of loss due to genetic drift.

Assuming a journey of 6,300 years, the astrophysicist Frédéric Marin and the particle physicist Camille Beluffi calculated that the minimum viable population for a generation ship to reach Proxima Centauri would be 98 settlers at the beginning of the mission (then the crew will breed until reaching a stable population of several hundred settlers within the ship) .

Experts have debated on the possible usage of money and currencies in societies that will be established in space. The Quasi Universal Intergalactic Denomination, or QUID, is a physical currency made from a space-qualified polymer PTFE for inter-planetary travelers. QUID was designed for the foreign exchange company Travelex by scientists from Britain's National Space Centre and the University of Leicester.

Location is a frequent point of contention between space colonization advocates. The location of colonization can be on a physical body planet, dwarf planet, natural satellite, or asteroid or orbiting one. For colonies not on a body see also space habitat.

Due to its proximity and familiarity, Earth's Moon is discussed as a target for colonization. It has the benefits of proximity to Earth and lower escape velocity, allowing for easier exchange of goods and services. A drawback of the Moon is its low abundance of volatiles necessary for life such as hydrogen, nitrogen, and carbon. Water-ice deposits that exist in some polar craters could serve as a source for these elements. An alternative solution is to bring hydrogen from near-Earth asteroids and combine it with oxygen extracted from lunar rock.

The Moon's low surface gravity is also a concern, as it is unknown whether 1/6g is enough to maintain human health for long periods.

The Moon's lack of atmosphere provides no protection from space radiation or meteoroids. The early Moon colonies may shelter in ancient Lunar lava tubes to gain protection. The two-week day/night cycle makes use of solar power more difficult.

Another near-Earth possibility are the five Earth–Moon Lagrange points. Although they would generally also take a few days to reach with current technology, many of these points would have near-continuous solar power because their distance from Earth would result in only brief and infrequent eclipses of light from the Sun. However, the fact that the Earth–Moon Lagrange points and tend to collect dust and debris, whereas - require active station-keeping measures to maintain a stable position, make them somewhat less suitable places for habitation than was originally believed. Additionally, the orbit of – takes them out of the protection of the Earth's magnetosphere for approximately two-thirds of the time, exposing them to the health threat from cosmic rays.

The five Earth–Sun Lagrange points would totally eliminate eclipses, but only and would be reachable in a few days' time. The other three Earth–Sun points would require months to reach.

See space habitat

Colonizing Mercury would involve similar challenges as the Moon as there are few volatile elements, no atmosphere and the surface gravity is lower than Earth's. However, the planet also receives almost seven times the solar flux as the Earth/Moon system.

Geologist Stephen Gillett suggested in 1996 that this could make Mercury an ideal place to build and launch solar sail spacecraft, which could launch as folded up "chunks" by mass driver from Mercury's surface. Once in space the solar sails would deploy. Since Mercury's solar constant is 6.5 times higher than Earth's, energy for the mass driver should be easy to come by, and solar sails near Mercury would have 6.5 times the thrust they do near Earth. This could make Mercury an ideal place to acquire materials useful in building hardware to send to (and terraform) Venus. Vast solar collectors could also be built on or near Mercury to produce power for large scale engineering activities such as laser-pushed lightsails to nearby star systems.

Colonization of asteroids would require space habitats. The asteroid belt has significant overall material available, the largest object being Ceres, although it is thinly distributed as it covers a vast region of space. Unmanned supply craft should be practical with little technological advance, even crossing 500 million kilometers of space. The colonists would have a strong interest in assuring their asteroid did not hit Earth or any other body of significant mass, but would have extreme difficulty in moving an asteroid of any size. The orbits of the Earth and most asteroids are very distant from each other in terms of delta-v and the asteroidal bodies have enormous momentum. Rockets or mass drivers can perhaps be installed on asteroids to direct their path into a safe course.

The Artemis Project designed a plan to colonize Europa, one of Jupiter's moons. Scientists were to inhabit igloos and drill down into the Europan ice crust, exploring any sub-surface ocean. This plan discusses possible use of "air pockets" for human habitation. Europa is considered one of the more habitable bodies in the Solar System and so merits investigation as a possible abode for life.

NASA performed a study called "HOPE" (Revolutionary Concepts for Human Outer Planet Exploration) regarding the future exploration of the Solar System. The target chosen was Callisto due to its distance from Jupiter, and thus the planet's harmful radiation. It could be possible to build a surface base that would produce fuel for further exploration of the Solar System.

Three of the Galilean moons (Europa, Ganymede, Callisto) have an abundance of volatiles that may support colonization efforts.

Titan is suggested as a target for colonization, because it is the only moon in the Solar System to have a dense atmosphere and is rich in carbon-bearing compounds. Titan has ice water and large methane oceans. Robert Zubrin identified Titan as possessing an abundance of all the elements necessary to support life, making Titan perhaps the most advantageous locale in the outer Solar System for colonization, and saying "In certain ways, Titan is the most hospitable extraterrestrial world within our solar system for human colonization".

Enceladus is a small, icy moon orbiting close to Saturn, notable for its extremely bright surface and the geyser-like plumes of ice and water vapor that erupt from its southern polar region. If Enceladus has liquid water, it joins Mars and Jupiter's moon Europa as one of the prime places in the Solar System to look for extraterrestrial life and possible future settlements.

Other large satellites: Rhea, Iapetus, Dione, Tethys, and Mimas, all have large quantities of volatiles, which can be used to support settlements.

The Kuiper belt is estimated to have 70,000 bodies of 100 km or larger.

Freeman Dyson has suggested that within a few centuries human civilization will have relocated to the Kuiper belt.

The Oort cloud is estimated to have up to a trillion comets.

Looking beyond the Solar System, there are up to several hundred billion potential stars with possible colonization targets. The main difficulty is the vast distances to other stars: roughly a hundred thousand times further away than the planets in the Solar System. This means that some combination of very high speed (some more-than-fractional percentage of the speed of light), or travel times lasting centuries or millennia, would be required. These speeds are far beyond what current spacecraft propulsion systems can provide.

Many scientific papers have been published about interstellar travel. Given sufficient travel time and engineering work, both unmanned and generational voyages seem possible, though representing a very considerable technological and economic challenge unlikely to be met for some time, particularly for manned probes.

Space colonization technology could in principle allow human expansion at high, but sub-relativistic speeds, substantially less than the speed of light, "c".  An interstellar colony ship would be similar to a space habitat, with the addition of major propulsion capabilities and independent energy generation.

Hypothetical starship concepts proposed both by scientists and in hard science fiction include:

The above concepts which appear limited to high, but still sub-relativistic speeds, due to fundamental energy and reaction mass considerations, and all would entail trip times which might be enabled by space colonization technology, permitting self-contained habitats with lifetimes of decades to centuries. Yet human interstellar expansion at average speeds of even 0.1% of "c"  would permit settlement of the entire Galaxy in less than one half of the Sun's galactic orbital period of ~240,000,000 years, which is comparable to the timescale of other galactic processes. Thus, even if interstellar travel at near relativistic speeds is never feasible (which cannot be clearly determined at this time), the development of space colonization could allow human expansion beyond the Solar System without requiring technological advances that cannot yet be reasonably foreseen. This could greatly improve the chances for the survival of intelligent life over cosmic timescales, given the many natural and human-related hazards that have been widely noted.

If humanity does gain access to a large amount of energy, on the order of the mass-energy of entire planets, it may eventually become feasible to construct Alcubierre drives. These are one of the few methods of superluminal travel which may be possible under current physics. However it is probable that such a device could never exist, due to the fundamental challenges posed. For more on this see Difficulties of making and using an Alcubierre Drive.

Looking beyond the Milky Way, there are at least 2 trillion other galaxies in the observable universe. The distances between galaxies are on the order of a million times farther than those between the stars. Because of the speed of light limit on how fast any material objects can travel in space, intergalactic travel would either have to involve voyages lasting millions of years, or a possible faster than light propulsion method based on speculative physics, such as the Alcubierre drive. There are, however, no scientific reasons for stating that intergalactic travel is impossible in principle.

Uploaded human minds or AI may be transmitted to other galaxies in the hope some intelligence there would receive and activate them.

Space colonization can roughly be said to be possible when the necessary methods of space colonization become cheap enough (such as space access by cheaper launch systems) to meet the cumulative funds that have been gathered for the purpose, in addition to estimated profits from commercial use of space.

Although there are no immediate prospects for the large amounts of money required for space colonization to be available given traditional launch costs, there is some prospect of a radical reduction to launch costs in the 2010s, which would consequently lessen the cost of any efforts in that direction. With a published price of per launch of up to payload to low Earth orbit, SpaceX Falcon 9 rockets are already the "cheapest in the industry". Advancements currently being developed as part of the SpaceX reusable launch system development program to enable reusable Falcon 9s "could drop the price by an order of magnitude, sparking more space-based enterprise, which in turn would drop the cost of access to space still further through economies of scale." If SpaceX is successful in developing the reusable technology, it would be expected to "have a major impact on the cost of access to space", and change the increasingly competitive market in space launch services.

The President's Commission on Implementation of United States Space Exploration Policy suggested that an inducement prize should be established, perhaps by government, for the achievement of space colonization, for example by offering the prize to the first organization to place humans on the Moon and sustain them for a fixed period before they return to Earth.

The most famous attempt to build an analogue to a self-sufficient colony is Biosphere 2, which attempted to duplicate Earth's biosphere. BIOS-3 is another closed ecosystem, completed in 1972 in Krasnoyarsk, Siberia.

Many space agencies build testbeds for advanced life support systems, but these are designed for long duration human spaceflight, not permanent colonization.

Remote research stations in inhospitable climates, such as the Amundsen–Scott South Pole Station or Devon Island Mars Arctic Research Station, can also provide some practice for off-world outpost construction and operation. The Mars Desert Research Station has a habitat for similar reasons, but the surrounding climate is not strictly inhospitable.

The first known work on space colonization was "The Brick Moon", a work of fiction published in 1869 by Edward Everett Hale, about an inhabited artificial satellite.

The Russian schoolmaster and physicist Konstantin Tsiolkovsky foresaw elements of the space community in his book "Beyond Planet Earth" written about 1900. Tsiolkovsky had his space travelers building greenhouses and raising crops in space. Tsiolkovsky believed that going into space would help perfect human beings, leading to immortality and peace.

Others have also written about space colonies as Lasswitz in 1897 and Bernal, Oberth, Von Pirquet and Noordung in the 1920s. Wernher von Braun contributed his ideas in a 1952 "Colliers" article. In the 1950s and 1960s, Dandridge M. Cole published his ideas.

Another seminal book on the subject was the book "The High Frontier: Human Colonies in Space" by Gerard K. O'Neill in 1977 which was followed the same year by "Colonies in Space " by T. A. Heppenheimer.

M. Dyson wrote "Home on the Moon; Living on a Space Frontier" in 2003; Peter Eckart wrote "Lunar Base Handbook" in 2006 and then Harrison Schmitt's "Return to the Moon" written in 2007.

, Bigelow Aerospace is the only private commercial spaceflight company that has launched two experimental space station modules, Genesis I (2006) and Genesis II (2007), into Earth-orbit, and has indicated that their first production model of the space habitat, the BA 330, could be launched by 2017.

Robotic spacecraft to Mars are required to be sterilized, to have at most 300,000 spores on the exterior of the craft—and more thoroughly sterilized if they contact "special regions" containing water, otherwise there is a risk of contaminating not only the life-detection experiments but possibly the planet itself.

It is impossible to sterilize human missions to this level, as humans are host to typically a hundred trillion microorganisms of thousands of species of the human microbiome, and these cannot be removed while preserving the life of the human. Containment seems the only option, but it is a major challenge in the event of a hard landing (i.e. crash). There have been several planetary workshops on this issue, but with no final guidelines for a way forward yet. Human explorers would also be vulnerable to back contamination to Earth if they become carriers of microorganisms.

A corollary to the Fermi paradox—"nobody else is doing it"—is the argument that, because no evidence of alien colonization technology exists, it is statistically unlikely to even be possible to use that same level of technology ourselves.

Colonizing space would require massive amounts of financial, physical, and human capital devoted to research, development, production, and deployment. Earth's natural resources do not increase to a noteworthy extent (which is in keeping with the "only one Earth" position of environmentalists). Thus, considerable efforts in colonizing places outside Earth would appear as a hazardous waste of the Earth's limited resources for an aim without a clear end.

The fundamental problem of public things, needed for survival, such as space programs, is the free-rider problem. Convincing the public to fund such programs would require additional self-interest arguments: If the objective of space colonization is to provide a "backup" in case everyone on Earth is killed, then why should someone on Earth pay for something that is only useful after they are dead? This assumes that space colonization is not widely acknowledged as a sufficiently valuable social goal.

Seen as a relief to the problem of overpopulation even as early as 1758, and listed as one of Stephen Hawking's reasons for pursuing space exploration, it has become apparent that space colonisation in response to overpopulation is unwarranted. Indeed, the birth rates of many developed countries, specifically spacefaring ones, are at or below replacement rates, thus negating the need to use colonisation as a means of population control.

Other objections include concerns that the forthcoming colonization and commodification of the cosmos may be likely to enhance the interests of the already powerful, including major economic and military institutions e.g. the large financial institutions, the major aerospace companies and the military–industrial complex, to lead to new wars, and to exacerbate pre-existing exploitation of workers and resources, economic inequality, poverty, social division and marginalization, environmental degradation, and other detrimental processes or institutions.

Additional concerns include creating a culture in which humans are no longer seen as human, but rather as material assets. The issues of human dignity, morality, philosophy, culture, bioethics, and the threat of megalomaniac leaders in these new "societies" would all have to be addressed in order for space colonization to meet the psychological and social needs of people living in isolated colonies.

As an alternative or addendum for the future of the human race, many science fiction writers have focused on the realm of the 'inner-space', that is the computer-aided exploration of the human mind and human consciousness—possibly en route developmentally to a Matrioshka Brain.

Robotic exploration is proposed as an alternative to gain many of the same scientific advantages without the limited mission duration and high cost of life support and return transportation involved in manned missions. However, there are vast scientific domains that cannot be addressed with robots, especially biology in specific atmospheric and gravitational environments and human sciences in space.

Another concern is the potential to cause interplanetary contamination on planets that may harbor hypothetical extraterrestrial life.

Space colonization has been discussed as continuation of imperialism and colonialism. Questioning colonial decisionmaking and reasons for colonial labour and land exploitation with postcolonial critique. Seeing the need for inclusive and democratic participation and implementation of any space exploration, infrastructure or colonialization.

The narrative of space exploration as a "New Frontier" has been criticized as unreflected continuation of settler colonialism and manifest destiny, continuing the narrative of colonial exploration as fundamental to the assumed human nature.

The predominant perspective of territorial colonization in space has been called "surfacism", especially comparing advocacy for colonization of Mars opposed to Venus.

The health of the humans who may participate in a colonization venture would be subject to increased physical, mental and emotional risks. NASA learned that – without gravity – bones lose minerals, causing osteoporosis. Bone density may decrease by 1% per month, which may lead to a greater risk of osteoporosis-related fractures later in life. Fluid shifts towards to the head may cause vision problems. NASA found that isolation in closed environments aboard the International Space Station led to depression, sleep disorders, and diminished personal interactions, likely due to confined spaces and the monotony and boredom of long space flight. Circadian rhythm may also be susceptible to the effects of space life due to the effects on sleep of disrupted timing of sunset and sunrise. This can lead to exhaustion, as well as other sleep problems such as insomnia, which can reduce their productivity and lead to mental health disorders. High-energy radiation is a health risk that colonizers would face, as radiation in deep space is deadlier than what astronauts face now in low-earth orbit. Metal shielding on space vehicles protects against only 25-30% of space radiation, possibly leaving colonizers exposed to the other 70% of radiation and its short and long-term health complications.

Although there are many physical, mental, and emotional health risks for future colonizers and pioneers, solutions have been proposed to correct these problems. Mars500, HI-SEAS, and SMART-OP represent efforts to help reduce the effects of loneliness and confinement for long periods of time. Keeping contact with family members, celebrating holidays, and maintaining cultural identities all had an impact on minimizing the deterioration of mental health. There are also health tools in development to help astronauts reduce anxiety, as well as helpful tips to reduce the spread of germs and bacteria in a closed environment. Radiation risk may be reduced for astronauts by frequent monitoring and focusing work away from the shielding on the shuttle. Future space agencies can also ensure that every colonizer would have a mandatory amount of daily exercise to prevent degradation of muscle.

Organizations that contribute to space colonization include:

Although established space colonies are a stock element in science fiction stories, fictional works that explore the themes, social or practical, of the settlement and occupation of a habitable world are much rarer.



</doc>
<doc id="29250" url="https://en.wikipedia.org/wiki?curid=29250" title="Second Council of Nicaea">
Second Council of Nicaea

The Second Council of Nicaea is recognized as the last of the first seven ecumenical councils by the Eastern Orthodox Church and the Catholic Church. In addition, it is also recognized as such by the Old Catholics and others. Protestant opinions on it are varied.

It met in AD 787 in Nicaea (site of the First Council of Nicaea; present-day İznik in Turkey) to restore the use and veneration of icons (or, holy images), which had been suppressed by imperial edict inside the Byzantine Empire during the reign of Leo III (717–741). His son, Constantine V (741–775), had held the Council of Hieria to make the suppression official.

The veneration of icons had been banned by Byzantine Emperor Constantine V and supported by his Council of Hieria (754 AD), which had described itself as the seventh ecumenical council. The Council of Hieria was overturned by the Second Council of Nicaea only 33 years later, and has also been rejected by Catholic and Orthodox churches, since none of the five major patriarchs were represented. The emperor's vigorous enforcement of the ban included persecution of those who venerated icons and of monks in general. There were also political overtones to the persecution—images of emperors were still allowed by Constantine, which some opponents saw as an attempt to give wider authority to imperial power than to the saints and bishops. Constantine's iconoclastic tendencies were shared by Constantine's son, Leo IV. After the latter's early death, his widow, Irene of Athens, as regent for her son, began its restoration for personal inclination and political considerations.

In 784 the imperial secretary Patriarch Tarasius was appointed successor to the Patriarch Paul IV—he accepted on the condition that intercommunion with the other churches should be reestablished; that is, that the images should be restored. However, a council, claiming to be ecumenical, had abolished the veneration of icons, so another ecumenical council was necessary for its restoration.

Pope Adrian I was invited to participate, and gladly accepted, sending an archbishop and an abbot as his legates.
In 786, the council met in the Church of the Holy Apostles in Constantinople. However, soldiers in collusion with the opposition entered the church, and broke up the assembly. As a result, the government resorted to a stratagem. Under the pretext of a campaign, the iconoclastic bodyguard was sent away from the capital – disarmed and disbanded.

The council was again summoned to meet, this time in Nicaea, since Constantinople was still distrusted. The council assembled on September 24, 787 at the church of Hagia Sophia. It numbered about 350 members; 308 bishops or their representatives signed. Tarasius presided, and seven sessions were held in Nicaea.

The clear distinction between the adoration offered to God and that accorded to the images may well be looked upon as a result of the iconoclastic reform. The twenty-two canons drawn up in Constantinople also served ecclesiastical reform. Careful maintenance of the ordinances of the earlier councils, knowledge of the scriptures on the part of the clergy, and care for Christian conduct are required, and the desire for a renewal of ecclesiastical life is awakened.

The council also decreed that every altar should contain a relic, which remains the case in modern Catholic and Orthodox regulations (Canon VII), and made a number of decrees on clerical discipline, especially for monks when mixing with women.

The papal legates voiced their approval of the restoration of the veneration of icons in no uncertain terms, and the patriarch sent a full account of the proceedings of the council to Pope Hadrian I, who had it translated (Pope Anastasius III later replaced the translation with a better one). In the West, the Frankish clergy initially rejected the Council at a synod in 794, and Charlemagne, then King of the Franks, supported the composition of the "Libri Carolini" in response, which repudiated the teachings of both the Council and the iconoclasts. A copy of the "Libri" was sent to Pope Hadrian, who responded with a refutation of the Frankish arguments. The "Libri" would thereafter remain unpublished until the Reformation, and the Council is accepted as the Seventh Ecumenical Council by the Catholic Church.

The council is celebrated in the Eastern Orthodox Church, and Eastern Catholic Churches of Byzantine Rite as "The Sunday of the Triumph of Orthodoxy" each year on the first Sunday of Great Lent, the fast that leads up to Pascha (Easter), and again on the Sunday closest to October 11 (the Sunday on or after October 8). The former celebration commemorates the council as the culmination of the Church's battles against heresy, while the latter commemorates the council itself.

Many Protestants follow the French reformer John Calvin in rejecting the canons of the council, which they believe promoted of idolatry. He rejected the distinction between veneration ("douleia", "proskynesis") and adoration ("latreia") as unbiblical "sophistry" and condemned even the decorative use of images. In subsequent editions of the "Institutes", he cited an influential Carolingian source, now ascribed to Theodulf of Orleans, which reacts negatively to a poor Latin translation of the council's acts. Calvin did not engage the apologetic arguments of John of Damascus or Theodore the Studite, apparently because he was unaware of them.

There are only a few translations of the above Acts in the modern languages:





</doc>
<doc id="29252" url="https://en.wikipedia.org/wiki?curid=29252" title="Sexual orientation">
Sexual orientation

Sexual orientation is an enduring pattern of romantic or sexual attraction (or a combination of these) to persons of the opposite sex or gender, the same sex or gender, or to both sexes or more than one gender. These attractions are generally subsumed under heterosexuality, homosexuality, and bisexuality, while asexuality (the lack of sexual attraction to others) is sometimes identified as the fourth category.

These categories are aspects of the more nuanced nature of sexual identity and terminology. For example, people may use other labels, such as "pansexual" or "polysexual", or none at all. According to the American Psychological Association, sexual orientation "also refers to a person's sense of identity based on those attractions, related behaviors, and membership in a community of others who share those attractions". "Androphilia" and "gynephilia" are terms used in behavioral science to describe sexual orientation as an alternative to a gender binary conceptualization. "Androphilia" describes sexual attraction to masculinity; "gynephilia" describes the sexual attraction to femininity. The term "sexual preference" largely overlaps with sexual orientation, but is generally distinguished in psychological research. A person who identifies as bisexual, for example, may sexually prefer one sex over the other. "Sexual preference" may also suggest a degree of voluntary choice, whereas the scientific consensus is that sexual orientation is not a choice.

Scientists do not know the exact cause of sexual orientation, but they theorize that it is caused by a complex interplay of genetic, hormonal, and environmental influences. Although no single theory on the cause of sexual orientation has yet gained widespread support, scientists favor biologically-based theories. There is considerably more evidence supporting nonsocial, biological causes of sexual orientation than social ones, especially for males. There is no substantive evidence which suggests parenting or early childhood experiences play a role with regard to sexual orientation. Research over several decades has demonstrated that sexual orientation ranges along a continuum, from exclusive attraction to the opposite sex to exclusive attraction to the same sex.

Sexual orientation is reported primarily within biology and psychology (including sexology), but it is also a subject area in anthropology, history (including social constructionism), and law, and there are other explanations that relate to sexual orientation and culture.

Sexual orientation is traditionally defined as including heterosexuality, bisexuality, and homosexuality, while asexuality is considered the fourth category of sexual orientation by some researchers and has been defined as the absence of a traditional sexual orientation. An asexual has little to no sexual attraction to people. It may be considered a lack of a sexual orientation, and there is significant debate over whether or not it is a sexual orientation.

Most definitions of sexual orientation include a psychological component, such as the direction of an individual's erotic desires, or a behavioral component, which focuses on the sex of the individual's sexual partner/s. Some people prefer simply to follow an individual's self-definition or identity. Scientific and professional understanding is that "the core attractions that form the basis for adult sexual orientation typically emerge between middle childhood and early adolescence". Sexual orientation differs from sexual identity in that it encompasses relationships with others, while sexual identity is a concept of self.

The American Psychological Association states that "[s]exual orientation refers to an enduring pattern of emotional, romantic, and/or sexual attractions to men, women, or both sexes" and that "[t]his range of behaviors and attractions has been described in various cultures and nations throughout the world. Many cultures use identity labels to describe people who express these attractions. In the United States, the most frequent labels are lesbians (women attracted to women), gay men (men attracted to men), and bisexual people (men or women attracted to both sexes). However, some people may use different labels or none at all". They additionally state that sexual orientation "is distinct from other components of sex and gender, including biological sex (the anatomical, physiological, and genetic characteristics associated with being male or female), gender identity (the psychological sense of being male or female), and social gender role (the cultural norms that define feminine and masculine behavior)".

Sexual identity and sexual behavior are closely related to sexual orientation, but they are distinguished, with sexual identity referring to an individual's conception of themselves, behavior referring to actual sexual acts performed by the individual, and orientation referring to "fantasies, attachments and longings." Individuals may or may not express their sexual orientation in their behaviors. People who have a non-heterosexual sexual orientation that does not align with their sexual identity are sometimes referred to as 'closeted'. The term may, however, reflect a certain cultural context and particular stage of transition in societies which are gradually dealing with integrating sexual minorities. In studies related to sexual orientation, when dealing with the degree to which a person's sexual attractions, behaviors and identity match, scientists usually use the terms "concordance" or "discordance." Thus, a woman who is attracted to other women, but calls herself heterosexual and only has sexual relations with men, can be said to experience discordance between her sexual orientation (homosexual or lesbian) and her sexual identity and behaviors (heterosexual).

"Sexual identity" may also be used to describe a person's perception of his or her own "sex", rather than sexual orientation. The term "sexual preference" has a similar meaning to "sexual orientation", and the two terms are often used interchangeably, but "sexual preference" suggests a degree of voluntary choice. The term has been listed by the American Psychological Association's Committee on Gay and Lesbian Concerns as a wording that advances a "heterosexual bias".

"Androphilia" and "gynephilia" (or "gynecophilia") are terms used in behavioral science to describe sexual attraction, as an alternative to a homosexual and heterosexual conceptualization. They are used for identifying a subject's object of attraction without attributing a sex assignment or gender identity to the subject. Related terms such as "pansexual" and "polysexual" do not make any such assignations to the subject. People may also use terms such as "queer", "pansensual," "polyfidelitous," "ambisexual," or personalized identities such as "byke" or "biphilic".

"Same gender loving" (SGL) is considered to be more than a different term for gay; it introduces the concept of love into the discussion. SGL also acknowledges relationships between people of like identities; for example, third gender individuals who may be oriented toward each other, and expands the discussion of sexuality beyond the original man/woman gender duality. The complexity of transgender orientation is also more completely understood within this perspective.

Using "androphilia" and "gynephilia" can avoid confusion and offense when describing people in non-western cultures, as well as when describing intersex and transgender people. Psychiatrist Anil Aggrawal explains that androphilia, along with gynephilia, "is needed to overcome immense difficulties in characterizing the sexual orientation of trans men and trans women. For instance, it is difficult to decide whether a trans man erotically attracted to males is a heterosexual female or a homosexual male; or a trans woman erotically attracted to females is a heterosexual male or a lesbian female. Any attempt to classify them may not only cause confusion but arouse offense among the affected subjects. In such cases, while defining sexual attraction, it is best to focus on the object of their attraction rather than on the sex or gender of the subject." Sexologist Milton Diamond writes, "The terms heterosexual, homosexual, and bisexual are better used as adjectives, not nouns, and are better applied to behaviors, not people. This usage is particularly advantageous when discussing the partners of transsexual or intersexed individuals. These newer terms also do not carry the social weight of the former ones."

Some researchers advocate use of the terminology to avoid bias inherent in Western conceptualizations of human sexuality. Writing about the Samoan fa'afafine demographic, sociologist Johanna Schmidt writes that in cultures where a third gender is recognized, a term like "homosexual transsexual" does not align with cultural categories.

Some researchers, such as Bruce Bagemihl, have criticized the labels "heterosexual" and "homosexual" as confusing and degrading. Bagemihl writes, "...the point of reference for 'heterosexual' or 'homosexual' orientation in this nomenclature is solely the individual's genetic sex prior to reassignment (see for example, Blanchard et al. 1987, Coleman and Bockting, 1988, Blanchard, 1989). These labels thereby ignore the individual's personal sense of gender identity taking precedence over biological sex, rather than the other way around." Bagemihl goes on to take issue with the way this terminology makes it easy to claim transsexuals are really homosexual males seeking to escape from stigma.

The earliest writers on sexual orientation usually understood it to be intrinsically linked to the subject's own sex. For example, it was thought that a typical female-bodied person who is attracted to female-bodied persons would have masculine attributes, and vice versa. This understanding was shared by most of the significant theorists of sexual orientation from the mid nineteenth to early twentieth century, such as Karl Heinrich Ulrichs, Richard von Krafft-Ebing, Magnus Hirschfeld, Havelock Ellis, Carl Jung, and Sigmund Freud, as well as many gender-variant homosexual people themselves. However, this understanding of homosexuality as sexual inversion was disputed at the time, and, through the second half of the twentieth century, gender identity came to be increasingly seen as a phenomenon distinct from sexual orientation. Transgender and cisgender people may be attracted to men, women, or both, although the prevalence of different sexual orientations is quite different in these two populations. An individual homosexual, heterosexual or bisexual person may be masculine, feminine, or androgynous, and in addition, many members and supporters of lesbian and gay communities now see the "gender-conforming heterosexual" and the "gender-nonconforming homosexual" as negative stereotypes. Nevertheless, studies by J. Michael Bailey and Kenneth Zucker found a majority of the gay men and lesbians sampled reporting various degrees of gender-nonconformity during their childhood years.

Transgender people today identify with the sexual orientation that corresponds with their gender; meaning that a trans woman who is solely attracted to women would often identify as a lesbian. A trans man solely attracted to women would be a straight man.

Sexual orientation sees greater intricacy when non-binary understandings of both sex (male, female, or intersex) and gender (man, woman, transgender, third gender, etc. are considered. Sociologist Paula Rodriguez Rust (2000) argues for a more multifaceted definition of sexual orientation:

Gay and lesbian people can have sexual relationships with someone of the opposite sex for a variety of reasons, including the desire for a perceived traditional family and concerns of discrimination and religious ostracism. While some LGBT people hide their respective orientations from their spouses, others develop positive gay and lesbian identities while maintaining successful heterosexual marriages. Coming out of the closet to oneself, a spouse of the opposite sex, and children can present challenges that are not faced by gay and lesbian people who are not married to people of the opposite sex or do not have children.

Often, sexual orientation and sexual orientation identity are not distinguished, which can impact accurately assessing sexual identity and whether or not sexual orientation is able to change; sexual orientation identity can change throughout an individual's life, and may or may not align with biological sex, sexual behavior or actual sexual orientation.<ref name="Concordance/discordance in SO"></ref> Sexual orientation is stable and unlikely to change for the vast majority of people, but some research indicates that some people may experience change in their sexual orientation, and this is more likely for women than for men. The American Psychological Association distinguishes between sexual orientation (an innate attraction) and sexual orientation identity (which may change at any point in a person's life).

Some research suggests that "[f]or some <nowiki>[</nowiki>people<nowiki>]</nowiki> the focus of sexual interest will shift at various points through the life span..." "There... <nowiki>[</nowiki>was, as of 1995,<nowiki>]</nowiki> essentially no research on the longitudinal stability of sexual orientation over the adult life span... It <nowiki>[</nowiki>was<nowiki>]</nowiki>... still an unanswered question whether... <nowiki>[</nowiki>the<nowiki>]</nowiki> measure <nowiki>[</nowiki>of 'the complex components of sexual orientation as differentiated from other aspects of sexual identity at one point in time'<nowiki>]</nowiki> will predict future behavior or orientation. Certainly, it is... not a good predictor of past behavior and self-identity, given the developmental process common to most gay men and lesbians (i.e., denial of homosexual interests and heterosexual experimentation prior to the coming-out process)." Some studies report that "[a number of] lesbian women, and some heterosexual women as well, perceive choice as an important element in their sexual orientations."

The exact causes for the development of a particular sexual orientation have yet to be established. To date, a lot of research has been conducted to determine the influence of genetics, hormonal action, development dynamics, social and cultural influences—which has led many to think that biology and environment factors play a complex role in forming it. It was once thought that homosexuality was the result of faulty psychological development, resulting from childhood experiences and troubled relationships, including childhood sexual abuse. It has been found that this was based on prejudice and misinformation.

Research has identified several biological factors which may be related to the development of sexual orientation, including genes, prenatal hormones, and brain structure. No single controlling cause has been identified, and research is continuing in this area.

Although researchers generally believe that sexual orientation is not determined by any one factor but by a combination of genetic, hormonal, and environmental influences, with biological factors involving a complex interplay of genetic factors and the early uterine environment, they favor biological models for the cause. There is considerably more evidence supporting nonsocial, biological causes of sexual orientation than social ones, especially for males. Scientists do not believe that sexual orientation is a choice, and some of them believe that it is established at conception. Current scientific investigation usually seeks to find biological explanations for the adoption of a particular sexual orientation. Scientific studies have found a number of statistical biological differences between gay people and heterosexuals, which may result from the same underlying cause as sexual orientation itself. 

Genes may be related to the development of sexual orientation. A twin study from 2001 appears to exclude genes as a major factor, while a twin study from 2010 found that homosexuality was explained by both genes and environmental factors. However, experimental design of the available twin studies has made their interpretation difficult.

In 2012, a large, comprehensive genome-wide linkage study of male sexual orientation was conducted by several independent groups of researchers. Significant linkage to homosexuality was found with genes on chromosome Xq28 and chromosome 8 in the pericentromeric region. The authors concluded that "our findings, taken in context with previous work, suggest that genetic variation in each of these regions contributes to development of the important psychological trait of male sexual orientation." It was the largest study of the genetic basis of homosexuality to date and was published online in November 2014.

The hormonal theory of sexuality holds that just as exposure to certain hormones plays a role in fetal sex differentiation, hormonal exposure also influences the sexual orientation that emerges later in the adult. Fetal hormones may be seen as either the primary influence upon adult sexual orientation or as a co-factor interacting with genes or environmental and social conditions.

For humans, the norm is that females possess two X sex chromosomes, while males have one X and one Y. The default developmental pathway for a human fetus being female, the Y chromosome is what induces the changes necessary to shift to the male developmental pathway. This differentiation process is driven by androgen hormones, mainly testosterone and dihydrotestosterone (DHT). The newly formed testicles in the fetus are responsible for the secretion of androgens, that will cooperate in driving the sexual differentiation of the developing fetus, including its brain. This results in sexual differences between males and females. This fact has led some scientists to test in various ways the result of modifying androgen exposure levels in mammals during fetus and early life.

Recent studies found an increased chance of homosexuality in men whose mothers previously carried to term many male children. This effect is nullified if the man is left-handed.

Known as the "fraternal birth order" (FBO) effect, this theory has been backed up by strong evidence of its prenatal origin, although no evidence thus far has linked it to an exact prenatal mechanism. However, research suggests that this may be of immunological origin, caused by a maternal immune reaction against a substance crucial to male fetal development during pregnancy, which becomes increasingly likely after every male gestation. As a result of this immune effect, alterations in later-born males' prenatal development have been thought to occur. This process, known as the maternal immunization hypothesis (MIH), would begin when cells from a male fetus enter the mother's circulation during pregnancy or while giving birth. These Y-linked proteins would not be recognized in the mother's immune system because she is female, causing her to develop antibodies which would travel through the placental barrier into the fetal compartment. From here, the anti-male bodies would then cross the blood–brain barrier of the developing fetal brain, altering sex-dimorphic brain structures relative to sexual orientation, causing the exposed son to be more attracted to men over women.

There is no substantive evidence to support the suggestion that early childhood experiences, parenting, sexual abuse, or other adverse life events influence sexual orientation. However, studies do find that aspects of sexuality expression have an experiential basis and that parental attitudes towards a particular sexual orientation may affect how children of the parents experiment with behaviors related to a certain sexual orientation.

The American Academy of Pediatrics in 2004 stated:
The American Psychological Association, the American Psychiatric Association, and the National Association of Social Workers in 2006 stated:
The Royal College of Psychiatrists in 2007 stated:
The American Psychiatric Association stated:
A legal brief dated September 26, 2007, and presented on behalf of the American Psychological Association, California Psychological Association, American Psychiatric Association, National Association of Social Workers, and National Association of Social Workers, California Chapter, stated:

Sexual orientation change efforts are methods that aim to change a same-sex sexual orientation. They may include behavioral techniques, cognitive behavioral therapy, reparative therapy, psychoanalytic techniques, medical approaches, and religious and spiritual approaches.

No major mental health professional organization sanctions efforts to change sexual orientation and virtually all of them have adopted policy statements cautioning the profession and the public about treatments that purport to change sexual orientation. These include the American Psychiatric Association, American Psychological Association, American Counseling Association, National Association of Social Workers in the US, the Royal College of Psychiatrists, and the Australian Psychological Society.

In 2009, the American Psychological Association Task Force on Appropriate Therapeutic Responses to Sexual Orientation conducted a systematic review of the peer-reviewed journal literature on sexual orientation change efforts (SOCE) and concluded:
Efforts to change sexual orientation are unlikely to be successful and involve some risk of harm, contrary to the claims of SOCE practitioners and advocates. Even though the research and clinical literature demonstrate that same-sex sexual and romantic attractions, feelings, and behaviors are normal and positive variations of human sexuality, regardless of sexual orientation identity, the task force concluded that the population that undergoes SOCE tends to have strongly conservative religious views that lead them to seek to change their sexual orientation. Thus, the appropriate application of affirmative therapeutic interventions for those who seek SOCE involves therapist acceptance, support, and understanding of clients and the facilitation of clients' active coping, social support, and identity exploration and development, without imposing a specific sexual orientation identity outcome.

In 2012, the Pan American Health Organization (the North and South American branch of the World Health Organization) released a statement cautioning against services that purport to "cure" people with non-heterosexual sexual orientations as they lack medical justification and represent a serious threat to the health and well-being of affected people, and noted that the global scientific and professional consensus is that homosexuality is a normal and natural variation of human sexuality and cannot be regarded as a pathological condition. The Pan American Health Organization further called on governments, academic institutions, professional associations and the media to expose these practices and to promote respect for diversity. The World Health Organization affiliate further noted that gay minors have sometimes been forced to attend these "therapies" involuntarily, being deprived of their liberty and sometimes kept in isolation for several months, and that these findings were reported by several United Nations bodies. Additionally, the Pan American Health Organization recommended that such malpractices be denounced and subject to sanctions and penalties under national legislation, as they constitute a violation of the ethical principles of health care and violate human rights that are protected by international and regional agreements.

The National Association for Research & Therapy of Homosexuality (NARTH), which described itself as a "professional, scientific organization that offers hope to those who struggle with unwanted homosexuality," disagreed with the mainstream mental health community's position on conversion therapy, both on its effectiveness and by describing sexual orientation not as a binary immutable quality, or as a disease, but as a continuum of intensities of sexual attractions and emotional affect. The American Psychological Association and the Royal College of Psychiatrists expressed concerns that the positions espoused by NARTH are not supported by the science and create an environment in which prejudice and discrimination can flourish.

Varying definitions and strong social norms about sexuality can make sexual orientation difficult to quantify.

One of the earliest sexual orientation classification schemes was proposed in the 1860s by Karl Heinrich Ulrichs in a series of pamphlets he published privately. The classification scheme, which was meant only to describe males, separated them into three basic categories: "dionings, urnings" and "uranodionings". An "urning" can be further categorized by degree of effeminacy. These categories directly correspond with the categories of sexual orientation used today: "heterosexual", "homosexual", and "bisexual". In the series of pamphlets, Ulrichs outlined a set of questions to determine if a man was an "urning". The definitions of each category of Ulrichs' classification scheme are as follows:

From at least the late nineteenth century in Europe, there was speculation that the range of human sexual response looked more like a continuum than two or three discrete categories. Berlin sexologist Magnus Hirschfeld published a scheme in 1896 that measured the strength of an individual's sexual desire on two independent 10-point scales, A (homosexual) and B (heterosexual). A heterosexual individual may be A0, B5; a homosexual individual may be A5, B0; an asexual would be A0, B0; and someone with an intense attraction to both sexes would be A9, B9.

The Kinsey scale, also called the Heterosexual-Homosexual Rating Scale, was first published in "Sexual Behavior in the Human Male" (1948) by Alfred Kinsey, Wardell Pomeroy, and Clyde Martin and also featured in "Sexual Behavior in the Human Female" (1953). The scale was developed to combat the assumption at the time that people are either heterosexual or homosexual and that these two types represent antitheses in the sexual world. Recognizing that a large portion of population is not completely heterosexual or homosexual and people can experience both heterosexual and homosexual behavior and psychic responses, Kinsey et al., stated:
The Kinsey scale provides a classification of sexual orientation based on the relative amounts of heterosexual and homosexual experience or psychic response in one's history at a given time. The classification scheme works such that individuals in the same category show the same balance between the heterosexual and homosexual elements in their histories. The position on the scale is based on the relation of heterosexuality to homosexuality in one's history, rather than the actual amount of overt experience or psychic response. An individual can be assigned a position on the scale in accordance with the following definitions of the points of the scale:

The Kinsey scale has been praised for dismissing the dichotomous classification of sexual orientation and allowing for a new perspective on human sexuality. Despite seven categories being able to provide a more accurate description of sexual orientation than a dichotomous scale, it is still difficult to determine which category individuals should be assigned to. In a major study comparing sexual response in homosexual males and females, Masters and Johnson discuss the difficulty of assigning the Kinsey ratings to participants. Particularly, they found it difficult to determine the relative amount heterosexual and homosexual experience and response in a person's history when using the scale. They report finding it difficult to assign ratings 2–4 for individuals with a large number of heterosexual and homosexual experiences. When there are a substantial number of heterosexual and homosexual experiences in one's history, it becomes difficult for that individual to be fully objective in assessing the relative amount of each.

Weinrich et al. (1993) and Weinberg et al. (1994) criticized the scale for lumping individuals who are different based on different dimensions of sexuality into the same categories. When applying the scale, Kinsey considered two dimensions of sexual orientation: overt sexual experience and psychosexual reactions. Valuable information was lost by collapsing the two values into one final score. A person who has only predominantly same sex reactions is different from someone with relatively little reaction but lots of same sex experience. It would have been quite simple for Kinsey to have measured the two dimensions separately and report scores independently to avoid loss of information. Furthermore, there are more than two dimensions of sexuality to be considered. Beyond behavior and reactions, one could also assess attraction, identification, lifestyle, etc. This is addressed by the Klein Sexual Orientation Grid.

A third concern with the Kinsey scale is that it inappropriately measures heterosexuality and homosexuality on the same scale, making one a tradeoff of the other. Research in the 1970s on masculinity and femininity found that concepts of masculinity and femininity are more appropriately measured as independent concepts on a separate scale rather than as a single continuum, with each end representing opposite extremes. When compared on the same scale, they act as tradeoffs such, whereby to be more feminine one had to be less masculine and vice versa. However, if they are considered as separate dimensions one can be simultaneously very masculine and very feminine. Similarly, considering heterosexuality and homosexuality on separate scales would allow one to be both very heterosexual and very homosexual or not very much of either. When they are measured independently, the degree of heterosexual and homosexual can be independently determined, rather than the balance between heterosexual and homosexual as determined using the Kinsey Scale.

In response to the criticism of the Kinsey scale only measuring two dimensions of sexual orientation, Fritz Klein developed the Klein sexual orientation grid (KSOG), a multidimensional scale for describing sexual orientation. Introduced in Klein's book "The Bisexual Option" (1978), the KSOG uses a 7-point scale to assess seven different dimensions of sexuality at three different points in an individual's life: past (from early adolescence up to one year ago), present (within the last 12 months), and ideal (what would you choose if it were completely your choice).

The Sell Assessment of Sexual Orientation (SASO) was developed to address the major concerns with the Kinsey Scale and Klein Sexual Orientation Grid and as such, measures sexual orientation on a continuum, considers various dimensions of sexual orientation, and considers homosexuality and heterosexuality separately. Rather than providing a final solution to the question of how to best measure sexual orientation, the SASO is meant to provoke discussion and debate about measurements of sexual orientation.

The SASO consists of 12 questions. Six of these questions assess sexual attraction, four assess sexual behavior, and two assess sexual orientation identity. For each question on the scale that measures homosexuality there is a corresponding question that measures heterosexuality giving six matching pairs of questions. Taken all together, the six pairs of questions and responses provide a profile of an individual's sexual orientation. However, results can be further simplified into four summaries that look specifically at responses that correspond to either homosexuality, heterosexuality, bisexuality or asexuality.

Of all the questions on the scale, Sell considered those assessing sexual attraction to be the most important as sexual attraction is a better reflection of the concept of sexual orientation which he defined as "extent of sexual attractions toward members of the other, same, both sexes or neither" than either sexual identity or sexual behavior. Identity and behavior are measured as supplemental information because they are both closely tied to sexual attraction and sexual orientation. Major criticisms of the SASO have not been established, but a concern is that the reliability and validity remains largely unexamined.

Research focusing on sexual orientation uses scales of assessment to identify who belongs in which sexual population group. It is assumed that these scales will be able to reliably identify and categorize people by their sexual orientation. However, it is difficult to determine an individual's sexual orientation through scales of assessment, due to ambiguity regarding the definition of sexual orientation. Generally, there are three components of sexual orientation used in assessment. Their definitions and examples of how they may be assessed are as follows:

Though sexual attraction, behavior, and identity are all components of sexual orientation, if a person defined by one of these dimensions were congruent with those defined by another dimension it would not matter which was used in assessing orientation, but this is not the case. There is "little coherent relationship between the amount and mix of homosexual and heterosexual behavior in a person's biography and that person's choice to label himself or herself as bisexual, homosexual, or heterosexual". Individuals typically experience diverse attractions and behaviors that may reflect curiosity, experimentation, social pressure and is not necessarily indicative of an underlying sexual orientation. For example, a woman may have fantasies or thoughts about sex with other women but never act on these thoughts and only have sex with opposite gender partners. If sexual orientation was being assessed based on one's sexual attraction then this individual would be considered homosexual, but her behavior indicates heterosexuality.

As there is no research indicating which of the three components is essential in defining sexual orientation, all three are used independently and provide different conclusions regarding sexual orientation. Savin Williams (2006) discusses this issue and notes that by basing findings regarding sexual orientation on a single component, researchers may not actually capture the intended population. For example, if homosexual is defined by same sex behavior, gay virgins are omitted, heterosexuals engaging in same sex behavior for other reasons than preferred sexual arousal are miscounted, and those with same sex attraction who only have opposite-sex relations are excluded. Because of the limited populations that each component captures, consumers of research should be cautious in generalizing these findings.

One of the uses for scales that assess sexual orientation is determining what the prevalence of different sexual orientations are within a population. Depending on subject's age, culture and sex, the prevalence rates of homosexuality vary depending on which component of sexual orientation is being assessed: sexual attraction, sexual behavior, or sexual identity. Assessing sexual attraction will yield the greatest prevalence of homosexuality in a population whereby the proportion of individuals indicating they are same sex attracted is two to three times greater than the proportion reporting same sex behavior or identify as gay, lesbian, or bisexual. Furthermore, reports of same sex behavior usually exceed those of gay, lesbian, or bisexual identification. The following chart demonstrates how widely the prevalence of homosexuality can vary depending on what age, location and component of sexual orientation is being assessed:

The variance in prevalence rates is reflected in people's inconsistent responses to the different components of sexual orientation within a study and the instability of their responses over time. Laumann et al., (1994) found that among U.S. adults 20% of those who would be considered homosexual on one component of orientation were homosexual on the other two dimensions and 70% responded in a way that was consistent with homosexuality on only one of the three dimensions. Furthermore, sexuality may be fluid; for example, a person's sexual orientation identity is not necessarily stable or consistent over time but is subject to change throughout life. Diamond (2003) found that over 7 years 2/3 of the women changed their sexual identity at least once, with many reporting that the label was not adequate in capturing the diversity of their sexual or romantic feelings. Furthermore, women who relinquished bisexual and lesbian identification did not relinquish same sex sexuality and acknowledged the possibility for future same sex attractions or behaviour. One woman stated "I'm mainly straight but I'm one of those people who, if the right circumstance came along, would change my viewpoint". Therefore, individuals classified as homosexual in one study might not be identified the same way in another depending on which components are assessed and when the assessment is made making it difficult to pin point who is homosexual and who is not and what the overall prevalence within a population may be.

Depending on which component of sexual orientation is being assessed and referenced, different conclusions can be drawn about the prevalence rate of homosexuality which has real world consequences. Knowing how much of the population is made up of homosexual individuals influences how this population may be seen or treated by the public and government bodies. For example, if homosexual individuals constitute only 1% of the general population they are politically easier to ignore or than if they are known to be a constituency that surpasses most ethnic and ad minority groups. If the number is relatively minor then it is difficult to argue for community based same sex programs and services, mass media inclusion of gay role models, or Gay/Straight Alliances in schools. For this reason, in the 1970s Bruce Voeller, the chair of the National Gay and Lesbian Task Force perpetuated a common myth that the prevalence of homosexuality is 10% for the whole population by averaging a 13% number for men and a 7% number for women. Voeller generalized this finding and used it as part of the modern gay rights movement to convince politicians and the public that "we [gays and lesbians] are everywhere".

In the paper "Who's Gay? Does It Matter?", Ritch Savin-Williams proposes two different approaches to assessing sexual orientation until well positioned and psychometrically sound and tested definitions are developed that would allow research to reliably identify the prevalence, causes, and consequences of homosexuality.
He first suggests that greater priority should be given to sexual arousal and attraction over behaviour and identity because it is less prone to self- and other-deception, social conditions and variable meanings. To measure attraction and arousal he proposed that biological measures should be developed and used. There are numerous biological/physiological measures that exist that can measure sexual orientation such as sexual arousal, brain scans, eye tracking, body odour preference, and anatomical variations such as digit-length ratio and right or left-handedness.
Secondly, Savin-Williams suggests that researchers should forsake the general notion of sexual orientation altogether and assess only those components that are relevant to the research question being investigated. For example:

Means typically used include surveys, interviews, cross-cultural studies, physical arousal measurements sexual behavior, sexual fantasy, or a pattern of erotic arousal. The most common is verbal self-reporting or self-labeling, which depend on respondents being accurate about themselves.

Studying human sexual arousal has proved a fruitful way of understanding how men and women differ as genders and in terms of sexual orientation. A clinical measurement may use penile or vaginal photoplethysmography, where genital engorgement with blood is measured in response to exposure to different erotic material.

Some researchers who study sexual orientation argue that the concept may "not" apply similarly to men and women. A study of sexual arousal patterns found that women, when viewing erotic films which show female-female, male-male and male-female sexual activity (oral sex or penetration), have patterns of arousal which do "not" match their declared sexual orientations as well as men's. That is, heterosexual and lesbian women's sexual arousal to erotic films do "not" differ significantly by the genders of the participants (male or female) or by the type of sexual activity (heterosexual or homosexual). On the contrary, men's sexual arousal patterns tend to be more in line with their stated orientations, with heterosexual men showing more penis arousal to female-female sexual activity and less arousal to female-male and male-male sexual stimuli, and homosexual and bisexual men being more aroused by films depicting male-male intercourse and less aroused by other stimuli.

Another study on men and women's patterns of sexual arousal confirmed that men and women have different patterns of arousal, independent of their sexual orientations. The study found that women's genitals become aroused to both human and nonhuman stimuli from movies showing humans of both genders having sex (heterosexual and homosexual) and from videos showing non-human primates (bonobos) having sex. Men did "not" show any sexual arousal to non-human visual stimuli, their arousal patterns being in line with their specific sexual interest (women for heterosexual men and men for homosexual men).

These studies suggest that men and women are different in terms of sexual arousal patterns and that this is also reflected in how their genitals react to sexual stimuli of both genders or even to non-human stimuli. Sexual orientation has many dimensions (attractions, behavior, identity), of which sexual arousal is the only product of sexual attractions which can be measured at present with some degree of physical precision. Thus, the fact that women are aroused by seeing non-human primates having sex does not mean that women's sexual orientation includes this type of sexual interest. Some researchers argue that women's sexual orientation depends less on their patterns of sexual arousal than men's and that other components of sexual orientation (like emotional attachment) must be taken into account when describing women's sexual orientations. In contrast, men's sexual orientations tend to be primarily focused on the physical component of attractions and, thus, their sexual feelings are more exclusively oriented according to sex.

More recently, scientists have started to focus on measuring changes in brain activity related to sexual arousal, by using brain-scanning techniques. A study on how heterosexual and homosexual men's brains react to seeing pictures of naked men and women has found that both hetero- and homosexual men react positively to seeing their preferred sex, using the same brain regions. The only significant group difference between these orientations was found in the amygdala, a brain region known to be involved in regulating fear.

Research suggests that sexual orientation is independent of cultural and other social influences, but that open identification of one's sexual orientation may be hindered by homophobic/heterosexist settings. Social systems such as religion, language and ethnic traditions can have a powerful impact on realization of sexual orientation. Influences of culture may complicate the process of measuring sexual orientation. The majority of empirical and clinical research on LGBT populations are done with largely white, middle-class, well-educated samples, however there are pockets of research that document various other cultural groups, although these are frequently limited in diversity of gender and sexual orientation of the subjects. Integration of sexual orientation with sociocultural identity may be a challenge for LGBT individuals. Individuals may or may not consider their sexual orientation to define their sexual identity, as they may experience various degrees of fluidity of sexuality, or may simply identify more strongly with another aspect of their identity such as family role. American culture puts a great emphasis on individual attributes, and views the self as unchangeable and constant. In contrast, East Asian cultures put a great emphasis on a person's social role within social hierarchies, and view the self as fluid and malleable. These differing cultural perspectives have many implications on cognition of the self, including perception of sexual orientation.

Translation is a major obstacle when comparing different cultures. Many English terms lack equivalents in other languages, while concepts and words from other languages fail to be reflected in the English language. Translation and vocabulary obstacles are not limited to the English language. Language can force individuals to identify with a label that may or may not accurately reflect their true sexual orientation. Language can also be used to signal sexual orientation to others. The meaning of words referencing categories of sexual orientation are negotiated in the mass media in relation to social organization. New words may be brought into use to describe new terms or better describe complex interpretations of sexual orientation. Other words may pick up new layers or meaning. For example, the heterosexual Spanish terms "marido" and "mujer" for "husband" and "wife", respectively, have recently been replaced in Spain by the gender-neutral terms "cónyuges" or "consortes" meaning "spouses".

One person may presume knowledge of another person's sexual orientation based upon perceived characteristics, such as appearance, clothing, voice (c.f. Gay male speech), and accompaniment by and behavior with other people. The attempt to detect sexual orientation in social situations is sometimes colloquially known as gaydar; some studies have found that guesses based on face photos perform better than chance. 2015 research suggests that "gaydar" is an alternate label for using LGBT stereotypes to infer orientation, and that face-shape is not an accurate indication of orientation.

Perceived sexual orientation may affect how a person is treated. For instance, in the United States, the FBI reported that 15.6% of hate crimes reported to police in 2004 were "because of a sexual-orientation bias". Under the UK Employment Equality (Sexual Orientation) Regulations 2003, as explained by Advisory, Conciliation and Arbitration Service, "workers or job applicants must not be treated less favourably because of their sexual orientation, their perceived sexual orientation or because they associate with someone of a particular sexual orientation".

In Euro-American cultures, sexual orientation is defined by the gender(s) of the people a person is romantically or sexually attracted to. Euro-American culture generally assumes heterosexuality, unless otherwise specified. Cultural norms, values, traditions and laws facilitate heterosexuality, including constructs of marriage and family. Efforts are being made to change these attitudes, and legislation is being passed to promote equality.
Some other cultures do not recognize a homosexual/heterosexual/bisexual distinction. It is common to distinguish a person's sexuality according to their sexual role (active/passive; insertive/penetrated). In this distinction, the passive role is typically associated with femininity or inferiority, while the active role is typically associated with masculinity or superiority. For example, an investigation of a small Brazilian fishing village revealed three sexual categories for men: men who have sex only with men (consistently in a passive role), men who have sex only with women, and men who have sex with women and men (consistently in an active role). While men who consistently occupied the passive role were recognized as a distinct group by locals, men who have sex with only women, and men who have sex with women and men, were not differentiated. Little is known about same-sex attracted females, or sexual behavior between females in these cultures.

In the United States, non-Caucasian LGBT individuals may find themselves in a double minority, where they are neither fully accepted or understood by mainly Caucasian LGBT communities, nor are they accepted by their own ethnic group. Many people experience racism in the dominant LGBT community where racial stereotypes merge with gender stereotypes, such that Asian-American LGBTs are viewed as more passive and feminine, while African-American LGBTs are viewed as more masculine and aggressive. There are a number of culturally specific support networks for LGBT individuals active in the United States. For example, "Ô-Môi" for Vietnamese American queer females.

Sexuality in the context of religion is often a controversial subject, especially that of sexual orientation. In the past, various sects have viewed homosexuality from a negative point of view and had punishments for same-sex relationships. In modern times, an increasing number of religions and religious denominations accept homosexuality. It is possible to integrate sexual identity and religious identity, depending on the interpretation of religious texts.

Some religious organizations object to the concept of sexual orientation entirely. In the 2014 revision of the code of ethics of the American Association of Christian Counselors, members are forbidden to "describe or reduce human identity and nature to sexual orientation or reference," even while counselors must acknowledge the client's fundamental right to self-determination.

The internet has influenced sexual orientation in two ways: it is a common mode of discourse on the subject of sexual orientation and sexual identity, and therefore shapes popular conceptions; and it allows anonymous attainment of sexual partners, as well as facilitates communication and connection between greater numbers of people.

The multiple aspects of sexual orientation and the boundary-drawing problems already described create methodological challenges for the study of the demographics of sexual orientation. Determining the frequency of various sexual orientations in real-world populations is difficult and controversial.

Modern scientific surveys find that the majority of people report a heterosexual orientation. However, the relative percentage of the population that reports a homosexual orientation varies with differing methodologies and selection criteria. Most of these statistical findings are in the range of 2.8 to 9% of males, and 1 to 5% of females for the United States – this figure can be as high as 12% for some large cities and as low as 1% for rural areas.

Estimates for the percentage of the population that are bisexual vary widely, at least in part due to differing definitions of bisexuality. Some studies only consider a person bisexual if they are nearly equally attracted to both sexes, and others consider a person bisexual if they are "at all" attracted to the same sex (for otherwise mostly heterosexual persons) or to the opposite sex (for otherwise mostly homosexual persons). A small percentage of people are not sexually attracted to anyone (asexuality). A study in 2004 placed the prevalence of asexuality at 1%.

In the oft-cited and oft-criticized "Sexual Behavior in the Human Male" (1948) and "Sexual Behavior in the Human Female" (1953), by Alfred C. Kinsey et al., people were asked to rate themselves on a scale from completely heterosexual to completely homosexual. Kinsey reported that when the individuals' behavior, as well as their identity, are analyzed, a significant number of people appeared to be at least somewhat bisexual – i.e., they have some attraction to either sex, although usually one sex is preferred. However, only a small minority can be considered fully bisexual (with an equal attraction to both sexes). Kinsey's methods have been criticized as flawed, particularly with regard to the randomness of his sample population, which included prison inmates, male prostitutes and those who willingly participated in discussion of previously taboo sexual topics. Nevertheless, Paul Gebhard, subsequent director of the Kinsey Institute for Sex Research, reexamined the data in the Kinsey Reports and concluded that removing the prison inmates and prostitutes barely affected the results.

Because sexual orientation is complex and multi-dimensional, some academics and researchers, especially in queer studies, have argued that it is a historical and social construction. In 1976, philosopher and historian Michel Foucault argued in "The History of Sexuality" that homosexuality as an identity did not exist in the eighteenth century; that people instead spoke of "sodomy," which referred to sexual acts. Sodomy was a crime that was often ignored, but sometimes punished severely (see sodomy law). He wrote, "'Sexuality' is an invention of the modern state, the industrial revolution, and capitalism."

Sexual orientation is argued as a concept that evolved in the industrialized West, and there is a controversy as to the universality of its application in other societies or cultures. Non-westernized concepts of male sexuality differ essentially from the way sexuality is seen and classified under the Western system of sexual orientation. The validity of the notion of sexual orientation as defined in the West, as a biological phenomenon rather than a social construction specific to a region and period, has also been questioned within the industrialized Western society).

Heterosexuality and homosexuality are terms often used in European and American cultures to encompass a person's entire social identity, which includes self and personality. In Western cultures, some people speak meaningfully of gay, lesbian, and bisexual identities and communities. In other cultures, homosexuality and heterosexual labels do not emphasize an entire social identity or indicate community affiliation based on sexual orientation.

Some historians and researchers argue that the emotional and affectionate activities associated with sexual-orientation terms such as "gay" and "heterosexual" change significantly over time and across cultural boundaries. For example, in many English-speaking nations, it is assumed that same-sex kissing, particularly between men, is a sign of homosexuality, whereas various types of same-sex kissing are common expressions of friendship in other nations. Also, many modern and historic cultures have formal ceremonies expressing long-term commitment between same-sex friends, even though homosexuality itself is taboo within the cultures.

Two researchers, raising (1995) 'serious doubt whether sexual orientation is a valid concept at all,' warned against increasing politicization of this area.

Professor Michael King stated, "The conclusion reached by scientists who have investigated the origins and stability of sexual orientation is that it is a human characteristic that is formed early in life, and is resistant to change. Scientific evidence on the origins of homosexuality is considered relevant to theological and social debate because it undermines suggestions that sexual orientation is a choice."

Legally as well, a person's sexual orientation is hard to establish as either an intrinsic or a binary quality. In 1999, law professor David Cruz wrote that "sexual orientation (and the related concept homosexuality) might plausibly refer to a variety of different attributes, singly or in combination. What is not immediately clear is whether one conception is most suited to all social, legal, and constitutional purposes."




</doc>
<doc id="29253" url="https://en.wikipedia.org/wiki?curid=29253" title="Spandrel">
Spandrel

A spandrel is a triangular space, usually found in pairs, between the top of an arch and a rectangular frame; between the tops of two adjacent arches or one of the four spaces between a circle within a square. They are frequently filled with decorative elements.

There are four or five accepted and cognate meanings of "spandrel" in architectural and art history, mostly relating to the space between a curved figure and a rectangular boundary – such as the space between the curve of an arch and a rectilinear bounding moulding, or the wallspace bounded by adjacent arches in an arcade and the stringcourse or moulding above them, or the space between the central medallion of a carpet and its rectangular corners, or the space between the circular face of a clock and the corners of the square revealed by its hood. Also included is the space under a flight of stairs, if it is not occupied by another flight of stairs.

In a building with more than one floor, the term spandrel is also used to indicate the space between the top of the window in one story and the sill of the window in the story above. The term is typically employed when there is a sculpted panel or other decorative element in this space, or when the space between the windows is filled with opaque or translucent glass, in this case called "spandrel glass". In concrete or steel construction, an exterior beam extending from column to column usually carrying an exterior wall load is known as a spandrel beam.

The spandrels over doorways in perpendicular work are generally richly decorated. At Magdalen College, Oxford, is one which is perforated. The spandrel of doors is sometimes ornamented in the Decorated Period, but seldom forms part of the composition of the doorway itself, being generally over the label.

Spandrels can also occur in the construction of domes and are typical in grand architecture from the medieval period onwards. Where a dome needed to rest on a square or rectangular base, the dome was raised above the level of the supporting pillars, with three-dimensional spandrels called pendentives taking the weight of the dome and concentrating it onto the pillars.




</doc>
<doc id="29257" url="https://en.wikipedia.org/wiki?curid=29257" title="SimpleText">
SimpleText

SimpleText is the native text editor for the Apple classic Mac OS. SimpleText allows editing including text formatting (underline, italic, bold, etc.), fonts, and sizes. It was developed to integrate the features included in the different versions of TeachText that were created by various software development groups within Apple.

It can be considered similar to Windows' WordPad application. In later versions it also gained additional read only display capabilities for PICT files, as well as other Mac OS built-in formats like Quickdraw GX and QTIF, 3DMF and even QuickTime movies. SimpleText can even record short sound samples and, using Apple's PlainTalk speech system, read out text in English. Users who wanted to add sounds longer than 24 seconds, however, needed to use a separate program to create the sound and then paste the desired sound into the document using ResEdit.

SimpleText superseded TeachText, which was included in System Software up until Mac OS 8. The need for SimpleText arose after Apple stopped bundling MacWrite, to ensure that every user could open and read Readme documents.

The key improvement of SimpleText over TeachText was the addition of text styling. The underlying OS required by SimpleText implemented a standard styled text format, which meant that SimpleText could support multiple fonts and font sizes. Prior Macintosh OS versions lacked this feature, so TeachText supported only a single font per document. Adding text styling features made SimpleText WorldScript-savvy, meaning that it can use Simplified and Traditional Chinese characters. Like TeachText, SimpleText was also limited to only 32 kB of text in a document, although images could increase the total file size beyond this limit. SimpleText style information was stored in the file's resource fork in such a way that if the resource fork was stripped (such as by uploading to a non-Macintosh server), the text information would be retained.

In Mac OS X, SimpleText is replaced by the more powerful TextEdit application, which reads and writes more document formats as well as including word processor-like features such as a ruler and spell checking. TextEdit's styled text format is RTF, which is able to survive a single-forked file system intact.

Apple has released the source code for a Carbon version of SimpleText in the Mac OS X Panther Developer Tools. If the Developer Tools are installed, it can be found at /Developer/Examples/Carbon/SimpleText.




</doc>
<doc id="29263" url="https://en.wikipedia.org/wiki?curid=29263" title="Statute of Westminster 1931">
Statute of Westminster 1931

The Statute of Westminster 1931 is an Act of the Parliament of the United Kingdom whose modified versions are now domestic law within Australia and Canada; it has been repealed in New Zealand and implicitly in former Dominions that are no longer Commonwealth realms. Passed on 11 December 1931, the act, either immediately or upon ratification, effectively both established the legislative independence of the self-governing Dominions of the British Empire from the United Kingdom and bound them all to seek each other's approval for changes to monarchical titles and the common line of succession. It thus became a statutory embodiment of the principles of equality and common allegiance to the Crown set out in the Balfour Declaration of 1926. As the statute removed nearly all of the British parliament's authority to legislate for the Dominions, it had the effect of making the Dominions largely sovereign nations in their own right. It was a crucial step in the development of the Dominions as separate states.

The Statute of Westminster's relevance today is that it sets the basis for the continuing relationship between the Commonwealth realms and the Crown.

The Statute of Westminster gave effect to certain political resolutions passed by the Imperial Conferences of 1926 and 1930; in particular, the Balfour Declaration of 1926. The main effect was the removal of the ability of the British parliament to legislate for the Dominions, part of which also required the repeal of the Colonial Laws Validity Act 1865 in its application to the Dominions. King George V expressed his desire that the laws of royal succession be exempt from the statute's provisions, but it was determined that this would be contrary to the principles of equality set out in the Balfour Declaration. Both Canada and the Irish Free State pushed for the ability to amend the succession laws themselves and section 2(2) (allowing a Dominion to amend or repeal laws of paramount force, such as the succession laws, insofar as they are part of the law of that Dominion) was included in the Statute of Westminster at Canada's insistence. After the Statute was passed, the British parliament could no longer make laws for the Dominions, other than with the request and consent of the government of that Dominion. Before then, the Dominions had legally been self-governing colonies of the United Kingdom. However, the statute had the effect of making them sovereign nations once they adopted it.

The statute provides in section 4:
It also provides in section 2(1):
The whole Statute applied to Canada, the Irish Free State, and the Union of South Africa without the need for any acts of ratification; the governments of those countries gave their consent to the application of the law to their respective jurisdiction. Section 10 of the Statute provided that sections 2 to 6 would apply in the other three Dominions—Australia, New Zealand, and Newfoundland—only after the parliament of that Dominion had legislated to adopt them.

Since 1931, over a dozen new Commonwealth realms have been created, all of which now hold the same powers as the United Kingdom, Canada, Australia, and New Zealand over matters of change to the monarchy, though the Statute of Westminster is not part of their laws. Ireland and South Africa are now republics and Newfoundland is part of Canada.

Australia adopted sections 2 to 6 of the Statute of Westminster with the Statute of Westminster Adoption Act 1942, in order to clarify the validity of certain Australian legislation relating to the Second World War; the adoption was backdated to 3 September 1939, the date that Britain and Australia joined the war.

Adopting section 2 of the statute clarified that the Commonwealth parliament was able to legislate inconsistently with British legislation, adopting section 3 clarified that it could legislate with extraterritorial effect. Adopting section 4 clarified that Britain could legislate with effect on Australia as a whole only with Australia's request and consent.

Nonetheless, under section 9 of the statute, on matters not within Commonwealth power Britain could still legislate with effect in all or any of the Australian states, without the agreement of the Commonwealth although only to the extent of "the constitutional practice existing before the commencement" of the statute. However, this capacity was never used. In particular, it was not used to implement the result of the 1933 Western Australian secession referendum, as it did not have the support of the Australian government.
All British power to legislate with effect in Australia ended with the Australia Act 1986, the British version of which says that it was passed with the request and consent of the Australian parliament, which had obtained the concurrence of the Australian states.

This statute limited the legislative authority of the British parliament over Canada, effectively giving the country legal autonomy as a self-governing Dominion, though the British parliament retained the power to amend Canada's constitution at the request of the Parliament of Canada. That authority remained in effect until the Constitution Act, 1982, which transferred it to Canada, the final step to achieving full sovereignty.
The British North America Acts—the written elements (in 1931) of the Canadian constitution—were excluded from the application of the statute because of disagreements between the Canadian provinces and the federal government over how the British North America Acts could be otherwise amended. These disagreements were resolved only in time for the passage of the Canada Act 1982, thus completing the patriation of the Canadian constitution to Canada. At that time, the Canadian parliament also repealed sections 4 and 7(1) of the Statute of Westminster. The Statute of Westminster remains a part of the constitution of Canada by virtue of section 52(2)(b) of the Constitution Act, 1982.

As a consequence of the statute's adoption, the Parliament of Canada gained the ability to abolish appeals to the Judicial Committee of the Privy Council. Criminal appeals were abolished in 1933, while civil appeals continued until 1949. The passage of the Statute of Westminster meant that changes in British legislation governing the succession to the throne no longer automatically applied to Canada.

The Irish Free State never formally adopted the Statute of Westminster, its Executive Council (cabinet) taking the view that the Anglo-Irish Treaty of 1921 had already ended Westminster's right to legislate for the Free State. The Free State's constitution gave the Oireachtas "sole and exclusive power of making laws". Hence, even before 1931, the Free State did not arrest British Army and Royal Air Force deserters on its territory, even though the UK believed post-1922 British laws gave the Free State's Garda Síochána the power to do so. The UK's Irish Free State Constitution Act 1922 said, however, " in the [Free State] Constitution shall be construed as prejudicing the power of [the British] Parliament to make laws affecting the Irish Free State in any case where, in accordance with constitutional practice, Parliament would make laws affecting other self-governing Dominions".
Motions of approval of the Report of the Commonwealth Conference had been passed by the Dáil and Seanad in May 1931 and the final form of the Statute of Westminster included the Irish Free State among the Dominions the British parliament could not legislate for without the Dominion's request and consent. Originally, the UK government had wanted to exclude from the Statute of Westminster the legislation underpinning the 1921 treaty, from which the Free State's constitution had emerged. Executive Council President (Prime Minister) W. T. Cosgrave objected, although he promised that the Executive Council would not amend the legislation unilaterally. The other Dominions backed Cosgrave and, when an amendment to similar effect was proposed at Westminster by John Gretton, parliament duly voted it down. When the Statute became law in the UK, Patrick McGilligan, the Free State Minister for External Affairs, stated: "It is a solemn declaration by the British people through their representatives in Parliament that the powers inherent in the Treaty position are what we have proclaimed them to be for the last ten years." He went on to present the Statute as largely the fruit of the Free State's efforts to secure for the other Dominions the same benefits it already enjoyed under the treaty. The Statute of Westminster had the effect of making the Free State the first internationally recognised independent Irish state.

After Éamon de Valera led Fianna Fáil to victory in the Free State election of 1932, he began removing the monarchical elements of the constitution, beginning with the Oath of Allegiance. De Valera initially considered invoking the Statute of Westminster in making these changes, but John J. Hearne advised him not to. Abolishing the Oath of Allegiance in effect abrogated the 1921 treaty. Generally, the British thought that this was morally objectionable but legally permitted by the Statute of Westminster. Robert Lyon Moore, a southern unionist from County Donegal, challenged the legality of the abolition in the Free State courts and then appealed to the Judicial Committee of the Privy Council (JCPC) in London. However, the Free State had also abolished the right of appeal to the JCPC. In 1935, the JCPC ruled that both abolitions were valid under the Statute of Westminster. The Free State, which in 1937 was renamed "Ireland", left the Commonwealth in 1949 upon the coming into force of its Republic of Ireland Act.

The Parliament of New Zealand adopted the Statute of Westminster by passing its Statute of Westminster Adoption Act 1947 in November 1947. The New Zealand Constitution Amendment Act, passed the same year, empowered the New Zealand parliament to change the constitution, but did not remove the ability of the British parliament to legislate regarding the New Zealand constitution. The remaining role of the British parliament was removed by the New Zealand Constitution Act 1986 and the Statute of Westminster was repealed in its entirety.

The Dominion of Newfoundland never adopted the Statute of Westminster, especially because of financial troubles and corruption there. By request of the Dominion's government, the United Kingdom established the Commission of Government in 1934, resuming direct rule of Newfoundland. That arrangement remained until Newfoundland became a province of Canada in 1949 following referendums on the issue in 1948.

Although the Union of South Africa was not among the Dominions that needed to adopt the Statute of Westminster for it to take effect, two laws—the Status of the Union Act, 1934, and the Royal Executive Functions and Seals Act of 1934—were passed to confirm South Africa's status as a sovereign state.

The preamble to the Statute of Westminster sets out conventions which affect attempts to change the rules of succession to the Crown. The second paragraph of the preamble to the Statute reads:

This means, for example, that any change in any realm to the Act of Settlement's provisions barring Roman Catholics from the throne would require the unanimous assent of the parliaments of all the other Commonwealth realms if the shared aspect of the Crown is to be retained. The preamble does not itself contain enforceable provisions, it merely expresses a constitutional convention, albeit one fundamental to the basis of the relationship between the Commonwealth realms. (As sovereign nations, each is free to withdraw from the arrangement, using their respective process for constitutional amendment.) Additionally, per section 4, if a realm wished for a British act amending the Act of Settlement in the UK to become part of that realm's laws, thereby amending the Act of Settlement in that realm, it would have to request and consent to the British act and the British act would have to state that such request and consent had been given. Section 4 of the Statute of Westminster has been repealed in a number of realms, however, and replaced by other constitutional clauses absolutely disallowing the British parliament from legislating for those realms.

This has raised some logistical concerns, as it would mean multiple parliaments would all have to assent to any future changes in any realm to its line of succession, as with the Perth Agreement's proposals to abolish male-preference primogeniture.

During the abdication crisis in 1936, British Prime Minister Stanley Baldwin consulted the Commonwealth prime ministers at the request of King Edward VIII. The King wanted to marry Wallis Simpson, whom Baldwin and other British politicians considered unacceptable as queen, as she was an American divorcée. Baldwin was able to get the then five Dominion prime ministers to agree with this and thus register their official disapproval at the King's planned marriage. The King later requested the Commonwealth prime ministers be consulted on a compromise plan, in which he would wed Simpson under a morganatic marriage pursuant to which she would not become queen. Under Baldwin's pressure, this plan was also rejected by the Dominions. All of these negotiations occurred at a diplomatic level and never went to the Commonwealth parliaments. However, the enabling legislation that allowed for the actual abdication (His Majesty's Declaration of Abdication Act 1936) did require the assent of each Dominion parliament to be passed and the request and consent of the Dominion governments so as to allow it to be part of the law of each Dominion. For expediency and to avoid embarrassment, the British government had suggested the Dominion governments regard whoever is monarch of the UK to automatically be their monarch. However, the Dominions rejected this; Prime Minister of Canada William Lyon Mackenzie King pointed out that the Statute of Westminster required Canada's request and consent to any legislation passed by the British parliament before it could become part of Canada's laws and affect the line of succession in Canada. The text of the British act states that Canada requested and consented (the only Dominion to formally do both) to the act applying in Canada under the Statute of Westminster, while Australia, New Zealand, and the Union of South Africa simply assented.

In February 1937, the South African parliament formally gave its assent by passing His Majesty King Edward the Eighth's Abdication Act, 1937, which declared that Edward had abdicated on 10 December 1936; that he and his descendants, if any, would have no right of succession to the throne; and that the Royal Marriages Act 1772 would not apply to him or his descendants, if any. The move was largely done for symbolic purposes, in an attempt by Prime Minister J. B. M. Hertzog to assert South Africa's independence from Britain. In Canada, the federal parliament passed the Succession to the Throne Act 1937, to assent to His Majesty's Declaration of Abdication Act and ratify the government's request and consent to it. In the Irish Free State, Prime Minister Éamon de Valera used the departure of Edward as an opportunity to remove all explicit mention of the monarch from the constitution of the Irish Free State, through the Constitution (Amendment No. 27) Act 1936, passed on 11 December 1936. The following day, the External Relations Act provided for the king to carry out certain diplomatic functions, if authorised by law; the same Act also brought Edward VIII's Instrument of Abdication into effect for the purposes of Irish law (s. 3(2)). A new Constitution of Ireland, with a president, was approved by Irish voters in 1937, with the Irish Free State becoming simply "Ireland", or, in the Irish language, "Éire". However, the head of state of Ireland remained unclear until 1949, when Ireland unambiguously became a republic outside the Commonwealth of Nations by enacting the Republic of Ireland Act 1948.

In some countries where the Statute of Westminster forms a part of the constitution, the anniversary of the date of the passage of the original British statute is commemorated as Statute of Westminster Day. In Canada, it is mandated that, on 11 December, the Royal Union Flag (as the Union Jack is called by law in Canada) is to be flown at properties owned by the federal Crown, where the requisite second flag pole is available.




</doc>
<doc id="29265" url="https://en.wikipedia.org/wiki?curid=29265" title="Serbia">
Serbia

Serbia (, ), officially the Republic of Serbia (, ), is a land-locked country situated at the crossroads of Central and Southeast Europe in the southern Pannonian Plain and the central Balkans. It borders Hungary to the north, Romania to the northeast, Bulgaria to the southeast, North Macedonia to the south, Croatia and Bosnia and Herzegovina to the west, and Montenegro to the southwest. The country claims a border with Albania through the disputed territory of Kosovo. Serbia's population numbers approximately seven million. Its capital, Belgrade, ranks among the largest citiеs in southeastern Europe. 

Inhabited since the Paleolithic Age, the territory of modern-day Serbia faced Slavic migrations to Southeastern Europe in the 6th century, establishing several regional states in the early Middle Ages at times recognised as tributaries to the Byzantine, Frankish and Hungarian kingdoms. The Serbian Kingdom obtained recognition by the Holy See and Constantinople in 1217, reaching its territorial apex in 1346 as the relatively short-lived Serbian Empire. By the mid-16th century, the Ottomans annexed the entirety of modern-day Serbia; their rule was at times interrupted by the Habsburg Empire, which began expanding towards Central Serbia from the end of the 17th century while maintaining a foothold in Vojvodina. In the early 19th century, the Serbian Revolution established the nation-state as the region's first constitutional monarchy, which subsequently expanded its territory. Following disastrous casualties in World War I, and the subsequent unification of the former Habsburg crownland of Vojvodina (and other lands) with Serbia, the country co-founded Yugoslavia with other South Slavic peoples, which would exist in various political formations until the Yugoslav Wars of the 1990s. During the breakup of Yugoslavia, Serbia formed a union with Montenegro, which was peacefully dissolved in 2006 and restoring Serbia's independence as a sovereign state for the first time since the late 1910s. In 2008, the parliament of the province of Kosovo unilaterally declared independence, with mixed responses from the international community.

A unitary parliamentary constitutional republic, Serbia is a member of the UN, CoE, OSCE, PfP, BSEC, CEFTA, and is acceding to the WTO. Since 2014, the country has been negotiating its EU accession with the perspective of joining the European Union by 2025. Like some other European countries, Serbia has suffered from democratic backsliding in recent years, having dropped in ranking from "Free" to "Partly Free" in the 2019 Freedom House report. Since 2007, Serbia formally adheres to the policy of military neutrality. The country provides social security, universal health care system, and a tuition-free primary and secondary education to its citizens. An upper-middle-income economy with a dominant service sector, the country ranks relatively high on the Human Development Index (63rd) and Social Progress Index (45th) as well as the Global Peace Index (50th).

The origin of the name "Serbia" is unclear. Historically, authors have mentioned the Serbs ( / Срби) and the Sorbs of eastern Germany (Upper Sorbian: "Serbja"; Lower Sorbian: "Serby") in a variety of ways: Surbii, Suurbi, Serbloi, Zeriuani, Sorabi, Surben, Sarbi, Serbii, Serboi, Zirbi, Surbi, Sorben, etc. These authors used these names to refer to Serbs and Sorbs in areas where their historical (or current) presence was/is not disputed (notably in the Balkans and Lusatia). However, there are also sources that mention same or similar names in other parts of the World (most notably in the Asiatic Sarmatia in the Caucasus).

The Proto-Slavic root word *sъrbъ has been variously connected with Russian "paserb" (пасерб, "stepson"), Ukrainian "pryserbytysia" (присербитися, "join in"), Old Indic "sarbh-" ("fight, cut, kill"), Latin "sero" ("make up, constitute"), and Greek "siro" (ειρω, "repeat"). Polish linguist Stanisław Rospond (1906–1982) derived the Serbian language ethnonym "Srb" from "srbati" (cf. "sorbo", "absorbo"). Sorbian scholar H. Schuster-Šewc suggested a connection with the Proto-Slavic verb for "to slurp" *sьrb-, with cognates such as "сёрбать" (Russian), "сьорбати" (Ukrainian), "сёрбаць" (Belarusian), "srbati" (Slovak), "сърбам" (Bulgarian) and "серебати" (Old Russian).

Some scholars based on the claim of Constantine VII Porphyrogenitus suggest that name Serb comes from the Latin servus, "servant" or "slave". English word "slave" has similar derivation, which is thought to have come from the name Slav. 

From 1945 to 1963, the official name for Serbia was the People's Republic of Serbia, later renamed the Socialist Republic of Serbia from 1963 to 1990. Since 1990, the official name of the country has been the Republic of Serbia. From 1992 to 2006, however, the official names of the country Serbia was a part of were the Federal Republic of Yugoslavia and then the State Union of Serbia and Montenegro.

Archaeological evidence of Paleolithic settlements on the territory of present-day Serbia is scarce. A fragment of a human jaw was found in Sićevo (Mala Balanica) and is believed to be up to 525,000–397,000 years old.
Approximately around 6,500 years BC, during the Neolithic, the Starčevo and Vinča cultures existed in the region of modern-day Belgrade. They dominated much of Southeastern Europe, (as well as parts of Central Europe and Asia Minor). Several important archaeological sites from this era, including Lepenski Vir and Vinča-Belo Brdo, still exist near the banks of the Danube.

During the Iron Age, local tribes of Triballi, Dardani, and Autariatae were encountered by the Ancient Greeks during their cultural and political expansion into the region, from the 5th up to the 2nd century BC. The Celtic tribe of Scordisci settled throughout the area in the 3rd century BC. It formed a tribal state, building several fortifications, including their capital at Singidunum (present-day Belgrade) and Naissos (present-day Niš).

The Romans conquered much of the territory in the 2nd century BC. In 167 BC the Roman province of Illyricum was established; the remainder was conquered around 75 BC, forming the Roman province of Moesia Superior; the modern-day Srem region was conquered in 9 BC; and Bačka and Banat in 106 AD after the Dacian Wars. As a result of this, contemporary Serbia extends fully or partially over several former Roman provinces, including Moesia, Pannonia, Praevalitana, Dalmatia, Dacia and Macedonia.

The chief towns of Upper Moesia (and broader) were: Singidunum (Belgrade), Viminacium (now Old Kostolac), Remesiana (now Bela Palanka), Naissos (Niš), and Sirmium (now Sremska Mitrovica), the latter of which served as a Roman capital during the Tetrarchy. Seventeen Roman Emperors were born in the area of modern-day Serbia, second only to contemporary Italy. The most famous of these was Constantine the Great, the first Christian Emperor, who issued an edict ordering religious tolerance throughout the Empire.

When the Roman Empire was divided in 395, most of Serbia remained under the Eastern Roman Empire. At the same time, its northwestern parts were included in the Western Roman Empire. By the 6th century, South Slavs migrated into the European provinces of the Byzantine Empire in large numbers. They merged with the local Romanised population that was gradually assimilated.

White Serbs, an early Slavic tribe from White Serbia first settled in an area near Thessaloniki on the Balkans and in the 6th and early 7th century, established the Serbian Principality by the 8th century. It was said in 822 that the Serbs inhabited the more significant part of Roman Dalmatia, their territory spanning what is today southwestern Serbia and parts of neighbouring countries. Meanwhile, the Byzantine Empire and the Bulgarian Empire held other parts of the territory. The Serbian rulers adopted Christianity in ca. 870, and by the mid-10th-century the Serbian state stretched the Adriatic Sea by the Neretva, the Sava, the Morava, and Skadar. Between 1166 and 1371 Serbia was ruled by the Nemanjić dynasty (whose legacy is especially cherished), under whom the state was elevated to a kingdom (and briefly an empire) and Serbian bishopric to an autocephalous archbishopric (through the effort of Sava, the country's patron saint). Monuments of the Nemanjić period survive in many monasteries (several being World Heritage sites) and fortifications. During these centuries the Serbian state (and influence) expanded significantly. The northern part, Vojvodina, was ruled by the Kingdom of Hungary. The period known as the Fall of the Serbian Empire saw the once-powerful state fragmented into duchies, culminating in the Battle of Kosovo (1389) against the rising Ottoman Empire. The Ottomans finally conquered the Serbian Despotate in 1459. The Ottoman threat and eventual conquest saw massive migrations of Serbs to the west and north.

In all Serbian lands conquered by the Ottomans, the native nobility was eliminated and the peasantry was enserfed to Ottoman rulers, while much of the clergy fled or were confined to the isolated monasteries. Under the Ottoman system, Serbs, as Christians, were considered an inferior class of people and subjected to heavy taxes, and a portion of the Serbian population experienced Islamisation. The Serbian Patriarchate of Peć was extinguished in 1463, but reestablished it in 1557, providing for limited continuation of Serbian cultural traditions within the Ottoman Empire, under the Millet system.

After the loss of statehood to the Ottoman Empire, Serbian resistance continued in northern regions (modern Vojvodina), under titular despots (until 1537), and popular leaders like Jovan Nenad (1526–1527). From 1521 to 1552, Ottomans conquered Belgrade and regions of Syrmia, Bačka, and Banat. Continuing wars and various rebellions constantly challenged Ottoman rule. One of the most significant was the Banat Uprising in 1594 and 1595, which was part of the Long War (1593–1606) between the Habsburgs and the Ottomans. The area of modern Vojvodina endured a century-long Ottoman occupation before being ceded to the Habsburg Empire, partially by the Treaty of Karlovci (1699), and fully by the Treaty of Požarevac (1718).

As the Great Serb Migrations depopulated most of southern Serbia, the Serbs sought refuge across the Danube River in Vojvodina to the north and the Military Frontier in the west, where they were granted rights by the Austrian crown under measures such as the "Statuta Wallachorum" of 1630. Much of central Serbia switched from Ottoman rule to Habsburg control (1686–91) during the Habsburg-Ottoman war (1683-1699). Following several petitions, Emperor Leopold I formally granted Serbs who wished to settle in the northern regions the right to their autonomous crown land. The ecclesiastical centre of the Serbs also moved northwards, to the Metropolitanate of Karlovci, and the Serbian Patriarchate of Peć was once-again abolished by the Ottomans in 1766. 

In 1718–39, the Habsburg Monarchy occupied much of Central Serbia and established the "Kingdom of Serbia" (1718–1739). Those gains were lost by the Treaty of Belgrade in 1739, when the Ottomans retook the region. Apart from territory of modern Vojvodina which remained under the Habsburg Empire, central regions of Serbia were occupied once again by the Habsburgs in 1788–1792.

The Serbian Revolution for independence from the Ottoman Empire lasted eleven years, from 1804 until 1815. The revolution comprised two separate uprisings which gained autonomy from the Ottoman Empire (1830) that eventually evolved towards full independence (1878). During the First Serbian Uprising (1804–1813), led by vožd Karađorđe Petrović, Serbia was independent for almost a decade before the Ottoman army was able to reoccupy the country. Shortly after this, the Second Serbian Uprising began in 1815. Led by Miloš Obrenović, it ended with a compromise between Serbian revolutionaries and Ottoman authorities. Likewise, Serbia was one of the first nations in the Balkans to abolish feudalism. The Akkerman Convention in 1826, the Treaty of Adrianople in 1829 and finally, the Hatt-i Sharif, recognised the suzerainty of Serbia. The First Serbian Constitution was adopted on 15 February 1835, making the country one of the first to adopt a democratic constitution in Europe. 

Following the clashes between the Ottoman army and Serbs in Belgrade in 1862, and under pressure from the Great Powers, by 1867 the last Turkish soldiers left the Principality, making the country "de facto" independent. By enacting a new constitution in 1869, without consulting the Porte, Serbian diplomats confirmed the "de facto" independence of the country. In 1876, Serbia declared war on the Ottoman Empire, siding with the ongoing Christian uprisings in Bosnia-Herzegovina and Bulgaria.

The formal independence of the country was internationally recognised at the Congress of Berlin in 1878, which ended the Russo-Turkish War; this treaty, however, prohibited Serbia from uniting with other Serbian regions by placing Bosnia and Herzegovina under Austro-Hungarian occupation, alongside the occupation of the region of Raška. From 1815 to 1903, the Principality of Serbia was ruled by the House of Obrenović, save for the rule of Prince Aleksandar Karađorđević between 1842 and 1858. In 1882, Principality of Serbia became the Kingdom of Serbia, ruled by King Milan I. The House of Karađorđević, descendants of the revolutionary leader Karađorđe Petrović, assumed power in 1903 following the May Overthrow. 
In the north, the 1848 revolution in Austria led to the establishment of the autonomous territory of Serbian Vojvodina; by 1849, the region was transformed into the Voivodeship of Serbia and Banat of Temeschwar.

In the course of the First Balkan War in 1912, the Balkan League defeated the Ottoman Empire and captured its European territories, which enabled territorial expansion of the Kingdom of Serbia into regions of Raška, Kosovo, Metohija, and Vardarian Macedonia. The Second Balkan War soon ensued when Bulgaria turned on its former allies, but was defeated, resulting in the Treaty of Bucharest. In two years, Serbia enlarged its territory and its population by 50%; it also suffered high casualties on the eve of World War I, with more than 36,000 dead. Austria-Hungary became wary of the rising regional power on its borders and its potential to become an anchor for unification of Serbs and other South Slavs, and the relationship between the two countries became tense.

The assassination of Archduke Franz Ferdinand of Austria on 28 June 1914 in Sarajevo by Gavrilo Princip, a member of the Young Bosnia organisation, led to Austria-Hungary declaring war on Serbia, on 28 July. Local war escalated, when Germany declared war on Russia, and invaded France and Belgium, thus drawing Great Britain into the conflict, that became the First World War. Serbia won the first major battles of World War I, including the Battle of Cer, and the Battle of Kolubara, marking the first Allied victories against the Central Powers in World War I.

Despite initial success, it was eventually overpowered by the Central Powers in 1915. Most of its army and some people retreated through Albania to Greece and Corfu, suffering immense losses on the way. Serbia was occupied by the Central Powers. After the Central Powers military situation on other fronts worsened, the remains of the Serb army returned east and lead a final breakthrough through enemy lines on 15 September 1918, liberating Serbia and defeating Bulgaria and Austria-Hungary. Serbia, with its campaign, was a major Balkan Entente Power which contributed significantly to the Allied victory in the Balkans in November 1918, especially by helping France force Bulgaria's capitulation.

Serbia's casualties accounted for 8% of the total Entente military deaths; 58% (243,600) soldiers of the Serbian army perished in the war. The total number of casualties is placed around 700,000, more than 16% of Serbia's prewar size, and a majority (57%) of its overall male population. Serbia suffered the biggest casualty rate in World War I. 

As the Austro-Hungarian Empire collapsed, the territory of Syrmia united with Serbia on 24 November 1918. Just a day later on November 25, 1918 Grand National Assembly of Serbs, Bunjevci and other Slavs in Banat, Bačka and Baranja declared the unification of Banat, Bačka and Baranja to the Kingdom of Serbia.

On 26 November 1918, the Podgorica Assembly deposed the House of Petrović-Njegoš and united Montenegro with Serbia. On 1 December 1918, in Belgrade, Serbian Prince Regent Alexander Karađorđević proclaimed the Kingdom of the Serbs, Croats, and Slovenes, under King Peter I of Serbia.

King Peter was succeeded by his son, Alexander, in August 1921. Serb centralists and Croat autonomists clashed in the parliament, and most governments were fragile and short-lived. Nikola Pašić, a conservative prime minister, headed or dominated most governments until his death. King Alexander established a dictatorship in 1929, changed the name of the country to Yugoslavia and changed the internal divisions from the 33 oblasts to nine new banovinas. The effect of Alexander's dictatorship was to further alienate the non-Serbs living in Yugoslavia from the idea of unity.

Alexander was assassinated in Marseille, during an official visit in 1934 by Vlado Chernozemski, member of the IMRO. Alexander was succeeded by his eleven-year-old son Peter II and a regency council was headed by his cousin, Prince Paul. In August 1939 the Cvetković–Maček Agreement established an autonomous Banate of Croatia as a solution to Croatian concerns.
In 1941, in spite of Yugoslav attempts to remain neutral in the war, the Axis powers invaded Yugoslavia. The territory of modern Serbia was divided between Hungary, Bulgaria, the Independent State of Croatia and Italy (Greater Albania and Montenegro), while the remaining part of Serbia was placed under German Military administration, with Serbian puppet governments led by Milan Aćimović and Milan Nedić.

During this period, hundreds of thousands of ethnic Serbs fled the Axis puppet state known as the Independent State of Croatia and sought refuge in German-occupied Serbia, seeking to escape the large-scale persecution and genocide of Serbs, Jews, and Roma being committed by the Ustaše regime.

The Yugoslav territory was the scene of a civil war between royalist Chetniks commanded by Draža Mihailović and communist partisans commanded by Josip Broz Tito. Axis auxiliary units of the Serbian Volunteer Corps and the Serbian State Guard fought against both of these forces. Siege of Kraljevo was a major battle of the Uprising in Serbia, led by Chetnik forces against the Nazis. Several days after the battle began the German forces committed a massacre of approximately 2,000 civilians in an event known as the Kraljevo massacre, in a reprisal for the attack. Draginac and Loznica massacre of 2,950 villagers in Western Serbia in 1941 was the first large execution of civilians in occupied Serbia by Germans, with Kragujevac massacre and Novi Sad Raid of Jews and Serbs by Hungarian fascists being the most notorious, with over 3,000 victims in each case. After one year of occupation, around 16,000 Serbian Jews were murdered in the area, or around 90% of its pre-war Jewish population. Many concentration camps were established across the area. Banjica concentration camp was the largest concentration camp, with primary victims being Serbian Jews, Roma, and Serb political prisoners.
According to Josip Broz Tito himself, Serbs made up the vast majority of Anti-fascist fighters and Yugoslav Partisans for the whole course of World War II. The Republic of Užice was a short-lived liberated territory established by the Partisans and the first liberated territory in World War II Europe, organised as a military mini-state that existed in the autumn of 1941 in the west of occupied Serbia. By late 1944, the Belgrade Offensive swung in favour of the partisans in the civil war; the partisans subsequently gained control of Yugoslavia. Following the Belgrade Offensive, the Syrmian Front was the last major military action of World War II in Serbia. A study by Vladimir Žerjavić estimates total war related deaths in Yugoslavia at 1,027,000, including 273,000 in Serbia. The Ustaše regime systematically murdered approximately 300,000 to 500,000 Serbs. Many historians and authors describe the Ustaše regime's mass killings of Serbs as meeting the definition of genocide, including Raphael Lemkin who is known for coining the word "genocide" and initiating the Genocide Convention.

The victory of the Communist Partisans resulted in the abolition of the monarchy and a subsequent constitutional referendum. A one-party state was soon established in Yugoslavia by the Communist Party of Yugoslavia. Between 60,000 and 70,000 people were killed in Serbia during the communist takeover. All opposition was suppressed and people deemed to be promoting opposition to socialism or promoting separatism were imprisoned or executed for sedition. Serbia became a constituent republic within the SFRY known as the Socialist Republic of Serbia, and had a republic-branch of the federal communist party, the League of Communists of Serbia.

Serbia's most powerful and influential politician in Tito-era Yugoslavia was Aleksandar Ranković, one of the "big four" Yugoslav leaders, alongside Tito, Edvard Kardelj, and Milovan Đilas. Ranković was later removed from the office because of the disagreements regarding Kosovo's nomenklatura and the unity of Serbia. Ranković's dismissal was highly unpopular among Serbs. Pro-decentralisation reformers in Yugoslavia succeeded in the late 1960s in attaining substantial decentralisation of powers, creating substantial autonomy in Kosovo and Vojvodina, and recognising a distinctive "Muslim" nationality. As a result of these reforms, there was a massive overhaul of Kosovo's nomenklatura and police, that shifted from being Serb-dominated to ethnic Albanian-dominated through firing Serbs on a large scale. Further concessions were made to the ethnic Albanians of Kosovo in response to unrest, including the creation of the University of Pristina as an Albanian language institution. These changes created widespread fear among Serbs of being treated as second-class citizens.

In 1989, Slobodan Milošević rose to power in Serbia. Milošević promised a reduction of powers for the autonomous provinces of Kosovo and Vojvodina, where his allies subsequently took over power, during the Anti-bureaucratic revolution. This ignited tensions between the communist leadership of the other republics of Yugoslavia, and awoke ethnic nationalism across Yugoslavia that eventually resulted in its breakup, with Slovenia, Croatia, Bosnia and Herzegovina, and Macedonia declaring independence during 1991 and 1992. Serbia and Montenegro remained together as the Federal Republic of Yugoslavia (FRY). However, according to the Badinter Commission, the country was not legally considered a continuation of the former SFRY, but a new state.

Fueled by ethnic tensions, the Yugoslav Wars (1991–2001) erupted, with the most severe conflicts taking place in Croatia and Bosnia, where the large ethnic Serb communities opposed independence from Yugoslavia. The FRY remained outside the conflicts, but provided logistic, military and financial support to Serb forces in the wars. In response, the UN imposed sanctions against Serbia which led to political isolation and the collapse of the economy (GDP decreased from $24 billion in 1990 to under $10 billion in 1993).

Multi-party democracy was introduced in Serbia in 1990, officially dismantling the one-party system. Critics of Milošević stated that the government continued to be authoritarian despite constitutional changes, as Milošević maintained strong political influence over the state media and security apparatus. When the ruling Socialist Party of Serbia refused to accept its defeat in municipal elections in 1996, Serbians engaged in large protests against the government.

In 1998, continued clashes between the Albanian guerilla Kosovo Liberation Army and Yugoslav security forces led to the short Kosovo War (1998–99), in which NATO intervened, leading to the withdrawal of Serbian forces and the establishment of UN administration in the province.

After presidential elections in September 2000, opposition parties accused Milošević of electoral fraud. A campaign of civil resistance followed, led by the Democratic Opposition of Serbia (DOS), a broad coalition of anti-Milošević parties. This culminated on 5 October when half a million people from all over the country congregated in Belgrade, compelling Milošević to concede defeat. The fall of Milošević ended Yugoslavia's international isolation. Milošević was sent to the International Criminal Tribunal for the former Yugoslavia. The DOS announced that FR Yugoslavia would seek to join the European Union. In 2003, the Federal Republic of Yugoslavia was renamed Serbia and Montenegro; the EU opened negotiations with the country for the Stabilisation and Association Agreement. Serbia's political climate remained tense and in 2003, the Prime Minister Zoran Đinđić was assassinated as result of a plot originating from circles of organised crime and former security officials.

On 21 May 2006, Montenegro held a referendum to determine whether to end its union with Serbia. The results showed 55.4% of voters in favour of independence, which was just above the 55% required by the referendum. On 5 June 2006, the National Assembly of Serbia declared Serbia to be the legal successor to the former state union. The Assembly of Kosovo unilaterally declared independence from Serbia on 17 February 2008. Serbia immediately condemned the declaration and continues to deny any statehood to Kosovo. The declaration has sparked varied responses from the international community, some welcoming it, while others condemned the unilateral move. Status-neutral talks between Serbia and Kosovo-Albanian authorities are held in Brussels, mediated by the EU.

In April 2008 Serbia was invited to join the Intensified Dialogue programme with NATO, despite the diplomatic rift with the alliance over Kosovo. Serbia officially applied for membership in the European Union on 22 December 2009, and received candidate status on 1 March 2012, following a delay in December 2011. Following a positive recommendation of the European Commission and European Council in June 2013, negotiations to join the EU commenced in January 2014.

Massive anti-government protests began in 2018 and continued into 2020, making them one of Europe's longest-running protests.

Situated at the crossroads between Central and Southern Europe, Serbia is located in the Balkan peninsula and the Pannonian Plain. Serbia lies between latitudes 41° and 47° N, and longitudes 18° and 23° E. The country covers a total of 88,361 km (including Kosovo), which places it at 113th place in the world; with Kosovo excluded, the total area is 77,474 km, which would make it 117th. Its total border length amounts to 2,027 km (Albania 115 km, Bosnia and Herzegovina 302 km, Bulgaria 318 km, Croatia 241 km, Hungary 151 km, North Macedonia 221 km, Montenegro 203 km and Romania 476 km). All of Kosovo's border with Albania (115 km), North Macedonia (159 km) and Montenegro (79 km) are under control of the Kosovo border police. Serbia treats the 352 km long border between Kosovo and rest of Serbia as an "administrative line"; it is under shared control of Kosovo border police and Serbian police forces, and there are 11 crossing points.
The Pannonian Plain covers the northern third of the country (Vojvodina and Mačva) while the easternmost tip of Serbia extends into the Wallachian Plain. 
The terrain of the central part of the country, with the region of Šumadija at its heart, consists chiefly of hills traversed by rivers. Mountains dominate the southern third of Serbia. Dinaric Alps stretch in the west and the southwest, following the flow of the rivers Drina and Ibar. The Carpathian Mountains and Balkan Mountains stretch in a north–south direction in eastern Serbia.

Ancient mountains in the southeast corner of the country belong to the Rilo-Rhodope Mountain system. Elevation ranges from the Midžor peak of the Balkan Mountains at (the highest peak in Serbia, excluding Kosovo) to the lowest point of just near the Danube river at Prahovo. The largest lake is Đerdap Lake (163 square kilometres) and the longest river passing through Serbia is the Danube (587.35 kilometres).

The climate of Serbia is under the influences of the landmass of Eurasia and the Atlantic Ocean and Mediterranean Sea. With mean January temperatures around , and mean July temperatures of , it can be classified as a warm-humid continental or humid subtropical climate. In the north, the climate is more continental, with cold winters, and hot, humid summers along with well distributed rainfall patterns. In the south, summers and autumns are drier, and winters are relatively cold, with heavy inland snowfall in the mountains.

Differences in elevation, proximity to the Adriatic Sea and large river basins, as well as exposure to the winds account for climate variations. Southern Serbia is subject to Mediterranean influences. The Dinaric Alps and other mountain ranges contribute to the cooling of most of the warm air masses. Winters are quite harsh in the Pešter plateau, because of the mountains which encircle it. One of the climatic features of Serbia is Košava, a cold and very squally southeastern wind which starts in the Carpathian Mountains and follows the Danube northwest through the Iron Gate where it gains a jet effect and continues to Belgrade and can spread as far south as Niš.

The average annual air temperature for the period 1961–1990 for the area with an altitude of up to is . The areas with an altitude of have an average annual temperature of around , and over of altitude around . The lowest recorded temperature in Serbia was on 13 January 1985, Karajukića Bunari in Pešter, and the highest was , on 24 July 2007, recorded in Smederevska Palanka.

Serbia is one of few European countries with "very high risk" exposure to natural hazards (earthquakes, storms, floods, droughts). It is estimated that potential floods, particularly in areas of Central Serbia, threaten over 500 larger settlements and an area of 16,000 square kilometres. The most disastrous were the floods in May 2014, when 57 people died and a damage of over a 1.5 billion euro was inflicted.

Almost all of Serbia's rivers drain to the Black Sea, by way of the Danube river. The Danube, the second largest European river, passes through Serbia with 588 kilometres (21% of its overall length) and represents the major source of fresh water. It is joined by its biggest tributaries, the Great Morava (longest river entirely in Serbia with 493 km of length), Sava and Tisza rivers. One notable exception is the Pčinja which flows into the Aegean. Drina river forms the natural border between Bosnia and Herzegovina and Serbia, and represents the main kayaking and rafting attraction in both countries.

Due to configuration of the terrain, natural lakes are sparse and small; most of them are located in the lowlands of Vojvodina, like the aeolian lake Palić or numerous oxbow lakes along river flows (like Zasavica and Carska Bara). However, there are numerous artificial lakes, mostly due to hydroelectric dams, the biggest being Đerdap (Iron Gates) on the Danube with 163 km on the Serbian side (a total area of 253 km is shared with Romania); Perućac on the Drina, and Vlasina. The largest waterfall, Jelovarnik, located in Kopaonik, is 71 m high. Abundance of relatively unpolluted surface waters and numerous underground natural and mineral water sources of high water quality presents a chance for export and economy improvement; however, more extensive exploitation and production of bottled water began only recently.

With 29.1% of its territory covered by forest, Serbia is considered to be a middle-forested country, compared on a global scale to world forest coverage at 30%, and European average of 35%. The total forest area in Serbia is 2,252,000 ha (1,194,000 ha or 53% are state-owned, and 1,058,387 ha or 47% are privately owned) or 0.3 ha per inhabitant. 

The most common trees are oak, beech, pines and firs. Serbia is a country of rich ecosystem and species diversity – covering only 1.9% of the whole European territory Serbia is home to 39% of European vascular flora, 51% of European fish fauna, 40% of European reptile and amphibian fauna, 74% of European bird fauna, 67% European mammal fauna. Its abundance of mountains and rivers make it an ideal environment for a variety of animals, many of which are protected including wolves, lynx, bears, foxes and stags. There are 17 snake species living all over the country, 8 of them are venomous. 

Mountain of Tara in western Serbia is one of the last regions in Europe where bears can still live in absolute freedom. Serbia is home home to about 380 species of bird. In Carska Bara, there are over 300 bird species on just a few square kilometres. Uvac Gorge is considered one of the last habitats of the Griffon vulture in Europe. In area around the city of Kikinda, in the northernmost part of the country, some 145 endangered long-eared owls are noted, making it the world's biggest settlement of these species. Country is considerably rich with threatened species of bats and butterflies as well.

There are 377 protected areas of Serbia, encompassing 4,947 square kilometres or 6.4% of the country. The "Spatial plan of the Republic of Serbia" states that the total protected area should be increased to 12% by 2021. Those protected areas include 5 national parks (Đerdap, Tara, Kopaonik, Fruška Gora and Šar Mountain), 15 nature parks, 15 "landscapes of outstanding features", 61 nature reserves, and 281 natural monuments.

Air pollution is a significant problem in Bor area, due to work of large copper mining and smelting complex, and Pančevo where oil and petrochemical industry is based. Some cities suffer from water supply problems, due to mismanagement and low investments in the past, as well as water pollution (like the pollution of the Ibar River from the Trepča zinc-lead combinate, affecting the city of Kraljevo, or the presence of natural arsenic in underground waters in Zrenjanin).

Poor waste management has been identified as one of the most important environmental problems in Serbia and the recycling is a fledgling activity, with only 15% of its waste being turned back for reuse. The 1999 NATO bombing caused serious damage to the environment, with several thousand tonnes of toxic chemicals stored in targeted factories and refineries released into the soil and water basins.

Serbia is a parliamentary republic, with the government divided into legislative, executive and judiciary branches.
Serbia had one of the first modern constitutions in Europe, the 1835 Constitution (known as the Sretenje Constitution), which was at the time considered among the most progressive and liberal constitutions in Europe. Since then it has adopted 10 different constitutions. The current constitution was adopted in 2006 in the aftermath of Montenegro independence referendum which by consequence renewed the independence of Serbia itself. The Constitutional Court rules on matters regarding the Constitution.

The President of the Republic ("Predsednik Republike") is the head of state, is elected by popular vote to a five-year term and is limited by the Constitution to a maximum of two terms. In addition to being the commander in chief of the armed forces, the president has the procedural duty of appointing the prime minister with the consent of the parliament, and has some influence on foreign policy. Aleksandar Vučić of the Serbian Progressive Party is the current president following the 2017 presidential election. Seat of the presidency is Novi Dvor.

The Government ("Vlada") is composed of the prime minister and cabinet ministers. The Government is responsible for proposing legislation and a budget, executing the laws, and guiding the foreign and internal policies. The current prime minister is Ana Brnabić, nominated by the Serbian Progressive Party.

The National Assembly ("Narodna skupština") is a unicameral legislative body. The National Assembly has the power to enact laws, approve the budget, schedule presidential elections, select and dismiss the Prime Minister and other ministers, declare war, and ratify international treaties and agreements. It is composed of 250 proportionally elected members who serve four-year terms.

The largest political parties in Serbia are the centre-right Serbian Progressive Party, leftist Socialist Party of Serbia and far-right Serbian Radical Party.

Serbia is the fourth modern-day European country, after France, Austria and the Netherlands, to have a codified legal system.

The country has a three-tiered judicial system, made up of the Supreme Court of Cassation as the court of the last resort, Courts of Appeal as the appellate instance, and Basic and High courts as the general jurisdictions at first instance.

Courts of special jurisdictions are the Administrative Court, commercial courts (including the Commercial Court of Appeal at second instance) and misdemeanor courts (including High Misdemeanor Court at second instance). The judiciary is overseen by the Ministry of Justice. Serbia has a typical civil law legal system.

Law enforcement is the responsibility of the Serbian Police, which is subordinate to the Ministry of the Interior. Serbian Police fields 27,363 uniformed officers.
National security and counterintelligence are the responsibility of the Security Intelligence Agency (BIA).

Serbia has established diplomatic relations with 188 UN member states, the Holy See, the Sovereign Military Order of Malta, and the European Union. Foreign relations are conducted through the Ministry of Foreign Affairs. Serbia has a network of 65 embassies and 23 consulates internationally. There are 65 foreign embassies, 5 consulates and 4 liaison offices in Serbia.

Serbian foreign policy is focused on achieving the strategic goal of becoming a member state of the European Union (EU). Serbia started the process of joining the EU by signing of the Stabilisation and Association Agreement on 29 April 2008 and officially applied for membership in the European Union on 22 December 2009. It received a full candidate status on 1 March 2012 and started accession talks on 21 January 2014. The European Commission considers accession possible by 2025.

The province of Kosovo declared independence from Serbia on 17 February 2008, which sparked varied responses from the international community, some welcoming it, while others condemn the unilateral move. In protest, Serbia initially recalled its ambassadors from countries that recognised Kosovo's independence. The resolution of 26 December 2007 by the National Assembly stated that both the Kosovo declaration of independence and recognition thereof by any state would be gross violation of international law.

Serbia began cooperation and dialogue with NATO in 2006, when the country joined the Partnership for Peace programme and the Euro-Atlantic Partnership Council. The country's military neutrality was formally proclaimed by a resolution adopted by Serbia's parliament in December 2007, which makes joining any military alliance contingent on a popular referendum, a stance acknowledged by NATO. On the other hand, Serbia's relations with Russia are habitually described by mass media as a "centuries-old religious, ethnic and political alliance" and Russia is said to have sought to solidify its relationship with Serbia since the imposition of sanctions against Russia in 2014.

The Serbian Armed Forces are subordinate to the Ministry of Defence, and are composed of the Army and the Air Force. Although a landlocked country, Serbia operates a River Flotilla which patrols on the Danube, Sava, and Tisza rivers. The Serbian Chief of the General Staff reports to the Defence Minister. The Chief of Staff is appointed by the President, who is the Commander-in-chief. , Serbian defence budget amounts to $804 million.

Traditionally having relied on a large number of conscripts, Serbian Armed Forces went through a period of downsizing, restructuring and professionalisation. Conscription was abolished in 2011. Serbian Armed Forces have 28,000 active troops, supplemented by the "active reserve" which numbers 20,000 members and "passive reserve" with about 170,000.

Serbia participates in the NATO Individual Partnership Action Plan programme, but has no intention of joining NATO, due to significant popular rejection, largely a legacy of the NATO bombing of Yugoslavia in 1999. It is an observer member of the Collective Securities Treaty Organisation (CSTO) The country also signed the Stability Pact for South Eastern Europe. The Serbian Armed Forces take part in several multinational peacekeeping missions, including deployments in Lebanon, Cyprus, Ivory Coast, and Liberia.

Serbia is a major producer and exporter of military equipment in the region. Defence exports totaled around $600 million in 2018. The defence industry has seen significant growth over the years and it continues to grow on a yearly basis.

Serbia is a unitary state composed of municipalities/cities, districts, and two autonomous provinces. In Serbia, excluding Kosovo, there are 145 municipalities ("opštine") and 29 cities ("gradovi"), which form the basic units of local self-government. Apart from municipalities/cities, there are 24 districts ("okruzi", 10 most populated listed below), with the City of Belgrade constituting an additional district. Except for Belgrade, which has an elected local government, districts are regional centres of state authority, but have no powers of their own; they present purely administrative divisions.

Serbia has two autonomous provinces, Vojvodina in the north, and Kosovo and Metohija in the south, while the remaining area of Central Serbia never had its own regional authority. Following the Kosovo War, UN peacekeepers entered Kosovo and Metohija, as per UNSC Resolution 1244. In 2008, Kosovo declared independence. The government of Serbia did not recognise the declaration, considering it illegal and illegitimate.

 census, Serbia (excluding Kosovo) has a total population of 7,186,862 and the overall population density is medium as it stands at 92.8 inhabitants per square kilometre. The census was not conducted in Kosovo which held its own census that numbered their total population at 1,739,825, excluding Serb-inhabited North Kosovo, as Serbs from that area (about 50,000) boycotted the census.

Serbia has been enduring a demographic crisis since the beginning of the 1990s, with a death rate that has continuously exceeded its birth rate. It is estimated that 300,000 people left Serbia during the 1990s, 20% of whom had a higher education. Serbia subsequently has one of the oldest populations in the world, with the average age of 42.9 years, and its population is shrinking at one of the fastest rates in the world. A fifth of all households consist of only one person, and just one-fourth of four and more persons. Average life expectancy in Serbia at birth is 75.6 years.

During the 1990s, Serbia had the largest refugee population in Europe. Refugees and internally displaced persons (IDPs) in Serbia formed between 7% and 7.5% of its population at the time – about half a million refugees sought refuge in the country following the series of Yugoslav wars, mainly from Croatia (and to a lesser extent from Bosnia and Herzegovina) and the IDPs from Kosovo.

Serbs with 5,988,150 are the largest ethnic group in Serbia, representing 83% of the total population (excluding Kosovo). Serbia is one of the European countries with high numbers of registered national minorities, while the Autonomous Province of Vojvodina is recognizable for its multi-ethnic and multi-cultural identity. With a population of 253,899, Hungarians are the largest ethnic minority in Serbia, concentrated predominantly in northern Vojvodina and representing 3.5% of the country's population (13% in Vojvodina). Romani population stands at 147,604 according to the 2011 census but unofficial estimates place their actual number between 400,000 and 500,000. Bosniaks with 145,278 are concentrated in Raška (Sandžak), in the southwest. Other minority groups include Croats, Slovaks, Albanians, Montenegrins, Vlachs, Romanians, Macedonians and Bulgarians. Chinese, estimated at about 15,000, are the only significant non-European immigrant minority.

The majority of the population, or 59.4%, reside in urban areas and some 16.1% in Belgrade alone. Belgrade is the only city with more than a million inhabitants and there are four more with over 100,000 inhabitants.
The Constitution of Serbia defines it as a secular state with guaranteed religious freedom. Orthodox Christians with 6,079,396 comprise 84.5% of country's population. The Serbian Orthodox Church is the largest and traditional church of the country, adherents of which are overwhelmingly Serbs. Other Orthodox Christian communities in Serbia include Montenegrins, Romanians, Vlachs, Macedonians and Bulgarians.

Roman Catholics number 356,957 in Serbia, or roughly 6% of the population, mostly in Vojvodina (especially its northern part) which is home to minority ethnic groups such as Hungarians, Croats, Bunjevci, as well as to some Slovaks and Czechs.

Protestantism accounts for about 1% of the country's population, chiefly Lutheranism among Slovaks in Vojvodina as well as Calvinism among Reformed Hungarians. Greek Catholic Church is adhered by around 25,000 citizens (0.37% of the population), mostly Rusyns in Vojvodina.

Muslims, with 222,282 or 3% of the population, form the third largest religious group. Islam has a strong historic following in the southern regions of Serbia, primarily in southern Raška. Bosniaks are the largest Islamic community in Serbia; estimates are that around a third of the country's Roma people are Muslim.

There are only 578 Jews in Serbia. Atheists numbered 80,053 or 1.1% of the population and an additional 4,070 declared themselves to be agnostics.

The official language is Serbian, native to 88% of the population. Serbian is the only European language with active digraphia, using both Cyrillic and Latin alphabets. Serbian Cyrillic is designated in the Constitution as the "official script" and was devised in 1814 by Serbian philologist Vuk Karadžić, who based it on phonemic principles. A survey from 2014 showed that 47% of Serbians favour the Latin alphabet, 36% favour the Cyrillic one and 17% have no preference.

Standard Serbian is based on the most widespread Shtokavian dialect (more specifically on the dialects of Šumadija-Vojvodina and Eastern Herzegovina). 

Recognised minority languages are: Hungarian, Bosnian, Slovak, Croatian, Albanian, Romanian, Bulgarian and Rusyn. All these languages are in official use in municipalities or cities where the ethnic minority exceeds 15% of the total population. In Vojvodina, the provincial administration uses, besides Serbian, five other languages (Hungarian, Slovak, Croatian, Romanian and Rusyn).

Serbia has an emerging market economy in upper-middle income range. According to the International Monetary Fund, Serbian nominal GDP in 2018 is officially estimated at $50.651 billion or $7,243 per capita while purchasing power parity GDP stood at $122.759 billion or $17,555 per capita. The economy is dominated by services which accounts for 67.9% of GDP, followed by industry with 26.1% of GDP, and agriculture at 6% of GDP. The official currency of Serbia is Serbian dinar (ISO code: RSD), and the central bank is National Bank of Serbia. The Belgrade Stock Exchange is the only stock exchange in the country, with market capitalisation of $8.65 billion and BELEX15 as the main index representing the 15 most liquid stocks.

The economy has been affected by the global economic crisis. After almost a decade of strong economic growth (average of 4.45% per year), Serbia entered the recession in 2009 with negative growth of −3% and again in 2012 and 2014 with −1% and −1.8%, respectively. As the government was fighting effects of crisis the public debt has more than doubled: from pre-crisis level of just under 30% to about 70% of GDP and trending downwards recently to around 50%. Labour force stands at 3.2 million, with 56% employed in services sector, 28.1% in industry and 15.9% in the agriculture. The average monthly net salary in May 2019 stood at 47,575 dinars or $525. The unemployment remains an acute problem, with rate of 12.7% .

Since 2000, Serbia has attracted over $40 billion in foreign direct investment (FDI). Blue-chip corporations making investments include: Fiat Chrysler Automobiles, Siemens, Bosch, Philip Morris, Michelin, Coca-Cola, Carlsberg and others. In the energy sector, Russian energy giants, Gazprom and Lukoil have made large investments. In metallurgy sector, Chinese steel and copper giants, Hesteel and Zijin Mining have acquired key complexes.

Serbia has an unfavourable trade balance: imports exceed exports by 25%. Serbia's exports, however, recorded a steady growth in last couple of years reaching $19.2 billion in 2018. The country has free trade agreements with the EFTA and CEFTA, a preferential trade regime with the European Union, a Generalised System of Preferences with the United States, and individual free trade agreements with Russia, Belarus, Kazakhstan, and Turkey.

Serbia has very favourable natural conditions (land and climate) for varied agricultural production. It has 5,056,000 ha of agricultural land (0.7 ha per capita), out of which 3,294,000 ha is arable land (0.45 ha per capita). In 2016, Serbia exported agricultural and food products worth $3.2 billion, and the export-import ratio was 178%. Agricultural exports constitute more than one-fifth of all Serbia's sales on the world market. Serbia is one of the largest provider of frozen fruit to the EU (largest to the French market, and 2nd largest to the German market). 

Agricultural production is most prominent in Vojvodina on the fertile Pannonian Plain. Other agricultural regions include Mačva, Pomoravlje, Tamnava, Rasina, and Jablanica.

In the structure of the agricultural production 70% is from the crop field production, and 30% is from the livestock production. Serbia is world's second largest producer of plums (582,485 tonnes; second to China), second largest of raspberries (89,602 tonnes, second to Poland), it is also a significant producer of maize (6.48 million tonnes, ranked 32nd in the world) and wheat (2.07 million tonnes, ranked 35th in the world). Other important agricultural products are: sunflower, sugar beet, soybean, potato, apple, pork meat, beef, poultry and dairy.

There are 56,000 ha of vineyards in Serbia, producing about 230 million litres of wine annually. Most famous viticulture regions are located in Vojvodina and Šumadija.

The industry was the economic sector hardest hit by the UN sanctions and trade embargo and NATO bombing during the 1990s and transition to market economy during the 2000s. The industrial output saw dramatic downsizing: in 2013 it was expected to be only a half of that of 1989. Main industrial sectors include: automotive, mining, non-ferrous metals, food-processing, electronics, pharmaceuticals, clothes. Serbia has 14 free economic zones as of September 2017, in which many foreign direct investments are realised.

Automotive industry (with Fiat Chrysler Automobiles as a forebearer) is dominated by cluster located in Kragujevac and its vicinity, and contributes to export with about $2 billion. Country is a leading steel producer in the wider region of Southeast Europe and had production of nearly 2 million tonnes of raw steel in 2018, coming entirely from Smederevo steel mill, owned by the Chinese Hesteel. Serbia's mining industry is comparatively strong: Serbia is the 18th largest producer of coal (7th in the Europe) extracted from large deposits in Kolubara and Kostolac basins; it is also world's 23rd largest (3rd in Europe) producer of copper which is extracted by Zijin Bor Copper, a large copper mining company, acquired by Chinese Zijin Mining in 2018; significant gold extraction is developed around Majdanpek. Serbia notably manufactures intel smartphones named Tesla smartphones.

Food industry is well known both regionally and internationally and is one of the strong points of the economy. Some of the international brand-names established production in Serbia: PepsiCo and Nestlé in food-processing sector; Coca-Cola (Belgrade), Heineken (Novi Sad) and Carlsberg (Bačka Palanka) in beverage industry; Nordzucker in sugar industry. Serbia's electronics industry had its peak in the 1980s and the industry today is only a third of what it was back then, but has witnessed a something of revival in last decade with investments of companies such as Siemens (wind turbines) in Subotica, Panasonic (lighting devices) in Svilajnac, and Gorenje (electrical home appliances) in Valjevo. The pharmaceutical industry in Serbia comprises a dozen manufacturers of generic drugs, of which Hemofarm in Vršac and Galenika in Belgrade, account for 80% of production volume. Domestic production meets over 60% of the local demand.

The energy sector is one of the largest and most important sectors to the country's economy. Serbia is a net exporter of electricity and importer of key fuels (such as oil and gas).

Serbia has an abundance of coal, and significant reserves of oil and gas. Serbia's proven reserves of 5.5 billion tonnes of coal lignite are the 5th largest in the world (second in Europe, after Germany). Coal is found in two large deposits: Kolubara (4 billion tonnes of reserves) and Kostolac (1.5 billion tonnes). Despite being small on a world scale, Serbia's oil and gas resources (77.4 million tonnes of oil equivalent and 48.1 billion cubic metres, respectively) have a certain regional importance since they are largest in the region of former Yugoslavia as well as the Balkans (excluding Romania). Almost 90% of the discovered oil and gas are to be found in Banat and those oil and gas fields are by size among the largest in the Pannonian basin but are average on a European scale.

The production of electricity in 2015 in Serbia was 36.5 billion kilowatt-hours (KWh), while the final electricity consumption amounted to 35.5 billion kilowatt-hours (KWh). Most of the electricity produced comes from thermal-power plants (72.7% of all electricity) and to a lesser degree from hydroelectric-power plants (27.3%). There are 6 lignite-operated thermal-power plants with an installed power of 3,936 MW; largest of which are 1,502 MW-Nikola Tesla 1 and 1,160 MW-Nikola Tesla 2, both in Obrenovac. Total installed power of 9 hydroelectric-power plants is 2,831 MW, largest of which is Đerdap 1 with capacity of 1,026 MW. In addition to this, there are mazute and gas-operated thermal-power plants with an installed power of 353 MW. The entire production of electricity is concentrated in Elektroprivreda Srbije (EPS), public electric-utility power company.

The current oil production in Serbia amounts to over 1.1 million tonnes of oil equivalent and satisfies some 43% of country's needs while the rest is imported. National petrol company, Naftna Industrija Srbije (NIS), was acquired in 2008 by Gazprom Neft. The company's refinery in Pančevo (capacity of 4.8 million tonnes) is one of the most modern oil-refineries in Europe; it also operates network of 334 filling stations in Serbia (74% of domestic market) and additional 36 stations in Bosnia and Herzegovina, 31 in Bulgaria, and 28 in Romania. There are 155 kilometers of crude oil pipelines connecting Pančevo and Novi Sad refineries as a part of trans-national Adria oil pipeline.

Serbia is heavily dependent on foreign sources of natural gas, with only 17% coming from domestic production (totalling 491 million cubic meters in 2012) and the rest is imported, mainly from Russia (via gas pipelines that run through Ukraine and Hungary). Srbijagas, public company, operates the natural gas transportation system which comprise 3,177 kilometers of trunk and regional natural gas pipelines and a 450 million cubic meter underground gas storage facility at Banatski Dvor.

Serbia has a strategic transportation location since the country's backbone, Morava Valley, represents by far the easiest route of land travel from continental Europe to Asia Minor and the Near East.

Serbian road network carries the bulk of traffic in the country. Total length of roads is 45,419 km of which 962 km are "class-IA state roads" (i.e. motorways); 4,517 km are "class-IB state roads" (national roads); 10,941 km are "class-II state roads" (regional roads) and 23,780 km are "municipal roads". The road network, except for the most of class-IA roads, are of comparatively lower quality to the Western European standards because of lack of financial resources for their maintenance in the last 20 years.

Over 300 kilometers of new motorways has been constructed in the last decade and additional 142 kilometers are currently under construction: A5 motorway (from south of Pojate (north of Kruševac ) to Čačak) and 30 km-long segment of A2 (between Čačak and Požega). Coach transport is very extensive: almost every place in the country is connected by bus, from largest cities to the villages; in addition there are international routes (mainly to countries of Western Europe with large Serb diaspora). Routes, both domestic and international, are served by more than hundred intercity coach services, biggest of which are Lasta and Niš-Ekspres. , there were 1,999,771 registered passenger cars or 1 passenger car per 3.5 inhabitants.

Serbia has 3,819 kilometres of rail tracks, of which 1,279 are electrified and 283 kilometres are double-track railroad. The major rail hub is Belgrade (and to a lesser degree Niš), while the most important railroads include: Belgrade–Bar (Montenegro), Belgrade–Šid–Zagreb (Croatia)/Belgrade–Niš–Sofia (Bulgaria) (part of Pan-European Corridor X), Belgrade–Subotica–Budapest (Hungary) and Niš–Thessaloniki (Greece). Although still a major mode of freight transportation, railroads face increasing problems with the maintenance of the infrastructure and lowering speeds. Rail services are operated Srbija Voz (passenger transport) and Srbija Kargo (freight transport).

There are only two airports with regular passenger traffic. Belgrade Nikola Tesla Airport served 5.6 million passengers in 2018 and is a hub of flagship carrier Air Serbia which flies to 59 destinations in 32 countries and carried some 2.5 million passengers in 2018. Niš Constantine the Great Airport is mainly catering low-cost airlines. 

Serbia has a developed inland water transport since there are 1,716 kilometres of navigable inland waterways (1,043 km of navigable rivers and 673 km of navigable canals), which are almost all located in northern third of the country. The most important inland waterway is the Danube (part of Pan-European Corridor VII). Other navigable rivers include Sava, Tisza, Begej and Timiş River, all of which connect Serbia with Northern and Western Europe through the Rhine–Main–Danube Canal and North Sea route, to Eastern Europe via the Tisza, Begej and Danube Black Sea routes, and to Southern Europe via the Sava river. More than 2 million tonnes of cargo were transported on Serbian rivers and canals in 2016 while the largest river ports are: Novi Sad, Belgrade, Pančevo, Smederevo, Prahovo and Šabac.

Fixed telephone lines connect 81% of households in Serbia, and with about 9.1 million users the number of cellphones surpasses the total population of by 28%. The largest mobile operator is Telekom Srbija with 4.2 million subscribers, followed by Telenor with 2.8 million users and Vip mobile with about 2 million. Some 58% of households have fixed-line (non-mobile) broadband Internet connection while 67% are provided with pay television services (i.e. 38% cable television, 17% IPTV, and 10% satellite). Digital television transition has been completed in 2015 with DVB-T2 standard for signal transmission.

Serbia is not a mass-tourism destination but nevertheless has a diverse range of touristic products. In 2018, total of over 3.4 million tourists were recorded in accommodations, of which half were foreign. Foreign exchange earnings from tourism were estimated at $1.5 billion.

Tourism is mainly focused on the mountains and spas of the country, which are mostly visited by domestic tourists, as well as Belgrade and, to a lesser degree, Novi Sad, which are preferred choices of foreign tourists (almost two-thirds of all foreign visits are made to these two cities). The most famous mountain resorts are Kopaonik, Stara Planina and Zlatibor. There are also many spas in Serbia, the biggest of which are Vrnjačka Banja, Soko Banja, and Banja Koviljača. City-break and conference tourism is developed in Belgrade and Novi Sad. Other touristic products that Serbia offer are natural wonders like Đavolja varoš, Christian pilgrimage to the many Orthodox monasteries across the country and the river cruising along the Danube. There are several internationally popular music festivals held in Serbia, such as EXIT (with 25–30,000 foreign visitors coming from 60 different countries) and the Guča trumpet festival.

According to 2011 census, literacy in Serbia stands at 98% of population while computer literacy is at 49% (complete computer literacy is at 34.2%). Same census showed the following levels of education: 16.2% of inhabitants have higher education (10.6% have bachelors or master's degrees, 5.6% have an associate degree), 49% have a secondary education, 20.7% have an elementary education, and 13.7% have not completed elementary education.

Education in Serbia is regulated by the Ministry of Education and Science. Education starts in either preschools or elementary schools. Children enroll in elementary schools at the age of seven. Compulsory education consists of eight grades of elementary school. Students have the opportunity to attend gymnasiums and vocational schools for another four years, or to enroll in vocational training for 2 to 3 years. Following the completion of gymnasiums or vocational schools, students have the opportunity to attend university. Elementary and secondary education are also available in languages of recognised minorities in Serbia, where classes are held in Hungarian, Slovak, Albanian, Romanian, Rusyn, Bulgarian as well as Bosnian and Croatian languages. Petnica Science Center is a notable institution for extracurricular science education focusing on gifted students.

There are 19 universities in Serbia (nine public universities with a total number of 86 faculties and ten private universities with 51 faculties). In 2018/2019 academic year, 210,480 students attended 19 universities (181,310 at public universities and some 29,170 at private universities) while 47,169 attended 81 "higher schools". Public universities in Serbia are: the University of Belgrade (oldest, founded in 1808, and largest university with 97,696 undergraduates and graduates), University of Novi Sad (founded in 1960 and with student body of 42,489), University of Niš (founded in 1965; 20,559 students), University of Kragujevac (founded in 1976; 14,053 students), University of Priština (located in North Mitrovica), Public University of Novi Pazar as well as three specialist universities – University of Arts, University of Defence and University of Criminal Investigation and Police Studies. Largest private universities include Megatrend University and Singidunum University, both in Belgrade, and Educons University in Novi Sad. The University of Belgrade (placed in 301–400 bracket on 2013 Shanghai Ranking of World Universities, being best-placed university in Southeast Europe after those in Athens and Thessaloniki) and University of Novi Sad are generally considered as the best institutions of higher learning in the country.
Serbia spent 0.9% of GDP on scientific research in 2017, which is slightly below the European average. Since 2018, Serbia is a full member of CERN. Serbia has a long history of excellence in maths and computer sciences which has created a strong pool of engineering talent, although economic sanctions during the 1990s and chronic underinvestment in research forced many scientific professionals to leave the country. Nevertheless, there are several areas in which Serbia still excels such as growing information technology sector, which includes software development as well as outsourcing. It generated over $1.2 billion in exports in 2018, both from international investors and a significant number of dynamic homegrown enterprises. Serbia is one of the countries with the highest proportion of women in science.
Among the scientific institutes operating in Serbia, the largest are the Mihajlo Pupin Institute and Vinča Nuclear Institute, both in Belgrade. The Serbian Academy of Sciences and Arts is a learned society promoting science and arts from its inception in 1841. With a strong science and technological ecosystem, Serbia has produced a number of renowned scientists that have greatly contributed to the field of science and technology.

For centuries straddling the boundaries between East and West, the territory of Serbia had been divided among the Eastern and Western halves of the Roman Empire; then between Byzantium and the Kingdom of Hungary; and in the Early modern period between the Ottoman Empire and the Habsburg Empire. These overlapping influences have resulted in cultural varieties throughout Serbia; its north leans to the profile of Central Europe, while the south is characteristic of the wider Balkans and even the Mediterranean. The Byzantine influence on Serbia was profound, firstly through the introduction of Eastern Christianity in the Early Middle Ages. The Serbian Orthodox Church has had an enduring status in Serbia, with the many Serbian monasteries constituting cultural monuments left from Serbia in the Middle Ages. Serbia has seen influences of Republic of Venice as well, mainly though trade, literature and romanesque architecture.

Serbia has five cultural monuments inscribed in the list of UNESCO World Heritage: the early medieval capital Stari Ras and the 13th-century monastery Sopoćani; the 12th-century Studenica monastery; the Roman complex of Gamzigrad–Felix Romuliana; medieval tombstones Stećci; and finally the endangered Medieval Monuments in Kosovo (the monasteries of Visoki Dečani, Our Lady of Ljeviš, Gračanica and Patriarchal Monastery of Peć).

There are two literary monuments on UNESCO's Memory of the World Programme: the 12th-century "Miroslav Gospel", and scientist Nikola Tesla's archive. The "slava" (patron saint veneration), kolo (traditional folk dance) and singing to the accompaniment of the gusle are inscribed on UNESCO Intangible Cultural Heritage Lists. The Ministry of Culture and Information is tasked with preserving the nation's cultural heritage and overseeing its development. Further activities supporting development of culture are undertaken at local government level.

Traces of Roman and early Byzantine Empire architectural heritage are found in many royal cities and palaces in Serbia, like Sirmium, Felix Romuliana and Justiniana Prima, since 535 the seat of the Archbishopric of Justiniana Prima.

Serbian monasteries are the pinnacle of Serbian medieval art. At the beginning, they were under the influence of Byzantine Art which was particularly felt after the fall of Constantinople in 1204, when many Byzantine artists fled to Serbia. Noted of these monasteries is Studenica (built around 1190). It was a model for later monasteries, like the Mileševa, Sopoćani, Žiča, Gračanica and Visoki Dečani. In the end of 14th and the 15th centuries, autochthonous architectural style known as Morava style evolved in area around Morava Valley. A characteristic of this style was the wealthy decoration of the frontal church walls. Examples of this include Manasija, Ravanica and Kalenić monasteries. 

Icons and fresco paintings are often considered the peak of Serbian art. The most famous frescos are White Angel (Mileševa monastery), "Crucifixion" (Studenica monastery) and "Dormition of the Virgin" (Sopoćani).

Country is dotted with many well-preserved medieval fortifications and castles such as Smederevo Fortress (largest lowland fortress in Europe), Golubac, Maglič, Soko grad, Belgrade Fortress, Ostrvica and Ram.

During the time of Ottoman occupation, Serbian art was virtually non-existent, with the exception of several Serbian artists who lived in the lands ruled by the Habsburg Monarchy. 
Traditional Serbian art showed Baroque influences at the end of the 18th century as shown in the works of Nikola Nešković, Teodor Kračun, Zaharije Orfelin and Jakov Orfelin.
Serbian painting showed the influence of Biedermeier and Neoclassicism as seen in works by Konstantin Danil, Arsenije Teodorović and Pavel Đurković. Many painters followed the artistic trends set in the 19th century Romanticism, notably Đura Jakšić, Stevan Todorović, Katarina Ivanović and Novak Radonić.

Important Serbian painters of the first half of the 20th century were Paja Jovanović and Uroš Predić of Realism, Cubist Sava Šumanović, Milena Pavlović-Barili and Nadežda Petrović of Impressionism, Expressionist Milan Konjović. Noted painters of the second half of 20th century include Marko Čelebonović, Petar Lubarda, Milo Milunović, Ljubomir Popović and Vladimir Veličković.

Anastas Jovanović was one of the earliest photographes in the world, while Marina Abramović is one of the world leading performance artists. Pirot carpet is known as one of the most important traditional handicrafts in Serbia.

There are around 180 museums in Serbia, of which the most prominent is the National Museum of Serbia, founded in 1844. It houses one of the largest art collections in the Balkans, including many foreign masterpiece collections. Other art museums of note are Museum of Contemporary Art in Belgrade, Museum of Vojvodina and the Gallery of Matica Srpska in Novi Sad.

The beginning of Serbian literacy dates back to the activity of the brothers Cyril and Methodius in the Balkans. Monuments of Serbian literacy from the early 11th century can be found, written in Glagolitic. Starting in the 12th century, books were written in Cyrillic. From this epoch, the oldest Serbian Cyrillic book editorial are the Miroslav Gospels from 1186. "The Miroslav Gospels" are considered to be the oldest book of Serbian medieval history and as such has entered UNESCO's Memory of the World Register.

Notable medieval authors include Saint Sava, Jefimija, Stefan Lazarević, Constantine of Kostenets and others. Due to Ottoman occupation, when every aspect of formal literacy stopped, Serbia stayed excluded from the entire Renaissance flow in Western culture. However, the tradition of oral story-telling blossomed, shaping itself through epic poetry inspired by at the times still recent Kosovo battle and folk tales deeply rooted in Slavic mythology. Serbian epic poetry in those times has seen as the most effective way in preserving the national identity. The oldest known, entirely fictional poems, make up the "Non-historic cycle"; this one is followed by poems inspired by events before, during and after Kosovo Battle. The special cycles are dedicated to Serbian legendary hero, Marko Kraljević, then about hajduks and uskoks, and the last one dedicated to the liberation of Serbia in 19th century. Some of the best known folk ballads are "The Death of the Mother of the Jugović Family" and The Mourning Song of the Noble Wife of the Asan Aga (1646), translated into European languages by Goethe, Walter Scott, Pushkin and Mérimée. One of the most notable tales from Serbian folklore is The Nine Peahens and the Golden Apples.
Baroque trends in Serbian literature emerged in the late 17th century. Notable Baroque-influenced authors were Gavril Stefanović Venclović, Jovan Rajić, Zaharije Orfelin, Andrija Zmajević and others. Dositej Obradović was a prominent figure of the Age of Enlightenment, while the notable Classicist writer was Jovan Sterija Popović, although his works also contained elements of Romanticism. In the era of national revival, in the first half of the 19th century, Vuk Stefanović Karadžić collected Serbian folk literature, and reformed the Serbian language and spelling, paving the way for Serbian Romanticism. The first half of the 19th century was dominated by Romanticism, with Petar II Petrović-Njegoš, Branko Radičević, Đura Jakšić, Jovan Jovanović Zmaj and Laza Kostić being the notable representatives, while the second half of the century was marked by Realist writers such as Milovan Glišić, Laza Lazarević, Simo Matavulj, Stevan Sremac, Vojislav Ilić, Branislav Nušić, Radoje Domanović and Borisav Stanković.

The 20th century was dominated by the prose writers Meša Selimović ("Death and the Dervish"), Miloš Crnjanski ("Migrations"), Isidora Sekulić ("The Cronicle of a Small Town Cemetery"), Branko Ćopić ("Eagles Fly Early"), Borislav Pekić ("The Time of Miracles"), Danilo Kiš ("The Encyclopedia of the Dead"), Dobrica Ćosić ("The Roots"), Aleksandar Tišma ("The Use of Man"), Milorad Pavić and others. Pavić is widely acclaimed Serbian author of the beginning of the 21st century, most notably for his "Dictionary of the Khazars" "(Хазарски речник/Hazarski rečnik)", which has been translated into 38 languages. Notable poets include Milan Rakić, Jovan Dučić, Vladislav Petković Dis, Rastko Petrović, Stanislav Vinaver, Dušan Matić, Branko Miljković, Vasko Popa, Oskar Davičo, Miodrag Pavlović, and Stevan Raičković. Notable contemporary authors include David Albahari, Svetislav Basara, Goran Petrović, Gordana Kuić, Vuk Drašković and Vladislav Bajac. Serbian comics emerged in the 1930s and the medium remains popular today.

Ivo Andrić ("The Bridge on the Drina") is possibly the best-known Serbian author,; he was awarded the Nobel Prize in Literature in 1961. The most beloved face of Serbian literature was Desanka Maksimović, who for seven decades remained "the leading lady of Yugoslav poetry". She is honoured with statues, postage stamps, and the names of streets across Serbia.

There are 551 public libraries biggest of which are: National Library of Serbia in Belgrade with funds of about 6 million items, and Matica Srpska (the oldest matica and Serbian cultural institution, founded in 1826) in Novi Sad with nearly 3.5 million volumes. In 2010, there were 10,989 books and brochures published. The book publishing market is dominated by several major publishers such as Laguna and Vulkan (both of which operate their own bookstore chains) and the industry's centrepiece event, annual Belgrade Book Fair, is the most visited cultural event in Serbia with 158,128 visitors in 2013. The highlight of the literary scene is awarding of NIN Prize, given every January since 1954 for the best newly published novel in Serbian language.

Composer and musicologist Stevan Stojanović Mokranjac is considered the founder of modern Serbian music. The Serbian composers of the first generation Petar Konjović, Stevan Hristić, and Miloje Milojević maintained the national expression and modernised the romanticism into the direction of impressionism. Other famous classical Serbian composers include Isidor Bajić, Stanislav Binički and Josif Marinković. There are three opera houses in Serbia: Opera of the National Theatre and Madlenianum Opera, both in Belgrade, and Opera of the Serbian National Theatre in Novi Sad. Four symphonic orchestra operate in the country: Belgrade Philharmonic Orchestra, Niš Symphony Orchestra, Symphonic Orchestra of Radio Television of Serbia, and Novi Sad Philharmonic Orchestra. The Choir of Radio Television of Serbia is a leading vocal ensemble in the country. The BEMUS is one of the most prominent classical music festivals in the South East Europe.

Traditional Serbian music includes various kinds of bagpipes, flutes, horns, trumpets, lutes, psalteries, drums and cymbals. The "kolo" is the traditional collective folk dance, which has a number of varieties throughout the regions. The most popular are those from Užice and Morava region. Sung epic poetry has been an integral part of Serbian and Balkan music for centuries. In the highlands of Serbia these long poems are typically accompanied on a one-string fiddle called the "gusle", and concern themselves with themes from history and mythology. There are records of "gusle" being played at the court of the 13th-century King Stefan Nemanjić.

Pop music has mainstream popularity. Željko Joksimović won second place at the 2004 Eurovision Song Contest and Marija Šerifović managed to win the 2007 Eurovision Song Contest with the song "Molitva", and Serbia was the host of the 2008 edition of the contest. Most popular pop singers include likes of Đorđe Balašević, Goca Tržan, Zdravko Čolić, Aleksandra Radović, Vlado Georgiev, Jelena Tomašević and Nataša Bekvalac among others.
The Serbian rock which was during the 1960s, 1970s and 1980s part of former Yugoslav rock scene, used to be well developed and covered in the media. During the 1990s and 2000s popularity of rock music declined in Serbia, and although several major mainstream acts managed to sustain their popularity, an underground and independent music scene developed. The 2000s saw a revival of the mainstream scene and the appearance of a large number of notable acts. Notable Serbian rock acts include Bajaga i Instruktori, Disciplina Kičme, Ekatarina Velika, Električni Orgazam, Eva Braun, Kerber, Neverne Bebe, Partibrejkers, Ritam Nereda, Orthodox Celts, Rambo Amadeus, Riblja Čorba, S.A.R.S., Smak, Van Gogh, YU Grupa and others.

Folk music in its original form has been a prominent music style since World War One following the early success of Sofka Nikolić. The music has been further promoted by Danica Obrenić, Anđelija Milić, Nada Mamula, and even later, during 60s and 70s, with stars like Silvana Armenulić, Toma Zdravković, Lepa Lukić, Vasilija Radojčić, Vida Pavlović and Gordana Stojićević.

Turbo-folk music is subgenre that has developed in Serbia in the late 1980s and the beginning of the 1990s and has since enjoyed an immense popularity through acts of Dragana Mirković, Zorica Brunclik, Šaban Šaulić, Ana Bekuta, Sinan Sakić, Vesna Zmijanac, Mile Kitić, Snežana Đurišić, Šemsa Suljaković, and Nada Topčagić. It is a blend of folk music with pop and/or dance elements and can be seen as a result of the urbanisation of folk music. In recent period turbo-folk featured even more pop music elements, and some of the performers were labeled as pop-folk. The most famous among them are Ceca (often considered to be the biggest music star of Serbia), Jelena Karleuša, Aca Lukas, Seka Aleksić, Dara Bubamara, Indira Radić, Saša Matić, Viki Miljković, Stoja and Lepa Brena, arguably the most prominent performer of former Yugoslavia.

Balkan Brass, or "truba" ("trumpet") is a popular genre, especially in Central and Southern Serbia where Balkan Brass originated. The music has its tradition from the First Serbian Uprising. The trumpet was used as a military instrument to wake and gather soldiers and announce battles, the trumpet took on the role of entertainment during downtime, as soldiers used it to transpose popular folk songs. When the war ended and the soldiers returned to the rural life, the music entered civilian life and eventually became a music style, accompanying births, baptisms, weddings, and funerals. There are two main varieties of this genre, one from Western Serbia and the other from Southern Serbia, with brass musician Boban Marković being one of the most respected names in the world of modern brass band bandleaders.

Most popular music festival are Guča Trumpet Festival with over 300,000 annual visitors and EXIT in Novi Sad (won the Best Major Festival award at the European Festivals Awards for 2013 and 2017.) with 200,000 visitors in 2013. Other festivals include Nišville Jazz Festival in Niš and Gitarijada rock festival in Zaječar.

Serbia has a well-established theatrical tradition with Joakim Vujić considered the founder of modern Serbian theatre. Serbia has 38 professional theatres and 11 theatres for children, the most important of which are National Theatre in Belgrade, Serbian National Theatre in Novi Sad, National Theatre in Subotica, National Theatre in Niš and Knjaževsko-srpski teatar in Kragujevac (the oldest theatre in Serbia, established in 1835). The Belgrade International Theatre Festival – BITEF, founded in 1967, is one of the oldest theatre festivals in the world, and it has become one of the five biggest European festivals. Sterijino pozorje is, on the other hand, festival showcasing national drama plays. The most important Serbian playwrighters were Jovan Sterija Popović and Branislav Nušić, while recent renowned names are Dušan Kovačević and Biljana Srbljanović.
The foundation of Serbian cinema dates back to 1896 with the release of the oldest movie in the Balkans, "The Life and Deeds of the Immortal Vožd Karađorđe", a biopic about Serbian revolutionary leader, Karađorđe. 

Serbian cinema is one of the dynamic smaller European cinematographies. Serbia's film industry is heavily subsidised by the government, mainly through grants approved by the Film Centre of Serbia. As of 2011, there were 17 domestic feature films produced. There are 22 operating cinemas in the country, of which 12 are multiplexes, with total attendance exceeding 2.6 million and comparatively high percentage of 32.3% of total sold tickets for domestic films. Modern PFI Studios located in Šimanovci is nowadays Serbia's only major film studio complex; it consists of 9 sound stages and attracts mainly international productions, primarily American and West European. The Yugoslav Film Archive used to be former Yugoslavia's and now is Serbia national film archive – with over 100 thousand film prints, it is among five largest film archives in the world.

Famous Serbian filmmaker Emir Kusturica won two Golden Palms for Best Feature Film at the Cannes Film Festival, for "When Father Was Away on Business" in 1985 and then again for "Underground" in 1995. Other renowned directors include Dušan Makavejev, Želimir Žilnik (Golden Berlin Bear winner), Aleksandar Petrović, Živojin Pavlović, Goran Paskaljević, Goran Marković, Srđan Dragojević, Srdan Golubović and Mila Turajlić among others. Serbian-American screenwriter Steve Tesich won the Academy Award for Best Original Screenplay in 1979 for the movie Breaking Away.

Prominent movie stars in Serbia have left celebrated heritage in cinematography of Yugoslavia as well. Notable mentions are Zoran Radmilović, Pavle Vuisić, Ljubiša Samardžić, Olivera Marković, Mija Aleksić, Miodrag Petrović Čkalja, Ružica Sokić, Velimir Bata Živojinović, Danilo Bata Stojković, Seka Sablić, Olivera Katarina, Dragan Nikolić, Mira Stupica, Nikola Simić, Bora Todorović and others. Milena Dravić was one of the most celebrated actress in Serbian cinematography. She has won Best Actress Award on Cannes Film Festival in 1980.

The freedom of the press and the freedom of speech are guaranteed by the constitution of Serbia. Serbia is ranked 90th out of 180 countries in the 2019 Press Freedom Index report compiled by Reporters Without Borders. Report noted that media outlets and journalists continue to face partisan and government pressure over editorial policies. Also, the media are now more heavily dependent on advertising contracts and government subsidies to survive financially.

According to AGB Nielsen Research in 2009, Serbs on average watch five hours of television per day, making it the highest average in Europe. There are seven nationwide free-to-air television channels, with public broadcaster Radio Television of Serbia (RTS) operating three (RTS1, RTS2 and RTS3) and private broadcasters operating four (Pink, Happy, Prva, and O2). In 2017, preferred usage of these channels were as follows: 20.2% for RTS1, 14.1% for Pink, 9.4% for Happy, 9.0% for Prva, 4.7% for O2, and 2.5% for RTS2. There are 28 regional television channels and 74 local television channels. Besides terrestrial channels there are dozens Serbian television channels available only on cable or satellite.

There are 247 radio stations in Serbia. Out of these, six are radio stations with national coverage, including two of public broadcaster Radio Television of Serbia (Radio Belgrade 1 and Radio Belgrade 2/Radio Belgrade 3) and four private ones (Radio S1, Radio S2, Play Radio, and Radio Hit FM). Also, there are 34 regional stations and 207 local stations.

There are 305 newspapers published in Serbia of which 12 are daily newspapers. Dailies "Politika" and "Danas" are Serbia's papers of record, former being the oldest newspaper in the Balkans, founded in 1904. Highest circulation newspapers are tabloids "Večernje Novosti", "Blic", "Kurir", and "Informer", all with more than 100,000 copies sold. There are one daily newspaper devoted to sports – "Sportski žurnal", one business daily "Privredni pregled", two regional newspapers ("Dnevnik" published in Novi Sad and "Narodne novine" from Niš), and one minority-language daily ("Magyar Szo" in Hungarian, published in Subotica).

There are 1,351 magazines published in the country. Those include weekly news magazines "NIN", "Vreme" and "Nedeljnik", popular science magazine of "Politikin Zabavnik", women's "Lepota & Zdravlje", auto magazine "SAT revija", IT magazine "Svet kompjutera". In addition, there is a wide selection of Serbian editions of international magazines, such as "Cosmopolitan", "Elle", "Men's Health", "National Geographic", "Le Monde diplomatique", "Playboy", and "Hello!", among others.

The main news agencies are Tanjug, Beta and Fonet.

, out of 432 web-portals (mainly on the .rs domain) the most visited are online editions of printed dailies Blic and Kurir, news web-portal B92, and classifieds KupujemProdajem.

Serbian cuisine is largely heterogeneous in a way characteristic of the Balkans and, especially, the former Yugoslavia. It features foods characteristic of lands formerly under Turkish suzerainty as well as cuisine originating from other parts of Central Europe (especially Austria and Hungary). Food is very important in Serbian social life, particularly during religious holidays such as Christmas, Easter and feast days i.e. slava.

Staples of the Serbian diet include bread, meat, fruits, vegetables, and dairy products. Bread is the basis of all Serbian meals, and it plays an important role in Serbian cuisine and can be found in religious rituals. A traditional Serbian welcome is to offer bread and salt to guests. Meat is widely consumed, as is fish. Serbian specialties include ćevapčići (caseless sausages made of minced meat, which is always grilled and seasoned), pljeskavica, sarma, kajmak (a dairy product similar to clotted cream), gibanica (cheese and kajmak pie), ajvar (a roasted red pepper spread), proja (cornbread), and kačamak (corn-flour porridge).

Serbians claim their country as the birthplace of rakia ("rakija"), a highly alcoholic drink primarily distilled from fruit. Rakia in various forms is found throughout the Balkans, notably in Bulgaria, Croatia, Slovenia, Montenegro, Hungary and Turkey. Slivovitz ("šljivovica"), a plum brandy, is a type of rakia which is considered the national drink of Serbia.

Winemaking traditions in Serbia dates back to Roman times. Serbian wines are produced in 22 different geographical regions, with white wine dominating the total amount. Besides rakia and beer, wine is a very popular alcoholic beverage in the country.

Sports play an important role in Serbian society, and the country has a strong sporting history. The most popular sports in Serbia are football, basketball, tennis, volleyball, water polo and handball.
Professional sports in Serbia are organised by sporting federations and leagues (in case of team sports). One of particularities of Serbian professional sports is existence of many multi-sports clubs (called "sports societies"), biggest and most successful of which are Red Star, Partizan, and Beograd in Belgrade, Vojvodina in Novi Sad, Radnički in Kragujevac, Spartak in Subotica.

Football is the most popular sport in Serbia, and the Football Association of Serbia with 146,845 registered players, is the largest sporting association in the country. FK Bačka 1901 is the oldest football club in Serbia and the former Yugoslavia. Dragan Džajić was officially recognised as "the best Serbian player of all times" by the Football Association of Serbia, and more recently the likes of Nemanja Vidić, Dejan Stanković, Branislav Ivanović, Aleksandar Kolarov and Nemanja Matić play for the elite European clubs, developing the nation's reputation as one of the world's biggest exporters of footballers. The Serbia national football team lacks relative success although it qualified for three of the last four FIFA World Cups. Serbia national youth football teams have won 2013 U-19 European Championship and 2015 U-20 World Cup. The two main football clubs in Serbia are Red Star (winner of the 1991 European Cup) and Partizan (finalist of the 1966 European Cup), both from Belgrade. The rivalry between the two clubs is known as the "Eternal Derby", and is often cited as one of the most exciting sports rivalries in the world.
Serbia is one of the traditional powerhouses of world basketball, as Serbia men's national basketball team have won two World Championships (in 1998 and 2002), three European Championships (1995, 1997, and 2001) and two Olympic silver medals (in 1996 and 2016) as well. The women's national basketball team won the European Championship in 2015 and Olympic bronze medal in 2016. A total of 31 Serbian players have played in the NBA in last three decades, including Nikola Jokić (2019 All-NBA First team), Predrag "Peja" Stojaković (2011 NBA champion and three-time NBA All-Star), and Vlade Divac (2001 NBA All-Star and Basketball Hall of Famer). The renowned "Serbian coaching school" produced many of the most successful European basketball coaches of all times, such as Željko Obradović (who won a record 9 Euroleague titles as a coach), Dušan Ivković, Svetislav Pešić, and Igor Kokoškov (the first coach born and raised outside of North America to be hired as a head coach in the NBA). KK Partizan basketball club was the 1992 European champion.

The Serbia men's national water polo team is the one of the most successful national teams, having won Olympic gold medal in 2016, three World Championships (2005, 2009 and 2015), and seven European Championships in 2001, 2003, 2006, 2012, 2014, 2016 and 2018, respectively. VK Partizan has won a joint-record seven European champion titles.

Recent success of Serbian tennis players has led to an immense growth in the popularity of tennis in the country. Novak Djokovic has won sixteen Grand Slam singles title and has held the No. 1 spot in the ATP rankings for over 260 weeks. He became the eighth player in history to achieve the Career Grand Slam and the third man to hold all four major titles at once and the first ever to do so on three different surfaces. Ana Ivanovic (champion of 2008 French Open) and Jelena Janković were both ranked No. 1 in the WTA Rankings. There were two No. 1 ranked-tennis double players as well: Nenad Zimonjić (three-time men's double and four-time mixed double Grand Slam champion) and Slobodan Živojinović. The Serbia men's tennis national team won the 2010 Davis Cup and 2020 ATP Cup, while Serbia women's tennis national team reached the final at 2012 Fed Cup.
Serbia is one of the leading volleyball countries in the world. Its men's national team won the gold medal at 2000 Olympics, the European Championship three times as well as the 2016 FIVB World League. The women's national volleyball team are current world Champions, has won European Championship three times as well as Olympic silver medal in 2016. 

Jasna Šekarić, sport shooter, is one of the athletes with the most appearances at the Olympic Games. She has won a total of five Olympic medals and also three World Championship gold medals. Other noted Serbian athletes include: swimmers Milorad Čavić (2009 World championships gold and silver medalist as well as 2008 Olympic silver medalist on 100-metre butterfly in historic race with American swimmer Michael Phelps) and Nađa Higl (2009 World champion in 200-metre breaststroke); track and field athletes Vera Nikolić (former world record holder in 800 metres) and Ivana Španović (long-jumper; four-time European champion, World indoor champion and bronze medalist at the 2016 Olympics); wrestler Davor Štefanek (2016 Olympic gold medalist and 2014 World champion), and taekwondoist Milica Mandić (2012 Olympic gold medalist and 2017 world champion).

Serbia has hosted several major sport competitions, including the 2005 Men's European Basketball Championship, 2005 Men's European Volleyball Championship, 2006 and 2016 Men's European Water Polo Championships, 2009 Summer Universiade, 2012 European Men's Handball Championship, and 2013 World Women's Handball Championship. The most important annual sporting events held in the country are the Belgrade Marathon and the Tour de Serbie cycling race.


Sources:



</doc>
<doc id="29266" url="https://en.wikipedia.org/wiki?curid=29266" title="Relationship between religion and science">
Relationship between religion and science

Historians of science and of religion, philosophers, theologians, scientists, and others from various geographical regions and cultures have addressed various aspects of the relationship between religion and science. Even though the ancient and medieval worlds did not have conceptions resembling the modern understandings of "science" or of "religion", certain elements of modern ideas on the subject recur throughout history. The pair-structured phrases "religion and science" and "science and religion" first emerged in the literature in the 19th century. This coincided with the refining of "science" (from the studies of "natural philosophy") and of "religion" as distinct concepts in the preceding few centuries - partly due to professionalization of the sciences, the Protestant Reformation, colonization, and globalization. Since then the relationship between science and religion have been characterized as conflict, harmony, complexity, or mutual independence.

Both science and religion are complex social and cultural endeavors that vary across cultures and have changed over time. Most scientific (and technical) innovations prior to the scientific revolution were achieved by societies organized by religious traditions. Ancient pagan, Islamic, and Christian scholars pioneered individual elements of the scientific method. Roger Bacon, often credited with formalizing the scientific method, was a Franciscan friar. Hinduism has historically embraced reason and empiricism, holding that science brings legitimate, but incomplete knowledge of the world and universe. Confucian thought, whether religious or non-religious in nature, has held different views of science over time. Most 21st-century Buddhists view science as complementary to their beliefs. While the classification of the material world by the ancient Indians and Greeks into air, earth, fire and water was more philosophical, and proto-scientists like Anaxagoras impiously questioned certain popular views of Greek divinities, medieval Middle Eastern scholars used practical and experimental observation to classify materials.

Events in Europe such as the Galileo affair of the early 17th century, associated with the scientific revolution and the Age of Enlightenment, led scholars such as John William Draper to postulate () a conflict thesis, suggesting that religion and science have been in conflict methodologically, factually and politically throughout history. Some contemporary scientists (such as Richard Dawkins, Lawrence Krauss, Peter Atkins, and Donald Prothero) subscribe to this thesis. However, the conflict thesis has lost favor among most contemporary historians of science.

Many scientists, philosophers, and theologians throughout history, such as Francisco Ayala, Kenneth R. Miller and Francis Collins, have seen compatibility or interdependence between religion and science. Biologist Stephen Jay Gould, other scientists, and some contemporary theologians regard religion and science as non-overlapping magisteria, addressing fundamentally separate forms of knowledge and aspects of life. Some theologians or historians of science, including John Lennox, Thomas Berry, Brian Swimme and Ken Wilber propose an interconnection between science and religion, while others such as Ian Barbour believe there are even parallels.

Public acceptance of scientific facts may sometimes be influenced by religious beliefs such as in the United States, where some reject the concept of evolution by natural selection, especially regarding human beings. Nevertheless, the American National Academy of Sciences has written that "the evidence for evolution can be fully compatible with religious faith",
a view endorsed by many religious denominations.

The concepts of "science" and "religion" are a recent invention: "religion" emerged in the 17th century in the midst of colonization and globalization and the Protestant Reformation, "science" emerged in the 19th century in the midst of attempts to narrowly define those who studied nature. Originally what is today known as "science" was pioneered as "natural philosophy". Furthermore, the phrase "religion and science" or "science and religion" emerged in the 19th century, not before, due to the reification of both concepts.

It was in the 19th century that the terms "Buddhism", "Hinduism", "Taoism", "Confucianism" and "World Religions" first emerged. In the ancient and medieval world, the etymological Latin roots of both science ("scientia") and religion ("religio") were understood as inner qualities of the individual or virtues, never as doctrines, practices, or actual sources of knowledge.

It was in the 19th century that the concept of "science" received its modern shape with new titles emerging such as "biology" and "biologist", "physics", and "physicist", among other technical fields and titles; institutions and communities were founded, and unprecedented applications to and interactions with other aspects of society and culture occurred. The term "scientist" was first coined by the naturalist-theologian William Whewell in 1834 and it was applied to those who sought knowledge and understanding of nature. From the ancient world, starting with Aristotle, to the 19th century, the practice of studying nature was commonly referred to as "natural philosophy". Isaac Newton's book Philosophiae Naturalis Principia Mathematica (1687), whose title translates to "Mathematical Principles of Natural Philosophy", reflects the then-current use of the words "natural philosophy", akin to "systematic study of nature". Even in the 19th century, a treatise by Lord Kelvin and Peter Guthrie Tait's, which helped define much of modern physics, was titled Treatise on Natural Philosophy (1867).

It was in the 17th century that the concept of "religion" received its modern shape despite the fact that ancient texts like the Bible, the Quran, and other sacred texts did not have a concept of religion in the original languages and neither did the people or the cultures in which these sacred texts were written. In the 19th century, Max Müller noted that what is called ancient religion today, would have been called "law" in antiquity. For example, there is no precise equivalent of "religion" in Hebrew, and Judaism does not distinguish clearly between religious, national, racial, or ethnic identities. The Sanskrit word "dharma", sometimes translated as "religion", also means law or duty. Throughout classical South Asia, the study of law consisted of concepts such as penance through piety and ceremonial as well as practical traditions. Medieval Japan at first had a similar union between "imperial law" and universal or "Buddha law", but these later became independent sources of power. Throughout its long history, Japan had no concept of "religion" since there was no corresponding Japanese word, nor anything close to its meaning, but when American warships appeared off the coast of Japan in 1853 and forced the Japanese government to sign treaties demanding, among other things, freedom of religion, the country had to contend with this Western idea.

The development of sciences (especially natural philosophy) in Western Europe during the Middle Ages, has considerable foundation in the works of the Arabs who translated Greek and Latin compositions. The works of Aristotle played a major role in the institutionalization, systematization, and expansion of reason. Christianity accepted reason within the ambit of faith. In Christendom, reason was considered subordinate to revelation, which contained the ultimate truth and this truth could not be challenged. In medieval universities, the faculty for natural philosophy and theology were separate, and discussions pertaining to theological issues were often not allowed to be undertaken by the faculty of philosophy.

Natural philosophy, as taught in the arts faculties of the universities, was seen as an essential area of study in its own right and was considered necessary for almost every area of study. It was an independent field, separated from theology, which enjoyed a good deal of intellectual freedom as long as it was restricted to the natural world. In general, there was religious support for natural science by the late Middle Ages and a recognition that it was an important element of learning.

The extent to which medieval science led directly to the new philosophy of the scientific revolution remains a subject for debate, but it certainly had a significant influence.

The Middle Ages laid ground for the developments that took place in science, during the Renaissance which immediately succeeded it. By 1630, ancient authority from classical literature and philosophy, as well as their necessity, started eroding, although scientists were still expected to be fluent in Latin, the international language of Europe's intellectuals. With the sheer success of science and the steady advance of rationalism, the individual scientist gained prestige. Along with the inventions of this period, especially the printing press by Johannes Gutenberg, allowed for the dissemination of the Bible in languages of the common people (languages other than Latin). This allowed more people to read and learn from the scripture, leading to the Evangelical movement. The people who spread this message, concentrated more on individual agency rather than the structures of the Church.

In the 17th century, founders of the Royal Society largely held conventional and orthodox religious views, and a number of them were prominent Churchmen. While theological issues that had the potential to be divisive were typically excluded from formal discussions of the early Society, many of its fellows nonetheless believed that their scientific activities provided support for traditional religious belief. Clerical involvement in the Royal Society remained high until the mid-nineteenth century, when science became more professionalised.

Albert Einstein supported the compatibility of some interpretations of religion with science. In "Science, Philosophy and Religion, A Symposium" published by the Conference on Science, Philosophy and Religion in Their Relation to the Democratic Way of Life, Inc., New York in 1941, Einstein stated:

Einstein thus expresses views of ethical non-naturalism (contrasted to ethical naturalism).

Prominent modern scientists who are atheists include evolutionary biologist Richard Dawkins and Nobel Prize–winning physicist Steven Weinberg. Prominent scientists advocating religious belief include Nobel Prize–winning physicist and United Church of Christ member Charles Townes, evangelical Christian and past head of the Human Genome Project Francis Collins, and climatologist John T. Houghton.

The kinds of interactions that might arise between science and religion have been categorized by theologian, Anglican priest, and physicist John Polkinghorne: (1) conflict between the disciplines, (2) independence of the disciplines, (3) dialogue between the disciplines where they overlap and (4) integration of both into one field.

This typology is similar to ones used by theologians Ian Barbour and John Haught. More typologies that categorize this relationship can be found among the works of other science and religion scholars such as theologian and biochemist Arthur Peacocke.

According to Guillermo Paz-y-Miño-C and Avelina Espinosa, the historical conflict between evolution and religion is intrinsic to the incompatibility between scientific rationalism/empiricism and the belief in supernatural causation. According to evolutionary biologist Jerry Coyne, views on evolution and levels of religiosity in some countries, along with the existence of books explaining reconciliation between evolution and religion, indicate that people have trouble in believing both at the same time, thus implying incompatibility. According to physical chemist Peter Atkins, "whereas religion scorns the power of human comprehension, science respects it." Planetary scientist Carolyn Porco describes a hope that "the confrontation between science and formal religion will come to an end when the role played by science in the lives of all people is the same played by religion today."
Geologist and paleontologist Donald Prothero has stated that religion is the reason "questions about evolution, the age of the earth, cosmology, and human evolution nearly always cause Americans to flunk science literacy tests compared to other nations." However, Jon Miller, who studies science literacy across nations, states that Americans in general are slightly more scientifically literate than Europeans and the Japanese.
According to cosmologist and astrophysicist Lawrence Krauss, compatibility or incompatibility is a theological concern, not a scientific concern. In Lisa Randall's view, questions of incompatibility or otherwise are not answerable, since by accepting revelations one is abandoning rules of logic which are needed to identify if there are indeed contradictions between holding certain beliefs. Daniel Dennett holds that incompatibility exists because religion is not problematic to a certain point before it collapses into a number of excuses for keeping certain beliefs, in light of evolutionary implications.

According to theoretical physicist Steven Weinberg, teaching cosmology and evolution to students should decrease their self-importance in the universe, as well as their religiosity. Evolutionary developmental biologist PZ Myers' view is that all scientists should be atheists, and that science should never accommodate any religious beliefs. Physicist Sean M. Carroll claims that since religion makes claims that are supernatural, both science and religion are incompatible.

Evolutionary biologist Richard Dawkins is openly hostile to religion because he believes it actively debauches the scientific enterprise and education involving science. According to Dawkins, religion "subverts science and saps the intellect". He believes that when science teachers attempt to expound on evolution, there is hostility aimed towards them by parents who are skeptical because they believe it conflicts with their own religious beliefs, and that even in some textbooks have had the word 'evolution' systematically removed. He has worked to argue the negative effects that he believes religion has on education of science.

According to Renny Thomas' study on Indian scientists, atheistic scientists in India called themselves atheists even while accepting that their lifestyle is very much a part of tradition and religion. Thus, they differ from Western atheists in that for them following the lifestyle of a religion is not antithetical to atheism.

Others such as Francis Collins, George F. R. Ellis,
Kenneth R. Miller, Katharine Hayhoe, George Coyne and Simon Conway Morris argue for compatibility since they do not agree that science is incompatible with religion and vice versa. They argue that science provides many opportunities to look for and find God in nature and to reflect on their beliefs. According to Kenneth Miller, he disagrees with Jerry Coyne's assessment and argues that since significant portions of scientists are religious and the proportion of Americans believing in evolution is much higher, it implies that both are indeed compatible. Elsewhere, Miller has argued that when scientists make claims on science and theism or atheism, they are not arguing scientifically at all and are stepping beyond the scope of science into discourses of meaning and purpose. What he finds particularly odd and unjustified is in how atheists often come to invoke scientific authority on their non-scientific philosophical conclusions like there being no point or no meaning to the universe as the only viable option when the scientific method and science never have had any way of addressing questions of meaning or God in the first place. Furthermore, he notes that since evolution made the brain and since the brain can handle both religion and science, there is no natural incompatibility between the concepts at the biological level.

Karl Giberson argues that when discussing compatibility, some scientific intellectuals often ignore the viewpoints of intellectual leaders in theology and instead argue against less informed masses, thereby, defining religion by non intellectuals and slanting the debate unjustly. He argues that leaders in science sometimes trump older scientific baggage and that leaders in theology do the same, so once theological intellectuals are taken into account, people who represent extreme positions like Ken Ham and Eugenie Scott will become irrelevant. Cynthia Tolman notes that religion does not have a method per se partly because religions emerge through time from diverse cultures, but when it comes to Christian theology and ultimate truths, she notes that people often rely on scripture, tradition, reason, and experience to test and gauge what they experience and what they should believe.

The conflict thesis, which holds that religion and science have been in conflict continuously throughout history, was popularized in the 19th century by John William Draper's and Andrew Dickson White's accounts. It was in the 19th century that relationship between science and religion became an actual formal topic of discourse, while before this no one had pitted science against religion or vice versa, though occasional complex interactions had been expressed before the 19th century. Most contemporary historians of science now reject the conflict thesis in its original form and no longer support it. Instead, it has been superseded by subsequent historical research which has resulted in a more nuanced understanding: Historian of science, Gary Ferngren, has stated: "Although popular images of controversy continue to exemplify the supposed hostility of Christianity to new scientific theories, studies have shown that Christianity has often nurtured and encouraged scientific endeavour, while at other times the two have co-existed without either tension or attempts at harmonization. If Galileo and the Scopes trial come to mind as examples of conflict, they were the exceptions rather than the rule."

Most historians today have moved away from a conflict model, which is based mainly on two historical episodes (Galileo and Darwin), toward compatibility theses (either the integration thesis or non-overlapping magisteria) or toward a "complexity" model, because religious figures were on both sides of each dispute and there was no overall aim by any party involved to discredit religion.

An often cited example of conflict, that has been clarified by historical research in the 20th century, was the Galileo affair, whereby interpretations of the Bible were used to attack ideas by Copernicus on heliocentrism. By 1616 Galileo went to Rome to try to persuade Catholic Church authorities not to ban Copernicus' ideas. In the end, a decree of the Congregation of the Index was issued, declaring that the ideas that the Sun stood still and that the Earth moved were "false" and "altogether contrary to Holy Scripture", and suspending Copernicus's "De Revolutionibus" until it could be corrected. Galileo was found "vehemently suspect of heresy", namely of having held the opinions that the Sun lies motionless at the center of the universe, that the Earth is not at its centre and moves. He was required to "abjure, curse and detest" those opinions. However, before all this, Pope Urban VIII had personally asked Galileo to give arguments for and against heliocentrism in a book, and to be careful not to advocate heliocentrism as physically proven since the scientific consensus at the time was that the evidence for heliocentrism was very weak. The Church had merely sided with the scientific consensus of the time. Pope Urban VIII asked that his own views on the matter be included in Galileo's book. Only the latter was fulfilled by Galileo. Whether unknowingly or deliberately, Simplicio, the defender of the Aristotelian/Ptolemaic geocentric view in "Dialogue Concerning the Two Chief World Systems", was often portrayed as an unlearned fool who lacked mathematical training. Although the preface of his book claims that the character is named after a famous Aristotelian philosopher (Simplicius in Latin, Simplicio in Italian), the name "Simplicio" in Italian also has the connotation of "simpleton". Unfortunately for his relationship with the Pope, Galileo put the words of Urban VIII into the mouth of Simplicio. Most historians agree Galileo did not act out of malice and felt blindsided by the reaction to his book. However, the Pope did not take the suspected public ridicule lightly, nor the physical Copernican advocacy. Galileo had alienated one of his biggest and most powerful supporters, the Pope, and was called to Rome to defend his writings.

The actual evidences that finally proved heliocentrism came centuries after Galileo: the stellar aberration of light by James Bradley in the 18th century, the orbital motions of binary stars by William Herschel in the 19th century, the accurate measurement of the stellar parallax in the 19th century, and Newtonian mechanics in the 17th century. According to physicist Christopher Graney, Galileo's own observations did not actually support the Copernican view, but were more consistent with Tycho Brahe's hybrid model where that Earth did not move and everything else circled around it and the Sun.

British philosopher A. C. Grayling, still believes there is competition between science and religions and point to the origin of the universe, the nature of human beings and the possibility of miracles

A modern view, described by Stephen Jay Gould as "non-overlapping magisteria" (NOMA), is that science and religion deal with fundamentally separate aspects of human experience and so, when each stays within its own domain, they co-exist peacefully. While Gould spoke of independence from the perspective of science, W. T. Stace viewed independence from the perspective of the philosophy of religion. Stace felt that science and religion, when each is viewed in its own domain, are both consistent and complete. They originate from different perceptions of reality, as Arnold O. Benz points out, but meet each other, for example, in the feeling of amazement and in ethics.

The USA's National Academy of Science supports the view that science and religion are independent.
Science and religion are based on different aspects of human experience. In science, explanations must be based on evidence drawn from examining the natural world. Scientifically based observations or experiments that conflict with an explanation eventually must lead to modification or even abandonment of that explanation. Religious faith, in contrast, does not depend on empirical evidence, is not necessarily modified in the face of conflicting evidence, and typically involves supernatural forces or entities. Because they are not a part of nature, supernatural entities cannot be investigated by science. In this sense, science and religion are separate and address aspects of human understanding in different ways. Attempts to put science and religion against each other create controversy where none needs to exist.

According to Archbishop John Habgood, both science and religion represent distinct ways of approaching experience and these differences are sources of debate. He views science as descriptive and religion as prescriptive. He stated that if science and mathematics concentrate on what the world "ought to be", in the way that religion does, it may lead to improperly ascribing properties to the natural world as happened among the followers of Pythagoras in the sixth century B.C. In contrast, proponents of a normative moral science take issue with the idea that science has "no" way of guiding "oughts". Habgood also stated that he believed that the reverse situation, where religion attempts to be descriptive, can also lead to inappropriately assigning properties to the natural world. A notable example is the now defunct belief in the Ptolemaic (geocentric) planetary model that held sway until changes in scientific and religious thinking were brought about by Galileo and proponents of his views.

According to Ian Barbour, Thomas S. Kuhn asserted that science is made up of paradigms that arise from cultural traditions, which is similar to the secular perspective on religion.

Michael Polanyi asserted that it is merely a commitment to universality that protects against subjectivity and has nothing at all to do with personal detachment as found in many conceptions of the scientific method. Polanyi further asserted that all knowledge is personal and therefore the scientist must be performing a very personal if not necessarily subjective role when doing science. Polanyi added that the scientist often merely follows intuitions of "intellectual beauty, symmetry, and 'empirical agreement'". Polanyi held that science requires moral commitments similar to those found in religion.

Two physicists, Charles A. Coulson and Harold K. Schilling, both claimed that "the methods of science and religion have much in common." Schilling asserted that both fields—science and religion—have "a threefold structure—of experience, theoretical interpretation, and practical application." Coulson asserted that science, like religion, "advances by creative imagination" and not by "mere collecting of facts," while stating that religion should and does "involve critical reflection on experience not unlike that which goes on in science." Religious language and scientific language also show parallels (cf. rhetoric of science).

The "religion and science community" consists of those scholars who involve themselves with what has been called the "religion-and-science dialogue" or the "religion-and-science field." The community belongs to neither the scientific nor the religious community, but is said to be a third overlapping community of interested and involved scientists, priests, clergymen, theologians and engaged non-professionals. Institutions interested in the intersection between science and religion include the Center for Theology and the Natural Sciences, the Institute on Religion in an Age of Science, the Ian Ramsey Centre, and the Faraday Institute. Journals addressing the relationship between science and religion include "Theology and Science" and "Zygon". Eugenie Scott has written that the "science and religion" movement is, overall, composed mainly of theists who have a healthy respect for science and may be beneficial to the public understanding of science. She contends that the "Christian scholarship" movement is not a problem for science, but that the "Theistic science" movement, which proposes abandoning methodological materialism, does cause problems in understanding of the nature of science. The Gifford Lectures were established in 1885 to further the discussion between "natural theology" and the scientific community. This annual series continues and has included William James, John Dewey, Carl Sagan, and many other professors from various fields.

The modern dialogue between religion and science is rooted in Ian Barbour's 1966 book "Issues in Science and Religion". Since that time it has grown into a serious academic field, with academic chairs in the subject area, and two dedicated academic journals, "Zygon" and "Theology and Science". Articles are also sometimes found in mainstream science journals such as "American Journal of Physics"
and "Science".

Philosopher Alvin Plantinga has argued that there is superficial conflict but deep concord between science and religion, and that there is deep conflict between science and naturalism. Plantinga, in his book "Where the Conflict Really Lies: Science, Religion, and Naturalism", heavily contests the linkage of naturalism with science, as conceived by Richard Dawkins, Daniel Dennett and like-minded thinkers; while Daniel Dennett thinks that Plantinga stretches science to an unacceptable extent. Philosopher Maarten Boudry, in reviewing the book, has commented that he resorts to creationism and fails to "stave off the conflict between theism and evolution." Cognitive scientist Justin L. Barrett, by contrast, reviews the same book and writes that "those most needing to hear Plantinga's message may fail to give it a fair hearing for rhetorical rather than analytical reasons."

As a general view, this holds that while interactions are complex between influences of science, theology, politics, social, and economic concerns, the productive engagements between science and religion throughout history should be duly stressed as the norm.

Scientific and theological perspectives often coexist peacefully. Christians and some non-Christian religions have historically integrated well with scientific ideas, as in the ancient Egyptian technological mastery applied to monotheistic ends, the flourishing of logic and mathematics under Hinduism and Buddhism, and the scientific advances made by Muslim scholars during the Ottoman empire. Even many 19th-century Christian communities welcomed scientists who claimed that science was not at all concerned with discovering the ultimate nature of reality. According to Lawrence M. Principe, the Johns Hopkins University Drew Professor of the Humanities, from a historical perspective this points out that much of the current-day clashes occur between limited extremists—both religious and scientistic fundamentalists—over a very few topics, and that the movement of ideas back and forth between scientific and theological thought has been more usual. To Principe, this perspective would point to the fundamentally common respect for written learning in religious traditions of rabbinical literature, Christian theology, and the Islamic Golden Age, including a Transmission of the Classics from Greek to Islamic to Christian traditions which helped spark the Renaissance. Religions have also given key participation in development of modern universities and libraries; centers of learning & scholarship were coincident with religious institutions – whether pagan, Muslim, or Christian.

A fundamental principle of the Bahá'í Faith is the harmony of religion and science. Bahá'í scripture asserts that true science and true religion can never be in conflict. `Abdu'l-Bahá, the son of the founder of the religion, stated that religion without science is superstition and that science without religion is materialism. He also admonished that true religion must conform to the conclusions of science.

Buddhism and science have been regarded as compatible by numerous authors. Some philosophic and psychological teachings found in Buddhism share points in common with modern Western scientific and philosophic thought. For example, Buddhism encourages the impartial investigation of nature (an activity referred to as "Dhamma-Vicaya" in the Pali Canon)—the principal object of study being oneself. Buddhism and science both show a strong emphasis on causality. However, Buddhism does not focus on materialism.

Tenzin Gyatso, the 14th Dalai Lama, mentions that empirical scientific evidence supersedes the traditional teachings of Buddhism when the two are in conflict. In his book "The Universe in a Single Atom" he wrote, "My confidence in venturing into science lies in my basic belief that as in science, so in Buddhism, understanding the nature of reality is pursued by means of critical investigation." He also stated, "If scientific analysis were conclusively to demonstrate certain claims in Buddhism to be false," he says, "then we must accept the findings of science and abandon those claims."

Among early Christian teachers, Tertullian (c. 160–220) held a generally negative opinion of Greek philosophy, while Origen (c. 185–254) regarded it much more favorably and required his students to read nearly every work available to them.

Earlier attempts at reconciliation of Christianity with Newtonian mechanics appear quite different from later attempts at reconciliation with the newer scientific ideas of evolution or relativity. Many early interpretations of evolution polarized themselves around a "struggle for existence." These ideas were significantly countered by later findings of universal patterns of biological cooperation. According to John Habgood, all man really knows here is that the universe seems to be a mix of good and evil, beauty and pain, and that suffering may somehow be part of the process of creation. Habgood holds that Christians should not be surprised that suffering may be used creatively by God, given their faith in the symbol of the Cross. 
Robert John Russell has examined consonance and dissonance between modern physics, evolutionary biology, and Christian theology.

Christian philosophers Augustine of Hippo (354–30) and Thomas Aquinas held that scriptures can have multiple interpretations on certain areas where the matters were far beyond their reach, therefore one should leave room for future findings to shed light on the meanings. The "Handmaiden" tradition, which saw secular studies of the universe as a very important and helpful part of arriving at a better understanding of scripture, was adopted throughout Christian history from early on. Also the sense that God created the world as a self operating system is what motivated many Christians throughout the Middle Ages to investigate nature.

Modern historians of science such as J.L. Heilbron, Alistair Cameron Crombie, David Lindberg, Edward Grant, Thomas Goldstein, and Ted Davis have reviewed the popular notion that medieval Christianity was a negative influence in the development of civilization and science. In their views, not only did the monks save and cultivate the remnants of ancient civilization during the barbarian invasions, but the medieval church promoted learning and science through its sponsorship of many universities which, under its leadership, grew rapidly in Europe in the 11th and 12th centuries. Saint Thomas Aquinas, the Church's "model theologian", not only argued that reason is in harmony with faith, he even recognized that reason can contribute to understanding revelation, and so encouraged intellectual development. He was not unlike other medieval theologians who sought out reason in the effort to defend his faith. Some of today's scholars, such as Stanley Jaki, have claimed that Christianity with its particular worldview, was a crucial factor for the emergence of modern science.

David C. Lindberg states that the widespread popular belief that the Middle Ages was a time of ignorance and superstition due to the Christian church is a "caricature". According to Lindberg, while there are some portions of the classical tradition which suggest this view, these were exceptional cases. It was common to tolerate and encourage critical thinking about the nature of the world. The relation between Christianity and science is complex and cannot be simplified to either harmony or conflict, according to Lindberg. Lindberg reports that "the late medieval scholar rarely experienced the coercive power of the church and would have regarded himself as free (particularly in the natural sciences) to follow reason and observation wherever they led. There was no warfare between science and the church." Ted Peters in "Encyclopedia of Religion" writes that although there is some truth in the "Galileo's condemnation" story but through exaggerations, it has now become "a modern myth perpetuated by those wishing to see warfare between science and religion who were allegedly persecuted by an atavistic and dogma-bound ecclesiastical authority". In 1992, the Catholic Church's seeming vindication of Galileo attracted much comment in the media.

A degree of concord between science and religion can be seen in religious belief and empirical science. The belief that God created the world and therefore humans, can lead to the view that he arranged for humans to know the world. This is underwritten by the doctrine of imago dei. In the words of Thomas Aquinas, "Since human beings are said to be in the image of God in virtue of their having a nature that includes an intellect, such a nature is most in the image of God in virtue of being most able to imitate God".

During the Enlightenment, a period "characterized by dramatic revolutions in science" and the rise of Protestant challenges to the authority of the Catholic Church via individual liberty, the authority of Christian scriptures became strongly challenged. As science advanced, acceptance of a literal version of the Bible became "increasingly untenable" and some in that period presented ways of interpreting scripture according to its spirit on its authority and truth.

In recent history, the theory of evolution has been at the center of some controversy between Christianity and science. Christians who accept a literal interpretation of the biblical account of creation find incompatibility between Darwinian evolution and their interpretation of the Christian faith. Creation science or scientific creationism is a branch of creationism that attempts to provide scientific support for the Genesis creation narrative in the Book of Genesis and attempts to disprove generally accepted scientific facts, theories and scientific paradigms about the geological history of the Earth, cosmology of the early universe, 
the chemical origins of life and biological evolution. It began in the 1960s as a fundamentalist Christian effort in the United States to prove Biblical inerrancy and falsify the scientific evidence for evolution. It has since developed a sizable religious following in the United States, with creation science ministries branching worldwide. In 1925, The State of Tennessee passed the Butler Act, which prohibited the teaching of the theory of evolution in all schools in the state. Later that year, a similar law was passed in Mississippi, and likewise, Arkansas in 1927. In 1968, these "anti-monkey" laws were struck down by the Supreme Court of the United States as unconstitutional, "because they established a religious doctrine violating both the First and Fourth Amendments to the Constitution.

Most scientists have rejected creation science for several reasons, including that its claims do not refer to natural causes and cannot be tested. In 1987, the United States Supreme Court ruled that creationism is religion, not science, and cannot be advocated in public school classrooms. In 2018, the "Orlando Sentinel" reported that "Some private schools in Florida that rely on public funding teach students" Creationism.

Theistic evolution attempts to reconcile Christian beliefs and science by accepting the scientific understanding of the age of the Earth and the process of evolution. It includes a range of beliefs, including views described as evolutionary creationism, which accepts some findings of modern science but also upholds classical religious teachings about God and creation in Christian context.

While refined and clarified over the centuries, the Roman Catholic position on the relationship between science and religion is one of harmony, and has maintained the teaching of natural law as set forth by Thomas Aquinas. For example, regarding scientific study such as that of evolution, the church's unofficial position is an example of theistic evolution, stating that faith and scientific findings regarding human evolution are not in conflict, though humans are regarded as a special creation, and that the existence of God is required to explain both monogenism and the spiritual component of human origins. Catholic schools have included all manners of scientific study in their curriculum for many centuries.

Galileo once stated "The intention of the Holy Spirit is to teach us how to go to heaven, not how the heavens go." In 1981 John Paul II, then pope of the Roman Catholic Church, spoke of the relationship this way: "The Bible itself speaks to us of the origin of the universe and its make-up, not in order to provide us with a scientific treatise, but in order to state the correct relationships of man with God and with the universe. Sacred Scripture wishes simply to declare that the world was created by God, and in order to teach this truth it expresses itself in the terms of the cosmology in use at the time of the writer".

According to Andrew Dickson White's "A History of the Warfare of Science with Theology in Christendom" from the 19th century, a biblical world view affected negatively the progress of science through time. Dickinson also argues that immediately following the Reformation matters were even worse . The interpretations of Scripture by Luther and Calvin became as sacred to their followers as the Scripture itself. For instance, when Georg Calixtus ventured, in interpreting the Psalms, to question the accepted belief that "the waters above the heavens" were contained in a vast receptacle upheld by a solid vault, he was bitterly denounced as heretical. Today, much of the scholarship in which the conflict thesis was originally based is considered to be inaccurate. For instance, the claim that early Christians rejected scientific findings by the Greco-Romans is false, since the "handmaiden" view of secular studies was seen to shed light on theology. This view was widely adapted throughout the early medieval period and afterwards by theologians (such as Augustine) and ultimately resulted in fostering interest in knowledge about nature through time. Also, the claim that people of the Middle Ages widely believed that the Earth was flat was first propagated in the same period that originated the conflict thesis and is still very common in popular culture. Modern scholars regard this claim as mistaken, as the contemporary historians of science David C. Lindberg and Ronald L. Numbers write: "there was scarcely a Christian scholar of the Middle Ages who did not acknowledge [earth's] sphericity and even know its approximate circumference." From the fall of Rome to the time of Columbus, all major scholars and many vernacular writers interested in the physical shape of the earth held a spherical view with the exception of Lactantius and Cosmas.

H. Floris Cohen argued for a biblical Protestant, but not excluding Catholicism, influence on the early development of modern science. He presented Dutch historian R. Hooykaas' argument that a biblical world-view holds all the necessary antidotes for the hubris of Greek rationalism: a respect for manual labour, leading to more experimentation and empiricism, and a supreme God that left nature open to emulation and manipulation. It supports the idea early modern science rose due to a combination of Greek and biblical thought.

Oxford historian Peter Harrison is another who has argued that a biblical worldview was significant for the development of modern science. Harrison contends that Protestant approaches to the book of scripture had significant, if largely unintended, consequences for the interpretation of the book of nature. Harrison has also suggested that literal readings of the Genesis narratives of the Creation and Fall motivated and legitimated scientific activity in seventeenth-century England. For many of its seventeenth-century practitioners, science was imagined to be a means of restoring a human dominion over nature that had been lost as a consequence of the Fall.

Historian and professor of religion Eugene M. Klaaren holds that "a belief in divine creation" was central to an emergence of science in seventeenth-century England. The philosopher Michael Foster has published analytical philosophy connecting Christian doctrines of creation with empiricism. Historian William B. Ashworth has argued against the historical notion of distinctive mind-sets and the idea of Catholic and Protestant sciences. Historians James R. Jacob and Margaret C. Jacob have argued for a linkage between seventeenth-century Anglican intellectual transformations and influential English scientists (e.g., Robert Boyle and Isaac Newton). John Dillenberger and Christopher B. Kaiser have written theological surveys, which also cover additional interactions occurring in the 18th, 19th, and 20th centuries. Philosopher of Religion, Richard Jones, has written a philosophical critique of the "dependency thesis" which assumes that modern science emerged from Christian sources and doctrines. Though he acknowledges that modern science emerged in a religious framework, that Christianity greatly elevated the importance of science by sanctioning and religiously legitimizing it in the medieval period, and that Christianity created a favorable social context for it to grow; he argues that direct Christian beliefs or doctrines were not primary sources of scientific pursuits by natural philosophers, nor was Christianity, in and of itself, exclusively or directly necessary in developing or practicing modern science.

Oxford University historian and theologian John Hedley Brooke wrote that "when natural philosophers referred to "laws" of nature, they were not glibly choosing that metaphor. Laws were the result of legislation by an intelligent deity. Thus the philosopher René Descartes (1596–1650) insisted that he was discovering the "laws that God has put into nature." Later Newton would declare that the regulation of the solar system presupposed the "counsel and dominion of an intelligent and powerful Being." Historian Ronald L. Numbers stated that this thesis "received a boost" from mathematician and philosopher Alfred North Whitehead's "Science and the Modern World" (1925). Numbers has also argued, "Despite the manifest shortcomings of the claim that Christianity gave birth to science—most glaringly, it ignores or minimizes the contributions of ancient Greeks and medieval Muslims—it too, refuses to succumb to the death it deserves." The sociologist Rodney Stark of Baylor University, argued in contrast that "Christian theology was essential for the rise of science."

Protestantism had an important influence on science. According to the Merton Thesis there was a positive correlation between the rise of Puritanism and Protestant Pietism on the one hand and early experimental science on the other. The Merton Thesis has two separate parts: Firstly, it presents a theory that science changes due to an accumulation of observations and improvement in experimental techniques and methodology; secondly, it puts forward the argument that the popularity of science in 17th-century England and the religious demography of the Royal Society (English scientists of that time were predominantly Puritans or other Protestants) can be explained by a correlation between Protestantism and the scientific values. In his theory, Robert K. Merton focused on English Puritanism and German Pietism as having been responsible for the development of the scientific revolution of the 17th and 18th centuries. Merton explained that the connection between religious affiliation and interest in science was the result of a significant synergy between the ascetic Protestant values and those of modern science. Protestant values encouraged scientific research by allowing science to study God's influence on the world and thus providing a religious justification for scientific research.

In "Reconciling Science and Religion: The Debate in Early-twentieth-century Britain", historian of biology Peter J. Bowler argues that in contrast to the conflicts between science and religion in the U.S. in the 1920s (most famously the Scopes Trial), during this period Great Britain experienced a concerted effort at reconciliation, championed by intellectually conservative scientists, supported by liberal theologians but opposed by younger scientists and secularists and conservative Christians. These attempts at reconciliation fell apart in the 1930s due to increased social tensions, moves towards neo-orthodox theology and the acceptance of the modern evolutionary synthesis.

In the 20th century, several ecumenical organizations promoting a harmony between science and Christianity were founded, most notably the American Scientific Affiliation, The Biologos Foundation, Christians in Science, The Society of Ordained Scientists, and The Veritas Forum.

The historical process of Confucianism has largely been antipathic towards scientific discovery. However the religio-philosophical system itself is more neutral on the subject than such an analysis might suggest. In his writings On Heaven, Xunzi espoused a proto-scientific world view. However, during the Han Synthesis the more anti-empirical Mencius was favored and combined with Daoist skepticism regarding the nature of reality. Likewise, during the Medieval period, Zhu Xi argued against technical investigation and specialization proposed by Chen Liang. After contact with the West, scholars such as Wang Fuzhi would rely on Buddhist/Daoist skepticism to denounce all science as a subjective pursuit limited by humanity's fundamental ignorance of the true nature of the world. After the May Fourth Movement, attempts to modernize Confucianism and reconcile it with scientific understanding were attempted by many scholars including Feng Youlan and Xiong Shili. Given the close relationship that Confucianism shares with Buddhism, many of the same arguments used to reconcile Buddhism with science also readily translate to Confucianism. However, modern scholars have also attempted to define the relationship between science and Confucianism on Confucianism's own terms and the results have usually led to the conclusion that Confucianism and science are fundamentally compatible.

In Hinduism, the dividing line between objective sciences and spiritual knowledge ("adhyatma vidya") is a linguistic paradox. Hindu scholastic activities and ancient Indian scientific advancements were so interconnected that many Hindu scriptures are also ancient scientific manuals and vice versa. In 1835, English was made the primary language for teaching in higher education in India, exposing Hindu scholars to Western secular ideas; this started a renaissance regarding religious and philosophical thought. Hindu sages maintained that logical argument and rational proof using Nyaya is the way to obtain correct knowledge. The scientific level of understanding focuses on how things work and from where they originate, while Hinduism strives to understand the ultimate purposes for the existence of living things. To obtain and broaden the knowledge of the world for spiritual perfection, many refer to the Bhāgavata for guidance because it draws upon a scientific and theological dialogue. Hinduism offers methods to correct and transform itself in course of time. For instance, Hindu views on the development of life include a range of viewpoints in regards to evolution, creationism, and the origin of life within the traditions of Hinduism. For instance, it has been suggested that Wallace-Darwininan evolutionary thought was a part of Hindu thought centuries before modern times. The Shankara and the Sāmkhya did not have a problem with the theory of evolution, but instead, argued about the existence of God and what happened after death. These two distinct groups argued among each other's philosophies because of their sacred texts, not the idea of evolution. With the publication of Darwin's "On the Origin of Species", many Hindus were eager to connect their scriptures to Darwinism, finding similarities between Brahma's creation, Vishnu's incarnations, and evolution theories.

Samkhya, the oldest school of Hindu philosophy prescribes a particular method to analyze knowledge. According to Samkhya, all knowledge is possible through three means of valid knowledge –

Nyaya, the Hindu school of logic, accepts all these 3 means and in addition accepts one more – "Upamāna" (comparison).

The accounts of the emergence of life within the universe vary in description, but classically the deity called Brahma, from a Trimurti of three deities also including Vishnu and Shiva, is described as performing the act of 'creation', or more specifically of 'propagating life within the universe' with the other two deities being responsible for 'preservation' and 'destruction' (of the universe) respectively. In this respect some Hindu schools do not treat the scriptural creation myth literally and often the creation stories themselves do not go into specific detail, thus leaving open the possibility of incorporating at least some theories in support of evolution. Some Hindus find support for, or foreshadowing of evolutionary ideas in scriptures, namely the Vedas.

The incarnations of Vishnu (Dashavatara) is almost identical to the scientific explanation of the sequence of biological evolution of man and animals. The sequence of avatars starts from an aquatic organism (Matsya), to an amphibian (Kurma), to a land-animal (Varaha), to a humanoid (Narasimha), to a dwarf human (Vamana), to 5 forms of well developed human beings (Parashurama, Rama, Balarama/Buddha, Krishna, Kalki) who showcase an increasing form of complexity (Axe-man, King, Plougher/Sage, wise Statesman, mighty Warrior). In fact, many Hindu gods are represented with features of animals as well as those of humans, leading many Hindus to easily accept evolutionary links between animals and humans. In India, the home country of Hindus, educated Hindus widely accept the theory of biological evolution. In a survey of 909 people, 77% of respondents in India agreed with Charles Darwin's Theory of Evolution, and 85 per cent of God-believing people said they believe in evolution as well.

As per Vedas, another explanation for the creation is based on the five elements: earth, water, fire, air and aether.
The Hindu religion traces its beginnings to the sacred Vedas. Everything that is established in the Hindu faith such as the gods and goddesses, doctrines, chants, spiritual insights, etc. flow from the poetry of Vedic hymns. The Vedas offer an honor to the sun and moon, water and wind, and to the order in Nature that is universal. This naturalism is the beginning of what further becomes the connection between Hinduism and science.

From an Islamic standpoint, science, the study of nature, is considered to be linked to the concept of "Tawhid" (the Oneness of God), as are all other branches of knowledge. In Islam, nature is not seen as a separate entity, but rather as an integral part of Islam's holistic outlook on God, humanity, and the world. The Islamic view of science and nature is continuous with that of religion and God. This link implies a sacred aspect to the pursuit of scientific knowledge by Muslims, as nature itself is viewed in the Qur'an as a compilation of signs pointing to the Divine. It was with this understanding that science was studied and understood in Islamic civilizations, specifically during the eighth to sixteenth centuries, prior to the colonization of the Muslim world. Robert Briffault, in "The Making of Humanity", asserts that the very existence of science, as it is understood in the modern sense, is rooted in the scientific thought and knowledge that emerged in Islamic civilizations during this time. Ibn al-Haytham, an Arab Muslim, was an early proponent of the concept that a hypothesis must be proved by experiments based on confirmable procedures or mathematical evidence—hence understanding the scientific method 200 years before Renaissance scientists. Ibn al-Haytham described his theology: 
With the decline of Islamic Civilizations in the late Middle Ages and the rise of Europe, the Islamic scientific tradition shifted into a new period. Institutions that had existed for centuries in the Muslim world looked to the new scientific institutions of European powers. This changed the practice of science in the Muslim world, as Islamic scientists had to confront the western approach to scientific learning, which was based on a different philosophy of nature. From the time of this initial upheaval of the Islamic scientific tradition to the present day, Muslim scientists and scholars have developed a spectrum of viewpoints on the place of scientific learning within the context of Islam, none of which are universally accepted or practiced. However, most maintain the view that the acquisition of knowledge and scientific pursuit in general is not in disaccord with Islamic thought and religious belief.

The Ahmadiyya movement emphasize that "there is no contradiction between Islam and science". For example, Ahmadi Muslims universally accept in principle the process of evolution, albeit divinely guided, and actively promote it. Over the course of several decades the movement has issued various publications in support of the scientific concepts behind the process of evolution, and frequently engages in promoting how religious scriptures, such as the Qur'an, supports the concept. For general purposes, the second Khalifa of the community, Mirza Basheer-ud-Din Mahmood Ahmad says:
The Holy Quran directs attention towards science, time and again, rather than evoking prejudice against it. The Quran has never advised against studying science, lest the reader should become a non-believer; because it has no such fear or concern. The Holy Quran is not worried that if people will learn the laws of nature its spell will break. The Quran has not prevented people from science, rather it states, "Say, 'Reflect on what is happening in the heavens and the earth.'" (Al Younus)

Jainism does not support belief in a creator deity. According to Jain doctrine, the universe and its constituents – soul, matter, space, time, and principles of motion have always existed (a static universe similar to that of Epicureanism and steady state cosmological model). All the constituents and actions are governed by universal natural laws. It is not possible to create matter out of nothing and hence the sum total of matter in the universe remains the same (similar to law of conservation of mass). Similarly, the soul of each living being is unique and uncreated and has existed since beginningless time.

The Jain theory of causation holds that a cause and its effect are always identical in nature and hence a conscious and immaterial entity like God cannot create a material entity like the universe. Furthermore, according to the Jain concept of divinity, any soul who destroys its karmas and desires, achieves liberation. A soul who destroys all its passions and desires has no desire to interfere in the working of the universe. Moral rewards and sufferings are not the work of a divine being, but a result of an innate moral order in the cosmos; a self-regulating mechanism whereby the individual reaps the fruits of his own actions through the workings of the karmas.

Through the ages, Jain philosophers have adamantly rejected and opposed the concept of creator and omnipotent God and this has resulted in Jainism being labeled as "nastika darsana" or atheist philosophy by the rival religious philosophies. The theme of non-creationism and absence of omnipotent God and divine grace runs strongly in all the philosophical dimensions of Jainism, including its cosmology, karma, moksa and its moral code of conduct. Jainism asserts a religious and virtuous life is possible without the idea of a creator god.

Sikh Gurus preached that devotion must be rooted in thought, contemplation and knowledge otherwise it leads to blind faith, superstitions and hypocrisy. The Gurus have thus emphasized the need to cultivate scientific attitude towards worldly affairs as well as spiritual progress. The Sikh scripture, Guru Granth Sahib, counters many prevalent beliefs like astrology, animal sacrifice, pilgrimage, fasting, ascetism, menstrual taboo, heaven, hell etc by giving logical reasoning. For example, Guru Nanak questions the belief of treating menstruation as ritually unclean and explains that going by the same logic, everything is impure.

Sikhs in general do not view evolution and the big bang as contradictory to the Sikh religion as Sikh Gurus didn't made any scientific claims in their writings. Sikh teachings mention little about how God created the Universe and how life developed on Earth. Rather than trying to offer a scientific answer to questions about the origins of the Universe, Guru Granth Sahib is more concerned about making it clear that God is in complete control of Universe.

Since 1901–2013, 22% of all Nobel prizes have been awarded to Jews despite them being less than 1% of the world population.

Between 1901 and 2000, 654 Laureates belonged to 28 different religions. Most (65%) have identified Christianity in its various forms as their religious preference. Specifically on the science related prizes, Christians have won a total of 73% of all the Chemistry, 65% in Physics, 62% in Medicine, and 54% in all Economics awards. Jews have won 17% of the prizes in Chemistry, 26% in Medicine, and 23% in Physics. Atheists, Agnostics, and Freethinkers have won 7% of the prizes in Chemistry, 9% in Medicine, and 5% in Physics. Muslims have won 13 prizes (three were in scientific categories).

In 1916, 1,000 leading American scientists were randomly chosen from "American Men of Science" and 42% believed God existed, 42% disbelieved, and 17% had doubts/did not know; however when the study was replicated 80 years later using "American Men and Women of Science" in 1996, results were very much the same with 39% believing God exists, 45% disbelieved, and 15% had doubts/did not know. In the same 1996 survey, for scientists in the fields of biology, mathematics, and physics/astronomy, belief in a god that is "in intellectual and affective communication with humankind" was most popular among mathematicians (about 45%) and least popular among physicists (about 22%). In total, in terms of belief toward a personal god and personal immortality, about 60% of United States scientists in these fields expressed either disbelief or agnosticism and about 40% expressed belief. This compared with 62.9% in 1914 and 33% in 1933.

A survey conducted between 2005 and 2007 by Elaine Howard Ecklund of University at Buffalo, The State University of New York of 1,646 natural and social science professors at 21 US research universities found that, in terms of belief in God or a higher power, more than 60% expressed either disbelief or agnosticism and more than 30% expressed belief. More specifically, nearly 34% answered "I do not believe in God" and about 30% answered "I do not know if there is a God and there is no way to find out." In the same study, 28% said they believed in God and 8% believed in a higher power that was not God. Ecklund stated that scientists were often able to consider themselves spiritual without religion or belief in god. Ecklund and Scheitle concluded, from their study, that the individuals from non-religious backgrounds disproportionately had self-selected into scientific professions and that the assumption that becoming a scientist necessarily leads to loss of religion is untenable since the study did not strongly support the idea that scientists had dropped religious identities due to their scientific training. Instead, factors such as upbringing, age, and family size were significant influences on religious identification since those who had religious upbringing were more likely to be religious and those who had a non-religious upbringing were more likely to not be religious. The authors also found little difference in religiosity between social and natural scientists.

Many studies have been conducted in the United States and have generally found that scientists are less likely to believe in God than are the rest of the population. Precise definitions and statistics vary, with some studies concluding that about of scientists in the U.S. are atheists, agnostic, and have some belief in God (although some might be deistic, for example). This is in contrast to the more than roughly of the general population that believe in some God in the United States. Other studies on scientific organizations like the AAAS show that 51% of their scientists believe in either God or a higher power and 48% having no religion. Belief also varies slightly by field. Two surveys on physicists, geoscientists, biologists, mathematicians, and chemists have noted that, from those specializing in these fields, physicists had lowest percentage of belief in God (29%) while chemists had highest (41%). Other studies show that among members of the National Academy of Sciences, concerning the existence of a personal god who answers prayer, 7% expressed belief, 72% expressed disbelief, and 21% were agnostic, however Eugenie Scott argued that there are methodological issues in the study, including ambiguity in the questions. A study with simplified wording to include impersonal or non-interventionist ideas of God concluded that 40% of leading scientists in the US scientists believe in a god.

In terms of perceptions, most social and natural scientists from 21 American universities did not perceive conflict between science and religion, while 37% did. However, in the study, scientists who had experienced limited exposure to religion tended to perceive conflict. In the same study they found that nearly one in five atheist scientists who are parents (17%) are part of religious congregations and have attended a religious service more than once in the past year. Some of the reasons for doing so are their scientific identity (wishing to expose their children to all sources of knowledge so they can make up their own minds), spousal influence, and desire for community.

A 2009 report by the Pew Research Center found that members of the American Association for the Advancement of Science (AAAS) were "much less religious than the general public," with 51% believing in some form of deity or higher power. Specifically, 33% of those polled believe in God, 18% believe in a universal spirit or higher power, and 41% did not believe in either God or a higher power. 48% say they have a religious affiliation, equal to the number who say they are not affiliated with any religious tradition. 17% were atheists, 11% were agnostics, 20% were nothing in particular, 8% were Jewish, 10% were Catholic, 16% were Protestant, 4% were Evangelical, 10% were other religion. The survey also found younger scientists to be "substantially more likely than their older counterparts to say they believe in God". Among the surveyed fields, chemists were the most likely to say they believe in God.

Elaine Ecklund conducted a study from 2011 to 2014 involving the general US population, including rank and file scientists, in collaboration with the American Association for the Advancement of Science (AAAS). The study noted that 76% of the scientists identified with a religious tradition. 85% of evangelical scientists had no doubts about the existence of God, compared to 35% of the whole scientific population. In terms of religion and science, 85% of evangelical scientists saw no conflict (73% collaboration, 12% independence), while 75% of the whole scientific population saw no conflict (40% collaboration, 35% independence).

Religious beliefs of US professors were examined using a nationally representative sample of more than 1,400 professors. They found that in the social sciences: 23% did not believe in God, 16% did not know if God existed, 43% believed God existed, and 16% believed in a higher power. Out of the natural sciences: 20% did not believe in God, 33% did not know if God existed, 44% believed God existed, and 4% believed in a higher power. Overall, out of the whole study: 10% were atheists, 13% were agnostic, 19% believe in a higher power, 4% believe in God some of the time, 17% had doubts but believed in God, 35% believed in God and had no doubts.

Farr Curlin, a University of Chicago Instructor in Medicine and a member of the MacLean Center for Clinical Medical Ethics, noted in a study that doctors tend to be science-minded religious people. He helped author a study that "found that 76 percent of doctors believe in God and 59 percent believe in some sort of afterlife." Furthermore, "90 percent of doctors in the United States attend religious services at least occasionally, compared to 81 percent of all adults." He reasoned, "The responsibility to care for those who are suffering and the rewards of helping those in need resonate throughout most religious traditions."

Physicians in the United States, by contrast, are much more religious than scientists, with 76% stating a belief in God.

According to the Study of Secularism in Society and Culture's report on 1,100 scientists in India: 66% are Hindu, 14% did not report a religion, 10% are atheist/no religion, 3% are Muslim, 3% are Christian, 4% are Buddhist, Sikh or other. 39% have a belief in a god, 6% have belief in a god sometimes, 30% do not believe in a god but believe in a higher power, 13% do not know if there is a god, and 12% do not believe in a god. 49% believe in the efficacy of prayer, 90% strongly agree or somewhat agree with approving degrees in Ayurvedic medicine. Furthermore, the term "secularism" is understood to have diverse and simultaneous meanings among Indian scientists: 93% believe it to be tolerance of religions and philosophies, 83% see it as involving separation of church and state, 53% see it as not identifying with religious traditions, 40% see it as absence of religious beliefs, and 20% see it as atheism. Accordingly, 75% of Indian scientists had a "secular" outlook in terms of being tolerant of other religions.

According to the Religion Among Scientists in International Context (RASIC) study on 1,581 scientists from the United Kingdom and 1,763 scientists from India, along with 200 interviews: 65% of U.K. scientists identified as nonreligious and only 6% of Indian scientists identify as nonreligious, 12% of scientists in the U.K. attend religious services on a regular basis and 32% of scientists in India do. In terms of the Indian scientists, 73% of scientists responded that there are basic truths in many religions, 27% said they believe in God and 38% expressed belief in a higher power of some kind. In terms of perceptions of conflict between science and religion, less than half of both U.K. scientists (38%) and Indian scientists (18%) perceived conflict between religion and science.

Global studies which have pooled data on religion and science from 1981–2001, have noted that countries with high religiosity also have stronger faith in science, while less religious countries have more skepticism of the impact of science and technology. The United States is noted there as distinctive because of greater faith in both God and scientific progress. Other research cites the National Science Foundation's finding that America has more favorable public attitudes towards science than Europe, Russia, and Japan despite differences in levels of religiosity in these cultures.

A study conducted on adolescents from Christian schools in Northern Ireland, noted a positive relationship between attitudes towards Christianity and science once attitudes towards scientism and creationism were accounted for.

A study on people from Sweden concludes that though the Swedes are among the most non-religious, paranormal beliefs are prevalent among both the young and adult populations. This is likely due to a loss of confidence in institutions such as the Church and Science.

Concerning specific topics like creationism, it is not an exclusively American phenomenon. A poll on adult Europeans revealed that 40% believed in naturalistic evolution, 21% in theistic evolution, 20% in special creation, and 19% are undecided; with the highest concentrations of young earth creationists in Switzerland (21%), Austria (20%), Germany (18%). Other countries such as Netherlands, Britain, and Australia have experienced growth in such views as well.

According to a 2015 Pew Research Center Study on the public perceptions on science, people's perceptions on conflict with science have more to do with their perceptions of other people's beliefs than their own personal beliefs. For instance, the majority of people with a religious affiliation (68%) saw no conflict between their own personal religious beliefs and science while the majority of those without a religious affiliation (76%) perceived science and religion to be in conflict. The study noted that people who are not affiliated with any religion, also known as "religiously unaffiliated", often have supernatural beliefs and spiritual practices despite them not being affiliated with any religion and also that "just one-in-six religiously unaffiliated adults (16%) say their own religious beliefs conflict with science." Furthermore, the study observed, "The share of all adults who perceive a conflict between science and their own religious beliefs has declined somewhat in recent years, from 36% in 2009 to 30% in 2014. Among those who are affiliated with a religion, the share of people who say there is a conflict between science and their personal religious beliefs dropped from 41% to 34% during this period."

The 2013 MIT Survey on Science, Religion and Origins examined the views of religious people in America on origins science topics like evolution, the Big Bang, and perceptions of conflicts between science and religion. It found that a large majority of religious people see no conflict between science and religion and only 11% of religious people belong to religions openly rejecting evolution. The fact that the gap between personal and official beliefs of their religions is so large suggests that part of the problem, might be defused by people learning more about their own religious doctrine and the science it endorses, thereby bridging this belief gap. The study concluded that "mainstream religion and mainstream science are neither attacking one another nor perceiving a conflict." Furthermore, they note that this conciliatory view is shared by most leading science organizations such as the American Association for the Advancement of Science (AAAS).

A study was made in collaboration with the American Association for the Advancement of Science (AAAS) collecting data on the general public from 2011 to 2014, with the focus on evangelicals and evangelical scientists. Even though evangelicals make up only 26% of the US population, the study found that nearly 70 percent of all evangelical Christians do not view science and religion as being in conflict with each other (48% saw them as complementary and 21% saw them as independent) while 73% of the general US population saw no conflict either.

Other lines of research on perceptions of science among the American public conclude that most religious groups see no general epistemological conflict with science and they have no differences with nonreligious groups in the propensity of seeking out scientific knowledge, although there may be subtle epistemic or moral conflicts when scientists make counterclaims to religious tenets. Findings from the Pew Center note similar findings and also note that the majority of Americans (80–90%) show strong support for scientific research, agree that science makes society and individual's lives better, and 8 in 10 Americans would be happy if their children were to become scientists. Even strict creationists tend to have very favorable views on science.

According to a 2007 poll by the Pew Forum, "while large majorities of Americans respect science and scientists, they are not always willing to accept scientific findings that squarely contradict their religious beliefs." The Pew Forum states that specific factual disagreements are "not common today", though 40% to 50% of Americans do not accept the evolution of humans and other living things, with the "strongest opposition" coming from evangelical Christians at 65% saying life did not evolve. 51% of the population believes humans and other living things evolved: 26% through natural selection only, 21% somehow guided, 4% don't know. In the U.S., biological evolution is the only concrete example of conflict where a significant portion of the American public denies scientific consensus for religious reasons. In terms of advanced industrialized nations, the United States is the most religious.

A 2009 study from the Pew Research Center on Americans perceptions of science, showed a broad consensus that most Americans, including most religious Americans, hold scientific research and scientists themselves in high regard. The study showed that 84% of Americans say they view science as having a mostly positive impact on society. Among those who attend religious services at least once a week, the number is roughly the same at 80%. Furthermore, 70% of U.S. adults think scientists contribute "a lot" to society.

A 2011 study on a national sample of US college students examined whether these students viewed the science / religion relationship as reflecting primarily conflict, collaboration, or independence. The study concluded that the majority of undergraduates in both the natural and social sciences do not see conflict between science and religion. Another finding in the study was that it is more likely for students to move away from a conflict perspective to an independence or collaboration perspective than towards a conflict view.

In the US, people who had no religious affiliation were no more likely than the religious population to have New Age beliefs and practices.

By tradition:

In the US:




</doc>
<doc id="29268" url="https://en.wikipedia.org/wiki?curid=29268" title="Stephen Sondheim">
Stephen Sondheim

Stephen Joshua Sondheim (; born March 22, 1930) is an American composer and lyricist known for his work in musical theater.

One of the most important figures in 20th-century musical theater, Sondheim has been praised for "[reinventing] the American musical" with shows that tackle "unexpected themes that range far beyond the [genre's] traditional subjects" with "music and lyrics of unprecedented complexity and sophistication". His shows have been praised for addressing "darker, more harrowing elements of the human experience", with songs often tinged with "ambivalence" about various aspects of life. His best-known works as composer and lyricist include "A Funny Thing Happened on the Way to the Forum" (1962), "Company" (1970), "Follies" (1971), "A Little Night Music" (1973), "" (1979), "Sunday in the Park with George" (1984), and "Into the Woods" (1987). He is also known for writing the lyrics for "West Side Story" (1957) and "Gypsy" (1959).

He has received an Academy Award, eight Tony Awards (more than any other composer, including a Special Tony Award for Lifetime Achievement in the Theatre), eight Grammy Awards, a Pulitzer Prize, a Laurence Olivier Award, and a 2015 Presidential Medal of Freedom. In 2010, the former Henry Miller's Theater on Broadway was renamed the Stephen Sondheim Theatre; in 2019, it was announced that the Queen's Theatre in the West End of London would be renamed the Sondheim Theatre at the end of the year.

Sondheim has written film music, contributing "Goodbye for Now" for Warren Beatty's 1981 "Reds". He wrote five songs for 1990's "Dick Tracy", including "Sooner or Later (I Always Get My Man)", sung in the film by Madonna, which won the Academy Award for Best Original Song. Film adaptations of Sondheim's work include "West Side Story" (1961), "" (2007), and "Into the Woods" (2014). 

Sondheim was born into a Jewish family in New York City, the son of Etta Janet ("Foxy", née Fox; 1897–1992) and Herbert Sondheim (1895–1966). His father manufactured dresses designed by his mother. The composer grew up on the Upper West Side of Manhattan and, after his parents divorced, on a farm near Doylestown, Pennsylvania. As the only child of well-to-do parents living in the San Remo on Central Park West, he was described in Meryle Secrest's biography ("Stephen Sondheim: A Life") as an isolated, emotionally neglected child. When he lived in New York, Sondheim attended ECFS, the Ethical Culture Fieldston School known simply as "Fieldston". He later attended the New York Military Academy and George School, a private Quaker preparatory school in Bucks County, Pennsylvania where he wrote his first musical, "By George," and from which he graduated in 1946. Sondheim spent several summers at Camp Androscoggin. He later matriculated to Williams College and graduated in 1950.

He traces his interest in theatre to "Very Warm for May", a Broadway musical he saw when he was nine. "The curtain went up and revealed a piano," Sondheim recalled. "A butler took a duster and brushed it up, tinkling the keys. I thought that was thrilling."

When Sondheim was ten years old, his father (already a distant figure) had left his mother for another woman (Alicia, with whom he had two sons). Herbert sought custody of Stephen but was unsuccessful. Sondheim explained to biographer Secrest that he was "what they call an institutionalized child, meaning one who has no contact with any kind of family. You're in, though it's luxurious, you're in an environment that supplies you with everything but human contact. No brothers and sisters, no parents, and yet plenty to eat, and friends to play with and a warm bed, you know?"

Sondheim detested his mother, who was said to be psychologically abusive and projected her anger from her failed marriage on her son: "When my father left her, she substituted me for him. And she used me the way she used him, to come on to and to berate, beat up on, you see. What she did for five years was treat me like dirt, but come on to me at the same time." She once wrote him a letter saying that the "only regret [she] ever had was giving him birth". When his mother died in the spring of 1992, Sondheim did not attend her funeral. He had already been estranged from her for nearly 20 years.

When Sondheim was about ten years old (around the time of his parents' divorce), he became friends with James Hammerstein, son of lyricist and playwright Oscar Hammerstein II. The elder Hammerstein became Sondheim's surrogate father, influencing him profoundly and developing his love of musical theatre. Sondheim met Hal Prince, who would direct many of his shows, at the opening of "South Pacific," Hammerstein's musical with Richard Rodgers. The comic musical he wrote at George School, "By George", was a success among his peers and buoyed the young songwriter's self-esteem. When Sondheim asked Hammerstein to evaluate it as though he had no knowledge of its author, he said it was the worst thing he had ever seen: "But if you want to know why it's terrible, I'll tell you." They spent the rest of the day going over the musical, and Sondheim later said, "In that afternoon I learned more about songwriting and the musical theater than most people learn in a lifetime."

Hammerstein designed a course of sorts for Sondheim on constructing a musical. He had the young composer write four musicals, each with one of the following conditions:

None of the "assignment" musicals were produced professionally. "High Tor" and "Mary Poppins" have never been produced: The rights holder for the original "High Tor" refused permission, and "Mary Poppins" was unfinished.

Sondheim began attending Williams College, a liberal arts college in Williamstown, Massachusetts whose theatre program attracted him. His first teacher there was Robert Barrow:

 ... everybody hated him because he was very dry, and I thought he was wonderful because he was very dry. And Barrow made me realize that all my romantic views of art were nonsense. I had always thought an angel came down and sat on your shoulder and whispered in your ear 'dah-dah-dah-DUM.' Never occurred to me that art was something worked out. And suddenly it was skies opening up. As soon as you find out what a leading tone is, you think, Oh my God. What a diatonic scale is – Oh my God! The logic of it. And, of course, what that meant to me was: Well, I can do that. Because you just don't know. You think it's a talent, you think you're born with this thing. What I've found out and what I believed is that everybody is talented. It's just that some people get it developed and some don't.

The composer told Meryle Secrest, "I just wanted to study composition, theory, and harmony without the attendant musicology that comes in graduate school. But I knew I wanted to write for the theatre, so I wanted someone who did not disdain theatre music." Barrow suggested that Sondheim study with Milton Babbitt, whom Sondheim described as "a frustrated show composer" with whom he formed "a perfect combination". When he met Babbitt, he was working on a musical for Mary Martin based on the myth of Helen of Troy. Sondheim and Babbitt would meet once a week in New York City for four hours (at the time, Babbitt was teaching at Princeton University). According to Sondheim, they spent the first hour dissecting Rodgers and Hart or George Gershwin or studying Babbitt's favorites (Buddy DeSylva, Lew Brown and Ray Henderson). They then proceeded to other forms of music (such as Mozart's Jupiter Symphony), critiquing them the same way. Babbitt and Sondheim, fascinated by mathematics, studied songs by a variety of composers (especially Jerome Kern). Sondheim told Secrest that Kern had the ability "to develop a single motif through tiny variations into a long and never boring line and his maximum development of the minimum of material". He said about Babbitt, "I am his maverick, his one student who went into the popular arts with all his serious artillery". At Williams, Sondheim wrote a musical adaption of "Beggar on Horseback" (a 1924 play by George S. Kaufman and Marc Connelly, with permission from Kaufman) which had three performances. A member of the Beta Theta Pi fraternity, he graduated "magna cum laude" in 1950.

"A few painful years of struggle" followed, when Sondheim auditioned songs, lived in his father's dining room to save money and spent time in Hollywood writing for the television series "Topper". He devoured 1940s and 1950s films, and has called cinema his "basic language"; his film knowledge got him through "The $64,000 Question" contestant tryouts. Sondheim dislikes movie musicals, favoring classic dramas such as "Citizen Kane", "The Grapes of Wrath" and "A Matter of Life and Death": "Studio directors like Michael Curtiz and Raoul Walsh ... were heroes of mine. They went from movie to movie to movie, and every third movie was good and every fifth movie was great. There wasn't any cultural pressure to make art".

At age 22, Sondheim had finished the four shows requested by Hammerstein. Julius and Philip Epstein's "Front Porch in Flatbush", unproduced at the time, was being shopped around by Lemuel (Lem) Ayers. Ayers approached Frank Loesser and another composer, who turned him down. Ayers and Sondheim met as ushers at a wedding, and Ayers commissioned Sondheim for three songs for the show; Julius Epstein flew in from California and hired Sondheim, who worked with him in California for four or five months. After eight auditions for backers, half the money needed was raised. The show, retitled "Saturday Night", was intended to open during the 1954–55 Broadway season; however, Ayers died of leukemia in his early forties. The rights transferred to his widow, Shirley, and due to her inexperience the show did not continue as planned; it opened off-Broadway in 2000. Sondheim later said, "I don't have any emotional reaction to "Saturday Night" at all – except fondness. It's not bad stuff for a 23-year-old. There are some things that embarrass me so much in the lyrics – the missed accents, the obvious jokes. But I decided, leave it. It's my baby pictures. You don't touch up a baby picture – you're a baby!"

Burt Shevelove invited Sondheim to a party; Sondheim arrived before him, and knew no one else well. He saw a familiar face: Arthur Laurents, who had seen one of the auditions of "Saturday Night", and they began talking. Laurents told him he was working on a musical version of "Romeo and Juliet" with Leonard Bernstein, but they needed a lyricist; Betty Comden and Adolph Green, who were supposed to write the lyrics, were under contract in Hollywood. He said that although he was not a big fan of Sondheim's music, he enjoyed the lyrics from "Saturday Night" and he could audition for Bernstein. The following day, Sondheim met and played for Bernstein, who said he would let him know. The composer wanted to write music and lyrics; after consulting with Hammerstein, Bernstein told Sondheim he could write music later. In 1957, "West Side Story" opened; directed by Jerome Robbins, it ran for 732 performances. Sondheim has expressed dissatisfaction with his lyrics, saying that they do not always fit the characters and are sometimes too consciously poetic. Initially Bernstein was also credited as a co-writer of the lyrics; later, however, Bernstein offered Sondheim solo credit, as Sondheim had essentially done all of them. Sondheim described the division of the royalties, saying that Bernstein received three percent and he received one percent. Bernstein suggested evening the percentage at two percent each, but Sondheim refused because he was satisfied just getting the credit. Sondheim later said he wished "someone stuffed a handkerchief in my mouth because it would have been nice to get that extra percentage".

After "West Side Story" opened, Shevelove lamented the lack of "low-brow comedy" on Broadway and mentioned a possible musical based on Plautus' Roman comedies. When Sondheim was interested in the idea he called a friend, Larry Gelbart, to co-write the script. The show went through a number of drafts, and was interrupted briefly by Sondheim's next project.

In 1959, Sondheim was approached by Laurents and Robbins for a musical version of Gypsy Rose Lee's memoir after Irving Berlin and Cole Porter turned it down. Sondheim agreed, but Ethel Merman – cast as Mama Rose – had just finished "Happy Hunting" with an unknown composer (Harold Karr) and lyricist (Matt Dubey). Although Sondheim wanted to write the music and lyrics, Merman refused to let another first-time composer write for her and demanded that Jule Styne write the music. Sondheim, concerned that writing lyrics again would pigeonhole him as a lyricist, called his mentor for advice. Hammerstein told him he should take the job, because writing a vehicle for a star would be a good learning experience. Sondheim agreed; "Gypsy" opened on May 21, 1959, and ran for 702 performances.

In 1960, Sondheim lost his mentor and father figure, Oscar Hammerstein. He remembered that shortly before Hammerstein's death, Hammerstein had given him a portrait of himself. Sondheim asked him to inscribe it, and said later about the request that it was "weird ... it's like asking your father to inscribe something". Reading the inscription ("For Stevie, My Friend and Teacher") choked up the composer, who said: "That describes Oscar better than anything I could say."

When he walked away from the house that evening, Sondheim remembered a sad, sinking feeling that they had said their final goodbye. He never saw his mentor again; three days later, Hammerstein died of stomach cancer and Hammerstein's protégé eulogized him at his funeral.

The first musical for which Sondheim wrote the music and lyrics was "A Funny Thing Happened on the Way to the Forum", which opened in 1962 and ran for 964 performances. The book, based on farces by Plautus, was written by Burt Shevelove and Larry Gelbart. Sondheim's score was not well received; although the show won several Tony Awards (including best musical), he did not receive a nomination.

Sondheim had participated in three straight hits, but his next show – 1964's "Anyone Can Whistle" – was a nine-performance bomb (although it introduced Angela Lansbury to musical theatre). "Do I Hear a Waltz?", based on Arthur Laurents' 1952 play "The Time of the Cuckoo", was intended as another Rodgers and Hammerstein musical with Mary Martin in the lead. A new lyricist was needed, and Laurents and Rodgers' daughter, Mary, asked Sondheim to fill in. Although Richard Rodgers and Sondheim agreed that the original play did not lend itself to musicalization, they began writing the musical version. The project had many problems, Rodgers' alcoholism among them; Sondheim, calling it the one project he regretted, then decided to work only when he could write both music and lyrics. He asked author and playwright James Goldman to join him as bookwriter for a new musical. Inspired by a "New York Times" article about a gathering of former Ziegfeld Follies showgirls, it was entitled "The Girl Upstairs" (and would later become "Follies").

In 1966, Sondheim semi-anonymously provided lyrics for "The Boy From...", a parody of "The Girl from Ipanema" in the off-Broadway revue "The Mad Show". The song was credited to "Esteban Ria Nido", Spanish for "Stephen River Nest", and in the show's playbill the lyrics were credited to "Nom De Plume". That year Goldman and Sondheim hit a creative wall on "The Girls Upstairs", and Goldman asked Sondheim about writing a TV musical. The result was "Evening Primrose", with Anthony Perkins and Charmian Carr. Written for the anthology series "ABC Stage 67" and produced by Hubbell Robinson, it was broadcast on November 16, 1966. According to Sondheim and director Paul Bogart, the musical was written only because Goldman needed money for rent. The network disliked the title and Sondheim's alternative, "A Little Night Music".

After Sondheim finished "Evening Primrose", Jerome Robbins asked him to adapt Bertolt Brecht's "The Measures Taken" despite the composer's general dislike of Brecht's work. Robbins wanted to adapt another Brecht play, "The Exception and the Rule", and asked John Guare to adapt the book. Leonard Bernstein had not written for the stage in some time, and his contract as conductor of the New York Philharmonic was ending. Sondheim was invited to Robbins' house in the hope that Guare would convince him to write the lyrics for a musical version of "The Exception and the Rule"; according to Robbins, Bernstein would not work without Sondheim. When Sondheim agreed, Guare asked: "Why haven't you all worked together since "West Side Story"?" Sondheim answered, "You'll see". Guare said that working with Sondheim was like being with an old college roommate, and he depended on him to "decode and decipher their crazy way of working"; Bernstein worked only after midnight, and Robbins only in the early morning. Bernstein's score, which was supposed to be light, was influenced by his need to make a musical statement. Stuart Ostrow, who worked with Sondheim on "The Girls Upstairs", agreed to produce the musical (now entitled "A Pray By Blecht" and, later, "The Race to Urga"). An opening night was scheduled, but during auditions Robbins asked to be excused for a moment. When he did not return, a doorman said he had gotten into a limousine to go to John F. Kennedy International Airport. Bernstein burst into tears and said, "It's over"; Sondheim said, "I was ashamed of the whole project. It was arch and didactic in the worst way." He wrote one-and-a-half songs and threw them away, the only time he has ever done that. Eighteen years later, Sondheim refused Bernstein and Robbins' request to retry the show.
He has lived in a Turtle Bay, Manhattan brownstone since writing "Gypsy" in 1959. Ten years later, while he was playing music he heard a knock on the door. His neighbor, Katharine Hepburn, was in "bare feet – this angry, red-faced lady" and told him "You have been keeping me awake all night!" (she was practicing for her musical debut in "Coco"). When Sondheim asked why she had not asked him to play for her, she said she lost his phone number. According to Sondheim, "My guess is that she wanted to stand there in her bare feet, suffering for her art".

After "Do I Hear a Waltz?", Sondheim devoted himself solely to writing both music and lyrics for the theater - and in 1970, he began a collaboration with director Harold Prince that would result in a body of work that is considered one of the high water marks of musical theater history. 

Their first show with Prince as director was the 1970 concept musical "Company". A show about a single man and his married friends, "Company" (with a book by George Furth) lacked a straightforward plot, and was instead centered around themes such as marriage and the difficulty of making an emotional connection with another person. It opened on April 26, 1970 at the Alvin Theatre, where it ran for 705 performances after seven previews, and won Tony Awards for best musical, best music and best lyrics. It was revived on Broadway in 1995 and 2006, and will be revived again in 2020 (in a version where the title character is gender-swapped). 

"Follies" (1971), with a book by James Goldman, opened on April 4, 1971 at the Winter Garden Theatre and ran for 522 performances after 12 previews. The plot centers on a reunion, in a crumbling Broadway theatre scheduled for demolition, of performers in "Weismann's" "Follies" (a musical revue, based on the "Ziegfeld Follies", which played in that theatre between the world wars). The production, one of the most lavish of its time, also featured choreography and co-direction by Michael Bennett"," who went on to create "A Chorus Line" (1975). The show enjoyed two revivals on Broadway in 2001 and 2011.

"A Little Night Music" (1973), with a more traditional plot based on Ingmar Bergman's "Smiles of a Summer Night" and a score primarily in waltz time, was one of the composer's greatest commercial successes. "Time" magazine called it "Sondheim's most brilliant accomplishment to date". "Send in the Clowns", a song from the musical, was a hit for Judy Collins, and became Sondheim's most well-known song. The show opened on Broadway at the Shubert Theatre on February 25, 1973, and ran for 601 performances and 12 previews. It was revived on Broadway in 2009.

"Pacific Overtures" (1976), with a book by John Weidman, was the most non-traditional of the Sondheim-Prince collaborations: the show explored the westernization of Japan, and was originally presented in Kabuki style. The show closed after a run of 193 performances, and was revived on Broadway in 2004.

"" (1979), Sondheim's most operatic score and libretto (which, with "Pacific Overtures" and "A Little Night Music", has been produced in opera houses), explores an unlikely topic: murderous revenge and cannibalism. The book, by Hugh Wheeler, is based on Christopher Bond's 1973 stage version of the Victorian original. The show has since been revived on Broadway twice (1989, 2005), and has been performed in musical theaters and opera houses alike. It ran off-Broadway at the Barrow Street Theatre until August 26, 2018.

"Merrily We Roll Along" (1981), with a book by George Furth, is one of Sondheim's more traditional scores; Frank Sinatra and Carly Simon have recorded songs from the musical. According to Sondheim's music director, Paul Gemignani, "Part of Steve's ability is this extraordinary versatility." However, the show was not the success their previous collaborations had been: after a chaotic series of preview performances, the show opened to widely negative reviews, and closed after a run of less than two weeks. Due to the high quality of Sondheim's score, however, the show has been repeatedly revised and produced in the ensuing years. Martin Gottfried wrote, "Sondheim had set out to write traditional songs ... But [despite] that there is nothing ordinary about the music." Sondheim later said: "Did I feel betrayed? I'm not sure I would put it like that. What did surprise me was the feeling around the Broadway community – if you can call it that, though I guess I will for lack of a better word – that they wanted Hal and me to fail."

"Merrily"s failure greatly affected Sondheim; he was ready to quit theatre and do movies, create video games or write mysteries: "I wanted to find something to satisfy myself that does not involve Broadway and dealing with all those people who hate me and hate Hal." Sondheim and Prince's collaboration was suspended from "Merrily" to the 2003 production of "Bounce", another failure.

However, Sondheim decided "that there are better places to start a show" and found a new collaborator in James Lapine after he saw Lapine's "Twelve Dreams" off-Broadway in 1981: "I was discouraged, and I don't know what would have happened if I hadn't discovered "Twelve Dreams" at the Public Theatre"; Lapine has a taste "for the avant-garde and for visually-oriented theatre in particular". Their first collaboration was "Sunday in the Park with George" (1984), with Sondheim's music evoking Georges Seurat's pointillism. Sondheim and Lapine won the 1985 Pulitzer Prize for Drama for the play, and it was revived on Broadway in 2008, and again in a limited run in 2017.

They collaborated on "Into the Woods" (1987), a musical based on several Brothers Grimm fairy tales. Although Sondheim has been called the first composer to bring rap music to Broadway (with the Witch in the opening number of "Into the Woods"), he attributes the first rap in theatre to Meredith Willson's "Rock Island" from "The Music Man". The show was revived on Broadway in 2002.

Sondheim and Lapine's last work together was the rhapsodic "Passion" (1994), adapted from Ettore Scola's Italian film "Passione D'Amore". With a run of 280 performances, "Passion" was the shortest-running show to win a Tony Award for Best Musical.

"Assassins" opened off-Broadway at Playwrights Horizons on December 18, 1990, with a book by John Weidman. The show explored, in revue form, a group of historical figures who tried (either with success or without) to assassinate the President of the United States. The musical closed on February 16, 1991, after 73 performances. The show eventually received a Broadway production in 2004.

"Saturday Night" was shelved until its 1997 production at London's Bridewell Theatre. The following year, its score was recorded; a revised version, with two new songs, ran off-Broadway at Second Stage Theatre in 2000 and at London's Jermyn Street Theatre in 2009.

During the late 1990s, Sondheim and Weidman reunited for "Wise Guys", a musical comedy following brothers Addison and Wilson Mizner. A Broadway production, starring Nathan Lane and Victor Garber, directed by Sam Mendes and planned for the spring of 2000, was delayed. Renamed "Bounce" in 2003, it was produced at the Goodman Theatre in Chicago and the Kennedy Center in Washington, D.C., in a production directed by Harold Prince, his first collaboration with Sondheim since 1981. Although after poor reviews "Bounce" never reached Broadway, a revised version opened off-Broadway as "Road Show" at the Public Theater on October 28, 2008. Directed by John Doyle, it closed on December 28, 2008.

Asked about writing new work, Sondheim replied in 2006: "No ... It's age. It's a diminution of energy and the worry that there are no new ideas. It's also an increasing lack of confidence. I'm not the only one. I've checked with other people. People expect more of you and you're aware of it and you shouldn't be." In December 2007 he said that in addition to continuing work on "Bounce", he was "nibbling at a couple of things with John Weidman and James Lapine".

Lapine created a multimedia production, originally entitled "Sondheim: a Musical Revue", which was scheduled to open in April 2009 at the Alliance Theatre in Atlanta; however, it was canceled due to "difficulties encountered by the commercial producers attached to the project ... in raising the necessary funds". A revised version, "Sondheim on Sondheim", was produced at Studio 54 by the Roundabout Theatre Company; previews began on March 19, 2010, and it ran from April 22 to June 13. The revue's cast included Barbara Cook, Vanessa L. Williams, Tom Wopat, Norm Lewis and Leslie Kritzer.

Sondheim collaborated with Wynton Marsalis on "A Bed and a Chair: A New York Love Affair", an Encores! concert on November 13–17, 2013 at New York City Center. Directed by John Doyle with choreography by Parker Esse, it consisted of "more than two dozen Sondheim compositions, each piece newly re-imagined by Marsalis". The concert featured Bernadette Peters, Jeremy Jordan, Norm Lewis, Cyrille Aimée, four dancers and the Jazz at Lincoln Center Orchestra conducted by David Loud. In "Playbill", Steven Suskin described the concert as "neither a new musical, a revival, nor a standard songbook revue; it is, rather, a staged-and-sung chamber jazz rendition of a string of songs ... Half of the songs come from "Company" and "Follies"; most of the other Sondheim musicals are represented, including the lesser-known "Passion" and "Road Show"".

For the 2014 film adaptation of "Into the Woods", Sondheim wrote a new song, "She'll Be Back", which was to be sung by The Witch, but was eventually cut. 

In February 2012 it was announced that Sondheim would collaborate on a new musical with David Ives, and he had "about 20–30 minutes of the musical completed". The show, tentatively called "All Together Now", was assumed to follow the format of "Merrily We Roll Along". Sondheim described the project as "two people and what goes into their relationship ... We'll write for a couple of months, then have a workshop. It seemed experimental and fresh 20 years ago. I have a feeling it may not be experimental and fresh any more". On October 11, 2014, it was confirmed the Sondheim and Ives musical would be based on two Luis Buñuel films ("The Exterminating Angel" and "The Discreet Charm of the Bourgeoisie") and would reportedly open (in previews) at the Public Theater in 2017.

In August 2016, a reading for the musical was held at the Public Theater, and it was reported that only the first act was finished, which cast doubt on the speculated 2017 start of previews. There was a workshop in November 2016, with the participation of Matthew Morrison, Shuler Hensley, Heidi Blickenstaff, Sierra Boggess, Gabriel Ebert, Sara Stiles, Michael Cerveris and Jennifer Simard. The working title was reported to be "Buñuel" by the New York Post and other outlets, but Sondheim later clarified that this was an error and that they still had no title. As of April 2019, a date for a musical titled "Buñuel" (by Sondheim and David Ives; directed by Joe Mantello) has been announced on the New York City Theatre website, beginning August 24, 2019. However, in June 2019, the Public Theatre announced that it would not be part of its 2019-2020 season, as it was still in development, but will be produced "when it is ready".

The Kennedy Center held a Sondheim Celebration, running from May to August 2002, consisting of six of Sondheim's musicals: "Sweeney Todd", "Company", "Sunday in the Park With George", "Merrily We Roll Along", "Passion" and "A Little Night Music". On April 28, 2002, in connection with the Sondheim Celebration Sondheim and Frank Rich of "the New York Times" had a conversation. They appeared in four interviews, entitled "A Little Night Conversation with Stephen Sondheim", in California and Portland, Oregon in March 2008 and at Oberlin College in September. The "Cleveland Jewish News" reported on their Oberlin appearance: "Sondheim said: 'Movies are photographs; the stage is larger than life.' What musicals does Sondheim admire the most? "Porgy and Bess" tops a list which includes "Carousel", "She Loves Me", and "The Wiz", which he saw six times. Sondheim took a dim view of today's musicals. What works now, he said, are musicals that are easy to take; audiences don't want to be challenged". Sondheim and Rich had additional conversations on January 18, 2009 at Avery Fisher Hall, on February 2 at the Landmark Theatre in Richmond, Virginia, on February 21 at the Kimmel Center in Philadelphia and on April 20 at the University of Akron in Akron, Ohio. The conversations were reprised at Tufts and Brown University in February 2010, at the University of Tulsa in April and at Lafayette College on March 8, 2011. Sondheim had another "conversation with" Sean Patrick Flahaven (associate editor of "The Sondheim Review") at the Kravis Center in West Palm Beach on February 4, 2009, in which he discussed many of his songs and shows: "On the perennial struggles of Broadway: 'I don't see any solution for Broadway's problems except subsidized theatre, as in most civilized countries of the world.'"

On February 1, 2011, Sondheim joined former "Salt Lake Tribune" theatre critic Nancy Melich before an audience of 1,200 at Kingsbury Hall. Melich described the evening:

He was visibly taken by the university choir, who sang two songs during the evening, "Children Will Listen" and "Sunday", and then returned to reprise "Sunday". During that final moment, Sondheim and I were standing, facing the choir of students from the University of Utah's opera program, our backs to the audience, and I could see tears welling in his eyes as the voices rang out. Then, all of a sudden, he raised his arms and began conducting, urging the student singers to go full out, which they did, the crescendo building, their eyes locked with his, until the final "on an ordinary Sunday" was sung. It was thrilling, and a perfect conclusion to a remarkable evening – nothing ordinary about it.

On March 13, 2008, "A Salon With Stephen Sondheim" (which sold out in three minutes) was hosted by the Academy for New Musical Theatre in Hollywood.

An avid fan of games, in 1968 and 1969 Sondheim published a series of cryptic crossword puzzles in "New York" magazine. In 1987 "Time" called his love of puzzlemaking "legendary in theater circles", adding that the central character of Anthony Shaffer's play "Sleuth" was inspired by the composer. According to a rumor (denied by Shaffer in a March 10, 1996 "New York Times" interview), "Sleuth" had the working title "Who's Afraid of Stephen Sondheim?" His love of puzzles and mysteries is evident in "The Last of Sheila", an intricate whodunit written with longtime friend Anthony Perkins. The 1973 film, directed by Herbert Ross, featured Dyan Cannon, Raquel Welch, James Mason, James Coburn and Richard Benjamin.

Sondheim tried playwriting one more time, collaborating with "Company" librettist George Furth on "Getting Away with Murder" in 1996, but the unsuccessful Broadway production closed after 29 previews and 17 performances. His compositions have included a number of film scores, including a set of songs written for Warren Beatty's 1990 film version of "Dick Tracy". One of Sondheim's songs for the film, "Sooner or Later (I Always Get My Man)", sung in the movie by Madonna, won him an Academy Award.

According to Sondheim, he was asked to translate "Mahagonny-Songspiel": "But, I'm not a Brecht/Weill fan and that's really all there is to it. I'm an apostate: I like Weill's music when he came to America better than I do his stuff before ... I love "The Threepenny Opera" but, outside of "The Threepenny Opera", the music of his I like is the stuff he wrote in America – when he was not writing with Brecht, when he was writing for Broadway." He turned down an offer to musicalize Nathanael West's "A Cool Million" with James Lapine around 1982.

Sondheim worked with William Goldman on "Singing Out Loud", a musical film, in 1992, penning the song "Water Under the Bridge". According to the composer, Goldman wrote one or two drafts of the script and Sondheim wrote six-and-a-half songs when director Rob Reiner lost interest in the project. "Dawn" and "Sand", from the film, were recorded for the albums "Sondheim at the Movies" and "Unsung Sondheim". Sondheim and Leonard Bernstein wrote "The Race to Urga", scheduled for Lincoln Center in 1969, but when Jerome Robbins left the project it was not produced.

In 1991 Sondheim worked with Terrence McNally on a musical, "All Together Now". McNally said, "Steve was interested in telling the story of a relationship from the present back to the moment when the couple first met. We worked together a while, but we were both involved with so many other projects that this one fell through". The story follows Arden Scott, a 30-something female sculptor, and Daniel Nevin (a slightly-younger, sexually attractive restaurateur). Its script, with concept notes by McNally and Sondheim, is archived in the Harry Ransom Center at the University of Texas at Austin.

In August 2003, Sondheim expressed interest in the idea of a creating a musical adaption of the 1993 comedy film "Groundhog Day". However, in a 2008 live chat, he said that "to make a musical of "Groundhog Day" would be to gild the lily. It cannot be improved." The musical was later created and premiered in 2016 with music and lyrics by Tim Minchin and book by Danny Rubin (screenwriter of the film) with Sondheim's blessing.

Sondheim's 2010 "Finishing the Hat" annotates his lyrics "from productions dating 1954–1981. In addition to published and unpublished lyrics from "West Side Story", "Follies" and "Company", the tome finds Sondheim discussing his relationship with Oscar Hammerstein II and his collaborations with composers, actors and directors throughout his lengthy career". The book, first of a two-part series, is named after a song from "Sunday in the Park With George". Sondheim said, "It's going to be long. I'm not, by nature, a prose writer, but I'm literate, and I have a couple of people who are vetting it for me, whom I trust, who are excellent prose writers". "Finishing the Hat" was published in October 2010. According to a "New York Times" review, "The lyrics under consideration here, written during a 27-year period, aren't presented as fixed and sacred paradigms, carefully removed from tissue paper for our reverent inspection. They're living, evolving, flawed organisms, still being shaped and poked and talked to by the man who created them". The book was 11th on the "New York Times" Hardcover Nonfiction list for November 5, 2010.

Its sequel, "Look, I Made a Hat: Collected Lyrics (1981–2011) with Attendant Comments, Amplifications, Dogmas, Harangues, Digressions, Anecdotes and Miscellany", was published on November 22, 2011. The book, continuing from "Sunday in the Park With George" (where "Finishing the Hat" ended), includes sections on Sondheim's work in film and television.

After he was mentored by Oscar Hammerstein II Sondheim has returned the favor, saying that he loves "passing on what Oscar passed on to me". In an interview with Sondheim for "The Legacy Project", composer-lyricist Adam Guettel (son of Mary Rodgers and grandson of Richard Rodgers) recalls how as a 14-year-old boy he showed Sondheim his work. Guettel was "crestfallen" since he had come in "sort of all puffed up thinking [he] would be rained with compliments and things", which was not the case since Sondheim had some "very direct things to say". Later, Sondheim wrote and apologized to Guettel for being "not very encouraging" when he was actually trying to be "constructive".

Sondheim also mentored a fledgling Jonathan Larson, attending Larson's workshop for his "Superbia" (a musical version of "Nineteen Eighty-Four"). In Larson's musical "Tick, Tick... Boom!", the phone message is played in which Sondheim apologizes for leaving early, says he wants to meet him and is impressed with his work. After Larson's death, Sondheim called him one of the few composers "attempting to blend contemporary pop music with theater music, which doesn't work very well; he was on his way to finding a real synthesis. A good deal of pop music has interesting lyrics, but they are not theater lyrics". A musical-theatre composer "must have a sense of what is theatrical, of how you use music to tell a story, as opposed to writing a song. Jonathan understood that instinctively."

Around 2008, Sondheim approached Lin-Manuel Miranda to work with him translating "West Side Story" lyrics into Spanish for an upcoming Broadway revival. Miranda then approached Sondheim with his new project "Hamilton", then called "The Hamilton Mixtape", which Sondheim gave notes on. Sondheim was originally wary of the project saying he was "worried that an evening of rap might get monotonous". However, Sondheim believed Miranda's attention to, and respect for, good rhyming made it work.

A supporter for writers' rights in the theatre industry, Stephen Sondheim is an active member of the Dramatists Guild of America. In 1973, he was elected as the Guild's sixteenth president, and he continued his presidency for the non-profit organization until 1981.

Unless otherwise noted, music and lyrics are by Stephen Sondheim.

"Side By Side By Sondheim" (1976), "Marry Me A Little" (1980), "Putting It Together" (1993) and "Sondheim on Sondheim" (2010): Anthologies or revues of Sondheim's work as composer and lyricist, with songs performed or cut from productions. "Jerome Robbins' Broadway" features "You Gotta Have a Gimmick" from "Gypsy", "Suite of Dances" from "West Side Story" and "Comedy Tonight" from "A Funny Thing Happened on the Way to the Forum". A new revue, "Secret Sondheim ... a celebration of his lesser known work", conceived and directed by Tim McArthur, was produced at the Jermyn Street Theatre in July 2010. Sondheim's "Pretty Women" and "Everybody Ought to Have a Maid" are featured in "The Madwoman of Central Park West".










In November 2015, Sondheim was awarded the Presidential Medal of Freedom by President Barack Obama in a ceremony at the White House.

Several benefits and concerts were performed to celebrate Sondheim's 80th birthday in 2010. Among them were the New York Philharmonic's March 15 and 16 "Sondheim: The Birthday Concert" at Lincoln Center's Avery Fisher Hall, hosted by David Hyde Pierce. The concert included Sondheim's music, performed by some of the original performers. Lonny Price directed, and Paul Gemignani conducted; performers included Laura Benanti, Matt Cavenaugh, Michael Cerveris, Victoria Clark, Jenn Colella, Jason Danieley, Alexander Gemignani, Joanna Gleason, Nathan Gunn, George Hearn, Patti LuPone, Marin Mazzie, Audra McDonald, John McMartin, Donna Murphy, Karen Olivo, Laura Osnes, Mandy Patinkin, Bernadette Peters, Bobby Steggert, Elaine Stritch, Jim Walton, Chip Zien and the 2009 Broadway revival cast of "West Side Story". A ballet was performed by Blaine Hoven and María Noel Riccetto to Sondheim's score for "Reds", and Jonathan Tunick paid tribute to his longtime collaborator. The concert was broadcast on PBS' "Great Performances" show in November, and its DVD was released on November 16.

"Sondheim 80", a Roundabout Theatre Company benefit, was held on March 22. The evening included a performance of "Sondheim on Sondheim", dinner and a show at the New York Sheraton. "A very personal star-studded musical tribute" featured new songs by contemporary musical-theatre writers. The composers (who sang their own songs) included Tom Kitt and Brian Yorkey, Michael John LaChiusa, Andrew Lippa, Robert Lopez and Kristen Anderson-Lopez, Lin-Manuel Miranda (accompanied by Rita Moreno), Duncan Sheik, and Jeanine Tesori and David Lindsay-Abaire. Bernadette Peters performed a song which had been cut from a Sondheim show.

An April 26 New York City Center birthday celebration and concert to benefit Young Playwrights, among others, featured (in order of appearance) Michael Cerveris, Alexander Gemignani, Donna Murphy, Debra Monk, Joanna Gleason, Maria Friedman, Mark Jacoby, Len Cariou, BD Wong, Claybourne Elder, Alexander Hanson, Catherine Zeta-Jones, Raúl Esparza, Sutton Foster, Nathan Lane, Michele Pawk, the original cast of "Into the Woods", Kim Crosby, Chip Zien, Danielle Ferland and Ben Wright, Angela Lansbury and Jim Walton. The concert, directed by John Doyle, was co-hosted by Mia Farrow; greetings from Sheila Hancock, Julia McKenzie, Milton Babbitt, Judi Dench and Glynis Johns were read. After Catherine Zeta-Jones performed "Send in the Clowns", Julie Andrews sang part of "Not a Day Goes By" in a recorded greeting. Although Patti LuPone, Barbara Cook, Bernadette Peters, Tom Aldredge and Victor Garber were originally scheduled to perform, they did not appear.

A July 31 BBC Proms concert celebrated Sondheim's 80th birthday at the Royal Albert Hall. The concert featured songs from many of his musicals, including "Send in the Clowns" sung by Judi Dench (reprising her role as Desirée in the 1995 production of "A Little Night Music"), and performances by Bryn Terfel and Maria Friedman.

On November 19 the New York Pops, led by Steven Reineke, performed at Carnegie Hall for the composer's 80th birthday. Kate Baldwin, Aaron Lazar, Christiane Noll, Paul Betz, Renee Rakelle, Marilyn Maye (singing "I'm Still Here"), and Alexander Gemignani appeared, and songs included "I Remember", "Another Hundred People", "Children Will Listen" and "Getting Married Today". Sondheim took the stage during an encore of his song, "Old Friends".

Sondheim founded Young Playwrights Inc. in 1981 to introduce young people to writing for the theatre, and is the organization's executive vice-president. The Stephen Sondheim Center for the Performing Arts, at the Fairfield Arts and Convention Center in Fairfield, Iowa, opened in December 2007 with performances by Len Cariou, Liz Callaway, and Richard Kind (all of whom had participated in Sondheim musicals).

The Stephen Sondheim Society was established in 1993 to provide information about his work, with its "Sondheim - the Magazine" provided to its membership. The society maintains a database, organizes productions, meetings, outings and other events and assists with publicity. Its annual Student Performer of the Year Competition awards a £1,000 prize to one of twelve musical-theatre students from UK drama schools and universities. At Sondheim's request, an additional prize is offered for a new song by a young composer. Judged by George Stiles and Anthony Drewe, each contestant performs a Sondheim song and a new song.

Most episode titles of the television series "Desperate Housewives" refer to Sondheim's song titles or lyrics, and the series finale is entitled "Finishing the Hat". In 1990 Sondheim, as the Cameron Mackintosh chair in musical theatre at Oxford, conducted workshops with promising musical writers including George Stiles, Anthony Drewe, Andrew Peggie, Paul James, Kit Hesketh-Harvey and Stephen Keeling. The writers founded the Mercury Workshop in 1992, which merged with the New Musicals Alliance to become MMD (a UK-based organization to develop new musical theatre, of which Sondheim is a patron).

Signature Theatre in Arlington, Virginia established its Sondheim Award, which includes a $5,000 donation to a nonprofit organization of the recipient's choice, "as a tribute to America's most influential contemporary musical theatre composer". The first award, to Sondheim, was presented at an April 27, 2009 benefit with performances by Bernadette Peters, Michael Cerveris, Will Gartshore and Eleasha Gamble. The 2010 recipient was Angela Lansbury, with Peters and Catherine Zeta-Jones hosting the April benefit. The 2011 honoree was Bernadette Peters. Other recipients were Patti LuPone in 2012, Hal Prince in 2013, Jonathan Tunick in 2014, and James Lapine in 2015. The 2016 awardee was John Weidman and the 2017 awardee was Cameron Mackintosh.

Henry Miller's Theatre, on West 43rd Street in New York City, was renamed the Stephen Sondheim Theatre on September 15, 2010 for the composer's 80th birthday. In attendance were Nathan Lane, Patti LuPone and John Weidman. Sondheim said in response to the honor, "I'm deeply embarrassed. Thrilled, but deeply embarrassed. I've always hated my last name. It just doesn't sing. I mean, it's not Belasco. And it's not Rodgers and it's not Simon. And it's not Wilson. It just doesn't sing. It sings better than Schoenfeld and Jacobs. But it just doesn't sing". Lane said, "We love our corporate sponsors and we love their money, but there's something sacred about naming a theatre, and there's something about this that is right and just".

According to "The Daily Telegraph", Sondheim is "almost certainly" the only living composer with a quarterly journal published in his name; "The Sondheim Review", founded in 1994, chronicles and promotes his work.

According to Sondheim, when he asked Milton Babbitt if he could study atonality, Babbitt replied: "You haven't exhausted tonal resources for yourself yet, so I'm not going to teach you atonal". Sondheim agreed, and despite frequent dissonance and a highly-chromatic style, his music is tonal.

He is noted for complex polyphony in his vocals, such as the five minor characters who make up a Greek chorus in 1973's "A Little Night Music". Sondheim uses angular harmonies and intricate melodies. His musical influences are varied; although he has said that he "loves Bach", his favorite musical period is from Brahms to Stravinsky.

Sondheim has been described as introverted and solitary. In an interview with Frank Rich, he said, "The outsider feeling—somebody who people want to both kiss and kill—occurred quite early in my life". He lived with dramatist Peter Jones for eight years in the 1990s. As of 2010, the composer was in a relationship with Jeffrey Scott Romley. They were married on December 31, 2017.




 


</doc>
<doc id="29269" url="https://en.wikipedia.org/wiki?curid=29269" title="Self-determination">
Self-determination

The right of a people to self-determination is a cardinal principle in modern international law (commonly regarded as a "jus cogens" rule), binding, as such, on the United Nations as authoritative interpretation of the Charter's norms. It states that people, based on respect for the principle of equal rights and fair equality of opportunity, have the right to freely choose their sovereignty and international political status with no interference.

The concept was first expressed in the 1860s, and spread rapidly thereafter. During and after World War I, the principle was encouraged by both Vladimir Lenin and United States President Woodrow Wilson. Having announced his Fourteen Points on 8 January 1918, on 11 February 1918 Wilson stated: "National aspirations must be respected; people may now be dominated and governed only by their own consent. 'Self determination' is not a mere phrase; it is an imperative principle of action."

During World War II, the principle was included in the Atlantic Charter, signed on 14 August 1941, by Franklin D. Roosevelt, President of the United States, and Winston Churchill, Prime Minister of the United Kingdom, who pledged The Eight Principal points of the Charter. It was recognized as an international legal right after it was explicitly listed as a right in the UN Charter.

The principle does not state how the decision is to be made, nor what the outcome should be, whether it be independence, federation, protection, some form of autonomy or full assimilation. Neither does it state what the delimitation between peoples should be—nor what constitutes a people. There are conflicting definitions and legal criteria for determining which groups may legitimately claim the right to self-determination.

By extension, the term self-determination has come to mean the free choice of one's own acts without external compulsion.

The employment of imperialism, through the expansion of empires, and the concept of political sovereignty, as developed after the Treaty of Westphalia, also explain the emergence of self-determination during the modern era. During, and after, the Industrial Revolution many groups of people recognized their shared history, geography, language, and customs. Nationalism emerged as a uniting ideology not only between competing powers, but also for groups that felt subordinated or disenfranchised inside larger states; in this situation, self-determination can be seen as a reaction to imperialism. Such groups often pursued independence and sovereignty over territory, but sometimes a different sense of autonomy has been pursued or achieved.

The world possessed several traditional, continental empires such as the Ottoman, Russian, Austrian/Habsburg, and the Qing Empire. Political scientists often define competition in Europe during the Modern Era as a balance of power struggle, which also induced various European states to pursue colonial empires, beginning with the Spanish and Portuguese, and later including the British, French, Dutch, and German.
During the early 19th century, competition in Europe produced multiple wars, most notably the Napoleonic Wars. After this conflict, the British Empire became dominant and entered its "imperial century", while nationalism became a powerful political ideology in Europe.

Later, after the Franco-Prussian War in 1870, "New Imperialism" was unleashed with France and later Germany establishing colonies in Asia, the Pacific, and Africa. Japan also emerged as a new power. Multiple theaters of competition developed across the world:


The Ottoman Empire, Austrian Empire, Russian Empire, Qing Empire and the new Empire of Japan maintained themselves, often expanding or contracting at the expense of another empire. All ignored notions of self-determination for those governed.

The revolt of New World British colonists in North America, during the mid-1770s, has been seen as the first assertion of the right of national and democratic self-determination, because of the explicit invocation of natural law, the natural rights of man, as well as the consent of, and sovereignty by, the people governed; these ideas were inspired particularly by John Locke's enlightened writings of the previous century. Thomas Jefferson further promoted the notion that the will of the people was supreme, especially through authorship of the United States Declaration of Independence which inspired Europeans throughout the 19th century. The French Revolution was motivated similarly and legitimatized the ideas of self-determination on that Old World continent.

Within the New World during the early 19th century, most of the nations of Spanish America achieved independence from Spain. The United States supported that status, as policy in the hemisphere relative to European colonialism, with the Monroe Doctrine. The American public, organized associated groups, and Congressional resolutions, often supported such movements, particularly the Greek War of Independence (1821–29) and the demands of Hungarian revolutionaries in 1848. Such support, however, never became official government policy, due to balancing of other national interests. After the American Civil War and with increasing capability, the United States government did not accept self-determination as a basis during its Purchase of Alaska and attempted purchase of the West Indian islands of Saint Thomas and Saint John in the 1860s, or its growing influence in the Hawaiian Islands, that led to annexation in 1898. With its victory in the Spanish–American War in 1899 and its growing stature in the world, the United States supported annexation of the former Spanish colonies of Guam, Puerto Rico and the Philippines, without the consent of their peoples, and it retained "quasi-suzerainty" over Cuba, as well.

Nationalist sentiments emerged inside the traditional empires including: Pan-Slavism in Russia; Ottomanism, Kemalist ideology and Arab nationalism in the Ottoman Empire; State Shintoism and Japanese identity in Japan; and Han identity in juxtaposition to the Manchurian ruling class in China. Meanwhile, in Europe itself there was a rise of nationalism, with nations such as Greece, Hungary, Poland and Bulgaria seeking or winning their independence.

Karl Marx supported such nationalism, believing it might be a "prior condition" to social reform and international alliances. In 1914 Vladimir Lenin wrote: "[It] would be wrong to interpret the right to self-determination as meaning anything but the right to existence as a separate state."

Woodrow Wilson revived America's commitment to self-determination, at least for European states, during World War I. When the Bolsheviks came to power in Russia in November 1917, they called for Russia's immediate withdrawal as a member of the Allies of World War I. They also supported the right of all nations, including colonies, to self-determination." The 1918 Constitution of the Soviet Union acknowledged the right of secession for its constituent republics.

This presented a challenge to Wilson's more limited demands. In January 1918 Wilson issued his Fourteen Points of January 1918 which, among other things, called for adjustment of colonial claims, insofar as the interests of colonial powers had equal weight with the claims of subject peoples. The Treaty of Brest-Litovsk in March 1918 led to Soviet Russia's exit from the war and the nominal independence of Armenia, Finland, Estonia, Latvia, Ukraine, Lithuania, Georgia and Poland, though in fact those territories were under German control. The end of the war led to the dissolution of the defeated Austro-Hungarian Empire and Czechoslovakia and the union of the State of Slovenes, Croats and Serbs and the Kingdom of Serbia as new states out of the wreckage of the Habsburg empire. However, this imposition of states where some nationalities (especially Poles, Czechs, and Serbs and Romanians) were given power over nationalities who disliked and distrusted them eventually used as a pretext for German aggression in World War II.

One of the German objections to the Treaty of Versailles was a somewhat selective application of the principle of self-determination as the majority of the people in Austria and in the Sudetenland region of Czechoslovakia wanted to join Germany while the majority of people in Danzig wanted to remain within the "Reich", but the Allies ignored the German objections. Wilson's 14 Points had called for Polish independence to be restored and Poland to have "secure access to the sea", which would imply that the German city of Danzig (modern Gdańsk, Poland), which occupied a strategic location where the Vistula river flowed into the Baltic sea, be ceded to Poland. At the Paris peace conference in 1919, the Polish delegation led by Roman Dmowski asked for Wilson to honor point 14 of the 14 points by transferring Danzig to Poland. arguing that Poland would not be economically viable without Danzig. However, as the 90% of the people in Danzig in this period were German, the Allied leaders at the Paris peace conference compromised by creating the Free City of Danzig, a city-state in which Poland had certain special rights. Through the city of Danzig was 90% German and 10% Polish, the surrounding countryside around Danzig was overwhelmingly Polish, and the ethnically Polish rural areas included in the Free City of Danzig objected, arguing that they wanted to be part of Poland. Neither the Poles nor the Germans were happy with this compromise and the Danzig issue became a flash-point of German-Polish tension throughout the interwar period.

Germany lost land after WWI: Northern Schleswig voted to return to Denmark after a referendum. On 11 July 1920, the East Prussian plebiscite called for by the Treaty of Versailles led to two disputed regions between Germany and Poland choosing the former. In 1921, a plebiscite in Silesia concerning partitioning the region between Germany and Poland led to fighting breaking out between the ethnic German and ethnic Polish residents of Silesia. The defeated Ottoman empire was dissolved into the Republic of Turkey and several smaller nations, including Yemen, plus the new Middle East Allied "mandates" of Syria and Lebanon (future Syria, Lebanon and Hatay State), Palestine (future Transjordan and Israel), Mesopotamia (future Iraq). In 1919, a Greek attempt to add the mostly Greek-speaking western regions of Anatolia led to a war between Greece and Turkey when the Greeks occupied the largely Greek-speaking city of Smyrna (modern İzmir, Turkey) in May 1919. In 1922, the Greeks were defeated and under the terms of the 1923 Treaty of Lausanne compulsorily population exchanges led to almost all the Turks in Greece being expelled into Turkey and almost all of the Greeks in Turkey being expelled into Greece. The League of Nations was proposed as much as a means of consolidating these new states, as a path to peace.

During the 1920s and 1930s there were some successful movements for self-determination in the beginnings of the process of decolonization. In the Statute of Westminster the United Kingdom granted independence to Canada, New Zealand, Newfoundland, the Irish Free State, the Commonwealth of Australia, and the Union of South Africa after the British parliament declared itself as incapable of passing laws over them without their consent. Egypt, Afghanistan and Iraq also achieved independence from Britain and Lebanon from France. Other efforts were unsuccessful, like the Indian independence movement. And Italy, Japan and Germany all initiated new efforts to bring certain territories under their control, leading to World War II. In particular, the National Socialist Program invoked this right of nations in its first point (out of 25), as it was publicly proclaimed on 24 February 1920 by Adolf Hitler.

In Asia, Japan became a rising power and gained more respect from Western powers after its victory in the Russo-Japanese War. Japan joined the Allied Powers in World War I and attacked German colonial possessions in the Far East, adding former German possessions to its own empire. In the 1930s, Japan gained significant influence in Inner Mongolia and Manchuria after it invaded Manchuria. It established Manchukuo, a puppet state in Manchuria and eastern Inner Mongolia. This was essentially the model Japan followed as it invaded other areas in Asia and established the Greater East Asia Co-Prosperity Sphere. Japan went to considerable trouble to argue that Manchukuo was justified by the principle of self-determination, claiming that people of Manchuria wanted to break away from China and asked the Kwantung Army to intervene on their behalf. However, the Lytton commission which had been appointed by the League of Nations to decide if Japan had committed aggression or not, stated the majority of people in Manchuria who were Han Chinese who did not wish to leave China.

In 1912, the Republic of China officially succeeded the Qing Dynasty, while Outer Mongolia, Tibet and Tuva proclaimed their independence. Independence was not accepted by the government of China. By the Treaty of Kyakhta (1915) Outer Mongolia recognized China's sovereignty. However, the Soviet threat of seizing parts of Inner Mongolia induced China to recognize Outer Mongolia's independence, provided that a referendum was held. The referendum took place on October 20, 1945, with (according to official numbers) 100% of the electorate voting for independence.

Many of Eastern Asia's current disputes to sovereignty and self-determination stem from unresolved disputes from World War II. After its fall, the Empire of Japan renounced control over many of its former possessions including Korea, Sakhalin Island, and Taiwan. In none of these areas were the opinions of affected people consulted, or given significant priority. Korea was specifically granted independence but the receiver of various other areas was not stated in the Treaty of San Francisco, giving Taiwan "de facto" independence although its political status continues to be ambiguous.

In 1941 Allies of World War II declared the Atlantic Charter and accepted the principle of self-determination. In January 1942 twenty-six states signed the Declaration by United Nations, which accepted those principles. The ratification of the United Nations Charter in 1945 at the end of World War II placed the right of self-determination into the framework of international law and diplomacy.

On 14 December 1960, the United Nations General Assembly adopted United Nations General Assembly Resolution 1514 (XV) subtitled "Declaration on the Granting of Independence to Colonial Countries and Peoples", which supported the granting of independence to colonial countries and people by providing an inevitable legal linkage between self-determination and its goal of decolonisation. It postulated a new international law-based right of freedom to exercise economic self-determination. Article 5 states: Immediate steps shall be taken in Trust and Non-Self-Governing Territories, or all other territories which have not yet attained independence, to transfer all powers to the people of those territories, without any conditions or reservations, in accordance with their freely expressed will and desire, without any distinction as to race, creed or colour, in order to enable them to enjoy complete independence and freedom.

On 15 December 1960 the United Nations General Assembly adopted United Nations General Assembly Resolution 1541 (XV), subtitled "Principles which should guide members in determining whether or nor an obligation exists to transmit the information called for under Article 73e of the United Nations Charter in Article 3", which provided that "[t]he inadequacy of political, economic, social and educational preparedness should never serve as a pretext for delaying the right to self-determination and independence." To monitor the implementation of Resolution 1514, in 1961 the General Assembly created the Special Committee referred to popularly as the Special Committee on Decolonization to ensure decolonization complete compliance with the principles of self-determination in General Assembly Resolution 1541 (XV).

However, the charter and other resolutions did not insist on full independence as the best way of obtaining self-government, nor did they include an enforcement mechanism. Moreover, new states were recognized by the legal doctrine of uti possidetis juris, meaning that old administrative boundaries would become international boundaries upon independence if they had little relevance to linguistic, ethnic, and cultural boundaries. Nevertheless, justified by the language of self-determination, between 1946 and 1960, thirty-seven new nations in Asia, Africa, and the Middle East gained independence from colonial powers. The territoriality issue inevitably would lead to more conflicts and independence movements within many states and challenges to the assumption that territorial integrity is as important as self-determination.

Decolonization in the world was contrasted by the Soviet Union's successful post-war expansionism. Tuva and several regional states in Eastern Europe, the Baltic, and Central Asia had been fully annexed by the Soviet Union during World War II. Now, it extended its influence by establishing satellite states Eastern Germany and the countries of Eastern Europe, along with support for revolutionary movements in China and North Korea. Although satellite states were independent and possessed sovereignty, the Soviet Union violated principles of self-determination by suppressing the Hungarian revolution of 1956 and the Prague Spring Czechoslovak reforms of 1968. It invaded Afghanistan to support a communist government assailed by local tribal groups. However, Marxism–Leninism and its theory of imperialism were also strong influences in the national emancipation movements of Third World nations rebelling against colonial or puppet regimes. In many Third World countries, communism became an ideology that united groups to oppose imperialism or colonization.

Soviet actions were contained by the United States which saw communism as a menace to its interests. Throughout the cold war, the United States created, supported, and sponsored regimes with various success that served their economic and political interests, among them anti-communist regimes such as that of Augusto Pinochet in Chile and Suharto in Indonesia. To achieve this, a variety of means was implemented, including the orchestration of coups, sponsoring of anti-communist countries and military interventions. Consequently, many self-determination movements, which spurned some type of anti-communist government, were accused of being Soviet-inspired or controlled.

In Asia, the Soviet Union had already converted Mongolia into a satellite state but abandoned propping up the Second East Turkestan Republic and gave up its Manchurian claims to China. The new People's Republic of China had gained control of mainland China in the Chinese Civil War. The Korean War shifted the focus of the Cold War from Europe to Asia, where competing superpowers took advantage of decolonization to spread their influence.

In 1947, India gained independence from the British Empire. The empire was in decline but adapted to these circumstances by creating the British Commonwealth—since 1949 the Commonwealth of Nations—which is a free association of equal states. As India obtained its independence, multiple ethnic conflicts emerged in relation to the formation of a statehood during the Partition of India which resulted in Islamic Pakistan and Secular India. Before the advent of the British, no empire based in mainland India had controlled any part of what now makes up the country's Northeast, part of the reason for the ongoing insurgency in Northeast India. In 1971 Bangladesh obtained independence from Pakistan.

Burma also gained independence from the British Empire, but declined membership in the Commonwealth.

Indonesia gained independence from the Netherlands in 1949 after the latter failed to restore colonial control. As mentioned above, Indonesia also wanted a powerful position in the region that could be lessened by the creation of united Malaysia. The Netherlands retained Dutch New Guinea, but Indonesia threatened to invade and annex it. A vote was supposedly taken under the UN sponsored Act of Free Choice to allow West New Guineans to decide their fate, although many dispute its veracity. Later, Portugal relinquished control over East Timor in 1975, at which time Indonesia promptly invaded and annexed it.

The Cold War began to wind down after Mikhail Gorbachev assumed power in March 1985. With the cooperation of the American president Ronald Reagan, Gorbachev wound down the size of the Soviet Armed Forces and reduced nuclear arms in Europe, while liberalizing the economy.

In 1989 – 90, the communist regimes of Soviet satellite states collapsed in rapid succession in Poland, Hungary, Czechoslovakia, East Germany, Bulgaria, Romania, and Mongolia. East and West Germany united, Czechoslovakia peacefully split into Czech Republic and Slovakia, while in 1990 Yugoslavia began a violent break up into 6 states. Kosovo, which was previously an autonomous unit of Serbia declared independence in 2008, but has received less international recognition.

In December 1991, Gorbachev resigned as president and the Soviet Union dissolved relatively peacefully into fifteen sovereign republics, all of which rejected communism and most of which adopted democratic reforms and free-market economies. Inside those new republics, four major areas have claimed their own independence, but not received widespread international recognition.

After decades of civil war, Indonesia finally recognized the independence of East Timor in 2002.

In 1949, the Communists won the civil war and established the People's Republic of China in Mainland China. The Kuomintang-led Republic of China government retreated to Taipei, its jurisdiction now limited to Taiwan and several outlying islands. Since then, the People's Republic of China has been involved in disputes with the ROC over issues of sovereignty and the political status of Taiwan.

As noted, self-determination movements remain strong in some areas of the world. Some areas possess "de facto" independence, such as Taiwan, North Cyprus, Kosovo, and South Ossetia, but their independence is disputed by one or more major states. Significant movements for self-determination also persist for locations that lack "de facto" independence, such as Kurdistan, Balochistan, Chechnya, and the State of Palestine

Since the early 1990s, the legitimatization of the principle of national self-determination has led to an increase in the number of conflicts within states, as sub-groups seek greater self-determination and full secession, and as their conflicts for leadership within groups and with other groups and with the dominant state become violent. The international reaction to these new movements has been uneven and often dictated more by politics than principle. The 2000 United Nations Millennium Declaration failed to deal with these new demands, mentioning only "the right to self-determination of peoples which remain under colonial domination and foreign occupation."

In an issue of "Macquarie University Law Journal" Associate Professor Aleksandar Pavkovic and Senior Lecturer Peter Radan outlined current legal and political issues in self-determination. These include:

There is not yet a recognized legal definition of "peoples" in international law. Vita Gudeleviciute of Vytautas Magnus University Law School, reviewing international law and UN resolutions, finds in cases of non-self-governing peoples (colonized and/or indigenous) and foreign military occupation "a people" is the entire population of the occupied territorial unit, no matter their other differences. In cases where people lack representation by a state's government, the unrepresented become a separate people. Present international law does not recognize ethnic and other minorities as separate peoples, with the notable exception of cases in which such groups are systematically disenfranchised by the government of the state they live in. Other definitions offered are "peoples" being self-evident (from ethnicity, language, history, etc.), or defined by "ties of mutual affection or sentiment", i.e. "loyalty", or by mutual obligations among peoples. Or the definition may be simply that a people is a group of individuals who unanimously choose a separate state. If the "people" are unanimous in their desire for self-determination, it strengthens their claim. For example, the populations of federal units of the Yugoslav federation were considered a people in the breakup of Yugoslavia, although some of those units had very diverse populations. Although there is no fully accepted definition of peoples, references are often made to a definition proposed by UN Special Rapporteur Martínez Cobo in his study on discrimination against indigenous populations. UN Independent Expert on the Promotion of a democratic and equitable International Order, Alfred de Zayas, relied on the "Kirby definition" in his 2014 Report to the General Assembly A/69/272 as "a group of persons with a common historical tradition, racial or ethnic identity, cultural homogeneity, linguistic unity, religious or ideological affinity, territorial connection,or common economic life. To this should be added a subjective element: the will to be identified as a people and the consciousness of being a people.". 

Abulof suggests that self-determination entails the "moral double helix" of duality (personal right to align with a people, and the people's right to determine their politics) and mutuality (the right is as much the other's as the self's). Thus, self-determination grants individuals the right to form "a people," which then has the right to establish an independent state, as long as they grant the same to all other individuals and peoples.

Criteria for the definition of "people having the right of self-determination" was proposed during 2010 Kosovo case decision of the International Court of Justice: 1. traditions and culture 2. ethnicity 3. historical ties and heritage 4. language 5. religion 6. sense of identity or kinship 7. the will to constitute a people 8. common suffering.

National self-determination appears to challenge the principle of territorial integrity (or sovereignty) of states as it is the will of the people that makes a state legitimate. This implies a people should be free to choose their own state and its territorial boundaries. However, there are far more self-identified nations than there are existing states and there is no legal process to redraw state boundaries according to the will of these peoples. According to the Helsinki Final Act of 1975, the UN, ICJ and international law experts, there is no contradiction between the principles of self-determination and territorial integrity, with the latter taking precedence.
Pavkovic and Radan describe three theories of international relations relevant to self-determination.


Allen Buchanan, author of seven books on self-determination and secession, supports territorial integrity as a moral and legal aspect of constitutional democracy. However, he also advances a "Remedial Rights Only Theory" where a group has "a general right to secede if and only if it has suffered certain injustices, for which secession is the appropriate remedy of last resort. " He also would recognize secession if the state grants, or the constitution includes, a right to secede.

Vita Gudeleviciute holds that in cases of non-self-governing peoples and foreign military occupation the principle of self-determination trumps that of territorial integrity. In cases where people lack representation by a state's government, they also may be considered a separate people, but under current law cannot claim the right to self-determination. On the other hand, she finds that secession within a single state is a domestic matter not covered by international law. Thus there are no on what groups may constitute a seceding people.

A number of states have laid claim to territories, which they allege were removed from them as a result of colonialism. This is justified by reference to Paragraph 6 of UN Resolution 1514(XV), which states that any attempt "aimed at partial or total disruption of the national unity and the territorial integrity of a country is incompatible with the purposes and principles of the Charter". This, it is claimed, applies to situations where the territorial integrity of a state had been disrupted by colonisation, so that the people of a territory subject to a historic territorial claim are prevented from exercising a right to self-determination. This interpretation is rejected by many states, who argue that Paragraph 2 of UN Resolution 1514(XV) states that "all peoples have the right to self-determination" and Paragraph 6 cannot be used to justify territorial claims. The original purpose of Paragraph 6 was "to ensure that acts of self-determination occur within the established boundaries of colonies, rather than within sub-regions". Further, the use of the word "attempt" in Paragraph 6 denotes future action and cannot be construed to justify territorial redress for past action. An attempt sponsored by Spain and Argentina to qualify the right to self-determination in cases where there was a territorial dispute was rejected by the UN General Assembly, which re-iterated the right to self-determination was a universal right.

In order to accommodate demands for minority rights and avoid secession and the creation of a separate new state, many states decentralize or devolve greater decision-making power to new or existing subunits or autonomous areas. More limited measures might include restricting demands to the maintenance of national cultures or granting non-territorial autonomy in the form of national associations which would assume control over cultural matters. This would be available only to groups that abandoned secessionist demands and the territorial state would retain political and judicial control, but only if would remain with the territorially organized state.

Pavković explores how national self-determination, in the form of creation of a new state through secession, could override the principles of majority rule and of equal rights, which are primary liberal principles. This includes the question of how an unwanted state can be imposed upon a minority. He explores five contemporary theories of secession. In "anarcho-capitalist" theory only landowners have the right to secede. In communitarian theory, only those groups that desire direct or greater political participation have the right, including groups deprived of rights, per Allen Buchanan. In two nationalist theories, only national cultural groups have a right to secede. Australian professor Harry Beran's democratic theory endorses the equality of the right of secession to all types of groups. Unilateral secession against majority rule is justified if the group allows secession of any other group within its territory.

Most sovereign states do not recognize the right to self-determination through secession in their constitutions. Many expressly forbid it. However, there are several existing models of self-determination through greater autonomy and through secession.

In liberal constitutional democracies the principle of majority rule has dictated whether a minority can secede. In the United States Abraham Lincoln acknowledged that secession might be possible through amending the United States Constitution. The Supreme Court in "Texas v. White" held secession could occur "through revolution, or through consent of the States." The British Parliament in 1933 held that Western Australia only could secede from Australia upon vote of a majority of the country as a whole; the previous two-thirds majority vote for secession via referendum in Western Australia was insufficient.

The Chinese Communist Party followed the Soviet Union in including the right of secession in its 1931 constitution in order to entice ethnic nationalities and Tibet into joining. However, the Party eliminated the right to secession in later years, and had anti-secession clause written into the Constitution before and after the founding the People's Republic of China. The 1947 Constitution of the Union of Burma contained an express state right to secede from the union under a number of procedural conditions. It was eliminated in the 1974 constitution of the Socialist Republic of the Union of Burma (officially the "Union of Myanmar"). Burma still allows "local autonomy under central leadership".

As of 1996 the constitutions of Austria, Ethiopia, France, and Saint Kitts and Nevis have express or implied rights to secession. Switzerland allows for the secession from current and the creation of new cantons. In the case of proposed Quebec separation from Canada the Supreme Court of Canada in 1998 ruled that only both a clear majority of the province and a constitutional amendment confirmed by all participants in the Canadian federation could allow secession.

The 2003 draft of the European Union Constitution allowed for the voluntary withdrawal of member states from the union, although the State which wanted to leave could not be involved in the vote deciding whether or not they can leave the Union. There was much discussion about such self-determination by minorities before the final document underwent the unsuccessful ratification process in 2005.

As a result of the successful constitutional referendum held in 2003, every municipality in the Principality of Liechtenstein has the right to secede from the Principality by a vote of a majority of the citizens residing in this municipality.

In determining international borders between sovereign states, self-determination has yielded to a number of other principles. Once groups exercise self-determination through secession, the issue of the proposed borders may prove more controversial than the fact of secession. The bloody Yugoslav wars in the 1990s were related mostly to border issues because the international community applied a version of uti possidetis juris in transforming the existing internal borders of the various Yugoslav republics into international borders, despite the conflicts of ethnic groups within those boundaries. In the 1990s indigenous populations of the northern two-thirds of Quebec province opposed being incorporated into a Quebec nation and stated a determination to resist it by force.

The border between Northern Ireland and the Irish Free State was based on the borders of existing counties and did not include all of historic Ulster. A Boundary Commission was established to consider re-drawing it. Its proposals, which amounted to a small net transfer to Northern Ireland, were leaked to the press and then not acted upon. In December 1925, the governments of the Irish Free State, Northern Ireland, and the United Kingdom agreed to accept the existing border.

There have been a number of notable cases of self-determination. For more information on past movements see list of historical autonomist and secessionist movements and lists of decolonized nations. Also see list of autonomous areas by country and list of territorial autonomies and list of active autonomist and secessionist movements.

The Republic of Artsakh (Republic of Nagorno-Karabakh), in the Caucasus region, declared its independence basing on self-determination rights on September 2, 1991. It successfully defended its independence in subsequent war with Azerbaijan, but remains largely unrecognized by UN states today. It is a member of the Community for Democracy and Rights of Nations along with three other Post-Soviet disputed republics.

From 2003 onwards, self-determination has become the topic of some debate in Australia in relation to Aboriginal Australians and Torres Strait Islanders. In the 1970s, the Indigenous community approached the Federal Government and requested the right to administer their own communities. This encompassed basic local government functions, ranging from land dealings and management of community centres to road maintenance and garbage collection, as well as setting education programmes and standards in their local schools.
The traditional homeland of the Tuareg peoples was divided up by the modern borders of Mali, Algeria and Niger. Numerous rebellions occurred over the decades, but in 2012 the Tuaregs succeeded in occupying their land and declaring the independence of Azawad. However, their movement was hijacked by the Islamist terrorist group Ansar Dine.

The Basque Country (, , ) as a cultural region (not to be confused with the homonym Autonomous Community of the Basque country) is a European region in the western Pyrenees that spans the border between France and Spain, on the Atlantic coast. It comprises the autonomous communities of the Basque Country and Navarre in Spain and the Northern Basque Country in France.
Since the 19th century, Basque nationalism has demanded the right of some kind of self-determination. This desire for independence is particularly stressed among leftist Basque nationalists. The right of self-determination was asserted by the Basque Parliament in 1990, 2002 and 2006.
Since self-determination is not recognized in the Spanish Constitution of 1978, some Basques abstained and some voted against it in the referendum of December 6 of that year. It was approved by a clear majority at the Spanish level, and with 74.6% of the votes in the Basque Country. However, the overall turnout in the Basque Country was 45% when the Spanish overall turnover was 67.9%. The derived autonomous regime for the BAC was approved by Spanish Parliament and also by the Basque citizens in referendum. The autonomous statue of Navarre ("Amejoramiento del Fuero": "improvement of the charter") was approved by the Spanish Parliament and, like the statues of 13 out of 17 Spanish autonomous communities, it didn't need a referendum to enter into force.

"Euskadi Ta Askatasuna" or ETA (; pronounced ), was an armed Basque nationalist, separatist and terrorist organization. Founded in 1959, it evolved from a group advocating traditional cultural ways to a paramilitary group with the goal of Basque independence. Its ideology was Marxist–Leninist.

The Nigerian Civil War was fought between Biafran secessionists of the Republic of Biafra and the Nigerian central government. From 1999 to the present day, the indigenous people of Biafra have been agitating for independence to revive their country. They have registered a human rights organization known as Bilie Human Rights Initiative both in Nigeria and in the United Nations to advocate for their right to self-determination and achieve independence by the rule of law.

After the 2012 Catalan march for independence, in which between 600,000 and 1.5 million citizens marched, the President of Catalonia, Artur Mas, called for new parliamentary elections on 25 November 2012 to elect a new parliament that would exercise the right of self-determination for Catalonia, a right not recognised under the Spanish constitution. The Parliament of Catalonia voted to hold a vote in the next four-year legislature on the question of self-determination. The parliamentary decision was approved by a large majority of MPs: 84 voted for, 21 voted against, and 25 abstained. The Catalan Parliament applied to the Spanish Parliament for the power to call a referendum to be devolved, but this was turned down. In December 2013 the President of the Generalitat Artur Mas and the governing coalition agreed to set the referendum for self-determination on 9 November 2014, and legislation specifically saying that the consultation would not be a "referendum" was enacted, only to be blocked by the Spanish Constitutional Court, at the request of the Spanish government. Given the block, the Government turned it into a simple "consultation to the people" instead.

The question in the consultation was "Do you want Catalonia to be a State?" and, if the answer to this question was yes, "Do you want this State to be an independent State?". However, as the consultation was not a formal referendum, these (printed) answers were just suggestions and other answers were also accepted and catalogued as "other answers" instead as null votes. The turnout in this consultation was about 2·3m people out of 6·2m people that were called to vote (this figure does not coincide with the census figure of 5·3m for two main reasons: first, because organisers had no access to an official census due to the non-binding character of the consultation, and second, because the legal voting age was set to 16 rather than 18). Due to the lack of an official census, potential voters were assigned to electoral tables according to home address and first family name. Participants had to sign up first with their full name and national ID in a voter registry before casting their ballot, which prevented participants from potentially casting multiple ballots. The overall result was 80·76% in favor of both questions, 11% in favor of the first question but not of the second questions, 4·54% against both; the rest were classified as "other answers". The voter turnout was around 37% (most people against the consultation didn't go to vote). Four top members of Catalonia's political leadership were barred from public office for having defied the Constitutional court's last-minute ban.
Almost three years later (1 October 2017), the Catalan government called a referendum for independence under legislation adopted in September 2017 (despite being blocked by the Constitutional Court of Spain), with the question "Do you want Catalonia to become an independent state in the form of a Republic?". On polling day, the Catalan police prevented voting in over 500 polling stations, without incident, while the Spanish police confiscated ballot boxes and closed down 92, voting centres with violent truncheon charges. The opposition parties had called for non-participation. The turnout (according to the votes that were counted) was 2.3m out of 5.3m (43.03% of the census), and 90.18% of the ballots were in favour of independence. The turnout, ballot count and results were similar to those of the 2014 "consultation".

Under Dzhokhar Dudayev, Chechnya declared independence as the Chechen Republic of Ichkeria, using self-determination, Russia's history of bad treatment of Chechens, and a history of independence before invasion by Russia as main motives. Russia has restored control over Chechnya, but the separatist government functions still in exile, though it has been split into two entities: the Akhmed Zakayev-run secular Chechen Republic (based in Poland, the UK and the US), and the Islamic Caucasus Emirate.

There is an active secessionist movement based on the self-determination of the residents of the Donetsk and Luhansk regions of eastern Ukraine, allegedly against the illegitimacy and corruption of the Ukrainian government. However, many in the international community assert that referendums held there in 2014 regarding independence from Ukraine were illegitimate and undemocratic. Similarly, there are reports that presidential elections in May 2014 were prevented from taking place in the two regions after armed gunmen took control of polling stations, kidnapped election officials, and stole lists of electors, thus denying the population the chance to express their will in a free, fair, and internationally recognised election. There are also arguments, that the de facto separation of the part of Eastern Ukraine from the rest of the country is not in fact an expression of self-determination, but rather a manipulation through pro-Soviet sentiment revival and an invasion by neighbouring Russia, with Ukrainian President Petro Poroshenko claining in 2015 that up to 9,000 Russian soldiers were deployed in Ukraine.

Self-determination is referred to in the Falkland Islands Constitution and is a factor in the Falkland Islands sovereignty dispute. The population has existed for over nine generations, continuously for over 185 years. In the 2013 referendum organised by the Falkland Islands Government, 99.8% voted to remain British. As administering power, the British Government considers since the majority of inhabitants wish to remain British, transfer of sovereignty to Argentina would be counter to their right to self-determination.
Argentina states the principle of self-determination is not applicable since the current inhabitants are not aboriginal and were brought to replace the Argentine population, which was expelled by an 'act of force', forcing the Argentinian inhabitants to directly leave the islands. This refers to the re-establishment of British rule in the year 1833 during which Argentina claims the existing population living in the islands was expelled. Argentina thus argues that, in the case of the Falkland Islands, the principle of territorial integrity should have precedence over self-determination. Historical records dispute Argentina's claims and whilst acknowledging the garrison was expelled note the existing civilian population remained at Port Louis and there was no attempt to settle the islands until 1841.

The right to self-determination is referred to in the pre-amble of Chapter 1 of the Gibraltar constitution, and, since the United Kingdom also gave assurances that the right to self-determination of Gibraltarians would be respected in any transfer of sovereignty over the territory, is a factor in the dispute with Spain over the territory. The impact of the right to self-determination of Gibraltarians was seen in the 2002 Gibraltar sovereignty referendum, where Gibraltarian voters overwhelmingly rejected a plan to share sovereignty over Gibraltar between the UK and Spain. However, the UK government differs with the Gibraltarian government in that it considers Gibraltarian self-determination to be limited by the Treaty of Utrecht, which prevents Gibraltar achieving independence without the agreement of Spain, a position that the Gibraltarian government does not accept. 

The Spanish government denies that Gibraltarians have the right to self-determination, considering them to be "an artificial population without any genuine autonomy" and not "indigenous". However, the Partido Andalucista has agreed to recognise the right to self-determination of Gibraltarians.

Before the United Nations's adoption of resolution 2908 (XXVII) on 2 November 1972, The People's Republic of China vetoed the former British colony of Hong Kong's right to self-determination on 8 March 1972. This sparked several nation's protest along with Great Britain's declaration on 14 December that the decision is invalid. 
Decades later , a nationalist independence movement, dubbed as the Hong Kong independence movement emerged in the now Communist Chinese controlled territory. It advocates the autonomous region to become a fully independent sovereign state.

The city is considered a special administrative region (SAR) which, according to the PRC, enjoys a high degree of autonomy under the People's Republic of China (PRC), guaranteed under Article 2 of Hong Kong Basic Law (which is ratified under the Sino-British Joint Declaration), since the transfer of the sovereignty of Hong Kong from the United Kingdom to the PRC in 1997. Since the handover, many Hongkongers are increasingly concerned about Beijing's growing encroachment on the territory's freedoms and the failure of the Hong Kong government to deliver 'true' democracy.

The 2014–15 Hong Kong electoral reform package deeply divided the city, as it allowed Hongkongers to have universal suffrage, but Beijing would have authority to screen the candidates to restrict the electoral method for the Chief Executive of Hong Kong (CE), the highest-ranking official of the territory. This sparked the 79-day massive peaceful protests which was dubbed as the "Umbrella Revolution" and the pro-independence movement emerged on the Hong Kong political scene. 

Since then, localism has gained momentum, particularly after the failure of the peaceful Umbrella Movement. Young localist leaders have led numerous protest actions against pro-Chinese policies to raise awareness of social problems of Hong Kong under Chinese rule. These include the sit-in protest against the Bill to Strengthen Internet Censorship, demonstrations against Chinese political interference in the University of Hong Kong, the Recover Yuen Long protests and the 2016 Mong Kok civil unrest. According to a survey conducted by the Chinese University of Hong Kong (CUHK) in July 2016, 17.4% of respondents supported the city becoming an independent entity after 2047, while 3.6% stated that it is "possible".

Ever since Pakistan and India's inception in 1947 the legal state of Jammu and Kashmir, the land between India and Pakistan, has been contested as Britain was resigning from their rule over this land. Maharaja Hari Singh, the ruler of Kashmir at the time of accession, signed the Instrument of Accession Act on October 26, 1947 as his territory was being attacked by Pakistani tribesmen. The passing of this Act allowed Jammu and Kashmir to accede to India on legal terms. When this Act was taken to Lord Mountbatten, the last viceroy of British India, he agreed to it and stated that a referendum needed to be held by the citizens in India, Pakistan, and Kashmir so that they could vote as to where Kashmir should accede to. This referendum that Mountbatten called for never took place and framed one of the legal disputes for Kashmir. In 1948 the United Nations intervened and ordered a plebiscite to be taken in order to hear the voices of the Kashmiris if they would like to accede to Pakistan or India. This plebiscite left out the right for Kashmiris to have the right of self-determination and become an autonomous state. To this date the Kashmiris have been faced with numerous human rights violations committed by both India and Pakistan and have yet to gain complete autonomy which they have been seeking through self-determination. 

The insurgency in Kashmir against Indian rule has existed in various forms. A widespread armed insurgency started in Kashmir against India rule in 1989 after allegations of rigging by the Indian government in the 1987 Jammu and Kashmir state election. This led to some parties in the state assembly forming militant wings, which acted as a catalyst for the emergence of armed insurgency in the region. The conflict over Kashmir has resulted in tens of thousands of deaths.

The Inter-Services Intelligence of Pakistan has been accused by India of supporting and training both pro-Pakistan and pro-independence militants to fight Indian security forces in Jammu and Kashmir, a charge that Pakistan denies. According to official figures released in the Jammu and Kashmir assembly, there were 3,400 disappearance cases and the conflict has left more than 47,000 to 100,000 people dead as of July 2009. However, violence in the state had fallen sharply after the start of a slow-moving peace process between India and Pakistan. After the peace process failed in 2008, mass demonstrations against Indian rule, and also low-scale militancy have emerged again.

However, despite boycott calls by separatist leaders in 2014, the Jammu and Kashmir Assembly elections saw highest voters turnout in last 25 years since insurgency erupted. As per the Indian government, it recorded more than 65% of voters turnout which was more than usual voters turnout in other state assembly elections of India. It considered as increase in faith of Kashmiri people in democratic process of India. However, activists say that the voter turnout is highly exaggerated and that elections are held under duress. Votes are cast because the people want stable governance of the state and this cannot be mistaken as an endorsement of Indian rule.

Kurdistan is a historical region primarily inhabited by the Kurdish people of the middle east. The territory is currently part of 4 states Turkey, Iraq, Syria and Iran. There are Kurdish self-determination movements in each of the 4 states. Iraqi Kurdistan has to date achieved the largest degree of self-determination through the formation of the Kurdistan Regional Government, an entity recognised by the Iraqi Federal Constitution.

Although the right of the creation of a Kurdish state was recognized following World War I in the Treaty of Sèvres, the treaty was then annulled by the Treaty of Lausanne (1923). To date two separate Kurdish republics and one Kurdish Kingdom have declared sovereignty. The Republic of Ararat (Ağrı Province, Turkey), the Republic of Mehabad (West Azerbaijan Province, Iran) and the Kingdom of Kurdistan (Sulaymaniyah Governorate, Iraqi Kurdistan, Iraq), each of these fledgling states was crushed by military intervention. The Patriotic Union of Kurdistan which currently holds the Iraqi presidency and the Kurdistan Democratic Party which governs the Kurdistan Regional Government both explicitly commit themselves to the development of Kurdish self-determination, but opinions vary as to the question of self-determination sought within the current borders and countries.

Efforts towards Kurdish self-determination are considered illegal separatism by the governments of Turkey and Iran, and the movement is politically repressed in both states. This is intertwined with Kurdish nationalist insurgencies in Iran and in Turkey, which in turn justify and are justified by the repression of peaceful advocacy. In Syria, a self-governing local Kurdish-dominated polity was established in 2012, amongst the upheaval of the Syrian Civil War, but has not been recognized by any foreign state.

Naga refers to a vaguely-defined conglomeration of distinct tribes living on the border of India and Burma. Each of these tribes lived in a sovereign village before the arrival of the British, but developed a common identity as the area was Christianized. After the British left India, a section of Nagas under the leadership of Angami Zapu Phizo sought to establish a separate country for the Nagas. Phizo's group, the Naga National Council (NNC), claimed that 99. 9% of the Nagas wanted an independent Naga country according to a referendum conducted by it. It waged a secessionist insurgency against the Government of India. The NNC collapsed after Phizo got his dissenters killed or forced them to seek refuge with the Government. Phizo escaped to London, while NNC's successor secessionist groups continued to stage violent attacks against the Indian Government. The Naga People's Convention (NPC), another major Naga organization, was opposed to the secessionists. Its efforts led to the creation of a separate Nagaland state within India in 1963. The secessionist violence declined considerably after the Shillong Accord of 1975. However, three factions of the National Socialist Council of Nagaland (NSCN) continue to seek an independent country which would include parts of India and Burma. They envisage a sovereign, predominantly Christian nation called "Nagalim".

Another controversial episode with perhaps more relevance was the British beginning their exit from British Malaya. An experience concerned the findings of a "United Nations Assessment Team" that led the British territories of North Borneo and Sarawak in 1963 to determine whether or not the populations wished to become a part of the new Malaysia Federation. The United Nation Team's mission followed on from an earlier assessment by the British-appointed Cobbold Commission which had arrived in the territories in 1962 and held hearings to determine public opinion. It also sifted through 1600 letters and memoranda submitted by individuals, organisations and political parties. Cobbold concluded that around two thirds of the population favoured to the formation of Malaysia while the remaining third wanted either independence or continuing control by the United Kingdom. The United Nations team largely confirmed these findings, which were later accepted by the General Assembly, and both territories subsequently wish to form the new Federation of Malaysia. The conclusions of both the Cobbold Commission and the United Nations team were arrived at without any referendums self-determination being held. Unlike in Singapore, however, no referendum was ever conducted in Sarawak and North Borneo. they sought to consolidate several of the previous ruled entities then there was Manila Accord, an agreement between the Philippines, Federation of Malaya and Indonesia on 31 July 1963 to abide by the wishes of the people of North Borneo and Sarawak within the context of United Nations General Assembly Resolution 1541 (XV), Principle 9 of the Annex taking into account referendums in North Borneo and Sarawak that would be free and without coercion. This also triggered the Indonesian confrontation because Indonesia opposed the violation of the agreements.

Cyprus was settled by Mycenaean Greeks in two waves in the 2nd millennium BC. As a strategic location in the Middle East, it was subsequently occupied by several major powers, including the empires of the Assyrians, Egyptians and Persians, from whom the island was seized in 333 BC by Alexander the Great. Subsequent rule by Ptolemaic Egypt, the Classical and Eastern Roman Empire, Arab caliphates for a short period and the French Lusignan dynasty. Following the death in 1473 of James II, the last Lusignan king, the Republic of Venice assumed control of the island, while the late king's Venetian widow, Queen Catherine Cornaro, reigned as figurehead. Venice formally annexed the Kingdom of Cyprus in 1489, following the abdication of Catherine. The Venetians fortified Nicosia by building the Walls of Nicosia, and used it as an important commercial hub.

Although the Lusignan French aristocracy remained the dominant social class in Cyprus throughout the medieval period, the former assumption that Greeks were treated only as serfs on the island is no longer considered by academics to be accurate. It is now accepted that the medieval period saw increasing numbers of Greek Cypriots elevated to the upper classes, a growing Greek middle ranks, and the Lusignan royal household even marrying Greeks. This included King John II of Cyprus who married Helena Palaiologina.

Throughout Venetian rule, the Ottoman Empire frequently raided Cyprus. In 1539 the Ottomans destroyed Limassol and so fearing the worst, the Venetians also fortified Famagusta and Kyrenia.

Invaded in 1570, Turks controlled and solely governed all of the Cyprus island from 1571 till its leasing to the United Kingdom in 1878. Cyprus was placed under British administration based on Cyprus Convention in 1878 and formally annexed by Britain in 1914. While Turkish Cypriots made up 18% of the population, the partition of Cyprus and creation of a Turkish state in the north became a policy of Turkish Cypriot leaders and Turkey in the 1950s. Politically, there was no majority/minority relation between Greek Cypriots and Turkish Cypriots; and hence, in 1960, Republic of Cyprus was founded by the constituent communities in Cyprus (Greek Cypriots and Turkish Cypriots) as a non-unitary state; the 1960 Constitution set both Turkish and Greek as the
official languages. During 1963-74, the island experienced ethnic clashes and turmoil, the coup to unify the island to Greece and eventual Turkish invasion in 1974. Ethnic Cleansing and the European Union, p. 12</ref> Turkish Republic of Northern Cyprus was declared in 1983 and recognized only by Turkey. Monroe Leigh, 1990, The Legal Status in International Law of the Turkish Cypriot and the Greek Cypriot Communities in Cyprus. The Greek Cypriot and Turkish Cypriot regimes participating in these negotiations, and the respective communities which they represent, are presently entitled to exercise equal rights under international law, including rights of self-determination.</ref> Before the Turkey's invasion in 1974, Turkish Cypriots were concentrated in Turkish Cypriot enclaves in the island.

Northern Cyprus fulfills all the classical criteria of statehood. United Nations Peace Force in Cyprus (UNFICYP) operates based on the laws of Northern Cyprus in north of Cyprus island. According to European Court of Human Rights (ECtHR), the laws of Northern Cyprus is valid in the north of Cyprus. ECtHR did "not" accept the claim that the Courts of Northern Cyprus lacked "independence and/or impartiality". ECtHR directed all Cypriots to exhaust "domestic remedies" applied by Northern Cyprus before taking their cases to ECtHR. In 2014, United States' Federal Court qualified Turkish Republic of Northern Cyprus as a "democratic country". In 2017, United Kingdom's High Court decided that "There was no duty in UK law upon the UK's Government to refrain from recognising Northern Cyprus. The United Nations itself works with Northern Cyprus law enforcement agencies and facilitates cooperation between the two parts of the island." UK's High Court also dismissed the claim that "cooperation between UK police and law agencies in northern Cyprus was illegal".

In Canada, many in the province of Quebec have wanted the province to separate from Confederation. The Parti Québécois has asserted Quebec's "right to self-determination. " There is debate on under which conditions would this right be realized. French-speaking Quebec nationalism and support for maintaining Québécois culture would inspire Quebec nationalists, many of whom were supporters of the Quebec sovereignty movement during the late-20th century.

Scotland has a long-standing independence movement. A referendum was held in 2014. Following the results of the UK referendum on EU membership, there have been moves towards holding a second referendum on independence, supported by the Scottish Government and the Scottish Parliament.

Section 235 of the South African Constitution allows for the right to self-determination of a community, within the framework of "the right of the South African people as a whole to self-determination", and pursuant to national legislation. This section of the constitution was one of the negotiated settlements during the handing over of political power in 1994. Supporters of an independent Afrikaner homeland have argued that their goals are reasonable under this new legislation.

In Italy, South Tyrol/Alto Adige was annexed after the First World War. The German-speaking inhabitants of South Tyrol are protected by the Gruber-De Gasperi Agreement, but there are still supporters of the self determination of South Tyrol, e.g. the party Die Freiheitlichen and the South Tyrolean independence movement. At the end of WWII the Allies offered to separate South Tyrol from Italy, but the South Tyrolean People's Party refused, preferring to obtain huge fiscal and economic advantages from Rome.

The colonization of the North American continent and its Native American population has been the source of legal battles since the early 19th century. Many Native American tribes were resettled onto separate tracts of land (reservations), which have retained a certain degree of autonomy within the United States. The federal government recognizes Tribal Sovereignty and has established a number of laws attempting to clarify the relationship among the federal, state, and tribal governments. The Constitution and later federal laws recognize the local sovereignty of tribal nations, but do not recognize full sovereignty equivalent to that of foreign nations, hence the term "domestic dependent nations" to qualify the federally recognized tribes.

Certain Chicano nationalist groups seek to "recreate" an ethnic-based state to be called Aztlán, after the legendary homeland of the Aztecs. It would comprise the Southwestern United States, historic territory of indigenous peoples and their descendants, as well as colonists and later settlers under the Spanish colonial and Mexican governments. Black nationalists have argued that, by virtue of slaves' unpaid labor and the harsh experiences of African Americans under slavery and Jim Crow, African Americans have a moral claim to the areas where the highest percentage of the population classified as black lives. They believe this area should be the basis of forming an independent state of New Afrika, designed to have an African-American majority and political control.

There are several active Hawaiian autonomy or independence movements, each with the goal of realizing some level of political control over single or several islands. The groups range from those seeking territorial units similar to Indian reservations under the United States, with the least amount of independent control, to the Hawaiian sovereignty movement, which is projected to have the most independence. The Hawaiian Sovereignty movement seeks to revive the Hawaiian nation under the Hawaiian constitution. Supporters of this concept say that Hawaii retained its sovereignty while under control of the United States.

Since 1972, the U.N. Decolonization Committee has called for Puerto Rico's "decolonization" and for the US to recognize the island's right to self-determination and independence. In 2007 the Decolonization Subcommittee called for the United Nations General Assembly to review the political status of Puerto Rico, a power reserved by the 1953 Resolution. This followed the 1967 passage of a plebiscite act that provided for a vote on the status of Puerto Rico with three status options: continued commonwealth, statehood, and independence. In the first plebscite, the commonwealth option won with 60.4% of the votes, but US congressional committees failed to enact legislation to address the status issue. In subsequent plebiscites in 1993 and 1998, the status quo was favored.

In a referendum that took place in November 2012, a majority of Puerto Rican residents voted to change the territory's relationship with the United States, with the statehood option being the preferred option. But a large number of ballots—one-third of all votes cast—were left blank on the question of preferred alternative status. Supporters of the commonwealth status had urged voters to blank their ballots. When the blank votes are counted as anti-statehood votes, the statehood option would have received less than 50% of all ballots received. As of January 2014, Washington has not taken action to address the results of this plebiscite.

Many current US state, regional and city secession groups use the language of self-determination. A 2008 Zogby International poll revealed that 22% of Americans believe that "any state or region has the right to peaceably secede and become an independent republic."

Since the late 20th century, some states periodically discuss desires to secede from the United States. Unilateral secession was ruled unconstitutional by the US Supreme Court in "Texas v. White" (1869).

In the case of Hawaii, the struggle for self-determination does not fall under secession, as it is less a break from federal administration, than a return to the process through which cession was claimed to have occurred: namely the ongoing occupation via a US imposed military coup; and/or removal from the UN list of Non-Self-Governing Territories. to educate or properly inform the citizenry of Hawaii of its options for self-determination and sidestepped guidelines laid out in UN General Assembly resolution 742 (1953).

The self-determination of the West Papuan people has been violently suppressed by the Indonesian government since the withdrawal of Dutch colonial rule under the Netherlands New Guinea in 1962.

There is an active movement based on the self-determination of the Sahrawi people in the Western Sahara region. Morocco also claims the entire territory, and maintains control of about two-thirds of the region.



</doc>
<doc id="29271" url="https://en.wikipedia.org/wiki?curid=29271" title="Scale">
Scale

Scale or scale may refer to:










</doc>
<doc id="29275" url="https://en.wikipedia.org/wiki?curid=29275" title="Southcentral Alaska">
Southcentral Alaska

Southcentral Alaska is the portion of the U.S. state of Alaska consisting of the shorelines and uplands of the central Gulf of Alaska. Most of the population of the state lives in this region, concentrated in and around the city of Anchorage.

The area includes Cook Inlet, the Matanuska-Susitna Valley, the Kenai Peninsula, Prince William Sound, and the Copper River Valley. Tourism, fisheries, and petroleum production are important economic activities.

The major city is Anchorage. Other towns include Palmer, Wasilla, Kenai, Soldotna, Homer, Seward, Valdez, and Cordova.

The climate of Southcentral Alaska is subarctic. Temperatures range from an average high of 65°F (18°C) in July to an average low of 10°F (-12°C) in December. The hours of daylight per day varies from 20 hours in June and July to 6 hours in December and January. The coastal areas consist of temperate rainforests and alder shrublands. The interior areas are covered by boreal forests.

The terrain of Southcentral Alaska is shaped by six mountain ranges:

Southcentral Alaska contains several dormant and active volcanoes. The Wrangell Volcanoes are older, lie in the East, and include Mount Blackburn, Mount Bona, Mount Churchill, Mount Drum, Mount Gordon, Mount Jarvis, Mount Sanford, and Mount Wrangell. The Cook Inlet volcanoes, located in the Tordrillo Mountains and in the north end of the Aleutian Range, are newer, lie in the West, and include Mount Redoubt, Mount Iliamna, Hayes Volcano, Mount Augustine, Fourpeaked Mountain and Mount Spurr. Most recently, Augustine and Fourpeaked erupted in 2006, and Mount Redoubt erupted in March 2009, resulting in airplane flight cancellations.



</doc>
<doc id="29276" url="https://en.wikipedia.org/wiki?curid=29276" title="Spinor">
Spinor

In geometry and physics, spinors are elements of a complex vector space that can be associated with Euclidean space. Like geometric vectors and more general tensors, spinors transform linearly when the Euclidean space is subjected to a slight (infinitesimal) rotation. However, when a sequence of such small rotations is composed (integrated) to form an overall final rotation, the resulting spinor transformation depends on which sequence of small rotations was used: unlike vectors and tensors, a spinor transforms to its negative when the space is continuously rotated through a complete turn from 0° to 360° (see picture). This property characterizes spinors: spinors can be viewed as the "square roots" of vectors. 

It is also possible to associate a substantially similar notion of spinor to Minkowski space in which case the Lorentz transformations of special relativity play the role of rotations. Spinors were introduced in geometry by Élie Cartan in 1913. In the 1920s physicists discovered that spinors are essential to describe the intrinsic angular momentum, or "spin", of the electron and other subatomic particles.

Spinors are characterized by the specific way in which they behave under rotations. They change in different ways depending not just on the overall final rotation, but the details of how that rotation was achieved (by a continuous path in the rotation group). There are two topologically distinguishable classes (homotopy classes) of paths through rotations that result in the same overall rotation, as illustrated by the belt trick puzzle. These two inequivalent classes yield spinor transformations of opposite sign. The spin group is the group of all rotations keeping track of the class. It doubly covers the rotation group, since each rotation can be obtained in two inequivalent ways as the endpoint of a path. The space of spinors by definition is equipped with a (complex) linear representation of the spin group, meaning that elements of the spin group act as linear transformations on the space of spinors, in a way that genuinely depends on the homotopy class. In mathematical terms, spinors are described by a double-valued projective representation of the rotation group SO(3).

Although spinors can be defined purely as elements of a representation space of the spin group (or its Lie algebra of infinitesimal rotations), they are typically defined as elements of a vector space that carries a linear representation of the Clifford algebra. The Clifford algebra is an associative algebra that can be constructed from Euclidean space and its inner product in a basis-independent way. Both the spin group and its Lie algebra are embedded inside the Clifford algebra in a natural way, and in applications the Clifford algebra is often the easiest to work with. After choosing an orthonormal basis of Euclidean space, a representation of the Clifford algebra is generated by gamma matrices, matrices that satisfy a set of canonical anti-commutation relations. The spinors are the column vectors on which these matrices act. In three Euclidean dimensions, for instance, the Pauli spin matrices are a set of gamma matrices, and the two-component complex column vectors on which these matrices act are spinors. However, the particular matrix representation of the Clifford algebra, hence what precisely constitutes a "column vector" (or spinor), involves the choice of basis and gamma matrices in an essential way. As a representation of the spin group, this realization of spinors as (complex) column vectors will either be irreducible if the dimension is odd, or it will decompose into a pair of so-called "half-spin" or Weyl representations if the dimension is even.

What characterizes spinors and distinguishes them from geometric vectors and other tensors is subtle. Consider applying a rotation to the coordinates of a system. No object in the system itself has moved, only the coordinates have, so there will always be a compensating change in those coordinate values when applied to any object of the system. Geometrical vectors, for example, have components that will undergo "the same" rotation as the coordinates. More broadly, any tensor associated with the system (for instance, the stress of some medium) also has coordinate descriptions that adjust to compensate for changes to the coordinate system itself.

Spinors do not appear at this level of the description of a physical system, when one is concerned only with the properties of a single isolated rotation of the coordinates. Rather, spinors appear when we imagine that instead of a single rotation, the coordinate system is gradually (continuously) rotated between some initial and final configuration. For any of the familiar and intuitive ("tensorial") quantities associated with the system, the transformation law does not depend on the precise details of how the coordinates arrived at their final configuration. Spinors, on the other hand, are constructed in such a way that makes them "sensitive" to how the gradual rotation of the coordinates arrived there: they exhibit path-dependence. It turns out that, for any final configuration of the coordinates, there are actually two ("topologically") inequivalent "gradual" (continuous) rotations of the coordinate system that result in this same configuration. This ambiguity is called the homotopy class of the gradual rotation. The belt trick puzzle (shown) demonstrates two different rotations, one through an angle of 2π and the other through an angle of 4π, having the same final configurations but different classes. Spinors actually exhibit a sign-reversal that genuinely depends on this homotopy class. This distinguishes them from vectors and other tensors, none of which can feel the class.

Spinors can be exhibited as concrete objects using a choice of Cartesian coordinates. In three Euclidean dimensions, for instance, spinors can be constructed by making a choice of Pauli spin matrices corresponding to (angular momenta about) the three coordinate axes. These are 2×2 matrices with complex entries, and the two-component complex column vectors on which these matrices act by matrix multiplication are the spinors. In this case, the spin group is isomorphic to the group of 2×2 unitary matrices with determinant one, which naturally sits inside the matrix algebra. This group acts by conjugation on the real vector space spanned by the Pauli matrices themselves, realizing it as a group of rotations among them, but it also acts on the column vectors (that is, the spinors).

More generally, a Clifford algebra can be constructed from any vector space "V" equipped with a (nondegenerate) quadratic form, such as Euclidean space with its standard dot product or Minkowski space with its standard Lorentz metric. The space of spinors is the space of column vectors with formula_1 components. The orthogonal Lie algebra (i.e., the infinitesimal "rotations") and the spin group associated to the quadratic form are both (canonically) contained in the Clifford algebra, so every Clifford algebra representation also defines a representation of the Lie algebra and the spin group. Depending on the dimension and metric signature, this realization of spinors as column vectors may be irreducible or it may decompose into a pair of so-called "half-spin" or Weyl representations. When the vector space "V" is four-dimensional, the algebra is described by the gamma matrices.

The space of spinors is formally defined as the fundamental representation of the Clifford algebra. (This may or may not decompose into irreducible representations.) The space of spinors may also be defined as a spin representation of the orthogonal Lie algebra. These spin representations are also characterized as the finite-dimensional projective representations of the special orthogonal group that do not factor through linear representations. Equivalently, a spinor is an element of a finite-dimensional group representation of the spin group on which the center acts non-trivially.

There are essentially two frameworks for viewing the notion of a spinor.

From a representation theoretic point of view, one knows beforehand that there are some representations of the Lie algebra of the orthogonal group that cannot be formed by the usual tensor constructions. These missing representations are then labeled the spin representations, and their constituents "spinors". From this view, a spinor must belong to a representation of the double cover of the rotation group , or more generally of a double cover of the generalized special orthogonal group on spaces with a metric signature of . These double covers are Lie groups, called the spin groups or . All the properties of spinors, and their applications and derived objects, are manifested first in the spin group. Representations of the double covers of these groups yield double-valued projective representations of the groups themselves. (This means that the action of a particular rotation on vectors in the quantum Hilbert space is only defined up to a sign.)

From a geometrical point of view, one can explicitly construct the spinors and then examine how they behave under the action of the relevant Lie groups. This latter approach has the advantage of providing a concrete and elementary description of what a spinor is. However, such a description becomes unwieldy when complicated properties of the spinors, such as Fierz identities, are needed.

The language of Clifford algebras (sometimes called geometric algebras) provides a complete picture of the spin representations of all the spin groups, and the various relationships between those representations, via the classification of Clifford algebras. It largely removes the need for "ad hoc" constructions.

In detail, let "V" be a finite-dimensional complex vector space with nondegenerate bilinear form "g". The Clifford algebra is the algebra generated by "V" along with the anticommutation relation . It is an abstract version of the algebra generated by the gamma or Pauli matrices. If "V" = C, with the standard form we denote the Clifford algebra by Cℓ(C). Since by the choice of an orthonormal basis every complex vectorspace with non-degenerate form is isomorphic to this standard example, this notation is abused more generally if . If is even, Cℓ(C) is isomorphic as an algebra (in a non-unique way) to the algebra of complex matrices (by the Artin-Wedderburn theorem and the easy to prove fact that the Clifford algebra is central simple). If is odd, Cℓ(C) is isomorphic to the algebra of two copies of the complex matrices. Therefore, in either case has a unique (up to isomorphism) irreducible representation (also called simple Clifford module), commonly denoted by Δ, of dimension 2. Since the Lie algebra is embedded as a Lie subalgebra in equipped with the Clifford algebra commutator as Lie bracket, the space Δ is also a Lie algebra representation of called a spin representation. If "n" is odd, this Lie algebra representation is irreducible. If "n" is even, it splits further into two irreducible representations called the Weyl or "half-spin representations".

Irreducible representations over the reals in the case when "V" is a real vector space are much more intricate, and the reader is referred to the Clifford algebra article for more details.

Spinors form a vector space, usually over the complex numbers, equipped with a linear group representation of the spin group that does not factor through a representation of the group of rotations (see diagram). The spin group is the group of rotations keeping track of the homotopy class. Spinors are needed to encode basic information about the topology of the group of rotations because that group is not simply connected, but the simply connected spin group is its double cover. So for every rotation there are two elements of the spin group that represent it. Geometric vectors and other tensors cannot feel the difference between these two elements, but they produce "opposite" signs when they affect any spinor under the representation. Thinking of the elements of the spin group as homotopy classes of one-parameter families of rotations, each rotation is represented by two distinct homotopy classes of paths to the identity. If a one-parameter family of rotations is visualized as a ribbon in space, with the arc length parameter of that ribbon being the parameter (its tangent, normal, binormal frame actually gives the rotation), then these two distinct homotopy classes are visualized in the two states of the belt trick puzzle (above). The space of spinors is an auxiliary vector space that can be constructed explicitly in coordinates, but ultimately only exists up to isomorphism in that there is no "natural" construction of them that does not rely on arbitrary choices such as coordinate systems. A notion of spinors can be associated, as such an auxiliary mathematical object, with any vector space equipped with a quadratic form such as Euclidean space with its standard dot product, or Minkowski space with its Lorentz metric. In the latter case, the "rotations" include the Lorentz boosts, but otherwise the theory is substantially similar.

The constructions given above, in terms of Clifford algebra or representation theory, can be thought of as defining spinors as geometric objects in zero-dimensional space-time. To obtain the spinors of physics, such as the Dirac spinor, one extends the construction to obtain a spin structure on 4-dimensional space-time (Minkowski space). Effectively, one starts with the tangent manifold of space-time, each point of which is a 4-dimensional vector space with "SO"(3,1) symmetry, and then builds the spin group at each point. The neighborhoods of points are endowed with concepts of smoothness and differentiability: the standard construction is one of a fibre bundle, the fibers of which are affine spaces transforming under the spin group. After constructing the fiber bundle, one may then consider differential equations, such as the Dirac equation, or the Weyl equation on the fiber bundle. These equations (Dirac or Weyl) have solutions that are plane waves, having symmetries characteristic of the fibers, "i.e." having the symmetries of spinors, as obtained from the (zero-dimensional) Clifford algebra/spin representation theory described above. Such plane-wave solutions (or other solutions) of the differential equations can then properly be called fermions; fermions have the algebraic qualities of spinors. By general convention, the terms "fermion" and "spinor" are often used interchangeably in physics, as synonyms of one-another.

It appears that all fundamental particles in nature that are spin-1/2 are described by the Dirac equation, with the possible exception of the neutrino. There does not seem to be any "a priori" reason why this would be the case. A perfectly valid choice for spinors would be the non-complexified version of , the Majorana spinor. There also does not seem to be any particular prohibition to having Weyl spinors appear in nature as fundamental particles.

The Dirac, Weyl, and Majorana spinors are interrelated, and their relation can be elucidated on the basis of real geometric algebra. Dirac and Weyl spinors are complex representations while Majorana spinors are real representations.

Weyl spinors are insufficient to describe massive particles, such as electrons, since the Weyl plane-wave solutions necessarily travel at the speed of light; for massive particles, the Dirac equation is needed. The initial construction of the Standard Model of particle physics starts with both the electron and the neutrino as massless Weyl spinors; the Higgs mechanism gives electrons a mass; the classical neutrino remained massless, and was thus an example of a Weyl spinor. However, because of observed neutrino oscillation, it is now believed that they are not Weyl spinors, but perhaps instead Majorana spinors. It is not known whether Weyl spinor fundamental particles exist in nature. 

The situation for condensed matter physics is different: one can can construct two and three-dimensional "spacetimes" in a large variety of different physical materials, ranging from semiconductors to far more exotic materials. In 2015, an international team led by Princeton University scientists announced that they had found a quasiparticle that behaves as a Weyl fermion.

One major mathematical application of the construction of spinors is to make possible the explicit construction of linear representations of the Lie algebras of the special orthogonal groups, and consequently spinor representations of the groups themselves. At a more profound level, spinors have been found to be at the heart of approaches to the Atiyah–Singer index theorem, and to provide constructions in particular for discrete series representations of semisimple groups.

The spin representations of the special orthogonal Lie algebras are distinguished from the tensor representations given by Weyl's construction by the weights. Whereas the weights of the tensor representations are integer linear combinations of the roots of the Lie algebra, those of the spin representations are half-integer linear combinations thereof. Explicit details can be found in the spin representation article.

The spinor can be described, in simple terms, as "vectors of a space the transformations of which are related in a particular way to rotations in physical space". Stated differently:
Several ways of illustrating everyday analogies have been formulated in terms of the plate trick, tangloids and other examples of orientation entanglement.

Nonetheless, the concept is generally considered notoriously difficult to understand, as illustrated by Michael Atiyah's statement that is recounted by Dirac's biographer Graham Farmelo:
The most general mathematical form of spinors was discovered by Élie Cartan in 1913. The word "spinor" was coined by Paul Ehrenfest in his work on quantum physics.

Spinors were first applied to mathematical physics by Wolfgang Pauli in 1927, when he introduced his spin matrices. The following year, Paul Dirac discovered the fully relativistic theory of electron spin by showing the connection between spinors and the Lorentz group. By the 1930s, Dirac, Piet Hein and others at the Niels Bohr Institute (then known as the Institute for Theoretical Physics of the University of Copenhagen) created toys such as Tangloids to teach and model the calculus of spinors.

Spinor spaces were represented as left ideals of a matrix algebra in 1930, by G. Juvet and by Fritz Sauter. More specifically, instead of representing spinors as complex-valued 2D column vectors as Pauli had done, they represented them as complex-valued 2 × 2 matrices in which only the elements of the left column are non-zero. In this manner the spinor space became a minimal left ideal in .

In 1947 Marcel Riesz constructed spinor spaces as elements of a minimal left ideal of Clifford algebras. In 1966/1967, David Hestenes replaced spinor spaces by the even subalgebra Cℓ(R) of the spacetime algebra Cℓ(R). As of the 1980s, the theoretical physics group at Birkbeck College around David Bohm and Basil Hiley has been developing algebraic approaches to quantum theory that build on Sauter and Riesz' identification of spinors with minimal left ideals.

Some simple examples of spinors in low dimensions arise from considering the even-graded subalgebras of the Clifford algebra . This is an algebra built up from an orthonormal basis of mutually orthogonal vectors under addition and multiplication, "p" of which have norm +1 and "q" of which have norm −1, with the product rule for the basis vectors

The Clifford algebra Cℓ(R) is built up from a basis of one unit scalar, 1, two orthogonal unit vectors, "σ" and "σ", and one unit pseudoscalar . From the definitions above, it is evident that , and .

The even subalgebra Cℓ(R), spanned by "even-graded" basis elements of Cℓ(R), determines the space of spinors via its representations. It is made up of real linear combinations of 1 and "σ""σ". As a real algebra, Cℓ(R) is isomorphic to the field of complex numbers C. As a result, it admits a conjugation operation (analogous to complex conjugation), sometimes called the "reverse" of a Clifford element, defined by

which, by the Clifford relations, can be written

The action of an even Clifford element on vectors, regarded as 1-graded elements of Cℓ(R), is determined by mapping a general vector to the vector

where "γ" is the conjugate of "γ", and the product is Clifford multiplication. In this situation, a spinor is an ordinary complex number. The action of "γ" on a spinor "φ" is given by ordinary complex multiplication:

An important feature of this definition is the distinction between ordinary vectors and spinors, manifested in how the even-graded elements act on each of them in different ways. In general, a quick check of the Clifford relations reveals that even-graded elements conjugate-commute with ordinary vectors:

On the other hand, comparing with the action on spinors , "γ" on ordinary vectors acts as the "square" of its action on spinors.

Consider, for example, the implication this has for plane rotations. Rotating a vector through an angle of "θ" corresponds to , so that the corresponding action on spinors is via . In general, because of logarithmic branching, it is impossible to choose a sign in a consistent way. Thus the representation of plane rotations on spinors is two-valued.

In applications of spinors in two dimensions, it is common to exploit the fact that the algebra of even-graded elements (that is just the ring of complex numbers) is identical to the space of spinors. So, by abuse of language, the two are often conflated. One may then talk about "the action of a spinor on a vector." In a general setting, such statements are meaningless. But in dimensions 2 and 3 (as applied, for example, to computer graphics) they make sense.





The Clifford algebra Cℓ(R) is built up from a basis of one unit scalar, 1, three orthogonal unit vectors, "σ", "σ" and "σ", the three unit bivectors "σ""σ", "σ""σ", "σ""σ" and the pseudoscalar . It is straightforward to show that , and .

The sub-algebra of even-graded elements is made up of scalar dilations,

and vector rotations

where

corresponds to a vector rotation through an angle "θ" about an axis defined by a unit vector .

As a special case, it is easy to see that, if , this reproduces the "σ""σ" rotation considered in the previous section; and that such rotation leaves the coefficients of vectors in the "σ" direction invariant, since

The bivectors "σ""σ", "σ""σ" and "σ""σ" are in fact Hamilton's quaternions i, j and k, discovered in 1843:

With the identification of the even-graded elements with the algebra H of quaternions, as in the case of two dimensions the only representation of the algebra of even-graded elements is on itself. Thus the (real) spinors in three-dimensions are quaternions, and the action of an even-graded element on a spinor is given by ordinary quaternionic multiplication.

Note that the expression (1) for a vector rotation through an angle , "the angle appearing in γ was halved". Thus the spinor rotation (ordinary quaternionic multiplication) will rotate the spinor through an angle one-half the measure of the angle of the corresponding vector rotation. Once again, the problem of lifting a vector rotation to a spinor rotation is two-valued: the expression (1) with in place of "θ"/2 will produce the same vector rotation, but the negative of the spinor rotation.

The spinor/quaternion representation of rotations in 3D is becoming increasingly prevalent in computer geometry and other applications, because of the notable brevity of the corresponding spin matrix, and the simplicity with which they can be multiplied together to calculate the combined effect of successive rotations about different axes.

A space of spinors can be constructed explicitly with concrete and abstract constructions. The
equivalence of these constructions are a consequence of the uniqueness of the spinor representation of the complex Clifford algebra. For a complete example in dimension 3, see spinors in three dimensions.

Given a vector space "V" and a quadratic form "g" an explicit matrix representation of the Clifford algebra can be defined as follows. Choose an orthonormal basis for "V" i.e. where and for . Let . Fix a set of matrices such that (i.e. fix a convention for the gamma matrices). Then the assignment extends uniquely to an algebra homomorphism by sending the monomial in the Clifford algebra to the product of matrices and extending linearly. The space on which the gamma matrices act is now a space of spinors. One needs to construct such matrices explicitly, however. In dimension 3, defining the gamma matrices to be the Pauli sigma matrices gives rise to the familiar two component spinors used in non relativistic quantum mechanics. Likewise using the Dirac gamma matrices gives rise to the 4 component Dirac spinors used in 3+1 dimensional relativistic quantum field theory. In general, in order to define gamma matrices of the required kind, one can use the Weyl–Brauer matrices.

In this construction the representation of the Clifford algebra , the Lie algebra , and the Spin group , all depend on the choice of the orthonormal basis and the choice of the gamma matrices. This can cause confusion over conventions, but invariants like traces are independent of choices. In particular, all physically observable quantities must be independent of such choices. In this construction a spinor can be represented as a vector of 2 complex numbers and is denoted with spinor indices (usually "α", "β", "γ"). In the physics literature, abstract spinor indices are often used to denote spinors even when an abstract spinor construction is used.

There are at least two different, but essentially equivalent, ways to define spinors abstractly. One approach seeks to identify the minimal ideals for the left action of on itself. These are subspaces of the Clifford algebra of the form , admitting the evident action of by left-multiplication: . There are two variations on this theme: one can either find a primitive element that is a nilpotent element of the Clifford algebra, or one that is an idempotent. The construction via nilpotent elements is more fundamental in the sense that an idempotent may then be produced from it. In this way, the spinor representations are identified with certain subspaces of the Clifford algebra itself. The second approach is to construct a vector space using a distinguished subspace of , and then specify the action of the Clifford algebra "externally" to that vector space.

In either approach, the fundamental notion is that of an isotropic subspace . Each construction depends on an initial freedom in choosing this subspace. In physical terms, this corresponds to the fact that there is no measurement protocol that can specify a basis of the spin space, even if a preferred basis of is given.

As above, we let be an -dimensional complex vector space equipped with a nondegenerate bilinear form. If is a real vector space, then we replace by its complexification and let denote the induced bilinear form on . Let be a maximal isotropic subspace, i.e. a maximal subspace of such that . If is even, then let be an isotropic subspace complementary to . If is odd, let be a maximal isotropic subspace with , and let be the orthogonal complement of . In both the even- and odd-dimensional cases and have dimension . In the odd-dimensional case, is one-dimensional, spanned by a unit vector .

Since "W" is isotropic, multiplication of elements of "W" inside is skew. Hence vectors in "W" anti-commute, and is just the exterior algebra Λ"W". Consequently, the "k"-fold product of "W" with itself, "W", is one-dimensional. Let "ω" be a generator of "W". In terms of a basis of in "W", one possibility is to set

Note that (i.e., "ω" is nilpotent of order 2), and moreover, for all . The following facts can be proven easily:

In detail, suppose for instance that "n" is even. Suppose that "I" is a non-zero left ideal contained in . We shall show that "I" must be equal to by proving that it contains a nonzero scalar multiple of "ω".

Fix a basis "w" of "W" and a complementary basis "w"′ of "W" so that

Note that any element of "I" must have the form "αω", by virtue of our assumption that . Let be any such element. Using the chosen basis, we may write

where the "a" are scalars, and the "B" are auxiliary elements of the Clifford algebra. Observe now that the product
Pick any nonzero monomial "a" in the expansion of "α" with maximal homogeneous degree in the elements "w":
then
is a nonzero scalar multiple of "ω", as required.

Note that for "n" even, this computation also shows that
as a vector space. In the last equality we again used that "W" is isotropic. In physics terms, this shows that Δ is built up like a Fock space by creating spinors using anti-commuting creation operators in "W" acting on a vacuum "ω".

The computations with the minimal ideal construction suggest that a spinor representation can
also be defined directly using the exterior algebra of the isotropic subspace "W".
Let denote the exterior algebra of "W" considered as vector space only. This will be the spin representation, and its elements will be referred to as spinors.

The action of the Clifford algebra on Δ is defined first by giving the action of an element of "V" on Δ, and then showing that this action respects the Clifford relation and so extends to a homomorphism of the full Clifford algebra into the endomorphism ring End(Δ) by the universal property of Clifford algebras. The details differ slightly according to whether the dimension of "V" is even or odd.

When dim("V") is even, where "W"′ is the chosen isotropic complement. Hence any decomposes uniquely as with and . The action of "v" on a spinor is given by
where "i"("w"′) is interior product with "w"′ using the non degenerate quadratic form to identify "V" with "V", and ε(w) denotes the exterior product. It may be verified that
and so "c" respects the Clifford relations and extends to a homomorphism from the Clifford algebra to End(Δ).

The spin representation Δ further decomposes into a pair of irreducible complex representations of the Spin group (the half-spin representations, or Weyl spinors) via

When dim("V") is odd, , where "U" is spanned by a unit vector "u" orthogonal to "W". The Clifford action "c" is defined as before on , while the Clifford action of (multiples of) "u" is defined by
As before, one verifies that "c" respects the Clifford relations, and so induces a homomorphism.

If the vector space "V" has extra structure that provides a decomposition of its complexification into two maximal isotropic subspaces, then the definition of spinors (by either method) becomes natural.

The main example is the case that the real vector space "V" is a hermitian vector space , i.e., "V" is equipped with a complex structure "J" that is an orthogonal transformation with respect to the inner product "g" on "V". Then splits in the ±"i" eigenspaces of "J". These eigenspaces are isotropic for the complexification of "g" and can be identified with the complex vector space and its complex conjugate . Therefore, for a hermitian vector space the vector space Λ (as well as its complex conjugate Λ"V") is a spinor space for the underlying real euclidean vector space.

With the Clifford action as above but with contraction using the hermitian form, this construction gives a spinor space at every point of an almost Hermitian manifold and is the reason why every almost complex manifold (in particular every symplectic manifold) has a Spin structure. Likewise, every complex vector bundle on a manifold carries a Spin structure.

A number of Clebsch–Gordan decompositions are possible on the tensor product of one spin representation with another. These decompositions express the tensor product in terms of the alternating representations of the orthogonal group.

For the real or complex case, the alternating representations are

In addition, for the real orthogonal groups, there are three characters (one-dimensional representations)

The Clebsch–Gordan decomposition allows one to define, among other things:

If is even, then the tensor product of Δ with the contragredient representation decomposes as
which can be seen explicitly by considering (in the Explicit construction) the action of the Clifford algebra on decomposable elements . The rightmost formulation follows from the transformation properties of the Hodge star operator. Note that on restriction to the even Clifford algebra, the paired summands are isomorphic, but under the full Clifford algebra they are not.

There is a natural identification of Δ with its contragredient representation via the conjugation in the Clifford algebra:
So also decomposes in the above manner. Furthermore, under the even Clifford algebra, the half-spin representations decompose

For the complex representations of the real Clifford algebras, the associated reality structure on the complex Clifford algebra descends to the space of spinors (via the explicit construction in terms of minimal ideals, for instance). In this way, we obtain the complex conjugate of the representation Δ, and the following isomorphism is seen to hold:

In particular, note that the representation Δ of the orthochronous spin group is a unitary representation. In general, there are Clebsch–Gordan decompositions

In metric signature , the following isomorphisms hold for the conjugate half-spin representations
Using these isomorphisms, one can deduce analogous decompositions for the tensor products of the half-spin representations .

If is odd, then
In the real case, once again the isomorphism holds
Hence there is a Clebsch–Gordan decomposition (again using the Hodge star to dualize) given by

There are many far-reaching consequences of the Clebsch–Gordan decompositions of the spinor spaces. The most fundamental of these pertain to Dirac's theory of the electron, among whose basic requirements are





</doc>
<doc id="29278" url="https://en.wikipedia.org/wiki?curid=29278" title="Safety engineering">
Safety engineering

Safety engineering is an engineering discipline which assures that engineered systems provide acceptable levels of safety. It is strongly related to industrial engineering/systems engineering, and the subset system safety engineering. Safety engineering assures that a life-critical system behaves as needed, even when components fail.

Analysis techniques can be split into two categories: qualitative and quantitative methods. Both approaches share the goal of finding causal dependencies between a hazard on system level and failures of individual components. Qualitative approaches focus on the question "What must go wrong, such that a system hazard may occur?", while quantitative methods aim at providing estimations about probabilities, rates and/or severity of consequences.

The complexity of the technical systems such as Improvements of Design and Materials, Planned Inspections, Fool-proof design, and Backup Redundancy decreases risk and increases the cost. The risk can be decreased to ALARA (as low as reasonably achievable) or ALAPA (as low as practically achievable) levels.

Traditionally, safety analysis techniques rely solely on skill and expertise of the safety engineer. In the last decade model-based approaches have become prominent. In contrast to traditional methods, model-based techniques try to derive relationships between causes and consequences from some sort of model of the system.

The two most common fault modeling techniques are called failure mode and effects analysis and fault tree analysis. These techniques are just ways of finding problems and of making plans to cope with failures, as in probabilistic risk assessment. One of the earliest complete studies using this technique on a commercial nuclear plant was the WASH-1400 study, also known as the Reactor Safety Study or the Rasmussen Report.

Failure Mode and Effects Analysis (FMEA) is a bottom-up, inductive analytical method which may be performed at either the functional or piece-part level. For functional FMEA, failure modes are identified for each function in a system or equipment item, usually with the help of a functional block diagram. For piece-part FMEA, failure modes are identified for each piece-part component (such as a valve, connector, resistor, or diode). The effects of the failure mode are described, and assigned a probability based on the failure rate and failure mode ratio of the function or component. This quantiazation is difficult for software ---a bug exists or not, and the failure models used for hardware components do not apply. Temperature and age and manufacturing variability affect a resistor; they do not affect software.

Failure modes with identical effects can be combined and summarized in a Failure Mode Effects Summary. When combined with criticality analysis, FMEA is known as Failure Mode, Effects, and Criticality Analysis or FMECA, pronounced "fuh-MEE-kuh".

Fault tree analysis (FTA) is a top-down, deductive analytical method. In FTA, initiating primary events such as component failures, human errors, and external events are traced through Boolean logic gates to an undesired top event such as an aircraft crash or nuclear reactor core melt. The intent is to identify ways to make top events less probable, and verify that safety goals have been achieved.

Fault trees are a logical inverse of success trees, and may be obtained by applying de Morgan's theorem to success trees (which are directly related to reliability block diagrams).

FTA may be qualitative or quantitative. When failure and event probabilities are unknown, qualitative fault trees may be analyzed for minimal cut sets. For example, if any minimal cut set contains a single base event, then the top event may be caused by a single failure. Quantitative FTA is used to compute top event probability, and usually requires computer software such as CAFTA from the Electric Power Research Institute or SAPHIRE from the Idaho National Laboratory.

Some industries use both fault trees and event trees. An event tree starts from an undesired initiator (loss of critical supply, component failure etc.) and follows possible further system events through to a series of final consequences. As each new event is considered, a new node on the tree is added with a split of probabilities of taking either branch. The probabilities of a range of "top events" arising from the initial event can then be seen.

The offshore oil and gas industry uses a qualitative safety systems analysis technique to ensure the protection of offshore production systems and platforms. The analysis is used during the design phase to identify process engineering hazards together with risk mitigation measures. The methodology is described in the American Petroleum Institute Recommended Practice 14C "Analysis, Design, Installation, and Testing of Basic Surface Safety Systems for Offshore Production Platforms."

The technique uses system analysis methods to determine the safety requirements to protect any individual process component, e.g. a vessel, pipeline, or pump. The safety requirements of individual components are integrated into a complete platform safety system, including liquid containment and emergency support systems such as fire and gas detection.

The first stage of the analysis identifies individual process components, these can include: flowlines, headers, pressure vessels, atmospheric vessels, fired heaters, exhaust heated components, pumps, compressors, pipelines and heat exchangers. Each component is subject to a safety analysis to identify undesirable events (equipment failure, process upsets, etc.) for which protection must be provided. The analysis also identifies a detectable condition (e.g. high pressure) which is used to initiate actions to prevent or minimize the effect of undesirable events. A Safety Analysis Table (SAT) for pressure vessels includes the following details.

Other undesirable events for a pressure vessel are under-pressure, gas blowby, leak, and excess temperature together with their associated causes and detectable conditions.

Once the events, causes and detectable conditions have been identified the next stage of the methodology uses a Safety Analysis Checklist (SAC) for each component. This lists the safety devices that may be required or factors that negate the need for such a device. For example, for the case of liquid overflow from a vessel (as above) the SAC identifies:


The analysis ensures that two levels of protection are provided to mitigate each undesirable event. For example, for a pressure vessel subjected to over-pressure the primary protection would be a PSH (pressure switch high) to shut off inflow to the vessel, secondary protection would be provided by a pressure safety valve (PSV) on the vessel.

The next stage of the analysis relates all the sensing devices, shutdown valves (ESVs), trip systems and emergency support systems in the form of a Safety Analysis Function Evaluation (SAFE) chart.

X denotes that the detection device on the left (e.g. PSH) initiates the shutdown or warning action on the top right (e.g. ESV closure).

The SAFE chart constitutes the basis of Cause and Effect Charts which relate the sensing devices to shutdown valves and plant trips which defines the functional architecture of the process shutdown system.

The methodology also specifies the systems testing that is necessary to ensure the functionality of the protection systems.

API RP 14C was first published in June 1974. The 8th edition was published in February 2017. API RP 14C was adapted as ISO standard ISO 10418 in 1993 entitled "Petroleum and natural gas industries — Offshore production installations — Analysis, design, installation and testing of basic surface process safety systems." The latest 2003 edition of ISO 10418 is currently (2019) undergoing revision.

Typically, safety guidelines prescribe a set of steps, deliverable documents, and exit criterion focused around planning, analysis and design, implementation, verification and validation, configuration management, and quality assurance activities for the development of a safety-critical system. In addition, they typically formulate expectations regarding the creation and use of traceability in the project. For example, depending upon the criticality level of a requirement, the US Federal Aviation Authority guideline DO-178B/C requires traceability from requirements to design, and from requirements to source code and executable object code for software components of a system. Thereby, higher quality traceability information can simplify the certification process and help to establish trust in the maturity of the applied development process.

Usually a failure in safety-certified systems is acceptable if, on average, less than one life per 10 hours of continuous operation is lost to failure.{as per FAA document AC 25.1309-1A} Most Western nuclear reactors, medical equipment, and commercial aircraft are certified to this level. The cost versus loss of lives has been considered appropriate at this level (by FAA for aircraft systems under Federal Aviation Regulations).

Once a failure mode is identified, it can usually be mitigated by adding extra or redundant equipment to the system. For example, nuclear reactors contain dangerous radiation, and nuclear reactions can cause so much heat that no substance might contain them. Therefore, reactors have emergency core cooling systems to keep the temperature down, shielding to contain the radiation, and engineered barriers (usually several, nested, surmounted by a containment building) to prevent accidental leakage. Safety-critical systems are commonly required to permit no single event or component failure to result in a catastrophic failure mode.

Most biological organisms have a certain amount of redundancy: multiple organs, multiple limbs, etc.

For any given failure, a fail-over or redundancy can almost always be designed and incorporated into a system.

There are two categories of techniques to reduce the probability of failure:
Fault avoidance techniques increase the reliability of individual items (increased design margin, de-rating, etc.).
Fault tolerance techniques increase the reliability of the system as a whole (redundancies, barriers, etc.).

Safety engineering and reliability engineering have much in common, but safety is not reliability. If a medical device fails, it should fail safely; other alternatives will be available to the surgeon. If the engine on a single-engine aircraft fails, there is no backup. Electrical power grids are designed for both safety and reliability; telephone systems are designed for reliability, which becomes a safety issue when emergency (e.g. US "911") calls are placed.

Probabilistic risk assessment has created a close relationship between safety and reliability. Component reliability, generally defined in terms of component failure rate, and external event probability are both used in quantitative safety assessment methods such as FTA. Related probabilistic methods are used to determine system Mean Time Between Failure (MTBF), system availability, or probability of mission success or failure. Reliability analysis has a broader scope than safety analysis, in that non-critical failures are considered. On the other hand, higher failure rates are considered acceptable for non-critical systems.

Safety generally cannot be achieved through component reliability alone. Catastrophic failure probabilities of 10 per hour correspond to the failure rates of very simple components such as resistors or capacitors. A complex system containing hundreds or thousands of components might be able to achieve a MTBF of 10,000 to 100,000 hours, meaning it would fail at 10 or 10 per hour. If a system failure is catastrophic, usually the only practical way to achieve 10 per hour failure rate is through redundancy.

When adding equipment is impractical (usually because of expense), then the least expensive form of design is often "inherently fail-safe". That is, change the system design so its failure modes are not catastrophic. Inherent fail-safes are common in medical equipment, traffic and railway signals, communications equipment, and safety equipment.

The typical approach is to arrange the system so that ordinary single failures cause the mechanism to shut down in a safe way (for nuclear power plants, this is termed a passively safe design, although more than ordinary failures are covered). Alternately, if the system contains a hazard source such as a battery or rotor, then it may be possible to remove the hazard from the system so that its failure modes cannot be catastrophic. The U.S. Department of Defense Standard Practice for System Safety (MIL–STD–882) places the highest priority on elimination of hazards through design selection.

One of the most common fail-safe systems is the overflow tube in baths and kitchen sinks. If the valve sticks open, rather than causing an overflow and damage, the tank spills into an overflow. Another common example is that in an elevator the cable supporting the car keeps spring-loaded brakes open. If the cable breaks, the brakes grab rails, and the elevator cabin does not fall.

Some systems can never be made fail safe, as continuous availability is needed. For example, loss of engine thrust in flight is dangerous. Redundancy, fault tolerance, or recovery procedures are used for these situations (e.g. multiple independent controlled and fuel fed engines). This also makes the system less sensitive for the reliability prediction errors or quality induced uncertainty for the separate items. On the other hand, failure detection & correction and avoidance of common cause failures becomes here increasingly important to ensure system level reliability.





</doc>
<doc id="29279" url="https://en.wikipedia.org/wiki?curid=29279" title="SIGGRAPH">
SIGGRAPH

SIGGRAPH (Special Interest Group on Computer GRAPHics and Interactive Techniques) is an annual conference on computer graphics (CG) organized by the ACM SIGGRAPH, starting in 1974. The main conference is held in North America; SIGGRAPH Asia, a second yearly conference, has been held since 2008 in countries throughout Asia.

The conference incorporates both academic presentations as well as an industry trade show. Other events at the conference include educational courses and panel discussions on recent topics in computer graphics and interactive techniques.

The SIGGRAPH conference proceedings, which are published in the ACM Transactions on Graphics, has one of the highest impact factors among academic publications in the field of computer graphics. The paper acceptance rate for SIGGRAPH has historically been between 17% and 29%, with the average accept rate between 2015 and 2019 of 27%. The submitted papers are peer-reviewed under a process that was historically single-blind, but has recently changed to double-blind. The papers accepted for presentation at SIGGRAPH are printed since 2003 in a special issue of the "ACM Transactions on Graphics" journal. 
Prior to 1992, SIGGRAPH papers were printed as part of the "Computer Graphics" publication; between 1993 and 2001, there was a dedicated "SIGGRAPH Conference Proceedings" series of publications.

SIGGRAPH has several awards programs to recognize contributions to computer graphics. The most prestigious is the Steven Anson Coons Award for Outstanding Creative Contributions to Computer Graphics. It has been awarded every two years since 1983 to recognize an individual's lifetime achievement in computer graphics.

The SIGGRAPH conference experienced significant growth starting in the 1970s, peaking around the turn of the century. A second conference, SIGGRAPH Asia, started in 2008.




</doc>
<doc id="29285" url="https://en.wikipedia.org/wiki?curid=29285" title="Semtex">
Semtex

Semtex is a general-purpose plastic explosive containing RDX and PETN. It is used in commercial blasting, demolition, and in certain military applications. 

Semtex was developed and manufactured in Czechoslovakia, originally under the name B 1 and then under the "Semtex" designation since 1964, labeled as "SEMTEX 1A", since 1967 as "SEMTEX H" and since 1987 as "SEMTEX 10".

Originally developed for Czechoslovak military use and export, Semtex eventually became popular with paramilitary groups and rebels or terrorists because prior to 2000 it was extremely difficult to detect, as in the case of Pan Am Flight 103.

The composition of the two most common variants differ according to their use. The 1A (or 10) variant is used for mining, and is based mostly on crystalline PETN. The version 1AP and 2P are formed as hexagonal booster charges; a special assembly of PETN and wax inside the charge assures high reliability for detonating cord or detonator. The H (or SE) variant is intended for explosion hardening.

Semtex was invented in the late 1950s by Stanislav Brebera and Radim Fukátko, chemists at VCHZ Synthesia, Czechoslovakia (now Czech Republic). The explosive is named after Semtín, a suburb of Pardubice where the mixture was first manufactured starting in 1964. The plant was later renamed to become Explosia a.s., a subsidiary of Synthesia.

Semtex was very similar to other plastic explosives, especially C-4, in being highly malleable; but it is usable over a greater temperature range than other plastic explosives, since it stays plastic between −40 and +60 °C. It is also waterproof. There are visual differences between Semtex and other plastic explosives, too: while C-4 is off-white in colour, Semtex is red or brick-orange.

The new explosive was widely exported, notably to the government of North Vietnam, which received 14 tons during the Vietnam War. However, the main consumer was Libya; about 700 tons of Semtex were exported to Libya between 1975 and 1981 by Omnipol. It has also been used by Islamic militants in the Middle East and by the Provisional Irish Republican Army (PIRA) and the Irish National Liberation Army in Northern Ireland.

Exports fell after the name became closely associated with terrorist attacks. Export of Semtex was progressively tightened and since 2002 all of Explosia's sales have been controlled by a government ministry. , only approximately 10 tons of Semtex were produced annually, almost all for domestic use. On December 21, 1988, 12 ounces (340g) of Semtex brought down a Boeing 747 over Lockerbie, Scotland killing all 259 aboard the aircraft and 11 on the ground.

Also in response to international agreements, Semtex has a detection taggant added to produce a distinctive vapor signature to aid detection. First, ethylene glycol dinitrate was used, later switched to 2,3-dimethyl-2,3-dinitrobutane (DMDNB) or "p"-mononitrotoluene (1-methyl-4-nitrobenzene), which is used currently. According to the manufacturer, the taggant agent was voluntarily being added by 1991, years before the protocol became compulsory. Batches of Semtex made before 1990, however, are untagged, though it is not known whether there are still major stocks of such old batches of Semtex. According to the manufacturer, even this untagged Semtex can now be detected. The shelf life of Semtex was reduced from ten years before the 1990s to five years now. Explosia states that there is no compulsory tagging allowing reliable post-detonation detection of a certain plastic explosive (such as incorporating a unique metallic code into the mass of the explosive), so Semtex is not tagged in this way.

On 25 May 1997, Bohumil Šole, a scientist who claimed to have been involved with inventing Semtex, committed suicide at the spa in Jeseník by blowing himself up with explosives. Šole, 63, was being treated there for psychological problems. It was unclear what explosives were used. Twenty other people were hurt in the explosion, while six were seriously injured. According to the manufacturer, Explosia, he was not a member of the team that developed the explosive in the 1960s.

According to the producer's 2017 catalog, several variants of Semtex are offered: Semtex 1A, Semtex 1H, Semtex 10, Semtex 10-SE, Semtex S 30, Semtex C-4, Semtex PW 4, and Semtex 90.



</doc>
<doc id="29286" url="https://en.wikipedia.org/wiki?curid=29286" title="Schedl">
Schedl

Schedl is a German surname. Notable people with the surname include: 



</doc>
<doc id="29287" url="https://en.wikipedia.org/wiki?curid=29287" title="Lehi (militant group)">
Lehi (militant group)

Lehi (; "Lohamei Herut Israel – Lehi", "Fighters for the Freedom of Israel – Lehi"), often known pejoratively as the Stern Gang, was a Zionist paramilitary organization founded by Avraham ("Yair") Stern in Mandatory Palestine. Its avowed aim was to evict the British authorities from Palestine by resort to force, allowing unrestricted immigration of Jews and the formation of a Jewish state, a "new totalitarian Hebrew republic". It was initially called the National Military Organization in Israel, upon being founded in August 1940, but was renamed Lehi one month later. The group referred to its members as terrorists and admitted to having used terrorist attacks.

Lehi split from the Irgun militant group in 1940 in order to continue fighting the British during World War II. Lehi initially sought an alliance with Fascist Italy and Nazi Germany, offering to fight alongside them against the British in return for the transfer of all Jews from Nazi-occupied Europe to Palestine. Believing that Nazi Germany was a lesser enemy of the Jews than Britain, Lehi twice attempted to form an alliance with the Nazis. During World War II, it declared that it would establish a Jewish state based upon "nationalist and totalitarian principles". After Stern's death in 1942, the new leadership of Lehi began to move it towards support for Joseph Stalin's Soviet Union. In 1944, Lehi officially declared its support for National Bolshevism. It said that its National Bolshevism involved an amalgamation of left-wing and right-wing political elements – Stern said Lehi incorporated elements of both the left and the right – however this change was unpopular and Lehi began to lose support as a result.

Lehi and the Irgun were jointly responsible for the massacre in Deir Yassin. Lehi assassinated Lord Moyne, British Minister Resident in the Middle East, and made many other attacks on the British in Palestine. On 29 May 1948, the government of Israel, having inducted its activist members into the Israel Defense Forces, formally disbanded Lehi, though some of its members carried out one more terrorist act, the assassination of Folke Bernadotte some months later, an act condemned by Bernadotte's replacement as mediator, Ralph Bunche. After the assassination, the new Israeli government declared Lehi a terrorist organization, arresting some 200 members and convicting some of the leaders. Just before the first Israeli elections in January 1949, a general amnesty to Lehi members was granted by the government. In 1980, Israel instituted a military decoration, an "award for activity in the struggle for the establishment of Israel", the Lehi ribbon. Former Lehi leader Yitzhak Shamir became Prime Minister of Israel in 1983.

Lehi was created in August 1940 by Avraham Stern. Stern had been a member of the Irgun ("Irgun Tsvai Leumi" – "National Military Organization") high command. Zeev Jabotinsky, then the Irgun's supreme commander, had decided that diplomacy and working with Britain would best serve the Zionist cause. World War II was in progress, and Britain was fighting Nazi Germany. The Irgun suspended its underground military activities against the British for the duration of the war.

Stern argued that the time for Zionist diplomacy was over and that it was time for armed struggle against the British. Like other Zionists, he objected to the White Paper of 1939, which restricted both Jewish immigration and Jewish land purchases in Palestine. For Stern, "no difference existed between Hitler and Chamberlain, between Dachau or Buchenwald and sealing the gates of Eretz Israel."

Stern wanted to open Palestine to all Jewish refugees from Europe, and considered this as by far the most important issue of the day. Britain would not allow this. Therefore, he concluded, the "Yishuv" (Jews of Palestine) should fight the British rather than support them in the war. When the Irgun made a truce with the British, Stern left the Irgun to form his own group, which he called "Irgun Tsvai Leumi B'Yisrael" ("National Military Organization in Israel"), later "Lohamei Herut Israel" ("Fighters for the Freedom of Israel"). In September 1940, the organization was officially named "Lehi", the Hebrew acronym of the latter name.

Stern and his followers believed that dying for the "foreign occupier" who was obstructing the creation of the Jewish State was useless. They differentiated between "enemies of the Jewish people" (the British) and "Jew haters" (the Nazis), believing that the former needed to be defeated and the latter manipulated.

In 1940, the idea of the Final Solution was still "unthinkable", and Stern believed that Hitler wanted to make Germany "judenrein" through emigration, as opposed to extermination. In December 1940, Lehi even contacted Germany with a proposal to aid German conquest in the Middle East in return for recognition of a Jewish state open to unlimited immigration.
Lehi had three main goals:

Lehi believed in its early years that its goals would be achieved by finding a strong international ally that would expel the British from Palestine, in return for Jewish military help; this would require the creation of a broad and organised military force "demonstrating its desire for freedom through military operations."

Lehi also referred to themselves as 'terrorists' and may have been one of the last organizations to do so.

An article titled "Terror" in the Lehi underground newspaper "He Khazit" ("The Front") argued as follows:

Neither Jewish ethics nor Jewish tradition can disqualify terrorism as a means of combat. We are very far from having any moral qualms as far as our national war goes. We have before us the command of the Torah, whose morality surpasses that of any other body of laws in the world: "Ye shall blot them out to the last man."
But first and foremost, terrorism is for us a part of the political battle being conducted under the present circumstances, and it has a great part to play: speaking in a clear voice to the whole world, as well as to our wretched brethren outside this land, it proclaims our war against the occupier.
We are particularly far from this sort of hesitation in regard to an enemy whose moral perversion is admitted by all.

The article described the goals of terror:


Yitzhak Shamir, one of the three leaders of Lehi after Avraham Stern's assassination, argued for the legitimacy of Lehi's actions:
There are those who say that to kill [T.G.] Martin [a CID sergeant who had recognised Shamir in a lineup] is terrorism, but to attack an army camp is guerrilla warfare and to bomb civilians is professional warfare. But I think it is the same from the moral point of view. Is it better to drop an atomic bomb on a city than to kill a handful of persons? I don’t think so. But nobody says that President Truman was a terrorist. All the men we went for individually – Wilkin, Martin, MacMichael and others – were personally interested in succeeding in the fight against us.

So it was more efficient and more moral to go for selected targets. In any case, it was the only way we could operate, because we were so small. For us it was not a question of the professional honor of a soldier, it was the question of an idea, an aim that had to be achieved. We were aiming at a political goal. There are many examples of what we did to be found in the Bible – Gideon and Samson, for instance. This had an influence on our thinking. And we also learned from the history of other peoples who fought for their freedom – the Russian and Irish revolutionaries, Giuseppe Garibaldi and Josip Broz Tito.

Avraham Stern laid out the ideology of Lehi in the essay "18 Principles of Rebirth":




Unlike the left-wing Haganah and right-wing Irgun, Lehi members were not a homogeneous collective with a single political, religious, or economic ideology. They were a combination of militants united by the goal of liberating the land of Israel from British rule. Most Lehi leaders defined their organization as an anti-imperialism movement and stated that their opposition to British colonial rule in Palestine was not based on a particular policy but rather on the presence of a foreign power over the homeland of the Jewish people. Avraham Stern defined the British Mandate as "foreign rule" regardless of British policies and took a radical position against such imperialism even if it were to be benevolent.

In the early years of the state of Israel Lehi veterans could be found supporting nearly all political parties and some Lehi leaders founded a left-wing political party called the Fighters' List with Natan Yellin-Mor as its head. The party took part in the elections in January 1949 and won a single parliamentary seat. A number of Lehi veterans established the Semitic Action movement in 1956 which sought the creation of a regional federation encompassing Israel and its Arab neighbors on the basis of an anti-colonialist alliance with other indigenous inhabitants of the Middle East.

Some writers have stated that Lehi's true goals were the creation of a totalitarian state. Perlinger and Weinberg write that the organisation's ideology placed "its world view in the quasi-fascist radical Right, which is characterised by xenophobia, a national egotism that completely subordinates the individual to the needs of the nation, anti-liberalism, total denial of democracy and a highly centralised government." Perliger and Weinberg state that most Lehi members were admirers of the Italian Fascist movement. According to Kaplan and Penslar, Lehi's ideology was a mix of fascist and communist thought combined with racism and universalism.

Others counter these claims. They note that when Lehi founder Avraham Stern went to study in fascist Italy, he refused to join the for foreign students, even though members got large reductions in tuition.

According to Yaacov Shavit professor at the Department of Jewish History, Tel Aviv University articles in publications by Lehi wrote about Jewish "master race", contrasting them with Arabs who were seen as "nation of slaves" Sasha Polakow-Suransky writes about Lehi "Lehi was also unabashedly racist towards Arabs. Their publications described Jews as a master race and Arabs as a slave race." Lehi advocated mass expulsion of all Arabs from Palestine and Transjordan or even their physical annihilation.

Many Lehi combatants received professional training. Some attended the state military academy in Civitavecchia, in Fascist Italy. Others received military training from instructors of the Polish Armed Forces in 1938–1939. This training was conducted in Trochenbrod (Zofiówka) in Wołyń Voivodeship, Podębin near Łódź, and the forests around Andrychów. They were taught how to use explosives. One of them reported later: "Poles treated terrorism as a science. We have mastered mathematical principles of demolishing constructions made of concrete, iron, wood, bricks and dirt."

The group was initially unsuccessful. Early attempts to raise funds through criminal activities, including a bank robbery in Tel Aviv in 1940 and another robbery on 9 January 1942 in which Jewish passers-by were killed, brought about the temporary collapse of the group. An attempt to assassinate the head of the British secret police in Lod in which three police personnel were killed, two Jewish and one British, elicited a severe response from the British and Jewish establishments who collaborated against Lehi.

Stern's group was seen as a terrorist organisation by the British authorities, who instructed the Defence Security Office (the colonial branch of MI5) to track down its leaders. In 1942, Stern, after he was arrested, was shot dead in disputed circumstances by Inspector Geoffrey J. Morton of the CID. The arrest of several other members led momentarily to the group's eclipse, until it was revived after the September 1942 escape of two of its leaders, Yitzhak Shamir and Eliyahu Giladi, aided by two other escapees Natan Yellin-Mor (Friedman) and Israel Eldad (Sheib). (Giladi was later killed by Lehi under circumstances that remain mysterious.) Shamir's codename was "Michael", a reference to one of Shamir's heroes, Michael Collins. Lehi was guided by spiritual and philosophical leaders such as Uri Zvi Greenberg and Israel Eldad. After the killing of Giladi, the organization was led by a triumvirate of Eldad, Shamir, and Yellin-Mor.

Lehi adopted a non-socialist platform of Anti-Imperialist ideology. It viewed the continued British rule of Palestine as a violation of the Mandate's provision generally, and its restrictions on Jewish immigration to be an intolerable breach of international law. However they also targeted Jews whom they regarded as traitors, and during the 1948 Arab-Israeli War they joined in operations with the Haganah and Irgun against Arab targets, for example Deir Yassin.

According to a compilation by Nachman Ben-Yehuda, Lehi was responsible for 42 assassinations, more than twice as many as the Irgun and Haganah combined during the same period. Of those Lehi assassinations that Ben-Yehuda classified as political, more than half the victims were Jews.

Lehi also rejected the authority of the Jewish Agency for Israel and related organizations, operating entirely on its own throughout nearly all of its existence.

Lehi prisoners captured by the British generally refused to employ lawyers in their defense. The defendants would conduct their own defense, and would deny the right of the military court to try them, saying that in accordance with the Hague Convention they should be accorded the status of prisoners of war. For the same reason, Lehi prisoners refused to plead for amnesty, even when it was clear that this would have spared them the death penalty. In one case Moshe Barazani, a Lehi member, and Meir Feinstein, an Irgun member, committed suicide in prison with a grenade smuggled inside an orange so the British could not hang them.

In mid-1940, Stern became convinced that the Italians were interested in the establishment of a fascist Jewish state in Palestine. He conducted negotiations, he thought, with the Italians via an intermediary Moshe Rotstein, and drew up a document that became known as the "Jerusalem Agreement". In exchange for Italy's recognition of, and aid in obtaining, Jewish sovereignty over Palestine, Stern promised that Zionism would come under the aegis of Italian fascism, with Haifa as its base, and the Old City of Jerusalem under Vatican control, except for the Jewish quarter. In Heller's words, Stern's proposal would "turn the 'Kingdom of Israel' into a satellite of the Axis powers."

However, the "intermediary" Rotstein was in fact an agent of the Irgun, conducting a sting operation under the direction of the Irgun intelligence leader in Haifa, Israel Pritzker, in cooperation with the British. Secret British documents about the affair were uncovered by historian Eldad Harouvi (now director of the Palmach Archives) and confirmed by former Irgun intelligence officer Yitzhak Berman. When Rotstein's role later became clear, Lehi sentenced him to death and assigned Yaacov Eliav to kill him, but the assassination never took place. However, Pritzker was killed by Lehi in 1943.

Late in 1940, Lehi, having identified a common interest between the intentions of the new German order and Jewish national aspirations, proposed forming an alliance in World War II with Nazi Germany. The organization offered cooperation in the following terms: Lehi would rebel against the British, while Germany would recognize an independent Jewish state in Palestine/Eretz Israel, and all Jews leaving their homes in Europe, by their own will or because of government injunctions, could enter Palestine with no restriction of numbers. Late in 1940, Lehi representative Naftali Lubenchik went to Beirut to meet German official Werner Otto von Hentig. The Lehi documents outlined that its rule would be authoritarian and indicated similiarites between the organization and Nazis. Israel Eldad, one of the leading members of Lehi, wrote about Hitler "it is not Hitler who is the hater of the kingdom of Israel and the return to Zion, it is not Hitler who subjects us to the cruel fate of falling a second and a third time into Hitler's hands, but the British."

Stern also proposed recruiting some 40,000 Jews from occupied Europe to invade Palestine with German support to oust the British. On 11 January 1941, Vice Admiral Ralf von der Marwitz, the German Naval attaché in Turkey, filed a report (the "Ankara document") conveying an offer by Lehi to "actively take part in the war on Germany's side" in return for German support for "the establishment of the historic Jewish state on a national and totalitarian basis, bound by a treaty with the German Reich."

According to Yellin-Mor:Lubenchik did not take along any written memorandum for the German representatives. Had there been a need for one, he would have formulated it on the spot, since he was familiar with the episode of the Italian "intermediary" and with the numerous drafts connected with it. Apparently one of von Hentig's secretaries noted down the essence of the proposal in his own words.According to Joseph Heller, "The memorandum arising from their conversation is an entirely authentic document, on which the stamp of the 'IZL in Israel' is clearly embossed."

Von der Marwitz delivered the offer, classified as secret, to the German Ambassador in Turkey and on 21 January 1941 it was sent to Berlin. There was never any response.

A second attempt to contact the Nazis was made at the end of 1941, but it was even less successful. The emissary Yellin-Mor was arrested in Syria before he could carry out his mission.

This proposed alliance with Nazi Germany cost Lehi and Stern much support. The Stern Gang also had links with, and support from, the Vichy France Sûreté's Lebanese offices.

Even as the full scale of Nazi atrocities became more evident in 1943, Lehi refused to accept Hitler as main foe (as opposed to Great Britain).

As a group that never had over a few hundred members, Lehi relied on audacious but small-scale operations to bring their message home. They adopted the tactics of groups such as the Socialist Revolutionaries and the Combat Organization of the Polish Socialist Party in Czarist Russia, and the Irish Republican Army. To this end, Lehi conducted small-scale operations such as individual assassinations of British officials (notable targets included Lord Moyne, CID detectives, and Jewish "collaborators"), and random shootings against soldiers and police officers. Another strategy, adopted in 1946, was to send bombs in the mail to British politicians. Other actions included sabotaging infrastructure targets: bridges, railroads, telephone and telegraph lines, and oil refineries, as well as the use of vehicle bombs against British military, police, and administrative targets. Lehi financed its operations from private donations, extortion, and bank robbery. Its campaign of violence lasted from 1944 to 1948. Initially conducted together with the Irgun, it included a six-month suspension to avoid being targeted by the Haganah during the Hunting Season, and later operated jointly with the Haganah and Irgun under the Jewish Resistance Movement. After the Jewish Resistance Movement was dissolved, it operated independently as part of the general Jewish insurgency in Palestine.

On 6 November 1944, Lehi assassinated Lord Moyne, the British Minister Resident in the Middle East, in Cairo. Moyne was the highest ranking British official in the region. Yitzhak Shamir claimed later that Moyne was assassinated because of his support for a Middle Eastern Arab Federation and anti-Semitic lectures in which Arabs were held to be racially superior to Jews. The assassination rocked the British government, and outraged Winston Churchill, the British Prime Minister. The two assassins, Eliahu Bet-Zouri and Eliahu Hakim were captured and used their trial as a platform to make public their political propaganda. They were executed. In 1975 their bodies were returned to Israel and given a state funeral. In 1982, postage stamps were issued for 20 Olei Hagardom, including Bet-Zouri and Hakim, in a souvenir sheet called "Martyrs of the struggle for Israel's independence."

On 25 April 1946, a Lehi unit attacked a car park in Tel Aviv occupied by the British 6th Airborne Division. Under a barrage of heavy covering fire, Lehi fighters broke into the car park, shot soldiers they encountered at close range, stole rifles from arms racks, laid mines to cover the retreat, and withdrew. Seven soldiers were killed in the attack, which caused widespread outrage among the British security forces in Palestine. It resulted in retaliatory anti-Jewish violence by British troops and a punitive curfew imposed on Tel Aviv's roads and a closure of places of entertainment in the city by the British Army.

On 12 January 1947, Lehi members drove a truckload of explosives into a British police station in Haifa killing four and injuring 140, in what has been called 'the world's first true truck bomb'.

Following the bombing of the British embassy in Rome, October 1946, a series of operations against targets in the United Kingdom were launched. On 7 March 1947, Lehi's only successful operation in Britain was carried out when a Lehi bomb severely damaged the British Colonial Club, a London recreational facility for soldiers and students from Britain's colonies in Africa and the West Indies. On 15 April 1947 a bomb consisting of twenty-four sticks of explosives was planted in the Colonial Office, Whitehall. It failed to explode due to a fault in the timer. Five weeks later, on 22 May, five alleged Lehi members were arrested in Paris with bomb making material including explosives of the same type as found in London. On 2 June, two Lehi members, Betty Knouth and Yaakov Levstein, were arrested crossing from Belgium to France. Envelopes addressed to British officials, with detonators, batteries and a time fuse were found in one of Knouth's suitcases. Knouth was sentenced to a year in prison, Levstein to eight months. The British Security Services identified Knouth as the person who planted the bomb in the Colonial Office. Shortly after their arrest, 21 letter bombs were intercepted addressed to senior British figures. The letters had been posted in Italy. The intended recipients included Bevin, Attlee, Churchill and Eden. Knouth aka Gilberte/Elizabeth Lazarus. Levstein was travelling as Jacob Elias; his fingerprints connected him to the deaths of several Palestine Policemen as well as an attempt on the life of the British High Commissioner. In 1973, Margaret Truman wrote that letter bombs were also posted to her father, U.S. President Harry S. Truman, in 1947. Former Lehi leader Yellin-Mor admitted that letter bombs had been sent to British targets but denied that any had been sent to Truman.

Shortly after the 1947 publication of "The Last Days of Hitler", Lehi issued a death threat against the author, Hugh Trevor-Roper, for his portrayal of Hitler, feeling that Trevor-Roper had attempted to exonerate the German populace from responsibility.

During the lead-up to the 1948 Arab–Israeli War, Lehi mined the Cairo–Haifa train several times. On 29 February 1948, Lehi mined the train north of Rehovot, killing 28 British soldiers and wounding 35. On 31 March, Lehi mined the train near Binyamina, killing 40 civilians and wounding 60.

Shlomo Sand writes that as a method of applying pressure on Arab villagers to abandon their settlements, Lehi planned a terror attack on Nablus and its Arab city headquarters; Lehi fighter Elisha Ibzov( Avraham Cohen) was captured with a truck filled with explosives on his way to the city. Lehi fighters in return abducted four adult villagers and a youth from al-Sheikh Muwannis with no connection to Ibzov's capture, and threatened to kill them. As rumours spread that they were already murdered, panic set out in the villagers and the settlement became increasingly abandoned, despite eventual release of the hostages

One of the most widely known acts of Lehi was the attack on the Palestinian-Arab village of Deir Yassin.

In the months before the British evacuation from Palestine, the Arab League-sponsored Arab Liberation Army (ALA) occupied several strategic points along the road between Jerusalem and Tel Aviv, cutting off supplies to the Jewish part of Jerusalem. One of these points was Deir Yassin. By March 1948, the road was cut off and Jewish Jerusalem was under siege. The Haganah launched Operation Nachshon to break the siege.

On 6 April, the Haganah attacked al-Qastal, a village two kilometers north of Deir Yassin, also overlooking the Jerusalem-Tel Aviv road.

Then on 9 April 1948, about 120 Lehi and Irgun fighters, acting in cooperation with the Haganah, attacked and captured Deir Yassin. The attack was at night, the fighting was confused, and many civilian inhabitants of the village were killed. This action had great consequences for the war, and remains a cause celebre for Palestinians ever since.

Exactly what happened has never been established clearly. The Arab League reported a great massacre: 254 killed, with rape and lurid mutilations. Israeli investigations claimed the actual number of dead was between 100 and 120, and there were no mass rapes, but most of the dead were civilians, and admitted some were killed deliberately. Lehi and Irgun both denied an organized massacre. Accounts by Lehi veterans such as Ezra Yakhin note that many of the attackers were killed or wounded, assert that Arabs fired from every building and that Iraqi and Syrian soldiers were among the dead, and even that some Arab fighters dressed as women.

However, Jewish authorities, including Haganah, the Chief Rabbinate, the Jewish Agency, and David Ben-Gurion, also condemned the attack, lending credence to the charge of massacre. The Jewish Agency even sent a letter of condemnation, apology, and condolence to King Abdullah I of Jordan.

Both the Arab reports and Jewish responses had hidden motives: the Arab leaders wanted to encourage Palestinian Arabs to fight rather than surrender, to discredit the Zionists with international opinion, and to increase popular support in their countries for an invasion of Palestine. The Jewish leaders wanted to discredit Irgun and Lehi.

Ironically, the Arab reports backfired in one respect: frightened Palestinian Arabs did not surrender, but did not fight either – they fled, allowing Israel to gain much territory with little fighting and also without absorbing many Arabs.

Lehi similarly interpreted events at Deir Yassin as turning the tide of war in favor of the Jews. Lehi leader Israel Eldad later wrote in his memoirs from the underground period that "without Deir Yassin the State of Israel could never have been established".

The Deir Yassin story did not much sway international opinion. It did increase not only support but pressure on Arab governments to intervene, notably Abdullah of Jordan, who was now compelled to join the invasion of Palestine after Israel's declaration of independence on 14 May.

Although Lehi had stopped operating nationally after May 1948, the group continued to function in Jerusalem. On 17 September 1948, Lehi assassinated UN mediator Count Folke Bernadotte. The assassination was directed by Yehoshua Zettler and carried out by a four-man team led by Meshulam Makover. The fatal shots were fired by Yehoshua Cohen. The Security Council described the assassination as a "cowardly act which appears to have been committed by a criminal group of terrorists".

Three days after the assassination, the Israeli government passed the Ordinance to Prevent Terrorism and declared Lehi to be a terrorist organization. Many Lehi members were arrested, including leaders Nathan Yellin-Mor and Matitiahu Schmulevitz who were arrested on 29 September. Eldad and Shamir managed to escape arrest. Yellin-Mor and Schmulevitz were charged with leadership of a terrorist organization and on 10 February 1949 were sentenced to 8 years and 5 years imprisonment, respectively. However the State (Temporary) Council soon announced a general amnesty for Lehi members and they were released.

Between 5 December 1948 and 25 January 1949, Yellin-Mor and Schmuelevitch were tried in a military court on terrorism charges. The prosecution accused them of the murder of Bernadotte, though they were not specifically charged with it. Senior officers of the IDF, including Yisrael Galili and David Shaltiel, told the court that Lehi had hindered, rather than assisted the fight against the British and the Arabs.

While the trial was in progress, some of the Lehi leadership founded a USSR-leaning political party called the Fighters' List with Yellin-Mor as its leader. The party took part in the elections in January 1949 with Yellin-Mor and Schmuelevitch heading the list. The trial verdict was handed down on 10 February, soon after the Fighters' List had won one seat with only 1.2% of the vote. Yellin-Mor was sentenced to 8 years and Schmuelevitch to 5 years imprisonment, but the court agreed to remit the sentences if the prisoners agreed to a list of conditions. The Provisional State Council then authorised their pardon. The party disbanded after several years and did not contest the 1951 elections.

In 1956, some Lehi veterans established the Semitic Action movement, which sought the creation of a regional federation encompassing Israel and its Arab neighbors on the basis of an anti-colonialist alliance with other indigenous inhabitants of the Middle East.

Not all Lehi alumni gave up political violence after independence: former members were involved in the activities of the Kingdom of Israel militant group, the 1957 assassination of Rudolf Kastner, and likely the 1952 attempted assassination of David-Zvi Pinkas.

In 1980, Israel instituted the Lehi ribbon, red, black, grey, pale blue and white, which is awarded to former members of the Lehi underground who wished to carry it, "for military service towards the establishment of the State of Israel".

The words and music of a song "Unknown Soldiers" (also translated "Anonymous Soldiers") were written by Avraham Stern in 1932 during the early days of the Irgun. It became the Irgun's anthem until the split with Lehi in 1940, after which it became the Lehi anthem.

A number of Lehi's members went on to play important roles in Israel's public life.





</doc>
<doc id="29288" url="https://en.wikipedia.org/wiki?curid=29288" title="Server-side scripting">
Server-side scripting

Server-side scripting is a technique used in web development which involves employing scripts on a web server which produce a response customized for each user's (client's) request to the website. The alternative is for the web server itself to deliver a static web page. Scripts can be written in any of a number of server-side scripting languages that are available (see below). Server-side scripting is distinguished from client-side scripting where embedded scripts, such as JavaScript, are run client-side in a web browser, but both techniques are often used together.

Server-side scripting is often used to provide a customized interface for the user. These scripts may assemble client characteristics for use in customizing the response based on those characteristics, the user's requirements, access rights, etc. Server-side scripting also enables the website owner to hide the source code that generates the interface, whereas with client-side scripting, the user has access to all the code received by the client. A down-side to the use of server-side scripting is that the client needs to make further requests over the network to the server in order to show new information to the user via the web browser. These requests can slow down the experience for the user, place more load on the server, and prevent use of the application when the user is disconnected from the server.

When the server serves data in a commonly used manner, for example according to the HTTP or FTP protocols, users may have their choice of a number of client programs (most modern web browsers can request and receive data using both of those protocols). In the case of more specialized applications, programmers may write their own server, client, and communications protocol, that can only be used with one another.

Programs that run on a user's local computer without ever sending or receiving data over a network are not considered clients, and so the operations of such programs would not be considered client-side operations.

Netscape introduced an implementation of JavaScript for server-side scripting with Netscape Enterprise Server, first released in December, 1994 (soon after releasing JavaScript for browsers).

Server-side scripting was later used in early 1995 by Fred DuFresne while developing the first web site for Boston, MA television station WCVB. The technology is described in US patent 5835712. The patent was issued in 1998 and is now owned by Open Invention Network (OIN). In 2010 OIN named Fred DuFresne a "Distinguished Inventor" for his work on server-side scripting.

Today, a variety of services use server-side scripting to deliver results back to a client as a paid or free service. An example would be WolframAlpha, which is a computational knowledge engine that computes results outside the clients environment and returns the computed result back. A more commonly used service is Google's proprietary search engine, which searches millions of cached results related to the user specified keyword and returns an ordered list of links back to the client. Apple's Siri application also employs server-side scripting outside of a web application. The application takes an input, computes a result, and returns the result back to the client.

In the earlier days of the web, server-side scripting was almost exclusively performed by using a combination of C programs, Perl scripts, and shell scripts using the Common Gateway Interface (CGI). Those scripts were executed by the operating system, and the results were served back by the web server. Many modern web servers can directly execute on-line scripting languages such as ASP, JSP, Perl, PHP and Ruby either by the web server itself or via extension modules (e.g. mod_perl or mod_php) to the web server. For example, WebDNA includes its own embedded database system. Either form of scripting (i.e., CGI or direct execution) can be used to build up complex multi-page sites, but direct execution usually results in less overhead because of the lower number of calls to external interpreters.

Dynamic websites sometimes use custom web application servers, such as Glassfish, Plack and Python's "Base HTTP Server" library, although some may not consider this to be server-side scripting. When using dynamic web-based scripting techniques, developers must have a keen understanding of the logical, temporal, and physical separation between the client and the server. For a user's action to trigger the execution of server-side code, for example, a developer working with classic ASP must explicitly cause the user's browser to make a request back to the web server. Creating such interactions can easily consume much development time and lead to unreadable code.

Server-side scripts are completely processed by the servers instead of clients. When clients request a page containing server-side scripts, the applicable server processes the scripts and returns an HTML page to the client.

There are a number of server-side scripting languages available, including:



</doc>
<doc id="29290" url="https://en.wikipedia.org/wiki?curid=29290" title="Samuel Huntington">
Samuel Huntington

Samuel Huntington may refer to:




</doc>
<doc id="29292" url="https://en.wikipedia.org/wiki?curid=29292" title="Script">
Script

Script may refer to:







</doc>
<doc id="29293" url="https://en.wikipedia.org/wiki?curid=29293" title="Optical spectrometer">
Optical spectrometer

An optical spectrometer (spectrophotometer, spectrograph or spectroscope) is an instrument used to measure properties of light over a specific portion of the electromagnetic spectrum, typically used in spectroscopic analysis to identify materials. The variable measured is most often the light's intensity but could also, for instance, be the polarization state. The independent variable is usually the wavelength of the light or a unit directly proportional to the photon energy, such as reciprocal centimeters or electron volts, which has a reciprocal relationship to wavelength.

A spectrometer is used in spectroscopy for producing spectral lines and measuring their wavelengths and intensities. Spectrometers may also operate over a wide range of non-optical wavelengths, from gamma rays and X-rays into the far infrared. If the instrument is designed to measure the spectrum in absolute units rather than relative units, then it is typically called a spectrophotometer. The majority of spectrophotometers are used in spectral regions near the visible spectrum.

In general, any particular instrument will operate over a small portion of this total range because of the different techniques used to measure different portions of the spectrum. Below optical frequencies (that is, at microwave and radio frequencies), the spectrum analyzer is a closely related electronic device.

Spectrometers are used in many fields. For example, they are used in astronomy to analyze the radiation from astronomical objects and deduce chemical composition. The spectrometer uses a prism or a grating to spread the light from a distant object into a spectrum. This allows astronomers to detect many of the chemical elements by their characteristic spectral fingerprints. If the object is glowing by itself, it will show spectral lines caused by the glowing gas itself. These lines are named for the elements which cause them, such as the hydrogen alpha, beta, and gamma lines. Chemical compounds may also be identified by absorption. Typically these are dark bands in specific locations in the spectrum caused by energy being absorbed as light from other objects passes through a gas cloud. Much of our knowledge of the chemical makeup of the universe comes from spectra.

Spectroscopes are often used in astronomy and some branches of chemistry. Early spectroscopes were simply prisms with graduations marking wavelengths of light. Modern spectroscopes generally use a diffraction grating, a movable slit, and some kind of photodetector, all automated and controlled by a computer.

Joseph von Fraunhofer developed the first modern spectroscope by combining a prism, diffraction slit and telescope in a manner that increased the spectral resolution and was reproducible in other laboratories. Fraunhofer also went on to invent the first diffraction spectroscope. Gustav Robert Kirchhoff and Robert Bunsen discovered the application of spectroscopes to chemical analysis and used this approach to discover caesium and rubidium. Kirchhoff and Bunsen's analysis also enabled a chemical explanation of stellar spectra, including Fraunhofer lines.

When a material is heated to incandescence it emits light that is characteristic of the atomic makeup of the material.
Particular light frequencies give rise to sharply defined bands on the scale which can be thought of as fingerprints. For example, the element sodium has a very characteristic double yellow band known as the Sodium D-lines at 588.9950 and 589.5924 nanometers, the color of which will be familiar to anyone who has seen a low pressure sodium vapor lamp.

In the original spectroscope design in the early 19th century, light entered a slit and a collimating lens transformed the light into a thin beam of parallel rays. The light then passed through a prism (in hand-held spectroscopes, usually an Amici prism) that refracted the beam into a spectrum because different wavelengths were refracted different amounts due to dispersion. This image was then viewed through a tube with a scale that was transposed upon the spectral image, enabling its direct measurement.

With the development of photographic film, the more accurate spectrograph was created. It was based on the same principle as the spectroscope, but it had a camera in place of the viewing tube. In recent years, the electronic circuits built around the photomultiplier tube have replaced the camera, allowing real-time spectrographic analysis with far greater accuracy. Arrays of photosensors are also used in place of film in spectrographic systems. Such spectral analysis, or spectroscopy, has become an important scientific tool for analyzing the composition of unknown material and for studying astronomical phenomena and testing astronomical theories.

In modern spectrographs in the UV, visible, and near-IR spectral ranges, the spectrum is generally given in the form of photon number per unit wavelength (nm or μm), wavenumber (μm, cm), frequency (THz), or energy (eV), with the units indicated by the abscissa. In the mid- to far-IR, spectra are typically expressed in units of Watts per unit wavelength (μm) or wavenumber (cm). In many cases, the spectrum is displayed with the units left implied (such as "digital counts" per spectral channel).

A spectrograph is an instrument that separates light by its wavelengths and records this data. A spectrograph typically has a multi-channel detector system or camera that detects and records the spectrum of light. 

The term was first used in 1876 by Dr. Henry Draper when he invented the earliest version of this device, and which he used to take several photographs of the spectrum of Vega. This earliest version of the spectrograph was cumbersome to use and difficult to manage.

There are several kinds of machines referred to as "spectrographs", depending on the precise nature of the waves. The first spectrographs used photographic paper as the detector. The star spectral classification and discovery of the main sequence, Hubble's law and the Hubble sequence were all made with spectrographs that used photographic paper. The plant pigment phytochrome was discovered using a spectrograph that used living plants as the detector. More recent spectrographs use electronic detectors, such as CCDs which can be used for both visible and UV light. The exact choice of detector depends on the wavelengths of light to be recorded.

A spectrograph is sometimes called polychromator, as an analogy to monochromator.

The first spectrographs used photographic paper as the detector. The star spectral classification and discovery of the main sequence, Hubble's law and the Hubble sequence were all made with spectrographs that used photographic paper. The plant pigment phytochrome was discovered using a spectrograph that used living plants as the detector. More recent spectrographs use electronic detectors, such as CCDs which can be used for both visible and UV light. The exact choice of detector depends on the wavelengths of light to be recorded.

The forthcoming James Webb Space Telescope will contain both a near-infrared spectrograph (NIRSpec) and a mid-infrared spectrograph (MIRI).

An Echelle spectrograph uses two diffraction gratings, rotated 90 degrees with respect to each other and placed close to one another. Therefore, an entrance point and not a slit is used and a 2d CCD-chip records the spectrum. Usually one would guess to retrieve a spectrum on the diagonal, but when both gratings have a wide spacing and one is blazed so that only the first order is visible and the other is blazed that a lot of higher orders are visible, one gets a very fine spectrum nicely folded onto a small common CCD-chip. The small chip also means that the collimating optics need not to be optimized for coma or astigmatism, but the spherical aberration can be set to zero.




</doc>
<doc id="29294" url="https://en.wikipedia.org/wiki?curid=29294" title="IBM System/360">
IBM System/360

The IBM System/360 (S/360) is a family of mainframe computer systems that was announced by IBM on April 7, 1964, and delivered between 1965 and 1978. It was the first family of computers designed to cover the complete range of applications, from small to large, both commercial and scientific. The design made a clear distinction between architecture and implementation, allowing IBM to release a suite of compatible designs at different prices. All but the only partially compatible Model 44 and the most expensive systems use microcode to implement the instruction set, which features 8-bit byte addressing and binary, decimal and hexadecimal floating-point calculations.

The launch of the System/360 family introduced IBM's Solid Logic Technology (SLT), a new technology that was the start of more powerful but smaller computers.

The slowest System/360 model announced in 1964, the Model 30, could perform up to 34,500 instructions per second, with memory from 8 to 64 KB. High performance models came later. The 1967 IBM System/360 Model 91 could execute up to 16.6 million instructions per second. The larger 360 models could have up to 8 MB of main memory, though that much main memory was unusual—a large installation might have as little as 256 KB of main storage, but 512 KB, 768 KB or 1024 KB was more common. Up to 8 megabytes of slower (8 microsecond) Large Capacity Storage (LCS) was also available for some models.

The IBM 360 was extremely successful in the market, allowing customers to purchase a smaller system with the knowledge they would always be able to migrate upward if their needs grew, without reprogramming of application software or replacing peripheral devices. Many consider the design one of the most successful computers in history, influencing computer design for years to come.

The chief architect of System/360 was Gene Amdahl, and the project was managed by Fred Brooks, responsible to Chairman Thomas J. Watson Jr. The commercial release was piloted by another of Watson's lieutenants, John R. Opel, who managed the launch of IBM’s System 360 mainframe family in 1964.

Application-level compatibility (with some restrictions) for System/360 software is maintained to the present day with the System z mainframe servers.

Contrasting with at-the-time normal industry practice, IBM created an entire new series of computers, from small to large, low- to high-performance, all using the same instruction set (with two exceptions for specific markets). This feat allowed customers to use a cheaper model and then upgrade to larger systems as their needs increased without the time and expense of rewriting software. Before the introduction of System/360, business and scientific applications used different computers with different instruction sets and operating systems. Different-sized computers also had their own instruction sets. IBM was the first manufacturer to exploit microcode technology to implement a compatible range of computers of widely differing performance, although the largest, fastest, models had hard-wired logic instead.

This flexibility greatly lowered barriers to entry. With most other vendors customers had to choose between machines they could outgrow and machines that were potentially too powerful and thus too costly. This meant that many companies simply did not buy computers.

IBM initially announced a series of six computers and forty common peripherals. IBM eventually delivered fourteen models, including rare one-off models for NASA. The least expensive model was the Model 20 with as little as 4096 bytes of core memory, eight 16-bit registers instead of the sixteen 32-bit registers of other System/360 models, and an instruction set that was a subset of that used by the rest of the range.

The initial announcement in 1964 included Models 30, 40, 50, 60, 62, and 70. The first three were low- to middle-range systems aimed at the IBM 1400 series market. All three first shipped in mid-1965. The last three, intended to replace the 7000 series machines, never shipped and were replaced with the 65 and 75, which were first delivered in November 1965, and January 1966, respectively.

Later additions to the low-end included models 20 (1966, mentioned above), 22 (1971), and 25 (1968). The Model 20 had several sub-models; sub-model 5 was at the higher end of the model. The Model 22 was a recycled Model 30 with minor limitations: a smaller maximum memory configuration, and slower I/O channels, which limited it to slower and lower-capacity disk and tape devices than on the 30.

The Model 44 (1966) was a specialized model, designed for scientific computing and for real-time computing and process control, featuring some additional instructions, and with all storage-to-storage instructions and five other complex instructions eliminated.
A succession of high-end machines included the Model 67 (1966, mentioned below, briefly anticipated as the 64 and 66), 85 (1969), 91 (1967, anticipated as the 92), 95 (1968), and 195 (1971). The 85 design was intermediate between the System/360 line and the follow-on System/370 and was the basis for the 370/165. There was a System/370 version of the 195, but it did not include Dynamic Address Translation.

The implementations differed substantially, using different native data path widths, presence or absence of microcode, yet were extremely compatible. Except where specifically documented, the models were architecturally compatible. The 91, for example, was designed for scientific computing and provided out-of-order instruction execution (and could yield "imprecise interrupts" if a program trap occurred while several instructions were being read), but lacked the decimal instruction set used in commercial applications. New features could be added without violating architectural definitions: the 65 had a dual-processor version (M65MP) with extensions for inter-CPU signalling; the 85 introduced cache memory. Models 44, 75, 91, 95, and 195 were implemented with hardwired logic, rather than microcoded as all other models.

The Model 67, announced in August 1965, was the first production IBM system to offer dynamic address translation (virtual memory) hardware to support time-sharing. "DAT" is now more commonly referred to as an MMU. An experimental one-off unit was built based on a model 40. Before the 67, IBM had announced models 64 and 66, DAT versions of the 60 and 62, but they were almost immediately replaced with the 67 at the same time that the 60 and 62 were replaced with the 65. DAT hardware would reappear in the S/370 series in 1972, though it was initially absent from the series. Like its close relative, the 65, the 67 also offered dual CPUs.

IBM stopped marketing all System/360 models by the end of 1977.

IBM's existing customers had a large investment in software that executed on second-generation machines. Several models offered the option of emulation of the customer's previous computer using a combination of special hardware, special microcode and an emulation program that used the emulation instructions to simulate the target system, so that old programs could run on the new machine.

However, customers initially had to halt the computer and load the emulation program.

IBM later added features and modified emulator programs to allow emulation of the 1401, 1440, 1460, 1410 and 7010 under the control of an operating system.
The Model 85 and later System/370 maintained the precedent, retaining emulation options and allowing emulator programs to execute under operating system control alongside native programs.

System/360 (excepting the Model 20) was replaced with the compatible System/370 range in 1970 and Model 20 users were targeted to move to the IBM System/3. (The idea of a major breakthrough with FS technology was dropped in the mid-1970s for cost-effectiveness and continuity reasons.) Later compatible IBM systems include the 4300 family, the 308x family, the 3090, the ES/9000 and 9672 families (System/390 family), and the IBM Z series.

Computers that were mostly identical or compatible in terms of the machine code or architecture of the System/360 included Amdahl's 470 family (and its successors), Hitachi mainframes, the UNIVAC 9000 series, Fujitsu as the Facom, the RCA Spectra 70 series, and the English Electric System 4. The System 4 machines were built under license to RCA. RCA sold the Spectra series to what was then UNIVAC, where they became the UNIVAC Series 70. UNIVAC also developed the UNIVAC Series 90 as successors to the 9000 series and Series 70. The Soviet Union produced a System/360 clone named the ES EVM.

The IBM 5100 portable computer, introduced in 1975, offered an option to execute the System/360's APL.SV programming language through a hardware emulator. IBM used this approach to avoid the costs and delay of creating a 5100-specific version of APL.

Special radiation-hardened and otherwise somewhat modified System/360s, in the form of the System/4 Pi avionics computer, are used in several fighter and bomber jet aircraft. In the complete 32-bit AP-101 version, 4 Pi machines were used as the replicated computing nodes of the fault-tolerant Space Shuttle computer system (in five nodes). The U.S. Federal Aviation Administration operated the IBM 9020, a special cluster of modified System/360s for air traffic control, from 1970 until the 1990s. (Some 9020 software is apparently still used via emulation on newer hardware.)

The System/360 introduced a number of industry standards to the marketplace, such as:

The System/360 series has a computer system architecture specification. This specification makes no assumptions on the implementation itself, but rather describes the interfaces and expected behavior of an implementation. The architecture describes mandatory interfaces that must be available on all implementations, and optional interfaces. Some aspects of this architecture are:

Some of the optional features are:

All models of System/360, except for the Model 20 and Model 44, implemented that specification.

Binary arithmetic and logical operations are performed as register-to-register and as memory-to-register/register-to-memory as a standard feature. If the Commercial Instruction Set option was installed, packed decimal arithmetic could be performed as memory-to-memory with some memory-to-register operations. The Scientific Instruction Set feature, if installed, provided access to four floating point registers that could be programmed for either 32-bit or 64-bit floating point operations. The Models 85 and 195 could also operate on 128-bit extended-precision floating point numbers stored in pairs of floating point registers, and software provided emulation in other models. The System/360 used an 8-bit byte, 32-bit word, 64-bit double-word, and 4-bit nibble. Machine instructions had operators with operands, which could contain register numbers or memory addresses. This complex combination of instruction options resulted in a variety of instruction lengths and formats.

Memory addressing was accomplished using a base-plus-displacement scheme, with registers 1 through F (15). A displacement was encoded in 12 bits, thus allowing a 4096-byte displacement (0-4095), as the offset from the address put in a base register.

Register 0 could not be used as a base register nor as an index register (nor as a branch address register), as "0" was reserved to indicate an address in the first 4 KB of memory, that is, if register 0 was specified as described, the value 0x00000000 was implicitly input to the effective address calculation in place of whatever value might be contained within register 0 (or if specified as a branch address register, then no branch was taken, and the content of register 0 was ignored, but any side effect of the instruction was performed).

This specific behavior permitted initial execution of an interrupt routines, since base registers would not necessarily be set to 0 during the first few instruction cycles of an interrupt routine. It isn't needed for IPL ("Initial Program Load" or boot), as one can always clear a register without the need to save it.

With the exception of the Model 67, all addresses were real memory addresses. Virtual memory was not available in most IBM mainframes until the System/370 series. The Model 67 introduced a virtual memory architecture, which MTS, CP-67, and TSS/360 used—but not IBM's mainline System/360 operating systems.

The System/360 machine-code instructions are 2 bytes long (no memory operands), 4 bytes long (one operand), or 6 bytes long (two operands). Instructions are always situated on 2-byte boundaries.

Operations like MVC (Move-Characters) (Hex: D2) can only move at most 256 bytes of information. Moving more than 256 bytes of data required multiple MVC operations. (The System/370 series introduced a family of more powerful instructions such as the MVCL "Move-Characters-Long" instruction, which supports moving up to 16 MB as a single block.)

An operand is two bytes long, typically representing an address as a 4-bit nibble denoting a base register and a 12-bit displacement relative to the contents of that register, in the range 000–FFF (shown here as hexadecimal numbers). The address corresponding to that operand is the contents of the specified general-purpose register plus the displacement. For example, an MVC instruction that moves 256 bytes (with length code 255 in hexadecimal as FF) from base register 7, plus displacement 000, to base register 8, plus displacement 001, would be coded as the 6-byte instruction "D2FF 8001 7000" (operator/length/address1/address2).

The System/360 was designed to separate the "system state" from the "problem state". This provided a basic level of security and recoverability from programming errors. Problem (user) programs could not modify data or program storage associated with the system state. Addressing, data, or operation exception errors made the machine enter the system state through a controlled routine so the operating system could try to correct or terminate the program in error. Similarly, it could recover certain processor hardware errors through the "machine check" routines.

Peripherals interfaced to the system via "channels". A channel is a specialized processor with the instruction set optimized for transferring data between a peripheral and main memory. In modern terms, this could be compared to direct memory access (DMA). The S/360 connects channels to control units with bus and tag cables; IBM eventually replaced these with (Enterprise Systems Connection (ESCON) and Fibre Connection (FICON) channels.

There were initially two types of channels; byte-multiplexer channels (known at the time simply as "multiplexor channels"), for connecting "slow speed" devices such as card readers and punches, line printers, and communications controllers, and selector channels for connecting high speed devices, such as disk drives, tape drives, data cells and drums. Every System/360 (except for the Model 20, which was not a standard 360) has a byte-multiplexer channel and 1 or more selector channels. The smaller models (up to the model 50) have integrated channels, while for the larger models (model 65 and above) the channels are large separate units in separate cabinets, such as the IBM 2860 and 2870. (The 60, 62, and 70 allowed only for 2860 selector channels, on the assumption that they would all have smaller 360s attached, which would do the slow-speed work.)

The byte-multiplexer channel is able to handle I/O to/from several devices simultaneously at the device's highest rated speeds, hence the name, as it multiplexed I/O from those devices onto a single data path to main memory. Devices connected to a byte-multiplexer channel are configured to operate in 1-byte, 2-byte, 4-byte, or "burst" mode. The larger "blocks" of data are used to handle progressively faster devices. For example, a 2501 card reader operating at 600 cards per minute would be in 1-byte mode, while a 1403-N1 printer would be in burst mode. Also, the byte-multiplexer channels on larger models have an optional selector subchannel section that would accommodate tape drives. The byte-multiplexor's channel address was typically "0" and the selector subchannel addresses were from "C0" to "FF." Thus, tape drives on System/360 were commonly addressed at 0C0-0C7. Other common byte-multiplexer addresses are: 00A: 2501 Card Reader, 00C/00D: 2540 Reader/Punch, 00E/00F: 1403-N1 Printers, 010-013: 3211 Printers, 020-0BF: 2701/2703 Telecommunications Units. These addresses are still commonly used in z/VM virtual machines.

System/360 models 40 and 50 have an integrated 1052-7 console that is usually addressed as 01F, however, this was not connected to the byte-multiplexer channel, but rather, had a direct internal connection to the mainframe. The model 30 attached a different model of 1052 through a 1051 control unit. The models 60 through 75 also use the 1052-7.

Selector channels enabled I/O to high speed devices. These storage devices were attached to a control unit and then to the channel. The control unit let clusters of devices be attached to the channels. On higher speed models, multiple selector channels, which could operate simultaneously or in parallel, improved overall performance.

Control units are connected to the channels with "bus and tag" cable pairs. The bus cables carried the address and data information and the tag cables identified what data was on the bus. The general configuration of a channel is to connect the devices in a chain, like this: Mainframe—Control Unit X—Control Unit Y—Control Unit Z. Each control unit is assigned a "capture range" of addresses that it services. For example, control unit X might capture addresses 40-4F, control unit Y: C0-DF, and control unit Z: 80-9F. Capture ranges had to be a multiple of 8, 16, 32, 64, or 128 devices and be aligned on appropriate boundaries. Each control unit in turn has one or more devices attached to it. For example, you could have control unit Y with 6 disks, that would be addressed as C0-C5.

There are three general types of bus-and-tag cables produced by IBM. The first is the standard gray bus-and-tag cable, followed by the blue bus-and-tag cable, and finally the tan bus-and-tag cable. Generally, newer cable revisions are capable of higher speeds or longer distances, and some peripherals specified minimum cable revisions both upstream and downstream.

The cable ordering of the control units on the channel is also significant. Each control unit is "strapped" as High or Low priority. When a device selection was sent out on a mainframe's channel, the selection was sent from X->Y->Z->Y->X. If the control unit was "high" then the selection was checked in the outbound direction, if "low" then the inbound direction. Thus, control unit X was either 1st or 5th, Y was either 2nd or 4th, and Z was 3rd in line. It is also possible to have multiple channels attached to a control unit from the same or multiple mainframes, thus providing a rich high-performance, multiple-access, and backup capability.

Typically the total cable length of a channel is limited to 200 feet, less being preferred. Each control unit accounts for about 10 "feet" of the 200-foot limit.

IBM first introduced a new type of I/O channel on the Model 85 and Model 195, the 2880 block multiplexer channel, and then made them standard on the System/370. This channel allowed a device to suspend a channel program, pending the completion of an I/O operation and thus to free the channel for use by another device. A block multiplexer channel can support either standard 1.5 MB/second connections or, with the 2-byte interface feature, 3 MB/second; the latter use one tag cable and two bus cables. On the S/370 there is an option for a 3.0 MB/s data streaming channel with one bus cable and one tag cable.

The initial use for this was the 2305 fixed-head disk, which has 8 "exposures" (alias addresses) and rotational position sensing (RPS).

Block multiplexer channels can operate as a selector channel to allow compatible attachment of legacy subsystems.

Being uncertain of the reliability and availability of the then new monolithic integrated circuits, IBM chose instead to design and manufacture its own custom hybrid integrated circuits. These were built on 11 mm square ceramic substrates. Resistors were silk screened on and discrete glass encapsulated transistors and diodes were added. The substrate was then covered with a metal lid or encapsulated in plastic to create a "Solid Logic Technology" (SLT) module.

A number of these SLT modules were then flip chip mounted onto a small multi-layer printed circuit "SLT card". Each card had one or two sockets on one edge that plugged onto pins on one of the computer's "SLT boards". This was the reverse of how most other company's cards were mounted, where the cards had pins or printed contact areas and plugged into sockets on the computer's boards.

Up to twenty SLT boards could be assembled side-by-side (vertically and horizontally) to form a "logic gate". Several gates mounted together constituted a box-shaped "logic frame". The outer gates were generally hinged along one vertical edge so they could be swung open to provide access to the fixed inner gates. The larger machines could have more than one frame bolted together to produce the final unit, such as a multi-frame Central Processing Unit (CPU).

The smaller System/360 models used the Basic Operating System/360 (BOS/360), Tape Operating System (TOS/360), or Disk Operating System/360 (DOS/360, which evolved into DOS/VS, DOS/VSE, VSE/AF, VSE/SP, VSE/ESA, and then z/VSE).

The larger models used Operating System/360 (OS/360). IBM developed several versions of OS/360, with increasingly powerful features: Primary Control Program (PCP), Multiprogramming with a Fixed number of Tasks (MFT), and Multiprogramming with a Variable number of Tasks (MVT). MVT took a long time to develop into a usable system, and the less ambitious MFT was widely used. PCP was used on intermediate machines; the final releases of OS/360 included only MFT and MVT. For the System/370 and later machines, MFT evolved into OS/VS1, while MVT evolved into OS/VS2 (SVS) (Single Virtual Storage), then various versions of MVS (Multiple Virtual Storage) culminating in the current z/OS.

When it announced the Model 67 in August 1965, IBM also announced TSS/360 (Time-Sharing System) for delivery at the same time as the 67. TSS/360, a response to Multics, was an ambitious project that included many advanced features. It had performance problems, was delayed, canceled, reinstated, and finally canceled again in 1971. Customers migrated to CP-67, MTS (Michigan Terminal System), TSO (Time Sharing Option for OS/360), or one of several other time-sharing systems.

CP-67, the original virtual machine system, was also known as CP/CMS. CP/67 was developed outside the IBM mainstream at IBM's Cambridge Scientific Center, in cooperation with MIT researchers. CP/CMS eventually won wide acceptance, and led to the development of VM/370 (Virtual Machine) which had a primary interactive "sub" operating system known as VM/CMS (Conversational Monitoring System). This evolved into today's z/VM.

The Model 20 offered a simplified and rarely used tape-based system called TPS (Tape Processing System), and DPS (Disk Processing System) that provided support for the 2311 disk drive. TPS could run on a machine with 8 KB of memory; DPS required 12 KB, which was pretty hefty for a Model 20. Many customers ran quite happily with 4 KB and CPS (Card Processing System). With TPS and DPS, the card reader was used to read the Job Control Language cards that defined the stack of jobs to run and to read in transaction data such as customer payments. The operating system was held on tape or disk, and results could also be stored on the tapes or hard drives. Stacked job processing became an exciting possibility for the small but adventurous computer user.

A little-known and little-used suite of 80-column punched-card utility programs known as Basic Programming Support (BPS) (jocularly: Barely Programming Support), a precursor of TOS, was available for smaller systems.

IBM created a new naming system for the new components created for System/360, although well-known old names, like IBM 1403 and IBM 1052, were retained. In this new naming system, components were given four-digit numbers starting with 2. The second digit described the type of component, as follows:
IBM developed a new family of peripheral equipment for System/360, carrying over a few from its older 1400 series. Interfaces were standardized, allowing greater flexibility to mix and match processors, controllers and peripherals than in the earlier product lines.

In addition, System/360 computers could use certain peripherals that were originally developed for earlier computers. These earlier peripherals used a different numbering system, such as the IBM 1403 chain printer. The 1403, an extremely reliable device that had already earned a reputation as a workhorse, was sold as the 1403-N1 when adapted for the System/360.

Also available were optical character recognition (OCR) readers IBM 1287 and IBM 1288 which could read Alpha Numeric (A/N) and Numeric Hand Printed (NHP/NHW) Characters from Cashier's rolls of tape to full legal size pages. At the time this was done with very large optical/logic readers. Software was too slow and expensive at that time.

Most small systems were sold with an IBM 1052-7 as the console typewriter. This was tightly integrated into the CPU — the keyboard would physically lock under program control. Certain high-end machines could optionally be purchased with a 2250 graphical display, costing upwards of US $100,000. The 360/85 used a 5450 display console that was not compatible with anything else in the line; the later 3066 console for the 370/165 and 370/168 used the same basic display design as the 360/85.

The first disk drives for System/360 were IBM 2302s and IBM 2311s. The first drum for System/360 was the IBM 7320.
The 156 KB/second 2302 was based on the earlier 1302 and was available as a model 3 with two 112.79 MB modules or as a model 4 with four such modules.

The 2311, with a removable 1316 disk pack, was based on the IBM 1311 and had a theoretical capacity of 7.2 MB, although actual capacity varied with record design. (When used with a 360/20, the 1316 pack was formatted into fixed-length 270 byte sectors, giving a maximum capacity of 5.4MB.)
In 1966, the first 2314s shipped. This device had up to eight usable disk drives with an integral control unit; there were nine drives, but one was reserved as a spare. Each drive used a removable 2316 disk pack with a capacity of nearly 28 MB. The disk packs for the 2311 and 2314 were "physically" large by today's standards — e.g., the 1316 disk pack was about in diameter and had six platters stacked on a central spindle. The top and bottom outside platters did not store data. Data were recorded on the inner sides of the top and bottom platters and both sides of the inner platters, providing 10 recording surfaces. The 10 read/write heads moved together across the surfaces of the platters, which were formatted with 203 concentric tracks. To reduce the amount of head movement (seeking), data was written in a virtual cylinder from inside top platter down to inside bottom platter. These disks were not usually formatted with fixed-sized sectors as are today's hard drives (though this "was" done with CP/CMS). Rather, most System/360 I/O software could customize the length of the data record (variable-length records), as was the case with magnetic tapes.

Some of the most powerful early System/360s used high-speed head-per-track drum storage devices. The 3,500 RPM 2301, which replaced the 7320, was part of the original System/360 announcement, with a capacity of 4 MB. The 303.8 KB/second IBM 2303 was announced on January 31, 1966, with a capacity of 3.913 MB. These were the only drums announced for System/360 and System/370, and their niche was later filled by fixed-head disks.

The 6,000 RPM 2305 appeared in 1970, with capacities of 5 MB (2305-1) or 11 MB (2305-2) per module. Although these devices did not have large capacity, their speed and transfer rates made them attractive for high-performance needs. A typical use was overlay linkage (e.g. for OS and application subroutines) for program sections written to alternate in the same memory regions. Fixed head disks and drums were particularly effective as paging devices on the early virtual memory systems. The 2305, although often called a "drum" was actually a head-per-track disk device, with 12 recording surfaces and a data transfer rate up to 3 MB per second.

Rarely seen was the IBM 2321 Data Cell, a mechanically complex device that contained multiple magnetic strips to hold data; strips could be randomly accessed, placed upon a cylinder-shaped drum for read/write operations; then returned to an internal storage cartridge. The IBM Data Cell [noodle picker] was among several IBM trademarked "speedy" mass online direct-access storage peripherals (reincarnated in recent years as "virtual tape" and automated tape librarian peripherals). The 2321 file had a capacity of 400 MB, at the time when the 2311 disk drive only had 7.2 MB. The IBM Data Cell was proposed to fill cost/capacity/speed gap between magnetic tapes—which had high capacity with relatively low cost per stored byte—and disks, which had higher expense per byte. Some installations also found the electromechanical operation less dependable and opted for less mechanical forms of direct-access storage.

The Model 44 was unique in offering an integrated single-disk drive as a standard feature. This drive used the 2315 "ramkit" cartridge and provided 1,171,200 bytes of storage.

The 2400 tape drives consisted of a combined drive and control unit, plus individual 1/2" tape drives attached. With System/360, IBM switched from IBM 7 track to 9 track tape format. 2400 drives could be purchased that read and wrote 7-track tapes for compatibility with the older IBM 729 tape drives. In 1967, a slower and cheaper pair of tape drives with integrated control unit was introduced: the 2415. In 1968, the IBM 2420 tape system was released, offering much higher data rates, self-threading tape operation and 1600bpi packing density. It remained in the product line until 1979.


Despite having been sold or leased in very large numbers for a mainframe system of its era only a few of System/360 computers remain mainly as non-operating property of museums or collectors. Examples of existing systems include:
A running list of remaining System/360s can be found at World Inventory of remaining System/360 CPUs.

This gallery shows the operator's console, with register value lamps, toggle switches (middle of pictures), and "emergency pull" switch (upper right of pictures) of the various models.

In the US television series "Mad Men" (2007–2015), the "IBM 360" was featured as a plot device in which a company leased the system to the advertising agency and was a prominent background in the seventh season.

A crowdfunding campaign for rescuing and restoring an IBM 360 system from Nuremberg has received successful funding.






</doc>
<doc id="29298" url="https://en.wikipedia.org/wiki?curid=29298" title="Spouse">
Spouse

A spouse is a significant other in a marriage, civil union, or common-law marriage. The term is gender neutral, whereas a male spouse is a husband and a female spouse is a wife. Although a spouse is a form of significant other, the latter term also includes non-marital partners who play a social role similar to that of a spouse, but do not have rights and duties reserved by law to a spouse.

The legal status of a spouse, and the specific rights and obligations associated with that status, vary significantly among the jurisdictions of the world. These regulations are usually described in family law statutes. However, in many parts of the world, where civil marriage is not that prevalent, there is instead customary marriage, which is usually regulated informally by the community. In many parts of the world, spousal rights and obligations are related to the payment of bride price, dowry or dower. Historically, many societies have given sets of rights and obligations to male marital partners that have been very different from the sets of rights and obligations given to female marital partners. In particular, the control of marital property, inheritance rights, and the right to dictate the activities of children of the marriage, have typically been given to male marital partners. However, this practice was curtailed to a great deal in many countries in the twentieth century, and more modern statutes tend to define the rights and duties of a spouse without reference to gender. Among the last European countries to establish full gender equality in marriage were Switzerland, Greece, Spain, and France in the 1980s. In various marriage laws around the world, however, the husband continues to have authority; for instance the Civil Code of Iran states at Article 1105: ""In relations between husband and wife; the position of the head of the family is the exclusive right of the husband"".

Depending on jurisdiction, the refusal or inability of a spouse to perform the marital obligations may constitute a ground for divorce, legal separation or annulment. The latter two options are more prevalent in countries where the dominant religion is Roman Catholicism, some of which introduced divorce only recently (i.e. Italy in 1970, Portugal in 1975, Brazil in 1977, Spain in 1981, Argentina in 1987, Paraguay in 1991, Colombia in 1991, Ireland in 1996, Chile in 2004 and Malta in 2011). In recent years, many Western countries have adopted no fault divorce. In some parts of the world, the formal dissolution of a marriage is complicated by the payments and goods which have been exchanged between families (this is common where marriages are arranged). This often makes it difficult to leave a marriage, especially for the woman: in some parts of Africa, once the bride price has been paid, the wife is seen as belonging to the husband and his family; and if she wants to leave, the husband may demand back the bride price that he had paid to the girl's family. The girl's family often cannot or does not want to pay it back.

Regardless of legislation, personal relations between spouses may also be influenced by local culture and religion, which may promote male authority over the wife: for instance the word (ba`al), Hebrew for "husband", used throughout the Bible, is synonymous with "owner" and "master".

There is often a minimum legal marriageable age. The United Nations Population Fund stated the following:

Although in Western countries spouses sometimes choose not to have children, such a choice is not accepted in some parts of the world. In some cultures and religions, the quality of a spouse imposes an obligation to have children. In northern Ghana, for example, the payment of bride price signifies a woman's requirement to bear children, and women using birth control are at risks of threats and coercion.

There are many ways in which a spouse is chosen, which vary across the world, and include love marriage, arranged marriage, and forced marriage. The latter is in some jurisdictions a void marriage or a voidable marriage. Forcing someone to marry is also a criminal offense in some countries.



</doc>
<doc id="29299" url="https://en.wikipedia.org/wiki?curid=29299" title="Sexuality (disambiguation)">
Sexuality (disambiguation)

Human sexuality is the capacity to have erotic experiences and responses.

Sexuality may also refer to:



</doc>
<doc id="29301" url="https://en.wikipedia.org/wiki?curid=29301" title="Semiotics">
Semiotics

Semiotics (also called semiotic studies) is the study of sign process (semiosis), which is any form of activity, conduct, or any process that involves signs, including the production of meaning. A sign is anything that communicates a meaning, that is not the sign itself, to the interpreter of the sign. The meaning can be intentional such as a word uttered with a specific meaning, or unintentional, such as a symptom being a sign of a particular medical condition. Signs can communicate through any of the senses, visual, auditory, tactile, olfactory, or gustatory.

The semiotic tradition explores the study of signs and symbols as a significant part of communications. Unlike linguistics, semiotics also studies non-linguistic sign systems. Semiotics includes the study of signs and sign processes, indication, designation, likeness, analogy, allegory, metonymy, metaphor, symbolism, signification, and communication. 

Semiotics is frequently seen as having important anthropological and sociological dimensions; for example, the Italian semiotician and novelist Umberto Eco proposed that every cultural phenomenon may be studied as communication. Some semioticians focus on the logical dimensions of the science, however. They examine areas belonging also to the life sciences—such as how organisms make predictions about, and adapt to, their semiotic niche in the world (see semiosis). In general, semiotic theories take "signs" or sign systems as their object of study: the communication of information in living organisms is covered in biosemiotics (including zoosemiotics and phytosemiotics).

Semiotics is not to be confused with the Saussurean tradition called semiology, which is a subset of semiotics.

The term derives from the Greek σημειωτικός "sēmeiōtikos", "observant of signs" (from σημεῖον "sēmeion", "a sign, a mark") and it was first used in English prior to 1676 by Henry Stubbe (spelt "semeiotics") in a very precise sense to denote the branch of medical science relating to the interpretation of signs. John Locke used the term "sem(e)iotike" in book four, chapter 21 of "An Essay Concerning Human Understanding" (1690). Here he explains how science may be divided into three parts:

Locke then elaborates on the nature of this third category, naming it Σημειωτική ("Semeiotike") and explaining it as "the doctrine of signs" in the following terms:

In the nineteenth century, Charles Sanders Peirce defined what he termed "semiotic" (which he sometimes spelled as "semeiotic") as the "quasi-necessary, or formal doctrine of signs", which abstracts "what must be the characters of all signs used by ... an intelligence capable of learning by experience", and which is philosophical logic pursued in terms of signs and sign processes. The Peirce scholar and editor Max H. Fisch claimed in 1978 that "semeiotic" was Peirce's own preferred rendering of Locke's σημιωτική.

Charles W. Morris followed Peirce in using the term "semiotic" and in extending the discipline beyond human communication to animal learning and use of signals.

Ferdinand de Saussure, however, founded his semiotics, which he called semiology, in the social sciences:

While the Saussurean semiotic is dyadic (sign/syntax, signal/semantics), the Peircean semiotic is triadic (sign, object, interpretant), being conceived as philosophical logic studied in terms of signs that are not always linguistic or artificial. The Peircean semiotic addresses not only the external communication mechanism, as per Saussure, but the internal representation machine, investigating not just sign processes, or modes of inference, but the whole inquiry process in general. Peircean semiotics further subdivides each of the three triadic elements into three sub-types. For example, signs can be icons, indices, and symbols.

Yuri Lotman introduced Eastern Europe to semiotics and adopted Locke's coinage as the name to subtitle ("Σημειωτική") his founding at the University of Tartu in Estonia in 1964 of the first semiotics journal, "Sign Systems Studies".

Thomas Sebeok assimilated "semiology" to "semiotics" as a part to a whole, and was involved in choosing the name "Semiotica" for the first international journal devoted to the study of signs.

Saussurean semiotics have exercised a great deal of influence on the schools of Structuralism and Post-Structuralism. Jacques Derrida, for example, takes as his object the Saussurean relationship of signifier and signified, asserting that signifier and signified are not fixed, coining the expression "différance", relating to the endless deferral of meaning, and to the absence of a 'transcendent signified'. For Derrida, 'il n'y a pas de hors-texte' ("there is nothing outside the text").

The importance of signs and signification has been recognized throughout much of the history of philosophy, and in psychology as well. Plato and Aristotle both explored the relationship between signs and the world, and Augustine of Hippo considered the nature of the sign within a conventional system. These theories have had a lasting effect in Western philosophy, especially through scholastic philosophy. (More recently, Umberto Eco, in his "Semiotics and the Philosophy of Language", has argued that semiotic theories are implicit in the work of most, perhaps all, major thinkers.)

The general study of signs that began in Latin with Augustine culminated with the 1632 "Tractatus de Signis" of John Poinsot, and then began anew in late modernity with the attempt in 1867 by Charles Sanders Peirce to draw up a "new list of categories". Peirce aimed to base his new list directly upon experience precisely as constituted by action of signs, in contrast with the list of Aristotle's categories which aimed to articulate within experience the dimension of being that is independent of experience and knowable as such, through human understanding.

The estimative powers of animals interpret the environment as sensed to form a "meaningful world" of objects, but the objects of this world (or "Umwelt", in Jakob von Uexküll's term,) consist exclusively of objects related to the animal as desirable (+), undesirable (–), or "safe to ignore" (0).

In contrast to this, human understanding adds to the animal "Umwelt" a relation of self-identity within objects which transforms objects experienced into "things" as well as +, –, 0 objects. Thus, the generically animal objective world as "Umwelt", becomes a species-specifically human objective world or "Lebenswelt" (life-world), wherein linguistic communication, rooted in the biologically underdetermined "Innenwelt" (inner-world) of humans, makes possible the further dimension of cultural organization within the otherwise merely social organization of non-human animals whose powers of observation may deal only with directly sensible instances of objectivity. This further point, that human culture depends upon language understood first of all not as communication, but as the biologically underdetermined aspect or feature of the human animal's "Innenwelt", was originally clearly identified by Thomas A. Sebeok. Sebeok also played the central role in bringing Peirce's work to the center of the semiotic stage in the twentieth century, first with his expansion of the human use of signs ("anthroposemiosis") to include also the generically animal sign-usage ("zoösemiosis"), then with his further expansion of semiosis (based initially on the work of Martin Krampen, but taking advantage of Peirce's point that an interpretant, as the third item within a sign relation, "need not be mental") to include the vegetative world ("phytosemiosis").

Peirce's distinguished between the interpretant and the interpreter. The interpretant is the internal, mental representation that mediates between the object and its sign. The interpreter is the human who is creating the interpretant. Peirce's "interpretant" notion opened the way to understanding an action of signs beyond the realm of animal life (study of "phytosemiosis" + "zoösemiosis" + "anthroposemiosis" = "biosemiotics"), which was his first advance beyond Latin Age semiotics. Other early theorists in the field of semiotics include Charles W. Morris. Max Black argued that the work of Bertrand Russell was seminal in the field.

Semioticians classify signs or sign systems in relation to the way they are transmitted (see modality). This process of carrying meaning depends on the use of codes that may be the individual sounds or letters that humans use to form words, the body movements they make to show attitude or emotion, or even something as general as the clothes they wear. To coin a word to refer to a "thing" (see lexical words), the community must agree on a simple meaning (a denotative meaning) within their language, but that word can transmit that meaning only within the language's grammatical structures and codes (see syntax and semantics). Codes also represent the values of the culture, and are able to add new shades of connotation to every aspect of life.

To explain the relationship between semiotics and communication studies, communication is defined as the process of transferring data and-or meaning from a source to a receiver. Hence, communication theorists construct models based on codes, media, and contexts to explain the biology, psychology, and mechanics involved. Both disciplines recognize that the technical process cannot be separated from the fact that the receiver must decode the data, i.e., be able to distinguish the data as salient, and make meaning out of it. This implies that there is a necessary overlap between semiotics and communication. Indeed, many of the concepts are shared, although in each field the emphasis is different. In "Messages and Meanings: An Introduction to Semiotics", Marcel Danesi (1994) suggested that semioticians' priorities were to study signification first, and communication second. A more extreme view is offered by Jean-Jacques Nattiez (1987; trans. 1990: 16), who, as a musicologist, considered the theoretical study of communication irrelevant to his application of semiotics.
Semiotics differs from linguistics in that it generalizes the definition of a sign to encompass signs in any medium or sensory modality. Thus it broadens the range of sign systems and sign relations, and extends the definition of language in what amounts to its widest analogical or metaphorical sense. The branch of semiotics that deals with such formal relations between signs or expressions in abstraction from their signification and their interpreters, or – more generally – with formal properties of symbol systems (specifically – with reference to linguistic signs – syntax) is referred to as syntactics.

Peirce's definition of the term "semiotic" as the study of necessary features of signs also has the effect of distinguishing the discipline from linguistics as the study of contingent features that the world's languages happen to have acquired in the course of their evolutions. From a subjective standpoint, perhaps more difficult is the distinction between semiotics and the philosophy of language. In a sense, the difference lies between separate traditions rather than subjects. Different authors have called themselves "philosopher of language" or "semiotician". This difference does "not" match the separation between analytic and continental philosophy. On a closer look, there may be found some differences regarding subjects. Philosophy of language pays more attention to natural languages or to languages in general, while semiotics is deeply concerned with non-linguistic signification. Philosophy of language also bears connections to linguistics, while semiotics might appear closer to some of the humanities (including literary theory) and to cultural anthropology.

Semiosis or "semeiosis" is the process that forms meaning from any organism's apprehension of the world through signs. Scholars who have talked about semiosis in their subtheories of semiotics include C. S. Peirce, John Deely, and Umberto Eco. Cognitive semiotics is combining methods and theories developed in the disciplines of cognitive methods and theories developed in semiotics and the humanities, with providing new information into human signification and its manifestation in cultural practices. The research on cognitive semiotics brings together semiotics from linguistics, cognitive science, and related disciplines on a common meta-theoretical platform of concepts, methods, and shared data.

Cognitive semiotics may also be seen as the study of meaning-making by employing and integrating methods and theories developed in the cognitive sciences. This involves conceptual and textual analysis as well as experimental investigations. Cognitive semiotics initially was developed at the Center for Semiotics at Aarhus University (Denmark), with an important connection with the Center of Functionally Integrated Neuroscience (CFIN) at Aarhus Hospital. Amongst the prominent cognitive semioticians are Per Aage Brandt, Svend Østergaard, Peer Bundgård, Frederik Stjernfelt, Mikkel Wallentin, Kristian Tylén, Riccardo Fusaroli, and Jordan Zlatev. Zlatev later in co-operation with Göran Sonesson established CCS (Center for Cognitive Semiotics) at Lund University, Sweden.

Finite semiotics, developed by Cameron Shackell, aims to unify existing theories of semiotics for application to the post-Baudrillardian world of ubiquitous technology. Its central move is to place the finiteness of thought at the root of semiotics and the sign as a secondary but fundamental analytical construct. The theory contends that the levels of reproduction that technology is bringing to human environments demands this reprioritisation if semiotics is to remain relevant in the face of effectively infinite signs. The shift in emphasis allows practical definitions of many core constructs in semiotics which Shackell has applied to areas such as human computer interaction, creativity theory, and a computational semiotics method for generating semiotic squares from digital texts.

Syntactics is the Morris'ean branch of semiotics that deals with the formal properties of signs and symbols; the interrelation of the signs, without regard to meaning. Semantics deals with the relation of signs to their designata and the objects that they may or do denote; the relation between the signs and the objects to which they apply. Finally, pragmatics deals with the biotic aspects of semiosis, with all the psychological, biological, and sociological phenomena that occur in the functioning of signs; the relation between the sign system and its human (or animal) user. Unlike his mentor George Herbert Mead, Morris was a behaviorist and sympathetic to the Vienna Circle positivism of his colleague, Rudolf Carnap. Morris was accused by John Dewey of misreading Peirce.


The flexibility of human semiotics is well demonstrated in dreams. Sigmund Freud spelled out how meaning in dreams rests on a blend of images, affects, sounds, words, and kinesthetic sensations. In his chapter on "The Means of Representation" he showed how the most abstract sorts of meaning and logical relations can be represented by spatial relations. Two images in sequence may indicate "if this, then that" or "despite this, that". Freud thought the dream started with "dream thoughts" which were like logical, verbal sentences. He believed that the dream thought was in the nature of a taboo wish that would awaken the dreamer. In order to safeguard sleep, the mindbrain converts and disguises the verbal dream thought into an imagistic form, through processes he called the "dream-work".

Applications of semiotics include:

In some countries, its role is limited to literary criticism and an appreciation of audio and visual media. This narrow focus may inhibit a more general study of the social and political forces shaping how different media are used and their dynamic status within modern culture. Issues of technological determinism in the choice of media and the design of communication strategies assume new importance in this age of mass media.

Publication of research is both in dedicated journals such as "Sign Systems Studies", established by Yuri Lotman and published by Tartu University Press; "Semiotica", founded by Thomas A. Sebeok and published by Mouton de Gruyter; "Zeitschrift für Semiotik"; "European Journal of Semiotics"; "Versus" (founded and directed by Umberto Eco), et al.; "The American Journal of Semiotics"; and as articles accepted in periodicals of other disciplines, especially journals oriented toward philosophy and cultural criticism.

The major semiotic book series "Semiotics, Communication, Cognition", published by De Gruyter Mouton (series editors Paul Cobley and Kalevi Kull) replaces the former "Approaches to Semiotics" (more than 120 volumes) and "Approaches to Applied Semiotics" (series editor Thomas A. Sebeok). Since 1980 the Semiotic Society of America has produced an annual conference series: "".

Marketing is another application of semiotics. Epure, Eisenstat and Dinu (2014) said, "semiotics allows for the practical distinction of persuasion from manipulation in marketing communication" (p. 592). Semiotics are used in marketing as a persuasive device to influence buyers to change their attitudes and behaviors in the market place. Two ways that Epure, Eisenstat and Dinu (2014) state that semiotics are used are:
Semiotics analysis is used by scholars and professional researchers as a method to interpret meanings behind symbols and how the meanings are created. Below is an example of how semiotic analysis is utilized in a research paper published in an academic journal: Educational Research and Reviews.

Semiotics has sprouted subfields including, but not limited to, the following:

Pictorial semiotics is intimately connected to art history and theory. It goes beyond them both in at least one fundamental way, however. While art history has limited its visual analysis to a small number of pictures that qualify as "works of art", pictorial semiotics focuses on the properties of pictures in a general sense, and on how the artistic conventions of images can be interpreted through pictorial codes. Pictorial codes are the way in which viewers of pictorial representations seem automatically to decipher the artistic conventions of images by being unconsciously familiar with them.

According to Göran Sonesson, a Swedish semiotician, pictures can be analyzed by three models: (a) the narrative model, which concentrates on the relationship between pictures and time in a chronological manner as in a comic strip; (b) the rhetoric model, which compares pictures with different devices as in a metaphor; and (c) the laokoon (or laocoon) model, which considers the limits and constraints of pictorial expressions by comparing textual mediums that utilize time with visual mediums that utilize space.

The break from traditional art history and theory—as well as from other major streams of semiotic analysis—leaves open a wide variety of possibilities for pictorial semiotics. Some influences have been drawn from phenomenological analysis, cognitive psychology, structuralist, and cognitivist linguistics, and visual anthropology and sociology.

Studies have shown that semiotics may be used to make or break a brand. Culture codes strongly influence whether a population likes or dislikes a brand's marketing, especially internationally. If the company is unaware of a culture's codes, it runs the risk of failing in its marketing. Globalization has caused the development of a global consumer culture where products have similar associations, whether positive or negative, across numerous markets.

Mistranslations may lead to instances of "Engrish" or "Chinglish", terms for unintentionally humorous cross-cultural slogans intended to be understood in English. This may be caused by a sign that, in Peirce's terms, mistakenly indexes or symbolizes something in one culture, that it does not in another. In other words, it creates a connotation that is culturally-bound, and that violates some culture code. Theorists who have studied humor (such as Schopenhauer) suggest that contradiction or incongruity creates absurdity and therefore, humor. Violating a culture code creates this construct of ridiculousness for the culture that owns the code. Intentional humor also may fail cross-culturally because jokes are not on code for the receiving culture.

A good example of branding according to cultural code is Disney's international theme park business. Disney fits well with Japan's cultural code because the Japanese value "cuteness", politeness, and gift giving as part of their culture code; Tokyo Disneyland sells the most souvenirs of any Disney theme park. In contrast, Disneyland Paris failed when it launched as Euro Disney because the company did not research the codes underlying European culture. Its storybook retelling of European folktales was taken as elitist and insulting, and the strict appearance standards that it had for employees resulted in discrimination lawsuits in France. Disney souvenirs were perceived as cheap trinkets. The park was a financial failure because its code violated the expectations of European culture in ways that were offensive.

On the other hand, some researchers have suggested that it is possible to successfully pass a sign perceived as a cultural icon, such as the Coca-Cola or McDonald's logos, from one culture to another. This may be accomplished if the sign is migrated from a more economically-developed to a less developed culture. The intentional association of a product with another culture has been called Foreign Consumer Culture Positioning (FCCP). Products also may be marketed using global trends or culture codes, for example, saving time in a busy world; but even these may be fine-tuned for specific cultures.

Research also found that, as airline industry brandings grow and become more international, their logos become more symbolic and less iconic. The iconicity and symbolism of a sign depends on the cultural convention and, are on that ground in relation with each other. If the cultural convention has greater influence on the sign, the signs get more symbolic value.

A world organisation of semioticians, the International Association for Semiotic Studies, and its journal "Semiotica", was established in 1969. The larger research centers together with teaching program include the semiotics departments at the University of Tartu, University of Limoges, Aarhus University, and Bologna University.




</doc>
<doc id="29305" url="https://en.wikipedia.org/wiki?curid=29305" title="Sojourner Truth">
Sojourner Truth

Sojourner Truth (; born Isabella [Belle] Baumfree; – November 26, 1883) was an African-American abolitionist and women's rights activist. Truth was born into slavery in Swartekill, Ulster County, New York, but escaped with her infant daughter to freedom in 1826. After going to court to recover her son in 1828, she became the first black woman to win such a case against a white man.

She gave herself the name Sojourner Truth in 1843 after she became convinced that God had called her to leave the city and go into the countryside "testifying the hope that was in her". Her best-known speech was delivered extemporaneously, in 1851, at the Ohio Women's Rights Convention in Akron, Ohio. The speech became widely known during the Civil War by the title "Ain't I a Woman?," a variation of the original speech re-written by someone else using a stereotypical Southern dialect, whereas Sojourner Truth was from New York and grew up speaking Dutch as her first language. During the Civil War, Truth helped recruit black troops for the Union Army; after the war, she tried unsuccessfully to secure land grants from the federal government for former slaves (summarised as the promise of "forty acres and a mule").

In 2014, Truth was included in "Smithsonian" magazine's list of the "100 Most Significant Americans of All Time".

Truth was one of the 10 or 12 children born to James and Elizabeth Baumfree (or Bomefree). Colonel Hardenbergh bought James and Elizabeth Baumfree from slave traders and kept their family at his estate in a big hilly area called by the Dutch name Swartekill (just north of present-day Rifton), in the town of Esopus, New York, north of New York City. Charles Hardenbergh inherited his father's estate and continued to enslave people as a part of that estate's property.

When Charles Hardenbergh died in 1806, nine-year-old Truth (known as Belle), was sold at an auction with a flock of sheep for $100 to John Neely, near Kingston, New York. Until that time, Truth spoke only Dutch. She later described Neely as cruel and harsh, relating how he beat her daily and once even with a bundle of rods. In 1808 Neely sold her for $105 to tavern keeper Martinus Schryver of Port Ewen, New York, who owned her for 18 months. Schryver then sold Truth in 1810 to John Dumont of West Park, New York. Although this fourth owner was kindly disposed toward her, considerable tension existed between Truth and Dumont's wife, Elizabeth Waring Dumont, who harassed her and made her life more difficult.

In around 1815, Truth met and fell in love with an enslaved man named Robert from a neighbouring farm. Robert's owner (Charles Catton, Jr., a landscape painter) forbade their relationship; he did not want the people he enslaved to have children with people he was not enslaving, because he would not own the children. One day Robert snuck over to see Truth. When Catton and his son found him, they savagely beat Robert until Dumont finally intervened. Truth never saw Robert again after that day and he died a few years later. The experience haunted Truth throughout her life. Truth eventually married an older enslaved man named Thomas. She bore five children: James, her firstborn, who died in childhood, Diana (1815), the result of a rape by John Dumont, and Peter (1821), Elizabeth (1825), and Sophia (ca. 1826), all born after she and Thomas united.

In 1799, the State of New York began to legislate the abolition of slavery, although the process of emancipating those people enslaved in New York was not complete until July 4, 1827. Dumont had promised to grant Truth her freedom a year before the state emancipation, "if she would do well and be faithful." However, he changed his mind, claiming a hand injury had made her less productive. She was infuriated but continued working, spinning 100 pounds of wool, to satisfy her sense of obligation to him.

Late in 1826, Truth escaped to freedom with her infant daughter, Sophia. She had to leave her other children behind because they were not legally freed in the emancipation order until they had served as bound servants into their twenties. She later said "I did not run off, for I thought that wicked, but I walked off, believing that to be all right."

She found her way to the home of Isaac and Maria Van Wagenen in New Paltz, who took her and her baby in. Isaac offered to buy her services for the remainder of the year (until the state's emancipation took effect), which Dumont accepted for $20. She lived there until the New York State Emancipation Act was approved a year later.

Truth learned that her son Peter, then five years old, had been sold illegally by Dumont to an owner in Alabama. With the help of the Van Wagenens, she took the issue to court and in 1828, after months of legal proceedings, she got back her son, who had been abused by those who were enslaving him. Truth became one of the first black women to go to court against a white man and win the case.

Truth had a life-changing religious experience during her stay with the Van Wagenens, and became a devout Christian. In 1829 she moved with her son Peter to New York City, where she worked as a housekeeper for Elijah Pierson, a Christian Evangelist. While in New York, she befriended Mary Simpson, a grocer on John Street who claimed she had once been enslaved by George Washington. They shared an interest in charity for the poor and became intimate friends. In 1832, she met Robert Matthews, also known as Prophet Matthias, and went to work for him as a housekeeper at the Matthias Kingdom communal colony. Elijah Pierson died, and Robert Matthews and Truth were accused of stealing from and poisoning him. Both were acquitted of the murder, though Matthews was convicted of lesser crimes, served time, and moved west.

In 1839, Truth's son Peter took a job on a whaling ship called the "Zone of Nantucket". From 1840 to 1841, she received three letters from him, though in his third letter he told her he had sent five. Peter said he also never received any of her letters. When the ship returned to port in 1842, Peter was not on board and Truth never heard from him again.

The year 1843 was a turning point for Baumfree. She became a Methodist, and on June 1, she changed her name to Sojourner Truth. She told her friends: "The Spirit calls me, and I must go," and left to make her way traveling and preaching about the abolition of slavery. At that time, Truth began attending Millerite Adventist camp meetings. However, that did not last since Jesus failed to appear in 1843 and then again in 1844. Like many others disappointed, Truth distanced herself from her Millerite friends for a while.

In 1844, she joined the Northampton Association of Education and Industry in Northampton, Massachusetts. Founded by abolitionists, the organization supported women's rights and religious tolerance as well as pacifism. There were, in its four-and-a-half year history, a total of 240 members, though no more than 120 at any one time. They lived on , raising livestock, running a sawmill, a gristmill, and a silk factory. While there, Truth met William Lloyd Garrison, Frederick Douglass, and David Ruggles. In 1846, the group disbanded, unable to support itself. In 1845, she joined the household of George Benson, the brother-in-law of William Lloyd Garrison. In 1849, she visited John Dumont before he moved west.

Truth started dictating her memoirs to her friend Olive Gilbert, and in 1850 William Lloyd Garrison privately published her book, "The Narrative of Sojourner Truth: a Northern Slave". That same year, she purchased a home in what would become the village of Florence in Northampton for $300, and spoke at the first National Women's Rights Convention in Worcester, Massachusetts. In 1854, with proceeds from sales of the narrative and "cartes-de-visite" captioned, "I sell the shadow to support the substance," she paid off the mortgage held by her friend from the community, Samuel L. Hill.

In 1851, Truth joined George Thompson, an abolitionist and speaker, on a lecture tour through central and western New York State. In May, she attended the Ohio Women's Rights Convention in Akron, Ohio, where she delivered her famous extemporaneous speech on women's rights, later known as "Ain't I a Woman?". Her speech demanded equal human rights for all women as well as for all blacks. Advocating for women and African Americans was dangerous and challenging enough, but being one and doing so was far more difficult. The pressures and severity of her speech did not get to Truth, however. Truth took to the stage with a demanding and composed presence. Audience members were baffled by the way she carried herself and were hesitant to believe that she was even a woman, prompting the name of her speech "Ain't I a Woman?" The convention was organized by Hannah Tracy and Frances Dana Barker Gage, who both were present when Truth spoke. Different versions of Truth's words have been recorded, with the first one published a month later in the "Anti-Slavery Bugle" by Rev. Marius Robinson, the newspaper owner and editor who was in the audience. Robinson's recounting of the speech included no instance of the question "Ain't I a Woman?" Nor did any of the other newspapers reporting of her speech at the time. Twelve years later, in May 1863, Gage published another, very different, version. In it, Truth's speech pattern had characteristics of Southern slaves, and the speech was vastly different than the one Robinson had reported. Gage's version of the speech became the historic standard version, and is known as "Ain't I a Woman?" because that question was repeated four times. It is highly unlikely that Truth's own speech pattern was Southern in nature, as she was born and raised in New York, and she spoke only upper New York State low-Dutch until she was nine years old.

In contrast to Robinson's report, Gage's 1863 version included Truth saying her 13 children were sold away from her into slavery. Truth is widely believed to have had five children, with one sold away, and was never known to boast more children. Gage's 1863 recollection of the convention conflicts with her own report directly after the convention: Gage wrote in 1851 that Akron in general and the press in particular were largely friendly to the woman's rights convention, but in 1863 she wrote that the convention leaders were fearful of the "mobbish" opponents. Other eyewitness reports of Truth's speech told a calm story, one where all faces were "beaming with joyous gladness" at the session where Truth spoke; that not "one discordant note" interrupted the harmony of the proceedings. In contemporary reports, Truth was warmly received by the convention-goers, the majority of whom were long-standing abolitionists, friendly to progressive ideas of race and civil rights. In Gage's 1863 version, Truth was met with hisses, with voices calling to prevent her from speaking.

According to Frances Gage's recount in 1863, Truth argued, "That man over there says that women need to be helped into carriages, and lifted over ditches, and to have the best place everywhere. Nobody helps "me" any best place. "And ain't I a woman?"" Truth's "Ain't I a Woman" showed the lack of recognition that black women received during this time and whose lack of recognition will continue to be seen long after her time. "Black women, of course, were virtually invisible within the protracted campaign for woman suffrage," wrote Angela Davis, supporting Truth's argument that nobody gives her "any best place"; and not just her, but black women in general.

Over the next 10 years, Truth spoke before dozens, perhaps hundreds, of audiences. From 1851 to 1853, Truth worked with Marius Robinson, the editor of the Ohio "Anti-Slavery Bugle", and traveled around that state speaking. In 1853, she spoke at a suffragist "mob convention" at the Broadway Tabernacle in New York City; that year she also met Harriet Beecher Stowe. In 1856, she traveled to Battle Creek, Michigan, to speak to a group called the "Friends of Human Progress". In 1858, someone interrupted a speech and accused her of being a man; Truth opened her blouse and revealed her breasts.

Northampton Camp Meeting – 1844, Northampton, Massachusetts: At a camp meeting where she was participating as an itinerant preacher, a band of "wild young men" disrupted the camp meeting, refused to leave, and threatened to burn down the tents. Truth caught the sense of fear pervading the worshipers and hid behind a trunk in her tent, thinking that since she was the only black person present, the mob would attack her first. However, she reasoned with herself and resolved to do something: as the noise of the mob increased and a female preacher was "trembling on the preachers' stand," Truth went to a small hill and began to sing "in her most fervid manner, with all the strength of her most powerful voice, the hymn on the resurrection of Christ." Her song, "It was Early in the Morning," gathered the rioters to her and quieted them. They urged her to sing, preach, and pray for their entertainment. After singing songs and preaching for about an hour, Truth bargained with them to leave after one final song. The mob agreed and left the camp meeting.

Abolitionist Convention – 1840s, Boston, Massachusetts: William Lloyd Garrison invited Sojourner Truth to give a speech at an annual antislavery convention. Wendell Phillips was supposed to speak after her, which made her nervous since he was known as such a good orator. So Truth sang a song, "I am Pleading for My people," which was her own original composition sung to the tune of Auld Lang Syne.

Mob Convention – September 7, 1853: At the convention, young men greeted her with "a perfect storm," hissing and groaning. In response, Truth said, "You may hiss as much as you please, but women will get their rights anyway. You can't stop us, neither". Sojourner, like other public speakers, often adapted her speeches to how the audience was responding to her. In her speech, Sojourner speaks out for women's rights. She incorporates religious references in her speech, particularly the story of Esther. She then goes on to say that, just as women in scripture, women today are fighting for their rights. Moreover, Sojourner scolds the crowd for all their hissing and rude behavior, reminding them that God says to "Honor thy father and thy mother."

American Equal Rights Association – May 9–10, 1867: Her speech was addressed to the American Equal Rights Association, and divided into three sessions. Sojourner was received with loud cheers instead of hisses, now that she had a better-formed reputation established. "The Call" had advertised her name as one of the main convention speakers. For the first part of her speech, she spoke mainly about the rights of black women. Sojourner argued that because the push for equal rights had led to black men winning new rights, now was the best time to give black women the rights they deserve too. Throughout her speech she kept stressing that "we should keep things going while things are stirring" and fears that once the fight for colored rights settles down, it would take a long time to warm people back up to the idea of colored women's having equal rights.

In the second sessions of Sojourner's speech, she utilized a story from the Bible to help strengthen her argument for equal rights for women. She ended her argument by accusing men of being self-centered, saying, "man is so selfish that he has got women's rights and his own too, and yet he won't give women their rights. He keeps them all to himself." For the final session of Sojourner's speech, the center of her attention was mainly on women's right to vote. Sojourner told her audience that she owned her own house, as did other women, and must therefore pay taxes. Nevertheless, they were still unable to vote because they were women. Black women who were enslaved were made to do hard manual work, such as building roads. Sojourner argues that if these women were able to perform such tasks, then they should be allowed to vote because surely voting is easier than building roads.

Eighth Anniversary of Negro Freedom – New Year's Day, 1871: On this occasion the Boston papers related that "...seldom is there an occasion of more attraction or greater general interest. Every available space of sitting and standing room was crowded". She starts off her speech by giving a little background about her own life. Sojourner recounts how her mother told her to pray to God that she may have good masters and mistresses. She goes on to retell how her masters were not good to her, about how she was whipped for not understanding English, and how she would question God why he had not made her masters be good to her. Sojourner admits to the audience that she had once hated white people, but she says once she met her final master, Jesus, she was filled with love for everyone. Once enslaved folks were emancipated, she tells the crowd she knew her prayers had been answered.
That last part of Sojourner's speech brings in her main focus. Some freed enslaved people were living on government aid at that time, paid for by taxpayers. Sojourner announces that this is not any better for those colored people than it is for the members of her audience. She then proposes that black people are given their own land. Because a portion of the South's population contained rebels that were unhappy with the abolishment of slavery, that region of the United States was not well suited for colored people. She goes on to suggest that colored people be given land out west to build homes and prosper on.

Second Annual Convention of the American Woman Suffrage Association – Boston, 1871: In a brief speech, Truth argued that women's rights were essential, not only to their own well-being, but "for the benefit of the whole creation, not only the women, but all the men on the face of the earth, for they were the mother of them."

In 1856, Truth bought a neighboring lot in Northampton, but she did not keep the new property for long. On September 3, 1857, she sold all her possessions, new and old, to Daniel Ives and moved to Battle Creek, Michigan, where she rejoined former members of the Millerite movement who had formed the Seventh-day Adventist Church. Antislavery movements had begun early in Michigan and Ohio. Here, she also joined the nucleus of the Michigan abolitionists, the Progressive Friends, some who she had already met at national conventions. According to the 1860 census, her household in Harmonia included her daughter, Elizabeth Banks (age 35), and her grandsons James Caldwell (misspelled as "Colvin"; age 16) and Sammy Banks (age 8).

During the Civil War, Truth helped recruit black troops for the Union Army. Her grandson, James Caldwell, enlisted in the 54th Massachusetts Regiment. In 1864, Truth was employed by the National Freedman's Relief Association in Washington, D.C., where she worked diligently to improve conditions for African-Americans. In October of that year, she met President Abraham Lincoln. In 1865, while working at the Freedman's Hospital in Washington, Truth rode in the streetcars to help force their desegregation.

Truth is credited with writing a song, "", for the 1st Michigan Colored Regiment; it was said to be composed during the war and sung by her in Detroit and Washington, D.C. It is sung to the tune of "John Brown's Body" or "The Battle Hymn of the Republic". Although Truth claimed to have written the words, it has been disputed (see "Marching Song of the First Arkansas").

In 1867, Truth moved from Harmonia to Battle Creek. In 1868, she traveled to western New York and visited with Amy Post, and continued traveling all over the East Coast. At a speaking engagement in Florence, Massachusetts, after she had just returned from a very tiring trip, when Truth was called upon to speak she stood up and said "Children, I have come here like the rest of you, to hear what I have to say."

In 1870, Truth tried to secure land grants from the federal government to former enslaved people, a project she pursued for seven years without success. While in Washington, D.C., she had a meeting with President Ulysses S. Grant in the White House. In 1872, she returned to Battle Creek, became active in Grant's presidential re-election campaign, and even tried to vote on Election Day, but was turned away at the polling place.

Truth spoke about abolition, women's rights, prison reform, and preached to the Michigan Legislature against capital punishment. Not everyone welcomed her preaching and lectures, but she had many friends and staunch support among many influential people at the time, including Amy Post, Parker Pillsbury, Frances Gage, Wendell Phillips, William Lloyd Garrison, Laura Smith Haviland, Lucretia Mott, Ellen G. White, and Susan B. Anthony."

Several days before Sojourner Truth died, a reporter came from the "Grand Rapids Eagle" to interview her. "Her face was drawn and emaciated and she was apparently suffering great pain. Her eyes were very bright and mind alert although it was difficult for her to talk." Truth died at her Battle Creek home on November 26, 1883. On November 28 her funeral was held at the Congregational-Presbyterian Church officiated by its pastor, the Reverend Reed Stuart. Some of the prominent citizens of Battle Creek acted as pall-bearers. Truth was buried in the city's Oak Hill Cemetery.

The calendar of saints of the Episcopal Church remembers Sojourner Truth annually, together with Elizabeth Cady Stanton, Amelia Bloomer and Harriet Ross Tubman on July 20 individually or Nov 26. The calendar of saints of the Lutheran Church remembers Sojourner Truth together with Harriet Tubman on March 10.

A larger-than-life sculpture of Sojourner Truth by Tina Allen was dedicated in 1999, which is the estimated bicentennial of Sojourner's birth, in Battle Creek's Monument Park. The 12-foot tall Sojourner monument is cast bronze. There is also a statue of Sojourner Truth in Florence, Massachusetts, as well as one, in bronze, on the campus of the University of California, San Diego, by sculptor Manuelita Brown.

The U.S. Treasury Department announced in 2016 that an image of Sojourner Truth will appear on the back of a newly designed $10 bill along with Lucretia Mott, Susan B. Anthony, Elizabeth Cady Stanton, Alice Paul and the 1913 Woman Suffrage Procession. Designs for new $5, $10 and $20 bills will be unveiled in 2020 in conjunction with the 100th anniversary of American women winning the right to vote via the Nineteenth Amendment to the United States Constitution.

On September 19, 2018, the U.S. Secretary of the Navy Ray Mabus announced the name of the last ship of a six unit construction contract as USNS Sojourner Truth (T-AO 210). This ship with be part of the latest John Lewis-class of Fleet Replenishment Oilers named in honor of U.S. civil and human rights heroes currently under construction at General Dynamics NASSCO in San Diego, CA.

Other honors and commemorations include (by year):

As of March 2015, K-12 schools in several states are named after Truth, as is Sojourner–Douglass College in Baltimore.






</doc>
<doc id="29306" url="https://en.wikipedia.org/wiki?curid=29306" title="STOVL">
STOVL

A short take-off and vertical landing aircraft (STOVL aircraft) is a fixed-wing aircraft that is able to take off from a short runway (or take off vertically if it does not have a heavy payload) and land vertically (i.e. with no runway). The formal NATO definition (since 1991) is:

On aircraft carriers, non-catapult-assisted, fixed-wing short takeoffs are accomplished with the use of thrust vectoring, which may also be used in conjunction with a runway "ski-jump". Use of STOVL tends to allow aircraft to carry a larger payload compared to vertical take-off and landing (VTOL), while still only requiring a short runway. The most famous examples are the Hawker Siddeley Harrier and the Sea Harrier. Although technically VTOL aircraft, they are operationally STOVL aircraft due to the extra weight carried at take-off for fuel and armaments. The same is true of the F-35B Lightning II, which demonstrated VTOL capability in test flights but is operationally a STOVL.

In 1951, the Lockheed XFV and the Convair XFY Pogo tailsitters were both designed around the Allison YT40 turboprop engine driving contra-rotating propellers.

The British Hawker P.1127 took off vertically in 1960, and demonstrated conventional take-off in 1961. It was developed into the Hawker Siddeley Harrier which flew in 1967.

In 1962, Lockheed built the XV-4 Hummingbird for the U.S. Army. It sought to "augment" available thrust by injecting the engine exhaust into an ejector pump in the fuselage. First flying vertically in 1963, it suffered a fatal crash in 1964. It was converted into the XV-4B Hummingbird for the U.S. Air Force as a testbed for separate, vertically mounted lift engines, similar to those used in the Yak-38 Forger. That plane flew and later crashed in 1969. The Ryan XV-5 Vertifan, which was also built for the U.S. Army at the same time as the Hummingbird, experimented with gas-driven lift fans. That plane used fans in the nose and each wing, covered by doors which resembled half garbage can lids when raised. However, it crashed twice, and proved to generate a disappointing amount of lift, and was difficult to transition to horizontal flight.

Of dozens of VTOL and V/STOL designs tried from the 1950s to 1980s, only the subsonic Hawker Siddeley Harrier and Yak-38 Forger reached operational status, with the Forger being withdrawn after the fall of the Soviet Union.

Rockwell International built, and then abandoned, the Rockwell XFV-12 supersonic fighter which had an unusual wing which opened up like window blinds to create an ejector pump for vertical flight. It never generated enough lift to get off the ground despite developing 20,000 lbf of thrust. The French had a nominally Mach 2 Dassault Mirage IIIV fitted with no less than 8 lift engines that flew (and crashed), but did not have enough space for fuel or payload for combat missions. The German EWR VJ 101 used swiveling engines mounted on the wingtips with fuselage mounted lift engines, and the VJ 101C X1 reached supersonic flight (Mach 1.08) on 29 July 1964. The supersonic Hawker Siddeley P.1154, which competed with the Mirage IIIV for use in NATO, was cancelled even as the aircraft were being built.

NASA uses the abbreviation SSTOVL for Supersonic Short Take-Off / Vertical Landing, and as of 2012, the X-35B/F-35B are the only aircraft to conform with this combination within one flight.

The experimental Mach 1.7 Yakovlev Yak-141 did not find an operational customer, but similar rotating rear nozzle technology is used on the F-35B. The F-35B Lightning II entered service on July 31, 2015. 

Larger STOVL designs were considered, the Armstrong Whitworth AW.681 cargo aircraft was under development when cancelled in 1965. The Dornier Do 31 got as far as three experimental aircraft before cancellation in 1970.

Although mostly a VTOL design, the V-22 Osprey has increased payload when taking off from a short runway.


</doc>
<doc id="29307" url="https://en.wikipedia.org/wiki?curid=29307" title="Russian aircraft carrier Admiral Kuznetsov">
Russian aircraft carrier Admiral Kuznetsov

Admiral Flota Sovetskogo Soyuza Kuznetsov ( "Admiral of the Fleet of the Soviet Union Kuznetsov", originally the name of the fifth ) is an aircraft carrier (heavy aircraft cruiser in Russian classification) serving as the flagship of the Russian Navy. It was built by the Black Sea Shipyard, the sole manufacturer of Soviet aircraft carriers, in Nikolayev within the Ukrainian Soviet Socialist Republic (SSR). The initial name of the ship was Riga; it was launched as Leonid Brezhnev, embarked on sea trials as Tbilisi, and finally named "Admiral Flota Sovetskogo Soyuza Kuznetsov" after Admiral of the fleet of the Soviet Union Nikolay Gerasimovich Kuznetsov.

It was originally commissioned in the Soviet Navy, and was intended to be the lead ship of the two-ship . However, its sister ship "Varyag" was still incomplete when the Soviet Union collapsed in 1991. The second hull was eventually sold by Ukraine to the People's Republic of China, completed in Dalian and commissioned as .

The design of "Admiral Kuznetsov" class implies a mission different from that of the United States Navy's carriers. The term used by her builders to describe the Russian ships is (TAVKR) – "heavy aircraft-carrying cruiser" – intended to support and defend strategic missile-carrying submarines, surface ships, and naval missile-carrying aircraft of the Russian Navy.

"Admiral Kuznetsov"s main fixed-wing aircraft is the multi-role Sukhoi Su-33. It can perform air superiority, fleet defence, and air support missions and can also be used for direct fire support of amphibious assault, reconnaissance and placement of naval mines. The carrier also carries the Kamov Ka-27 and Kamov Ka-27S helicopters for anti-submarine warfare, search and rescue, and small transport.

For take-off of fixed wing aircraft, "Admiral Kuznetsov" uses a ski-jump at the end of her bow. On take-off aircraft accelerate toward and up the ski-jump using their afterburners. This results in the aircraft leaving the deck at a higher angle and elevation than on an aircraft carrier with a flat deck and catapults. The ski-jump take-off is less demanding on the pilot, since the acceleration is lower, but results in a clearance speed of only requiring an aircraft design which will not stall at those speeds. The "cruiser" role is facilitated by "Admiral Kuznetsov"s complement of 12 long-range surface-to-surface anti-ship P-700 Granit (NATO reporting name: Shipwreck) cruise missiles. As a result, this armament is the basis for the ship's Russian type designator of "heavy aircraft-carrying missile cruiser".

"Admiral Kuznetsov"s designation as an aircraft-carrying cruiser is very important under the Montreux Convention, as it allows the ship to transit the Turkish Straits. The Convention prohibits countries from sending an aircraft carrier heavier than 15,000 tons through the Straits. Since the ship was built in the Ukrainian SSR, "Admiral Kuznetsov" would have been stuck in the Black Sea if Turkey had refused permission to pass into the Mediterranean Sea. However, the Convention does not limit the displacement of capital ships operated by Black Sea powers. Turkey allowed "Admiral Kuznetsov" to transit the Straits, and no signatory to the Montreux Convention ever issued a formal protest of her classification as an aircraft-carrying cruiser.

"Admiral Flota Sovetskovo Soyuza Kuznetsov", constructed at Chernomorskiy Shipyard, also known as Nikolayev South Shipyard, in Nikolayev, now Mykolaiv, Ukrainian SSR, was launched in 1985, and became fully operational in 1995. An official ceremony marking the start of construction took place on 1 September 1982; in fact she was laid down in 1983. The vessel was first named "Riga", then the name was changed to "Leonid Brezhnev", this was followed by "Tbilisi". Finally, on 4 October 1990, she was renamed "Admiral Flota Sovetskovo Soyuza N.G. Kuznetsov", referred to in short as "Admiral Kuznetsov". The ship was 71% complete by mid-1989. In November 1989 she undertook her first aircraft operation trials. In December 1991, she sailed from the Black Sea to join the Northern Fleet. Only from 1993 on did she receive aircraft.

From 23 December 1995 through 22 March 1996 "Admiral Kuznetsov" made her first 90-day Mediterranean deployment with 13 Su-33, 2 Su-25 UTG, and 11 helicopters aboard. The deployment of the Russian Navy's flagship was undertaken to mark the 300th anniversary of the establishment of the Russian Navy in October 1696. The deployment was to allow the carrier, which was accompanied by a frigate, destroyer and oiler, to adapt to the Mediterranean climate and to perform continuous flight operations until 21:00 each day, as the Barents Sea only receives about one hour of sunlight during this time of year. During that period the carrier lay at anchor off the port of Tartus, Syria. Her aircraft often made flights close to the Israeli shore line and were escorted by Israeli F-16s. During the deployment, a severe water shortage occurred due to evaporators breaking down.

At the end of 1997 she remained immobilized in a Northern Fleet shipyard, awaiting funding for major repairs, which were halted when they were only 20% complete. The overhaul was completed in July 1998, and the ship returned to active service in the Northern fleet on 3 November 1998.

"Admiral Kuznetsov" remained in port for two years before preparing for another Mediterranean deployment scheduled for the winter of 2000–2001. This deployment was cancelled due to the explosion and sinking of the nuclear-powered submarine . "Admiral Kuznetsov" participated in the "Kursk" rescue and salvage operations in late 2000. Plans for further operations were postponed or cancelled. In late 2003 and early 2004, "Admiral Kuznetsov" went to sea for inspection and trials. In October 2004, she participated in a fleet exercise of the Russian Navy in the Atlantic Ocean. During a September 2005 exercise, a Su-33 accidentally fell from the carrier into the Atlantic Ocean. On 27 September 2006, it was announced that "Admiral Kuznetsov" would return to service in the Northern Fleet by the year's end, following another modernization to correct some technical issues. Admiral Vladimir Masorin, Commander-in-Chief of the Russian Navy, also stated that Su-33 fighters assigned to her would return after undergoing their own maintenance and refits.

From 5 December 2007 through 3 February 2008 "Admiral Kuznetsov" made its second Mediterranean deployment. On 11 December 2007, "Admiral Kuznetsov" passed by Norwegian oil platforms in the North Sea, outside Bergen, Norway. Su-33 fighters and Kamov helicopters were launched from "Admiral Kuznetsov" while within international waters; Norwegian helicopter services to the rigs were halted due to the collision risk with the Russian aircraft. "Admiral Kuznetsov" later participated in an exercise on the Mediterranean Sea, together with 11 other Russian surface ships and 47 aircraft, performing three tactical training missions using live and simulated air and surface missile launches. "Admiral Kuznetsov" and her escorts returned to Severomorsk on 3 February 2008. Following maintenance, she returned to sea on 11 October 2008 for the Stability-2008 strategic exercises held in the Barents Sea. On 12 October 2008, Russian President Dmitry Medvedev visited the ship during the exercise.

From 5 December 2008 through 2 March 2009, "Admiral Kuznetsov" made her third Mediterranean deployment. On 5 December 2008, she and several other vessels left Severomorsk for the Atlantic for a combat training tour, including joint drills with Russia's Black Sea Fleet and visits to several Mediterranean ports. On 7 January 2009, a small fire broke out onboard "Admiral Kuznetsov" while anchored off Turkey. The fire, caused by a short-circuit, led to the death of one crew member by carbon monoxide poisoning. On 16 February 2009, she was involved in a large oil spill, along with other Russian naval vessels, while refuelling off the south coast of Ireland. On 2 March 2009, "Admiral Kuznetsov" returned to Severomorsk, and on September 2010 she left dry dock after scheduled repairs and preparations for a training mission in the Barents Sea, later that month.

The Russian Main Navy Staff announced that "Admiral Kuznetsov" would begin a deployment to the Atlantic and Mediterranean in December 2011. In November 2011, it was announced that "Admiral Kuznetsov" would lead a squadron to Russia's naval facility in Tartus.

A Russian naval spokesman announced via the "Izvestia" daily that "The call of the Russian ships in Tartus should not be seen as a gesture towards what is going on in Syria... This was planned already in 2010 when there were no such events there" noting that "Admiral Kuznetsov" would also be making port calls in Beirut, Genoa and Cyprus. On 29 November 2011, Army General Nikolay Makarov, Chief of the Russian General Staff, said that Russian ships in the Mediterranean were due to exercises rather than events in Syria, and noted that "Admiral Kuznetsov"s size does not allow her to moor in Tartus.
On 6 December 2011, "Admiral Kuznetsov" and her escort ships departed the Northern Fleet homebase for a Mediterranean deployment to exercise with ships from the Russian Baltic and Black Sea Fleets. On 12 December 2011, "Admiral Kuznetsov" and her escorts, were spotted northeast of Orkney off the coast of northern Scotland, the first such time she had deployed near the UK. shadowed the group for a week; due to severe weather, the group took shelter in international waters in the Moray Firth, some from the UK coast. "Admiral Kuznetsov" then sailed around the top of Scotland and into the Atlantic past western Ireland, where she conducted flight operations with her Sukhoi Su-33 'Flanker' jets and Kamov Ka-27 helicopters in international airspace. On 8 January 2012, "Admiral Kuznetsov" anchored near shore outside Tartus while other ships from her escort entered the port to use the leased Russian naval support facility to replenish their supplies, after which all ships continued their deployment on 9 January. On 17 February 2012, "Admiral Kuznetsov" returned to her homebase of Severomorsk.

On 1 June 2013, it was announced that the ship would return to the Mediterranean by the end of the year, and on 17 December, "Admiral Kuznetsov" departed her homebase for the Mediterranean.
On 1 January 2014, "Admiral Kuznetsov" celebrated New Year's Day while at anchor in international waters of the Moray Firth off northeast Scotland. The anchorage allowed replenishment of ship's supplies and respite for the crew from stormy weather off the southwest coast of Norway. She then proceeded to the Mediterranean Sea, docking in Cyprus on 28 February. In May 2014, the ship and her task group: the "Kirov"-class nuclear-powered cruiser "Petr Velikiy"; tankers; "Sergey Osipov", "Kama" and "Dubna"; ocean-going tug "Altay" and Ropucha-class landing ship "Minsk" (a part of the Black Sea Fleet), passed the UK while sailing for home. Despite financial and technical problems, resulting in limited operations for the ship, it is expected that "Admiral Kuznetsov" will remain in active service until at least 2030.

In April 2010, it was announced that by late 2012, the ship would enter Severodvinsk Sevmash shipyard for a major refit and modernization, including upgrades to obsolete electronics and sensor equipment, installation of a new anti-aircraft system (Pantsir-M) and an increase of the air wing with the removal of the P-700 Granit anti-ship missiles. Possible upgrades include exchanging the troublesome steam powerplant to gas-turbine, or even nuclear propulsion, and installation of catapults to the angled deck.

The Navy expected to acquire Mikoyan MiG-29K aircraft for "Admiral Kuznetsov" by 2011; this later was confirmed by a defense sub-contractor The MiG-29Ks would replace the 19 carrier-based Su-33 fighters, a resource set to expire by 2015. Producing more Su-33s is possible but not cost-effective for such small volumes; the MiG-29K is more convenient as the Indian Navy also placed an order for a total for 45, reducing development and manufacture costs. India paid $730 million for the development and delivery of 16 MiG-29Ks; 24 more for the Russian Navy would cost about $1 billion.

Following ongoing maintenance, "Admiral Kuznetsov" set sail on 15 October 2016 from the Kola Bay for the Mediterranean, accompanied by seven other Russian Navy vessels including the nuclear-powered battlecruiser "Pyotr Velikiy" and two Udaloy-class destroyers. The carrier was accompanied by an ocean-going tugboat, as a precaution due to potential propulsion failure. The airwing included 6-8 Su-33 fighters, four Mig-29KR/KUBR multi-role aircraft, Ka-52K "Katran" navalised attack helicopters, Ka-31R "Helix" AEW&C helicopters and Ka-27PS "Helix-D" search and rescue helicopters. All the Su-33 aircraft had been upgraded with the Gefest SVP-24 bombsights for free-fall bombs, giving them a limited ground-attack capability. Analysts, including Mikhail Barabanov of the Moscow Defense Brief, suggested that a lack of trained pilots restricted the number of fixed-wing aircraft that could be deployed from the carrier.

On 21 October, the "Admiral Kuznetsov" battle group sailed through the English Channel, escorted by Royal Navy ships, while UK Defence Minister Michael Fallon speculated that the taskforce was designed to "test" the British naval response. On 26 October 2016, the ship was reported to have passed through the Strait of Gibraltar and refuelled at sea off North Africa the following day. On 3 November 2016, the "Admiral Kuznetsov" battle group paused off the east coast of Crete. On 14 November 2016, a MiG-29K crashed into the sea after taking off from the carrier. The pilot ejected safely from the plane and was rescued by helicopter. According to initial reports from Russian officials, the crash was a result of technical malfunction, but it was later revealed that the plane had actually run out of fuel waiting to land while the crew was attempting to repair a broken arresting wire. The carrier commander could have diverted the aircraft to land at a nearby airbase, but hesitated in the hope that the arrestor gear would be repaired in time.

On 15 November 2016, "Admiral Kuznetsov", took part in "a large-scale operation against the positions of terrorist groups Islamic State and Al-Nusra, in the provinces of Idlib and Homs" in Syria by launching Su-33 fighter strikes. This was the first time a Russian aircraft carrier would take part in combat operations. Russian Defence Ministry later reported that 30 militants had been killed as a result of those strikes, including 3 field commanders, among them" "Abul Baha al-Asfari, leader of Al-Nusra reserves in the provinces of Homs and Aleppo. Al-Asfari had also planned and led several insurgent attacks on the city of Aleppo itself. The Su-33s reportedly used precision bombs. On 3 December 2016, an Su-33 crashed into the sea after attempting to land on the carrier. The plane crashed on its second attempt to land on the aircraft carrier in good weather conditions. The pilot was safely recovered by a search and rescue helicopter. Initially it was suspected that the plane missed the wires and failed to go around, falling short of the bow of the warship, but later it was revealed that the arresting cable failed to hold the aircraft, and was damaged in the attempt. Following the two incidents, the air wing was transferred to shore at Khmeimim Air Base near Latakia, Syria to continue military operations while the carrier's arresting gear issues were addressed.

In early January 2017, it was announced that "Admiral Kuznetsov" and her battlegroup would be ceasing operations in Syria and returning to Russia as part of a scaling back of Russian involvement in the conflict. During her deployment off Syria, aircraft from "Admiral Kuznetsov" carried out 420 combat missions, hitting 1,252 hostile targets. On 11 January 2017, "Admiral Kuznetsov" was conducting live-fire training exercises in the Mediterranean off the coast of Libya. The Russian defence ministry announced that on 11 January, "Admiral Kuznetsov" was visited by Libya′s military leader Khalifa Haftar, who had a video conference with Russian defence minister Sergey Shoygu while on board.

On 20 January, "Admiral Kuznetsov" was sighted passing west through the Strait of Gibraltar and six days later she was escorted back along the English Channel by three Eurofighter Typhoons of the Royal Air Force and the Type 23 frigate . She arrived back in Severomorsk on 9 February. On 23 February 2017, President Vladimir Putin said that the ship′s deployment to the Mediterranean had been his personal initiative.

The carrier started an overhaul and modernisation in the first quarter of 2017. This is expected to extend its service life by 25 years. "Admiral Kuznetsov" is expected to undergo modernization at the 35th Ship Repair Plant in Murmansk between 2020 and 2021, upgrading the ship's power plant and electronics systems.

On 30 October 2018, "Admiral Kuznetsov" was damaged when Russia's biggest floating dry dock, "PD-50", sank and one of the dock's 70-ton cranes crashed onto the ship's flight deck leaving behind a hole in the flight deck. One person was reported missing and four injured as the dry dock sank in Kola Bay. "Admiral Kuznetsov" was in the process of being removed from the dock when the incident happened, and was towed to a nearby yard after the incident. According to Alexei Rakhmanov, the president of the United Shipbuilding Corporation, the cost for repairs of the damage was estimated to be RUB70 million (about US$1 million) and should not affect the timing of the currently underway overhaul and modernization of the ship. Although it is unclear how the overhaul and repair schedule would not be affected with the dry dock sunk.

The fallen crane was removed within two to three months. In late May 2019, seven months later, information posted on Digital Forensic Research Lab's blog suggested that repair work of the aircraft carrier was underway. That same month it was also announced that two graving docks in Roslyakovo, Murmansk Oblast would be merged and enlarged to accommodate "Admiral Kuznetsov", with work taking 1.5 years. 

In December 2019, a major fire broke out on board "Admiral Kuznetsov" as work continued on the ship's refit. Two people died and fourteen suffered injuries from the fire and smoke inhalation.




</doc>
<doc id="29311" url="https://en.wikipedia.org/wiki?curid=29311" title="Subaru Forester">
Subaru Forester

The Subaru Forester is a compact crossover SUV (sport utility vehicle) manufactured since 1997 by Subaru. Available in Japan from 1997, the Forester shares its platform with the Impreza. It has been awarded "Motor Trend's" 2009 and 2014 SUV of the Year and The Car Connection's Best Car To Buy 2014.

The Forester was introduced at the Tokyo Motor Show in November 1995 as the Streega concept, and made available for sale February 1997 in Japan, and to the US market in 1997 for MY1998. The Forester was one of the first emerging crossover SUVs. It was built in the style of a car, but had a taller stance, higher h-point seating, and an all-wheel drive drivetrain. Subaru advertising employed the slogan "SUV tough, Car Easy". It used the Impreza platform but with the larger 2.5-liter DOHC EJ25D four-cylinder boxer engine from the Subaru Outback, making at 5,600 rpm and of torque at 4,000 rpm.

In Japan, the Forester replaced the Subaru Impreza Gravel Express, known in the US as the Subaru Outback Sport. However, the Outback Sport remained in production for the U.S. market. The Forester appeared after the introduction of the Nissan Rasheen in Japan with a similar appearance, and the Forester's Japanese competitors include the Toyota RAV4, Mitsubishi RVR, and the Suzuki Grand Vitara. Due to the Forester's low center of gravity, it meets the United States federal safety standards for passenger vehicles, and does not require a "risk of rollover" warning label on the driver's visor. Size and price-wise, it fits between the shared Impreza platform, and the larger Legacy.

The automatic transmissions used on AWD equipped vehicles will normally send 60% of the engine's torque to the front wheels and 40% to the rear wheels, using a computer-controlled, continuously variable, multi-plate transfer clutch. When the transmission detects a speed difference between the front and rear axle sets, the transmission progressively sends power to the rear wheels. Under slip conditions it can achieve an equal split in front and rear axle speeds.

When accelerating or driving uphill, the vehicle's weight shifts rearward, reducing front wheel traction, causing the transmission to automatically send torque to the rear wheels to compensate. When braking or driving downhill, the vehicle's weight shifts towards the front, reducing rear wheel traction. The transmission again compensates by sending torque to the front wheels for better steering control and braking performance. If the automatic is placed in reverse or first gear, the transmission divides the torque 50/50 to both front and rear wheels. The manual transmission cars are set up with a near 50/50 torque split as a base setting, and it varies from there. Essentially, the manual cars are set up with more bias towards the rear than the automatic cars.

The trim levels were the basic model "L" and the fully equipped "S" for the USA versions.

Forester L came with a high level of standard equipment, including ABS, air conditioning, power windows, power locks, cruise control, digital temperature gauge, multi-reflector halogen headlights, fog lights, roof rack, rear window defogger, trailer harness connector, reclining front bucket seats with adjustable lumbar support, tilt steering, tinted glass, AM/FM/cassette stereo with its antenna laminated in the left-rear quarter window. Notably new in 2001 were the three-point seatbelts for all five seating positions, including force limiters in front and height-adjustable shoulder belt anchors for front and rear outboard positions, plus rear seat headrests for all three seating positions.

Forester S adds a viscous limited-slip differential, rear disc brakes, 16 × 6.5-inch alloy wheels with 215/60R16 tires (the L uses 15 × 6-inch steel wheels), upgraded moquette upholstery, heated front seats with net storage pockets in back, dual vanity mirrors, heated sideview mirrors, heated windshield wipers, and keyless entry. New equipment for 2001 included Titanium pearl paint for the bumpers and cladding; six-disc in-dash CD sound system; leather-wrapped steering wheel, shift knob and handbrake handle; variable intermittent wipers with de-icers and driver's side fin; and the five-spoke alloy wheels. Some models were equipped with the $1,000 optional premium package on the Forester S, including monotone paint (Sedona Red Pearl), power moonroof, front side-impact airbags, and gold accent wheels. Other options were the $800 automatic transmission, $39 chrome tailpipe cover and $183 auto-dimming rear-view mirror with compass, bringing the sticker price to $25,412 including $495 delivery (U.S. dollars quoted).

There was a change in body styling for all 2001–2002 models, and the 2001/2002 GT spec also had a change in engine management and power output was increased from 125 to .



The U.S. market was offered the car starting in 1997 with either the 2.5-liter DOHC (MY1998 only) or 2.5-liter SOHC naturally aspirated engine (no turbocharged engines). In 2000 Subaru updated the exterior with a modest facelift to the front, rear and sides, and the interior's dashboard MY2001.

MY1998 - 2000 versions sold in the United States:

The MY2001-2002 versions carried over adding the S Premium model, albeit with the aforementioned mild redesign:

The second generation was introduced as a 2003 model at the 2002 Chicago Auto Show, based on the new Impreza platform, featuring several fine-tune improvements over the past model. The 2003 Forester features weight-saving refinements such as an aluminum hood, perforated rails, and a hydro-formed front sub-frame. The most noticeable change was the offering of 2.5 L versions (normally aspirated and turbocharged) and in the U.S. the introduction of the turbo charged 2.5-liter model.

In the U.S., the naturally aspirated (non-turbo) X (previously L) and XS (previously S) were released in 2003. In 2004, the turbocharged XT version was released. However, the same model had been available since the late 1990s elsewhere in the world. The X and XS models feature a 2.5 L SOHC engine, while the XT model features a 2.5 L turbocharged DOHC engine. Both engines have timing belt driven camshafts. The XT model uses the same Mitsubishi TD04 turbocharger used in the Subaru Impreza WRX. Those seeking additional power for their Forester XT can replace the turbocharger and intercooler with used STI components which are readily available. All Forester 2.5 L engines are of the interference engine type.

In 2004, Subaru launched an STI variant of the Forester, the Forester STI, for the Japanese Market, sharing the same engine as the 2005 Subaru Impreza WRX STI. Starting with the 2004 XT, the turbocharged version had active valve control system AVCS cylinder heads. The i-AVLS active valve lift system became standard on the naturally aspirated version of the Forester in 2006. This increased horsepower and torque figures to 173 HP and 166 ft-lbs. The 2006 XT received a higher compression ratio to 8.4:1 from 8.2:1. This increased the XT's power to 230 HP and 235 ft-lbs.

For the 2006 model year, Subaru gave the SG a facelift, using redesigned headlights, tail-lights, bonnet, grille, front bumper and side-moldings.

MY03-04 Models has a 4-Star ANCAP safety rating. MY05 Forester Model had a mid-life update, which update increased its ANCAP safety rating to 5 Stars.

In 2006, the turbocharged engine (powering the Forester XT) was awarded International Engine of the Year. This engine is also used in the Subaru Impreza WRX, as well as the re-badged Saab 9-2XAero.

All of the 2.5-liter 4-cylinder engines for this generation have a timing belt made of rubber and cord. A belt must be replaced at . These engines are interference engines, meaning that if the timing belt breaks or stretches, the pistons will hit the valves, resulting in an engine teardown, and a likely rebuild. Also, if this belt is replaced around 105,000 miles, it is a good idea to change the water pump, thermostat, belt tensioner and all the idler pulleys for this belt. The water pump and thermostat are behind this belt.
In Australia for the Series II (MY06) cars, Subaru changed the recommended service interval for the timing belt replacement from 100,000 kilometers to 125,000 kilometers.
The 2.5-liter 4-cylinder engine in the first-generation Foresters featured head gaskets which were prone to premature failure. For 2003 and later, this problem was addressed with a revised, higher performing design, but is still a problem.

The U.S. Market was offered the car with either the 2.5 SOHC naturally aspirated engine, or the 2.5 DOHC turbocharged version
added in 2004.

2004 versions sold in the United States:

In 2005, the L.L. Bean edition is added:

In 2006, styling is updated, Active valve lift system is added to non-turbo engines to improve power and efficiency, XS model deleted, Premium model added:

In 2007, a bottle holder was added to front door panels, the 'Sports' trim level was added, which changed some interior and exterior features and added the VDT/VDC transmission to the XT Sports turbo Automatic model:

In 2008, TPMS was added, L.L. Bean model deletes rear load-leveling suspension, but gains radio upgrade, the XT Turbo Limited models gets the VDT/VDC Auto transmission as well:

The Forester had three main models available in Australia until July 2005:

The Forester at the time had three main models available in Australia from August 2005 Series II:

The Luxury Pack edition was an option on all models - allowing for leather seats and a sunroof. These options were also included with the Columbia edition. The Weekender edition included fog lights, roof racks and alloy wheels.
Standard with the Manufacture Year 2006 (MY06) Forester came with larger side mirrors with indicator lights, curtain airbags giving a 5 star safety rating, remodelled centre console and exterior with a new look nose, lights and bumpers and the rear lost the large Subaru badge under the rear window.

The Forester was sold in India as a Chevrolet alongside other unique Chevrolet models sold there. However, since General Motors no longer holds an ownership stake in Subaru's parent company, Fuji Heavy Industries, sales in India of the Chevrolet-badged Forester have ended.

A look-alike was produced by Yema and known as the Yema F99 in China. It was a similar design to the pre-facelifted model. Production ran from 2012 to 2014. The engine was a 1.5l 4 cylinder mated to a 5 speed manual gearbox. The car was not related to the Forester even though they look very similar.

Despite the existence of counterfeiting in China, the second generation of Subaru Forester can still be purchased by Chinese consumers back from 2004 to 2007. 

2004 Version sold in China:

2006 Version (Facelift) sold in China:

2007 Version (Facelift) sold in China:

The third generation Forester began to move away from a traditional wagon design towards becoming a crossover SUV. It was larger in nearly every dimension and featured a sloping roof line with more cargo space. Subaru unveiled the model year 2008 Forester in Japan on December 25, 2007. The North American version made its debut at the 2008 North American International Auto Show in Detroit.

Styling was by Subaru Chief Designer Mamoru Ishii. The dimensions derive from engineers using the basic body structure of the Japanese-spec Impreza wagon with the rear platform of the U.S.-spec Impreza sedan. The Forester's wheelbase was increased , with overall increases of in length, in width and in height.

The independent double wishbone rear suspension was redesigned for better handling and a smoother ride. A "Sportshift" mode was added to the four-speed computer-controlled automatic transmission. The in-dash, touch-screen satellite navigation system became Bluetooth compatible, and integrated with a premium stereo. A six-speaker surround sound enhancement was optional.

The new model added to the Forester's wheelbase, improving interior space and cargo room ( expandable to ). Ground clearance was .

The Forester was available in Europe from 2008 with either the 2.0-liter EJ20 () 196 Nm gasoline engine with Active Valve Control System (AVCS) matched to either five-speed manual or four-speed automatic gearbox, or an all-new diesel-powered horizontally opposed Subaru EE boxer engine, and six-speed manual gearbox. The new model was introduced at the 2008 Paris Motor Show in October. The diesel engine produces a power output of 147 PS () 350 Nm.

In the UK, the gasoline-powered Forester was offered in the popular X and XS models, while trim level for the diesel models were X, XC, and XS NavPlus.

In Russia, Belarus and Ukraine 2.5 and 2.5 Turbo engines were also available.

In Netherlands gasoline or gasoline with an extra Liquefied petroleum gas installation (LPG)(mostly aftermarket installs by LPG Installers(if people made more than 15.000/35.000 KM A year above this is mostly diesel)) models Intro, Comfort, Luxury, Premium. Diesel Models Comfort, Luxury, Premium. towing a caravan or trailer gasoline or gasoline with LPG M=2000KG A=1500KG Diesel M=2000KG (KG=KiloGrams)

There were seven specifications with various trim and performance levels:
Summary of standard trim and equipment over different Australian models.

The Forester trim levels were the 2.5X, the 2.5X Premium, the 2.5X Limited and the 2.5XT and 2.5XT Limited both with turbo. The interior color was either black or light gray, with three upholstery selections, including leather. Nine exterior colors were offered, with four colors in a pearlescent appearance.

Starting July 2009, Subaru no longer offered a special-edition L.L. Bean trim level on the Forester.

The USA 2.5X model was certified PZEV emissions (Rated instead ), with a badge attached to the rear of the vehicle on the bottom right-hand side of the tailgate. All other USA models were certified LEV2. The PZEV Forester was available for sale in all fifty states, unlike other manufacturers who only sold PZEV-certified vehicles in states that had adopted California emission standards. The engine without the turbo runs on unleaded gasoline rated at 87 octane, and the turbo engine (EJ255) requires premium fuel rated minimum 91 octane.

Safety equipment included front airbags with side curtain airbags and front passenger side airbags (for a total of six airbags) and brake assist that detects panic-braking situations and applies maximum braking force more quickly. The five-speed manual transmission was equipped with Incline Start Assist.

Some of the standard equipment found on the 2.5X included Subaru's VDC (Vehicle Dynamics Control), 16 inch steel wheels, and an auxiliary audio jack for MP3 players. Optional equipment included 17 inch alloy wheels, panoramic moonroof, heated front seats and heated side-view mirrors. The L.L. Bean edition added automatic climate control, leather upholstery, an upgraded stereo with six speakers and a six disc in-dash CD changer over the four-speaker stereo with single disc CD player, and an in-dash navigation system, as well as L.L. Bean signature floor mats and rear cargo tray.

The 2.5 XT came with the premium stereo standard, as well as 17-inch alloy wheels, and the panoramic moonroof. The 2.5 XT Limited added leather upholstery with heated front seats, in-dash navigation, a rear spoiler, and automatic climate control. For 2009, XT models came only with a four-speed automatic with Sport Shift.

The Forester XTI concept vehicle used the 2.5-liter intercooled turbo engine from the Subaru WRX STI, six-speed manual transmission, 18 × 8-inch S204 forged alloy wheels with Yokohama Advan Neova 255/40R18 performance tires, adjustable coil-over suspension, Brembo brakes with four-piston front calipers, 2-piston rear calipers, Super Sport ABS and Electronic Brake-force Distribution (EBD), leather and Alcantara sport seats, a special instrument cluster, front dash and center console and leather-wrapped steering wheel. Engine is rated and torque.

The vehicle was unveiled in the 2008 SEMA Show.

Subaru produced a specialized vehicle for the National Ski Patrol based on the 2.5XT turbo. It includes diamond plate floor, rear steel walls, a 9,500-pound winch and a roof-mounted toboggan. The vehicle was unveiled in the 2008 SEMA Show.

In 2010 for the 2011 model year, the Subaru Forester received a new grille insert. An optional roof-rack would raise total height by 2.5 inches. The naturally aspirated Foresters were equipped with an all new third generation motor with SOHC 2.5l EJ25 and 2.0l flat four EJ20.

Pre-facelift styling

Post-facelift styling
The fourth-generation Forester was unveiled in the 2012 Guangzhou Motor Show, followed by the 2013 New York International Auto Show.

Changes to the line-up include:

Japan models went on sale in November 2012. Early model includes 2.0i, 2.0i-L, 2.0i-L EyeSight, 2.0i-S EyeSight, 2.0XT (280 PS), 2.0XT EyeSight (280 PS). 2.0i engine models include six-speed manual (2.0i, 2.0i-L) or Lineartronic CVT transmission; 2.0XT (280 PS) engine models include Lineartronic CVT transmission.

Asian models went on sale in March 2013 as 2014 model year. Early model includes 2.0i-L, 2.0i Premium and 2.0XT. ASEAN production of the Subaru Forester began in February 2016. Malaysia-based Tan Chong Motor Assemblies (TCMA) will assemble approximately 10,000 Forester units annually for Malaysia, Thailand and Indonesia respectively.

US models went on sale in March 2013 as 2014 model year vehicles. Early models include 2.5i in base, Premium, Limited and top-line Touring versions, and performance-oriented turbocharged 2.0XT (253 PS) in Premium and Touring versions. Base and Premium model 2014 Foresters can be equipped with the manual six-speed transmission or the Lineartronic CVT. All other models are equipped with the Lineartronic CVT. An option on Limited/Touring 2.5i and Premium/Touring 2.0XT is new X-Mode control and Hill Descent Control (HDC) features. These are not available on other models.

According to IIHS (Insurance Institute for Highway Safety) the 2014 Forester achieved Good crash test ratings in Small Overlap Front, Moderate Overlap Front, Side, Roof Strength, and Head Restraining & Seats categories. The Forester had not been rated Good in the Small Overlap Front test until modifications were made for the 2014 model year. The small overlap test, introduced in 2012 by the IIHS, simulates a frontal collision on 25 percent of the driver's side front corner. Since its adoption, the IIHS has noticed several automakers making non-symmetrical modifications to their vehicles. Another small overlap test was conducted on a number of vehicles, including a 2014 Forester, but was conducted on the passenger side instead. The crash test showed substantially more intrusion into the passenger side than into the driver's side of the Forester, it would have been rated Marginal

The 2014 Forester has a new feature called X Mode that allows owners to go through more extreme conditions both on the road and off. The concept is that any driver, regardless of skill level, can drive safely on wet roads or muddy areas. It works by monitoring wheel-slip on all four wheels; should one or more wheels begin to slip, X Mode kicks in and applies the brakes to the affected wheel which results in a transfer of power to the opposite wheel. After it is engaged by a simple push button, X Mode stays engaged up until the vehicle's speed is about then disengages itself.

The 2014 top-of-the-line Touring model Forester offers Subaru's EyeSight driver assist technology that uses stereoscopic CCD cameras mounted on either side of the rearview mirror. Eyesight offers several driver assist technologies/features which include:

The system can be manually turned on or off. Being an optical, instead of radar, based system, it has limitations in limited visibility situations; driving into the sun, fog, or where the windshield is not cleared (snow, mud, etc.) may cause the system to disengage.

The 2014 and 2015 models had a major revamp of interior comfort. The passenger seat is higher, the sound system has been upgraded, the rear bench seats are higher and the console is re-positioned for the person riding in the center. The manual transmission models were also upgraded to a six-speed transmission instead of the previous generation's five-speed transmission. Engines during these year models do have an issue, this seems to be related predominantly to US manufactured vehicles and Subaru Australia have had no significant issues with excess oil consumption in Australia with the Japanese manufactured vehicles.(Source Subaru Australia and extensive research prior to purchase by Australian customer (KRH)) with oil consumption, along with numerous other automotive manufactures at the time. Subaru had its settlement of a lawsuit approved by a US Court in August 2016. The US complainants experienced over 1.5 quarts of oil consumed between change intervals with low-level dash lights coming on. The settlement provided for such complainants to receive an extended eight year warranty on the engine, allowing for an engine rebuild for that excessive oil consumption. Some speculation on this is the use of different style piston rings on the engine style change and synthetic oil.

Pre-facelift styling

Post-facelift styling (2017–2018)

The 2019 Subaru Forester was revealed on March 28, 2018 at the New York International Auto Show. Like contemporary Subaru models, the 2019 model year moved the Forester to the Subaru Global Platform.

In the US market, the 2019 Subaru Forester is available in the following trims:

As with all auto makers, each trim comes with a different level of standard features. The 2019 model year also comes standard with of ground clearance.

All 2019 Subaru Foresters have one of three versions of Subaru's Symmetrical All Wheel Drive (AWD) system. The trim level determines which system is installed. All provide a nominal torque split biased 60 front to 40 rear.


For the first time, all trim levels of Forester are only available with one engine: Subaru's new FB25 DI. The engine is a non-turbo, direct injection flat (boxer) 4 cylinder producing at 5800 rpm and at 4400 rpm. There is also only a single transmission option: the Lineartronic CVT.

All Foresters come standard with Subaru's Eyesight Driver Assistance Technology.

For the first time Subaru DriverFocus™ Distraction Mitigation System comes standard on the Touring trim, which provides an alert when it detects the driver is distracted or is drowsy. In addition, the DriverFocus system is able to recognize five different drivers and will set seat and mirror positions and climate control settings accordingly.

In 2019, the Start/Stop feature was added to all Forester models. This feature turns off the engine when the brake pedal is pressed. The engine restarts only after the gas pedal is pressed. There is some evidence that it negatively affects starter life and distracts the driver in stop and go traffic. This feature is on by default every time the engine is turned . The feature can be disabled after the engine is on.

Subaru introduced the e-BOXER hybrid powertrain for the European-market Forester and XV at Geneva in March 2019; the e-BOXER integrates an electric motor into the Lineartronic CVT to improve fuel economy and increase power. The e-BOXER powertrain features a modified FB20 rated at at 5,600–6,000 rpm and of torque at 4,000 rpm. Like the first-generation XV Crosstrek Hybrid, the Forester e-BOXER adds a single electric motor rated at maximum output. The battery for the traction motor is placed above the rear axle, improving the front/rear weight balance.



</doc>
<doc id="29313" url="https://en.wikipedia.org/wiki?curid=29313" title="Second-system effect">
Second-system effect

The second-system effect (also known as second-system syndrome) is the tendency of small, elegant, and successful systems, to be succeeded by over-engineered, bloated systems, due to inflated expectations and overconfidence.

The phrase was first used by Fred Brooks in his book "The Mythical Man-Month", first published in 1975. It described the jump from a set of simple operating systems on the IBM 700/7000 series to OS/360 on the 360 series, which happened in 1964.




</doc>
<doc id="29316" url="https://en.wikipedia.org/wiki?curid=29316" title="Sandinista National Liberation Front">
Sandinista National Liberation Front

The Sandinista National Liberation Front (, FSLN) is a socialist political party in Nicaragua. Its members are called Sandinistas in both English and Spanish. The party is named after Augusto César Sandino, who led the Nicaraguan resistance against the United States occupation of Nicaragua in the 1930s.

The FSLN overthrew Anastasio Somoza DeBayle in 1979, ending the Somoza dynasty, and established a revolutionary government in its place. Having seized power, the Sandinistas ruled Nicaragua from 1979 to 1990, first as part of a Junta of National Reconstruction. Following the resignation of centrist members from this Junta, the FSLN took exclusive power in March 1981. They instituted a policy of mass literacy, devoted significant resources to health care, and promoted gender equality but came under international criticism for human rights abuses, mass execution and oppression of indigenous peoples. A U.S.-backed group, known as the Contras, was formed in 1981 to overthrow the Sandinista government and was funded and trained by the Central Intelligence Agency. In 1984 elections were held but were boycotted by some opposition parties. The FSLN won the majority of the votes, and those who opposed the Sandinistas won approximately a third of the seats. The civil war between the Contras and the government continued until 1989. After revising the constitution in 1987, and after years of fighting the Contras, the FSLN lost the 1990 election to Violeta Barrios de Chamorro but retained a plurality of seats in the legislature.

The FSLN is now Nicaragua's sole leading party. The FSLN often polls in opposition to the much smaller Constitutionalist Liberal Party, or PLC. In the 2006 Nicaraguan general election, former FSLN President Daniel Ortega was re-elected President of Nicaragua with 38.7% of the vote compared to 29% for his leading rival, bringing in the country's second Sandinista government after 17 years of the opposition winning elections. Ortega and the FSLN were re-elected again in the presidential elections of November 2011 and of November 2016.

The Sandinistas took their name from Augusto César Sandino (1895–1934), the leader of Nicaragua's nationalist rebellion against the US occupation of the country during the early 20th century (ca. 1922–1934). The suffix "-ista" is simply the Spanish equivalent of "-ist".

Sandino was assassinated in 1934 by the Nicaraguan National Guard (), the US-equipped police force of Anastasio Somoza, whose family ruled the country from 1936 until they were overthrown by the Sandinistas in 1979.

The FSLN originated in the milieu of various oppositional organizations, youth and student groups in the late 1950s and early 1960s. The University of Léon, and the National Autonomous University of Nicaragua (UNAN) in Managua were two of the principal centers of activity. Inspired by the Revolution and the FLN in Algeria, the FSLN itself was founded in 1961 by Carlos Fonseca, , Tomás Borge and others as "The National Liberation Front" (FLN). Only Tomás Borge lived long enough to see the Sandinista victory in 1979.

The term "Sandinista", was added two years later, establishing continuity with Sandino's movement, and using his legacy in order to develop the newer movement's ideology and strategy. By the early 1970s, the FSLN was launching limited military initiatives.

On December 23, 1972, a magnitude 6.2 earthquake leveled the capital city, Managua. The earthquake killed 10,000 of the city's 400,000 residents and left another 50,000 homeless. About 80% of Managua's commercial buildings were destroyed. President Anastasio Somoza Debayle's National Guard embezzled much of the international aid that flowed into the country to assist in reconstruction, and several parts of downtown Managua were never rebuilt. The president gave reconstruction contracts preferentially to family and friends, thereby profiting from the quake and increasing his control of the city's economy. By some estimates, his personal wealth rose to US$400 million in 1974.

In December 1974, a guerrilla group affiliated with FSLN directed by Eduardo Contreras and Germán Pomares seized government hostages at a party in the house of the Minister of Agriculture in the Managua suburb Los Robles, among them several leading Nicaraguan officials and Somoza relatives. The siege was carefully timed to take place after the departure of the US ambassador from the gathering. At 10:50 pm, a group of 15 young guerrillas and their commanders, Pomares and Contreras, entered the house. They killed the Minister, who tried to shoot them, during the takeover. The guerrillas received US$2 million ransom, and had their official communiqué read over the radio and printed in the newspaper "La Prensa".

Over the next year, the guerrillas also succeeded in getting 14 Sandinista prisoners released from jail, and with them, were flown to Cuba. One of the released prisoners was Daniel Ortega, who would later become the president of Nicaragua. The group also lobbied for an increase in wages for National Guard soldiers to 500 córdobas ($71 at the time). The Somoza government responded with further censorship, intimidation, torture, and murder.

In 1975, Somoza imposed a state of siege, censoring the press, and threatening all opponents with internment and torture. Somoza's National Guard also increased its violence against individuals and communities suspected of collaborating with the Sandinistas. Many of the FSLN guerrillas were killed, including its leader and founder Carlos Fonseca in 1976. Fonseca had returned to Nicaragua in 1975 from his exile in Cuba to try to reunite fractures that existed in the FSLN. He and his group were betrayed by a peasant who informed the National Guard that they were in the area. The guerrilla group was ambushed, and Fonseca was wounded in the process. The next morning Fonseca was executed by the National Guard.

Following the FSLN's defeat at the battle of Pancasán in 1967, the organization adopted the "Prolonged Popular War" ("Guerra Popular Prolongada", GPP) theory as its strategic doctrine. The GPP was based on the "accumulation of forces in silence": while the urban organization recruited on the university campuses and robbed money from banks, the main cadres were to permanently settle in the north central mountain zone. There they would build a grassroots peasant support base in preparation for renewed rural guerrilla warfare.

As a consequence of the repressive campaign of the National Guard, in 1975 a group within the FSLN's urban mobilization arm began to question the viability of the GPP. In the view of the young orthodox Marxist intellectuals, such as Jaime Wheelock, economic development had turned Nicaragua into a nation of factory workers and wage-earning farm laborers. Wheelock's faction was known as the "Proletarian Tendency".

Shortly after, a third faction arose within the FSLN. The "Insurrectional Tendency", also known as the "Third Way" or "Terceristas", led by Daniel Ortega, his brother Humberto Ortega, and Mexican-born Victor Tirado Lopez, was more pragmatic and called for tactical, temporary alliances with non-communists, including the right-wing opposition, in a popular front against the Somoza regime. By attacking the Guard directly, the Terceristas would demonstrate the weakness of the regime and encourage others to take up arms.

In October 1977, a group of prominent Nicaraguan professionals, business leaders, and clergymen allied with the Terceristas to form ""El Grupo de los Doce"" (The Group of Twelve) in Costa Rica. The group's main idea was to organize a provisional government in Costa Rica. The new strategy of the Terceristas also included unarmed strikes and rioting by labor and student groups coordinated by the FSLN's "United People's Movement" (Movimiento Pueblo Unido – MPU).

On 10 January 1978, Pedro Joaquín Chamorro, the editor of the opposition newspaper "La Prensa" and leader of the "Democratic Union of Liberation" (Unión Democrática de Liberación – UDEL), was assassinated. Although his assassins were not identified at the time, evidence implicated President Somoza's son and other members of the National Guard. Spontaneous riots followed in several cities, while the business community organized a general strike demanding Somoza's resignation.

The Terceristas carried out attacks in early February in several Nicaraguan cities. The National Guard responded by further increasing repression and using force to contain and intimidate all government opposition. The nationwide strike that paralyzed the country for ten days weakened the private enterprises and most of them decided to suspend their participation in less than two weeks. Meanwhile, Somoza asserted his intention to stay in power until the end of his presidential term in 1981. The United States government showed its displeasure with Somoza by suspending all military assistance to the regime, but continued to approve economic assistance to the country for humanitarian reasons.

In August, the Terceristas staged a hostage-taking. Twenty-three Tercerista commandos led by Edén Pastora seized the entire Nicaraguan congress and took nearly 1,000 hostages, including Somoza's nephew José Somoza Abrego and cousin Luis Pallais Debayle. Somoza gave in to their demands and paid a $500,000 ransom, released 59 political prisoners (including GPP chief Tomás Borge), broadcast a communiqué with FSLN's call for general insurrection and gave the guerrillas safe passage to Panama.

A few days later six Nicaraguan cities rose in revolt. Armed youths took over the highland city of Matagalpa. Tercerista cadres attacked Guard posts in Managua, Masaya, León, Chinandega and Estelí. Large numbers of semi-armed civilians joined the revolt and put the Guard garrisons of the latter four cities under siege. The September Insurrection of 1978 was subdued at the cost of several thousand, mostly civilian, casualties. Members of all three factions fought in these uprisings, which began to blur the divisions and prepare the way for unified action.

In early 1979, President Jimmy Carter and the United States no longer supported the Somoza regime, but did not want a left-wing government to take power in Nicaragua. The moderate "Broad Opposition Front" ("Frente Amplio Opositor" – FAO) which opposed Somoza was made up of a conglomeration of dissidents within the government as well as the "Democratic Union of Liberation" (UDEL) and the "Twelve", representatives of the Terceristas (whose founding members included Casimiro A. Sotelo, later to become Ambassador to the U.S. AND Canada representing the FSLN). The FAO and Carter came up with a plan that would remove Somoza from office but left no part in government power for the FSLN. The FAO's efforts lost political legitimacy, as Nicaraguans protested that they did not want ""Somocismo sin Somoza"" (Somocism without Somoza).

The "Twelve" abandoned the coalition in protest and formed the "National Patriotic Front" ("Frente Patriotico Nacional" – FPN) together with the "United People's Movement" (MPU). This strengthened the revolutionary organizations as tens of thousands of youths joined the FSLN and the fight against Somoza. A direct consequence of the spread of the armed struggle in Nicaragua was the official reunification of the FSLN that took place on 7 March 1979. Nine men, three from each tendency, formed the National Directorate which would lead the reunited FSLN. They were: Daniel Ortega, Humberto Ortega and Víctor Tirado (Terceristas); Tomás Borge, , and Henry Ruiz (GPP faction); and Jaime Wheelock, Luis Carrión and Carlos Núñez.

The FSLN evolved from one of many opposition groups to a leadership role in the overthrow of the Somoza regime. By mid-April 1979, five guerrilla fronts opened under the joint command of the FSLN, including an internal front in the capital city Managua. Young guerrilla cadres and the National Guardsmen were clashing almost daily in cities throughout the country. The strategic goal of the Final Offensive was the division of the enemy's forces. Urban insurrection was the crucial element because the FSLN could never hope to achieve simple superiority in men and firepower over the National Guard.

On June 4, a general strike was called by the FSLN to last until Somoza fell and an uprising was launched in Managua. On June 16, the formation of a provisional Nicaraguan government in exile, consisting of a five-member Junta of National Reconstruction, was announced and organized in Costa Rica. The members of the new junta were Daniel Ortega (FSLN), Moisés Hassan (FPN), Sergio Ramírez (the "Twelve"), Alfonso Robelo (MDN) and Violeta Barrios de Chamorro, the widow of "La Prensa"s director Pedro Joaquín Chamorro. By the end of that month, with the exception of the capital, most of Nicaragua was under FSLN control, including León and Matagalpa, the two largest cities in Nicaragua after Managua.

On July 9, the provisional government in exile released a government program, in which it pledged to organize an effective democratic regime, promote political pluralism and universal suffrage, and ban ideological discrimination, except for those promoting the "return of Somoza's rule". On July 17, Somoza resigned, handed over power to Francisco Urcuyo, and fled to Miami. While initially seeking to remain in power to serve out Somoza's presidential term, Urcuyo ceded his position to the junta and fled to Guatemala two days later.

On July 19, the FSLN army entered Managua, culminating the first goal of the Nicaraguan revolution. The war left approximately 30,000-50,000 dead and 150,000 Nicaraguans in exile. The five-member junta entered the Nicaraguan capital the next day and assumed power, reiterating its pledge to work for political pluralism, a mixed economic system, and a nonaligned foreign policy.

The Sandinistas inherited a country with a debt of 1.6 billion dollars (US), an estimated 30,000 to 50,000 war dead, 600,000 homeless, and a devastated economic infrastructure. To begin the task of establishing a new government, they created a Council (or ) of National Reconstruction, made up of five appointed members. Three of the appointed members—Sandinista militants Daniel Ortega, Moises Hassan, and novelist Sergio Ramírez (a member of Los Doce "the Twelve")—belonged to the FSLN. Two opposition members, businessman Alfonso Robelo, and Violeta Barrios de Chamorro (the widow of Pedro Joaquín Chamorro), were also appointed. Only three votes were needed to pass law.

The FSLN also established a Council of State, subordinate to the junta, which was composed of representative bodies. However, the Council of State only gave political parties twelve of forty-seven seats; the rest of the seats were given to Sandinista mass-organizations. Of the twelve seats reserved for political parties, only three were not allied to the FSLN. Due to the rules governing the Council of State, in 1980 both non-FSLN junta members resigned. Nevertheless, as of the 1982 State of Emergency, opposition parties were no longer given representation in the council. The preponderance of power also remained with the Sandinistas through their mass organizations, including the Sandinista Workers' Federation (), the Luisa Amanda Espinoza Nicaraguan Women's Association (), the National Union of Farmers and Ranchers (), and most importantly the Sandinista Defense Committees (CDS). The Sandinista-controlled mass organizations were extremely influential over civil society and saw their power and popularity peak in the mid-1980s.

Upon assuming power, the FSLN's official political platform included the following: nationalization of property owned by the Somozas and their supporters; land reform; improved rural and urban working conditions; free unionization for all workers, both urban and rural; price fixing for commodities of basic necessity; improved public services, housing conditions, education; abolition of torture, political assassination and the death penalty; protection of democratic liberties; equality for women; non-aligned foreign policy; formation of a "popular army" under the leadership of the FSLN and Humberto Ortega.

The FSLN's literacy campaign sent teachers into the countryside and within six months, half a million people had been taught rudimentary reading, bringing the national illiteracy rate down from over 50% to just under 12%. Over 100,000 Nicaraguans participated as literacy teachers. One of the stated aims of the literacy campaign was to create a literate electorate which would be able to make informed choices at the promised elections. The successes of the literacy campaign was recognized by UNESCO with the award of a Nadezhda Krupskaya International Prize.

The FSLN also created neighborhood groups similar to the Cuban Committees for the Defense of the Revolution, called Sandinista Defense Committees ( or CDS). Especially in the early days following the overthrow of Somoza, the CDS's served as "de facto" units of local governance. Their obligations included political education, the organization of Sandinista rallies, the distribution of food rations, organization of neighborhood/regional cleanup and recreational activities, and policing to control looting, and the apprehension of counter-revolutionaries. The CDS's organized civilian defense efforts against Contra activities and a network of intelligence systems in order to apprehend their supporters. These activities led critics of the Sandinistas to argue that the CDS was a system of local spy networks for the government used to stifle political dissent, and the CDS did hold limited powers—such as the ability to suspend privileges such as driver licenses and passports—if locals refused to cooperate with the new government. After the initiation of heavier U.S. military involvement in the Nicaraguan conflict the CDS was empowered to enforce wartime bans on political assembly and association with other political parties (i.e., parties associated with the "Contras").

By 1980, conflicts began to emerge between the Sandinista and non-Sandinista members of the governing junta. Violeta Chamorro and Alfonso Robelo resigned from the governing junta in 1980, and rumours began that members of the Ortega junta would consolidate power amongst themselves. These allegations spread, and rumors intensified that it was Ortega's goal to turn Nicaragua into a state modeled after Cuban socialism. In 1979 and 1980, former Somoza supporters and ex-members of Somoza's National Guard formed irregular military forces, while the original core of the FSLN began to splinter. Armed opposition to the Sandinista Government eventually divided into two main groups: The Fuerza Democrática Nicaragüense (FDN), a U.S. supported army formed in 1981 by the CIA, U.S. State Department, and former members of the widely condemned Somoza-era Nicaraguan National Guard; and the Alianza Revolucionaria Democratica (ARDE) Democratic Revolutionary Alliance, a group that had existed since before the FSLN and was led by Sandinista founder and former FSLN supreme commander, Edén Pastora, a.k.a. "Commander Zero". and Milpistas, former anti-Somoza rural militias, which eventually formed the largest pool of recruits for the Contras. Although independent and often at conflict with each other, these guerrilla bands—along with several others—all became generally known as "Contras" (short for "", en. "counter-revolutionaries").

The opposition militias were initially organized and largely remained segregated according to regional affiliation and political backgrounds. They conducted attacks on economic, military, and civilian targets. During the Contra war, the Sandinistas arrested suspected members of the Contra militias and censored publications they accused of collaborating with the enemy (i.e. the U.S., the FDN, and ARDE, among others).

In March 1982 the Sandinistas declared an official State of Emergency. They argued that this was a response to attacks by counter-revolutionary forces. The State of Emergency lasted six years, until January 1988, when it was lifted.

Under the new "Law for the Maintenance of Order and Public Security" the "Tribunales Populares Anti-Somozistas" allowed for the indefinite holding of suspected counter-revolutionaries without trial. The State of Emergency, however, most notably affected rights and guarantees contained in the "Statute on Rights and Guarantees of Nicaraguans". Many civil liberties were curtailed or canceled such as the freedom to organize demonstrations, the inviolability of the home, freedom of the press, freedom of speech, and the freedom to strike.

All independent news program broadcasts were suspended. In total, twenty-four programs were cancelled. In addition, Sandinista censor Nelba Cecilia Blandón issued a decree ordering all radio stations to take broadcasts from government radio station La Voz de La Defensa de La Patria every six hours.

The rights affected also included certain procedural guarantees in the case of detention including habeas corpus. The State of Emergency was not lifted during the 1984 elections. There were many instances where rallies of opposition parties were physically broken up by Sandinista Youth or pro-Sandinista mobs. Opponents to the State of Emergency argued its intent was to crush resistance to the FSLN. James Wheelock justified the actions of the Directorate by saying "... We are annulling the license of the false prophets and the oligarchs to attack the revolution."

Some emergency measures were taken before 1982. In December 1979 special courts called "Tribunales Especiales" were established to speed up the processing of 7,000-8,000 National Guard prisoners. These courts operated through relaxed rules of evidence and due process and were often staffed by law students and inexperienced lawyers. However, the decisions of the "Tribunales Especiales" were subject to appeal in regular courts. Many of the National Guard prisoners were released immediately due to lack of evidence. Others were pardoned or released by decree. By 1986 only 2,157 remained in custody and only 39 were still being held in 1989 when they were released under the Esquipulas II agreement.

On October 5, 1985 the Sandinistas broadened the 1982 State of Emergency and suspended many more civil rights. A new regulation also forced any organization outside of the government to first submit any statement it wanted to make public to the censorship bureau for prior approval.

The FSLN lost power in the presidential election of 1990 when Daniel Ortega was defeated in an election for the Presidency of Nicaragua by Violetta Chamorro.

Upon assuming office in 1981, U.S. President Ronald Reagan condemned the FSLN for joining with Cuba in supporting "Marxist" revolutionary movements in other Latin American countries such as El Salvador. His administration authorized the CIA to begin financing, arming and training rebels, most of whom were the remnants of Somoza's National Guard, as anti-Sandinista guerrillas that were branded "counter-revolutionary" by leftists ( in Spanish). This was shortened to "Contras", a label the force chose to embrace. Edén Pastora and many of the indigenous guerrilla forces, who were not associated with the "Somozistas", also resisted the Sandinistas.

The Contras operated out of camps in the neighboring countries of Honduras to the north and Costa Rica (see Edén Pastora cited below) to the south. As was typical in guerrilla warfare, they were engaged in a campaign of economic sabotage in an attempt to combat the Sandinista government and disrupted shipping by planting underwater mines in Nicaragua's Corinto harbour, an action condemned by the International Court of Justice as illegal. The U.S. also sought to place economic pressure on the Sandinistas, and, as with Cuba, the Reagan administration imposed a full trade embargo.

The Contras also carried out a systematic campaign to disrupt the social reform programs of the government. This campaign included attacks on schools, health centers and the majority of the rural population that was sympathetic to the Sandinistas. Widespread murder, rape, and torture were also used as tools to destabilize the government and to "terrorize" the population into collaborating with the Contras. Throughout this campaign, the Contras received military and financial support from the CIA and the Reagan Administration. This campaign has been condemned internationally for its many human rights violations. Contra supporters have often tried to downplay these violations, or countered that the Sandinista government carried out much more. In particular, the Reagan administration engaged in a campaign to alter public opinion on the Contras that has been termed "white propaganda". In 1984, the International Court of Justice judged that the United States Government had been in violation of International law when it supported the Contras.

After the U.S. Congress prohibited federal funding of the Contras through the Boland Amendment in 1983, the Reagan administration continued to back the Contras by raising money from foreign allies and covertly selling arms to Iran (then engaged in a war with Iraq), and channelling the proceeds to the Contras (see the Iran–Contra affair). When this scheme was revealed, Reagan admitted that he knew about Iranian "arms for hostages" dealings but professed ignorance about the proceeds funding the Contras; for this, National Security Council aide Lt. Col. Oliver North took much of the blame.

Senator John Kerry's 1988 U.S. Senate Committee on Foreign Relations report on links between the Contras and drug imports to the US concluded that "senior U.S. policy makers were not immune to the idea that drug money was a perfect solution to the Contras' funding problems". According to the National Security Archive, Oliver North had been in contact with Manuel Noriega, the US-backed president of Panama. The Reagan administration's support for the Contras continued to stir controversy well into the 1990s. In August 1996, "San Jose Mercury News" reporter Gary Webb published a series titled "Dark Alliance", linking the origins of crack cocaine in California to the CIA-Contra alliance. Webb's allegations were repudiated by reports from the "Los Angeles Times", "The New York Times", and "The Washington Post", and the "San Jose Mercury News" eventually disavowed his work. An investigation by the United States Department of Justice also stated that their "review did not substantiate the main allegations stated and implied in the Mercury News articles". Regarding the specific charges towards the CIA, the DOJ wrote "the implication that the drug trafficking by the individuals discussed in the "Mercury News" articles was connected to the CIA was also not supported by the facts". The CIA also investigated and rejected the allegations.

The Contra war unfolded differently in the northern and southern zones of Nicaragua. Contras based in Costa Rica operated on Nicaragua's Caribbean coast, which is sparsely populated by indigenous groups including the Miskito, Sumo, Rama, Garifuna, and Mestizo. Unlike Spanish-speaking western Nicaragua, the Caribbean Coast is predominantly English-speaking and was largely ignored by the Somoza regime. The "costeños" did not participate in the uprising against Somoza and viewed Sandinismo with suspicion from the outset.

While the Sandinistas encouraged grassroots pluralism, they were perhaps less enthusiastic about national elections. They argued that popular support was expressed in the insurrection and that further appeals to popular support would be a waste of scarce resources. International pressure and domestic opposition eventually pressed the government toward a national election. Tomás Borge warned that the elections were a concession, an act of generosity and of political necessity. On the other hand, the Sandinistas had little to fear from the election given the advantages of incumbency and the restrictions on the opposition, and they hoped to discredit the armed efforts to overthrow them.

A broad range of political parties, ranging in political orientation from far-left to far-right, competed for power. Following promulgation of a new populist constitution, Nicaragua held national elections in 1984. Independent electoral observers from around the world—including groups from the UN as well as observers from Western Europe—found that the elections had been fair. Several groups, however, disputed this, including UNO, a broad coalition of anti-Sandinista activists, COSEP, an organization of business leaders, the Contra group "FDN", organized by former Somozan-era National Guardsmen, landowners, businessmen, peasant highlanders, and what some claimed as their patron, the U.S. government.

Although initially willing to stand in the 1984 elections, the UNO, headed by Arturo Cruz (a former Sandinista), declined participation in the elections based on their own objections to the restrictions placed on the electoral process by the State of Emergency and the official advisement of President Ronald Reagan's State Department, who wanted to de-legitimize the election process. Among other parties that abstained was COSEP, who had warned the FSLN that they would decline participation unless freedom of the press was reinstituted. Coordinadora Democrática (CD) also refused to file candidates and urged Nicaraguans not to take part in the election. The Independent Liberal Party (PLI), headed by Virgilio Godoy Reyes, announced its refusal to participate in October. Consequently, when the elections went ahead the U.S. raised objections based upon political restrictions instituted by the State of Emergency (e.g., censorship of the press, cancellation of habeas corpus, and the curtailing of free assembly).

Daniel Ortega and Sergio Ramírez were elected president and vice-president, and the FSLN won an overwhelming 61 out of 96 seats in the new National Assembly, having taken 67% of the vote on a turnout of 75%. Despite international validation of the elections by multiple political and independent observers (virtually all from among U.S. allies), the United States refused to recognize the elections, with President Ronald Reagan denouncing the elections as a sham. According to a study, since the 1984 election was for posts subordinate to the Sandinista Directorate, the elections were no more subject to approval by vote than the Central Committee of the Communist Party is in countries of the East Bloc. Daniel Ortega began his six-year presidential term on January 10, 1985. After the United States Congress turned down continued funding of the Contras in April 1985, the Reagan administration ordered a total embargo on United States trade with Nicaragua the following month, accusing the Sandinista government of threatening United States security in the region.

The elections of 1990, which had been mandated by the constitution passed in 1987, saw the Bush administration funnel $49.75 million of 'non-lethal' aid to the Contras, as well as $9 million to the opposition UNO—equivalent to $2 billion worth of intervention by a foreign power in a US election at the time, and proportionately five times the amount George Bush had spent on his own election campaign. When Violetta Chamorro visited the White House in November 1989, the US pledged to maintain the embargo against Nicaragua unless Violeta Chamorro won.

In August 1989, the month that campaigning began, the Contras redeployed 8,000 troops into Nicaragua, after a funding boost from Washington, continued their guerrilla war. 50 FSLN candidates were assassinated. The Contras also distributed thousands of UNO leaflets.

Years of conflict had left 50,000 casualties and $12 billion of damages in a society of 3.5 million people and an annual GNP of $2 billion. After the war, a survey was taken of voters: 75.6% agreed that if the Sandinistas had won, the war would never have ended. 91.8% of those who voted for the UNO agreed with this (William I Robinson, op cit). The Library of Congress Country Studies on Nicaragua states:

Despite limited resources and poor organization, the UNO coalition under Violeta Chamorro directed a campaign centered around the failing economy and promises of peace. Many Nicaraguans expected the country's economic crisis to deepen and the Contra conflict to continue if the Sandinistas remained in power. Chamorro promised to end the unpopular military draft, bring about democratic reconciliation, and promote economic growth. In the February 25, 1990, elections, Violeta Barrios de Chamorro carried 55 percent of the popular vote against Daniel Ortega's 41 percent.

In 1987, due to a stalemate with the Contras, the Esquipulas II treaty was brokered by Costa Rican President Óscar Arias Sánchez. The treaty's provisions included a call for a cease-fire, freedom of expression, and national elections. After the February 26, 1990 elections, the Sandinistas lost and peacefully passed power to the National Opposition Union (UNO), an alliance of 14 opposition parties ranging from the conservative business organization COSEP to Nicaraguan communists. UNO's candidate, Violeta Barrios de Chamorro, replaced Daniel Ortega as president of Nicaragua.

Reasons for the Sandinista loss in 1990 are disputed. Defenders of the defeated government assert that Nicaraguans voted for the opposition due to the continuing U.S. economic embargo and potential Contra threat. Others have alleged that the United States threatened to continue to support the Contras and continue the civil war if the regime was not voted out of power.

After their loss, the Sandinista leaders held most of the private property and businesses that had been confiscated and nationalized by the FSLN government. This process became known as the "piñata" and was tolerated by the new Chamorro government. Ortega also claimed to "rule from below" through groups he controls such as labor unions and student groups. Prominent Sandinistas also created nongovernmental organizations to promote their ideas and social goals.

Ortega remained the head of the FSLN, but his brother Humberto resigned from the party and remained at the head of the Sandinista Army, becoming a close confidante and supporter of Chamorro. The party also experienced internal divisions, with prominent Sandinistas such as Ernesto Cardenal and Sergio Ramírez resigning to protest what they described as heavy-handed domination of the party by Daniel Ortega. Ramírez also founded a separate political party, the Sandinista Renovation Movement (MRS); his faction came to be known as the , who favor a more social democratic approach than the "ortodoxos", or hardliners. In the 1996 Nicaraguan election, Ortega and Ramírez both campaigned unsuccessfully as presidential candidates on behalf of their respective parties, with Ortega receiving 43% of the vote while Arnoldo Alemán of the Constitutional Liberal Party received 51%. The Sandinistas won second place in the congressional elections, with 36 of 93 seats.

Ortega was re-elected as leader of the FSLN in 1998. Municipal elections in November 2000 saw a strong Sandinista vote, especially in urban areas, and former Tourism Minister Herty Lewites was elected mayor of Managua. This result led to expectations of a close race in the presidential elections scheduled for November 2001. Daniel Ortega and Enrique Bolaños of the Constitutional Liberal Party (PLC) ran neck-and-neck in the polls for much of the campaign, but in the end the PLC won a clear victory. The results of these elections were that the FSLN won 42.6% of the vote for parliament (versus 52.6% for the PLC), giving them 41 out of the 92 seats in the National Assembly (versus 48 for the PLC). In the presidential race, Ortega lost to Bolaños 46.3% to 53.6%.

Daniel Ortega was once again re-elected as leader of the FSLN in March 2002 and re-elected as president of Nicaragua in November 2006.

In 2006, Daniel Ortega was elected president with 38% of the vote (see 2006 Nicaraguan general election). This occurred despite the fact that the breakaway Sandinista Renovation Movement continued to oppose the FSLN, running former Mayor of Managua Herty Lewites as its candidate for president. However, Lewites died several months before the elections.

The FSLN also won 38 seats in the congressional elections, becoming the party with the largest representation in parliament. The split in the Constitutionalist Liberal Party helped to allow the FSLN to become the largest party in Congress. The Sandinista vote was also split between the FSLN and MRS, but the split was more uneven, with limited support for the MRS. The vote for the two liberal parties combined was larger than the vote for the two Sandinista parties. In 2010, several liberal congressmen raised accusations about the FSLN presumably attempting to buy votes in order to pass constitutional reforms that would allow Ortega to run for office for the 6th time since 1984. In 2011, Ortega was re-elected as President.

Ortega was allowed by Nicaraguan Supreme Court to run again as President, despite having already served two mandates, in a move which was strongly criticized by the opposition. The Supreme Court also banned the leader of the Independent Liberal Party Eduardo Montealegre from running in the election. Ortega was re-elected as President, amid claims of electoral fraud; data about turnout were unclear: while the Supreme Electoral Council claimed a turnout of 66% of voters, the opposition claimed only 30% of voters actually went to the polls.

The year 2018 was marked by particular unrest in Nicaragua that had not been seen in the country in three decades. It came in two different phases, with initial unrest in the context of a fire at the Indio Maíz Biological Reserve in the Río San Juan department (which came to an end when rain abruptly put the fire out), leading on to an outbreak of violence a few weeks later after social security reforms were announced by the government.

During this unrest there were many deaths linked to the violence, as well as many instances of torture, sexual assaults, death threats, intimidation and the ransacking and burning of building and violence against journalists. Opposition figures argued that the government was responsible for the violence, a view supported by some press outlets and NGOs such as Amnesty International. Many opposition figures and independent journalists have been arrested and police raids of opposition forces and independent media have occurred frequently.

On September 29, 2018, President Ortega declared that political protests were "illegal" in Nicaragua, stating that demonstrators would "respond to justice" if they attempted to publicly voice their opinions. The United Nations condemned the actions as being a violation of human rights regarding freedom of assembly.

Carlos Fernando Chamorro, son of former president Violetta Chamorro and editor of "Confidencial", left the country after his office was subject to police search in December 2018.

In December 2018, the government revoked the licenses of five human rights organizations, closed the offices of the cable news and online show "Confidencial", and beat journalists when they protested.

The Confidential newspaper and other media were seized and taken by the government of Daniel Ortega Several service stations of the Puma brand were closed on the afternoon, December 20, by representatives of the Nicaraguan Energy Institute (INE), a state entity that has the mandate to regulate, among others, the hydrocarbons sector.Puma Energy entered the Nicaraguan oil and fuel derivatives market at the end of March 2011, when it bought the entire network of Esso stations in Nicaragua, as part of a regional operation that involved the purchase of 290 service stations and eight storage terminals. of fuel in four countries of Central America.

On December 21, 2018, the Nicaraguan police raided the offices of the 100% News Channel. They arrested Miguel Mora, owner of the Canal; Lucía Pineda, Head of Press of 100% Noticias and Verónica Chávez, wife of Miguel Mora and host of the Ellas Lo Dicen Program. Subsequently, Verónica Chávez was released. Miguel Mora and Lucia Pineda were accused of terrorist crimes and provoking hatred and discrimination between the police and Sandinistas.

On January 30, 2019, the FSLN was expelled from the Socialist International citing "gross violations of human rights and democratic values committed by the government of Nicaragua". The ruling Democratic Revolutionary Party of Panama, also a member of the Socialist International, rejected the expulsion of the FSLN and threatened to leave the International, saying that it has abandoned its principles and made a decision regarding Latin America without consulting the Latin American parties, and referred to a "history of brotherhood in the struggle for social justice in Central America" between the two parties .

Through the media and the works of FSLN leaders such as Carlos Fonseca, the life and times of Augusto César Sandino became its unique symbol in Nicaragua. The ideology of Sandinismo gained momentum in 1974, when a Sandinista-initiated hostage situation resulted in the Somoza government adhering to FSLN demands and publicly printing and airing work on Sandino in well known newspapers and media outlets.

During the struggle against Somoza, the FSLN leaders' internal disagreements over strategy and tactics were reflected in three main factions:

Nevertheless, while ideologies varied between FSLN leaders, all leaders essentially agreed that Sandino provided a path for the Nicaragua masses to take charge, and the FSLN would act as the legitimate vanguard. The extreme end of the ideology links Sandino to Roman Catholicism and portrays him as descending from the mountains in Nicaragua knowing he would be betrayed and killed. Generally however, most Sandinistas associated Sandino on a more practical level, as a heroic and honest person who tried to combat the evil forces of imperialist national and international governments that existed in Nicaragua's history.

For purposes of making sense of how to govern, the FSLN drew four fundamental principles from the work of Carlos Fonseca and his understanding of the lessons of Sandino. According to Bruce E. Wright, "the Governing Junta of National Reconstruction agreed, under Sandinista leadership, that these principles had guided it in putting into practice a form of government that was characterized by those principles." It is generally accepted that these following principles have evolved the "ideology of Sandinismo". Three of these (excluding popular participation, which was presumably contained in Article 2 of the Constitution of Nicaragua) were to ultimately be guaranteed by Article 5 of the Constitution of Nicaragua. They are as follows:

Bruce E. Wright claims that "this was a crucial contribution from Fonseca's work that set the template for FSLN governance during the revolutionary years and beyond".

Beginning in 1967, the Cuban General Intelligence Directorate, or DGI, had begun to establish ties with Nicaraguan revolutionary organizations. By 1970 the DGI had managed to train hundreds of Sandinista guerrilla leaders and had vast influence over the organization. After the successful ousting of Somoza, DGI involvement in the new Sandinista government expanded rapidly. An early indication of the central role that the DGI would play in the Cuban-Nicaraguan relationship is a meeting in Havana on July 27, 1979, at which diplomatic ties between the two countries were re-established after more than 25 years. Julián López Díaz, a prominent DGI agent, was named Ambassador to Nicaragua. Cuban military and DGI advisors, initially brought in during the Sandinista insurgency, would swell to over 2,500 and operated at all levels of the new Nicaraguan government.

The Cubans would like to have helped more in the development of Nicaragua towards socialism. Following the US invasion of Grenada, countries previously looking for support from Cuba saw that the United States was likely to take violent action to discourage this.

The early years of the Nicaraguan revolution had strong ties to Cuba. The Sandinista leaders acknowledged that the FSLN owed a great debt to the socialist island. Once the Sandinistas assumed power, Cuba gave Nicaragua military advice, as well as aid in education, health care, vocational training and industry building for the impoverished Nicaraguan economy. In return, Nicaragua provided Cuba with grains and other foodstuffs to help Cuba overcome the effects of the US embargo.

According to Cambridge University historian Christopher Andrew, who undertook the task of processing the Mitrokhin Archive, Carlos Fonseca Amador, one of the original three founding members of the FSLN had been recruited by the KGB in 1959 while on a trip to Moscow. This was one part of Aleksandr Shelepin's 'grand strategy' of using national liberation movements as a spearhead of the Soviet Union's foreign policy in the Third World, and in 1960 the KGB organized funding and training for twelve individuals that Fonseca handpicked. These individuals were to be the core of the new Sandinista organization. In the following several years, the FSLN tried with little success to organize guerrilla warfare against the government of Luis Somoza Debayle. After several failed attempts to attack government strongholds and little initial support from the local population, the National Guard nearly annihilated the Sandinistas in a series of attacks in 1963. Disappointed with the performance of Shelepin's new Latin American "revolutionary vanguard", the KGB reconstituted its core of the Sandinista leadership into the ISKRA group and used them for other activities in Latin America.

According to Andrew, Mitrokhin says during the following three years the KGB handpicked several dozen Sandinistas for intelligence and sabotage operations in the United States. Andrew and Mitrokhin say that in 1966, this KGB-controlled Sandinista sabotage and intelligence group was sent to northern Mexico near the US border to conduct surveillance for possible sabotage.

In July 1961 during the Berlin Crisis of 1961 KGB chief Alexander Shelepin sent a memorandum to Soviet premier Nikita Khrushchev containing proposals to create a situation in various areas of the world which would favor dispersion of attention and forces by the US and their satellites, and would tie them down during the settlement of the question of a German peace treaty and West Berlin. It was planned, inter alia, to organize an armed mutiny in Nicaragua in coordination with Cuba and with the "Revolutionary Front Sandino". Shelepin proposed to make appropriations from KGB funds in addition to the previous assistance $10,000 for purchase of arms.

Khrushchev sent the memo with his approval to his deputy Frol Kozlov and on August 1 it was, with minor revisions, passed as a CPSU Central Committee directive. The KGB and the Soviet Ministry of Defense were instructed to work out more specific measures and present them for consideration by the Central Committee.

Other researchers have documented the contribution made from other Warsaw Pact intelligence agencies to the fledgling Sandinista government including the East German Stasi, by using recently declassified documents from Berlin as well as from former Stasi spymaster Markus Wolf who described the Stasi's assistance in the creation of a secret police force modeled on East Germany's.

Cuba was instrumental in the Nicaraguan Literacy Campaign. Nicaragua was a country with a very high rate of illiteracy, but the campaign succeeded in lowering the rate from 50% to 12%. The revolution in Cuban education since the ousting of the US-backed Batista regime not only served as a model for Nicaragua but also provided technical assistance and advice. Cuba played an important part in the Campaign, providing teachers on a yearly basis after the revolution. Prevost states that "Teachers were not the only ones studying in Cuba, about 2,000 primary and secondary students were studying on the Isle of Youth and the cost was covered by the host country (Cuba)".

The goals of the 1980 Literacy Campaign were socio-political, strategic as well as educational. It was the most prominent campaign with regards to the new education system. Illiteracy in Nicaragua was significantly reduced from 50.3% to 12.9%. One of the government's major concerns was the previous education system under the Somoza regime which did not see education as a major factor on the development of the country. As mentioned in the Historical Program of the FSLN of 1969, education was seen as a right and the pressure to stay committed to the promises made in the program was even stronger. 1980 was declared the "Year of Literacy" and the major goals of the campaign that started only 8 months after the FSLN took over. This included the eradication of illiteracy and the integration of different classes, races, gender and age. Political awareness and the strengthening of political and economic participation of the Nicaraguan people was also a central goal of the Literacy Campaign. The campaign was a key component of the FSLN's cultural transformation agenda.

The basic reader which was disseminated and used by teacher was called "Dawn of the People" based on the themes of Sandino, Carlos Fonseca, and the Sandinista struggle against imperialism and defending the revolution. Political education was aimed at creating a new social values based on the principles of Sandinista socialism, such as social solidarity, worker's democracy, egalitarianism, and anti-imperialism.

Health care was another area where the Sandinistas made significant improvements and are widely recognized for this accomplishment, e.g. by Oxfam. In this area Cuba also played a role by again offering expertise to Nicaragua. Over 1,500 Cuban doctors worked in Nicaragua and provided more than five million consultations. Cuban personnel were essential in the elimination of polio, the decrease in whooping cough, rubella, measles and the lowering of the infant mortality rate. Gary Prevost states that Cuban personnel made it possible for Nicaragua to have a national health care system that reached the majority of its citizens.

Cuba has participated in the training of Nicaraguan workers in the use of new machinery imported to Nicaragua. The Nicaraguan revolution caused the United States to oppose the country's government; therefore the Sandinistas would not receive any aid from the United States. The United States embargo against Nicaragua, imposed by the Reagan administration in May 1985, made it impossible for Nicaragua to receive spare parts for US-made machines, so this led Nicaragua to look to other countries for help. Cuba was the best choice because of the shared language and proximity and also because it had imported similar machinery over the years. Nicaraguans went to Cuba for short periods of three to six months and this training involved close to 3,000 workers. Countries such as the UK, sent farm equipment to Nicaragua.

Cuba helped Nicaragua in large projects such as building roads, power plants and sugar mills. Cuba also attempted to help Nicaragua build the first overland route linking Nicaraguas Atlantic and Pacific coasts. The road was meant to traverse 420 kilometres (260 mi) of jungle, but completion of the road and usage was hindered by the Contra war, and it was never completed.

Another significant feat was the building of the Tipitapa-Malacatoya sugar mill. It was completed and inaugurated during a visit by Fidel Castro in January 1985. The plant used the newest technology available and was built by workers trained in Cuba. Also during this visit Castro announced that all debts incurred on this project were absolved. Cuba also provided technicians to aid in the sugar harvest and assist in the rejuvenation of several old sugar mills. Cubans also assisted in building schools and similar projects.

After the Nicaraguan revolution, the Sandinista government established a Ministry of Culture in 1980. The ministry was spearheaded by Ernesto Cardenal, a poet and priest. The ministry was established in order to socialize the modes of cultural production. This extended to art forms including dance, music, art, theatre and poetry. The project was created to democratize culture on a national level. The aim of the ministry was to "democratize art" by making it accessible to all social classes as well as protecting the right of the oppressed to produce, distribute and receive art. In particular, the ministry was devoted to the development of working class and "campesino", or peasant culture. Therefore, the ministry sponsored cultural workshops throughout the country until October 1988 when the Ministry of Culture was integrated into the Ministry of Education because of financial troubles.

The objective of the workshops was to recognize and celebrate neglected forms of artistic expression. The ministry created a program of cultural workshops known as, "Casas de Cultura and Centros Populares de Cultura". The workshops were set up in poor neighbourhoods and rural areas and advocated universal access and consumption of art in Nicaragua. The ministry assisted in the creation of theatre groups, folklore and artisanal production, song groups, new journals of creation and cultural criticism, and training programs for cultural workers. The ministry created a Sandinista daily newspaper named "Barricada" and its weekly cultural addition named "Ventana" along with the "Television Sandino, Radio Sandino" and the Nicaraguan film production unit called the INCINE. There were existing papers which splintered after the revolution and produced other independent, pro-Sandinista newspapers, such as "El Nuevo Diario" and its literary addition "Nuevo Amanecer Cultural". Editorial Nueva Nicaragua, a state publishing house for literature, was also created. The ministry collected and published political poetry of the revolutionary period, known as testimonial narrative, a form of literary genre that recorded the experiences of individuals in the course of the revolution.

The ministry developed a new anthology of Rubén Darío, a Nicaraguan poet and writer, established a Rubén Darío prize for Latin American writers, the Leonel Rugama prize for young Nicaraguan writers, as well as public poetry readings and contests, cultural festivals and concerts. The Sandinista regime tried to keep the revolutionary spirit alive by empowering its citizens artistically. At the time of its inception, the Ministry of Culture needed, according to Cardenal, "to bring a culture to the people who were marginalized from it. We want a culture that is not the culture of an elite, of a group that is considered 'cultivated', but rather of an entire people." Nevertheless, the success of the Ministry of Culture had mixed results and by 1985 criticism arose over artistic freedom in the poetry workshops. The poetry workshops became a matter for criticism and debate. Critics argued that the ministry imposed too many principles and guidelines for young writers in the workshop, such as, asking them to avoid metaphors in their poetry and advising them to write about events in their everyday life. Critical voices came from established poets and writers represented by the "Asociacion Sandinista de Trabajadores de la Cultura" (ASTC) and from the "Ventana" both of which were headed by Rosario Murillo. They argued that young writers should be exposed to different poetic styles of writing and resources developed in Nicaragua and elsewhere. Furthermore, they argued that the ministry exhibited a tendency that favored and fostered political and testimonial literature in post-revolutionary Nicaragua.

The new government, formed in 1979 and dominated by the Sandinistas, resulted in a socialist model of economic development. The new leadership was conscious of the social inequities produced during the previous thirty years of unrestricted economic growth and was determined to make the country's workers and peasants, the "economically underprivileged", the prime beneficiaries of the new society. Consequently, in 1980 and 1981, unbridled incentives to private investment gave way to institutions designed to redistribute wealth and income. Private property would continue to be allowed, but all land belonging to the Somozas was confiscated.

However, the ideology of the Sandinistas put the future of the private sector and of private ownership of the means of production in doubt. Although under the new government both public and private ownership were accepted, government spokespersons occasionally referred to a reconstruction phase in the country's development, in which property owners and the professional class would be tapped for their managerial and technical expertise. After reconstruction and recovery, the private sector would give way to expanded public ownership in most areas of the economy. Despite such ideas, which represented the point of view of a faction of the government, the Sandinista government remained officially committed to a mixed economy.

Economic growth was uneven in the 1980s. Restructuring of the economy and the rebuilding immediately following the end of the civil war caused the GDP to rise about 5 percent in 1980 and 1981. Each year from 1984 to 1990, however, showed a drop in the GDP. Reasons for the contraction included the reluctance of foreign banks to offer new loans, the diversion of funds to fight the new insurrection against the government, and, after 1985, the total embargo on trade with the United States, formerly Nicaragua's largest trading partner. After 1985 the government chose to fill the gap between decreasing revenues and mushrooming military expenditures by printing large amounts of paper money. Inflation rose rapidly, peaking in 1988 at more than 14,000 percent annually.

Measures taken by the government to lower inflation were largely defeated by natural disaster. In early 1988, the administration of Daniel José Ortega Saavedra (Sandinista junta coordinator 1979–85, president 1985–90) established an austerity program to lower inflation. Price controls were tightened, and a new currency was introduced. As a result, by August 1988, inflation had dropped to an annual rate of 240 percent. The following month, however, Hurricane Joan cut a path directly across the center of the country. Damage was extensive, and the government's program of large spending to repair the infrastructure destroyed its anti-inflation measures.

In its eleven years in power, the Sandinista government never overcame most of the economic inequalities that it inherited from the Somoza era. Years of war, policy missteps, natural disasters, and the effects of the United States trade embargo all hindered economic development.

The women of Nicaragua prior to, during and after the revolution played a prominent role within the nation's society as they have commonly been recognized, throughout history and across all Latin American states, as its backbone. Nicaraguan women were therefore directly affected by all of the positive and negative events that took place during this revolutionary period. The victory of the Sandinista National Liberation Front (FSLN) in 1979 brought about major changes and gains for women, mainly in legislation, broad educational opportunities, training programs for working women, childcare programs to help women enter the work force and greatly increased participation and leadership positions in a range of political activities. This, in turn, reduced the burdens that the women of Nicaragua were faced with prior to the revolution. During the Sandinista government, women were more active politically. The large majority of members of the neighborhood committees (Comités de Defensa Sandinista) were women. By 1987, 31% of the executive positions in the Sandinista government, 27% of the leadership positions of the FSLN, and 25% of the FSLN's active membership were women.

Supporters of the Sandinistas see their era as characterized by the creation and implementation of successful social programs which were free and made widely available to the entire nation. Some of the more successful programs for women that were implemented by the Sandinistas were in the areas of education (see; Nicaraguan Literacy Campaign), health, and housing. Providing subsidies for basic foodstuffs and the introduction of mass employment were also contributions of the FSLN. The Sandinistas were particularly advantageous for the women of Nicaraguan as they promoted progressive views on gender as early as 1969 claiming that the revolution would "abolish the detestable discrimination that women have suffered with regard to men and establish economic, political and cultural equality between men and women". This was evident as the FSLN began integrating women into their ranks by 1967, unlike other left-wing guerilla groups in the region. This goal was not fully reached because the roots of gender inequality were not explicitly challenged. Women's participation within the public sphere was also substantial, as many took part in the armed struggle as part of the FSLN or as part of counter-revolutionary forces.

Nicaraguan women organized independently in support of the revolution and their cause. Some of those organizations were the Socialist Party (1963), Federación Democrática (which support the FSLN in rural areas), and Luisa Amanda Espinoza Association of Nicaraguan Women (, AMNLAE). However, since Daniel Ortega, was defeated in the 1990 election by the United Nicaraguan Opposition (UNO) coalition headed by Violeta Chamorro, the situation for women in Nicaragua was seriously altered. In terms of women and the labor market, by the end of 1991 AMNLAE reported that almost 16,000 working women—9,000 agricultural laborers, 3,000 industrial workers, and 3,800 civil servants, including 2,000 in health, 800 in education, and 1,000 in administration—had lost their jobs. The change in government also resulted in the drastic reduction or suspension of all Nicaraguan social programs, which brought back the burdens characteristic of pre-revolutionary Nicaragua. The women were forced to maintain and supplement community social services on their own without economic aid or technical and human resource.

Between 2007 and 2018 under Sandinista administrations, Nicaragua has advanced from 62nd to 6th in the world in terms of gender equality, according to the Global Gender Gap Report from the World Economic Forum.

The Roman Catholic Church's relationship with the Sandinistas was extremely complex. Initially, the Church was committed to supporting the Somoza regime. The Somoza dynasty was willing to secure the Church a prominent place in society as long as it did not attempt to subvert the authority of the regime. Under the constitution of 1950 the Roman Catholic Church was recognized as the official religion and church-run schools flourished. It was not until the late 1970s that the Church began to speak out against the corruption and human rights abuses that characterized the Somoza regime.

The Catholic hierarchy initially disapproved of the Sandinistas' revolutionary struggle against the Somoza dynasty. The revolutionaries were perceived as proponents of "godless communism" that posed a threat to the traditionally privileged place that the Church occupied within Nicaraguan society. Nevertheless, the increasing corruption and repression characterizing the Somoza rule and the likelihood that the Sandinistas would emerge victorious ultimately influenced Archbishop Miguel Obando y Bravo to declare formal support for the Sandinistas' armed struggle. Throughout the revolutionary struggle, the Sandinistas had the grassroots support of clergy who were influenced by the reforming zeal of Vatican II and dedicated to a "preferential option for the poor" (for comparison, see liberation theology). Numerous Christian base communities (CEBs) were created in which lower level clergy and laity took part in consciousness raising initiatives to educate the peasants about the institutionalized violence they were suffering from. Some priests took a more active role in supporting the revolutionary struggle. For example, Father Gaspar García Laviana took up arms and became a member of FSLN.

Soon after the Sandinistas assumed power, the hierarchy began to oppose the Sandinistas government. The Archbishop was a vocal source of domestic opposition. The hierarchy was alleged to be motivated by fear of the emergence of the 'popular church' which challenged their centralized authority. The hierarchy also opposed social reforms implemented by the Sandinistas to aid the poor, allegedly because they saw it as a threat to their traditionally privileged position within society. In response to this perceived opposition, the Sandinistas shut down the church-run Radio Católica radio station on multiple occasions.

The Sandinistas' relationship with the Roman Catholic Church deteriorated as the Contra War continued. The hierarchy refused to speak out against the counterrevolutionary activities of the contras and failed to denounce American military aid. State media accused the Catholic Church of being reactionary and supporting the Contras. According to former President Ortega, "The conflict with the church was strong, and it costs us, but I don't think it was our fault. ... There were so many people being wounded every day, so many people dying, and it was hard for us to understand the position of the church hierarchy in refusing to condemn the contras." The hierarchy-state tensions were brought to the fore with Pope John Paul II 1983 visit to Nicaragua. Hostility to the Catholic Church became so great that at one point, FSLN militants shouted down Pope John Paul II as he tried to say Mass. Therefore, while the activities of the Catholic church contributed to the success of the Sandinista revolution, the hierarchy's opposition was a major factor in the downfall of the revolutionary government.

"Time" magazine in 1983 published reports of human rights violations in an article which stated that "According to Nicaragua's Permanent Commission on Human Rights, the regime detains several hundred people a month; about half of them are eventually released, but the rest simply disappear." "Time" also interviewed a former deputy chief of Nicaraguan military counterintelligence, who stated that he had fled Nicaragua after being ordered to kill 800 Miskito prisoners and make it look like they had died in combat. Another article described Sandinista neighbourhood "Defense Committees", modeled on similar Cuban Committees for the Defense of the Revolution, which according to critics were used to unleash mobs on anyone who was labeled a counterrevolutionary. Nicaragua's only opposition newspaper, La Prensa, was subject to strict censorship. The newspaper's editors were forbidden to print anything negative about the Sandinistas either at home or abroad.

Nicaragua's Permanent Commission on Human Rights reported 2,000 murders in the first six months and 3,000 disappearances in the first few years. It has since documented 14,000 cases of torture, rape, kidnapping, mutilation and murder.

The Inter-American Commission on Human Rights (IACHR) in a 1981 report found evidence for mass executions in the period following the revolution. It stated: "In the Commission's view, while the government of Nicaragua clearly intended to respect the lives of all those defeated in the civil war, during the weeks immediately subsequent to the Revolutionary triumph, when the government was not in effective control, illegal executions took place which violated the right to life, and these acts have not been investigated and the persons responsible have not been punished." The IACHR also stated that: "The Commission is of the view that the new regime did not have, and does not now have, a policy of violating the right to life of political enemies, including among the latter the former guardsmen of the Government of General Somoza, whom a large sector of the population of Nicaragua held responsible for serious human rights violations during the former regime; proof of the foregoing is the abolition of the death penalty and the high number of former guardsmen who were prisoners and brought to trial for crimes that constituted violations of human rights."

A 1983 IACHR report documented allegations of human rights violations against the Miskito Indians, which were alleged to have taken place after opposition forces (the Contras) infiltrated a Miskito village in order to launch attacks against government soldiers, and as part of a subsequent forced relocation program. Allegations included arbitrary imprisonment without trial, "disappearances" of such prisoners, forced relocation, and destruction of property. In late 1981, the CIA conspiracy "Operation Red Christmas" was exposed to separate the Atlantic region from the rest of Nicaragua. Red Christmas aimed to seize territory on Nicaragua's mainland and overthrow the Nicaraguan government. The Nicaraguan government responded to the provocations by transferring 8,500 Miskitos 80 kilometres (50 mi) south to a settlement called Tasba Pri. The U.S. government accused Nicaragua of genocide. The U.S. government produced a photo alleged to show Miskito bodies being burned by Sandinista troops; however, the photo was actually of people killed by Somoza's National Guard in 1978.

The IACHR's 1991 annual report states: "In September 1990, the Commission was informed of the discovery of common graves in Nicaragua, especially in areas where fighting had occurred. The information was provided by the Nicaraguan Pro Human Rights Association, which had received its first complaint in June 1990. By December 1991, that Association had received reports of 60 common graves and had investigated 15 of them. While most of the graves seem to be the result of summary executions by members of the Sandinista People's Army or the State Security, some contain the bodies of individuals executed by the Nicaraguan Resistance."

The IACHR's 1992 annual report contains details of mass graves and investigations which suggest that mass executions had been carried out. One such grave contained 75 corpses of peasants who were believed to have been executed in 1984 by government security forces pretending to be members of the Contras. Another grave was also found in the town of Quininowas which contained six corpses, believed to be an entire family killed by government forces when the town was invaded. A further 72 graves were reported as being found, containing bodies of people, the majority of whom were believed to have been executed by agents of the state and some also by the Contras.

The issue of human rights also became highly politicized at this time as human rights is claimed to be a key component of propaganda created by the Reagan administration to help legitimize its policies in the region. The Inter-Church Committee on Human Rights in Latin America (ICCHRLA) in its "Newsletter" stated in 1985 that: "The hostility with which the Nicaraguan government is viewed by the Reagan administration is an unfortunate development. Even more unfortunate is the expression of that hostility in the destabilization campaign developed by the US administration. ... An important aspect of this campaign is misinformation and frequent allegations of serious human rights violations by the Nicaraguan authorities." Among the accusations in The Heritage Foundation report and the Demokratizatsiya article are references to alleged policies of religious persecution, particularly anti-semitism. The ICCHRLA in its newsletter stated that: "From time to time the current U.S. administration, and private organizations sympathetic to it, have made serious and extensive allegations of religious persecution in Nicaragua. Colleague churches in the United States undertook onsite investigation of these charges in 1984. In their report, the delegation organized by the Division of Overseas Ministries of the National Council of Churches of Christ in the United States concluded that there is 'no basis for the charge of systematic religious persecution'. The delegation 'considers this issue to be a device being used to justify aggressive opposition to the present Nicaraguan government.'" On the other hand, some elements of the Catholic Church in Nicaragua, among them Archbishop Miguel Obando y Bravo, strongly criticized the Sandinistas. The Archbishop stated "The government wants a church that is aligned with the Marxist–Leninist regime." The Inter-American Commission on Human Rights states that: "Although it is true that much of the friction between the Government and the churches arises from positions that are directly or indirectly linked to the political situation of the country, it is also true that statements by high government officials, official press statements, and the actions of groups under the control of the Government have gone beyond the limits within which political discussions should take place and have become obstacles to certain specifically religious activities."

Human Rights Watch also stated in its 1989 report on Nicaragua that: "Under the Reagan administration, U.S. policy toward Nicaragua's Sandinista government was marked by constant hostility. This hostility yielded, among other things, an inordinate amount of publicity about human rights issues. Almost invariably, U.S. pronouncements on human rights exaggerated and distorted the real human rights violations of the Sandinista regime, and exculpated those of the U.S.-supported insurgents, known as the "contras"."

In 1987, a report was published by the UK based NGO Catholic Institute for International Relations (CIIR, now known as "Progressio"), a human rights organization which identifies itself with Liberation theology. The report, "Right to Survive: Human Rights in Nicaragua", discussed the politicization of the human rights issue: "The Reagan administration, with scant regard for the truth, has made a concerted effort to paint as evil a picture as possible of Nicaragua, describing it as a 'totalitarian dungeon'. Supporters of the Sandinistas ... have argued that Nicaragua has a good record of human rights compared with other Central American countries and have compared Nicaragua with other countries at war." The CIIR report refers to estimates made by the NGO Americas Watch which count the number of non-battle related deaths and disappearances for which the government was responsible up to the year 1986 as "close to 300".

According to the CIIR report, Amnesty International and Americas Watch stated that there is no evidence that the use of torture was sanctioned by the Nicaraguan authorities, although prisoners reported the use of conditions of detention and interrogation techniques that could be described as psychological torture. The Red Cross made repeated requests to be given access to prisoners held in state security detention centers, but were refused. The CIIR was critical of the Permanent Commission on Human Rights (PCHR or CPDH in Spanish), claiming that the organisation had a tendency to immediately publish accusations against the government without first establishing a factual basis for the allegations. The CIIR report also questioned the independence of the Permanent Commission on Human Rights, referring to an article in "The Washington Post" which claims that the National Endowment for Democracy, an organization funded by the US government, allocated a concession of US$50,000 for assistance in the translation and distribution outside Nicaragua of its monthly report, and that these funds were administered by the Committee for Democracy in Central America (Prodemca), a US-based organization which later published full-page advertisements in "The Washington Post" and "The New York Times" supporting military aid to the Contras. The Permanent Commission denies that it received any money which it claims was instead used by others for translating and distributing their monthly reports in other nations.

The Nicaraguan-based magazine "Revista Envio", which describes its stance as one of "critical support for the Sandinistas", refers to the report: "The CPDH: Can It Be Trusted?" written by Scottish lawyer Paul Laverty. In the report, Laverty observes that: "The entire board of directors [of the Permanent Commission], are members of or closely identify with the 'Nicaraguan Democratic Coordinating Committee' (Coordinadora), an alliance of the more right wing parties and COSEP, the business organization." He goes on to express concern about CPDH's alleged tendency to provide relatively few names and other details in connection with alleged violations. "According to the 11 monthly bulletins of 1987 (July being the only month without an issue), the CPDH claims to have received information on 1,236 abuses of all types. However, of those cases, only 144 names are provided. The majority of those 144 cases give dates and places of alleged incidents, but not all. This means that only in 11.65% of its cases is there the minimal detail provided to identify the person, place, date, incident and perpetrator of the abuse."

On the other hand, the Inter-American Commission on Human Rights states: "During its on-site observation in 1978 under the Government of General Somoza, the Permanent Commission on Human Rights in Nicaragua, (CPDH) gave the Commission notable assistance, which certainly helped it to prepare its report promptly and correctly." and in 1980 "It cannot be denied that the CPDH continues to play an important role in the protection of human rights, and that a good number of people who consider that their human rights have been ignored by the Government are constantly coming to it." The IACHR continued to meet with representatives of the Permanent Commission and report their assessments in later years.

The Heritage Foundation stated that: "While elements of the Somoza National Guard tortured political opponents, they did not employ psychological torture." The International Commission of Jurists stated that under the Somoza regime cruel physical torture was regularly used in the interrogation of political prisoners.

Throughout the 1980s the Sandinista government was regarded as "Partly Free" by Freedom House.

The United States State Department accused the Sandinistas of many cases of illegal foreign intervention.

The first allegation was supporting the FMLN rebels in El Salvador with safe haven, training, command-and-control headquarters, advice, weapons, ammunition, and other vital supplies. Captured documents, testimonials of former rebels and Sandinistas, aerial photographs, the tracing of captured weapons back to Nicaragua, and captured vehicles from Nicaragua smuggling weapons were cited as evidence. El Salvador was in a civil war in the period in question and the US was heavily supporting the Salvadoran government against the FMLN guerrillas.

There were also accusations of subversive activities in Honduras, Costa Rica, and Colombia, and in the case of Honduras and Costa Rica outright military operations by Nicaraguan troops.

The flag of the FSLN consists of an upper half in red, a lower half in black, and the letters F S L N in white. It is a modified version of the flag Sandino used in the 1930s, during the war against the U.S. occupation of Nicaragua which consisted of two vertical stripes, equally in size, one red and the other black with a skull (like the traditional Jolly Roger flag). These colors came from the Mexican anarchist movements that Sandino was involved with during his stay in Mexico in the early 1920s. (The traditional flag of anarcho-syndicalism, which joins diagonally the red color of the labour movement and the black color of anarchism, as in the flag of the CNT, is a negation of nationalism and reaffirmation of internationalism.)

In recent times, there has been a dispute between the FSLN and the dissident Sandinista Renovation Movement (MRS) about the use of the red and black flag in public activities. Although the MRS has its own flag (orange with a silhouette of Sandino's hat in black), they also use the red-and-black flag in honor of Sandino's legacy. They state that the red-and-black flag is a symbol of Sandinismo as a whole, not only of the FSLN party.





I grew up in Urbana three houses down from the Sanderson family – Milton and Virginia and their boys Steve and Joe. My close friend was Joe. His bedroom was filled with aquariums, terrariums, snakes, hamsters, spiders, and butterfly and beetle collections. I envied him like crazy. After college he hit the road. He never made a break from his parents, but they rarely knew where he was. Sometimes he came home and his mother would have to sew $100 bills into the seams of his blue jeans. He disappeared in Nicaragua. His body was later identified as a dead Sandinista freedom fighter. From a nice little house surrounded by evergreens at the other end of Washington Street, he left to look for something he needed to find. I believe in Sean Penn's Christopher McCandless. I grew up with him.



The party has given the following Presidents of the Republic, namely:

Won, getting the 67.20% of the valid votes cast 735.067 votes equivalent to well above the second party of the Democratic Conservative Party (PCD ) who won 154,127 corresponding to 14.00% of the valid votes.
Lost, as 579,886 A total valid votes equivalent to 40.82%, below that obtained by the main opposition Mrs. Violeta Barrios de Chamorro candidate of the National Opposition Union (UNO) who won 777,552 to obtain valid votes equivalent to 54.74%.
Lost, as 669,443 A total valid votes equivalent to 37.75%, below that obtained by his main opponent on Arnoldo Alemán Lacayo candidate of the Liberal Alliance (AL) who won 904,908 to obtain valid votes equivalent to 51.03%.
Lost, as 915,417 A total valid votes equivalent to 42.30%, below that obtained by the main opposition Enrique Bolaños Geyer candidate Liberal Constitutionalist Party (PLC) who won by getting 1,216,863 valid votes equivalent to 56.30%.
Won, getting the 37.99% of the valid votes cast, 930,802 votes equivalent to relatively higher than the two main opposition parties. They were the party of the Second Nicaraguan Liberal Alliance (ALN) with the degree candidate Eduardo Montealegre Rivas who won 693,391 votes recorded corresponding to a 28.30% and third place went to the Constitutionalist Liberal Party with Dr. José Rizo Castellón who earned a total 664,225 of valid votes corresponding to 27.11%.
Won in National Elections held on November 6, 2011, was the amount of 1,569,287 for 62.46% of the total valid votes, at that moment Commander Daniel Ortega Saavedra became the presidential candidate who won a presidential election with the most votes in the history of Nicaragua, in addition to that obtained a lead of more than 30% of valid votes doubling the number of votes obtained by radial businessman Fabio Gadea Mantilla on behalf of the Independent Liberal Party (PLI) who obtained the amount of 778,889 votes recorded for 31.00%. The big loser of these elections was the former President Arnoldo Alemán Lacayo candidate Liberal Constitutionalist Party (PLC) who was located in a distant third with a 5.91% equivalent to 148,507 votes. These results hint at a continuing and still fluid change in the correlation of political forces in the country.
Won in National Elections held on November 6, 2016, was the amount of 1,806,651 for 72.44% of the total valid votes. In the 2016 Presidential Elections, Commander Daniel Ortega Saavedra accompanied by Rosario Murillo Zambrana became the Presidential Formula that obtained the most votes in a Presidential Election in the history of Nicaragua, obtaining an advantage of more than 57% on the formula of Secondly, demonstrating that the application of the Christian, Socialist and Solidarity Model of the Government of Reconciliation and National Unity implemented by the Sandinista National Liberation Front has the support of the immense majority of Nicaraguans. 





</doc>
<doc id="29318" url="https://en.wikipedia.org/wiki?curid=29318" title="Streptococcus">
Streptococcus

Streptococcus is a genus of gram-positive ' (plural ) or spherical bacteria that belongs to the family Streptococcaceae, within the order Lactobacillales (lactic acid bacteria), in the phylum Firmicutes. Cell division in streptococci occurs along a single axis, so as they grow, they tend to form pairs or chains that may appear bent or twisted. (Contrast with that of staphylococci, which divide along multiple axes, thereby generating irregular, grape-like clusters of cells.)

The term was coined in 1877 by Viennese surgeon Albert Theodor Billroth (1829–1894), by combining the prefix "strepto-" (from ), together with the suffix "-coccus" (from Modern , from .)

Most streptococci are oxidase-negative and catalase-negative, and many are facultative anaerobes (capable of growth both aerobically and anaerobically).

In 1984, many bacteria formerly grouped in the genus "Streptococcus" were separated out into the genera "Enterococcus" and "Lactococcus". Currently, over 50 species are recognised in this genus. This genus has been found to be part of the salivary microbiome.

In addition to streptococcal pharyngitis (strep throat), certain "Streptococcus" species are responsible for many cases of pink eye, meningitis, bacterial pneumonia, endocarditis, erysipelas, and necrotizing fasciitis (the 'flesh-eating' bacterial infections). However, many streptococcal species are not pathogenic, and form part of the commensal human microbiota of the mouth, skin, intestine, and upper respiratory tract. Streptococci are also a necessary ingredient in producing Emmentaler ("Swiss") cheese.

Species of "Streptococcus" are classified based on their hemolytic properties. Alpha-hemolytic species cause oxidization of iron in hemoglobin molecules within red blood cells, giving it a greenish color on blood agar. Beta-hemolytic species cause complete rupture of red blood cells. On blood agar, this appears as wide areas clear of blood cells surrounding bacterial colonies. Gamma-hemolytic species cause no hemolysis.

Beta-hemolytic streptococci are further classified by Lancefield grouping, a serotype classification (that is, describing specific carbohydrates present on the bacterial cell wall). The 20 described serotypes are named Lancefield groups A to V (excluding I and J). This system of classification was developed by Rebecca Lancefield, a scientist at Rockefeller University.

In the medical setting, the most important groups are the alpha-hemolytic streptococci "S. pneumoniae" and "Streptococcus" "viridans "group, and the beta-hemolytic streptococci of Lancefield groups A and B (also known as “group A strep” and “group B strep”).

Table: Medically relevant streptococci (not all are alpha-hemolytic)
When alpha-hemolysis (α-hemolysis) is present, the agar under the colony will appear dark and greenish due to the conversion of hemoglobin to green biliverdin. "Streptococcus pneumoniae" and a group of oral streptococci ("Streptococcus viridans" or viridans streptococci) display alpha-hemolysis. 
Alpha-hemolysis is also termed incomplete hemolysis or partial hemolysis because the cell membranes of the red blood cells are left intact. This is also sometimes called green hemolysis because of the color change in the agar.



Beta hemolysis (β-hemolysis), sometimes called complete hemolysis, is a complete lysis of red cells in the media around and under the colonies: the area appears lightened (yellow) and transparent. Streptolysin, an exotoxin, is the enzyme produced by the bacteria which causes the complete lysis of red blood cells. There are two types of streptolysin: Streptolysin O (SLO) and streptolysin S (SLS). Streptolysin O is an oxygen-sensitive cytotoxin, secreted by most group A "Streptococcus" (GAS), and interacts with cholesterol in the membrane of eukaryotic cells (mainly red and white blood cells, macrophages, and platelets), and usually results in beta-hemolysis under the surface of blood agar. Streptolysin S is an oxygen-stable cytotoxin also produced by most GAS strains which results in clearing on the surface of blood agar. SLS affects immune cells, including polymorphonuclear leukocytes and lymphocytes, and is thought to prevent the host immune system from clearing infection. "Streptococcus pyogenes", or GAS, displays beta hemolysis.

Some weakly beta-hemolytic species cause intense hemolysis when grown together with a strain of "Staphylococcus". This is called the CAMP test. "Streptococcus agalactiae" displays this property. "Clostridium perfringens" can be identified presumptively with this test. "Listeria monocytogenes" is also positive on sheep's blood agar.
Group A "S. pyogenes" is the causative agent in a wide range of group A streptococcal infections (GAS). These infections may be noninvasive or invasive. The noninvasive infections tend to be more common and less severe. The most common of these infections include streptococcal pharyngitis (strep throat) and impetigo. Scarlet fever is also a noninvasive infection, but has not been as common in recent years. 
The invasive infections caused by group A beta-hemolytic streptococci tend to be more severe and less common. This occurs when the bacterium is able to infect areas where it is not usually found, such as the blood and the organs. The diseases that may be caused include streptococcal toxic shock syndrome, necrotizing fasciitis, pneumonia, and bacteremia. Globally, GAS has been estimated to cause more than 500,000 deaths every year, making it one of the world's leading pathogens.

Additional complications may be caused by GAS, namely acute rheumatic fever and acute glomerulonephritis. Rheumatic fever, a disease that affects the joints, kidneys, and heart valves, is a consequence of untreated strep A infection caused not by the bacterium itself. Rheumatic fever is caused by the antibodies created by the immune system to fight off the infection cross-reacting with other proteins in the body. This "cross-reaction" causes the body to essentially attack itself and leads to the damage above. A similar autoimmune mechanism initiated by Group A beta-hemolytic streptococcal (GABHS) infection is hypothesized to cause pediatric autoimmune neuropsychiatric disorders associated with streptococcal infections (PANDAS), wherein autoimmune antibodies affect the basal ganglia, causing rapid onset of psychiatric, motor, sleep, and other symptoms in pediatric patients.

GAS infection is generally diagnosed with a rapid strep test or by culture.

"S. agalactiae", or group B "Streptococcus", GBS, causes pneumonia and meningitis in newborns and the elderly, with occasional systemic bacteremia. Importantly, "Streptococcus agalactiae" is the most common cause of meningitis in infants from one month to three months old. They can also colonize the intestines and the female reproductive tract, increasing the risk for premature rupture of membranes during pregnancy, and transmission of the organism to the infant. The American Congress of Obstetricians and Gynecologists (formerly the American College of Obstetricians and Gynecologists), American Academy of Pediatrics, and the Centers for Disease Control recommend all pregnant women between 35 and 37 weeks gestation to be tested for GBS. Women who test positive should be given prophylactic antibiotics during labor, which will usually prevent transmission to the infant.

The United Kingdom has chosen to adopt a risk factor-based protocol, rather than the culture-based protocol followed in the US. Current guidelines state that if one or more of the following risk factors is present, then the woman should be treated with "intrapartum" antibiotics:
This protocol results in treatment of 15–20% of pregnant women and prevention of 65–70% of cases of early onset GBS sepsis.

This group includes "S. equi", which causes strangles in horses, and "S. zooepidemicus"—"S. equi" is a clonal descendant or biovar of the ancestral "S. zooepidemicus"—which causes infections in several species of mammals, including cattle and horses. "S. dysgalactiae" is also a member of group C, beta-haemolytic streptococci that can cause pharyngitis and other pyogenic infections similar to group A streptococci.

Many former group D streptococci have been reclassified and placed in the genus "Enterococcus" (including "E. faecalis", "E. faecium", "E. durans", and "E. avium"). For example, "Streptococcus faecalis" is now "Enterococcus faecalis". "E. faecalis" is sometimes alpha-hemolytic and "E. faecium" is sometimes beta hemolytic.

The remaining nonenterococcal group D strains include "Streptococcus bovis" and "Streptococcus equinus".

Nonhemolytic streptococci rarely cause illness. However, weakly hemolytic group D beta-hemolytic streptococci and "Listeria monocytogenes" (which is actually a gram-positive bacillus) should not be confused with nonhemolytic streptococci.

Group F streptococci were first described in 1934 by Long and Bliss amongst the "minute haemolytic streptococci". They are also known as "Streptococcus anginosus" (according to the Lancefield classification system) or as members of the "S. milleri" group (according to the European system).

These streptococci are usually, but not exclusively, beta-hemolytic. "Streptococcus dysgalactiae" is the predominant species encountered, particularly in human disease. "S. canis" is an example of a GGS which is typically found on animals, but can cause infection in humans. "S. phocae" is a GGS subspecies that has been found in marine mammals and marine fish species. In marine mammals it has been mainly associated with meningoencephalitis, sepsis, and endocarditis, but is also associated with many other pathologies. Its environmental reservoir and means of transmission in marine mammals is not well characterized.

Group H streptococci cause infections in medium-sized canines. Group H streptococci rarely cause illness unless a human has direct contact with the mouth of a canine. One of the most common ways this can be spread is human-to-canine, mouth-to-mouth contact. However, the canine may lick the human's hand and infection can be spread, as well.

Streptococci have been divided into six groups on the basis of their 16S rDNA sequences: "S. anginosus, S.bovis, S. mitis, S. mutans, S. pyogenes" and "S. salivarius". The 16S groups have been confirmed by whole genome sequencing (see figure). The important pathogens "S. pneumoniae" and "S. pyogenes" belong to the "S. mitis" and "S. pyogenes" groups, respectively, while the causative agent of dental caries, "Streptococcus mutans", is basal to the "Streptococcus" group.

The genomes of hundreds of species have been sequenced. Most "Streptococcus" genomes are 1.8 to 2.3 Mb in size and encode 1,700 to 2,300 proteins. Some important genomes are listed in the table. The four species shown in the table ("S. pyogenes, S. agalactiae, S. pneumoniae", and "S. mutans") have an average pairwise protein sequence identity of about 70%.

Bacteriophages have been described for many species of "Streptococcus". 18 prophages have been described in "S. pneumoniae" that range in size from 38 to 41 kb in size, encoding from 42 to 66 genes each. Some of the first "Streptococcus" phages discovered were Dp-1 and ω1. In 1981 the Cp (Complutense phage) family was discovered with Cp-1 as its first member. Dp-1 and Cp-1 infect both "S. pneumoniae" and "S. mitis". However, the host ranges of most "Streptococcus" phages have not been investigated systematically.

Natural genetic transformation involves the transfer of DNA from one bacterium to another through the surrounding medium. Transformation is a complex process dependent on expression of numerous genes. To be capable of transformation a bacterium must enter a special physiologic state referred to as competence. "S. pneumoniae", "S. mitis" and "S. oralis" can become competent, and as a result actively acquire homologous DNA for transformation by a predatory fratricidal mechanism This fratricidal mechanism mainly exploits non-competent siblings present in the same niche Among highly competent isolates of "S. pneumoniae", Li et al. showed that nasal colonization fitness and virulence (lung infectivity) depend on an intact competence system. Competence may allow the streptococcal pathogen to use external homologous DNA for recombinational repair of DNA damages caused by the hosts oxidative attack.




</doc>
<doc id="29322" url="https://en.wikipedia.org/wiki?curid=29322" title="SignWriting">
SignWriting

Sutton SignWriting, or simply, SignWriting, is a system of writing sign languages. It is highly featural and visually iconic, both in the shapes of the characters, which are abstract pictures of the hands, face, and body, and in their spatial arrangement on the page, which does not follow a sequential order like the letters that make up written English words. It was developed in 1974 by Valerie Sutton, a dancer who had, two years earlier, developed DanceWriting. Some newer standardized forms are known as the International Sign Writing Alphabet (ISWA).

As Sutton was teaching DanceWriting to the Royal Danish Ballet, Lars von der Lieth, who was doing research on sign language at the University of Copenhagen, thought it would be useful to use a similar notation for the recording of sign languages. Sutton based SignWriting on DanceWriting, and finally expanded the system to the complete repertoire of MovementWriting. However, only SignWriting and DanceWriting have been widely used.

Although not the first writing system for sign languages (see Stokoe notation), SignWriting is the first to adequately represent facial expressions and shifts in posture, and to accommodate representation of series of signs longer than compound words and short phrases. It is the only system in regular use, used for example to publish college newsletters in American Sign Language, and has been used for captioning of YouTube videos. Sutton notes that SignWriting has been used or investigated in over 40 countries on every inhabited continent. However, it is not clear how widespread its use is in each country.
In Brazil, during the FENEIS (National Association of the Deaf) annual meeting in 2001, the association voted to accept SignWriting as the preferred method of transcribing Lingua Brasileira de Sinais (Libras) into a written form. The strong recommendation to the Brazilian government from that association was that SignWriting be taught in all Deaf schools. Currently SignWriting is taught on an academic level at the Federal University of Santa Catarina as part of its Brazilian Sign Language curriculum. SignWriting is also being used in the recently published Brazilian Sign Language Dictionary containing more than 3,600 signs used by the deaf of São Paulo, published by the University of São Paulo under the direction of Prof. Fernando Capovilla (EJ669813 – Brazilian Sign Language Lexicography and Technology: Dictionary, Digital Encyclopedia, Chereme-based Sign Retrieval, and Quadriplegic Deaf Communication Systems. Abstracted from Educational Resources Information Center).

Some initial studies found that Deaf communities prefer video or writing systems for the dominant language, however this claim has been disputed by the work of Steve and Dianne Parkhurst in Spain where they found initial resistance, later renewed interest, and finally pride. "If Deaf people learn to read and write in their own signing system, that increases their self-esteem", says Dianne Parkhurst.

, SignWriting is widely used at International Sign forums. It is adopted in as many as 40 countries, among which are Brazil, Ethiopia, France, Germany, Italy, Portugal, Saudi Arabia, Slovenia, Tunisia, and the United States.

SignWriting, as the International Sign Writing Alphabet (ISWA), has been proposed as the manual equivalent to the International Phonetic Alphabet. However, some researchers argue that the SignWriting is not a phonemic orthography and does not have a one-to-one map from phonological forms to written forms. Although such a claim is disputed, it has been recommended that countries adapt this sign on a language-by-language basis. There are two doctoral dissertations that study and promote the application of SignWriting to a specific sign language. Maria Galea wrote about using SignWriting to write Maltese Sign Language. Also, Claudia Savina Bianchini wrote her doctoral dissertation on the implementation of SignWriting to write Italian Sign Language.

In SignWriting, a combination of iconic symbols for handshapes, orientation, body locations, facial expressions, contacts, and movement are used to represent words in sign language. Since SignWriting, as a featural script, represents the actual physical formation of signs rather than their meaning, no phonemic or semantic analysis of a language is required to write it. A person who has learned the system can "feel out" an unfamiliar sign in the same way an English speaking person can "sound out" an unfamiliar word written in the Latin alphabet, without even needing to know what the sign means.

The number of symbols is extensive and often provides multiple ways to write a single sign. Just as it took many centuries for English spelling to become standardized, spelling in SignWriting is not yet standardized for any sign language.

Words may be written from the point of view of the signer or the viewer. However, almost all publications use the point of view of the signer, and assume the right hand is dominant. Sutton originally designed the script to be written horizontally (left-to-right), like English, and from the point of view of the observer, but later changed it to vertical (top-to-bottom) and from the point of view of the signer, to conform to the wishes of Deaf writers.

The orientation of the palm is indicated by filling in the glyph for the hand shape. A hollow outline (white) glyph indicates that one is facing the palm of the hand, a filled (black) glyph indicates that one is facing the back of the hand, and split shading indicates that one is seeing the hand from the side. Although in reality the wrist may turn to intermediate positions, only the four orientations of palm, back, and either side are represented in SignWriting, as they are enough to represent sign languages.

If an unbroken glyph is used, then the hand is placed in the vertical (wall or face) plane in front of the signer, as occurs when finger spelling. A band erased across the glyph through the knuckles shows that the hand lies in the horizontal plane, parallel to the floor. (If one of the basic hand-shape glyphs is used, such as the simple square or circle, this band breaks it in two; however, if there are lines for fingers extended from the base, then they become detached from the base, but the base itself remains intact.)

The diagram to the left shows a BA-hand (flat hand) in six orientations. For the three vertical orientations on the left side, the hand is held in front of the signer, fingers pointing upward. All three glyphs can be rotated, like the hands of a clock, to show the fingers pointing at an angle, to the side, or downward. For the three horizontal orientations on the right side of the diagram, the hand is held outward, with the fingers pointing away from the signer, and presumably toward the viewer. They can also be rotated to show the fingers pointing to the side or toward the signer. Although an indefinite number of orientations can be represented this way, in practice only eight are used for each plane—that is, only multiples of 45° are found.

There are over a hundred glyphs for hand shapes, but all the ones used in ASL are based on five basic elements:

A line halfway across the square or pentagon shows the thumb across the palm. These are the E, B, and (with spread fingers) 4 hands of fingerspelling.

These basic shapes are modified with lines jutting from their faces and corners to represent fingers that are not positioned as described above. Straight lines represent straight fingers (these may be at an angle to indicate that they are not in line with the palm; if they point toward or away from the signer, they have a diamond shape at the tip); curved lines for curved (cupped) fingers; hooked lines for hooked fingers; right-angle lines, for fingers bent at only one joint; and crossed lines, for crossed fingers, as shown in the chart at right. The pentagon and C are only modified to show that the fingers are spread rather than in contact; the angle is only modified to show whether the thumb touches the finger tips or juts out to the side. Although there are some generalizations which can be made for the dozens of other glyphs, which are based on the circle and square, the details are somewhat idiosyncratic and each needs to be memorized.

There are only a few symbols for finger movement. They may be doubled to show that the movement is repeated.

A solid bullet represents flexing the middle joint of a finger or fingers, and a hollow bullet represents straightening a flexed finger. That is, a 'D' hand with a solid bullet means that it becomes an 'X' hand, while an 'X' hand with a hollow bullet means that it becomes a 'D' hand. If the fingers are already flexed, then a solid bullet shows that they squeeze. For example, a square (closed fist, 'S' hand) with double solid bullets is the sign for 'milk' (iconically squeezing an udder).

A downward-pointing chevron represents flexing at the knuckles, while an upward-pointing chevron (^) shows that the knuckles straighten. That is, a 'U' hand with a down chevron becomes an 'N' hand, while and 'N' hand with an up chevron becomes a 'U' hand.

A zigzag like two chevrons (^^) joined together means that the fingers flex repeatedly and in sync. A double-line zigzag means that the fingers wriggle or flutter out of sync.

Hundreds of arrows of various sorts are used to indicate movement of the hands through space. Movement notation gets quite complex, and because it is more exact than it needs to be for any one sign language, different people may choose to write the same sign in different ways.

For movement with the left hand, the Δ-shaped arrowhead is hollow (white); for movement with the right hand, it is solid (black). When both hands move as one, an open (Λ-shaped) arrowhead is used.

As with orientation, movement arrows distinguish two planes: Movement in the vertical plane (up & down) is represented by arrows with double stems, as at the bottom of the diagram at left, while single-stemmed arrows represent movement parallel to the floor (to & fro). In addition, movement in a diagonal plane uses modified double-stemmed arrows: A cross bar on the stem indicates that the motion is away as well up or down, and a solid dot indicates approaching motion. To & fro movement that also goes over or under something uses modified single-stemmed arrows, with the part of the arrow representing near motion thicker than the rest. These are iconic, but conventionalized, and so need to be learned individually.

Straight movements are in one of eight directions for either plane, as in the eight principal directions of a compass. A long straight arrow indicates movement from the elbow, a short arrow with a cross bar behind it indicates motion from the wrist, and a simple short arrow indicates a small movement. (Doubled, in opposite directions, these can show nodding from the wrist.) A secondary curved arrow crossing the main arrow shows that the arm twists while it moves. (Doubled, in opposite directions, these can show shaking of the hand.) Arrows can turn, curve, zigzag, and loop-the-loop.

Arrows on the face at the eyes show the direction of gaze.

Six contact glyphs show hand contact with the location of the sign. That is, a handshape glyph located at the side of the face, together with a contact glyph, indicates that the hand touches the side of the face. The choice of the contact glyph indicates the manner of the contact:

If the signing hand is located at the other hand, the symbol for it is one of the hand shapes above. In practice, only a subset of the more simple hand shapes occurs.

Additional symbols are used to represent sign locations at the face or body parts other than the hands. A circle shows the head.

There are symbols to represent facial movements that are used in various sign languages, including eyes, eyebrows, nose movements, cheeks, mouth movements, and breathing changes. The direction of head movement and eyegaze can also be shown.

Shoulders are shown with a horizontal line. Small arrows can be added to show shoulder and torso movement. Arms and even legs can be added if necessary.

There are also symbols that indicate speed of movement, whether movement is simultaneous or alternating, and punctuation.

Various punctuation symbols exist that correspond to commas, periods, question and exclamation marks, and other punctuation symbols of other scripts. These are written between signs, and lines do not break between a sign and its following punctuation symbol.

One of the unusual characteristics of SignWriting is its use of two-dimensional layout within an invisible 'sign box'. The relative positions of the symbols within the box iconically represent the locations of the hands and other parts of the body involved in the sign being represented. As such, there is no obvious linear relationship between the symbols within each sign box, unlike the sequence of characters within each word in most scripts for spoken languages. This is also unlike other sign language scripts which arrange symbols linearly as in spoken languages. However, since in sign languages many phonetic parameters are articulated simultaneously, these other scripts require arbitrary conventions for specifying the order of different parameters of handshape, location, motion, etc. Although SignWriting does have conventions for how symbols are to be arranged relative to each other within a sign, the two-dimensional layout results in less arbitrariness and more iconicity than other sign language scripts.

Outside of each sign, however, the script is linear, reflecting the temporal order of signs. Signs are most commonly now written in vertical columns (although formerly they were written horizontally). Sign boxes are arranged from top to bottom within the column, interspersed with punctuation symbols, and the columns progress left to right across the page. Within a column, signs may be written down the center or shifted left or right in 'lanes' to indicate side-to-side shifts of the body.

Sutton orders signs in ten groups based on which fingers are extended on the dominant hand. These are equivalent to the numerals one through ten in ASL. Each group is then subdivided according to the actual hand shape, and then subdivided again according to the plane the hand is in (vertical, then horizontal), then again according to the basic orientation of the hand (palm, side, back). An ordering system has been proposed using this beginning and examples from both American Sign Language and Brazilian Sign Language (LIBRAS). The current system of ordering for SignWriting is called the Sign Symbol Sequence which is parsed by the creator of each sign as recorded into the on-line dictionary. This system allows for internal ordering by features including handshape, orientation, speed, location, and other clustered features not found in spoken dictionaries.

Some of the advantages of SignWriting, compared to other writing systems for sign languages, are:
However, it has a few disadvantages as well.

SignWriting is the first writing system for sign languages to be included in the Unicode Standard. 672 characters were added in the Sutton SignWriting (Unicode block) of Unicode version 8.0 released in June 2015. This set of characters is based on SignWriting's standardized symbol set and defined character encoding model.

The Unicode Standard only covers the symbol set. It does not address layout, the positioning of the symbols in two dimensions. Historically, software has recorded position using Cartesian (X-Y) coordinates for each symbol. Since Unicode focuses on symbols that make sense in a one-dimensional plain-text context, the number characters required for two-dimensional placement were not included in the Unicode proposal.

The Unicode block for Sutton SignWriting is U+1D800–U+1DAAF:

Current software records each sign as a string of characters in either ASCII or Unicode. Older software may use XML or a custom binary format to represent a sign. Formal SignWriting uses ASCII characters to define the two-dimensional layout within a sign and other simple structures. It would be possible to fully define a sign in Unicode with seventeen additional characters. With either character set (Unicode or ASCII), the spelling of a sign produces a word that the can be efficiently processed with regular expressions. These sets are isomorphic.

Sutton has released the International SignWriting Alphabet 2010 under the SIL Open Font License. The symbols of the ISWA 2010 are available as individual SVG or as TrueType Fonts.

SignWriting is enabled on with “The Javascript-based SignWriting Keyboard for Use on Wikimedia and throughout the Web” by Yair Rand. Test wikis include the and the other .





</doc>
<doc id="29323" url="https://en.wikipedia.org/wiki?curid=29323" title="Suez Canal">
Suez Canal

The Suez Canal ( "") is a sea-level waterway in Egypt, connecting the Mediterranean Sea to the Red Sea through the Isthmus of Suez. Constructed by the Suez Canal Company between 1859 and 1869, it officially opened on 17 November 1869. The canal offers watercraft a more direct route between the North Atlantic and northern Indian oceans via the Mediterranean and Red seas, thus avoiding the South Atlantic and southern Indian oceans and reducing the journey distance from the Arabian Sea to London, for example, by approximately . It extends from the northern terminus of Port Said to the southern terminus of Port Tewfik at the city of Suez. Its length is including its northern and southern access-channels. In 2012, 17,225 vessels traversed the canal (an average of 47 per day).

The original canal featured a single-lane waterway with passing locations in the Ballah Bypass and the Great Bitter Lake. It contains no lock system, with seawater flowing freely through it. In general, the canal north of the Bitter Lakes flows north in winter and south in summer. South of the lakes, the current changes with the tide at Suez.

The United Kingdom and France owned the canal until July 1956, when the President of Egypt, Gamal Abdel Nasser, nationalized it - an event which led to the Suez Crisis of October–November 1956. The canal is owned and maintained by the Suez Canal Authority (SCA) of Egypt. Under the Convention of Constantinople, it may be used "in time of war as in time of peace, by every vessel of commerce or of war, without distinction of flag". Nevertheless, the canal has played an important military strategic role as a naval short-cut and choke-point. Navies with coastlines and bases on both the Mediterranean and Red Seas (Egypt and Israel) have a particular interest in the Suez Canal.

In August 2014 the Egyptian government launched construction to expand and widen the Ballah Bypass for to speed the canal's transit-time. The expansion intended to nearly double the capacity of the Suez Canal - from 49 to 97 ships per day. At a cost of $8.4 billion, this project was funded with interest-bearing investment certificates issued exclusively to Egyptian entities and individuals. The "New Suez Canal", as the expansion was dubbed, was opened with great fanfare in a ceremony on 6 August 2015.

On 24 February 2016, the Suez Canal Authority officially opened the new side channel. This side channel, located at the northern side of the east extension of the Suez Canal, serves the East Terminal for berthing and unberthing vessels from the terminal. As the East Container Terminal is located on the Canal itself, before the construction of the new side channel it was not possible to berth or unberth vessels at the terminal while a convoy was running.

Ancient west–east canals were built to facilitate travel from the Nile River to the Red Sea. One smaller canal is believed to have been constructed under the auspices of Senusret II or Ramesses II. Another canal, probably incorporating a portion of the first, was constructed under the reign of Necho II, but the only fully functional canal was engineered and completed by Darius I.

The legendary Sesostris (likely either Pharaoh Senusret II or Senusret III of the Twelfth dynasty of Egypt) may have started work on an ancient canal joining the Nile with the Red Sea (1897 BCE – 1839 BCE), when an irrigation channel was constructed around 1850 BCE that was navigable during the flood season, leading into a dry river valley east of the Nile River Delta named Wadi Tumilat. (It is said that in ancient times the Red Sea reached northward to the Bitter Lakes and Lake Timsah.)

In his "Meteorology", Aristotle wrote:
One of their kings tried to make a canal to it (for it would have been of no little advantage to them for the whole region to have become navigable; Sesostris is said to have been the first of the ancient kings to try), but he found that the sea was higher than the land. So he first, and Darius afterwards, stopped making the canal, lest the sea should mix with the river water and spoil it.

Strabo wrote that Sesostris started to build a canal, and Pliny the Elder wrote:

165. Next comes the Tyro tribe and, the harbour of the Daneoi, from which Sesostris, king of Egypt, intended to carry a ship-canal to where the Nile flows into what is known as the Delta; this is a distance of over 60 miles. Later the Persian king Darius had the same idea, and yet again Ptolemy II, who made a trench 100 feet wide, 30 feet deep and about 35 miles long, as far as the Bitter Lakes.
In the second half of the 19th century, French cartographers discovered the remnants of an ancient north–south canal past the east side of Lake Timsah and ending near the north end of the Great Bitter Lake. This proved to be the celebrated canal made by the Persian king Darius I, as his stele commemorating its construction was found at the site. (This ancient, second canal may have followed a course along the shoreline of the Red Sea when it once extended north to Lake Timsah.) In the 20th century the northward extension of this ancient canal was discovered, extending from Lake Timsah to the Ballah Lakes. This was dated to the Middle Kingdom of Egypt by extrapolating the dates of ancient sites along its course.

The reliefs of the Punt expedition under Hatshepsut, 1470 BCE, depict seagoing vessels carrying the expeditionary force returning from Punt. This suggests that a navigable link existed between the Red Sea and the Nile. Recent excavations in Wadi Gawasis may indicate that Egypt's maritime trade started from the Red Sea and did not require a canal. Evidence seems to indicate its existence by the 13th century BCE during the time of Ramesses II.

Remnants of an ancient west–east canal through the ancient Egyptian cities of Bubastis, Pi-Ramesses, and Pithom were discovered by Napoleon Bonaparte and his engineers and cartographers in 1799.

According to the "Histories" of the Greek historian Herodotus, about 600 BCE, Necho II undertook to dig a west–east canal through the Wadi Tumilat between Bubastis and Heroopolis, and perhaps continued it to the Heroopolite Gulf and the Red Sea. Regardless, Necho is reported as having never completed his project.

Herodotus was told that 120,000 men perished in this undertaking, but this figure is doubtless exaggerated. According to Pliny the Elder, Necho's extension to the canal was about 57 English miles, equal to the total distance between Bubastis and the Great Bitter Lake, allowing for winding through valleys. The length that Herodotus tells, of over 1000 stadia (i.e., over ), must be understood to include the entire distance between the Nile and the Red Sea at that time.

With Necho's death, work was discontinued. Herodotus tells that the reason the project was abandoned was because of a warning received from an oracle that others would benefit from its successful completion. Necho's war with Nebuchadnezzar II most probably prevented the canal's continuation.

Necho's project was completed by Darius I of Persia, who ruled over Ancient Egypt after it had been conquered by his predecessor Cambyses II. It may be that by Darius's time a natural waterway passage which had existed between the Heroopolite Gulf and the Red Sea in the vicinity of the Egyptian town of Shaluf (alt. "Chalouf" or "Shaloof"), located just south of the Great Bitter Lake, had become so blocked with silt that Darius needed to clear it out so as to allow navigation once again. According to Herodotus, Darius's canal was wide enough that two triremes could pass each other with oars extended, and required four days to traverse. Darius commemorated his achievement with a number of granite stelae that he set up on the Nile bank, including one near Kabret, and a further one a few miles north of Suez. The Darius Inscriptions read:

The canal left the Nile at Bubastis. An inscription on a pillar at Pithom records that in 270 or 269 BCE, it was again reopened, by Ptolemy II Philadelphus. In Arsinoe, Ptolemy constructed a navigable lock, with sluices, at the Heroopolite Gulf of the Red Sea, which allowed the passage of vessels but prevented salt water from the Red Sea from mingling with the fresh water in the canal.

The Red Sea is believed by some historians to have gradually receded over the centuries, its coastline slowly moving southward away from Lake Timsah and the Great Bitter Lake. Coupled with persistent accumulations of Nile silt, maintenance and repair of Ptolemy's canal became increasingly cumbersome over each passing century.

Two hundred years after the construction of Ptolemy's canal, Cleopatra seems to have had no west–east waterway passage, because the Pelusiac branch of the Nile, which fed Ptolemy's west–east canal, had by that time dwindled, being choked with silt.

By the 8th century, a navigable canal existed between Old Cairo and the Red Sea, but accounts vary as to who ordered its construction—either Trajan or 'Amr ibn al-'As, or Omar the Great. This canal was reportedly linked to the River Nile at Old Cairo and ended near modern Suez. A geography treatise by Dicuil reports a conversation with an English monk, Fidelis, who had sailed on the canal from the Nile to the Red Sea during a pilgrimage to the Holy Land in the first half of the 8th century

The Abbasid Caliph al-Mansur is said to have ordered this canal closed in 767 to prevent supplies from reaching Arabian detractors.

Al-Hakim bi-Amr Allah is claimed to have repaired the Cairo to Red Sea passageway, but only briefly, circa 1000 CE, as it soon "became choked with sand". However, we are told that parts of this canal still continued to fill in during the Nile's annual inundations.

The successful 1488 navigation of southern Africa by Bartolomeu Dias opened a direct maritime trading route to India and the spice islands, and forever changed the balance of Mediterranean trade. One of the most prominent losers in the new order, as former middlemen, was the former spice trading center of Venice.

Despite entering negotiations with Egypt's ruling Mamelukes, the Venetian plan to build the canal was quickly put to rest by the Ottoman conquest of Egypt in 1517, led by Sultan Selim I.

During the 16th century, the Ottoman Grand Vizier Sokollu Mehmed Pasha attempted to construct a canal connecting the Red Sea and the Mediterranean. This was motivated by a desire to connect Constantinople to the pilgrimage and trade routes of the Indian Ocean, as well as by strategic concerns—as the European presence in the Indian Ocean was growing, Ottoman mercantile and strategic interests were increasingly challenged, and the Sublime Porte was increasingly pressed to assert its position. A navigable canal would allow the Ottoman Navy to connect its Red Sea, Black Sea, and Mediterranean fleets. However, this project was deemed too expensive, and was never completed.

During the French campaign in Egypt and Syria in late 1798, Napoleon showed an interest in finding the remnants of an ancient waterway passage. This culminated in a cadre of archaeologists, scientists, cartographers and engineers scouring northern Egypt. Their findings, recorded in the "Description de l'Égypte", include detailed maps that depict the discovery of an ancient canal extending northward from the Red Sea and then westward toward the Nile.

Later, Napoleon, who would become French Emperor in 1804, contemplated the construction of a north–south canal to connect the Mediterranean with the Red Sea. But the plan was abandoned because it wrongly concluded that the waterway would require locks to operate. These would be very expensive and take a long time to construct. This decision was based on an erroneous belief that the Red Sea was higher than the Mediterranean. The error was the result of using fragmentary survey measurements taken in wartime during Napoleon's Egyptian Expedition. In 1819 the Pacha of Egypt undertook some canal work.

However, as late as 1861, the unnavigable ancient route discovered by Napoleon from Bubastis to the Red Sea still channeled water in spots as far east as Kassassin.

Although the alleged difference in sea levels could be problematic for construction, the idea of finding a shorter route to the east remained alive. In 1830, F. R. Chesney submitted a report to the British government that stated that there was no difference in elevation and that the Suez Canal was feasible, but his report received no further attention. Lieutenant Waghorn established his "Overland Route", which transported post and passengers to India via Egypt. Linant de Bellefonds, a French explorer of Egypt, became chief engineer of Egypt's Public Works. In addition to his normal duties, he surveyed the Isthmus of Suez and made plans for the Suez Canal. French Saint-Simonianists showed an interest in the canal and in 1833, Barthélemy Prosper Enfantin tried to draw Muhammad Ali's attention to the canal but was unsuccessful. Alois Negrelli, the Austrian railroad pioneer, became interested in the idea in 1836. In 1846, Prosper Enfantin's Société d'Études du Canal de Suez invited a number of experts, among them Robert Stephenson, Negrelli and Paul-Adrien Bourdaloue to study the feasibility of the Suez Canal (with the assistance of Linant de Bellefonds). Bourdaloue's survey of the isthmus was the first generally accepted evidence that there was no practical difference in altitude between the two seas. Britain, however, feared that a canal open to everyone might interfere with its India trade and therefore preferred a connection by train from Alexandria via Cairo to Suez, which was eventually built by Stephenson.

In 1854 and 1856, Ferdinand de Lesseps obtained a concession from Sa'id Pasha, the Khedive of Egypt and Sudan, to create a company to construct a canal open to ships of all nations. The company was to operate the canal for 99 years from its opening. De Lesseps had used his friendly relationship with Sa'id, which he had developed while he was a French diplomat in the 1830s. As stipulated in the concessions, Ferdinand convened the International Commission for the piercing of the isthmus of Suez ("Commission Internationale pour le percement de l'isthme des Suez") consisting of 13 experts from seven countries, among them John Robinson McClean, later President of the Institution of Civil Engineers in London, and again Negrelli, to examine the plans developed by Linant de Bellefonds, and to advise on the feasibility of and the best route for the canal. After surveys and analyses in Egypt and discussions in Paris on various aspects of the canal, where many of Negrelli's ideas prevailed, the commission produced a unanimous report in December 1856 containing a detailed description of the canal complete with plans and profiles. The Suez Canal Company ("Compagnie universelle du canal maritime de Suez") came into being on 15 December 1858.

Work started on the shore of the future Port Said on 25 April 1859.

The excavation took some 10 years, with forced labour (corvée) being employed until 1864 to dig-out the canal.Some sources estimate that over 30,000 people were working on the canal at any given period, that more than 1.5 million people from various countries were employed, and that thousands of labourers died, many of them from cholera and similar epidemics.

The British government had opposed the project from the outset to its completion. As one of the diplomatic moves against the canal, it disapproved of the use of "slave labour" of forced workers. The British Empire was the major global naval force and officially condemned the forced work and sent armed Bedouins to start a revolt among workers. Involuntary labour on the project ceased, and the viceroy condemned the corvée, halting the project.

Angered by the British opportunism, de Lesseps sent a letter to the British government remarking on the British lack of remorse a few years earlier when forced workers died in similar conditions building the British railway in Egypt.

Initially international opinion was skeptical and Suez Canal Company shares did not sell well overseas. Britain, Austria, and Russia did not buy a significant number of shares. However, with assistance from the Cattaui banking family, and their relationship with James de Rothschild of the French House of Rothschild bonds and shares were successfully promoted in France and other parts of Europe. All French shares were quickly sold in France. A contemporary British skeptic claimed "One thing is sure... our local merchant community doesn't pay practical attention at all to this grand work, and it is legitimate to doubt that the canal's receipts... could ever be sufficient to recover its maintenance fee. It will never become a large ship's accessible way in any case."

The canal opened under French control in November 1869. The opening ceremonies began at Port Said on the evening of 15 November, with illuminations, fireworks, and a banquet on the yacht of the Khedive Isma'il Pasha of Egypt and Sudan. The royal guests arrived the following morning: the French Empress Eugenie in the Imperial yacht "L'Aigle"; the Crown Prince of Prussia; and Prince Louis of Hesse. In the afternoon there were blessings of the canal with both Muslim and Christian ceremonies, a temporary mosque and church having been built side by side on the beach. In the evening there were more illuminations and fireworks.

On the morning of the 17th November, a procession of ships entered the canal, headed by the "Aigle". Among the ships following was HMS "Newport", captained by George Nares which would survey the canal on behalf of the Admiralty a few months later. The "Newport" was involved in an incident that demonstrated some of the problems with the canal. There were suggestions that the depth of parts of the canal at the time of the inauguration were not as great as promised, and that the deepest part of the channel was not always clear, leading to a risk of grounding. The first day of the passage ended at Lake Timsah, 41 nautical miles south of Port Said. The French ship "Péluse" anchored close to the entrance, then swung around and grounded, the ship and its hawser blocking the way into the lake. The following boats had to anchor in the canal itself until the "Péluse" was hauled clear the next morning, making it difficult for them to join that night's celebration in Ismailia. Except for the "Newport". Nares sent out a boat to carry out soundings, and was able to manoeuver around the "Péluse" to enter the lake and anchor there for the night.

Ismailia was the scene of more celebrations the following day, including a military "march past", illuminations and fireworks, and a ball at the Governor's Palace. The convoy set off again on the morning of the 19th November, for the remainder of the trip to Suez. After Suez, many of the participants headed for Cairo, and then to the Pyramids, where a new road had been built for the occasion

An Anchor Line ship, the S.S. "Dido", became the first to pass through the Canal from South to North.

Although numerous technical, political, and financial problems had been overcome, the final cost was more than double the original estimate.

The Khedive, in particular, was able to overcome initial reservations held by both British and French creditors by enlisting the help of the Sursock family, whose deep connections proved invaluable in securing much international support for the project.

After the opening, the Suez Canal Company was in financial difficulties. The remaining works were completed only in 1871, and traffic was below expectations in the first two years. De Lesseps therefore tried to increase revenues by interpreting the kind of net ton referred to in the second concession ("tonneau de capacité") as meaning a ship's cargo capacity and not only the theoretical net tonnage of the "Moorsom System" introduced in Britain by the Merchant Shipping Act in 1854. The ensuing commercial and diplomatic activities resulted in the International Commission of Constantinople establishing a specific kind of net tonnage and settling the question of tariffs in its protocol of 18 December 1873. This was the origin of the Suez Canal Net Tonnage and the Suez Canal Special Tonnage Certificate, both of which are still in use today.

The canal had an immediate and dramatic effect on world trade. Combined with the American transcontinental railroad completed six months earlier, it allowed the world to be circled in record time. It played an important role in increasing European colonization of Africa. The construction of the canal was one of the reasons for the Panic of 1873, because goods from the Far East were carried in sailing vessels around the Cape of Good Hope and were stored in British warehouses. An inability to pay his bank debts led Said Pasha's successor, Isma'il Pasha, in 1875 to sell his 44% share in the canal for £4,000,000 ($19.2 million) equivalent to £432 million to £456 million ($540 million to $570 million) to the government of the United Kingdom. French shareholders still held the majority. Local unrest caused the British to invade in 1882 and take full control, although nominally Egypt remained part of the Ottoman Empire. The British representative 1883 to 1907 was Evelyn Baring, 1st Earl of Cromer, who reorganized and modernized the government and suppressed rebellions and corruption, thereby facilitating increased traffic on the canal.

The Convention of Constantinople in 1888 declared the canal a neutral zone under the protection of the British, who had occupied Egypt and Sudan at the request of Khedive Tewfiq to suppress the Urabi Revolt against his rule. The revolt went on from 1879 to 1882. As a result of British involvement on the side of Khedive Tewfiq, Britain gained control of the canal in 1882. The British defended the strategically important passage against a major Ottoman attack in 1915, during the First World War. Under the Anglo-Egyptian Treaty of 1936, the UK retained control over the canal. The canal was again strategically important in the 1939–1945 Second World War, and Italo-German attempts to capture it were repulsed during the North Africa Campaign, during which the canal was closed to Axis shipping. In 1951 Egypt repudiated the treaty and in October 1954 the UK agreed to remove its troops. Withdrawal was completed on 18 July 1956.

Because of Egyptian overtures towards the Soviet Union, the United Kingdom and the United States withdrew their pledge to support the construction of the Aswan Dam. Egyptian President Gamal Abdel Nasser responded by nationalizing the canal on 26 July 1956 and transferring it to the Suez Canal Authority, intending to finance the dam project using revenue from the canal. On the same day that the canal was nationalized Nasser also closed the Straits of Tiran to all Israeli ships. This led to the Suez Crisis in which the UK, France, and Israel invaded Egypt. According to the pre-agreed war plans under the Protocol of Sèvres, Israel invaded the Sinai Peninsula on 29 October, forcing Egypt to engage them militarily, and allowing the Anglo-French partnership to declare the resultant fighting a threat to stability in the Middle East and enter the war - officially to separate the two forces but in reality to regain the Canal and bring down the Nasser government. 

To save the British from what he thought was a disastrous action and to stop the war from a possible escalation, Canadian Secretary of State for External Affairs Lester B. Pearson proposed the creation of the first United Nations peacekeeping force to ensure access to the canal for all and an Israeli withdrawal from the Sinai Peninsula. On 4 November 1956, a majority at the United Nations voted for Pearson's peacekeeping resolution, which mandated the UN peacekeepers to stay in Sinai unless both Egypt and Israel agreed to their withdrawal. The United States backed this proposal by putting pressure on the British government through the selling of sterling, which would cause it to depreciate. Britain then called a ceasefire, and later agreed to withdraw its troops by the end of the year. Pearson was later awarded the Nobel Peace Prize. As a result of damage and ships sunk under orders from Nasser the canal was closed until April 1957, when it was cleared with UN assistance. A UN force (UNEF) was established to maintain the free navigability of the canal, and peace in the Sinai Peninsula.

In May 1967, Nasser ordered the UN peacekeeping forces out of Sinai, including the Suez Canal area. Israel objected to the closing of the Straits of Tiran to Israeli shipping. The canal had been closed to Israeli shipping since 1949, except for a short period in 1951–1952.

After the 1967 Six-Day War, Israeli forces occupied the Sinai peninsula, including the entire east bank of the Suez Canal. Unwilling to allow the Israelis to use the canal, Egypt immediately imposed a blockade which closed the canal to all shipping. Fifteen cargo ships, known as the "Yellow Fleet", were trapped in the canal, and would remain there until 1975.

In 1973, during the Yom Kippur War, the canal was the scene of a major crossing by the Egyptian army into Israeli-occupied Sinai and a counter-crossing by the Israeli army to Egypt. Much wreckage from this conflict remains visible along the canal's edges.

After the Yom Kippur War the United States initiated Operation Nimbus Moon. The amphibious assault ship USS "Inchon (LPH-12)" was sent to the Canal, carrying 12 RH-53D minesweeping helicopters of HM-12. These partly cleared the canal between May and December 1974. She was relieved by the LST USS "Barnstable County" (LST1197). The British Royal Navy initiated Operation Rheostat and Task Group 65.2 provided for Operation Rheostat One (six months in 1974), the minehunters HMS "Maxton", HMS "Bossington", and HMS "Wilton", the Fleet Clearance Diving Team (FCDT) and HMS "Abdiel", a practice minelayer/MCMV support ship; and for Operation Rheostat Two (six months in 1975) the minehunters HMS Hubberston and HMS Sheraton, and HMS Abdiel. When the Canal Clearance Operations were completed, the canal and its lakes were considered 99% clear of mines. The canal was then reopened by Egyptian President Anwar Sadat aboard an Egyptian destroyer, which led the first convoy northbound to Port Said in 1975. At his side stood the Iranian Crown Prince Reza Pahlavi, delegated to represent his father, Mohammed Reza Pahlavi, the Shah of Iran. The cruiser USS "Little Rock" was the only American naval ship in the convoy.

The UNEF mandate expired in 1979. Despite the efforts of the United States, Israel, Egypt, and others to obtain an extension of the UN role in observing the peace between Israel and Egypt, as called for under the Egypt–Israel Peace Treaty of 1979, the mandate could not be extended because of the veto by the Soviet Union in the UN Security Council, at the request of Syria. Accordingly, negotiations for a new observer force in the Sinai produced the Multinational Force and Observers (MFO), stationed in Sinai in 1981 in coordination with a phased Israeli withdrawal. It is there under agreements between the United States, Israel, Egypt, and other nations.

In the summer of 2014, months after taking office as President of Egypt, Abdel Fattah el-Sisi ordered the expansion of the Ballah Bypass from wide to wide for . The project was called the New Suez Canal, as it would allow ships to transit the canal in both directions simultaneously. The project cost more than $8 billion and was completed within one year. Sisi declared the expanded channel open for business in a ceremony on 6 August 2015.


When built, the canal was long and deep. After several enlargements, it is long, deep and wide. It consists of the northern access channel of , the canal itself of and the southern access channel of .

The so-called "New Suez Canal", functional since 6 August 2015, currently has a new parallel canal in the middle part, with its length over . The current parameters of the Suez Canal, including both individual canals of the parallel section are: depth and width at least (that width measured at of depth).

The canal allows passage of ships up to draft or 240,000 deadweight tons and up to a height of above water level and a maximum beam of under certain conditions. The canal can handle more traffic and larger ships than the Panama Canal, as Suezmax dimensions are greater than both Panamax and New Panamax. Some supertankers are too large to traverse the canal. Others can offload part of their cargo onto a canal-owned boat to reduce their draft, transit, and reload at the other end of the canal.

The canal has no locks because of the flat terrain, and the minor sea level difference between each end is inconsequential for shipping. As the canal has no sea surge gates, the ports at the ends would be subject to the sudden impact of tsunamis from the Mediterranean Sea and Red Sea, according to a 2012 article in the "Journal of Coastal Research".

There is one shipping lane with passing areas in Ballah-Bypass near El Qantara and in the Great Bitter Lake. On a typical day, three convoys transit the canal, two southbound and one northbound. The passage takes between 11 and 16 hours at a speed of around . The low speed helps prevent erosion of the banks by ships' wakes.

By 1955, about two-thirds of Europe's oil passed through the canal. Around 8% of world sea trade is carried via the canal. In 2008, 21,415 vessels passed through the canal and the receipts totaled $5.381 billion, with an average cost per ship of $251,000.

New Rules of Navigation came into force on 1 January 2008, passed by the board of directors of the Suez Canal Authority (SCA) to organise vessels' transit. The most important amendments include allowing vessels with draught to pass, increasing the allowed breadth from to (following improvement operations), and imposing a fine on vessels using divers from outside the SCA inside the canal boundaries without permission. The amendments allow vessels loaded with dangerous cargo (such as radioactive or flammable materials) to pass if they conform with the latest amendments provided by international conventions.

The SCA has the right to determine the number of tugs required to assist warships traversing the canal, to achieve the highest degree of safety during transit.

Before August 2015, the canal was too narrow for free two-way traffic, so ships would pass in convoys and use bypasses. The by-passes were out of (40%). From north to south, they are: Port Said by-pass (entrances) , Ballah by-pass & anchorage, , Timsah by-pass , and the Deversoir by-pass (northern end of the Great Bitter Lake) . The bypasses were completed in 1980.

Typically, it would take a ship 12 to 16 hours to transit the canal. The canal's 24-hour capacity was about 76 standard ships.

In August 2014, Egypt chose a consortium that includes the Egyptian army and global engineering firm Dar Al-Handasah to develop an international industrial and logistics hub in the Suez Canal area, and began the construction of a new canal section from km 60 to km 95 combined with expansion and deep digging of the other 37 km of the canal. This will allow navigation in both directions simultaneously in the 72 km long central section of the canal. These extensions were formally opened on 6 August 2015 by President Al-Sisi.

Since the canal does not cater to unregulated two-way traffic, all ships transit in convoys on regular times, scheduled on a 24-hour basis. Each day, a single northbound convoy starts at 04:00 from Suez. At dual lane sections, the convoy uses the eastern route. Synchronised with this convoy's passage is the southbound convoy. It starts at 03:30 from Port Said and so passes the Northbound convoy in the two-lane section.

From north to south, the crossings are:

A railway on the west bank runs parallel to the canal for its entire length.

Six new tunnels for cars and trains are also planned across the canal. Currently the Ahmed Hamdi is the only tunnel connecting Suez to the Sinai.

The main alternative is around Cape Agulhas, the southernmost point of Africa, commonly referred as the Cape of Good Hope route. This was the only sea route before the canal was constructed, and when the canal was closed. It is still the only route for ships that are too large for the canal. In the early 21st century, the Suez Canal has suffered from diminished traffic due to piracy in Somalia, with many shipping companies choosing to take the long route instead. Between 2008 and 2010, it is estimated that the canal lost 10% of traffic due to the threat of piracy, and another 10% due to the financial crisis. An oil tanker going from Saudi Arabia to the United States has longer to go when taking the route south of Africa rather than the canal.

Before the canal's opening in 1869, goods were sometimes offloaded from ships and carried overland between the Mediterranean and the Red Sea.

In recent years, the shrinking Arctic sea ice has made the Northern Sea Route feasible for commercial cargo ships between Europe and East Asia during a six-to-eight-week window in the summer months, shortening the voyage by thousands of miles compared to that through the Suez Canal. According to polar climate researchers, as the extent of the Arctic summer ice pack recedes the route will become passable without the help of icebreakers for a greater period each summer.

The Bremen-based Beluga Group claimed in 2009 to be the first Western company to attempt using the Northern Sea Route without assistance from icebreakers, cutting 4000 nautical miles off the journey between Ulsan, Korea and Rotterdam, the Netherlands.

Israel has declared that it will construct a railroad through the Negev desert to compete with the canal, with construction partly financed by China.

The opening of the canal created the first salt-water passage between the Mediterranean Sea and the Red Sea. Although the Red Sea is about higher than the eastern Mediterranean, the current between the Mediterranean and the middle of the canal at the Bitter Lakes flows north in winter and south in summer. The current south of the Bitter Lakes is tidal, varying with the tide at Suez. The Bitter Lakes, which were hypersaline natural lakes, blocked the migration of Red Sea species into the Mediterranean for many decades, but as the salinity of the lakes gradually equalised with that of the Red Sea the barrier to migration was removed, and plants and animals from the Red Sea have begun to colonise the eastern Mediterranean.

The Red Sea is generally saltier and more nutrient-poor than the Atlantic, so the Red Sea species have advantages over Atlantic species in the less salty and nutrient-rich eastern Mediterranean. Accordingly, most Red Sea species invade the Mediterranean biota, and only few do the opposite. This migratory phenomenon is called Lessepsian migration (after Ferdinand de Lesseps) or "Erythrean invasion". Also impacting the eastern Mediterranean, starting in 1968, was the operation of Aswan High Dam across the Nile. While providing for increased human development, the project reduced the inflow of freshwater and ended all natural nutrient-rich silt entering the eastern Mediterranean at the Nile Delta. This provided less natural dilution of Mediterranean salinity and ended the higher levels of natural turbidity, additionally making conditions more like those in the Red Sea.

Invasive species originating from the Red Sea and introduced into the Mediterranean by the canal have become a major component of the Mediterranean ecosystem and have serious impacts on the ecology, endangering many local and endemic species. About 300 species from the Red Sea have been identified in the Mediterranean, and there are probably others yet unidentified. The Egyptian government's intent to enlarge the canal has raised concerns from marine biologists, fearing that this will worsen the invasion of Red Sea species.

Construction of the canal was preceded by cutting a small fresh-water canal called Sweet Water Canal from the Nile delta along Wadi Tumilat to the future canal, with a southern branch to Suez and a northern branch to Port Said. Completed in 1863, these brought fresh water to a previously arid area, initially for canal construction, and subsequently facilitating growth of agriculture and settlements along the canal.

The Suez Canal Economic Zone, sometimes shortened to the Suez Canal Zone, describes the set of locations neighbouring the canal where customs rates have been reduced to zero in order to attract investment. The zone comprises over 600 km within the governorates of Port Said, Ismailia and Suez. Projects in the zone are collectively described as the Suez Canal Area Development Project (SCADP).

The plan focuses on development of East Port Said and the port of Ain Sokhna, and hopes to extend to four more ports at West Port Said, El-Adabiya, Arish and El Tor.

The zone incorporates the three "Qualifying Industrial Zones" at Port Said, Ismailia and Suez, a 1996 American initiative to encourage economic ties between Israel and its neighbours.





</doc>
<doc id="29324" url="https://en.wikipedia.org/wiki?curid=29324" title="Signal processing">
Signal processing

Signal processing is an electrical engineering subfield that focuses on analysing, modifying and synthesizing signals such as sound, images and biological measurements. Signal processing techniques can be used to improve transmission, storage efficiency and subjective quality and to also emphasize or detect components of interest in a measured signal.

According to Alan V. Oppenheim and Ronald W. Schafer, the principles of signal processing can be found in the classical numerical analysis techniques of the 17th century. Oppenheim and Schafer further state that the digital refinement of these techniques can be found in the digital control systems of the 1940s and 1950s.

In 1948, Claude Shannon wrote the influential paper "A Mathematical Theory of Communication" which was published in the Bell System Technical Journal. The paper laid the groundwork for later development of information communication systems. Around the same time, methods of signal transmission were being rapidly developed, as a new type of signal emerged called "processing signals".

Electronic signal processing was revolutionized by the MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor), which was originally invented by Mohamed M. Atalla and Dawon Kahng in 1959. MOS integrated circuit technology was the basis for the first single-chip microprocessors and microcontrollers in the early 1970s, and then the first single-chip digital signal processor (DSP) in 1979.

Analog signal processing is for signals that have not been digitized, as in legacy radio, telephone, radar, and television systems. This involves linear electronic circuits as well as non-linear ones. The former are, for instance, passive filters, active filters, additive mixers, integrators and delay lines. Non-linear circuits include compandors, multiplicators (frequency mixers and voltage-controlled amplifiers), voltage-controlled filters, voltage-controlled oscillators and phase-locked loops.

Continuous-time signal processing is for signals that vary with the change of continuous domain (without considering some individual interrupted points).

The methods of signal processing include time domain, frequency domain, and complex frequency domain. This technology mainly discusses the modeling of linear time-invariant continuous system, integral of the system's zero-state response, setting up system function and the continuous time filtering of deterministic signals

Discrete-time signal processing is for sampled signals, defined only at discrete points in time, and as such are quantized in time, but not in magnitude.

"Analog discrete-time signal processing" is a technology based on electronic devices such as sample and hold circuits, analog time-division multiplexers, analog delay lines and analog feedback shift registers. This technology was a predecessor of digital signal processing (see below), and is still used in advanced processing of gigahertz signals.

The concept of discrete-time signal processing also refers to a theoretical discipline that establishes a mathematical basis for digital signal processing, without taking quantization error into consideration.

Digital signal processing is the processing of digitized discrete-time sampled signals. Processing is done by general-purpose computers or by digital circuits such as ASICs, field-programmable gate arrays or specialized digital signal processors (DSP chips). Typical arithmetical operations include fixed-point and floating-point, real-valued and complex-valued, multiplication and addition. Other typical operations supported by the hardware are circular buffers and lookup tables. Examples of algorithms are the fast Fourier transform (FFT), finite impulse response (FIR) filter, Infinite impulse response (IIR) filter, and adaptive filters such as the Wiener and Kalman filters.

Nonlinear signal processing involves the analysis and processing of signals produced from nonlinear systems and can be in the time, frequency, or spatio-temporal domains. Nonlinear systems can produce highly complex behaviors including bifurcations, chaos, harmonics, and subharmonics which cannot be produced or analyzed using linear methods.

Statistical signal processing is an approach which treats signals as stochastic processes, utilizing their statistical properties to perform signal processing tasks. Statistical techniques are widely used in signal processing applications. For example, one can model the probability distribution of noise incurred when photographing an image, and construct techniques based on this model to reduce the noise in the resulting image.


In communication systems, signal processing may occur at:








</doc>
<doc id="29328" url="https://en.wikipedia.org/wiki?curid=29328" title="Six-Day War">
Six-Day War

The Six-Day War (, "Milhemet Sheshet Ha Yamim"; Arabic: , "an-Naksah", "The Setback" or , "Ḥarb 1967", "War of 1967"), also known as the June War, 1967 Arab–Israeli War, or Third Arab–Israeli War, was fought between 5 and 10 June 1967 by Israel and the neighboring states of Egypt (known at the time as the United Arab Republic), Jordan, and Syria.

Relations between Israel and its neighbours were not fully normalised after the 1948 Arab–Israeli War. In 1956 Israel invaded the Sinai peninsula in Egypt, with one of its objectives being the reopening of the Straits of Tiran that Egypt had blocked to Israeli shipping since 1950. Israel was eventually forced to withdraw, but was guaranteed that the Straits of Tiran would remain open. A United Nations Emergency Force was deployed along the border, but there was no demilitarisation agreement.

In the months prior to June 1967, tensions became dangerously heightened. Israel reiterated its post-1956 position that the closure of the Straits of Tiran to Israeli shipping would be a cause for war (a "casus belli"). In May Egyptian President Gamal Abdel Nasser announced that the straits would be closed to Israeli vessels and then mobilised its Egyptian forces along its border with Israel. On 5 June, Israel launched what it claimed were a series of preemptive airstrikes against Egyptian airfields. The question of which side caused the war is one of a number of controversies relating to the conflict.

The Egyptians were caught by surprise, and nearly the entire Egyptian air force was destroyed with few Israeli losses, giving the Israelis air supremacy. Simultaneously, the Israelis launched a ground offensive into the Gaza Strip and the Sinai, which again caught the Egyptians by surprise. After some initial resistance, Nasser ordered the evacuation of the Sinai. Israeli forces rushed westward in pursuit of the Egyptians, inflicted heavy losses, and conquered the Sinai.

Jordan had entered into a defence pact with Egypt a week before the war began; the agreement envisaged that in the event of war Jordan would not take an offensive role but would attempt to tie down Israeli forces to prevent them making territorial gains. About an hour after the Israeli air attack, the Egyptian commander of the Jordanian army was ordered by Cairo to begin attacks on Israel; in the initially confused situation, the Jordanians were told that Egypt had repelled the Israeli air strikes.

Egypt and Jordan agreed to a ceasefire on 8 June, and Syria agreed on 9 June; a ceasefire was signed with Israel on 11 June. In the aftermath of the war, Israel had crippled the Egyptian, Syrian and Jordanian militaries, having killed over 20,000 troops while only losing fewer than 1,000 of its own. The Israeli success was the result of a well-prepared and enacted strategy, the poor leadership of the Arab states, and their poor military leadership and strategy. Israel seized the Gaza Strip and the Sinai Peninsula from Egypt, the West Bank, including East Jerusalem, from Jordan and the Golan Heights from Syria. Israel's international standing greatly improved in the following years. Its victory humiliated Egypt, Jordan and Syria, leading Nasser to resign in shame; he was later reinstated after protests in Egypt against his resignation. The speed and ease of Israel's victory would later lead to a dangerous overconfidence within the ranks of the Israel Defense Forces (IDF), contributing to initial Arab successes in the subsequent 1973 Yom Kippur War, although ultimately Israeli forces were successful and defeated the Arab militaries. The displacement of civilian populations resulting from the war would have long-term consequences, as 300,000 Palestinians fled the West Bank and about 100,000 Syrians left the Golan Heights. Across the Arab world, Jewish minority communities fled or were expelled, with refugees going mainly to Israel or Europe.

After the 1956 Suez Crisis, Egypt agreed to the stationing of a United Nations Emergency Force (UNEF) in the Sinai to ensure all parties would comply with the 1949 Armistice Agreements. In the following years there were numerous minor border clashes between Israel and its Arab neighbours, particularly Syria. In early November 1966, Syria signed a mutual defence agreement with Egypt. Soon after this, in response to Palestine Liberation Organisation (PLO) guerilla activity, including a mine attack that left three dead, the Israeli Defence Force (IDF) attacked the village of as-Samu in the Jordanian-occupied West Bank. Jordanian units that engaged the Israelis were quickly beaten back. King Hussein of Jordan criticized Egyptian President Gamal Abdel Nasser for failing to come to Jordan's aid, and "hiding behind UNEF skirts".

In May 1967, Nasser received false reports from the Soviet Union that Israel was massing on the Syrian border. Nasser began massing his troops in two defensive lines in the Sinai Peninsula on Israel's border (16 May), expelled the UNEF force from Gaza and Sinai (19 May) and took over UNEF positions at Sharm el-Sheikh, overlooking the Straits of Tiran. Israel repeated declarations it had made in 1957 that any closure of the Straits would be considered an act of war, or justification for war, but Nasser closed the Straits to Israeli shipping on 22–23 May. After the war, U.S. President Lyndon Johnson commented:

On 30 May, Jordan and Egypt signed a defence pact. The following day, at Jordan's invitation, the Iraqi army began deploying troops and armoured units in Jordan. They were later reinforced by an Egyptian contingent. On 1 June, Israel formed a National Unity Government by widening its cabinet, and on 4 June the decision was made to go to war. The next morning, Israel launched Operation Focus, a large-scale, surprise air strike that was the opening of the Six-Day War.

Before the war, Israeli pilots and ground crews had trained extensively in rapid refitting of aircraft returning from sorties, enabling a single aircraft to sortie up to four times a day (as opposed to the norm in Arab air forces of one or two sorties per day). This enabled the Israeli Air Force (IAF) to send several attack waves against Egyptian airfields on the first day of the war, overwhelming the Egyptian Air Force, and allowed it to knock out other Arab air forces on the same day. This has contributed to the Arab belief that the IAF was helped by foreign air forces (see Controversies relating to the Six-Day War). Pilots were extensively schooled about their targets, and were forced to memorise every single detail, and rehearsed the operation multiple times on dummy runways in total secrecy.

The Egyptians had constructed fortified defences in the Sinai. These designs were based on the assumption that an attack would come along the few roads leading through the desert, rather than through the difficult desert terrain. The Israelis chose not to risk attacking the Egyptian defences head-on, and instead surprised them from an unexpected direction.

James Reston, writing in "The New York Times" on 23 May 1967, noted, "In discipline, training, morale, equipment and general competence his [Nasser's] army and the other Arab forces, without the direct assistance of the Soviet Union, are no match for the Israelis. ... Even with 50,000 troops and the best of his generals and air force in Yemen, he has not been able to work his way in that small and primitive country, and even his effort to help the Congo rebels was a flop."

On the eve of the war, Israel believed it could win a war in 3–4 days. The United States estimated Israel would need 7–10 days to win, with British estimates supporting the U.S. view.

The Israeli army had a total strength, including reservists, of 264,000, though this number could not be sustained, as the reservists were vital to civilian life.

Against Jordan's forces on the West Bank, Israel deployed about 40,000 troops and 200 tanks (eight brigades). Israeli Central Command forces consisted of five brigades. The first two were permanently stationed near Jerusalem and were the Jerusalem Brigade and the mechanized Harel Brigade. Mordechai Gur's 55th Paratroopers Brigade was summoned from the Sinai front. The 10th Armored Brigade was stationed north of the West Bank. The Israeli Northern Command comprised a division of three brigades led by Major General Elad Peled which was stationed in the Jezreel Valley to the north of the West Bank.

On the eve of the war, Egypt massed approximately 100,000 of its 160,000 troops in the Sinai, including all seven of its divisions (four infantry, two armoured and one mechanized), four independent infantry brigades and four independent armoured brigades. Over a third of these soldiers were veterans of Egypt's continuing intervention into the North Yemen Civil War and another third were reservists. These forces had 950 tanks, 1,100 APCs, and more than 1,000 artillery pieces.

Syria's army had a total strength of 75,000 and was deployed along the border with Israel. Professor David W. Lesch wrote that "One would be hard-pressed to find a military less prepared for war with a clearly superior foe", since Syria's army had been decimated in the months and years prior through coups and attempted coups that had resulted in a series of purges, fracturings and uprisings within the armed forces.

The Jordanian Armed Forces included 11 brigades, totalling 55,000 troops. Nine brigades (45,000 troops, 270 tanks, 200 artillery pieces) were deployed in the West Bank, including the elite armoured 40th, and two in the Jordan Valley. They possessed sizable numbers of M113 APCs and were equipped with some 300 modern Western tanks, 250 of which were U.S. M48 Pattons. They also had 12 battalions of artillery, six batteries of 81 mm and 120 mm mortars, a paratrooper battalion trained in the new U.S.-built school and a new battalion of mechanized infantry. The Jordanian Army, then known as the Arab Legion, was a long-term-service, professional army, relatively well-equipped and well-trained. Israeli post-war briefings said that the Jordanian staff acted professionally, but was always left "half a step" behind by the Israeli moves. The small Royal Jordanian Air Force consisted of only 24 British-made Hawker Hunter fighters, six transports, and two helicopters. According to the Israelis, the Hawker Hunter was essentially on par with the French-built Dassault Mirage III – the IAF's best plane.

100 Iraqi tanks and an infantry division were readied near the Jordanian border. Two squadrons of Iraqi fighter-aircraft, Hawker Hunters and MiG 21s, were rebased adjacent to the Jordanian border.

The Arab air forces were reinforced by some aircraft from Libya, Algeria, Morocco, Kuwait, and Saudi Arabia to make up for the massive losses suffered on the first day of the war. They were also aided by volunteer pilots from the Pakistan Air Force acting in an independent capacity. PAF pilots shot down several Israeli planes.

With the exception of Jordan, the Arabs relied principally on Soviet weaponry. Jordan's army was equipped with American weaponry, and its air force was composed of British aircraft.

Egypt had by far the largest and the most modern of all the Arab air forces, consisting of about 420 combat aircraft, all of them Soviet-built and with a heavy quota of top-of-the-line MiG-21s. Of particular concern to the Israelis were the 30 Tu-16 "Badger" medium bombers, capable of inflicting heavy damage on Israeli military and civilian centers.

Israeli weapons were mainly of Western origin. Its air force was composed principally of French aircraft, while its armoured units were mostly of British and American design and manufacture. Some infantry weapons, including the ubiquitous Uzi, were of Israeli origin.

The first and most critical move of the conflict was a surprise Israeli attack on the Egyptian Air Force. Initially, both Egypt and Israel announced that they had been attacked by the other country.

On 5 June at 7:45 Israeli time, as civil defence sirens sounded all over Israel, the IAF launched Operation Focus ("Moked"). All but 12 of its nearly 200 operational jets launched a mass attack against Egypt's airfields. The Egyptian defensive infrastructure was extremely poor, and no airfields were yet equipped with hardened aircraft shelters capable of protecting Egypt's warplanes. Most of the Israeli warplanes headed out over the Mediterranean Sea, flying low to avoid radar detection, before turning toward Egypt. Others flew over the Red Sea.

Meanwhile, the Egyptians hindered their own defence by effectively shutting down their entire air defence system: they were worried that rebel Egyptian forces would shoot down the plane carrying Field Marshal Abdel Hakim Amer and Lt-Gen. Sidqi Mahmoud, who were en route from al Maza to Bir Tamada in the Sinai to meet the commanders of the troops stationed there. In any event, it did not make a great deal of difference as the Israeli pilots came in below Egyptian radar cover and well below the lowest point at which its SA-2 surface-to-air missile batteries could bring down an aircraft.

Although the powerful Jordanian radar facility at Ajloun detected waves of aircraft approaching Egypt and reported the code word for "war" up the Egyptian command chain, Egyptian command and communications problems prevented the warning from reaching the targeted airfields. The Israelis employed a mixed-attack strategy: bombing and strafing runs against planes parked on the ground, and bombing to disable runways with special tarmac-shredding penetration bombs developed jointly with France, leaving surviving aircraft unable to take off. The runway at the Arish airfield was spared, as the Israelis expected to turn it into a military airport for their transports after the war. Surviving aircraft were taken out by later attack waves. The operation was more successful than expected, catching the Egyptians by surprise and destroying virtually all of the Egyptian Air Force on the ground, with few Israeli losses. Only four unarmed Egyptian training flights were in the air when the strike began. A total of 338 Egyptian aircraft were destroyed and 100 pilots were killed, although the number of aircraft lost by the Egyptians is disputed.

Among the Egyptian planes lost were all 30 Tu-16 bombers, 27 out of 40 Il-28 bombers, 12 Su-7 fighter-bombers, over 90 MiG-21s, 20 MiG-19s, 25 MiG-17 fighters, and around 32 assorted transport planes and helicopters. In addition, Egyptian radars and SAM missiles were also attacked and destroyed. The Israelis lost 19 planes, including two destroyed in air-to-air combat and 13 downed by anti-aircraft artillery. One Israeli plane, which was damaged and unable to break radio silence, was shot down by Israeli Hawk missiles after it strayed over the Negev Nuclear Research Center. Another was destroyed by an exploding Egyptian bomber.

The attack guaranteed Israeli air supremacy for the rest of the war. Attacks on other Arab air forces by Israel took place later in the day as hostilities broke out on other fronts.

The large numbers of Arab aircraft claimed destroyed by Israel on that day were at first regarded as "greatly exaggerated" by the Western press. However, the fact that the Egyptian Air Force, along with other Arab air forces attacked by Israel, made practically no appearance for the remaining days of the conflict proved that the numbers were most likely authentic. Throughout the war, Israeli aircraft continued strafing Arab airfield runways to prevent their return to usability. Meanwhile, Egyptian state-run radio had reported an Egyptian victory, falsely claiming that 70 Israeli planes had been downed on the first day of fighting.

The Egyptian forces consisted of seven divisions: four armoured, two infantry, and one mechanized infantry. Overall, Egypt had around 100,000 troops and 900–950 tanks in the Sinai, backed by 1,100 APCs and 1,000 artillery pieces. This arrangement was thought to be based on the Soviet doctrine, where mobile armour units at strategic depth provide a dynamic defense while infantry units engage in defensive battles.

Israeli forces concentrated on the border with Egypt included six armoured brigades, one infantry brigade, one mechanized infantry brigade, three paratrooper brigades, giving a total of around 70,000 men and 700 tanks, who were organized in three armoured divisions. They had massed on the border the night before the war, camouflaging themselves and observing radio silence before being ordered to advance.

The Israeli plan was to surprise the Egyptian forces in both timing (the attack exactly coinciding with the IAF strike on Egyptian airfields), location (attacking via northern and central Sinai routes, as opposed to the Egyptian expectations of a repeat of the 1956 war, when the IDF attacked via the central and southern routes) and method (using a combined-force flanking approach, rather than direct tank assaults).

On 5 June, at 7:50 a.m., the northernmost Israeli division, consisting of three brigades and commanded by Major General Israel Tal, one of Israel's most prominent armour commanders, crossed the border at two points, opposite Nahal Oz and south of Khan Yunis. They advanced swiftly, holding fire to prolong the element of surprise. Tal's forces assaulted the "Rafah Gap", a seven-mile stretch containing the shortest of three main routes through the Sinai towards El-Qantarah el-Sharqiyya and the Suez Canal. The Egyptians had four divisions in the area, backed by minefields, pillboxes, underground bunkers, hidden gun emplacements and trenches. The terrain on either side of the route was impassable. The Israeli plan was to hit the Egyptians at selected key points with concentrated armour.

Tal's advance was led by the 7th Armored Brigade under Colonel Shmuel Gonen. The Israeli plan called for the 7th Brigade to outflank Khan Yunis from the north and the 60th Armored Brigade under Colonel Menachem Aviram would advance from the south. The two brigades would link up and surround Khan Yunis, while the paratroopers would take Rafah. Gonen entrusted the breakthrough to a single battalion of his brigade.

Initially, the advance was met with light resistance, as Egyptian intelligence had concluded that it was a diversion for the main attack. However, as Gonen's lead battalion advanced, it suddenly came under intense fire and took heavy losses. A second battalion was brought up, but was also pinned down. Meanwhile, the 60th Brigade became bogged down in the sand, while the paratroopers had trouble navigating through the dunes. The Israelis continued to press their attack, and despite heavy losses, cleared the Egyptian positions and reached the Khan Yunis railway junction in little over four hours.

Gonen's brigade then advanced nine miles to Rafah in twin columns. Rafah itself was circumvented, and the Israelis attacked Sheikh Zuweid, eight miles to the southwest, which was defended by two brigades. Though inferior in numbers and equipment, the Egyptians were deeply entrenched and camouflaged. The Israelis were pinned down by fierce Egyptian resistance, and called in air and artillery support to enable their lead elements to advance. Many Egyptians abandoned their positions after their commander and several of his staff were killed.

The Israelis broke through with tank-led assaults. However, Aviram's forces misjudged the Egyptians' flank, and were pinned between strongholds before they were extracted after several hours. By nightfall, the Israelis had finished mopping up resistance. Israeli forces had taken significant losses, with Colonel Gonen later telling reporters that "we left many of our dead soldiers in Rafah, and many burnt-out tanks." The Egyptians suffered some 2,000 casualties and lost 40 tanks.

On 5 June, with the road open, Israeli forces continued advancing towards Arish. Already by late afternoon, elements of the 79th Armored Battalion had charged through the seven-mile long Jiradi defile, a narrow pass defended by well-emplaced troops of the Egyptian 112th Infantry Brigade. In fierce fighting, which saw the pass change hands several times, the Israelis charged through the position. The Egyptians suffered heavy casualties and tank losses, while Israeli losses stood at 66 dead, 93 wounded and 28 tanks. Emerging at the western end, Israeli forces advanced to the outskirts of Arish. As it reached the outskirts of Arish, Tal's division also consolidated its hold on Rafah and Khan Yunis.

The following day, 6 June, the Israeli forces on the outskirts of Arish were reinforced by the 7th Brigade, which fought its way through the Jiradi pass. After receiving supplies via an airdrop, the Israelis entered the city and captured the airport at 7:50 am. The Israelis entered the city at 8:00 am. Company commander Yossi Peled recounted that "Al-Arish was totally quiet, desolate. Suddenly, the city turned into a madhouse. Shots came at us from every alley, every corner, every window and house." An IDF record stated that "clearing the city was hard fighting. The Egyptians fired from the rooftops, from balconies and windows. They dropped grenades into our half-tracks and blocked the streets with trucks. Our men threw the grenades back and crushed the trucks with their tanks." Gonen sent additional units to Arish, and the city was eventually taken.

Brigadier-General Avraham Yoffe's assignment was to penetrate Sinai south of Tal's forces and north of Sharon's. Yoffe's attack allowed Tal to complete the capture of the Jiradi defile, Khan Yunis. All of them were taken after fierce fighting. Gonen subsequently dispatched a force of tanks, infantry and engineers under Colonel Yisrael Granit to continue down the Mediterranean coast towards the Suez Canal, while a second force led by Gonen himself turned south and captured Bir Lahfan and Jabal Libni.

Further south, on 6 June, the Israeli 38th Armored Division under Major-General Ariel Sharon assaulted Um-Katef, a heavily fortified area defended by the Egyptian 2nd Infantry Division under Major-General Sa'adi Naguib (though Naguib was actially absent) and consisting of some 16,000 troops. The Egyptians also had a battalion of tank destroyers and a tank regiment, formed of Soviet World War II armour, which included 90 T-34-85 tanks, 22 SU-100 tank destroyers, and about 16,000 men. The Israelis had about 14,000 men and 150 post-World War II tanks including the AMX-13, Centurions, and M50 Super Shermans (modified M-4 Sherman tanks).

Two armoured brigades in the meantime, under Avraham Yoffe, slipped across the border through sandy wastes that Egypt had left undefended because they were considered impassable. Simultaneously, Sharon's tanks from the west were to engage Egyptian forces on Um-Katef ridge and block any reinforcements. Israeli infantry would clear the three trenches, while heliborne paratroopers would land behind Egyptian lines and silence their artillery. An armoured thrust would be made at al-Qusmaya to unnerve and isolate its garrison.
As Sharon's division advanced into the Sinai, Egyptian forces staged successful delaying actions at Tarat Umm, Umm Tarfa, and Hill 181. An Israeli jet was downed by anti-aircraft fire, and Sharon's forces came under heavy shelling as they advanced from the north and west. The Israeli advance, which had to cope with extensive minefields, took a large number of casualties. A column of Israeli tanks managed to penetrate the northern flank of Abu Ageila, and by dusk, all units were in position. The Israelis then brought up ninety 105 mm and 155 mm artillery guns for a preparatory barrage, while civilian buses brought reserve infantrymen under Colonel Yekutiel Adam and helicopters arrived to ferry the paratroopers. These movements were unobserved by the Egyptians, who were preoccupied with Israeli probes against their perimeter.

As night fell, the Israeli assault troops lit flashlights, each battalion a different color, to prevent friendly fire incidents. At 10:00 pm, Israeli artillery began a barrage on Um-Katef, firing some 6,000 shells in less than twenty minutes, the most concentrated artillery barrage in Israel's history. Israeli tanks assaulted the northernmost Egyptian defenses and were largely successful, though an entire armoured brigade was stalled by mines, and had only one mine-clearance tank. Israeli infantrymen assaulted the triple line of trenches in the east. To the west, paratroopers commanded by Colonel Danny Matt landed behind Egyptian lines, though half the helicopters got lost and never found the battlefield, while others were unable to land due to mortar fire. Those that successfully landed on target destroyed Egyptian artillery and ammunition dumps and separated gun crews from their batteries, sowing enough confusion to significantly reduce Egyptian artillery fire. Egyptian reinforcements from Jabal Libni advanced towards Um-Katef to counterattack, but failed to reach their objective, being subjected to heavy air attacks and encountering Israeli lodgements on the roads. Egyptian commanders then called in artillery attacks on their own positions. The Israelis accomplished and sometimes exceeded their overall plan, and had largely succeeded by the following day. The Egyptians took heavy casualties, while the Israelis lost 40 dead and 140 wounded.

Yoffe's attack allowed Sharon to complete the capture of the Um-Katef, after fierce fighting. The main thrust at Um-Katef was stalled due to mines and craters. After IDF engineers had cleared a path by 4:00 pm, Israeli and Egyptian tanks engaged in fierce combat, often at ranges as close as ten yards. The battle ended in an Israeli victory, with 40 Egyptian and 19 Israeli tanks destroyed. Meanwhile, Israeli infantry finished clearing out the Egyptian trenches, with Israeli casualties standing at 14 dead and 41 wounded and Egyptian casualties at 300 dead and 100 taken prisoner.

Further south, on 5 June, the 8th Armored Brigade under Colonel Albert Mandler, initially positioned as a ruse to draw off Egyptian forces from the real invasion routes, attacked the fortified bunkers at Kuntilla, a strategically valuable position whose capture would enable Mandler to block reinforcements from reaching Um-Katef and to join Sharon's upcoming attack on Nakhl. The defending Egyptian battalion, outnumbered and outgunned, fiercely resisted the attack, hitting a number of Israeli tanks. However, most of the defenders were killed, and only three Egyptian tanks, one of them damaged, survived. By nightfall, Mendler's forces had taken Kuntilla.

With the exceptions of Rafah and Khan Yunis, Israeli forces had initially avoided entering the Gaza Strip. Israeli Defense Minister Moshe Dayan had expressly forbidden entry into the area. After Palestinian positions in Gaza opened fire on the Negev settlements of Nirim and Kissufim, IDF Chief of Staff Yitzhak Rabin overrode Dayan's instructions and ordered the 11th Mechanized Brigade under Colonel Yehuda Reshef to enter the Strip. The force was immediately met with heavy artillery fire and fierce resistance from Palestinian forces and remnants of the Egyptian forces from Rafah.

By sunset, the Israelis had taken the strategically vital Ali Muntar ridge, overlooking Gaza City, but were beaten back from the city itself. Some 70 Israelis were killed, along with Israeli journalist Ben Oyserman and American journalist Paul Schutzer. Twelve members of UNEF were also killed. On the war's second day, 6 June, the Israelis were bolstered by the 35th Paratroopers Brigade under Colonel Rafael Eitan, and took Gaza City along with the entire Strip. The fighting was fierce, and accounted for nearly half of all Israeli casualties on the southern front. However, Gaza rapidly fell to the Israelis.

Meanwhile, on 6 June, two Israeli reserve brigades under Yoffe, each equipped with 100 tanks, penetrated the Sinai south of Tal's division and north of Sharon's, capturing the road junctions of Abu Ageila, Bir Lahfan, and Arish, taking all of them before midnight. Two Egyptian armoured brigades counterattacked, and a fierce battle took place until the following morning. The Egyptians were beaten back by fierce resistance coupled with airstrikes, sustaining heavy tank losses. They fled west towards Jabal Libni.

During the ground fighting, remnants of the Egyptian Air Force attacked Israeli ground forces, but took losses from the Israeli Air Force and from Israeli anti-aircraft units. Throughout the last four days, Egyptian aircraft flew 150 sorties against Israeli units in the Sinai.

Many of the Egyptian units remained intact and could have tried to prevent the Israelis from reaching the Suez Canal, or engaged in combat in the attempt to reach the canal. However, when the Egyptian Field Marshal Abdel Hakim Amer heard about the fall of Abu-Ageila, he panicked and ordered all units in the Sinai to retreat. This order effectively meant the defeat of Egypt.

Meanwhile, President Nasser, having learned of the results of the Israeli air strikes, decided together with Field Marshal Amer to order a general retreat from the Sinai within 24 hours. No detailed instructions were given concerning the manner and sequence of withdrawal.

As Egyptian columns retreated, Israeli aircraft and artillery attacked them. Israeli jets used napalm bombs during their sorties. The attacks destroyed hundreds of vehicles and caused heavy casualties. At Jabal Libni, retreating Egyptian soldiers were fired upon by their own artillery. At Bir Gafgafa, the Egyptians fiercely resisted advancing Israeli forces, knocking out three tanks and eight half-tracks, and killing 20 soldiers. Due to the Egyptians' retreat, the Israeli High Command decided not to pursue the Egyptian units but rather to bypass and destroy them in the mountainous passes of West Sinai.

Therefore, in the following two days ( 6 and 7 June), all three Israeli divisions (Sharon and Tal were reinforced by an armoured brigade each) rushed westwards and reached the passes. Sharon's division first went southward then westward, via An-Nakhl, to Mitla Pass with air support. It was joined there by parts of Yoffe's division, while its other units blocked the Gidi Pass. These passes became killing grounds for the Egyptians, who ran right into waiting Israeli positions and suffered heavy losses in both soldiers and vehicles. According to Egyptian diplomat Mahmoud Riad, 10,000 men were killed in one day alone, and many others died from hunger and thirst. Tal's units stopped at various points to the length of the Suez Canal.

Israel's blocking action was partially successful. Only the Gidi pass was captured before the Egyptians approached it, but at other places, Egyptian units managed to pass through and cross the canal to safety. Due to the haste of the Egyptian retreat, soldiers often abandoned weapons, military equipment, and hundreds of vehicles. Many Egyptian soldiers were cut off from their units had to walk about 200 kilometers on foot before reaching the Suez Canal with limited supplies of food and water and were exposed to intense heat. Thousands of soldiers died as a result. Many Egyptian soldiers chose instead to surrender to the Israelis. However, the Israelis eventually exceeded their capabilities to provide for prisoners. As a result, they began directing soldiers towards the Suez Canal and only taking prisoner high-ranking officers, who were expected to be exchanged for captured Israeli pilots.

According to some accounts, during the Egyptian retreat from the Sinai, a unit of Soviet Marines based on a Soviet warship in Port Said at the time came ashore and attempted to cross the Suez Canal eastward. The Soviet force was reportedly decimated by an Israeli air attack and lost 17 dead and 34 wounded. Among the wounded was the commander, Lt. Col. Victor Shevchenko.

During the offensive, the Israeli Navy landed six combat divers from the Shayetet 13 naval commando unit to infiltrate Alexandria harbour. The divers sank an Egyptian minesweeper before being taken prisoner. Shayetet 13 commandos also infiltrated into Port Said harbour, but found no ships there. A planned commando raid against the Syrian Navy never materialized. Both Egyptian and Israeli warships made movements at sea to intimidate the other side throughout the war, but did not engage each other. However, Israeli warships and aircraft did hunt for Egyptian submarines throughout the war.

On 7 June, Israel began the conquest of Sharm el-Sheikh. The Israeli Navy started the operation with a probe of Egyptian naval defenses. An aerial reconnaissance flight found that the area was less defended than originally thought. At about 4:30 am, three Israeli missile boats opened fire on Egyptian shore batteries, while paratroopers and commandos boarded helicopters and Nord Noratlas transport planes for an assault on Al-Tur, as Chief of Staff Rabin was convinced it was too risky to land them directly in Sharm el-Sheikh. However, the city had been largely abandoned the day before, and reports from air and naval forces finally convinced Rabin to divert the aircraft to Sharm el-Sheikh. There, the Israelis engaged in a pitched battle with the Egyptians and took the city, killing 20 Egyptian soldiers and taking 8 prisoner. At 12:15 pm, Defense Minister Dayan announced that the Straits of Tiran constituted an international waterway open to all ships without restriction.

On 8 June, Israel completed the capture of the Sinai by sending infantry units to Ras Sudar on the western coast of the peninsula.

Several tactical elements made the swift Israeli advance possible: first, the surprise attack that quickly gave the Israeli Air Force complete air superiority over the Egyptian Air Force; second, the determined implementation of an innovative battle plan; third, the lack of coordination among Egyptian troops. These factors would prove to be decisive elements on Israel's other fronts as well.

King Hussein had given control of his army to Egypt in 1 June, on which date Egyptian General Riad arrived in Amman to take control of the Jordanian military.

Egyptian Field Marshal Amer used the confusion of the first hours of the conflict to send a cable to Amman that he was victorious; he claimed as evidence a radar sighting of a squadron of Israeli aircraft returning from bombing raids in Egypt, which he said was an Egyptian aircraft en route to attack Israel. In this cable, sent shortly before 9:00am, Riad was ordered to attack.

One of the Jordanian brigades stationed in the West Bank was sent to the Hebron area in order to link with the Egyptians.

The IDF's strategic plan was to remain on the defensive along the Jordanian front, to enable focus in the expected campaign against Egypt.

Intermittent machine-gun exchanges began taking place in Jerusalem at 9:30 am, and the fighting gradually escalated as the Jordanians introduced mortar and recoilless rifle fire. Under the orders from General Narkis, the Israelis responded only with small-arms fire, firing in a flat trajectory to avoid hitting civilians, holy sites or the Old City. At 10:00 am on 5 June, the Jordanian Army began shelling Israel. Two batteries of 155 mm Long Tom cannons opened fire on the suburbs of Tel Aviv and Ramat David Airbase. The commanders of these batteries were instructed to lay a two-hour barrage against military and civilian settlements in central Israel. Some shells hit the outskirts of Tel Aviv.

By 10:30 am, Eshkol had sent a message via Odd Bull to King Hussein promising not to initiate any action against Jordan if it stayed out of the war. King Hussein replied that it was too late, "the die was cast". At 11:15 am, Jordanian howitzers began a 6,000-shell barrage at Israeli Jerusalem. The Jordanians initially targeted kibbutz Ramat Rachel in the south and Mount Scopus in the north, then ranged into the city center and outlying neighborhoods. Military installations, the Prime Minister's Residence, and the Knesset compound were also targeted. Israeli civilian casualties totalled 20 dead and about 1,000 wounded. Some 900 buildings were damaged, including Hadassah Ein Kerem Hospital.

At 11:50 am, sixteen Jordanian Hawker Hunters attacked Netanya, Kfar Sirkin and Kfar Saba, killing one civilian, wounding seven and destroying a transport plane. Three Iraqi Hawker Hunters strafed civilian settlements in the Jezreel Valley, and an Iraqi Tupolev Tu-16 attacked Afula, and was shot down near the Megiddo airfield. The attack caused minimal material damage, hitting only a senior citizens' home and several chicken coops, but sixteen Israeli soldiers were killed, most of them when the Tupolev crashed.

When the Israeli cabinet convened to decide what to do, Yigal Allon and Menahem Begin argued that this was an opportunity to take the Old City of Jerusalem, but Eshkol decided to defer any decision until Moshe Dayan and Yitzhak Rabin could be consulted. Uzi Narkiss made a number of proposals for military action, including the capture of Latrun, but the cabinet turned him down.
Dayan rejected multiple requests from Narkiss for permission to mount an infantry assault towards Mount Scopus. However, Dayan sanctioned a number of more limited retaliatory actions.

Shortly before 12:30 pm, the Israeli Air Force attacked Jordan's two airbases. The Hawker Hunters were refueling at the time of the attack. The Israeli aircraft attacked in two waves, the first of which cratered the runways and knocked out the control towers, and the second wave destroyed all 21 of Jordan's Hawker Hunter fighters, along with six transport aircraft and two helicopters. One Israeli jet was shot down by ground fire.

Israeli aircraft also attacked H-3, an Iraqi Air Force base in western Iraq. During the attack, 12 MiG-21s, 2 MiG-17s, 5 Hunter F6s, and 3 Il-28 bombers were destroyed or shot down. A Pakistani pilot stationed at the base shot down an Israeli fighter and a bomber during the raid. The Jordanian radar facility at Ajloun was destroyed in an Israeli airstrike. Israeli Fouga Magister jets attacked the Jordanian 40th Brigade with rockets as it moved south from the Damiya Bridge. Dozens of tanks were knocked out, and a convoy of 26 trucks carrying ammunition was destroyed. In Jerusalem, Israel responded to Jordanian shelling with a missile strike that devastated Jordanian positions. The Israelis used the L missile, a surface-to-surface missile developed jointly with France in secret.

A Jordanian battalion advanced up Government House ridge and dug in at the perimeter of Government House, the headquarters of the United Nations observers, and opened fire on Ramat Rachel, the Allenby Barracks and the Jewish section of Abu Tor with mortars and recoilless rifles. UN observers fiercely protested the incursion into the neutral zone, and several manhandled a Jordanian machine gun out of Government House after the crew had set it up in a second-floor window. After the Jordanians occupied Jabel Mukaber, an advance patrol was sent out and approached Ramat Rachel, where they came under fire from four civilians, including the wife of the director, who were armed with old Czech-made weapons.

The immediate Israeli response was an offensive to retake Government House and its ridge. The Jerusalem Brigade's Reserve Battalion 161, under Lieutenant-Colonel Asher Dreizin, was given the task. Dreizin had two infantry companies and eight tanks under his command, several of which broke down or became stuck in the mud at Ramat Rachel, leaving three for the assault. The Jordanians mounted fierce resistance, knocking out two tanks.

The Israelis broke through the compound's western gate and began clearing the building with grenades, before General Odd Bull, commander of the UN observers, compelled the Israelis to hold their fire, telling them that the Jordanians had already fled. The Israelis proceeded to take the Antenna Hill, directly behind Government House, and clear out a series of bunkers to the west and south. The fighting, often conducted hand-to-hand, continued for nearly four hours before the surviving Jordanians fell back to trenches held by the Hittin Brigade, which were steadily overwhelmed. By 6:30 pm, the Jordanians had retreated to Bethlehem, having suffered about 100 casualties. All but ten of Dreizin's soldiers were casualties, and Dreizin himself was wounded three times.

During the late afternoon of 5 June, the Israelis launched an offensive to encircle Jerusalem, which lasted into the following day. During the night, they were supported by intense tank, artillery and mortar fire to soften up Jordanian positions. Searchlights placed atop the Labor Federation building, then the tallest in Israeli Jerusalem, exposed and blinded the Jordanians. The Jerusalem Brigade moved south of Jerusalem, while the mechanized Harel Brigade and 55th Paratroopers Brigade under Mordechai Gur encircled it from the north.

A combined force of tanks and paratroopers crossed no-man's land near the Mandelbaum Gate. One of Gur's paratroop battalions approached the fortified Police Academy. The Israelis used bangalore torpedoes to blast their way through barbed wire leading up to the position while exposed and under heavy fire. With the aid of two tanks borrowed from the Jerusalem Brigade, they captured the Police Academy. After receiving reinforcements, they moved up to attack Ammunition Hill.

The Jordanian defenders, who were heavily dug-in, fiercely resisted the attack. All of the Israeli officers except for two company commanders were killed, and the fighting was mostly led by individual soldiers. The fighting was conducted at close quarters in trenches and bunkers, and was often hand-to-hand. The Israelis captured the position after four hours of heavy fighting. During the battle, 36 Israeli and 71 Jordanian soldiers were killed.

The battalion subsequently drove east, and linked up with the Israeli enclave on Mount Scopus and its Hebrew University campus. Gur's other battalions captured the other Jordanian positions around the American Colony, despite being short on men and equipment and having come under a Jordanian mortar bombardment while waiting for the signal to advance.

At the same time, the mechanized Harel Brigade attacked the fortress at Latrun, which the Jordanians had abandoned due to heavy Israeli tank fire. The brigade attacked Har Adar, but seven tanks were knocked out by mines, forcing the infantry to mount an assault without armoured cover. The Israeli soldiers advanced under heavy fire, jumping between stones to avoid mines. The fighting was conducted at close-quarters, often with knives and bayonets.

The Jordanians fell back after a battle that left two Israeli and eight Jordanian soldiers dead, and Israeli forces advanced through Beit Horon towards Ramallah, taking four fortified villages along the way. By the evening, the brigade arrived in Ramallah. Meanwhile, the 163rd Infantry Battalion secured Abu Tor following a fierce battle, severing the Old City from Bethlehem and Hebron.

Meanwhile, 600 Egyptian commandos stationed in the West Bank moved to attack Israeli airfields. Led by Jordanian intelligence scouts, they crossed the border and began infiltrating through Israeli settlements towards Ramla and Hatzor. They were soon detected and sought shelter in nearby fields, which the Israelis set on fire. Some 450 commandos were killed, and the remainder escaped to Jordan.

From the American Colony, the paratroopers moved towards the Old City. Their plan was to approach it via the lightly defended Salah al-Din Street. However, they made a wrong turn onto the heavily defended Nablus Road. The Israelis ran into fierce resistance. Their tanks fired at point-blank range down the street, while the paratroopers mounted repeated charges. Despite repelling repeated Israeli charges, the Jordanians gradually gave way to Israeli firepower and momentum. The Israelis suffered some 30 casualties – half the original force – while the Jordanians lost 45 dead and 142 wounded.

Meanwhile, the Israeli 71st Battalion breached barbed wire and minefields and emerged near Wadi Joz, near the base of Mount Scopus, from where the Old City could be cut off from Jericho and East Jerusalem from Ramallah. Israeli artillery targeted the one remaining route from Jerusalem to the West Bank, and shellfire deterred the Jordanians from counterattacking from their positions at Augusta-Victoria. An Israeli detachment then captured the Rockefeller Museum after a brief skirmish.

Afterwards, the Israelis broke through to the Jerusalem-Ramallah road. At Tel al-Ful, the Israelis fought a running battle with up to thirty Jordanian tanks. The Jordanians stalled the advance and destroyed a number of half-tracks, but the Israelis launched air attacks and exploited the vulnerability of the external fuel tanks mounted on the Jordanian tanks. The Jordanians lost half their tanks, and retreated towards Jericho. Joining up with the 4th Brigade, the Israelis then descended through Shuafat and the site of what is now French Hill, through Jordanian defenses at Mivtar, emerging at Ammunition Hill.
With Jordanian defenses in Jerusalem crumbling, elements of the Jordanian 60th Brigade and an infantry battalion were sent from Jericho to reinforce Jerusalem. Its original orders were to repel the Israelis from the Latrun corridor, but due to the worsening situation in Jerusalem, the brigade was ordered to proceed to Jerusalem's Arab suburbs and attack Mount Scopus. Parallel to the brigade were infantrymen from the Imam Ali Brigade, who were approaching Issawiya. The brigades were spotted by Israeli aircraft and decimated by rocket and cannon fire. Other Jordanian attempts to reinforce Jerusalem were beaten back, either by armoured ambushes or airstrikes.

Fearing damage to holy sites and the prospect of having to fight in built-up areas, Dayan ordered his troops not to enter the Old City. He also feared that Israel would be subjected to a fierce international backlash and the outrage of Christians worldwide if it forced its way into the Old City. Privately, he told David Ben-Gurion that he was also concerned over the prospect of Israel capturing Jerusalem's holy sites, only to be forced to give them up under the threat of international sanctions.

Israel was to gain almost total control of the West Bank by the evening of 7 June, and began its military occupation of the West Bank on that day, issuing a military order, the "Proclamation Regarding Law and Administration (The West Bank Area) (No. 2)—1967", which established the military government in the West Bank and granted the commander of the area full legislative, executive, and judicial power. Jordan had realised that it had no hope of defence as early as the morning of 6 June, just a day after the conflict had begun. At Nasser's request, Egypt's Abdul Munim Riad sent a situation update at midday on 6 June:
The situation on the West Bank is rapidly deteriorating. A concentrated attack has been launched on all axes, together with heavy fire, day and night. Jordanian, Syrian and Iraqi air forces in position H3 have been virtually destroyed. Upon consultation with King Hussein I have been asked to convey to you the following choices:
King Hussein has asked me to refer this matter to you for an immediate reply."

An Egyptian order for Jordanian forces to withdraw across the Jordan River was issued at 10am on June 6; however that afternoon King Hussein learned of the impending United Nations Security Council Resolution 233 and decided instead to hold out in the hope that a ceasefire would be implemented soon. It was already too late, as the counter-order caused confusion and in many cases it was not possible to regain positions which had previously been left.

On 7 June, Dayan had ordered his troops not to enter the Old City; however, upon hearing that the UN was about to declare a ceasefire, he changed his mind, and without cabinet clearance, decided to capture it. Two paratroop battalions attacked Augusta-Victoria Hill, high ground overlooking the Old City from the east. One battalion attacked from Mount Scopus, and another attacked from the valley between it and the Old City. Another paratroop battalion, personally led by Gur, broke into the Old City, and was joined by the other two battalions after their missions were complete. The paratroopers met little resistance. The fighting was conducted solely by the paratroopers; the Israelis did not use armour during the battle out of fear of severe damage to the Old City.

In the north, one battalion from Peled's division was sent to check Jordanian defenses in the Jordan Valley. A brigade belonging to Peled's division captured the western part of the West Bank. One brigade attacked Jordanian artillery positions around Jenin, which were shelling Ramat David Airbase. The Jordanian 12th Armored Battalion, which outnumbered the Israelis, held off repeated attempts to capture Jenin. However, Israeli air attacks took their toll, and the Jordanian M48 Pattons, with their external fuel tanks, proved vulnerable at short distances, even to the Israeli-modified Shermans. Twelve Jordanian tanks were destroyed, and only six remained operational.
Just after dusk, Israeli reinforcements arrived. The Jordanians continued to fiercely resist, and the Israelis were unable to advance without artillery and air support. One Israeli jet attacked the Jordanian commander's tank, wounding him and killing his radio operator and intelligence officer. The surviving Jordanian forces then withdrew to Jenin, where they were reinforced by the 25th Infantry Brigade. The Jordanians were effectively surrounded in Jenin.

Jordanian infantry and their three remaining tanks managed to hold off the Israelis until 4:00 am, when three battalions arrived to reinforce them in the afternoon. The Jordanian tanks charged, and knocked out multiple Israeli vehicles, and the tide began to shift. After sunrise, Israeli jets and artillery conducted a two-hour bombardment against the Jordanians. The Jordanians lost 10 dead and 250 wounded, and had only seven tanks left, including two without gas, and sixteen APCs. The Israelis then fought their way into Jenin, and captured the city after fierce fighting.

After the Old City fell, the Jerusalem Brigade reinforced the paratroopers, and continued to the south, capturing Judea and Gush Etzion. Hebron was taken without any resistance. Fearful that Israeli soldiers would exact retribution for the 1929 massacre of the city's Jewish community, Hebron's residents flew white sheets from their windows and rooftops, and voluntarily gave up their weapons. The Harel Brigade proceeded eastward, descending to the Jordan River.

On 7 June, Israeli forces seized Bethlehem, taking the city after a brief battle that left some 40 Jordanian soldiers dead, with the remainder fleeing. On the same day, one of Peled's brigades seized Nablus; then it joined one of Central Command's armoured brigades to fight the Jordanian forces; as the Jordanians held the advantage of superior equipment and were equal in numbers to the Israelis.

Again, the air superiority of the IAF proved paramount as it immobilized the Jordanians, leading to their defeat. One of Peled's brigades joined with its Central Command counterparts coming from Ramallah, and the remaining two blocked the Jordan river crossings together with the Central Command's 10th. Engineering Corps sappers blew up the Abdullah and Hussein bridges with captured Jordanian mortar shells, while elements of the Harel Brigade crossed the river and occupied positions along the east bank to cover them, but quickly pulled back due to American pressure. The Jordanians, anticipating an Israeli offensive deep into Jordan, assembled the remnants of their army and Iraqi units in Jordan to protect the western approaches to Amman and the southern slopes of the Golan Heights.

As Israel continued its offensive on 7 June, taking no account of the UN ceasefire resolution, the Egyptian-Jordanian command ordered a full Jordanian withdrawal for the second time, in order to avoid an annihilation of the Jordanian army. This was complete by nightfall on 7 June.

After the Old City was captured, Dayan told his troops to "dig in" to hold it. When an armoured brigade commander entered the West Bank on his own initiative, and stated that he could see Jericho, Dayan ordered him back. It was only after intelligence reports indicated that Hussein had withdrawn his forces across the Jordan River that Dayan ordered his troops to capture the West Bank. According to Narkis:

First, the Israeli government had no intention of capturing the West Bank. On the contrary, it was opposed to it. Second, there was not any provocation on the part of the IDF. Third, the rein was only loosened when a real threat to Jerusalem's security emerged. This is truly how things happened on June 5, although it is difficult to believe. The end result was something that no one had planned.

In May–June 1967, in preparation for conflict, the Israeli government planned to confine the confrontation to the Egyptian front, whilst taking into account the possibility of some fighting on the Syrian front.

Syria largely stayed out of the conflict for the first four days.

False Egyptian reports of a crushing victory against the Israeli army and forecasts that Egyptian forces would soon be attacking Tel Aviv influenced Syria's decision to enter the war – in a sporadic manner – during this period. Syrian artillery began shelling northern Israel, and twelve Syrian jets attacked Israeli settlements in the Galilee. Israeli fighter jets intercepted the Syrian aircraft, shooting down three and driving off the rest. In addition, two Lebanese Hawker Hunter jets, two of the twelve Lebanon had, crossed into Israeli airspace and began strafing Israeli positions in the Galilee. They were intercepted by Israeli fighter jets, and one was shot down.

On the evening of 5 June, the Israeli Air Force attacked Syrian airfields. The Syrian Air Force lost some 32 MiG 21s, 23 MiG-15 and MiG-17 fighters, and two Ilyushin Il-28 bombers, two-thirds of its fighting strength. The Syrian aircraft that survived the attack retreated to distant bases and played no further role in the war. Following the attack, Syria realised that the news it had received from Egypt of the near-total destruction of the Israeli military could not have been true.
On June 6, a minor Syrian force tried to capture the water plants at Tel Dan (the subject of a fierce escalation two years earlier), Dan, and She'ar Yashuv. These attacks were repulsed with the loss of twenty soldiers and seven tanks. An Israeli officer was also killed. But a broader Syrian offensive quickly failed. Syrian reserve units were broken up by Israeli air attacks, and several tanks were reported to have sunk in the Jordan River.

Other problems included tanks being too wide for bridges, lack of radio communications between tanks and infantry, and units ignoring orders to advance. A post-war Syrian army report concluded:

Our forces did not go on the offensive either because they did not arrive or were not wholly prepared or because they could not find shelter from the enemy's planes. The reserves could not withstand the air attacks; they dispersed after their morale plummeted.

The Syrians bombarded Israeli civilian settlements in the Galilee Panhandle, by two battalions of M-46 130mm guns, four companies of heavy mortars, and dug-in Panzer IV tanks. The Syrian bombardment killed two civilians, hit 205 houses as well as farming installations. An inaccurate report from a Syrian officer, however, said that as a result of the bombardment that "the enemy appears to have suffered heavy losses and is retreating".

On 7 and 8 June, the Israeli leadership debated about whether to attack the Golan Heights as well. Syria had supported pre-war raids that had helped raise tensions and had routinely shelled Israel from the Heights, so some Israeli leaders wanted to see Syria punished. Military opinion was that the attack would be extremely costly, since it would entail an uphill battle against a strongly fortified enemy. The western side of the Golan Heights consists of a rock escarpment that rises 500 meters (1,700 ft) from the Sea of Galilee and the Jordan River, and then flattens to a gently sloping plateau. Dayan opposed the operation bitterly at first, believing such an undertaking would result in losses of 30,000 and might trigger Soviet intervention. Prime Minister Eshkol, on the other hand, was more open to the possibility, as was the head of the Northern Command, David Elazar, whose unbridled enthusiasm for and confidence in the operation may have eroded Dayan's reluctance.

Eventually, the situation on the Southern and Central fronts cleared up, intelligence estimated that the likelihood of Soviet intervention had been reduced, reconnaissance showed some Syrian defenses in the Golan region collapsing, and an intercepted cable revealed that Nasser was urging the President of Syria to immediately accept a cease-fire. At 3 am on 9 June, Syria announced its acceptance of the cease-fire. Despite this announcement, Dayan became more enthusiastic about the idea and four hours later at 7 am, "gave the order to go into action against Syria" without consultation or government authorisation.

The Syrian army consisted of about 75,000 men grouped in nine brigades, supported by an adequate amount of artillery and armour. Israeli forces used in combat consisted of two brigades (the 8th Armored Brigade and the Golani Brigade) in the northern part of the front at Givat HaEm, and another two (infantry and one of Peled's brigades summoned from Jenin) in the center. The Golan Heights' unique terrain (mountainous slopes crossed by parallel streams every several kilometers running east to west), and the general lack of roads in the area channeled both forces along east-west axes of movement and restricted the ability of units to support those on either flank. Thus the Syrians could move north-south on the plateau itself, and the Israelis could move north-south at the base of the Golan escarpment. An advantage Israel possessed was the excellent intelligence collected by Mossad operative Eli Cohen (who was captured and executed in Syria in 1965) regarding the Syrian battle positions. Syria had built extensive defensive fortifications in depths up to 15 kilometers, comparable to the Maginot Line.

As opposed to all the other campaigns, IAF was only partially effective in the Golan because the fixed fortifications were so effective. However, the Syrian forces proved unable to put up effective defense largely because the officers were poor leaders and treated their soldiers badly; often officers would retreat from danger, leaving their men confused and ineffective. The Israelis also had the upper hand during close combat that took place in the numerous Syrian bunkers along the Golan Heights, as they were armed with the Uzi, a submachine gun designed for close combat, while Syrian soldiers were armed with the heavier AK-47 assault rifle, designed for combat in more open areas.

On the morning of 9 June, Israeli jets began carrying out dozens of sorties against Syrian positions from Mount Hermon to Tawfiq, using rockets salvaged from captured Egyptian stocks. The airstrikes knocked out artillery batteries and storehouses and forced transport columns off the roads. The Syrians suffered heavy casualties and a drop in morale, with a number of senior officers and troops deserting. The attacks also provided time as Israeli forces cleared paths through Syrian minefields. However, the airstrikes did not seriously damage the Syrians' bunkers and trench systems, and the bulk of Syrian forces on the Golan remained in their positions.

About two hours after the airstrikes began, the 8th Armored Brigade, led by Colonel Albert Mandler, advanced into the Golan Heights from Givat HaEm. Its advance was spearheaded by Engineering Corps sappers and eight bulldozers, which cleared away barbed wire and mines. As they advanced, the force came under fire, and five bulldozers were immediately hit. The Israeli tanks, with their maneuverability sharply reduced by the terrain, advanced slowly under fire toward the fortified village of Sir al-Dib, with their ultimate objective being the fortress at Qala. Israeli casualties steadily mounted. Part of the attacking force lost its way and emerged opposite Za'ura, a redoubt manned by Syrian reservists. With the situation critical, Colonel Mandler ordered simultaneous assaults on Za'ura and Qala. Heavy and confused fighting followed, with Israeli and Syrian tanks struggling around obstacles and firing at extremely short ranges. Mandler recalled that "the Syrians fought well and bloodied us. We beat them only by crushing them under our treads and by blasting them with our cannons at very short range, from 100 to 500 meters." The first three Israeli tanks to enter Qala were stopped by a Syrian bazooka team, and a relief column of seven Syrian tanks arrived to repel the attackers. The Israelis took heavy fire from the houses, but could not turn back, as other forces were advancing behind them, and they were on a narrow path with mines on either side. The Israelis continued pressing forward, and called for air support. A pair of Israeli jets destroyed two of the Syrian tanks, and the remainder withdrew. The surviving defenders of Qala retreated after their commander was killed. Meanwhile, Za'ura fell in an Israeli assault, and the Israelis also captured the 'Ein Fit fortress.

In the central sector, the Israeli 181st Battalion captured the strongholds of Dardara and Tel Hillal after fierce fighting. Desperate fighting also broke out along the operation's northern axis, where Golani Brigade attacked thirteen Syrian positions, including the formidable Tel Fakhr position. Navigational errors placed the Israelis directly under the Syrians' guns. In the fighting that followed, both sides took heavy casualties, with the Israelis losing all nineteen of their tanks and half-tracks. The Israeli battalion commander then ordered his twenty-five remaining men to dismount, divide into two groups, and charge the northern and southern flanks of Tel Fakhr. The first Israelis to reach the perimeter of the southern approach laid bodily down on the barbed wire, allowing their comrades to vault over them. From there, they assaulted the fortified Syrian positions. The fighting was waged at extremely close quarters, often hand-to-hand.

On the northern flank, the Israelis broke through within minutes and cleared out the trenches and bunkers. During the seven-hour battle, the Israelis lost 31 dead and 82 wounded, while the Syrians lost 62 dead and 20 captured. Among the dead was the Israeli battalion commander. The Golani Brigade's 51st Battalion took Tel 'Azzaziat, and Darbashiya also fell to Israeli forces.
By the evening of 9 June, the four Israeli brigades had all broken through to the plateau, where they could be reinforced and replaced. Thousands of reinforcements began reaching the front, those tanks and half-tracks that had survived the previous day's fighting were refueled and replenished with ammunition, and the wounded were evacuated. By dawn, the Israelis had eight brigades in the sector.

Syria's first line of defense had been shattered, but the defenses beyond that remained largely intact. Mount Hermon and the Banias in the north, and the entire sector between Tawfiq and Customs House Road in the south remained in Syrian hands. In a meeting early on the night of 9 June, Syrian leaders decided to reinforce those positions as quickly as possible, and to maintain a steady barrage on Israeli civilian settlements.

Throughout the night, the Israelis continued their advance. Though it was slowed by fierce resistance, an anticipated Syrian counterattack never materialized. At the fortified village of Jalabina, a garrison of Syrian reservists, leveling their anti-aircraft guns, held off the Israeli 65th Paratroop Battalion for four hours before a small detachment managed to penetrate the village and knock out the heavy guns.

Meanwhile, the 8th Brigade's tanks moved south from Qala, advancing six miles to Wasit under heavy artillery and tank bombardment. At the Banias in the north, Syrian mortar batteries opened fire on advancing Israeli forces only after Golani Brigade sappers cleared a path through a minefield, killing sixteen Israeli soldiers and wounding four.

On the next day, 10 June, the central and northern groups joined in a pincer movement on the plateau, but that fell mainly on empty territory as the Syrian forces retreated. At 8:30 am, the Syrians began blowing up their own bunkers, burning documents and retreating. Several units joined by Elad Peled's troops climbed to the Golan from the south, only to find the positions mostly empty. When the 8th Brigade reached Mansura, five miles from Wasit, the Israelis met no opposition and found abandoned equipment, including tanks, in perfect working condition. In the fortified Banias village, Golani Brigade troops found only several Syrian soldiers chained to their positions.

During the day, the Israeli units stopped after obtaining manoeuvre room between their positions and a line of volcanic hills to the west. In some locations, Israeli troops advanced after an agreed-upon cease-fire to occupy strategically strong positions. To the east, the ground terrain is an open gently sloping plain. This position later became the cease-fire line known as the "Purple Line".

"Time" magazine reported: "In an effort to pressure the United Nations into enforcing a ceasefire, Damascus Radio undercut its own army by broadcasting the fall of the city of Quneitra three hours before it actually capitulated. That premature report of the surrender of their headquarters destroyed the morale of the Syrian troops left in the Golan area."

By 10 June, Israel had completed its final offensive in the Golan Heights, and a ceasefire was signed the day after. Israel had seized the Gaza Strip, the Sinai Peninsula, the West Bank of the Jordan River (including East Jerusalem), and the Golan Heights. About one million Arabs were placed under Israel's direct control in the newly captured territories. Israel's strategic depth grew to at least 300 kilometers in the south, 60 kilometers in the east, and 20 kilometers of extremely rugged terrain in the north, a security asset that would prove useful in the Yom Kippur War six years later.

Speaking three weeks after the war ended, as he accepted an honorary degree from Hebrew University, Yitzhak Rabin gave his reasoning behind the success of Israel:

In recognition of contributions, Rabin was given the honour of naming the war for the Israelis. From the suggestions proposed, including the "War of Daring", "War of Salvation", and "War of the Sons of Light", he "chose the least ostentatious, the Six-Day War, evoking the days of creation".

Dayan's final report on the war to the Israeli general staff listed several shortcomings in Israel's actions, including misinterpretation of Nasser's intentions, overdependence on the United States, and reluctance to act when Egypt closed the Straits. He also credited several factors for Israel's success: Egypt did not appreciate the advantage of striking first and their adversaries did not accurately gauge Israel's strength and its willingness to use it.

In Egypt, according to Heikal, Nasser had admitted his responsibility for the military defeat in June 1967. According to historian Abd al-Azim Ramadan, Nasser's mistaken decisions to expel the international peacekeeping force from the Sinai Peninsula and close the Straits of Tiran in 1967 led to a state of war with Israel, despite Egypt's lack of military preparedness.

After the 1973 Yom Kippur War, Egypt reviewed the causes of its loss of the 1967 war. Issues that were identified included "the individualistic bureaucratic leadership"; "promotions on the basis of loyalty, not expertise, and the army's fear of telling Nasser the truth"; lack of intelligence; and better Israeli weapons, command, organization, and will to fight.

Between 776 and 983 Israelis were killed and 4,517 were wounded. Fifteen Israeli soldiers were captured. Arab casualties were far greater. Between 9,800 and 15,000 Egyptian soldiers were listed as killed or missing in action. An additional 4,338 Egyptian soldiers were captured. Jordanian losses are estimated to be 700 killed in action with another 2,500 wounded. The Syrians were estimated to have sustained between 1,000 and 2,500 killed in action. Between 367 and 591 Syrians were captured.

At the commencement of hostilities, both Egypt and Israel announced that they had been attacked by the other country. The Israeli government later abandoned its initial position, acknowledging Israel had struck first, claiming that it was a preemptive strike in the face of a planned invasion by Egypt. On the other hand, the Arab view was that it was unjustified to attack Egypt. Many commentators consider the war as the classic case of anticipatory attack in self-defense.

It has been alleged that Nasser did not want Egypt to learn of the true extent of his defeat and so ordered the killing of Egyptian army stragglers making their way back to the Suez canal zone. There have also been allegations from both Israeli and Egyptian sources that Israeli troops killed unarmed Egyptian prisoners.

There have been a number of allegations of direct military support of Israel during the war by the US and the UK, including the supply of equipment (despite an embargo) and the participation of US forces in the conflict. Many of these allegations and conspiracy theories have been disputed and it has been claimed that some were given currency in the Arab world to explain the Arab defeat.
It has also been claimed that the Soviet Union, in support of its Arab allies, used its naval strength in the Mediterranean to act as a major restraint on the US Navy.

America features prominently in Arab conspiracy theories purporting to explain the June 1967 defeat. Mohamed Hassanein Heikal, a confidant of Nasser, claims that President Lyndon B. Johnson was obsessed with Nasser and that Johnson conspired with Israel to bring him down. The reported Israeli troop movements seemed all the more threatening because they were perceived in the context of a US conspiracy against Egypt. Salah Bassiouny of the Foreign ministry, claims that Foreign Ministry saw the reported Israeli troop movements as credible because Israel had reached the level at which it could find strategic alliance with the United States. During the war, Cairo announced that American and British planes were participating in the Israeli attack. Nasser broke off diplomatic relations following this allegation. Nasser's image of the United States was such that he might well have believed the worst. However Anwar Sadat implied that Nasser used this deliberate conspiracy in order to accuse the United States as a political cover-up for domestic consumption. Lutfi Abd al-Qadir, the director of Radio Cairo during the late 1960s, who accompanied Nasser to his visits in Moscow, had his conspiracy theory that both the Soviets and the Western powers wanted to topple Nasser or to reduce his influence.

On 8 June 1967, USS "Liberty", a United States Navy electronic intelligence vessel sailing off Arish (just outside Egypt's territorial waters), was attacked by Israeli jets and torpedo boats, nearly sinking the ship, killing 34 sailors and wounding 171. Israel said the attack was a case of mistaken identity, and that the ship had been misidentified as the Egyptian vessel "El Quseir". Israel apologized for the mistake, and paid compensation to the victims or their families, and to the United States for damage to the ship. After an investigation, the U.S. accepted the explanation that the incident was friendly fire and the issue was closed by the exchange of diplomatic notes in 1987. Others however, including the then United States Secretary of State Dean Rusk, Chief of Naval Operations at the time, Admiral Thomas Moorer, some survivors of the attack and intelligence officials familiar with transcripts of intercepted signals on the day, have rejected these conclusions as unsatisfactory and maintain that the attack was made in the knowledge that the ship was American.

The political importance of the 1967 War was immense. Israel demonstrated again that it was able and willing to initiate strategic strikes that could change the regional balance. Egypt and Syria learned tactical lessons and would launch an attack in 1973 in an attempt to reclaim their lost territory.

After following other Arab nations in declaring war, Mauritania remained in a declared state of war with Israel until about 1999. The United States imposed an embargo on new arms agreements to all Middle East countries, including Israel. The embargo remained in force until the end of the year, despite urgent Israeli requests to lift it.

Following the war, Israel experienced a wave of national euphoria, and the press praised the military's performance for weeks afterward. New "victory coins" were minted to celebrate. In addition, the world's interest in Israel grew, and the country's economy, which had been in crisis before the war, flourished due to an influx of tourists and donations, as well as the extraction of oil from the Sinai's wells. The aftermath of the war also saw a baby boom, which lasted for four years.

The aftermath of the war is also of religious significance. Under Jordanian rule, Jews were expelled from Jerusalem and were effectively barred from visiting the Western Wall, despite Article VIII of the 1949 Armistice Agreement demanded Israeli Jewish access to the Western Wall. Jewish holy sites were not maintained, and Jewish cemeteries had been desecrated. After the annexation to Israel, each religious group was granted administration over its holy sites. For the first time since 1948, Jews could visit the Old City of Jerusalem and pray at the Western Wall, the holiest site where Jews are permitted to pray, an event celebrated every year during Yom Yerushalayim. Despite the Temple Mount being the most important holy site in Jewish tradition, the al-Aqsa Mosque has been under sole administration of the Jordanian Muslim Waqf, and Jews are barred from praying on the Temple Mount, although they are allowed to visit it. In Hebron, Jews gained access to the Cave of the Patriarchs – the second most holy site in Judaism, after the Temple Mount – for the first time since the 14th century (previously Jews were allowed to pray only at the entrance). Other Jewish holy sites, such as Rachel's Tomb in Bethlehem and Joseph's Tomb in Nablus, also became accessible.

The war inspired the Jewish diaspora, which was swept up in overwhelming support for Israel. According to Michael Oren, the war enabled American Jews to "walk with their backs straight and flex their political muscle as never before. American Jewish organizations which had previously kept Israel at arms length suddenly proclaimed their Zionism." Thousands of Jewish immigrants arrived from Western countries such as the United States, United Kingdom, Canada, France, and South Africa after the war. Many of them returned to their countries of origin after a few years; one survey found that 58% of American Jews who immigrated to Israel between 1961 and 1972 returned to the US. Nevertheless, this immigration to Israel of Jews from Western countries, which was previously only a trickle, was a significant force for the first time. Most notably, the war stirred Zionist passions among Jews in the Soviet Union, who had by that time been forcibly assimilated. Many Soviet Jews subsequently applied for exit visas and began protesting for their right to immigrate to Israel. Following diplomatic pressure from the West, the Soviet government began granting exit visas to Jews in growing numbers. From 1970 to 1988, some 291,000 Soviet Jews were granted exit visas, of whom 165,000 immigrated to Israel and 126,000 immigrated to the United States. The great rise in Jewish pride in the wake of Israel's victory also fueled the beginnings of the baal teshuva movement. The war gave impetus to a Chabad campaign in which the Lubavitcher Rebbe directed his followers to put tefillin on Jewish men around world.

In the Arab nations, populations of minority Jews faced persecution and expulsion following the Israeli victory. According to historian and ambassador Michael B. Oren:
Mobs attacked Jewish neighborhoods in Egypt, Yemen, Lebanon, Tunisia, and Morocco, burning synagogues and assaulting residents. A pogrom in Tripoli, Libya, left 18 Jews dead and 25 injured; the survivors were herded into detention centers. Of Egypt's 4,000 Jews, 800 were arrested, including the chief rabbis of both Cairo and Alexandria, and their property sequestered by the government. The ancient communities of Damascus and Baghdad were placed under house arrest, their leaders imprisoned and fined. A total of 7,000 Jews were expelled, many with merely a satchel.

Following the war, a series of antisemitic purges began in Communist countries. Some 11,200 Jews from Poland immigrated to Israel during the 1968 Polish political crisis and the following year.

Following the war, Egypt initiated clashes along the Suez Canal in what became known as the War of Attrition.

Following the war, Israel made an offer for peace that included the return of most of the recently captured territories. According to Chaim Herzog:

The 19 June Israeli cabinet decision did not include the Gaza Strip, and left open the possibility of Israel permanently acquiring parts of the West Bank. On 25–27 June, Israel incorporated East Jerusalem together with areas of the West Bank to the north and south into Jerusalem's new municipal boundaries.

The Israeli decision was to be conveyed to the Arab nations by the United States. The U.S. was informed of the decision, but not that it was to transmit it. There is no evidence of receipt from Egypt or Syria, and some historians claim that they may never have received the offer.

In September, the Khartoum Arab Summit resolved that there would be "no peace, no recognition and no negotiation with Israel". However, as Avraham Sela notes, the Khartoum conference effectively marked a shift in the perception of the conflict by the Arab states away from one centered on the question of Israel's legitimacy, toward one focusing on territories and boundaries. This was shown on 22 November when Egypt and Jordan accepted United Nations Security Council Resolution 242. Nasser forestalled any movement toward direct negotiations with Israel. In dozens of speeches and statements, Nasser posited the equation that any direct peace talks with Israel were tantamount to surrender.

After the war, the entire Soviet bloc of Eastern Europe (with the exception of Romania) broke off diplomatic relations with Israel.

The 1967 War laid the foundation for future discord in the region, as the Arab states resented Israel's victory and did not want to give up territory.

On 22 November 1967, the United Nations Security Council adopted Resolution 242, the "land for peace" formula, which called for Israeli withdrawal "from territories occupied" in 1967 and "the termination of all claims or states of belligerency". Resolution 242 recognized the right of "every state in the area to live in peace within secure and recognized boundaries free from threats or acts of force." Israel returned the Sinai to Egypt in 1978, after the Camp David Accords. In the summer of 2005, Israel withdrew all military forces and evacuated all civilians from the Gaza Strip. Its army frequently re-enters Gaza for military operations and still retains control of the seaports, airports and most of the border crossings.

There was extensive displacement of populations in the occupied territories: of about one million Palestinians in the West Bank and Gaza, 300,000 (according to the United States Department of State) either fled, or were displaced from their homes, to Jordan, where they contributed to the growing unrest. The other 700,000 remained. In the Golan Heights, an estimated 80,000 Syrians fled. Israel allowed only the inhabitants of East Jerusalem and the Golan Heights to receive full Israeli citizenship, applying its law, administration and jurisdiction to these territories in 1967 and 1981, respectively. The vast majority of the populations in both territories declined to take citizenship. See also Israeli–Palestinian conflict and Golan Heights.

In his book "Righteous Victims" (1999), Israeli "New Historian" Benny Morris writes:
In addition, between 80,000 and 110,000 Syrians fled the Golan Heights, of which about 20,000 were from the city of Quneitra. According to more recent research by the Israeli daily "Haaretz", a total of 130,000 Syrian inhabitants fled or were expelled from the territory, most of them pushed out by the Israeli army.

Israel made peace with Egypt following the Camp David Accords of 1978 and completed a staged withdrawal from the Sinai in 1982. However, the position of the other occupied territories has been a long-standing and bitter cause of conflict for decades between Israel and the Palestinians, and the Arab world in general. Jordan and Egypt eventually withdrew their claims to sovereignty over the West Bank and Gaza, respectively. Israel and Jordan signed a peace treaty in 1994.

After the Israeli occupation of these territories, the Gush Emunim movement launched a large settlement effort in these areas to secure a permanent foothold. There are now hundreds of thousands of Israeli settlers in the West Bank. They are a matter of controversy within Israel, both among the general population and within different political administrations, supporting them to varying degrees. Palestinians consider them a provocation. The Israeli settlements in Gaza were evacuated in August 2005 as a part of Israel's disengagement from Gaza.


1. 

3.
4. Lenczowski 1990, pp. 105–15, Citing Moshe Dayan, "Story of My Life", and Nadav Safran, "From War to War: The Arab–Israeli Confrontation, 1948–1967", p. 375 Israel clearly did not want the US government to know too much about its dispositions for attacking Syria, initially planned for June 8, but postponed for 24 hours. It should be pointed out that the attack on the Liberty occurred on June 8, whereas on June 9 at 3 am, Syria announced its acceptance of the cease-fire. Despite this, at 7 am, that is, four hours later, Israel's minister of defense, Moshe Dayan, "gave the order to go into action against Syria.




</doc>
<doc id="29329" url="https://en.wikipedia.org/wiki?curid=29329" title="Spectrum">
Spectrum

A spectrum (plural "spectra" or "spectrums") is a condition that is not limited to a specific set of values but can vary, without steps, across a continuum. The word was first used scientifically in optics to describe the rainbow of colors in visible light after passing through a prism. As scientific understanding of light advanced, it came to apply to the entire electromagnetic spectrum.

Spectrum has since been applied by analogy to topics outside optics. Thus, one might talk about the "spectrum of political opinion", or the "spectrum of activity" of a drug, or the "autism spectrum". In these uses, values within a spectrum may not be associated with precisely quantifiable numbers or definitions. Such uses imply a broad range of conditions or behaviors grouped together and studied under a single title for ease of discussion. Nonscientific uses of the term "spectrum" are sometimes misleading. For instance, a single left–right spectrum of political opinion does not capture the full range of people's political beliefs. Political scientists use a variety of biaxial and multiaxial systems to more accurately characterize political opinion.

In most modern usages of "spectrum" there is a unifying theme between the extremes at either end. This was not always true in older usage.

In Latin, "spectrum" means "image" or "apparition", including the meaning "spectre". Spectral evidence is testimony about what was done by spectres of persons not present physically, or hearsay evidence about what ghosts or apparitions of Satan said. It was used to convict a number of persons of witchcraft at Salem, Massachusetts in the late 17th century. The word "spectrum" [Spektrum] was strictly used to designate a ghostly optical afterimage by Goethe in his "Theory of Colors" and Schopenhauer in "On Vision and Colors".

The prefix "spectro-" is used to form words relating to spectra. For example, a spectrometer is a device used to record spectra and spectroscopy is the use of a spectrometer for chemical analysis.

In the 17th century, the word "spectrum" was introduced into optics by Isaac Newton, referring to the range of colors observed when white light was dispersed through a prism. Soon the term referred to a plot of light intensity or power as a function of frequency or wavelength, also known as a "spectral density plot".

The term "spectrum" was expanded to apply to other waves, such as sound waves that could also be measured as a function of frequency, frequency spectrum and power spectrum of a signal. The term now applies to any signal that can be measured or decomposed along a continuous variable such as energy in electron spectroscopy or mass-to-charge ratio in mass spectrometry. Spectrum is also used to refer to a graphical representation of the signal as a function of the dependent variable.

Electromagnetic spectrum refers to the full range of all frequencies of electromagnetic radiation and also to the characteristic distribution of electromagnetic radiation emitted or absorbed by that particular object. Devices used to measure an electromagnetic spectrum are called spectrograph or spectrometer. The visible spectrum is the part of the electromagnetic spectrum that can be seen by the human eye. The wavelength of visible light ranges from 390 to 700 nm. The absorption spectrum of a chemical element or chemical compound is the spectrum of frequencies or wavelengths of incident radiation that are absorbed by the compound due to electron transitions from a lower to a higher energy state. The emission spectrum refers to the spectrum of radiation emitted by the compound due to electron transitions from a higher to a lower energy state.

Light from many different sources contains various colors, each with its own brightness or intensity. A rainbow, or prism, sends these component colors in different directions, making them individually visible at different angles. A graph of the intensity plotted against the frequency (showing the brightness of each color) is the frequency spectrum of the light. When all the visible frequencies are present equally, the perceived color of the light is white, and the spectrum is a flat line. Therefore, flat-line spectra in general are often referred to as "white", whether they represent light or another type of wave phenomenon (sound, for example, or vibration in a structure).

In radio and telecommunications, the frequency spectrum can be shared among many different broadcasters. The radio spectrum is the part of the electromagnetic spectrum corresponding to frequencies lower below 300 GHz, which corresponds to wavelengths longer than about 1 mm. The microwave spectrum corresponds to frequencies between 300 MHz (0.3 GHz) and 300 GHz and wavelengths between one meter and one millimeter. Each broadcast radio and TV station transmits a wave on an assigned frequency range, called a "channel". When many broadcasters are present, the radio spectrum consists of the sum of all the individual channels, each carrying separate information, spread across a wide frequency spectrum. Any particular radio receiver will detect a single function of amplitude (voltage) vs. time. The radio then uses a tuned circuit or tuner to select a single channel or frequency band and demodulate or decode the information from that broadcaster. If we made a graph of the strength of each channel vs. the frequency of the tuner, it would be the frequency spectrum of the antenna signal.

In astronomical spectroscopy, the strength, shape, and position of absorption and emission lines, as well as the overall spectral energy distribution of the continuum, reveal many properties of astronomical objects. Stellar classification is the categorisation of stars based on their characteristic electromagnetic spectra. The spectral flux density is used to represent the spectrum of a light-source, such as a star. 

In radiometry and colorimetry (or color science more generally), the spectral power distribution (SPD) of a light source is a measure of the power contributed by each frequency or color in a light source. The light spectrum is usually measured at points (often 31) along the visible spectrum, in wavelength space instead of frequency space, which makes it not strictly a spectral density. Some spectrophotometers can measure increments as fine as one to two nanometers. the values are used to calculate other specifications and then plotted to show the spectral attributes of the source. This can be helpful in analyzing the color characteristics of a particular source.

A plot of ion abundance as a function of mass-to-charge ratio is called a mass spectrum. It can be produced by a mass spectrometer instrument. The mass spectrum can be used to determine the quantity and mass of atoms and molecules. Tandem mass spectrometry is used to determine molecular structure.

In physics, the energy spectrum of a particle is the number of particles or intensity of a particle beam as a function of particle energy. Examples of techniques that produce an energy spectrum are alpha-particle spectroscopy, electron energy loss spectroscopy, and mass-analyzed ion-kinetic-energy spectrometry.

In physics, particularly in quantum mechanics, some differential operators have discrete spectra, with gaps between values. Common cases include the Hamiltonian and the angular momentum operator.

In acoustics, a spectrogram is a visual representation of the frequency spectrum of sound as a function of time or another variable.

A source of sound can have many different frequencies mixed. A Musical tone's timbre is characterized by its harmonic spectrum. Sound in our environment that we refer to as "noise" includes many different frequencies. When a sound signal contains a mixture of all audible frequencies, distributed equally over the audio spectrum, it is called white noise.

The spectrum analyzer is an instrument which can be used to convert the sound wave of the musical note into a visual display of the constituent frequencies. This visual display is referred to as an acoustic spectrogram. Software based audio spectrum analyzers are available at low cost, providing easy access not only to industry professionals, but also to academics, students and the hobbyist. The acoustic spectrogram generated by the spectrum analyzer provides an acoustic signature of the musical note. In addition to revealing the fundamental frequency and its overtones, the spectrogram is also useful for analysis of the temporal attack, decay, sustain, and release of the musical note.

Antibiotic spectrum of activity is a component of antibiotic classification. A broad-spectrum antibiotic is active against a wide range of bacteria, whereas a narrow-spectrum antibiotic is effective against specific families of bacteria. An example of a commonly used broad-spectrum antibiotic is ampicillin. An example of a narrow spectrum antibiotic is Dicloxacillin, which acts on beta-lactamase-producing Gram-positive bacteria such as "Staphylococcus aureus".

In psychiatry, the spectrum approach uses the term spectrum to describe a range of linked conditions, sometimes also extending to include singular symptoms and traits. For example, the autism spectrum describes a range of conditions classified as neurodevelopmental disorders.

In mathematics, the spectrum of a matrix is the multiset of the eigenvalues of the matrix.

In functional analysis, the concept of the spectrum of a bounded operator is a generalization of the eigenvalue concept for matrices.

In algebraic topology, a spectrum is an object representing a generalized cohomology theory.

In social science, economic spectrum is used to indicate the range of social class along some indicator of wealth or income. In political science, the term political spectrum refers to a system of classifying political positions in one or more dimensions, for example in a range including right wing and left wing.


</doc>
<doc id="29330" url="https://en.wikipedia.org/wiki?curid=29330" title="Social dynamics">
Social dynamics

Social dynamics (or sociodynamics) can refer to the behavior of groups that results from the interactions of individual group members as well to the study of the relationship between individual interactions and group level behaviors. The field of social dynamics brings together ideas from Economics, Sociology, Social Psychology, and other disciplines, and is a sub-field of complex adaptive systems or complexity science. The fundamental assumption of the field is that individuals are influenced by one another's behavior. The field is closely related to system dynamics. Like system dynamics, social dynamics is concerned with changes over time and emphasizes the role of feedbacks. However, in social dynamics individual choices and interactions are typically viewed as the source of aggregate level behavior, while system dynamics posits that the structure of feedbacks and accumulations are responsible for system level dynamics. Research in the field typically takes a behavioral approach, assuming that individuals are boundedly rational and act on local information. Mathematical and computational modeling are important tools for studying social dynamics. This field grew out of work done in the 1940s by game theorists such as Duncan & Luce, and even earlier works by mathematician Armand Borel. Because social dynamics focuses on individual level behavior, and recognizes the importance of heterogeneity Carlos Gilmore, individuals, strict analytic results are often impossible. Instead, approximation techniques, such as mean field approximations from statistical physics, or computer simulations are used to understand the behaviors of the system. In contrast to more traditional approaches in economics, scholars of social dynamics are often interested in non-equilibrium, or dynamic, behavior. That is, behavior that changes over time.


Available online: http://www.hindawi.com/GetArticle.aspx?doi=10.1155/S1026022697000101.




</doc>
<doc id="29333" url="https://en.wikipedia.org/wiki?curid=29333" title="Social evolution">
Social evolution

Social evolution is a subdiscipline of evolutionary biology that is concerned with social behaviors that have fitness consequences for individuals other than the actor. It is also a subdiscipline of sociology that studies evolution of social systems.

Social behaviors can be categorized according to the fitness consequences they entail for the actor and recipient.

This classification was proposed by W. D. Hamilton, arguing that natural selection favors mutually beneficial or selfish behaviors. Hamilton's insight was to show how kin selection could explain altruism and spite.

Social evolution is also often regarded (especially, in the field of social anthropology) as evolution of social systems and structures.

In 2010, Harvard biologist E. O. Wilson, a founder of modern sociobiology, proposed a new theory of social evolution. He argued that the traditional approach of focusing on eusociality had limitations, which he illustrated primarily with examples from the insect world.



</doc>
<doc id="29336" url="https://en.wikipedia.org/wiki?curid=29336" title="Systemic functional grammar">
Systemic functional grammar

Systemic functional grammar (SFG) is a form of grammatical description originated by Michael Halliday. It is part of a social semiotic approach to language called "systemic functional linguistics". In these two terms, "systemic" refers to the view of language as "a network of systems, or interrelated sets of options for making meaning"; "functional" refers to Halliday's view that language is as it is because of what it has evolved to do (see Metafunction). Thus, what he refers to as the "multidimensional architecture of language" "reflects the multidimensional nature of human experience and interpersonal relations."

Halliday describes his grammar as built on the work of Saussure, Louis Hjelmslev, Malinowski, J.R. Firth, and the Prague school linguists. In addition, he drew on the work of the American anthropological linguists Boas, Sapir and Whorf. His "main inspiration" was Firth, to whom he owes, among other things, the notion of language as system. Among American linguists, Whorf had "the most profound effect on my own thinking". Whorf "showed how it is that human beings do not all mean alike, and how their unconscious ways of meaning are among the most significant manifestations of their culture".

From his studies in China, he lists Luo Changpei and Wang Li as two scholars from whom he gained "new and exciting insights into language". He credits Luo for giving him a diachronic perspective and insights into a non-Indo-European language family. From Wang Li he learnt "many things, including research methods in dialectology, the semantic basis of grammar, and the history of linguistics in China".

Some interrelated key terms underpin Halliday's approach to grammar, which forms part of his account of how language works. These concepts are: system, (meta)function, and rank. Another key term is lexicogrammar. In this view, grammar and lexis are two ends of the same continuum.

Analysis of the grammar is taken from a trinocular perspective, meaning from three different levels. So to look at lexicogrammar, it can be analyzed from two more levels, 'above' (semantic) and 'below' (phonology). This grammar gives emphasis to the view from above.

For Halliday, grammar is described as systems not as rules, on the basis that every grammatical structure involves a choice from a describable set of options. Language is thus a "meaning potential". Grammarians in SF tradition use system networks to map the available options in a language. In relation to English, for instance, Halliday has described systems such as "mood", "agency", "theme", etc. Halliday describes grammatical systems as closed, i.e. as having a finite set of options. By contrast, lexical sets are open systems, since new words come into a language all the time.

These grammatical systems play a role in the construal of meanings of different kinds. This is the basis of Halliday's claim that language is "metafunctionally" organised. He argues that the raison d'être of language is meaning in social life, and for this reason all languages have three kinds of semantic components. All languages have resources for construing experience (the "ideational" component), resources for enacting humans' diverse and complex social relations (the "interpersonal" component), and resources for enabling these two kinds of meanings to come together in coherent text (the "textual" function). Each of the grammatical systems proposed by Halliday are related to these metafunctions. For instance, the grammatical system of 'mood' is considered to be centrally related to the expression of interpersonal meanings, 'process type' to the expression of experiential meanings, and 'theme' to the expression of textual meanings.

Traditionally the "choices" are viewed in terms of either the content or the structure of the language used. In SFG, language is analysed in three ways (strata): semantics, phonology, and lexicogrammar. SFG presents a view of language in terms of both structure (grammar) and words (lexis). The term "lexicogrammar" describes this combined approach.

From early on in his account of language, Halliday has argued that it is inherently functional. His early papers on the grammar of English make reference to the "functional components" of language, as "generalized uses of language, which, since they seem to determine the nature of the language system, require to be incorporated into our account of that system." Halliday argues that this functional organization of language "determines the form taken by grammatical structure".

Halliday refers to his functions of language as metafunctions. He proposes three general functions: the "ideational", the "interpersonal" and the "textual".

The ideational metafunction is the function for construing human experience. It is the means by which we make sense of "reality". Halliday divides the ideational into the logical and the experiential metafunctions. The logical metafunction refers to the grammatical resources for building up grammatical units into complexes, for instance, for combining two or more clauses into a clause complex. The experiential function refers to the grammatical resources involved in construing the flux of experience through the unit of the clause.

The ideational metafunction reflects the contextual value of "field", that is, the nature of the social process in which the language is implicated. An analysis of a text from the perspective of the ideational function involves inquiring into the choices in the grammatical system of "transitivity": that is, process types, participant types, circumstance types, combined with an analysis of the resources through which clauses are combined. Halliday's "An Introduction to Functional Grammar" (in the third edition, with revisions by Christian Matthiessen) sets out the description of these grammatical systems.

The interpersonal metafunction relates to a text's aspects of "tenor" or interactivity. Like field, tenor comprises three component areas: the speaker/writer persona, social distance, and relative social status. Social distance and relative social status are applicable only to spoken texts, although a case has been made that these two factors can also apply to written text.

The speaker/writer persona concerns the stance, personalisation and standing of the speaker or writer. This involves looking at whether the writer or speaker has a neutral attitude, which can be seen through the use of positive or negative language. Social distance means how close the speakers are, e.g. how the use of nicknames shows the degree to which they are intimate. Relative social status asks whether they are equal in terms of power and knowledge on a subject, for example, the relationship between a mother and child would be considered unequal. Focuses here are on speech acts (e.g. whether one person tends to ask questions and the other speaker tends to answer), who chooses the topic, turn management, and how capable both speakers are of evaluating the subject.

The textual metafunction relates to "mode"; the internal organisation and communicative nature of a text. This comprises textual interactivity, spontaneity and communicative distance.

Textual interactivity is examined with reference to disfluencies such as hesitators, pauses and repetitions.

Spontaneity is determined through a focus on lexical density, grammatical complexity, coordination (how clauses are linked together) and the use of nominal groups. The study of communicative distance involves looking at a text's cohesion—that is, how it hangs together, as well as any abstract language it uses.

Cohesion is analysed in the context of both lexical and grammatical as well as intonational aspects with reference to lexical chains and, in the speech register, tonality, tonicity, and tone. The lexical aspect focuses on sense relations and lexical repetitions, while the grammatical aspect looks at repetition of meaning shown through reference, substitution and ellipsis, as well as the role of linking adverbials.

Systemic functional grammar deals with all of these areas of meaning equally within the grammatical system itself.

Michael Halliday (1973) outlined seven functions of language with regard to the grammar used by children:

Halliday's theory sets out to explain how spoken and written texts construe meanings and how the resources of language are organised in open systems and functionally bound to meanings. It is a theory of language in use, creating systematic relations between choices and forms within the less abstract strata of grammar and phonology, on the one hand, and more abstract strata such as context of situation and context of culture on the other. It is a radically different theory of language from others which explore less abstract strata as autonomous systems, the most notable being Noam Chomsky's. Since the principal aim of systemic functional grammar is to represent the grammatical system as a resource for making meaning, it addresses different concerns. For example, it does not try to address Chomsky's thesis that there is a "finite rule system which generates all and only the grammatical sentences in a language". Halliday's theory encourages a more open approach to the definition of language as a resource; rather than focus on grammaticality as such, a systemic functional grammatical treatment focuses instead on the relative frequencies of choices made in uses of language and assumes that these relative frequencies reflect the probability that particular paths through the available resources will be chosen rather than others. Thus, SFG does not describe language as a finite rule system, but rather as a system, realised by instantiations, that is continuously expanded by the very instantiations that realise it and that is continuously reproduced and recreated with use.

Another way to understand the difference in concerns between systemic functional grammar and most variants of generative grammar is through Chomsky's claim that "linguistics is a sub-branch of psychology". Halliday investigates linguistics more as a sub-branch of "sociology". SFG therefore pays much more attention to pragmatics and discourse semantics than is traditionally the case in formalism.

The orientation of systemic functional grammar has served to encourage several further grammatical accounts that deal with some perceived weaknesses of the theory and similarly orient to issues not seen to be addressed in more structural accounts. Examples include the model of Richard Hudson called "word grammar".


Other significant systemic functional grammarians:

Linguists also involved with the early development of the approach:



</doc>
<doc id="29340" url="https://en.wikipedia.org/wiki?curid=29340" title="Starfleet">
Starfleet

Starfleet is a fictional organization in the "Star Trek" media franchise. Within this fictional universe, Starfleet is a uniformed space force maintained by the United Federation of Planets ("the Federation") as the principal means for conducting deep space exploration, research, defense, peacekeeping, and diplomacy, (although Starfleet predates the Federation, having originally been an Earth organization, as shown by the television series ""). While the majority of Starfleet's members are human and it is headquartered on Earth, hundreds of other species are also represented. The majority of the franchise's protagonists are Starfleet commissioned officers.

During production of early episodes of the , several details of the makeup of the "Star Trek" universe had yet to be worked out, including the operating authority for the USS "Enterprise". The terms "Star Service" (""), "Spacefleet Command" ("The Squire of Gothos"), "United Earth Space Probe Agency" ("Charlie X" and "Tomorrow Is Yesterday"), and "Space Central" ("") were all used to refer to the "Enterprise"s operating authority, before the term "Starfleet" became widespread from the episode "" onwards.

However, references to the United Earth Space Probe Agency, and its abbreviation UESPA, are to be found in episodes of later series. For example, the "Friendship One" probe (launched, on the fictional timeline, in 2067) is marked with the letters UESPA-1 in the "" episode "". Other background props included additional UESPA references, such as Captain Jean-Luc Picard's family album in "Star Trek Generations". During the production of "", some larger Starfleet insignia designs included the name "United Earth Space Probe Agency".

Many "" episodes refer to Starfleet having already been in operation in 2119, when it funded research begun by Cochrane and Henry Archer leading to the first successful flight of Warp 3 vessels in the 2140s. This research is said to have evolved into the NX Program, which led to Starfleet launching its first Warp 5-capable starship, "Enterprise" (NX-01), in 2151, followed by "Columbia" (NX-02), in 2155, as well as other vessels.

However, the Starfleet that is in existence before the Federation is a different organization (the "Earth Starfleet") than that of the Federation Starfleet.

Starfleet acts under a Prime Directive of non-interference with developing worlds or their internal politics. This is said not to be a Human construct, but stems from policies originally implemented by the Vulcans, who regarded an alien civilization's attainment of warp speed as the sign of their importance and reason for making first contact with them. The Prime Directive and Starfleet's first-contact policies are at the center of several episodes in each "Star Trek" series and the film "".

Starfleet Headquarters is shown to be located on Earth, northeast of the Golden Gate Bridge in the present-day Fort Baker area. Starfleet Academy is located in the same general area. Additionally, various episodes show Starfleet operating a series of starbases throughout Federation territory, as ground facilities, or as space stations in planetary orbit or in deep space.

Starfleet has been shown to handle scientific, defense, and diplomatic missions, although its primary mandate seems to be peaceful exploration in the search for sentient life, as seen in the mission statements of different incarnations of the USS "Enterprise". The flagship of Starfleet is often considered to be the starship USS "Enterprise".

Starfleet has many components, including:

As early as the original "", characters refer to attending Starfleet Academy. Later series establish it as an officer training facility with a four-year educational program. The main campus is located near Starfleet Headquarters in what is now Fort Baker, California.

Starfleet Command is the headquarters/command center of Starfleet. The term "Starfleet Command" is first used in episode "Court Martial". Its headquarters are depicted as being in Fort Baker, across the Golden Gate from San Francisco, in "" and "". Overlooking the Command from the other side of the Golden Gate is the permanent site of the Council of the United Federation of Planets in what is now the Presidio of San Francisco. Throughout the "Star Trek" franchise, the main characters' isolation from Starfleet Command compels them to make and act upon decisions without Starfleet Command's orders or information, particularly in "" when the main protagonists have no means of contacting Earth for several years.

StarTrek.com notes that many of Starfleet's ships are built on Mare Island near San Francisco. It states:

The "Enterprise-D" and USS "Voyager" are depicted to have been constructed at a shipyard named Utopia Planitia in Mars orbit. Utopia Planitia served as Starfleet's main ship yards throughout a large portion of Starfleet's existence.

After the "Enterprise-D" encountered the Borg in the episode "Q Who" the size of the Utopia Planitia shipyards was doubled out of fear of a Borg strike. They were once again doubled after the Dominion threat became more evident.

In the 2009 film, James T. Kirk arrives at a shipyard near his home in Iowa and boards a shuttle to enlist in Starfleet; as the shuttle leaves, we see that the ship under construction there is the "Enterprise".

In the 2013 sequel, Montgomery "Scotty" Scott discovers a covert Starfleet facility, near Jupiter, that has built a much larger Federation warship, USS "Vengeance".

The Starfleet Engineering Corps (also called the Starfleet Corps of Engineers) is mentioned in several episodes in conjunction with projects such as hollowing out the underground laboratory complex inside the Regula I asteroid in "", the design of the "Yellowstone"-class Runabout in the alternate timeline in the "" episode "", and devising a defense against the Breen energy-dampening weapon in the "" episode "When It Rains…" As a result of these successes, Starfleet engineers gained a reputation as the undisputed masters of technological adaptation and modification. As one minion of the Dominion in the "" episode, "" notes, Starfleet engineers are reputed to be able to "Turn rocks into replicators."

Additionally, Pocket Books has published a series of eBooks and novels in the "Starfleet Corps of Engineers" series.

Starfleet Intelligence is an intelligence agency of the United Federation of Planets. It is entrusted with foreign and domestic espionage, counter-espionage, and state security.

The Starfleet Judge Advocate General (or "JAG") is the branch charged with overseeing legal matters within Starfleet. Several episodes revolve around or involve JAG officers and procedures:

Dialog in "Court Martial" reveals that a court-martial may be convened in the absence of any JAG officers by three presiding command-level officers. Additionally, dialog in "The Measure of a Man" indicates that the loss of a starship automatically leads to a JAG court-martial. Courts-martial were held following the loss of the USS "Pegasus" and USS "Stargazer". In the "Voyager" episode "", Tuvok states that the Captain has the authority to conduct a court-martial on the ship, given the circumstance of the ship being isolated from the Federation.

Starfleet Medical is the medical branch of Starfleet.

Gates McFadden, who played Dr. Beverly Crusher, left "" during its second season. The character is described during this season, and after her return, as having been assigned to Starfleet Medical.

Numerous star ship dedication plaques identify other personnel associated with Starfleet Operations. Rear Admiral James T. Kirk served 18 months as Starfleet's Chief of Operations.

Starfleet Security is an agency of Starfleet referred to in several episodes of "" and "". Security is a branch of Starfleet first introduced in the . Main characters in subsequent series have been security officers.

Starfleet Tactical is a rarely-mentioned department in Starfleet that is responsible for planning defensive strategies, as well as engaging in weapons research and development.

Although Humans are the most-often-seen crew members onscreen, Starfleet is shown to be composed of individuals from over 150 races, with Vulcans perhaps being the most common aliens seen.

Already in "", the USS "Enterprise" and other ships have a mixed-species crew, although this does not appear to be an absolute rule; for instance, the episode "" refers to the USS "Intrepid" as having an all-Vulcan crew. The "" episode "Take Me Out to the Holosuite" also features such a crew, serving aboard the USS "T'Kumbra".

In keeping with this idea, "", in its first two seasons, was the only show to have an entirely human crew, as it was set before the formation of the Federation, although the vessel did carry Phlox, a Denobulan serving in a medical exchange program, and T'Pol, then serving as an observer from the Vulcan High Command.

"" saw the introduction of Starfleet's first Klingon officer. Other races—such as Bolians, Betazoids, and Trill—were seen, and given more central roles, in later series; some of these, notably Klingons, had been shown as enemies in earlier episodes.

Various episodes show that Earth/Federation citizenship is not a necessary pre-condition for joining Starfleet. T'Pol of Vulcan is shown to be the first non-human Starfleet officer, receiving a commission as a commander following the Xindi mission and her resignation from the Vulcan High Command. Even after the Federation's formation citizenship was not required; several officers are from planets that are not part of the Federation. For example, "Star Trek: TNG"s Ensign Ro Laren, a Bajoran aboard the USS "Enterprise"-D; her fellow Bajoran Kira Nerys, who was field-commissioned as a Starfleet commander so that she could aid the Cardassian resistance during the Dominion War; and Ferengi Nog, who enters Starfleet Academy in season four of Deep Space Nine; all were from non-member planets. In addition, Quinn and Icheb from "" both spoke of joining Starfleet.

An example of the process imagined by the writers is given when the character Nog attempts to apply to the Academy. He is told that since he is from a non-member world (Ferenginar), he requires a letter of recommendation from a command-level officer before his application can be considered, with the implication that this is the standard procedure for all non-Federation applicants to Starfleet.

In the "Star Trek" Expanded Universe, an example of what typically becomes of a new Federation member world's military is depicted when the Bajoran Militia is integrated into Starfleet upon Bajor's entry into the Federation.


</doc>
<doc id="29341" url="https://en.wikipedia.org/wiki?curid=29341" title="Superheterodyne receiver">
Superheterodyne receiver

A superheterodyne receiver, often shortened to superhet, is a type of radio receiver that uses frequency mixing to convert a received signal to a fixed intermediate frequency (IF) which can be more conveniently processed than the original carrier frequency. It was invented by US engineer Edwin Armstrong in 1918 during World War I. Virtually all modern radio receivers use the superheterodyne principle.

"Superheterodyne" is a contraction of "supersonic heterodyne", where "supersonic" indicates frequencies above the range of human hearing. The word "heterodyne" is derived from the Greek roots "hetero-" "different", and "-dyne" "power". In radio applications the term derives from the "heterodyne detector" pioneered by Canadian inventor Reginald Fessenden in 1905, describing his proposed method of producing an audible signal from the Morse code transmissions of the new continuous wave transmitters. With the older spark gap transmitters then in use, the Morse code signal consisted of short bursts of a carrier wave. Since these bursts were derived from the output of an alternator, they modulated the carrier at a frequency within the audio range and thus could be heard as a chirp or a buzz in the receiver's headphones. However, the signal from a continuous wave transmitter is at a single frequency well above the audio range, and Morse Code from one of these would be heard only as a series of clicks or thumps. Fessenden's idea was to run two Alexanderson alternators, one producing a carrier frequency 3 kHz higher than the other. In the receiver's detector, the two carriers would beat together to produce a 3 kHz tone, thus in the headphones the Morse signals would then be heard as a series of 3 kHz beeps. For this he coined the term "heterodyne," meaning "generated by a difference" (in frequency).

The French engineer Lucien Lévy filed a patent application for the superheterodyne principle in August 1917 with brevet n° 493660.
The American Edwin Howard Armstrong also filed a patent in 1917. 
Levy filed his original disclosure about seven months before Armstrong's.
The German inventor Walter H. Schottky also filed a patent in 1918.
At first the US recognised Armstrong as the inventor, and his US Patent 1,342,885 was issued on 8 June 1920.
After various changes and court hearings Lévy was awarded a US patent No 1,734,938 that included seven of the nine claims in Armstrong's application, while the two remaining claims were granted to Alexanderson of GE and Kendall of AT&T.
Armstrong invented his receiver as a means of overcoming the deficiencies of early vacuum tube triodes used as high-frequency amplifiers in radio direction finding equipment. Unlike simple radio communication, which needs only to make transmitted signals audible, direction-finders measure the received signal strength, which necessitates linear amplification of the actual carrier wave.

In a triode radio-frequency (RF) amplifier, if both the plate (anode) and grid are connected to resonant circuits tuned to the same frequency, stray capacitive coupling between the grid and the plate will cause the amplifier to go into oscillation if the stage gain is much more than unity. In early designs, dozens (in some cases over 100) low-gain triode stages had to be connected in cascade to make workable equipment, which drew enormous amounts of power in operation and required a team of maintenance engineers. The strategic value was so high, however, that the British Admiralty felt the high cost was justified.

Armstrong realized that if radio direction-finding (RDF) receivers could be operated at a higher frequency, this would allow better detection of enemy shipping. However, at that time, no practical "short wave" (defined then as any frequency above 500 kHz) amplifier existed, due to the limitations of existing triodes.

It had been noticed that when a regenerative receiver went into oscillation, other nearby receivers would suddenly start picking up stations on frequencies different from the stations' transmission frequency. Armstrong (and others) eventually deduced that this was caused by a "supersonic heterodyne" between the station's carrier frequency and the regenerative receiver's oscillation frequency. Thus if a station was transmitting on 300 kHz and the oscillating receiver was set to 400 kHz, the station would be heard not only at the original 300 kHz, but also at 100 kHz and 700 kHz.

Armstrong realized that this was a potential solution to the "short wave" amplification problem, since the beat frequency still retained its original modulation, but on a lower carrier frequency. To monitor a frequency of 1500 kHz for example, he could set up an oscillator at, for example, 1560 kHz, which would produce a heterodyne difference frequency of 60 kHz, a frequency that could then be more conveniently amplified by the triodes of the day. He termed this the "intermediate frequency" often abbreviated to "IF".

In December 1919, Major E. H. Armstrong gave publicity to an indirect method of obtaining short-wave amplification, called the super-heterodyne. The idea is to reduce the incoming frequency, which may be, say 1,500,000 cycles (200 meters), to some suitable super-audible frequency that can be amplified efficiently, then passing this current through an intermediate frequency amplifier, and finally rectifying and carrying on to one or two stages of audio frequency amplification.

Armstrong was able to put his ideas into practice, and the technique was soon adopted by the military. However, it was less popular when commercial radio broadcasting began in the 1920s, mostly due to the need for an extra tube (for the oscillator), the generally higher cost of the receiver, and the level of technical skill required to operate it. For early domestic radios, tuned radio frequency receivers (TRF) were more popular because they were cheaper, easier for a non-technical owner to use, and less costly to operate. Armstrong eventually sold his superheterodyne patent to Westinghouse, who in turn sold it to the Radio Corporation of America (RCA), the latter monopolizing the market for superheterodyne receivers until 1930.

Early superheterodyne receivers used IFs as low as 20 kHz, often based on the self-resonance of iron-core transformers. This made them extremely susceptible to image frequency interference, but at the time, the main objective was sensitivity rather than selectivity. Using this technique, a small number of triodes could be made to do the work that formerly required dozens of triodes.

In the 1920s, commercial IF filters looked very similar to 1920s audio interstage coupling transformers, had very similar construction and were wired up in an almost identical manner, and so they were referred to as "IF transformers". By the mid-1930s however, superheterodynes were using much higher intermediate frequencies, (typically around 440–470 kHz), with tuned coils similar in construction to the aerial and oscillator coils. However, the name "IF transformer" was retained and is still used today. Modern receivers typically use a mixture of ceramic resonator or SAW (surface-acoustic wave) resonators as well as traditional tuned-inductor IF transformers.

By the 1930s, improvements in vacuum tube technology rapidly eroded the TRF receiver's cost advantages, and the explosion in the number of broadcasting stations created a demand for cheaper, higher-performance receivers.

The development of the tetrode vacuum tube containing a screen grid led to a multi-element tube in which the mixer and oscillator functions could be combined, first used in the so-called autodyne mixer. This was rapidly followed by the introduction of tubes specifically designed for superheterodyne operation, most notably the pentagrid converter. By reducing the tube count, this further reduced the advantage of preceding receiver designs.

By the mid-1930s, commercial production of TRF receivers was largely replaced by superheterodyne receivers. By the 1940s the vacuum-tube superheterodyne AM broadcast receiver was refined into a cheap-to-manufacture design called the "All American Five", because it only uses five vacuum tubes: usually a converter (mixer/local oscillator), an IF amplifier, a detector/audio amp, audio power amp, and a rectifier. From this time, the superheterodyne design was used for virtually all commercial radio and TV receivers.

The diagram at right shows the block diagram of a typical single-conversion superheterodyne receiver. The diagram has blocks that are common to superheterodyne receivers. The antenna collects the radio signal. The tuned RF stage with optional RF amplifier provides some initial selectivity; it is necessary to suppress the "image frequency" (see below), and may also serve to prevent strong out-of-passband signals from saturating the initial amplifier. A local oscillator provides the mixing frequency; it is usually a variable frequency oscillator which is used to tune the receiver to different stations. The frequency mixer does the actual heterodyning that gives the superheterodyne its name; it changes the incoming radio frequency signal to a higher or lower, fixed, intermediate frequency (IF). The IF band-pass filter and amplifier supply most of the gain and the narrowband filtering for the radio. The demodulator extracts the audio or other modulation from the IF radio frequency; the extracted signal is then amplified by the audio amplifier.

To receive a radio signal, a suitable antenna is required. The output of the antenna may be very small, often only a few microvolts. The signal from the antenna is tuned and may be amplified in a so-called radio frequency (RF) amplifier, although this stage is often omitted. One or more tuned circuits at this stage block frequencies that are far removed from the intended reception frequency. In order to tune the receiver to a particular station, the frequency of the local oscillator is controlled by the tuning knob (for instance). Tuning of the local oscillator and the RF stage may use a variable capacitor, or varicap diode. The tuning of one (or more) tuned circuits in the RF stage must track the tuning of the local oscillator.

The signal is then fed into a circuit where it is mixed with a sine wave from a variable frequency oscillator known as the local oscillator (LO). The mixer uses a non-linear component to produce both sum and difference beat frequencies signals, each one containing the modulation contained in the desired signal. The output of the mixer may include the original RF signal at "f", the local oscillator signal at "f", and the two new heterodyne frequencies "f" + "f" and "f" − "f". The mixer may inadvertently produce additional frequencies such as third- and higher-order intermodulation products. Ideally, the IF bandpass filter removes all but the desired IF signal at "f". The IF signal contains the original modulation (transmitted information) that the received radio signal had at "f".

The frequency of the local oscillator "f" is set so the desired reception radio frequency "f" mixes to "f". There are two choices for the local oscillator frequency because the dominant mixer products are at "f" ± "f". If the local oscillator frequency is less than the desired reception frequency, it is called low-side injection ("f" = "f" − "f"); if the local oscillator is higher, then it is called high-side injection ("f" = "f" − "f").

The mixer will process not only the desired input signal at f, but also all signals present at its inputs. There will be many mixer products (heterodynes). Most other signals produced by the mixer (such as due to stations at nearby frequencies) can be filtered out in the IF tuned amplifier; that gives the superheterodyne receiver its superior performance. However, if "f" is set to "f" + "f", then an incoming radio signal at "f" + "f" will "also" produce a heterodyne at "f"; the frequency "f" + "f" is called the "image frequency" and must be rejected by the tuned circuits in the RF stage. The image frequency is 2 "f" higher (or lower) than the desired frequency "f", so employing a higher IF frequency "f" increases the receiver's "image rejection" without requiring additional selectivity in the RF stage.

To suppress the unwanted image, the tuning of the RF stage and the LO may need to "track" each other. In some cases, a narrow-band receiver can have a fixed tuned RF amplifier. In that case, only the local oscillator frequency is changed. In most cases, a receiver's input band is wider than its IF center frequency. For example, a typical AM broadcast band receiver covers 510 kHz to 1655 kHz (a roughly 1160 kHz input band) with a 455 kHz IF frequency; an FM broadcast band receiver covers 88 MHz to 108 MHz band with a 10.7 MHz IF frequency. In that situation, the RF amplifier must be tuned so the IF amplifier does not see two stations at the same time. If the AM broadcast band receiver LO were set at 1200 kHz, it would see stations at both 745 kHz (1200−455 kHz) and 1655 kHz. Consequently, the RF stage must be designed so that any stations that are twice the IF frequency away are significantly attenuated. The tracking can be done with a multi-section variable capacitor or some varactors driven by a common control voltage. An RF amplifier may have tuned circuits at both its input and its output, so three or more tuned circuits may be tracked. In practice, the RF and LO frequencies need to track closely but not perfectly.

In many superheterodyne receivers the same stage is used as both the local oscillator and the mixer, to save cost, power and size. This is called a "converter". In vacuum tube receivers, a single pentagrid converter tube would oscillate and also provide signal amplification as well as frequency shifting.

The stages of an intermediate frequency amplifier ("IF amplifier" or "IF strip") are tuned to a fixed frequency that does not change as the receiving frequency changes. The fixed frequency simplifies optimization of the IF amplifier. The IF amplifier is selective around its center frequency "f". The fixed center frequency allows the stages of the IF amplifier to be carefully tuned for best performance (this tuning is called "aligning" the IF amplifier). If the center frequency changed with the receiving frequency, then the IF stages would have had to track their tuning. That is not the case with the superheterodyne.

Typically, the IF center frequency "f" is chosen to be less than the desired reception frequency "f". The choice has some performance advantages. First, it is easier and less expensive to get high selectivity at a lower frequency. For the same bandwidth, a tuned circuit at a lower frequency needs a lower Q. Stated another way, for the same filter technology, a higher center frequency will take more IF filter stages to achieve the same selectivity bandwidth. Second, it is easier and less expensive to get high gain at a lower frequency. When used at high frequencies, many amplifiers show a constant gain–bandwidth product (dominant pole) characteristic. If an amplifier has a gain–bandwidth product of 100 MHz, then it would have a voltage gain of 100 at 1 MHz but only 10 at 10 MHz. If the IF amplifier needed a voltage gain of 10,000, then it would need only two stages with an IF at 1 MHz but four stages at 10 MHz.

Usually the intermediate frequency is lower than the reception frequency "f", but in some modern receivers (e.g. scanners and spectrum analyzers) a higher IF frequency is used to minimize problems with image rejection or gain the benefits of fixed-tuned stages. The Rohde & Schwarz EK-070 VLF/HF receiver covers 10 kHz to 30 MHz. It has a band switched RF filter and mixes the input to a first IF of 81.4 MHz. The first LO frequency is 81.4 to 111.4 MHz, so the primary images are far away. The first IF stage uses a crystal filter with a 12 kHz bandwidth. There is a second frequency conversion (making a triple-conversion receiver) that mixes the 81.4 MHz first IF with 80 MHz to create a 1.4 MHz second IF. Image rejection for the second IF is not a major problem because the first IF provides adequate image rejection and the second mixer is fixed tuned.

In order to avoid interference to receivers, licensing authorities will avoid assigning common IF frequencies to transmitting stations. Standard intermediate frequencies used are 455 kHz for medium-wave AM radio, 10.7 MHz for broadcast FM receivers, 38.9 MHz (Europe) or 45 MHz (US) for television, and 70 MHz for satellite and terrestrial microwave equipment. To avoid tooling costs associated with these components, most manufacturers then tended to design their receivers around a fixed range of frequencies offered, which resulted in a worldwide "de facto" standardization of intermediate frequencies.

In early superhets, the IF stage was often a regenerative stage providing the sensitivity and selectivity with fewer components. Such superhets were called super-gainers or regenerodynes. Another circuit added to the intermediate frequency chain is the Q multiplier.

The IF stage includes a filter and/or multiple tuned circuits in order to achieve the desired selectivity. This filtering must therefore have a band pass equal to or less than the frequency spacing between adjacent broadcast channels. Ideally a filter would have a high attenuation to adjacent channels, but maintain a flat response across the desired signal spectrum in order to retain the quality of the received signal. This may be obtained using one or more dual tuned IF transformers, a quartz crystal filter, or a multipole ceramic crystal filter.

In the case of television receivers, no other technique was able to produce the precise bandpass characteristic needed for vestigial sideband reception, such as that used in the NTSC system first approved by the U.S. in 1941. By the 1980s, multi-component capacitor-inductor filters had been replaced with precision electromechanical surface acoustic wave (SAW) filters. Fabricated by precision laser milling techniques, SAW filters are cheaper to produce, can be made to extremely close tolerances, and are very stable in operation.

The received signal is now processed by the demodulator stage where the audio signal (or other baseband signal) is recovered and then further amplified. AM demodulation requires the simple rectification of the RF signal (so-called envelope detection), and a simple RC low pass filter to remove remnants of the intermediate frequency. FM signals may be detected using a discriminator, ratio detector, or phase-locked loop. Continuous wave and single sideband signals require a product detector using a so-called beat frequency oscillator, and there are other techniques used for different types of modulation. The resulting audio signal (for instance) is then amplified and drives a loudspeaker.

When so-called high-side injection has been used, where the local oscillator is at a "higher" frequency than the received signal (as is common), then the frequency spectrum of the original signal will be reversed. This must be taken into account by the demodulator (and in the IF filtering) in the case of certain types of modulation such as single sideband.

To overcome obstacles such as image response, some receivers use multiple successive stages of frequency conversion and multiple IFs of different values. A receiver with two frequency conversions and IFs is called a "dual conversion superheterodyne", and one with three IFs is called a "triple conversion superheterodyne". 

The main reason that this is done is that with a single IF there is a tradeoff between low image response and selectivity. The separation between the received frequency and the image frequency is equal to twice the IF frequency, so the higher the IF, the easier it is to design an RF filter to remove the image frequency from the input and achieve low image response. However, the higher the IF, the more difficult it is to achieve high selectivity in the IF filter. At shortwave frequencies and above, the difficulty in obtaining sufficient selectivity in the tuning with the high IFs needed for low image response impacts performance. To solve this problem two IF frequencies can be used, first converting the input frequency to a high IF to achieve low image response, and then converting this frequency to a low IF to achieve good selectivity in the second IF filter. To improve tuning, a third IF can be used.

For example, for a receiver that can tune from 500 kHz to 30 MHz, three frequency converters might be used. With a 455 kHz IF it is easy to get adequate front end selectivity with broadcast band (under 1600 kHz) signals. For example, if the station being received is on 600 kHz, the local oscillator will be set to 600 + 455 = 1055 kHz. But a station on 1510 kHz could also potentially produce an IF of 455 kHz and so cause image interference. However, because 600 kHz and 1510 kHz are so far apart, it is easy to design the front end tuning to reject the 1510 kHz frequency.

However at 30 MHz, things are different. The oscillator would be set to 30.455 MHz to produce a 455 kHz IF, but a station on 30.910 would also produce a 455 kHz beat, so both stations would be heard at the same time. But it is virtually impossible to design an RF tuned circuit that can adequately discriminate between 30 MHz and 30.91 MHz, so one approach is to "bulk downconvert" whole sections of the shortwave bands to a lower frequency, where adequate front-end tuning is easier to arrange.

For example, the ranges 29 MHz to 30 MHz; 28 MHz to 29 MHz etc. might be converted down to 2 MHz to 3 MHz, there they can be tuned more conveniently. This is often done by first converting each "block" up to a higher frequency (typically 40 MHz) and then using a second mixer to convert it down to the 2 MHz to 3 MHz range. The 2 MHz to 3 MHz "IF" is basically another self-contained superheterodyne receiver, most likely with a standard IF of 455 kHz.

Microprocessor technology allows replacing the superheterodyne receiver design by a software defined radio architecture, where the IF processing after the initial IF filter is implemented in software. This technique is already in use in certain designs, such as very low-cost FM radios incorporated into mobile phones, since the system already has the necessary microprocessor.

Radio transmitters may also use a mixer stage to produce an output frequency, working more or less as the reverse of a superheterodyne receiver.

Superheterodyne receivers have essentially replaced all previous receiver designs. The development of modern semiconductor electronics negated the advantages of designs (such as the regenerative receiver) that used fewer vacuum tubes. The superheterodyne receiver offers superior sensitivity, frequency stability and selectivity. Compared with the tuned radio frequency receiver (TRF) design, superhets offer better stability because a tuneable oscillator is more easily realized than a tuneable amplifier. Operating at a lower frequency, IF filters can give narrower passbands at the same Q factor than an equivalent RF filter. A fixed IF also allows the use of a crystal filter or similar technologies that cannot be tuned. Regenerative and super-regenerative receivers offered a high sensitivity, but often suffer from stability problems making them difficult to operate.

Although the advantages of the superhet design are overwhelming, there are a few drawbacks that need to be tackled in practice.

One major disadvantage to the superheterodyne receiver is the problem of "image frequency". In heterodyne receivers, an image frequency is an undesired input frequency equal to the station frequency plus (or minus) twice the intermediate frequency. The image frequency results in two stations being received at the same time, thus producing interference. Image frequencies can be eliminated by sufficient attenuation on the incoming signal by the RF amplifier filter of the superheterodyne receiver.

For example, an AM broadcast station at 580 kHz is tuned on a receiver with a 455 kHz IF. The local oscillator is tuned to 1035 kHz. But a signal at 1490 kHz is also 455 kHz away from the local oscillator; so both the desired signal and the image, when mixed with the local oscillator, will also appear at the intermediate frequency. This image frequency is within the AM broadcast band. Practical receivers have a tuning stage before the converter, to greatly reduce the amplitude of image frequency signals; additionally, broadcasting stations in the same area have their frequencies assigned to avoid such images.

The unwanted frequency is called the "image" of the wanted frequency, because it is the "mirror image" of the desired frequency reflected formula_2. A receiver with inadequate filtering at its input will pick up signals at two different frequencies simultaneously: the desired frequency and the image frequency. Any noise or random radio station at the image frequency can interfere with reception of the desired signal.

Early Autodyne receivers typically used IFs of only 150 kHz or so, as it was difficult to maintain reliable oscillation if higher frequencies were used. As a consequence, most Autodyne receivers needed quite elaborate antenna tuning networks, often involving double-tuned coils, to avoid image interference. Later superhets used tubes specially designed for oscillator/mixer use, which were able to work reliably with much higher IFs, reducing the problem of image interference and so allowing simpler and cheaper aerial tuning circuitry.

Sensitivity to the image frequency can be minimized only by (1) a filter that precedes the mixer or (2) a more complex mixer circuit that suppresses the image. In most receivers, this is accomplished by a bandpass filter in the RF front end. In many tunable receivers, the bandpass filter is tuned in tandem with the local oscillator.

Image rejection is an important factor in choosing the intermediate frequency of a receiver. The farther apart the bandpass frequency and the image frequency are, the more the bandpass filter will attenuate any interfering image signal. Since the frequency separation between the bandpass and the image frequency is formula_3, a higher intermediate frequency improves image rejection. It may be possible to use a high enough first IF that a fixed-tuned RF stage can reject any image signals.

The ability of a receiver to reject interfering signals at the image frequency is measured by the image rejection ratio. This is the ratio (in decibels) of the output of the receiver from a signal at the received frequency, to its output for an equal-strength signal at the image frequency.

It is difficult to keep stray radiation from the local oscillator below the level that a nearby receiver can detect. The receiver's local oscillator can act like a low-power CW transmitter. Consequently, there can be mutual interference in the operation of two or more superheterodyne receivers in close proximity.

In intelligence operations, local oscillator radiation gives a means to detect a covert receiver and its operating frequency. The method was used by MI-5 during Operation RAFTER. This same technique is also used in radar detector detectors used by traffic police in jurisdictions where radar detectors are illegal.

A method of significantly reducing the local oscillator radiation from the receiver's antenna is to use an RF amplifier between the receiver's antenna and its mixer stage.

Local oscillators typically generate a single frequency signal that has negligible amplitude modulation but some random phase modulation. Either of these impurities spreads some of the signal's energy into sideband frequencies. That causes a corresponding widening of the receiver's frequency response, which would defeat the aim to make a very narrow bandwidth receiver such as to receive low-rate digital signals. Care needs to be taken to minimize oscillator phase noise, usually by ensuring that the oscillator never enters a non-linear mode.







</doc>
