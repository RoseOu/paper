<doc id="29462" url="https://en.wikipedia.org/wiki?curid=29462" title="Sabotage">
Sabotage

Sabotage is a deliberate action aimed at weakening a polity, effort, or organization through subversion, obstruction, disruption, or destruction. One who engages in sabotage is a "saboteur". Saboteurs typically try to conceal their identities because of the consequences of their actions.

Any unexplained adverse condition might be sabotage. Sabotage is sometimes called tampering, meddling, tinkering, malicious pranks, malicious hacking, a practical joke, or the like to avoid needing to invoke legal and organizational requirements for addressing sabotage.

A popular but false account of the origin of the term's present meaning is the story that poor workers in France, who wore wooden shoes called , used to throw them into the machines to disrupt production. This origin story is told in the 1991 movie . This account is not supported by the etymology. Rather, the French source word literally means to "walk noisily", as was done by sabot-wearing labourers, who interrupted production by means of labor disputes, not damage.

One of its first appearances in French literature is in the "Dictionnaire du Bas-Langage ou manières de parler usitées parmi le peuple" of D'Hautel, edited in 1808.

The verb "saboter" is also found in 1873–1874 in the "Dictionnaire de la langue française" of Émile Littré. But it is at the end of the 19th century that it really began to be used with the meaning of "deliberately and maliciously destroying property" or "working slower". In 1897, Émile Pouget, a famous syndicalist and anarchist wrote "action de saboter un travail" (action of sabotaging a work) in "Le Père Peinard" and in 1911 he also wrote a book entitled "Le Sabotage".

At the inception of the Industrial Revolution, skilled workers such as the Luddites (1811–1812) used sabotage as a means of negotiation in labor disputes.

Labor unions such as the Industrial Workers of the World (IWW) have advocated sabotage as a means of self-defense and direct action against unfair working conditions.

The IWW was shaped in part by the industrial unionism philosophy of Big Bill Haywood, and in 1910 Haywood was exposed to sabotage while touring Europe:

The experience that had the most lasting impact on Haywood was witnessing a general strike on the French railroads. Tired of waiting for parliament to act on their demands, railroad workers walked off their jobs all across the country. The French government responded by drafting the strikers into the army and then ordering them back to work. Undaunted, the workers carried their strike to the job. Suddenly, they could not seem to do anything right. Perishables sat for weeks, sidetracked and forgotten. Freight bound for Paris was misdirected to Lyon or Marseille instead. This tactic — the French called it "sabotage" — won the strikers their demands and impressed Bill Haywood.
For the IWW, sabotage's meaning expanded to include the original use of the term: any withdrawal of efficiency, including the slowdown, the strike, working to rule, or creative bungling of job assignments. 

One of the most severe examples was at the construction site of the Robert-Bourassa Generating Station in 1974, in Québec, Canada, when workers used bulldozers to topple electric generators, damaged fuel tanks, and set buildings on fire. The project was delayed a year, and the direct cost of the damage estimated at $2 million CAD. The causes were not clear, but three possible factors have been cited: inter-union rivalry, poor working conditions, and the perceived arrogance of American executives of the contractor, Bechtel Corporation.

Certain groups turn to destruction of property to stop environmental destruction or to make visible arguments against forms of modern technology they consider detrimental to the environment. The U.S. Federal Bureau of Investigation (FBI) and other law enforcement agencies use the term eco-terrorist when applied to damage of property. Proponents argue that since property cannot feel terror, damage to property is more accurately described as sabotage. Opponents, by contrast, point out that property owners and operators can indeed feel terror. The image of the monkey wrench thrown into the moving parts of a machine to stop it from working was popularized by Edward Abbey in the novel "The Monkey Wrench Gang" and has been adopted by eco-activists to describe destruction of earth damaging machinery.

From 1992 to late 2007 a radical environmental activist movement known as ELF or Earth Liberation Front engaged in a near constant campaign of decentralized sabotage of any construction projects near wild lands and extractive industries such as logging and even the burning down of a ski resort of Vail Colorado. ELF used sabotage tactics often in loose coordination with other environmental activist movements to physically delay or destroy threats to wild lands as the political will developed to protect the targeted wild areas that ELF engaged.

In war, the word is used to describe the activity of an individual or group not associated with the military of the parties at war, such as a foreign agent or an indigenous supporter, in particular when actions result in the destruction or damaging of a productive or vital facility, such as equipment, factories, dams, public services, storage plants or logistic routes. Prime examples of such sabotage are the events of Black Tom and the Kingsland Explosion. Like spies, saboteurs who conduct a military operation in civilian clothes or enemy uniforms behind enemy lines are subject to prosecution and criminal penalties instead of detention as prisoners of war. It is common for a government in power during war or supporters of the war policy to use the term loosely against opponents of the war. Similarly, German nationalists spoke of a stab in the back having cost them the loss of World War I.

A modern form of sabotage is the distribution of software intended to damage specific industrial systems. For example, the U.S. Central Intelligence Agency (CIA) is alleged to have sabotaged a Siberian pipeline during the Cold War, using information from the Farewell Dossier. A more recent case may be the Stuxnet computer worm, which was designed to subtly infect and damage specific types of industrial equipment. Based on the equipment targeted and the location of infected machines, security experts believe it was an attack on the Iranian nuclear program by the United States, Israel or, according to the latest news, even Russia.

Sabotage, done well, is inherently difficult to detect and difficult to trace to its origin. During World War II, the U.S. Federal Bureau of Investigation (FBI) investigated 19,649 cases of sabotage and concluded the enemy had not caused any of them.

Sabotage in warfare, according to the Office of Strategic Services (OSS) manual, varies from highly technical "coup de main" acts that require detailed planning and specially trained operatives, to innumerable simple acts that ordinary citizen-saboteurs can perform. Simple sabotage is carried out in such a way as to involve a minimum danger of injury, detection, and reprisal. There are two main methods of sabotage; physical destruction and the "human element". While physical destruction as a method is self-explanatory, its targets are nuanced, reflecting objects to which the saboteur has normal and inconspicuous access in everyday life. The "human element" is based on universal opportunities to make faulty decisions, to adopt a non-cooperative attitude, and to induce others to follow suit.

There are many examples of physical sabotage in wartime. However, one of the most effective uses of sabotage is against organizations. The OSS manual provides numerous techniques under the title "General Interference with Organizations and Production":


From the section entitled, "General Devices for Lowering Morale and Creating Confusion" comes the following quintessential simple sabotage advice: "Act stupid."

The United States Office of Strategic Services, later renamed the CIA, noted specific value in committing simple sabotage against the enemy during wartime: "... slashing tires, draining fuel tanks, starting fires, starting arguments, acting stupidly, short-circuiting electric systems, abrading machine parts will waste materials, manpower, and time." To underline the importance of simple sabotage on a widespread scale, they wrote, "Widespread practice of simple sabotage will harass and demoralize enemy administrators and police." The OSS was also focused on the battle for hearts and minds during wartime; "the very practice of simple sabotage by natives in enemy or occupied territory may make these individuals identify themselves actively with the United Nations War effort, and encourage them to assist openly in periods of Allied invasion and occupation."

On 30 July 1916, the Black Tom explosion occurred when German agents set fire to a complex of warehouses and ships in Jersey City, New Jersey that held munitions, fuel, and explosives bound to aid the Allies in their fight.

On 11 January 1917, Fiodore Wozniak, using a rag saturated with phosphorus or an incendiary pencil supplied by German sabotage agents, set fire to his workbench at an ammunition assembly plant near Lyndhurst, New Jersey, causing a four-hour fire that destroyed half a million 3-inch explosive shells and destroyed the plant for an estimated at $17 million in damages. Wozniak's involvement was not discovered until 1927.

On 12 February 1917, Bedouins allied with the British destroyed a Turkish railroad near the port of Wajh, derailing a Turkish locomotive. The Bedouins traveled by camel and used explosives to demolish a portion of track.

In Ireland, the Irish Republican Army (IRA) used sabotage against the British following the Easter 1916 uprising. The IRA compromised communication lines and lines of transportation and fuel supplies. The IRA also employed passive sabotage, refusing dock and train workers to work on ships and rail cars used by the government. In 1920, agents of the IRA committed arson against at least fifteen British warehouses in Liverpool. The following year, the IRA set fire to numerous British targets again, including the Dublin Customs House, this time sabotaging most of Liverpool's firetrucks in the firehouses before lighting the matches.

Lieutenant Colonel George T. Rheam was a British soldier, who ran ran Brickendonbury Manor from October 1941 to June 1945 during World War II, which was Station XVII of the Special Operations Executive (SOE), which trained specialists for the SOE. Rheam innovated many sabotage techniques, and is considered by M. R. D. Foot the "founder of modern industrial sabotage." 

Sabotage training for the Allies consisted of teaching would-be saboteurs key components of working machinery to destroy.
"Saboteurs learned hundreds of small tricks to cause the Germans big trouble. The cables in a telephone junction box ... could be jumbled to make the wrong connections when numbers were dialed. A few ounces of plastique, properly placed, could bring down a bridge, cave in a mine shaft, or collapse the roof of a railroad tunnel."

The Polish Home Army Armia Krajowa, who commanded the majority of resistance organizations in Poland (even the National Forces, except the Military Organization Lizard Union; The Home Army also included the Polish Socialist Party – Freedom, Equality, Independence) and coordinating and aiding the Jewish Military Union as well as more reluctantly helping the Jewish Combat Organization, was responsible for the greatest number of acts of sabotage in German—occupied Europe. The Home Army's sabotage operations Operation Garland and Operation Ribbon are just two examples. In all, the Home Army damaged 6,930 locomotives, set 443 rail transports on fire, damaged over 19,000 rail cars "wagony", and blew up 38 rail bridges, not to mention the attacks against the rail roads. The Home Army was also responsible for 4,710 built-in flaws in parts for aircraft engines and 92,000 built-in flaws in artillery projectiles, among other examples of significant sabotage. In addition, over 25,000 acts of more minor sabotage were committed. It continued to fight against both the Germans and the Soviets; however, it did aid the Western Allies by collecting constant and detailed information on the German rail, wheeled, and horse transports. As for Stalin's proxies, their actions lead to a great number of the Polish and Jewish hostages, mostly civilians, murdered in reprisal by the Germans. The Gwardia Ludowa destroyed around 200 German trains during the war, and indiscriminately threw hand grenades into places frequented by Germans.

The French Resistance ran an extremely effective sabotage campaign against the Germans during World War II. Receiving their sabotage orders through messages over the BBC radio or by aircraft, the French used both passive and active forms of sabotage. Passive forms included losing German shipments and allowing poor quality material to pass factory inspections. Many active sabotage attempts were against critical rail lines of transportation. German records count 1,429 instances of sabotage from French Resistance forces between January 1942 and February 1943. From January through March 1944, sabotage accounted for three times the number of locomotives damaged by Allied air power. See also Normandy Landings for more information about sabotage on D-Day.

During World War II, the Allies committed sabotage against the Peugeot truck factory. After repeated failures in Allied bombing attempts to hit the factory, a team of French Resistance fighters and Special Operations Executive (SOE) agents distracted the German guards with a game of soccer while part of their team entered the plant and destroyed machinery.

In December 1944, the Germans ran a false flag sabotage infiltration, Operation Greif, which was commanded by Waffen-SS commando Otto Skorzeny during the Battle of the Bulge. German commandos, wearing US Army uniforms, carrying US Army weapons, and using US Army vehicles, penetrated US lines to spread panic and confusion among US troops and to blow up bridges, ammunition dumps, and fuel stores and to disrupt the lines of communication. Many of the commandos were captured by the Americans. Because they were wearing US uniforms, a number of the Germans were executed as spies, either summarily or after military commissions.

From 1948 to 1960, the Malayan Communists committed numerous effective acts of sabotage against the British Colonial authorities, first targeting railway bridges, then hitting larger targets such as military camps. Most of their efforts were centered around crippling Malaysia's colonial economy and involved sabotage against trains, rubber trees, water pipes, and electric lines. The Communist's sabotage efforts were so successful that they caused backlash amongst the Malaysian population, who gradually withdrew support for the Communist movement as their livelihoods became threatened.

In Mandatory Palestine from 1945 to 1948, Jewish groups opposed British control. Though that control was to end according to the United Nations Partition Plan for Palestine in 1948, the groups used sabotage as an opposition tactic. The Haganah focused their efforts on camps used by the British to hold refugees and radar installations that could be used to detect illegal immigrant ships. The Stern Gang and the Irgun used terrorism and sabotage against the British government and against lines of communications. In November 1946, the Irgun and Stern Gang attacked a railroad twenty-one times in a three-week period, eventually causing shell-shocked Arab railway workers to strike. The 6th Airborne Division was called in to provide security as a means of ending the strike.

The Viet Cong used swimmer saboteurs often and effectively during the Vietnam War. Between 1969 and 1970, swimmer saboteurs sunk, destroyed, or damaged 77 assets of the U.S. and its allies. Viet Cong swimmers were poorly equipped but well-trained and resourceful. The swimmers provided a low-cost/low-risk option with high payoff; possible loss to the country for failure compared to the possible gains from a successful mission led to the obvious conclusion the swimmer saboteurs were a good idea.

On 1 January 1984, the Cuscatlan bridge over Lempa river in El Salvador, critical to flow of commercial and military traffic, was destroyed by guerrilla forces using explosives after using mortar fire to "scatter" the bridge's guards, causing an estimated $3.7 million in required repairs, and considerably impacting on El Salvadoran business and security.

In 1982 in Honduras, a group of nine Salvadorans and Nicaraguans destroyed a main electrical power station, leaving the capital city Tegucigalpa without power for three days.

Some criminals have engaged in acts of sabotage for reasons of extortion. For example, Klaus-Peter Sabotta sabotaged German railway lines in the late 1990s in an attempt to extort DM10 million from the German railway operator Deutsche Bahn. He is now serving a sentence of life imprisonment. In 1989, ex-Scotland Yard detective Rodney Whitchelo was sentenced to 17 years in prison for spiking Heinz baby food products in supermarkets, in an extortion attempt on the food manufacturer.

The term political sabotage is sometimes used to define the acts of one political camp to disrupt, harass or damage the reputation of a political opponent, usually during an electoral campaign, such as during Watergate. Smear campaigns are a commonly used tactic. The term could also describe the actions and expenditures of private entities, corporations and organizations against democratically approved or enacted laws, policies and programs.

After the Cold War ended, the Mitrokhin Archives were declassified, which included detailed KGB plans of active measures to subvert politics in opposing nations.

Sabotage is a crucial tool of the successful coup d'etat, which requires control of communications before, during, and after the coup is staged. Simple sabotage against physical communications platforms using semi-skilled technicians, or even those trained only for this task, could effectively silence the target government of the coup, leaving the information battle space open to the dominance of the coup's leaders. To underscore the effectiveness of sabotage, "A single cooperative technician will be able temporarily to put out of action a radio station which would otherwise require a full-scale assault."

Railroads, where strategically important to the regime the coup is against, are prime targets for sabotage—if a section of the track is damaged entire portions of the transportation network can be stopped until it is fixed.

A sabotage radio was a small two-way radio designed for use by resistance movements in World War II, and after the war often used by expeditions and similar parties.

Arquilla and Rondfeldt, in their work entitled "Networks and Netwars", differentiate their definition of "netwar" from a list of "trendy synonyms", including "cybotage", a portmanteau from the words "sabotage" and "cyber". They dub the practitioners of cybotage "cyboteurs" and note while all cybotage is not netwar, some netwar is cybotage.

Counter-sabotage, defined by "Webster's Dictionary", is "counterintelligence designed to detect and counteract sabotage". The United States Department of Defense definition, found in the "Dictionary of Military and Associated Terms", is "action designed to detect and counteract sabotage. See also counterintelligence".

During World War II, British subject Eddie Chapman, trained by the Germans in sabotage, became a double agent for the British. The German Abwehr entrusted Chapman to destroy the British de Havilland Company's main plant which manufactured the outstanding Mosquito light bomber, but required photographic proof from their agent to verify the mission's completion. A special unit of the Royal Engineers known as the Magic Gang covered the de Havilland plant with canvas panels and scattered papier-mâché furniture and chunks of masonry around three broken and burnt giant generators. Photos of the plant taken from the air reflected devastation for the factory and a successful sabotage mission, and Chapman, as a British sabotage double-agent, fooled the Germans for the duration of the war.

In Japanese, the verb saboru (サボる) means to skip school or loaf on the job.




</doc>
<doc id="29463" url="https://en.wikipedia.org/wiki?curid=29463" title="Scabbard">
Scabbard

A scabbard is a sheath for holding a sword, knife, or other large blade. As well, rifles may be stored in a scabbard by horse riders. Military cavalry and cowboys had scabbards for their saddle ring carbine rifles and lever action rifles on their horses for storage and protection. Scabbards have been made of many materials over the millennia, including leather, wood, and metals such as brass or steel.

Most commonly, sword scabbards were worn suspended from a sword belt or shoulder belt called a baldric.

Wooden scabbards were usually covered in fabric or leather; the leather versions also usually bore metal fittings for added protection and carrying ease. Japanese blades typically have their sharp cutting edge protected by a wooden scabbard called a saya. Many scabbards, such as ones the Greeks and Romans used, were small and light. They were designed for holding the sword rather than protecting it. All-metal scabbards were popular items for a display of wealth among elites in the European Iron Age, and often intricately decorated. Little is known about the scabbards of the early Iron Age, due to their wooden construction. However, during the Middle and late Iron Ages, the scabbard became important especially as a vehicle for decorative elaboration. After 200 BC fully decorated scabbards became rare. A number of ancient scabbards have been recovered from weapons sacrifices, a few of which had a lining of fur on the inside. The fur was probably kept oily, keeping the blade free from rust. The fur would also allow a smoother, quicker draw.

Entirely metal scabbards became popular in Europe early in the 19th century and eventually superseded most other types. Metal was more durable than leather and could better withstand the rigours of field use, particularly among troops mounted on horseback. In addition, metal offered the ability to present a more military appearance, as well as the opportunity to display increased ornamentation. Nevertheless, leather scabbards never entirely lost favour among military users and were widely used as late as the American Civil War (1861–65).

Some military police forces, naval shore patrols, law enforcement and other groups used leather scabbards as a kind of truncheon.

Scabbards were historically, albeit rarely, worn across the back, but only by a handful of Celtic tribes, and only with very short lengths of sword. This is because drawing a long, sharp blade over one's shoulder and past one's head from a scabbard on the back is relatively awkward, especially in a hurry, and the length of the arm sets a hard upper limit on how long a blade can be drawn at all in this way. Sheathing the sword again is even harder since it has to be done effectively blind unless the scabbard is taken off first. Common depictions of long swords being drawn from the back are a modern invention, born from safety and convenience considerations on a film set and typically enabled by creative editing, and have enjoyed such great popularity in fiction and fantasy that they are widely and incorrectly believed to have been common in Medieval times. Some more well-known examples of this include the back scabbard depicted in the film "Braveheart" and the back scabbard seen in the video game series "The Legend of Zelda". There is some limited data from woodcuts and textual fragments that Mongol light horse archers, Chinese soldiers, Japanese Samurai and European Knights wore a slung baldric over the shoulder, allowing longer blades such as greatswords/zweihanders and nodachi/ōdachi to be strapped across the back, though these would have to be removed from the back before the sword could be unsheathed.

In "The Ancient Celts" by Barry Cunliffe, Cunliffe writes, "All these pieces of equipment [shields, spears, swords, mail] mentioned in the texts, are reflected in the archaeological record and in the surviving iconography, though it is sometimes possible to detect regional variations (page 94). Among the Parisii of Yorkshire, for example, the "...sword was sometimes worn across the back and therefore had to be drawn over the shoulder from behind the head."

The metal fitting where the blade enters the leather or metal scabbard is called the throat, which is often part of a larger scabbard mount, or locket, that bears a carrying ring or stud to facilitate wearing the sword. The blade's point in leather scabbards is usually protected by a metal tip, or chape, which, on both leather and metal scabbards, is often given further protection from wear by an extension called a drag, or shoe.



</doc>
<doc id="29467" url="https://en.wikipedia.org/wiki?curid=29467" title="Spinel">
Spinel

Spinel () is the magnesium/aluminium member of the larger spinel group of minerals. It has the formula MgAlO in the cubic crystal system. Its name comes from the Latin word "spinella", which means "spine" in reference to its pointed crystals.

Spinel crystallizes in the isometric system; common crystal forms are octahedra, usually twinned. It has an imperfect octahedral cleavage and a conchoidal fracture. Its hardness is 8, its specific gravity is 3.5–4.1, and it is transparent to opaque with a vitreous to dull luster. It may be colorless, but is usually various shades of pink, rose, red, blue, green, yellow, brown, black, or (uncommon) violet. There is a unique natural white spinel, now lost, that surfaced briefly in what is now Sri Lanka. Some spinels are among the most famous gemstones; among them are the Black Prince's Ruby and the "Timur ruby" in the British Crown Jewels, and the "Côte de Bretagne", formerly from the French Crown jewels. The Samarian Spinel is the largest known spinel in the world, weighing .

The transparent red spinels were called spinel-rubies or balas rubies. In the past, before the arrival of modern science, spinels and rubies were equally known as rubies. After the 18th century the word ruby was only used for the red gem variety of the mineral corundum and the word spinel came to be used. "Balas" is derived from Balascia, the ancient name for Badakhshan, a region in central Asia situated in the upper valley of the Panj River, one of the principal tributaries of the Oxus River. Mines in the Gorno Badakhshan region of Tajikistan constituted for centuries the main source for red and pink spinels.

Spinel is found as a metamorphic mineral, and also as a primary mineral in rare mafic igneous rocks; in these igneous rocks, the magmas are relatively deficient in alkalis relative to aluminium, and aluminium oxide may form as the mineral corundum or may combine with magnesia to form spinel. This is why spinel and ruby are often found together. The spinel petrogenesis in mafic magmatic rocks is strongly debated, but certainly results from mafic magma interaction with more evolved magma or rock (e.g. gabbro, troctolite).

Spinel, (Mg,Fe)(Al,Cr)O, is common in peridotite in the uppermost Earth's mantle, between approximately 20 km to approximately 120 km, possibly to lower depths depending on the chromium content. At significantly shallower depths, above the Moho, calcic plagioclase is the more stable aluminous mineral in peridotite while garnet is the stable phase deeper in the mantle below the spinel stability region.

Spinel, (Mg,Fe)AlO, is a common mineral in the Ca-Al-rich inclusions (CAIs) in some chondritic meteorites.

Spinel has long been found in the gemstone-bearing gravel of Sri Lanka and in limestones of the Badakshan Province in modern-day Afghanistan and Tajikistan; and of Mogok in Myanmar. Over the last decades gem quality spinels are found in the marbles of Lục Yên District (Vietnam), Mahenge and Matombo (Tanzania), Tsavo (Kenya) and in the gravels of Tunduru (Tanzania) and Ilakaka (Madagascar).

Since 2000 in several locations around the world have been discovered spinels with unusual vivid pink or blue color. Such "glowing" spinels are known from Mogok (Myanmar), Mahenge plateau (Tanzania), Lục Yên District (Vietnam) and some more localities. In 2018 bright blue spinels have been reported also in the southern part of Baffin Island (Canada). The pure blue coloration of spinel is caused by small additions of cobalt.

Synthetic spinel, accidentally produced in the middle of the 18th century, has been described more recently in scientific publications in 2000 and 2004. By 2015, transparent spinel was being made in sheets and other shapes through sintering. Synthetic spinel, which looks like glass but has notably higher strength against pressure, can also have applications in military and commercial use.





</doc>
<doc id="29468" url="https://en.wikipedia.org/wiki?curid=29468" title="Speech recognition">
Speech recognition

Speech recognition is an interdisciplinary subfield of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.

Some speech recognition systems require "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called "speaker independent" systems. Systems that use training are called "speaker dependent".

Speech recognition applications include voice user interfaces such as voice dialing (e.g. "call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).

The term "voice recognition" or "speaker identification" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.

From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.

The key areas of growth were: vocabulary size, speaker independence and processing speed.


Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required users to pause after each word. Reddy's system issued spoken commands for playing game chess.

Around this time Soviet researchers invented the dynamic time warping (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. DTW processed speech by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique carried on. Achieving speaker independence remained unsolved at this time period.



During the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. A decade later, at CMU, Raj Reddy's students James Baker and Janet M. Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.



The 1980s also saw the introduction of the n-gram language model.

Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. It could take up to 100 minutes to decode just 30 seconds of speech.

Two practical products were:

By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. Raj Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the speech recognition group at Microsoft in 1993. Raj Reddy's student Kai-Fu Lee joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.

Lernout & Hauspie, a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the Windows XP operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.

In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, Cambridge University, and a team composed of ICSI, SRI and University of Washington. EARS funded the collection of the Switchboard telephone speech corpus containing 260 hours of recorded conversations from over 500 speakers. The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance. The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google Voice Search is now supported in over 30 languages.

In the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006. This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.

In the early 2000s, speech recognition was still dominated by traditional approaches such as Hidden Markov Models combined with feedforward artificial neural networks.
Today, however, many aspects of speech recognition have been taken over by a deep learning method called Long short-term memory (LSTM), a recurrent neural network published by Sepp Hochreiter & Jürgen Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks that require memories of events that happened thousands of discrete time steps ago, which is important for speech.
Around 2007, LSTM trained by Connectionist Temporal Classification (CTC) started to outperform traditional speech recognition in certain applications. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users.

The use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence "The shared views of four research groups" subtitle in their 2012 review paper). A Microsoft research executive called this innovation "the most dramatic change in accuracy since 1979". In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.

In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s.
But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing and weak temporal correlation structure in the neural predictive models. All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009–2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks to speech recognition.

By early 2010s "speech" recognition, also called voice recognition was clearly differentiated from "speaker" recognition, and speaker independence was considered a major breakthrough. Until then, systems required a "training" period. A 1987 ad for a doll had carried the tagline "Finally, the doll that understands you." – despite the fact that it was described as "which children could train to respond to their voice".

In 2017, Microsoft researchers reached a historical human parity milestone of transcribing conversational telephony speech on the widely benchmarked Switchboard task. Multiple deep learning models were used to optimize speech recognition accuracy. The speech recognition word error rate was reported to be as low as 4 professional human transcribers working together on the same benchmark, which was funded by IBM Watson speech team on the same task.

Both acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.

Modern general-purpose speech recognition systems are based on Hidden Markov Models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.

Another reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of "n"-dimensional real-valued vectors (with "n" being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.

Described above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).

Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information, and combining it statically beforehand (the finite state transducer, or FST, approach).

A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.

Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.

Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.

A well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are "warped" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.

Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, isolated word recognition, audiovisual speech recognition, audiovisual speaker recognition and speaker adaptation.

Neural networks make fewer explicit assumptions about feature statistical properties than HMMs and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words, early neural networks were rarely successful for continuous recognition tasks because of their limited ability to model temporal dependencies.

One approach to this limitation was to use neural networks as a pre-processing, feature transformation or dimensionality reduction, step prior to HMM based recognition. However, more recently, LSTM and related recurrent neural networks (RNNs) and Time Delay Neural Networks(TDNN's) have demonstrated improved performance in this area.

Deep Neural Networks and Denoising Autoencoders are also under investigation. A deep feedforward neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.

A success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.
recent overview articles.

One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.
The true "raw" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.

Since 2014, there has been much research interest in "end-to-end" ASR. Traditional phonetic-based (i.e., all HMM-based model) approaches required separate components and training for the pronunciation, acoustic and language model. End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a n-gram language model is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices. Consequently, modern commercial ASR systems from Google and Apple (as of 2017) are deployed on the cloud and require a network connection as opposed to the device locally.

The first attempt at end-to-end ASR was with Connectionist Temporal Classification (CTC)-based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014. The model consisted of recurrent neural networks and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to conditional independence assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later, Baidu expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English. In 2016, University of Oxford presented LipNet, the first end-to-end sentence-level lip reading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset. A large-scale CNN-RNN-CTC architecture was presented in 2018 by Google DeepMind achieving 6 times better performance than human experts.

An alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of Carnegie Mellon University and Google Brain and Bahdanau et al. of the University of Montreal in 2016. The model named "Listen, Attend and Spell" (LAS), literally "listens" to the acoustic signal, pays "attention" to different parts of the signal and "spells" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model). Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by Carnegie Mellon University, MIT and Google Brain to directly emit sub-word units which are more natural than English characters; University of Oxford and Google DeepMind extended LAS to "Watch, Listen, Attend and Spell" (WLAS) to handle lip reading surpassing human-level performance.

Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. Following the audio prompt, the system has a "listening window" during which it may accept a speech input for recognition.
Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.

In the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.

One of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to "Meaningful Use" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.

A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice "macros", where the use of certain phrases – e.g., "normal report", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam – e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.

As an alternative to this navigation by hand, cascaded use of speech recognition and information extraction has been studied as a way to fill out a handover form for clinical proofing and sign-off. The results are encouraging, and the paper also opens data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing.

Prolonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.

Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note have been the US program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the program in France for Mirage aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.

Working with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing g-loads. The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.

The Eurofighter Typhoon, currently in service with the UK RAF, employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands.

Speaker-independent systems are also being developed and are under test for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.

The problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.

As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.

Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a "pseudo-pilot", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation. Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.

The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.

ASR is now commonplace in the field of telephony and is becoming more widespread in the field of computer gaming and simulation. In telephony systems, ASR is now being predominantly used in contact centers by integrating it with IVR systems. Despite the high level of integration with word processing in general personal computing, in the field of document production, ASR has not seen the expected increases in use.

The improvement of mobile processor speeds has made speech recognition practical in smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands.

For language learning, speech recognition can be useful for learning a second language. It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.

Students who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.

Students who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.

Speech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. Also, see Learning disability.

Use of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.

People with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.

Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.

This type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.


The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).

Speech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:

As mentioned earlier in this article, accuracy of speech recognition may vary depending on the following factors:
With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. 
With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.


Constraints are often represented by a grammar. 
Speech recognition is a multi-levelled pattern recognition task.
e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;
For telephone speech the sampling rate is 8000 samples per second; 
computed every 10 ms, with one 10 ms section called a frame;

Analysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has two descriptions: amplitude (how strong is it), and frequency (how often it vibrates per second).

Speech recognition can become a means of attack, theft, or accidental operation. For example, activation words like "Alexa" spoken in an audio or video broadcast can cause devices in homes and offices to start listening for input inappropriately, or possibly take an unwanted action. Voice-controlled devices are also accessible to visitors to the building, or even those outside the building if they can be heard inside. Attackers may be able to gain access to personal information, like calendar, address book contents, private messages, and documents. They may also be able to impersonate the user to send messages or make online purchases.

Two attacks have been demonstrated that use artificial sounds. One transmits ultrasound and attempt to send commands without nearby people noticing. The other adds small, inaudible distortions to other speech or music that are specially crafted to confuse the specific speech recognition system into recognizing music as speech, or to make what sounds like one command to a human sound like a different command to the system.

Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing—after merging with an ACM publication), Computer Speech and Language, and Speech Communication.

Books like "Fundamentals of Speech Recognition" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be "Statistical Methods for Speech Recognition" by Frederick Jelinek and "Spoken Language Processing (2001)" by Xuedong Huang etc., "Computer Speech", by Manfred R. Schroeder, second edition published in 2004, and "Speech Processing: A Dynamic and Optimization-Oriented Approach" published in 2003 by Li Deng and Doug O'Shaughnessey. The updated textbook "Speech and Language Processing" (2008) by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A comprehensive textbook, "Fundamentals of Speaker Recognition" is an in depth source for up to date details on the theory and practice. A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).

A good and accessible introduction to speech recognition technology and its history is provided by the general audience book "The Voice in the Machine. Building Computers That Understand Speech" by Roberto Pieraccini (2012).

The most recent book on speech recognition is "Automatic Speech Recognition: A Deep Learning Approach" (Publisher: Springer) written by Microsoft researchers D. Yu and L. Deng and published near the end of 2014, with highly mathematically oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. A related book, published earlier in 2014, "Deep Learning: Methods and Applications" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009–2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.

In terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques, Kaldi toolkit can be used. In 2017 Mozilla launched the open source project called Common Voice to gather big database of voices that would help build free speech recognition project DeepSpeech (available free at GitHub) using Google open source platform TensorFlow.

The commercial cloud based speech recognition APIs are broadly available from AWS, Azure, IBM, and GCP.

A demonstration of an on-line speech recognizer is available on Cobalt's webpage.

For more software resources, see List of speech recognition software.




</doc>
<doc id="29469" url="https://en.wikipedia.org/wiki?curid=29469" title="Sapphire">
Sapphire

Sapphire is a precious gemstone, a variety of the mineral corundum, consisting of aluminum oxide () with trace amounts of elements such as iron, titanium, chromium, vanadium, or magnesium. It is typically blue, but natural "fancy" sapphires also occur in yellow, purple, orange, and green colors; "parti sapphires" show two or more colors. The only color corundum stone that the term sapphire is not used for is red, which is called a ruby. Pink colored corundum may be either classified as ruby or sapphire depending on locale. 
Commonly, natural sapphires are cut and polished into gemstones and worn in jewelry. They also may be created synthetically in laboratories for industrial or decorative purposes in large crystal boules. Because of the remarkable hardness of sapphires – 9 on the Mohs scale (the third hardest mineral, after diamond at 10 and moissanite at 9.5) – sapphires are also used in some non-ornamental applications, such as infrared optical components, high-durability windows, wristwatch crystals and movement bearings, and very thin electronic wafers, which are used as the insulating substrates of special-purpose solid-state electronics such as integrated circuits and GaN-based blue LEDs.

Sapphire is the birthstone for September and the gem of the 45th anniversary. A sapphire jubilee occurs after 65 years.

Sapphire is one of the two gem-varieties of corundum, the other being ruby (defined as corundum in a shade of red). Although blue is the best-known sapphire color, they occur in other colors, including gray and black, and they can be colorless. A pinkish orange variety of sapphire is called padparadscha.

Significant sapphire deposits are found in Australia, Cambodia, Cameroon, China (Shandong), Colombia, Ethiopia, India (Kashmir), Kenya, Laos, Madagascar, Malawi, Mozambique, Myanmar (Burma), Nigeria, Rwanda, Sri Lanka, Tanzania, Thailand, United States (Montana) and Vietnam. Sapphire and rubies are often found in the same geographical settings, but they generally have different geological formations. For example, both ruby and sapphire are found in Myanmar's Mogok Stone Tract, but the rubies form in marble, while the sapphire forms in granitic pegmatites or corundum syenites.

Every sapphire mine produces a wide range of quality, and origin is not a guarantee of quality. For sapphire, Kashmir receives the highest premium, although Burma, Sri Lanka, and Madagascar also produce large quantities of fine quality gems.

The cost of natural sapphires varies depending on their color, clarity, size, cut, and overall quality. Sapphires that are completely untreated are worth far more than those that have been treated. Geographical origin also has a major impact on price. For most gems of one carat or more, an independent report from a respected laboratory such as American Gemological Laboratories (AGL), Gem Research Swisslab (GRS), GIA, Gübelin, Lotus Gemology, or SSEF, is often required by buyers before they will make a purchase.

Gemstone color can be described in terms of hue, saturation, and tone. Hue is commonly understood as the "color" of the gemstone. Saturation refers to the vividness or brightness of the hue, and tone is the lightness to darkness of the hue. Blue sapphire exists in various mixtures of its primary (blue) and secondary hues, various tonal levels (shades) and at various levels of saturation (vividness).

Blue sapphires are evaluated based upon the purity of their blue hue. Violet, and green are the most common secondary hues found in blue sapphires. The highest prices are paid for gems that are pure blue and of vivid saturation. Gems that are of lower saturation, or are too dark or too light in tone are of less value. However, color preferences are a personal taste, like a flavor of ice cream 

The Logan sapphire in the National Museum of Natural History, in Washington, D.C., is one of the largest faceted gem-quality blue sapphires in existence. The 422.66-ct Siren of Serendip in the Houston Museum of Natural Science is another stunning example of a Sri Lankan sapphire on public display.

Sapphires in colors other than blue are called "fancy" or "parti colored" sapphires.

Fancy sapphires are often found in yellow, orange, green, brown, purple and violet hues.

Particolored sapphires are those stones which exhibit two or more colors within a single stone. Australia is the largest source of particolored sapphires; they are not commonly used in mainstream jewelry and remain relatively unknown. Particolored sapphires cannot be created synthetically and only occur naturally.

Colorless sapphires have historically been used as diamond substitutes in jewelry.

Pink sapphires occur in shades from light to dark pink, and deepen in color as the quantity of chromium increases. The deeper the pink color, the higher their monetary value. In the United States, a minimum color saturation must be met to be called a ruby, otherwise the stone is referred to as a "pink sapphire".

"Padparadscha" is a delicate, light to medium toned, pink-orange to orange-pink hued corundum, originally found in Sri Lanka, but also found in deposits in Vietnam and parts of East Africa. Padparadscha sapphires are rare; the rarest of all is the totally natural variety, with no sign of artificial treatment.

The name is derived from the Sanskrit "padma ranga" (padma = lotus; ranga = color), a color akin to the lotus flower ("Nelumbo nucifera").

Among the fancy (non-blue) sapphires, natural padparadscha fetch the highest prices. Since 2001, more sapphires of this color have appeared on the market as a result of artificial lattice diffusion of beryllium.

A "star sapphire" is a type of sapphire that exhibits a star-like phenomenon known as asterism; red stones are known as "star rubies". Star sapphires contain intersecting needle-like inclusions following the underlying crystal structure that causes the appearance of a six-rayed "star"-shaped pattern when viewed with a single overhead light source. The inclusion is often the mineral rutile, a mineral composed primarily of titanium dioxide. The stones are cut "en cabochon", typically with the center of the star near the top of the dome. Occasionally, twelve-rayed stars are found, typically because two different sets of inclusions are found within the same stone, such as a combination of fine needles of rutile with small platelets of hematite; the first results in a whitish star and the second results in a golden-colored star. During crystallization, the two types of inclusions become preferentially oriented in different directions within the crystal, thereby forming two six-rayed stars that are superimposed upon each other to form a twelve-rayed star. Misshapen stars or 12-rayed stars may also form as a result of twinning.
The inclusions can alternatively produce a "cat's eye" effect if the girdle plane of the cabochon is oriented parallel to the crystal's c-axis rather than perpendicular to it. To get a cat's eye, the planes of exsolved inclusions must be extremely uniform and tightly packed. If the dome is oriented in between these two directions, an 'off-center' star will be visible, offset away from the high point of the dome.

At 1404.49 carats, The Star of Adam is claimed to be the largest blue star sapphire, but whenever such claims are made, one should be careful not to equate size with quality or value. The gem was mined in the city of Ratnapura, southern Sri Lanka. The Black Star of Queensland, the second largest star sapphire in the world, weighs 733 carats. The Star of India mined in Sri Lanka and weighing 563.4 carats is thought to be the third-largest star sapphire, and is currently on display at the American Museum of Natural History in New York City. The 182-carat Star of Bombay, mined in Sri Lanka and located in the National Museum of Natural History in Washington, D.C., is another example of a large blue star sapphire. The value of a star sapphire depends not only on the weight of the stone, but also the body color, visibility, and intensity of the asterism. A common mistake made by novices is to value stones with strong stars the highest. In fact, the color of the stone has more impact on the value than the visibility of the star. Since more transparent stones tend to have better colors, the most expensive star stones are semi-transparent "glass body" stones with vivid colors.

Large rubies and sapphires of poor transparency are frequently used with suspect appraisals that vastly overstate their value. This was the case of the "Life and Pride of America Star Sapphire". Circa 1985, one Roy Whetstine claimed to have bought the 1905-ct stone for $10 at the Tucson gem show. But a reporter discovered that L.A. Ward of Fallbrook, CA, who appraised it at the whopping price of $1200/ct, had appraised another stone of the exact same weight several years before Whetstine claimed to have found it.

Whenever someone sees huge appraisals on oversized rubies or sapphires, caution is the watchword. Generally speaking, the biggest gem quality sapphires are under 500 carats. When it comes to rubies, divide by ten.

Bangkok-based Lotus Gemology maintains an updated listing of world auction records of ruby, sapphire and spinel. As of November 2019, no sapphire has ever sold at auction for more than $17,295,796.

A rare variety of natural sapphire, known as color-change sapphire, exhibits different colors in different light. Color change sapphires are blue in outdoor light and purple under incandescent indoor light, or green to gray-green in daylight and pink to reddish-violet in incandescent light. Color change sapphires come from a variety of locations, including Madagascar, Myanmar, Sri Lanka and Tanzania. Two types exist. The first features the chromium chromophore that creates the red color of ruby, combined with the iron + titanium chromophore that produces the blue color in sapphire. A more rare type, which comes from the Mogok area of Myanmar, features a vanadium chromophore, the same as is used in Verneuil synthetic color-change sapphire.

Virtually all gemstones that show the "alexandrite effect" (color change; a.k.a. 'metamerism') show similar absorption/transmission features in the visible spectrum. This is an absorption band in the yellow (~590 nm), along with valleys of transmission in the blue-green and red. Thus the color one sees depends on the spectral composition of the light source. Daylight is relatively balanced in its spectral power distribution (SPD) and since the human eye is most sensitive to green light, the balance is tipped to the green side. However incandescent light (including candle light) is heavily tilted to the red end of the spectrum, thus tipping the balance to red.

Color-change sapphires colored by the Cr + Fe/Ti chromophores generally change from blue or violetish blue to violet or purple. Those colored by the V chromophore can show a more pronounced change, moving from blue-green to purple.

Certain synthetic color-change sapphires have a similar color change to the natural gemstone alexandrite and they are sometimes marketed as "alexandrium" or "synthetic alexandrite". However, the latter term is a misnomer: synthetic color-change sapphires are, technically, not synthetic alexandrites but rather alexandrite "simulants". This is because genuine alexandrite is a variety of chrysoberyl: not sapphire, but an entirely different mineral.

Rubies are corundum with a dominant red body color. This is generally caused by traces of chromium (Cr) substituting for the (Al) ion in the corundum structure. The color can be modified by both iron and trapped hole color centers.

Unlike localized ("intra-atomic") absorption of light which causes color for chromium and vanadium impurities, blue color in sapphires comes from intervalence charge transfer, which is the transfer of an electron from one transition-metal ion to another via the conduction or valence band. The iron can take the form Fe or Fe, while titanium generally takes the form Ti. If Fe and Ti ions are substituted for Al, localized areas of charge imbalance are created. An electron transfer from Fe and Ti can cause a change in the valence state of both. Because of the valence change there is a specific change in energy for the electron, and electromagnetic energy is absorbed. The wavelength of the energy absorbed corresponds to yellow light. When this light is subtracted from incident white light, the complementary color blue results. Sometimes when atomic spacing is different in different directions there is resulting blue-green dichroism.

Purple sapphires contain trace amounts of chromium and iron plus titanium and come in a variety of shades. Corundum that contains extremely low levels of chromophores is near colorless. Completely colorless corundum generally does not exist in nature. If trace amounts of iron are present, a very pale yellow to green color may be seen. However, if both titanium and iron impurities are present together, and in the correct valence states, the result is a blue color.

Intervalence charge transfer is a process that produces a strong colored appearance at a low percentage of impurity. While at least 1% chromium must be present in corundum before the deep red ruby color is seen, sapphire blue is apparent with the presence of only 0.01% of titanium and iron.

The most complete description of the causes of color in corundum extant can be found in Chapter 4 of Ruby & Sapphire: A Gemologist's Guide (chapter authored by John Emmett, Emily Dubinsky and Richard Hughes).

Sapphires can be treated by several methods to enhance and improve their clarity and color. It is common practice to heat natural sapphires to improve or enhance their appearance. This is done by heating the sapphires in furnaces to temperatures between for several hours, or even weeks at a time. Different atmospheres may be used. Upon heating, the stone becomes more blue in color, but loses some of the rutile inclusions (silk). When high temperatures (1400 °C+) are used, exsolved rutile silk is dissolved and it becomes clear under magnification. The titanium from the rutile enters solid solution and thus creates with iron the blue color The inclusions in natural stones are easily seen with a jeweler's loupe. Evidence of sapphire and other gemstones being subjected to heating goes back at least to Roman times. Un-heated natural stones are somewhat rare and will often be sold accompanied by a certificate from an independent gemological laboratory attesting to "no evidence of heat treatment".
Yogo sapphires do not need heat treating because their cornflower blue color is attractive out of the ground; they are generally free of inclusions, and have high uniform clarity. When Intergem Limited began marketing the Yogo in the 1980s as the world's only guaranteed untreated sapphire, heat treatment was not commonly disclosed; by the late 1980s, heat treatment became a major issue. At that time, much of all the world's sapphires were being heated to enhance their natural color. Intergem's marketing of guaranteed untreated Yogos set them against many in the gem industry. This issue appeared as a front-page story in the "Wall Street Journal" on 29 August 1984 in an article by Bill Richards, "Carats and Schticks: Sapphire Marketer Upsets The Gem Industry". However, the biggest problem the Yogo mine faced was not competition from heated sapphires, but the fact that the Yogo stones could never produce quantities of sapphire above one carat after faceting. As a result, it has remained a niche product, with a market that largely exists in the US.

Lattice ('bulk') diffusion treatments are used to add impurities to the sapphire to enhance color. This process was originally developed and patented by Linde Air division of Union Carbide and involved diffusing titanium into synthetic sapphire to even out the blue color. It was later applied to natural sapphire. Today, titanium diffusion often uses a synthetic colorless sapphire base. The color layer created by titanium diffusion is extremely thin (less than 0.5 mm). Thus repolishing can and does produce slight to significant loss of color. Chromium diffusion has been attempted, but was abandoned due to the slow diffusion rates of chromium in corundum.

In the year 2000, beryllium diffused "padparadscha" colored sapphires entered the market. Typically beryllium is diffused into a sapphire under very high heat, just below the melting point of the sapphire. Initially ("c." 2000) orange sapphires were created, although now the process has been advanced and many colors of sapphire are often treated with beryllium. Due to the small size of the beryllium ion, the color penetration is far greater than with titanium diffusion. In some cases, it may penetrate the entire stone. Beryllium-diffused orange sapphires may be difficult to detect, requiring advanced chemical analysis by gemological labs ("e.g.", Gübelin, SSEF, GIA, American Gemological Laboratories (AGL), Lotus Gemology.

According to United States Federal Trade Commission guidelines, disclosure is required of any mode of enhancement that has a significant effect on the gem's value.

There are several ways of treating sapphire. Heat-treatment in a reducing or oxidizing atmosphere (but without the use of any other added impurities) is commonly used to improve the color of sapphires, and this process is sometimes known as "heating only" in the gem trade. In contrast, however, heat treatment combined with the deliberate addition of certain specific impurities (e.g. beryllium, titanium, iron, chromium or nickel, which are absorbed into the crystal structure of the sapphire) is also commonly performed, and this process can be known as "diffusion" in the gem trade. However, despite what the terms "heating only" and "diffusion" might suggest, both of these categories of treatment actually involve diffusion processes.

The most complete description of corundum treatments extant can be found in Chapter 6 of Ruby & Sapphire: A Gemologist's Guide (chapter authored by John Emmett, Richard Hughes and Troy R. Douthit).

Sapphires are mined from alluvial deposits or from primary underground workings. Commercial mining locations for sapphire and ruby include (but are not limited to) the following countries: Afghanistan, Australia, Myanmar/Burma, Cambodia, China, Colombia, India, Kenya, Laos, Madagascar, Malawi, Nepal, Nigeria, Pakistan, Sri Lanka, Tajikistan, Tanzania, Thailand, United States, and Vietnam. Sapphires from different geographic locations may have different appearances or chemical-impurity concentrations, and tend to contain different types of microscopic inclusions. Because of this, sapphires can be divided into three broad categories: classic metamorphic, non-classic metamorphic or magmatic, and classic magmatic.

Sapphires from certain locations, or of certain categories, may be more commercially appealing than others, particularly classic metamorphic sapphires from Kashmir, Burma, or Sri Lanka that have not been subjected to heat-treatment.

The Logan sapphire, the Star of India, The Star of Adam and the Star of Bombay originate from Sri Lankan mines. Madagascar is the world leader in sapphire production (as of 2007) specifically its deposits in and around the town of Ilakaka. Prior to the opening of the Ilakaka mines, Australia was the largest producer of sapphires (such as in 1987). In 1991 a new source of sapphires was discovered in Andranondambo, southern Madagascar. That area has been exploited for its sapphires started in 1993, but it was practically abandoned just a few years later—because of the difficulties in recovering sapphires in their bedrock.

In North America, sapphires have been mined mostly from deposits in Montana: fancies along the Missouri River near Helena, Montana, Dry Cottonwood Creek near Deer Lodge, Montana, and Rock Creek near Philipsburg, Montana. Fine blue Yogo sapphires are found at Yogo Gulch west of Lewistown, Montana. A few gem-grade sapphires and rubies have also been found in the area of Franklin, North Carolina.

The sapphire deposits of Kashmir are well known in the gem industry, although their peak production took place in a relatively short period at the end of the nineteenth and early twentieth centuries. They have a superior vivid blue hue, coupled with a mysterious and almost sleepy quality, described by some gem enthusiasts as ‘blue velvet”. Kashmir-origin contributes meaningfully to the value of a sapphire, and most corundum of Kashmir origin can be readily identified by its characteristic silky appearance and exceptional hue. The unique blue appears lustrous under any kind of light, unlike non-Kashmir sapphires which may appear purplish or grayish in comparison. Sotheby's has been in the forefront overseeing record-breaking sales of Kashmir sapphires worldwide. In October 2014, Sotheby's Hong Kong achieved consecutive per-carat price records for Kashmir sapphires – first with the 12.00 carat Cartier sapphire ring at US$193,975 per carat, then with a 17.16 carat sapphire at US$236,404, and again in June 2015 when the per-carat auction record was set at US$240,205. At present, the world record price-per-carat for sapphire at auction is held by a sapphire from Kashmir in a ring, which sold in October 2015 for approximately US$242,000 per carat (HK$52,280,000 in total, including buyer's premium, or more than US$6.74 million).

Antoine Lavoisier was the first to synthesize sapphire in 1782, by using an oxygen blowpipe to heat a sample of alumina upon a bed of burning charcoal. The result was a colourless crystal of corundum (also known as white sapphire). However, it was not until 1820 that Edward Daniel Clarke identified the result of the experiment as corundum.

Other significant developments in the synthesis of sapphire were achieved by: Jacques-Joseph Ébelmen, Edmond Frémy, Charles Feil, and Auguste Verneuil. 

After Verneuil published details of his more efficient method of flame fusion (now known as the Verneuil process) in 1902, the industrial production of synthetic corundum could begin. In the Verneuil process, fine alumina powder is added to an oxyhydrogen flame, and this is directed downward against a ceramic pedestal. Following the successful synthesis of ruby, Verneuil focussed his efforts on sapphire. Synthesis of blue sapphire came in 1909, after chemical analyses of sapphire suggested to Verneuil that iron and titanium were the cause of the blue color. Verneuil patented the process of producing synthetic blue sapphire in 1911. 

The key to the process is that the alumina powder does not melt as it falls through the flame. Instead it forms a sinter cone on the pedestal. When the tip of that cone reaches the hottest part of the flame, the tip melts. Thus the crystal growth is started from a tiny point, ensuring minimal strain.

Next, more oxygen is added to the flame, causing it to burn slightly hotter. This expands the growing crystal laterally. At the same time, the pedestal is lowered at the same rate that the crystal grows vertically. The alumina in the flame is slowly deposited, creating a teardrop shaped "boule" of sapphire material. This step is continued until the desired size is reached, the flame is shut off and the crystal cools. The now elongated crystal contains a lot of strain due to the high thermal gradient between the flame and surrounding air. To release this strain, the now finger-shaped crystal will be tapped with a chisel to split it into two halves.

Due to the vertical layered growth of the crystal and the curved upper growth surface (which starts from a drop), the crystals will display curved growth lines following the top surface of the boule. This is in contrast to natural corundum crystals, which feature angular growth lines expanding from a single point and following the planar crystal faces.

Chemical dopants can be added to create artificial versions of the ruby, and all the other natural colors of sapphire, and in addition, other colors never seen in geological samples. Artificial sapphire material is identical to natural sapphire, except it can be made without the flaws that are found in natural stones. The disadvantage of Verneuil process is that the grown crystals have high internal strains. Many methods of manufacturing sapphire today are variations of the Czochralski process, which was invented in 1916 by Polish chemist Jan Czochralski. In this process, a tiny sapphire seed crystal is dipped into a crucible made of the precious metal iridium or molybdenum, containing molten alumina, and then slowly withdrawn upward at a rate of 1 to 100 mm per hour. The alumina crystallizes on the end, creating long carrot-shaped boules of large size up to 200 kg in mass.

Synthetic sapphire is also produced industrially from agglomerated aluminum oxide, sintered and fused (such as by hot isostatic pressing) in an inert atmosphere, yielding a transparent but slightly porous polycrystalline product.

In 2003, the world's production of synthetic sapphire was 250 tons (1.25 × 10 carats), mostly by the United States and Russia. The availability of cheap synthetic sapphire unlocked many industrial uses for this unique material.

Synthetic sapphire—sometimes referred to as "sapphire glass"—is commonly used as a window material, because it is both highly transparent to wavelengths of light between 150 nm (UV) and 5500 nm (IR) (the visible spectrum extends about 380 nm to 750 nm), and extraordinarily scratch-resistant.

The key benefits of sapphire windows are:
Some sapphire-glass windows are made from pure sapphire boules that have been grown in a specific crystal orientation, typically along the optical axis, the c-axis, for minimum birefringence for the application.

The boules are sliced up into the desired window thickness and finally polished to the desired surface finish. Sapphire optical windows can be polished to a wide range of surface finishes due to its crystal structure and its hardness. The surface finishes of optical windows are normally called out by the scratch-dig specifications in accordance with the globally adopted MIL-O-13830 specification.

The sapphire windows are used in both high pressure and vacuum chambers for spectroscopy, crystals in various watches, and windows in grocery store barcode scanners since the material's exceptional hardness and toughness makes it very resistant to scratching.

It is used for end windows on some high-powered laser tubes as its wide-band transparency and thermal conductivity allow it to handle very high power densities in the infra-red or UV spectrum without degrading due to heating.

Along with zirconia and aluminum oxynitride, synthetic sapphire is used for shatter resistant windows in armored vehicles and various military body armor suits, in association with composites.

One type of xenon arc lamp – originally called the "Cermax" and now known generically as the "ceramic body xenon lamp" – uses sapphire crystal output windows. This product tolerates higher thermal loads and thus higher output powers when compared with conventional Xe lamps with pure silica window.

Thin sapphire wafers were the first successful use of an insulating substrate upon which to deposit silicon to make the integrated circuits known as silicon on sapphire or "SOS"; now other substrates can also be used for the class of circuits known more generally as silicon on insulator. Besides its excellent electrical insulating properties, sapphire has high thermal conductivity. CMOS chips on sapphire are especially useful for high-power radio-frequency (RF) applications such as those found in cellular telephones, public-safety band radios, and satellite communication systems. "SOS" also allows for the monolithic integration of both digital and analog circuitry all on one IC chip, and the construction of extremely low power circuits.

In one process, after single crystal sapphire boules are grown, they are core-drilled into cylindrical rods, and wafers are then sliced from these cores.

Wafers of single-crystal sapphire are also used in the semiconductor industry as substrates for the growth of devices based on gallium nitride (GaN). The use of sapphire significantly reduces the cost, because it has about one-seventh the cost of germanium. Gallium nitride on sapphire is commonly used in blue light-emitting diodes (LEDs).

The first laser was made with a rod of synthetic ruby. Titanium-sapphire lasers are popular due to their relatively rare capacity to be tuned to various wavelengths in the red and near-infrared region of the electromagnetic spectrum. They can also be easily mode-locked. In these lasers a synthetically produced sapphire crystal with chromium or titanium impurities is irradiated with intense light from a special lamp, or another laser, to create stimulated emission.

Monocrystalline sapphire is fairly biocompatible and the exceptionally low wear of sapphire–metal pairs has led to the introduction (in Ukraine) of sapphire monocrystals for hip 
joint endoprostheses.


Extensive tables listing over a hundred important and famous rubies and sapphires can be found in Chapter 10 of Ruby & Sapphire: A Gemologist's Guide.



 


</doc>
<doc id="29471" url="https://en.wikipedia.org/wiki?curid=29471" title="Slack voice">
Slack voice

Slack voice (or lax voice) is the pronunciation of consonant or vowels with a glottal opening slightly wider than that occurring in modal voice. Such sounds are often referred to informally as lenis or half-voiced in the case of consonants. In some Chinese varieties, such as Wu, and in a few Austronesian languages, the 'intermediate' phonation of slack stops confuses listeners of languages without these distinctions, so that different transcription systems may use or for the same consonant. In Xhosa, slack-voiced consonants have usually been transcribed as breathy voice. Although the IPA has no dedicated diacritic for slack voice, the voiceless diacritic (the under-ring) may be used with a voiced consonant letter, though this convention is also used for partially voiced consonants in languages such as English.

Wu Chinese "muddy" consonants are slack voice word-initially, the primary effect of which is a slightly breathy quality of the following vowel.

Javanese contrasts slack and stiff voiced bilabial, dental, retroflex, and velar stops.

Parauk contrasts slack voicing in its vowels. The contrast is between "slightly stiff" and "slightly breathy" vowels; the first are between modal and stiff voice, while the latter are captured by slack voice.


</doc>
<doc id="29472" url="https://en.wikipedia.org/wiki?curid=29472" title="SADC">
SADC

SADC may stand for:



</doc>
<doc id="29473" url="https://en.wikipedia.org/wiki?curid=29473" title="Salvation">
Salvation

Salvation (; ; ; ) is being saved or protected from harm or being saved or delivered from a dire situation. In religion, salvation is the saving of the soul from sin and its consequences.

The academic study of salvation is called soteriology.

In religion, salvation is the saving of the soul from sin and its consequences. It may also be called "deliverance" or "redemption" from sin and its effects. Historically, salvation is considered to be caused only by the grace of God (i.e. unmerited and unearned). Religions often emphasize that man is a sinner by nature and that the penalty of sin is death (physical death, spiritual death: spiritual separation from God and eternal punishment in hell). Therefore, God became a man to save anyone who will believe that He died, buried, and at the third day He rose again to save sinners.

In contemporary Judaism, redemption (Hebrew "ge'ulah"), refers to God redeeming the people of Israel from their various exiles. This includes the final redemption from the present exile.

Judaism holds that adherents do not need personal salvation as Christians believe. Jews do not subscribe to the doctrine of original sin. Instead, they place a high value on individual morality as defined in the law of God — embodied in what Jews know as the Torah or The Law, given to Moses by God on biblical Mount Sinai.

In Judaism, salvation is closely related to the idea of redemption, a saving from the states or circumstances that destroy the value of human existence. God, as the universal spirit and Creator of the World, is the source of all salvation for humanity, provided an individual honours God by observing his precepts. So redemption or salvation depends on the individual. Judaism stresses that salvation cannot be obtained through anyone else or by just invoking a deity or believing in any outside power or influence.

When examining Jewish intellectual sources throughout history, there is clearly a spectrum of opinions regarding death versus the afterlife. Possibly an over-simplification, one source says salvation can be achieved in the following manner: Live a holy and righteous life dedicated to Yahweh, the God of Creation. Fast, worship, and celebrate during the appropriate holidays.
By origin and nature, Judaism is an ethnic religion. Therefore, salvation has been primarily conceived in terms of the destiny of Israel as the elect people of Yahweh (often referred to as “the Lord”), the God of Israel. In the biblical text of Psalms, there is a description of death, when people go into the earth or the "realm of the dead" and cannot praise God. The first reference to resurrection is collective in Ezekiel's vision of the dry bones, when all the Israelites in exile will be resurrected. There is a reference to individual resurrection in the Book of Daniel (165 BCE), the last book of the Hebrew Bible. It was not until the 2nd century BCE that there arose a belief in an afterlife, in which the dead would be resurrected and undergo divine judgment. Before that time, the individual had to be content that his posterity continued within the holy nation.

The salvation of the individual Jew was connected to the salvation of the entire people. This belief stemmed directly from the teachings of the Torah. In the Torah, God taught his people sanctification of the individual. However, he also expected them to function together (spiritually) and be accountable to one another. The concept of salvation was tied to that of restoration for Israel.

According to the Gospel of John, Jesus said "salvation is from the Jews." This is in accordance with the Jewish concept of salvation, and is a possible reference to Isaiah 49:6.

Christianity’s primary premise is that the incarnation and death of Jesus Christ formed the climax of a divine plan for humanity’s salvation. This plan was conceived by God consequent on the Fall of Adam, the progenitor of the human race, and it would be completed at the Last Judgment, when the Second Coming of Christ would mark the catastrophic end of the world.

For Christianity, salvation is only possible through Jesus Christ. Christians believe that Jesus' death on the cross was the once-for-all sacrifice that atoned for the sin of humanity.

The Christian religion, though not the exclusive possessor of the idea of redemption, has given to it a special definiteness and a dominant position. Taken in its widest sense, as deliverance from dangers and ills in general, most religions teach some form of it. It assumes an important position, however, only when the ills in question form part of a great system against which human power is helpless.
According to Christian belief, sin as the human predicament is considered to be universal. For example, in the Apostle Paul declared everyone to be under sin—Jew and Gentile alike. Salvation is made possible by the life, death, and resurrection of Jesus, which in the context of salvation is referred to as the "atonement". Christian soteriology ranges from exclusive salvation to universal reconciliation concepts. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.

Variant views on salvation are among the main fault lines dividing the various Christian denominations, both between Roman Catholicism and Protestantism and within Protestantism, notably in the Calvinist–Arminian debate, and the fault lines include conflicting definitions of depravity, predestination, atonement, but most pointedly justification.

Salvation, according to most denominations, is believed to be a process that begins when a person first becomes a Christian, continues through that person's life, and is completed when they stand before Christ in judgment. Therefore, according to Catholic apologist James Akin, the faithful Christian can say in faith and hope, "I "have been" saved; I "am being" saved; and I "will be" saved."

Christian salvation concepts are varied and complicated by certain theological concepts, traditional beliefs, and dogmas. Scripture is subject to individual and ecclesiastical interpretations. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.

The purpose of salvation is debated, but in general most Christian theologians agree that God devised and implemented his plan of salvation because he loves them and regards human beings as his children. Since human existence on Earth is said to be "given to sin", salvation also has connotations that deal with the liberation of human beings from sin, and the suffering associated with the punishment of sin—i.e., "the wages of sin are death."

Christians believe that salvation depends on the grace of God. Stagg writes that a fact assumed throughout the Bible is that humanity is in, "serious trouble from which we need deliverance…. The fact of sin as the human predicament is implied in the mission of Jesus, and it is explicitly affirmed in that connection". By its nature, salvation must answer to the plight of humankind as it actually is. Each individual's plight as sinner is the result of a fatal choice involving the whole person in bondage, guilt, estrangement, and death. Therefore, salvation must be concerned with the total person. "It must offer redemption from bondage, forgiveness for guilt, reconciliation for estrangement, renewal for the marred image of God".

According to doctrine of the Latter Day Saint movement, the plan of salvation is a plan that God created to save, redeem, and exalt humankind. The elements of this plan are drawn from various sources, including the Bible, Book of Mormon, Doctrine & Covenants, Pearl of Great Price, and numerous statements made by the leadership of The Church of Jesus Christ of Latter-day Saints (LDS Church). The first appearance of the graphical representation of the plan of salvation is in the 1952 missionary manual entitled "A Systematic Program for Teaching the Gospel."

In Islam, salvation refers to the eventual entrance to Paradise. Islam teaches that people who die disbelieving in God do not receive salvation. It also teaches that non-Muslims who die believing in the God but disbelieving in his message (Islam), are left to his will. Those who die believing in the One God and his message (Islam) receive salvation.

Narrated Anas that Muhammad said,
Islam teaches that all who enter into Islam must remain so in order to receive salvation.
For those who have not been granted Islam or to whom the message has not been brought;
Belief in the “One God”, also known as the "Tawhid" (التَوْحيدْ) in Arabic, consists of two parts (or principles):

Islam also stresses that in order to gain salvation, one must also avoid sinning along with performing good deeds. Islam acknowledges the inclination of humanity towards sin. Therefore, Muslims are constantly commanded to seek God's forgiveness and repent. Islam teaches that no one can gain salvation simply by virtue of their belief or deeds, instead it is the Mercy of God, which merits them salvation. However, this repentance must not be used to sin any further. Islam teaches that God is Merciful.

Islam describes a true believer to have Love of God and Fear of God. Islam also teaches that every person is responsible for their own sins. The Quran states;

Al-Agharr al-Muzani, a companion of Mohammad, reported that Ibn 'Umar stated to him that Mohammad said,

Sin in Islam is not a state, but an action (a bad deed); Islam teaches that a child is born sinless, regardless of the belief of his parents, dies a Muslim; he enters heaven, and does not enter hell. 

There are acts of worship that Islam teaches to be mandatory. Islam is built on five principles. Narrated Ibn 'Umar that Muhammad said,
Not performing the mandatory acts of worship may deprive Muslims of the chance of salvation.

Hinduism, Buddhism, Jainism and Sikhism share certain key concepts, which are interpreted differently by different groups and individuals. In these religions one is not liberated from sin and its consequences, but from the "saṃsāra" (cycle of rebirth) perpetuated by passions and delusions and its resulting karma. They differ however on the exact nature of this liberation. Salvation is called "moksha" or "mukti" which mean liberation and release respectively. This state and the conditions considered necessary for its realization is described in early texts of Indian religion such as the Upanishads and the Pāli Canon, and later texts such the Yoga Sutras of Patanjali and the Vedanta tradition. "Moksha" can be attained by sādhanā, literally "means of accomplishing something". It includes a variety of disciplines, such as yoga and meditation.

Nirvana is the profound peace of mind that is acquired with moksha (liberation). In Buddhism and Jainism, it is the state of being free from suffering. In Hindu philosophy, it is union with the Brahman (Supreme Being). The word literally means "blown out" (as in a candle) and refers, in the Buddhist context, to the blowing out of the fires of desire, aversion, and delusion, and the imperturbable stillness of mind acquired thereafter.

In Theravada Buddhism the emphasis is on one's own liberation from samsara. The Mahayana traditions emphasize the bodhisattva path, in which "each Buddha and Bodhisattva is a redeemer", assisting the Buddhist in seeking to achieve the redemptive state. The assistance rendered is a form of self-sacrifice on the part of the teachers, who would presumably be able to achieve total detachment from worldly concerns, but have instead chosen to remain engaged in the material world to the degree that this is necessary to assist others in achieving such detachment.

In Jainism, "salvation", "moksa" and "nirvana" are one and the same. When a soul ("atman") achieves moksa, it is released from the cycle of births and deaths, and achieves its pure self. It then becomes a "siddha" (literally means one who has accomplished his ultimate objective). Attaining Moksa requires annihilation of all "karmas", good and bad, because if karma is left, it must bear fruit.





</doc>
<doc id="29475" url="https://en.wikipedia.org/wiki?curid=29475" title="Lockheed S-3 Viking">
Lockheed S-3 Viking

The Lockheed S-3 Viking is a 4-crew, twin-engine turbofan-powered jet aircraft that was used by the U.S. Navy (USN) primarily for anti-submarine warfare. In the late 1990s, the S-3B's mission focus shifted to surface warfare and aerial refueling. The Viking also provided electronic warfare and surface surveillance capabilities to a carrier battle group. A carrier-based, subsonic, all-weather, long-range, multi-mission aircraft; it carried automated weapon systems and was capable of extended missions with in-flight refueling. Because of its characteristic sound, it was nicknamed the "War Hoover" after the vacuum cleaner brand.

The S-3 was phased out from front-line fleet service aboard aircraft carriers in January 2009. Its missions being taken over by aircraft like the P-3C Orion, Sikorsky SH-60 Seahawk and Boeing F/A-18E/F Super Hornet. Several aircraft were flown by Air Test and Evaluation Squadron Thirty (VX-30) at Naval Base Ventura County / NAS Point Mugu, California, for range clearance and surveillance operations on the NAVAIR Point Mugu Range until 2016 and one S-3 is operated by the National Aeronautics and Space Administration (NASA) at the NASA Glenn Research Center.

In the mid-1960s, the USN developed the VSX (Heavier-than-air, Anti-submarine, Experimental) requirement for a replacement for the piston-engined Grumman S-2 Tracker as an anti-submarine aircraft to fly off aircraft carriers. In August 1968, a team led by Lockheed and a Convair/Grumman team were asked to further develop their proposals to meet this requirement. Lockheed recognised that it had little experience in designing carrier based aircraft, so Ling-Temco-Vought (LTV) was brought into the team, being responsible for the folding wings and tail, the engine nacelles, and the landing gear, which was derived from LTV A-7 Corsair II (nose) and Vought F-8 Crusader (main). Sperry Univac Federal Systems was assigned the task of developing the aircraft's onboard computers which integrated input from sensors and sonobuoys.

On 4 August 1969, Lockheed's design was selected as the winner of the contest and 8 prototypes, designated YS-3A were ordered. The first prototype was flown on 21 January 1972 by military test pilot John Christiansen, and the S-3 entered service in 1974. During the production run from 1974 to 1978, a total of 186 S-3As have been built. The majority of the surviving S-3As were later upgraded to the S-3B variant, with 16 aircraft converted into ES-3A Shadow electronic intelligence (ELINT) collection aircraft.

The S-3 is a conventional monoplane with a cantilever shoulder wing, very slightly swept with a leading edge angle of 15° and an almost straight trailing edge. Its 2 GE TF-34 high-bypass turbofan engines mounted in nacelles under the wings provide excellent fuel efficiency, giving the Viking the required long range and endurance, while maintaining docile engine-out characteristics.
The aircraft can seat 4 crew members (3 officers and 1 enlisted) with pilot and copilot/tactical coordinator (COTAC) in the front of the cockpit and the tactical coordinator (TACCO) and sensor operator (SENSO) in the back. Entry is via a hatch/ladder folding down out of the lower starboard side of the fuselage behind the cockpit, in between the rear and front seats on the starboard side. When the aircraft's anti-submarine warfare (ASW) role ended in the late 1990s, the enlisted SENSOs were removed from the crew. In tanker crew configuration, the S-3B typically flew with a pilot and co-pilot/COTAC. The wing is fitted with leading edge and Fowler flaps. Spoilers are fitted to both the upper and the lower surfaces of the wings. All control surfaces are actuated by dual hydraulically boosted irreversible systems. In the event of dual hydraulic failures, an Emergency Flight Control System (EFCS) permits manual control with greatly increased stick forces and reduced control authority.

Unlike many tactical jets which required ground service equipment, the S-3 was equipped with an auxiliary power unit (APU) and capable of unassisted starts. The aircraft's original APU could provide only minimal electric power and pressurized air for both aircraft cooling and for the engines' pneumatic starters. A newer, more powerful APU could provide full electrical service to the aircraft. The APU itself was started from a hydraulic accumulator by pulling a handle in the cockpit. The APU accumulator was fed from the primary hydraulic system, but could also be pumped up manually (with much effort) from the cockpit.

All crew members sit on forward-facing, upward-firing Douglas Escapac zero-zero ejection seats. In "group eject" mode, initiating ejection from either of the front seat ejects the entire crew in sequence, with the back seats ejecting 0.5 seconds before the front in order to provide safe separation (this was to prevent the pilots, who were more aware of what was happening outside the aircraft from ejecting without the rest of the crew, or being forced to delay ejection to order the crew to eject in an emergency; ejection from either rear seat would not eject the pilots, who had to initiate their own ejections, to prevent loss of the aircraft if a rear crewmember ejected prematurely. If a pilot ejected prematurely, the plane was lost anyway, and automatic ejection prevented the crew from crashing with a pilot-less aircraft before they were aware of what had happened). The rear seats are capable of self ejection and the ejection sequence includes a pyrotechnic charge that stows the rear keyboard trays out of the occupants' way immediately before ejection. Safe ejection requires the seats to be weighted in pairs and when flying with a single crewman in the back the unoccupied seat is fitted with ballast.

At the time it entered the fleet, the S-3 introduced an unprecedented level of systems integration. Previous ASW aircraft like the Lockheed P-3 Orion and S-3's predecessor, the Grumman S-2 Tracker, featured separate instrumentation and controls for each sensor system. Sensor operators often monitored paper traces, using mechanical calipers to make precise measurements and annotating data by writing on the scrolling paper. Beginning with the S-3, all sensor systems were integrated through a single General Purpose Digital Computer (GPDC). Each crew station had its own display, the co-pilot/COTAC, TACCO and SENSO displays were Multi-Purpose Displays (MPD) capable of displaying data from any of a number of systems. This new level of integration allowed the crew to consult with each other by examining the same data at multiple stations simultaneously, to manage workload by assigning responsibility for a given sensor from one station to another and to easily combine clues from each sensor to classify faint targets. Because of this, the 4-crew S-3 was considered roughly equivalent in capability to the much larger P-3 with a crew of 12.

The aircraft has two underwing hardpoints that can be used to carry fuel tanks, general purpose and cluster bombs, missiles, rockets, and storage pods. It also has four internal bomb bay stations that can be used to carry general-purpose bombs, aerial torpedoes, and special stores (B57 and B61 nuclear weapons). Fifty-nine sonobuoys are carried, as well as a dedicated Search and Rescue (SAR) chute. The S-3 is fitted with the ALE-39 countermeasure system and can carry up to 90 rounds of chaff, flares, and expendable jammers (or a combination of all) in three dispensers. A retractable magnetic anomaly detector (MAD) Boom is fitted in the tail.

In the late 1990s, the S-3B's role was changed from anti-submarine warfare (ASW) to anti-surface warfare (ASuW). At that time, the MAD Boom was removed, along with several hundred pounds of submarine detection electronics. With no remaining sonobuoy processing capability, most of the sonobuoy chutes were faired over with a blanking plate.

On 20 February 1974, the S-3A officially became operational with the Air Antisubmarine Squadron FORTY-ONE (VS-41), the "Shamrocks," at NAS North Island, California, which served as the initial S-3 Fleet Replacement Squadron (FRS) for both the Atlantic and Pacific Fleets until a separate Atlantic Fleet FRS, VS-27, was established in the 1980s. The first operational cruise of the S-3A took place in 1975 with the VS-21 "Fighting Redtails" aboard .

Starting in 1987, some S-3As were upgraded to S-3B standard with the addition of a number of new sensors, avionics, and weapons systems, including the capability to launch the AGM-84 Harpoon anti-ship missile. The S-3B could also be fitted with "buddy stores", external fuel tanks that allowed the Viking to refuel other aircraft. In July 1988, VS-30 became the first fleet squadron to receive the enhanced capability Harpoon/ISAR equipped S-3B, based at NAS Cecil Field in Jacksonville, Florida. 16 S-3As were converted to ES-3A Shadows for carrier-based electronic intelligence (ELINT) duties. Six aircraft, designated US-3A, were converted for a specialized utility and limited cargo COD requirement. Plans were also made to develop the KS-3A carrier-based tanker aircraft, but this program was ultimately cancelled after the conversion of just one early development S-3A.

With the collapse of the Soviet Union and the breakup of the Warsaw Pact, the Soviet-Russian submarine threat was perceived as much reduced, and the Vikings had the majority of their antisubmarine warfare equipment removed. The aircraft's mission subsequently changed to sea surface search, sea and ground attack, over-the-horizon targeting, and aircraft refueling. As a result, the S-3B after 1997 was typically crewed by one pilot and one copilot [NFO]; the additional seats in the S-3B could still support additional crew members for certain missions. To reflect these new missions the Viking squadrons were redesignated from "Air Antisubmarine Warfare Squadrons" to "Sea Control Squadrons."
Prior to the aircraft's retirement from front-line fleet use aboard US aircraft carriers, a number of upgrade programs were implemented. These include the Carrier Airborne Inertial Navigation System II (CAINS II) upgrade, which replaced older inertial navigation hardware with ring laser gyroscopes with a Honeywell EGI (Enhanced GPS Inertial Navigation System) and added digital electronic flight instruments (EFI). The Maverick Plus System (MPS) added the capability to employ the AGM-65E laser-guided or AGM-65F infrared-guided air-to-surface missile, and the AGM-84H/K Stand-off Land Attack Missile Expanded Response (SLAM/ER). The SLAM/ER is a GPS/inertial/infrared guided cruise missile derived from the AGM-84 Harpoon that can be controlled by the aircrew in the terminal phase of flight if an AWW-13 data link pod is carried by the aircraft.

The S-3B saw extensive service during the 1991 Gulf War, performing attack, tanker, and ELINT duties, and launching ADM-141 TALD decoys. This was the first time an S-3B was employed overland during an offensive air strike. The first mission occurred when an aircraft from VS-24, from the , attacked an Iraqi Silkworm missile site. The aircraft also participated in the Yugoslav wars in the 1990s and in Operation Enduring Freedom in 2001.

The first ES-3A was delivered in 1991, entering service after two years of testing. The Navy established two squadrons of eight ES-3A aircraft each in both the Atlantic and Pacific Fleets to provide detachments of typically two aircraft, ten officers, and 55 enlisted aircrew, maintenance and support personnel (which comprised/supported four complete aircrews) to deploying carrier air wings. The Pacific Fleet squadron, Fleet Air Reconnaissance Squadron FIVE (VQ-5), the "Sea Shadows," was originally based at the former NAS Agana, Guam but later relocated to NAS North Island in San Diego, California, with the Pacific Fleet S-3 Viking squadrons when NAS Agana closed in 1995 as a result of a 1993 Base Realignment and Closure (BRAC) decision. The Atlantic Fleet squadron, the VQ-6 "Black Ravens," were originally based with all Atlantic Fleet S-3 Vikings at the former NAS Cecil Field in Jacksonville, Florida, but later moved to NAS Jacksonville, approximately to the east, when NAS Cecil Field was closed in 1999 as a result of the same 1993 BRAC decision that closed NAS Agana.
The ES-3A operated primarily with carrier battle groups, providing organic 'Indications and Warning' support to the group and joint theater commanders. In addition to their warning and reconnaissance roles, and their extraordinarily stable handling characteristics and range, Shadows were a preferred recovery tanker (aircraft that provide refueling for returning aircraft). They averaged over 100 flight hours per month while deployed. Excessive utilization caused earlier than expected equipment replacement when Naval aviation funds were limited, making them an easy target for budget-driven decision makers. In 1999, both ES-3A squadrons and all 16 aircraft were decommissioned and the ES-3A inventory placed in Aerospace Maintenance and Regeneration Group (AMARG) storage at Davis-Monthan AFB, Arizona.

In March 2003, during Operation Iraqi Freedom, an S-3B Viking from Sea Control Squadron 38 (The "Red Griffins") piloted by Richard McGrath Jr. launched from . The crew successfully executed a time sensitive strike and fired a laser-guided Maverick missile to neutralize a significant Iraqi naval and leadership target in the port city of Basra, Iraq. This was one of the few times in its operational history that the S-3B Viking had been employed overland on an offensive combat air strike and the first time it launched a laser-guided Maverick missile in combat.
On 1 May 2003, US President George W. Bush flew in the co-pilot seat of a VS-35 Viking from NAS North Island, California, to off the California coast. There, he delivered his "Mission Accomplished" speech announcing the end of major combat in the 2003 invasion of Iraq. During the flight, the aircraft used the customary presidential callsign of "Navy One". The aircraft that President Bush flew in was retired shortly thereafter and on 15 July 2003 was accepted as an exhibit at the National Museum of Naval Aviation at NAS Pensacola, Florida.

Between July and December 2008 the VS-22 Checkmates, the last sea control squadron, operated a detachment of four S-3Bs from the Al Asad Airbase in Al Anbar Province, west of Baghdad. The planes were fitted with LANTIRN pods and they performed non-traditional intelligence, surveillance, and reconnaissance (NTISR). After more than 350 missions, the Checkmates returned to NAS Jacksonville, Florida, on 15 December 2008, prior to disestablishing on 29 January 2009.

Though a proposed airframe known as the Common Support Aircraft was once advanced as a successor to the S-3, E-2 and C-2, this plan failed to materialize. As the surviving S-3 airframes were forced into sundown retirement, a Lockheed Martin full scale fatigue test was performed and extended the service life of the aircraft by approximately 11,000 flight-hours. This supported Navy plans to retire all Vikings from front-line fleet service by 2009 so new strike fighter and multi-mission aircraft could be introduced to recapitalize the aging fleet inventory, with former Viking missions assumed by other fixed-wing and rotary-wing aircraft.

The final carrier based S-3B Squadron, VS-22 was decommissioned at NAS Jacksonville on 29 January 2009. Sea Control Wing Atlantic was decommissioned the following day on 30 January 2009, concurrent with the U.S. Navy retiring the last S-3B Viking from front-line Fleet service.

In June 2010 the first of three aircraft to patrol the Pacific Missile Test Center's range areas off of California was reactivated and delivered. The jet aircraft's higher speed, 10-hour endurance, modern radar, and a LANTIRN targeting pod allowed it to quickly confirm the test range being clear of wayward ships and aircraft before tests commence. These S-3Bs are flown by Air Test and Evaluation Squadron Thirty (VX-30) based out of NAS Point Mugu, California. Also, the NASA Glenn Research Center acquired four S-3Bs in 2005. Since 2009, one of these aircraft (USN BuNo 160607) has also carried the civil registration N601NA and is used for various tests.

By late 2015, the U.S. Navy had three Vikings remaining operational in support roles. One was moved to The Boneyard in November 2015, and the final two were retired, one stored and the other transferred to NASA, on 11 January 2016, officially retiring the S-3 from Navy service.

Naval analysts have suggested returning the stored S-3s to service with the U.S. Navy to fill gaps it left in the carrier air wing when it was retired. This is in response to the realization that the Chinese navy is producing new weapons that can threaten carriers beyond the range their aircraft can strike them. Against the DF-21D anti-ship ballistic missile, carrier-based F/A-18 Super Hornets and F-35C Lightning IIs have about half the unrefueled strike range, so bringing the S-3 back to aerial tanking duties would extend their range against it, as well as free up more Super Hornets that were forced to fill the role. Against submarines armed with anti-ship cruise missiles like the Klub and YJ-18, the S-3 would restore area coverage for ASW duties. Bringing the S-3 out of retirement could at least be a stop-gap measure to increase the survivability and capabilities of aircraft carriers until new aircraft can be developed for such purposes.

In October 2013, the Republic of Korea Navy expressed an interest in acquiring up to 18 ex-USN S-3s to augment their fleet of 16 Lockheed P-3 Orion aircraft. In August 2015, a military program review group approved a proposal to incorporate 12 mothballed S-3s to perform ASW duties; the Viking plan will be sent to the Defense Acquisition Program Administration for further assessment before final approval by the national defense system committee. Although the planes are old, being in storage kept them serviceable and using them is a cheaper way to fulfill short-range airborne ASW capabilities left after the retirement of the S-2 Tracker than buying newer aircraft. Refurbished S-3s could be returned to use by 2019. In 2017, the Republic of Korea Navy canceled plans to purchase refurbished and upgraded Lockheed S-3 Viking aircraft for maritime patrol and anti-submarine duties, leaving offers by Airbus, Boeing, Lockheed Martin, and Saab on the table.

In April 2014, Lockheed Martin announced that they would offer refurbished and remanufactured S-3s, dubbed the C-3, as a replacement for the Northrop Grumman C-2A Greyhound for carrier onboard delivery. The requirement for 35 aircraft would be met from the 91 S-3s currently in storage. In February 2015, the Navy announced that the Bell Boeing V-22 Osprey had been selected to replace the C-2 for the COD mission.





Notes
Bibliography



</doc>
<doc id="29476" url="https://en.wikipedia.org/wiki?curid=29476" title="Kaman SH-2 Seasprite">
Kaman SH-2 Seasprite

The Kaman SH-2 Seasprite is a ship-based helicopter originally developed and produced by American manufacturer Kaman Aircraft Corporation. It has been typically used as a compact and fast-moving rotorcraft for utility and anti-submarine warfare missions.

Development of the Seasprite had been initiated during the late 1950s in response to a request from the United States Navy, calling for a suitably fast and compact naval helicopter for utility missions. Kaman's submission, internally designated as the "K-20", was favourably evaluated, leading to the issuing of a contract for the construction of four prototypes and an initial batch of 12 production helicopters, designated as the "HU2K-1". Beyond the U.S. Navy, the company had also made efforts to acquire other customers for export sales, in particular the Royal Canadian Navy; however, the initial interest of the Canadians was quelled as a result of Kaman's demand for price increases and the Seasprite performing below company projections during its sea trials. Due to its unsatisfactory performance, from 1968 onwards, the U.S. Navy's existing UH-2s were remanufactured from their originally-delivered single-engine arrangement to a more powerful twin-engine configuration.

During October 1970, the Seasprite was selected by the U.S. Navy as the platform for the interim Light Airborne Multi-Purpose System (LAMPS) helicopter, which resulted in greatly enhanced anti-submarine and anti-surface threat capabilities being developed and installed upon a new variant of the type, designated as the "SH.2F". Accordingly, during the 1970s and 1980s, the majority of the existing UH-2 helicopters were remanufactured into the improved SH-2F model. In this configuration, the Seasprite extended and increased shipboard sensor and weapon capabilities against several types of enemy threats, including submarines of all types, surface ships and patrol craft that may be armed with anti-ship missiles.

The Seasprite served for many decades with the U.S. Navy. Highlights of its service life included operations during the lengthy Vietnam War, in which the type was primarily used to rescue downed friendly aircrews within the theatre of operations, and its deployment during the Gulf War, where Seasprites conducted combat support and surface warfare operations against hostile Iraqi forces. In more routine operations, the Seasprite was operated in a number of roles, including anti-submarine warfare (ASW), search and rescue (SAR), utility and plane guard (the latter being performed when on attachment to aircraft carriers). The type was finally withdrawn during 2001 when the last examples of the final variant, known as the SH-2G Super Seasprite were retired. During the 1990s and 2000s, ex-U.S. Navy Seasprites were offered to various nations as a form of foreign aid, which typically met with mixed interest and a limited uptake.

During 1956, the U.S. Navy launched a new competition with the intent of meeting its requirements for a compact, all-weather multipurpose naval helicopter, encouraging private companies to submit their proposals. American manufacturer Kaman Aircraft Corporation decided to produce its own response for the competition, their submitted design, which was given the internal company designation of "K-20", was of a relatively conventional helicopter powered by a single General Electric T58-8F turboshaft engine which drove a 44-foot four-bladed main rotor and a four-bladed tail rotor. Following an evaluation of the designs that had been bid in response, the U.S. Navy decided to select the submission by Kaman to proceed with further development. Accordingly, during late 1957, Kaman was promptly awarded with a contract calling for the construction of four prototypes and an initial batch of 12 production helicopters, designated as the "HU2K-1".

During 1960, the Royal Canadian Navy announced that the HU2K has been identified as the frontrunner for their own requirement for an anti-submarine warfare helicopter; this choice was confirmed when the Treasury Board of the Canadian government gave its approval for the initial procurement of 12 rotorcraft from Kaman at a price of $14.5 million. However, the Canadian purchase was disrupted by multiple factors, including Kaman's decision to abruptly raise the estimated price of the initial batch to $23 million; as the same time, there were concerns amongst officials that the manufacturer's projections of both the weight and performance criteria has been overly optimistic. In response, the Canadian Naval Board decided to hold off on issuing its approval to proceed with the HU2K purchase until after the US Navy had conducted sea trials with the type. During these sea trials, it was revealed that the HU2K was indeed overweight and underpowered; in light of this inferior performance, the HU2K was deemed to be incapable of meeting the Canadian requirements. Accordingly, during late 1961, the competing Sikorsky Sea King was selected to fulfil the intended role instead.

Having been unable to achieve any follow-on orders for the type, during the late 1960s, Kaman decided to terminate production following the completion of the delivery of 184 SH-2s to the U.S. Navy. However, during 1971, production was later restarted by Kaman in order to manufacture an improved variant of the helicopter, designated as the "SH-2F". A significant factor in the reopening of the production line was that the Navy's Sikorsky SH-60 Sea Hawk, which was both newer and more capable in anti-submarine operations, had been determined to be too large to allow it to be safely operated from the smaller flight decks present upon the older frigates then in service.

Upon the enactment of the 1962 United States Tri-Service aircraft designation system, the HU2K-1 had been redesignated as the "UH-2A", while the "HU2K-1U" model was redesignated as the "UH-2B". During its service, the UH-2 Seasprite would be subject to several modifications and improvements, such as the addition of fixtures for the mounting of external stores. Beginning in 1968, the Navy's remaining UH-2s were extensively remanufactured; perhaps the most extensive alteration performance was the replacement of their original single-engine arrangement with a more powerful twin-engine configuration.

During October 1970, the UH-2 was selected to be the platform to function as the interim Light Airborne Multi-Purpose System (LAMPS) helicopter. During the course of the 1960s, LAMPS had evolved out of an urgent requirement to develop a manned helicopter that would be capable of supporting a non-aviation vessel and serve as its tactical Anti-Submarine Warfare arm. Widely referred to as "LAMPS Mark I", the advanced sensors, processors, and display capabilities aboard the helicopter enabled such equipped ships to extend their situational awareness beyond the line-of-sight limitations that unavoidably hampered the performance of shipboard radars, as well as the short distances involved in the acoustic detection and prosecution of underwater threats associated with hull-mounted sonars. Those H-2s that were reconfigured to perform the LAMPS mission were accordingly re-designated as "SH-2D"s.

On 16 March 1971, the first SH-2D LAMPS prototype conducted its first flight. Beginning in 1973, production deliveries of the latest variant of the rotorcraft, designated as the "SH-2F", commenced. Amongst the features of the "SH-2F" model was the full suite of LAMPS I equipment, along with various other improvements, such as upgraded engines, an extended life main rotor, and an elevated take-off weight. During 1981, the Navy placed an order for 60 production SH-2Fs. From 1987 onwards, a total of 16 SH-2Fs were upgraded with a chin-mounted forward-looking infrared (FLIR) sensor, chaff/flare launchers, dual rear-mounted infrared countermeasures, and missile/mine detecting equipment.

Eventually, all but two H-2s that were then in the U.S. Navy inventory were remanufactured into the SH-2F configuration. The final production procurement of the SH-2F was in Fiscal Year 1986. The final six orders for production SH-2Fs were converted to the more extensive and newer SH-2G Super Seasprite variant.

During 1962, the initial UH-2 model commenced its operational service with the U.S. Navy. The U.S. Navy quickly determined that the helicopter's capabilities were greatly restricted by its single engine; thus, the service ordered Kaman to retrofit all of its Seasprites into a more capable twin-engine arrangement instead; when furnished with a pair of engines, the Seasprite was capable of attaining an airspeed of 130 knots and operating at a range of up to 411 nautical miles. The U.S. Navy would operate a total fleet of nearly 200 Seasprites to perform a variety of missions, ranging from anti-submarine warfare (ASW) operations, search and rescue (SAR) and utility transport. Under typical operational conditions, several UH-2s would be deployed upon each of the U.S. Navy's aircraft carriers in order to perform plane guard and SAR missions.

The UH-2 was introduced in time to see action in the Tonkin Gulf incident in August 1964. The Seasprite's principal contribution to what would escalate into the lengthy Vietnam War between the Soviet-backed North Vietnamese and the United States-backed South Vietnamese, was the retrieval of downed aircrews, both from the sea and from inside enemy territory. The type was increasingly relied upon to perform the retrieval mission as the conflict intensified, such as during Operation Rolling Thunder in 1965. During October 1966 alone, out of 269 downed pilots, helicopter-based SAR teams were recorded as having enabled the recovery of 103 men.
During the 1970s, the conversion of UH-2s to the SH-2 anti-submarine configuration provided the U.S. Navy with its first dedicated ASW helicopter capable of operating from vessels other than its aircraft carriers. The compact size of the SH-2 allowed the type to be operated from flight decks that were too small for the majority of helicopters; this factor would later play a role in the U.S. Navy's decision to acquire the improved SH-2F during the early 1980s.

The SH-2F fleet was utilized to enforce and support Operation Earnest Will in July 1987, Operation Praying Mantis in April 1988, and Operation Desert Storm during January 1991 in the Persian Gulf region. The countermeasures and additional equipment present upon the SH-2F allowed the type to conduct combat support and surface warfare missions within these hostile environments, which had an often-minimal submarine threat. During April 1994, the SH-2F was retired from active service with the U.S. Navy; the timing corresponded with the retirement of the last of the Vietnam-era Knox Class Frigates that were unable to accommodate the new and larger SH-60 Sea Hawks, which were used to replace the aging Seasprites.

During 1991, the U.S. Navy had begun to receive deliveries of the new SH-2G Super Seasprite; a total of 18 converted SH-2Fs and 6 new-built SH-2Gs were produced. These were assigned to Naval Reserve squadrons, the SH-2G entered service with HSL-84 in 1993. The SH-2 served in some 600 deployments and flew 1.5 million flight hours before the last of the type were finally retired in mid-2001.

The Royal New Zealand Navy (RNZN) replaced its Westland Wasps with an initial batch of four interim SH-2F Seasprites (formerly operated by the U.S. Navy), operated and maintained by a mix of Navy and Air Force personnel known as No. 3 Squadron RNZAF Naval Support Flight, to operate with ANZAC class frigates until the fleet of five new SH-2G(NZ) Super Seasprites were delivered. During October 2005, the Navy air element was transferred to No. 6 Squadron RNZAF at RNZAF Base Auckland in Whenuapai. RNZN Seasprites have seen service in East Timor. 10 of the 11 SH-2G(A)s rejected by the Royal Australian Navy were purchased in 2014 to replace the five RNZN SH-2G(NZ) Seasprites that had required either a MLU (Mid Life Upgrade) or replacement due to corrosion issues, maintenance problems and obsolescence. Kaman modified the ex-Australian aircraft and renamed them SH-2G(I), with the last one being delivered to New Zealand in early 2016. Eight of the aircraft are flying with the ninth and tenth aircraft being attritional aircraft used for spares etc. The 11th aircraft is held by Kaman as a prototype and test aircraft. The five SH-2G(NZ) have been sold to Peru. A SH-2F (ex-RNZN, NZ3442) is preserved in the Royal New Zealand Air Force Museum, donated to the museum by Kaman Aircraft Corporation after an accident while in service with the RNZN.

During the late 1990s, the United States decided to offer the surplus U.S. Navy SH-2Fs as foreign aid to a number of overseas countries. Amongst those to be offered the type included Greece, which had been offered six, and Turkey, which had been offered 14, but they rejected the offer. Egypt opted to acquire four SH-2F under this aid program, they were mainly used for spares in to support of their existing fleet of ten SH-2Gs. Poland chose to acquire the later SH-2G variant.






</doc>
<doc id="29480" url="https://en.wikipedia.org/wiki?curid=29480" title="Stop consonant">
Stop consonant

In phonetics, a stop, also known as a plosive or oral occlusive, is a consonant in which the vocal tract is blocked so that all airflow ceases.

The occlusion may be made with the tongue tip or blade (, ) tongue body (, ), lips (, ), or glottis (). Stops contrast with nasals, where the vocal tract is blocked but airflow continues through the nose, as in and , and with fricatives, where partial occlusion impedes but does not block airflow in the vocal tract.

The terms "stop, occlusive," and "plosive" are often used interchangeably. Linguists who distinguish them may not agree on the distinction being made. The terms refer to different features of the consonant. "Stop" refers to the airflow that is stopped. "Occlusive" refers to the articulation, which occludes (blocks) the vocal tract. "Plosive" refers to the release burst (plosion) of the consonant. Some object to the use of "plosive" for inaudibly released stops, which may then instead be called "applosives".

Either "occlusive" or "stop" may be used as a general term covering the other together with nasals. That is, 'occlusive' may be defined as oral occlusive (stops/plosives) plus nasal occlusives (nasals such as , ), or 'stop' may be defined as oral stops (plosives) plus nasal stops (nasals). Ladefoged and Maddieson (1996) prefer to restrict 'stop' to oral occlusives. They say,

In addition, they use "plosive" for a pulmonic stop; "stops" in their usage include ejective and implosive consonants.

If a term such as 'plosive' is used for oral obstruents, and nasals are not called nasal stops, then a "stop" may mean the glottal stop; 'plosive' may even mean non-glottal stop. In other cases, however, it may be the word 'plosive' that is restricted to the glottal stop. Note that, generally speaking, stops do not have plosion (a release burst). In English, for example, there are stops with no audible release, such as the in "apt". However, pulmonic stops do have plosion in other environments.

In Ancient Greek, the term for stop was ("áphōnon"), which means "unpronounceable", "voiceless", or "silent", because stops could not be pronounced without a vowel. This term was calqued into Latin as , and from there borrowed into English as "mute". "Mute" was sometimes used instead for voiceless consonants, whether stops or fricatives, a usage that was later replaced with "surd", from Latin "deaf" or "silent", a term still occasionally seen in the literature. For more information on the Ancient Greek terms, see .

A stop is typically analysed as having up to three phases:

Only the hold phase is requisite. A stop may lack an approach when it is preceded by a consonant that involves an occlusion at the same place of articulation, as in in "end" or "old". In many languages, such as Malay and Vietnamese, word-final stops lack a release burst, even when followed by a vowel, or have a nasal release. See no audible release.

Nasal occlusives are somewhat similar. In the catch and hold, airflow continues through the nose; in the release, there is no burst, and final nasals are typically unreleased across most languages.

In affricates, the catch and hold are those of a stop, but the release is that of a fricative. That is, affricates are stop–fricative contours.

All spoken natural languages in the world have stops, and most have at least the voiceless stops , , and . However, there are exceptions: Colloquial Samoan lacks the coronal , and several North American languages, such as the northern Iroquoian and southern Iroquoian languages (i.e., Cherokee), lack the labial . In fact, the labial is the least stable of the voiceless stops in the languages of the world, as the unconditioned sound change → (→ → Ø) is quite common in unrelated languages, having occurred in the history of Classical Japanese, Classical Arabic, and Proto-Celtic, for instance. Formal Samoan has only one word with velar ; colloquial Samoan conflates and to . Ni‘ihau Hawaiian has for to a greater extent than Standard Hawaiian, but neither distinguish a from a . It may be more accurate to say that Hawaiian and colloquial Samoan do not distinguish velar and coronal stops than to say they lack one or the other.

See Common occlusives for the distribution of both stops and nasals.

Voiced stops are pronounced with vibration of the vocal cords, voiceless stops without. Stops are commonly voiceless, and many languages, such as Mandarin Chinese and Hawaiian, have only voiceless stops. Others, such as most Australian languages, are indeterminate: stops may vary between voiced and voiceless without distinction.

In aspirated stops, the vocal cords (vocal folds) are abducted at the time of release. In a prevocalic aspirated stop (a stop followed by a vowel or sonorant), the time when the vocal cords begin to vibrate will be delayed until the vocal folds come together enough for voicing to begin, and will usually start with breathy voicing. The duration between the release of the stop and the voice onset is called the "voice onset time" (VOT) or the "aspiration interval". Highly aspirated stops have a long period of aspiration, so that there is a long period of voiceless airflow (a phonetic ) before the onset of the vowel. In tenuis stops, the vocal cords come together for voicing immediately following the release, and there is little or no aspiration (a voice onset time close to zero). In English, there may be a brief segment of breathy voice that identifies the stop as voiceless and not voiced. In voiced stops, the vocal folds are set for voice before the release, and often vibrate during the entire hold, and in English, the voicing after release is not breathy. A stop is called "fully voiced" if it is voiced during the entire occlusion. In English, however, initial voiced stops like or may have no voicing during the period of occlusion, or the voicing may start shortly before the release and continue after release, and word-final stops tend to be fully devoiced: In most dialects of English, the final /b/, /d/ and /g/ in words like "rib", "mad" and "dog" are fully devoiced. Initial voiceless stops, like the "p" in "pie", are aspirated, with a palpable puff of air upon release, whereas a stop after an "s", as in "spy", is tenuis (unaspirated). When spoken near a candle flame, the flame will flicker more after the words "par, tar," and "car" are articulated, compared with "spar, star," and "scar". In the common pronunciation of "papa", the initial "p" is aspirated whereas the medial "p" is not.

In a geminate or long consonant, the occlusion lasts longer than in simple consonants. In languages where stops are only distinguished by length (e.g., Arabic, Ilwana, Icelandic), the long stops may be held up to three times as long as the short stops. Italian is well known for its geminate stops, as the double "t" in the name "Vittoria" takes just as long to say as the "ct" does in English "Victoria". Japanese also prominently features geminate consonants, such as in the minimal pair 来た "kita" 'came' and 切った "kitta" 'cut'.

Note that there are many languages where the features voice, aspiration, and length reinforce each other, and in such cases it may be hard to determine which of these features predominates. In such cases, the terms fortis is sometimes used for aspiration or gemination, whereas lenis is used for single, tenuous, or voiced stops. Be aware, however, that the terms "fortis" and "lenis" are poorly defined, and their meanings vary from source to source.

Simple nasals are differentiated from stops only by a lowered velum that allows the air to escape through the nose during the occlusion. Nasals are acoustically sonorants, as they have a non-turbulent airflow and are nearly always voiced, but they are articulatorily obstruents, as there is complete blockage of the oral cavity. The term occlusive may be used as a cover term for both nasals and stops.

A prenasalized stop starts out with a lowered velum that raises during the occlusion. The closest examples in English are consonant clusters such as the [nd] in "candy", but many languages have prenasalized stops that function phonologically as single consonants. Swahili is well known for having words beginning with prenasalized stops, as in "ndege" 'bird', and in many languages of the South Pacific, such as Fijian, these are even spelled with single letters: "b" [mb], "d" [nd].

A postnasalized stop begins with a raised velum that lowers during the occlusion. This causes an audible nasal "release", as in English "sudden". This could also be compared to the /dn/ cluster found in Russian and other Slavic languages, which can be seen in the name of the Dnieper River.

Note that the terms "prenasalization" and "postnasalization" are normally used only in languages where these sounds are phonemic: that is, not analyzed into sequences of stop plus nasal.

Stops may be made with more than one airstream mechanism. The normal mechanism is pulmonic egressive, that is, with air flowing outward from the lungs. All languages have pulmonic stops. Some languages have stops made with other mechanisms as well: ejective stops (glottalic egressive), implosive stops (glottalic ingressive), or click consonants (lingual ingressive).

A fortis stop (in the narrow sense) is produced with more muscular tension than a lenis stop (in the narrow sense). However, this is difficult to measure, and there is usually debate over the actual mechanism of alleged fortis or lenis consonants.

There are a series of stops in the Korean language, sometimes written with the IPA symbol for ejectives, which are produced using "stiff voice", meaning there is increased contraction of the glottis than for normal production of voiceless stops. The indirect evidence for stiff voice is in the following vowels, which have a higher fundamental frequency than those following other stops. The higher frequency is explained as a result of the glottis being tense. Other such phonation types include breathy voice, or murmur; slack voice; and creaky voice.

The following stops have been given dedicated symbols in the IPA.

Many subclassifications of stops are transcribed by adding a diacritic or modifier letter to the IPA symbols above.





</doc>
<doc id="29482" url="https://en.wikipedia.org/wiki?curid=29482" title="Stayman convention">
Stayman convention

Stayman is a bidding convention in the card game contract bridge. It is used by a partnership to find a 4-4 or 5-3 trump fit in a suit after making a one (1NT) opening bid and it has been adapted for use after a 2NT opening, a 1NT overcall, and many other natural notrump bids.

The convention is named for Sam Stayman, who wrote the first published description in 1945, but its inventors were two other players: the British expert Jack Marx in 1939, who published it only in 1946, and Stayman's regular partner George Rapée in 1944.

A bid and made in a major suit (i.e. 4 or 4 ) scores better than a game contract bid and made in a minor suit (i.e. 5 or 5 ) or in notrump (i.e. 3NT). Also, the success rate for a game contract in a major suit when a partnership has a combined holding of 26 points and eight cards in the major is about 80%, whereas a game contract in 3NT with 26 (HCP) has a success rate of only 60%, or 50% with 25 HCP; the success rate for a minor suit game contract when holding 26 points is about 30%.

Accordingly, partnership priority is to find an eight card or better major suit fit when jointly holding sufficient values for a game contract. 5-3 and 6-2 fits are easy to find in basic methods as responder can bid 3 or 3 over 1NT, and opener will not normally have a 5 card major to bid 1NT. However, finding 4-4 fits presents a problem. The 2 and 2 bids cannot be used for this as they are weak takeouts, a sign-off bid.

After an opening bid or an overcall of 1NT (2NT), or bids an artificial 2 (3) to ask opener or overcaller if he holds a four- or five-card major suit; some partnership agreements may require the major to be headed by an honor of at least a specified rank, such as the queen. The artificial club bid typically promises four cards in at least one of the major suits (promissory Stayman) and, "in standard form", enough strength to continue bidding after partner's response (8 HCP for an invitational bid opposite a standard strong 1NT opening or overcall showing 15-17 HCP, 11 HCP opposite a weak notrump of 12-14 HCP, or 5 HCP to go to game opposite a standard 2NT showing 20-21 points). It also promises distribution that is not 4333. By invoking the Stayman convention, the responder takes control of the bidding since strength and distribution of the opener's hand is already known within a limited range. The opener responds with the following rebids.
A notrump opener should have neither a suit longer than five cards nor more than one 5-card suit since an opening notrump bid shows a balanced hand. A notrump bidder who has at least four cards in each major suit normally responds in hearts, as this can still allow a spade fit to be found. Variant methods are to bid the longer or stronger major, with a preference given to spades, or to use 2NT to show both majors.

In the standard form of Stayman over 1NT, the responder has a number of options depending on his partner's answer:
Over these bids, the notrump bidder (1) with a maximum hand (17 HCP), goes to game over an invitational bid and (2) with four (or more) cards in each major suit, corrects to the previously unbid major suit.

In the standard form of Stayman over 2NT, the responder has only two normal rebids.

In either case, a responder who rebids notrump over a response in a major suit promises four cards of the other major suit. Thus, a notrump opener who holds at least four cards in each major suit should "correct" by bidding the other major suit at the lowest level.

Of course, once a fit is found, responder who has sufficient strength also may bid 4 (Gerber) or 4NT (Blackwood), or cue bid aces, depending upon partnership agreement, to explore slam in any of the above sequences. Some partnerships also admit responder's rebids of a major suit that the notrump bidder did not name.

A bid of 4 over an opening bid of 3NT may be either Stayman or Gerber, depending upon the partnership agreement.

If an adverse suit bid is inserted immediately after a 1NT opening, Stayman may be employed via a double (by partnership agreement) or a cue bid, depending on the strength of his hand. The cue bid, which is conventional, is completely artificial and means nothing other than invoking Stayman. For example, if South opens 1NT, and West overcalls 2, North, if he has adequate values, may call 3, invoking Stayman. South would then show his major or bid game in notrump. Alternatively, North, if his hand lacks the values for game in notrump, may double, which by partnership agreement employs Stayman. This keeps the Stayman bidding at second level.

Partnerships who have not yet learned Stayman but choose to adopt Stayman (without having yet learned or having chosen not to use Jacoby Transfers) will need to adjust their use of normal two-level responses after a 1NT opening, because the availability of this convention changes the nature of what had been normal 1NT responses. When the notrump bidder's partner does not invoke Stayman but instead calls 2 or 2, it is a sign of relative weakness (since if responder held 8 HCP or more, he would have invoked Stayman). These bids are commonly referred to as "drop dead bids", as the opening notrump bidder is requested to withdraw from the auction. If opener has maximum values, a fit, and strong support, he may raise to the 3-level, but under no circumstances may he take any other action. This provides the partnership with an advantage that the non-Stayman partnership doesn't enjoy. For example, a responder may have no honors at all; that is, a total of zero HCP. His partner is likely to be set if he passes. A non-Stayman responder would have to pass, because to bid would provoke a rebid. But a Stayman responder can respond to his partner's 1NT opening at level 2 if he has a 6-card non-club suit. The responder with 3 HCP and a singleton can make a similar call with a 5-card non-club suit. This gives the partnership a better than even chance of success in making the contract, whereas without a response (and without Stayman), the contract would likely be set.

Similarly, a response of 2 indicates less than 8 HCP and should usually be passed. In rare cases, when the opener has maximum values and a fit in diamonds with at least two of the top three honors, he may raise diamonds, and responder may see a chance for game in notrump.

There are many variations on this basic theme, and partnership agreement may alter the details of its use. It is one of the most widely used conventions in bridge.

Some partnerships play that 2 Stayman does not absolutely promise a four-card major (non promissory Stayman). For example, if responder has a short suit and wishes to know if opener has four-card cover in it, so as to play in notrumps. If opener shows hearts initially, 2 can be used to find a fit in spades when the 2 does not promise a four-card major.

1NT - 2, 2 -
Alternatively 2 can be used for all hands with four spades and not four hearts, either invitational or game values, while 3NT denies four spades.

Today, most players use Stayman in conjunction with Jacoby transfers. With Stayman in effect, the responder practically denies having a five-card major, as otherwise he would transfer to the major immediately. The only exception is when responder has 5-4 in the majors; in that case, he could use Stayman, and in the case of a 2 response, bid the five-card major at the two level (weakness take-out / Garbage Stayman) or at the three level (forcing to game). However, the latter hand can also be bid by first using a transfer and then showing the second suit naturally. The Smolen convention provides an alternative method to show a five-card major and game-going values. A minor drawback of Jacoby transfers is that a 2 contract is not possible.

The Smolen convention is an adjunct to Stayman for situations in which the notrump opener has denied holding a four-card major and responder has a five-card major and a four-card major with game-going values.

If the notrump opener responds to the Stayman 2 asking bid with 2, denying a four-card major, responder initiates the Smolen Transfer with a jump shift to three of his four-card major. The jump shift shows which is the four-card major and promises five in the other major. The notrump opener then bids four of the other major with three cards in the suit or 3NT with fewer than three.

Smolen may also be used when responder has a six-card major and a four-card major with game-going values; after the 2 negative response by opener, responder double jump shifts to four in the suit just below his six-card major and the notrump opener transfers to four of his partner's six-card major.

This convention allows a partnership to find either a 5-3 fit, 6-3 and 6-2 fit while ensuring that the notrump opener, who has the stronger hand, will be declarer.

"Garbage" Stayman (or "Weak Stayman" or "Rescue Stayman") and "Crawling" Stayman are adaptations of Stayman frequently used for damage control when holding a weak hand opposite a 1NT opening bid. For example, on the following hand.

Partner opens 1NT (15-17), and right hand opponent passes. Opponents have 23-25 HCP. Thus, 1NT is virtually certain to go down by at least three or four tricks. Indeed, in No-trumps, this dummy will be completely worthless. 

In "Garbage Stayman", you bid 2 Stayman with this "garbage" hand rather than passing on the first round, and then "pass opener's response". If opener rebids a major suit you have found a 4-4 fit and ability to trump club losers. Likewise, a response of 2 guarantees no worse than a 5-2 fit in diamonds and, with a fifth trump, a potential additional ruff. Declarer can also reach dummy with ruffs and may then be able to take finesses or execute a squeeze that otherwise would not be possible. The result is a contract that will go down fewer tricks or may even make, rather than a contract that is virtually certain to go down at least three or four tricks. However the hand must be able to tolerate any rebid from opener.

"Crawling Stayman" is an optional extension of "Garbage Stayman" for situations in which the responder's diamond suit is short. In "Crawling Stayman", the responder rebids 2 over the Notrump bidder's 2 reply. This conventional bid shows a weak hand with at least four cards in each major suit, asking the Notrump bidder to choose between the major suits at the cheapest level by either passing the 2 bid or correcting to 2. The name "Crawling Stayman" comes from the fact that the bidding "crawls" at the slowest possible pace: (pass) – 1NT – (pass) – 2; (pass) – 2 – (pass) – 2; (pass) – 2; (pass) – pass – (pass).

Alternatively, responder's 2 and 2 bids after the 2 rebid can be weak sign-offs. This allows responder to effectively bid hands which are 5-4 in the majors, by looking first for a 4-4 fit and, if none is found, signing off in his 5 card suit.

"Garbage Stayman" is even more useful opposite a weak NT opening (12-14) as it occurs more frequently and can mitigate very expensive penalties if responder is weak. It is in frequent use in Acol.

"Garbage Stayman" and "Crawling Stayman" bids over a 2NT bid work the same way, but occur at the "three" level.

If Jacoby transfers are not played, there are two approaches to resolve the situation when responder has a 5-card major but only invitational values. In one, more common, referred to as "non-forcing Stayman", in the sequence:
responder's simple rebid of a major suit is invitational, showing 8-9 points and a 5-card spade suit. In the "forcing Stayman" variant, the bid is one-round forcing.

In the original Precision Club system, forcing and non-forcing Stayman are differentiated in the start: 2 by responder shows only invitational values (and the continuation is the same as in basic Stayman), while 2 is forcing to game (responder bids 2NT without majors).

This allows responder to find exact shape of 1NT opener. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2, 2, 2, 2NT, 3 allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. Disadvantage is 2 can't be used as a damage control bid.

1NT – 2♣

Developed to be used in combination with following other responses to 1NT: 2, 2 Jacoby transfers to majors; 2 range finder/transfer to minors (opener's rebids: 2NT 12-13 HCP, 3 14 HCP. Responder passes or corrects to 3 or 3 sign off if weak. After opener's 3 rebid responder bids 3 to show 4 hearts or 3 to show 4 spades both game forcing. Responder's rebid of 3NT denies 4 card major); 2NT invitational hand with both 4 card majors (opener's rebids: no bid no 4 card major 12-13 HCP, 3 4 hearts 12-13 HCP, 3 4 spades 12-13 HCP, 3 4 hearts 14 HCP, 3 4 spades 14 HCP, 3NT 14 HCP no 4 card major)

This allows responder to find exact shape of 1NT opener that may only contain a four-card major. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2, 2, 2 allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for notrumps. May be also used as a damage control bid, and for both invitational, and game forcing hands.

1NT – 2♣

1NT – 3♣ weak sign off.

Opener's rebids of 2, 2, 2 may all be passed if responder is weak.

Developed to be used in combination with following other responses to 1NT: 2, 2 Jacoby transfers to majors; 2 five spades four hearts 10-11 HCP; 2NT invitational hand with 5,5 minors 10-11 HCP.

This allows responder to find exact shape of 1NT opener that may contain a 5 card major. Developed for use with weak 1NT opening. Relay bids over opener's rebids of 2D, 2H, 2S allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. May be also used as a damage control bid, and for both invitational, and game forcing hands.

1NT – 2C

Opener's rebids of 2D, 2H, 2S may all be passed if responder is weak.

Developed to be used in combination with following other responses to 1NT: 2D, 2H Jacoby transfers to majors; 2S range finder/transfer C; 2NT invitational hand with 5,5 minors 10-11 HCP.

This allows responder to check for 5-3 major fits where it is possible that opener's 1NT or 2NT might include a five card major. As described by Australian Ron Klinger, it can be played with a weak or strong 1NT.

1NT - 2

1NT - 2, 2 OR 2NT

After a transfer, accept it with any 4333, bid 3NT with only two trumps, otherwise bid 4M.

1NT - 2, 2 OR 2NT - 3 = Stayman

1NT - 2, (2 OR 2NT) - 3, 3

An alternative, simpler version of 5 card Stayman is:

1NT - 2

This structure permits use by weak hands with 5+ diamonds and 2+ cards in each major.

After 1NT - 2, 2

If responder has a five-card major, he begins with a transfer. After completion of the transfer, bidding the other major at the three level shows four cards in it and a game forcing hand, in line with the 1NT - 2, 2 structure above (1NT - 2, 2 - 2 = invitational 5-4).

Similarly after 2NT - 3, 3

A drawback of Five Card Major Stayman (particularly the simpler version) is that the weaker hand may become declarer in a 4-4 major fit.

Puppet Stayman is similar to Five Card Stayman. It is more complex but has the major advantage that the strong hand virtually always becomes declarer.

Initially developed by Neil Silverman and refined by Kit Woolsey and Steve Robinson in 1977-78, is a variation of the Stayman convention designed to find a 5-3 fit in a major, augmenting the search for a 4-4 major fit by standard Stayman. In 1977, Woolsey wrote that Puppet Stayman has several advantages over standard Stayman:

As in standard Stayman, Puppet Stayman begins with a 2 response to a 1NT opening and is at least game invitational; this asks opener to bid a 5-card major if he has one and otherwise to bid 2. Over a 2 response, rebids by responder are intended to disclose his distributional features in the majors as well as his strength. The original 1977 and 1978 revised rebids described by Woolsey are tabulated below: 
Opener and responder continue the bidding having a clearer understanding of each other's distributional features and are better positioned to select the ultimate and level of the contract.

Many variations to the Puppet Stayman bidding structure have been devised since Woolsey's 1978 summary; partnership review and agreement on the preferred modern treatment is required.

Some no longer advocate use of Puppet Stayman over a 1NT opening preferring to use the concept exclusively over a 2NT opening and reserving other Stayman variations and conventions such Jacoby Transfers and Smolen Transfers in search of major-suit fits after a 1NT opening.

Puppet Stayman is more commonly used after a 2NT opening than after a 1NT opening. Responses to a 2NT opening or very strong 2NT rebid (20-22 or 23-24):

Responder bids 3 seeking information about opener's major suit holding. Opener replies:

By this means all 5-3 and 4-4 major suit fits can be found.

An alternative pattern frees up the 2NT-3 sequence as a slam try in the minors. To allow 3-5 spade fits to be found when responder holds 5 spades and 4 hearts, some of the responses change:

2 Checkback Stayman (or simply Checkback) is used after a 1NT rebid by opener rather than a 1NT opening. It is used to "check back" if opener has major suit support, saying nothing additional about the club suit. It can find 3-5 fits, 4-4 fits (in Standard American) and 5-3 fits (in Acol), and also shows whether opener was maximum or minimum strength for his notrump bid. In five-card major systems, bidding Checkback implies that the responder has five cards in his major, and may have four in the other.

1m – 1M; 1NT – 2

The 2 is "Checkback Stayman". Responses by opener shows the following:

Partnership agreement is required on how to handle the case of holding four of the other major and three of partner's suit. One could agree to bid up the line, or support partner's suit first. If partner cannot support your first suit, he will invite with 2NT or bid game with 3NT and you will then correct to your other suit.

In Acol, if the opening bid was a major, opener can rebid his major after a Checkback inquiry to show that it has five cards rather than four and find 5-3 fits. Moreover, 1M – 2m; 2NT – 3 can also be used as Checkback Stayman. It is useful also to include an indication of range, particularly if opener's 2NT rebid is forcing to game and shows a wide points range (15-19). This is achieved by using 3 for minimum hands and 3/3/3NT for maximum hands, or vice versa. After 3, responder can still bid 3/3 to look for a 5-3 fit.

New Minor Forcing is an alternative to Checkback Stayman where either 2 or 2 can be used as the checkback bid. It can be used by responder with invitational values or better to find three-card support for his major or to find a 4-4 heart fit if holding five spades and four hearts); it also allows a return to the minor to play.



</doc>
<doc id="29483" url="https://en.wikipedia.org/wiki?curid=29483" title="Saks Fifth Avenue">
Saks Fifth Avenue

Saks Fifth Avenue is an American chain of luxury department stores owned, since 2013, by the oldest commercial corporation in North America, the Hudson's Bay Company. Its main flagship store is located on Fifth Avenue in Midtown Manhattan, New York City.

Saks Fifth Avenue is the successor of a business founded by Andrew Saks in 1867 and incorporated in New York in 1902 as Saks & Company. Saks died in 1912, and in 1923 Saks & Co. merged with Gimbel Brothers, Inc., which was owned by a cousin of Horace Saks, Bernard Gimbel, operating as a separate autonomous subsidiary. On September 15, 1924, Horace Saks and Bernard Gimbel opened Saks Fifth Avenue in New York City, with a full-block avenue frontage south of St. Patrick's Cathedral, facing what would become Rockefeller Center. The architects were Starrett & van Vleck, who developed a reticent, genteel Anglophile classicizing facade similar to their Gimbels Department Store in Pittsburgh (1914).

When Bernard's brother, Adam Gimbel, became president of Saks Fifth Avenue in 1926 after Horace Saks's sudden passing, the company expanded, opening seasonal resort branches in Palm Beach, Florida, and Southampton, New York, in 1928. The first full-line year-round Saks store opened in Chicago, in 1929, followed by another resort store in Miami Beach, Florida. In 1938, Saks expanded to the West Coast, opening in Beverly Hills, California. By the end of the 1930s, Saks Fifth Avenue had a total of 10 stores, including resort locations such as Sun Valley, Idaho, Mount Stowe, and Newport, Rhode Island. More full-line stores followed with Detroit, Michigan, in 1940 and Pittsburgh, Pennsylvania, in 1949. In Downtown Pittsburgh, the company moved to its own freestanding location approximately one block from its former home on the fourth floor in the downtown Gimbel's flagship. The San Francisco location opened in 1952, competing locally with I. Magnin. BATUS Inc. acquired Gimbel Bros., Inc. and its Saks Fifth Avenue subsidiary in 1973 as part of its diversification strategy. More expansion followed from the 1960s through the 1990s including the Midwest, and the South, particularly in Texas. In 1990, BATUS sold Saks to Investcorp S.A., which took Saks public in 1996 as Saks Holdings, Inc.

In 1990, the company launched "Saks Off 5th", an outlet store offshoot of the main brand, with 107 stores worldwide by 2016.

In 1998, Proffitt's, Inc. the parent company of Proffitt's and other department stores, acquired Saks Holdings Inc. Upon completing the acquisition, Proffitt's, Inc. changed its name to Saks, Inc.

Since 2000 Saks has opened international locations in Saudi Arabia, United Arab Emirates, Bahrain, Kazakhstan, Canada, and Mexico City. 

In August 2007, the United States Postal Service began an experimental program selling the plus zip code extension to businesses. The first company to do so was Saks Fifth Avenue, which received the zip code of 10022-7463 ("SHOE") for the eighth-floor shoe department in its flagship Fifth Avenue store.

During the 2007–2009 recession, Saks Fifth Avenue closed some stores and to cut prices and profit margins, thus according to Reuters "training shoppers to expect discounts. It took three years before it could start selling at closer to full price". In the following years, the company closed stores in locations including Orange County (2010), Denver (2011), Pittsburgh (2012), Highland Park, Illinois (2012/13) and in June 2013 its last Dallas store to implement the "strategy of employing our resources in our most productive locations".
As of 2013, the New York flagship store, whose real estate value was estimated between $800 million and over $1 billion at the time, generated around 20% of Saks' annual sales at $620 million, with other stores being less profitable according to analysts.

On July 29, 2013, the Hudson's Bay Company (HBC), owner of the competing chain Lord & Taylor, announced it would acquire Saks Fifth Avenue's parent company for US$2.9 billion. Plans called for up to seven Saks Fifth Avenues to open in major Canadian markets. Expansion into Canada is expected to compete with Canadian Holt Renfrew chain and challenge Nordstrom's expansion into Canada, which began in summer 2014 with the opening of a Nordstrom store in Calgary. In January 2014, HBC announced the first Saks store in Canada would occupy in its flagship Queen Street building in downtown Toronto, connected to the Toronto Eaton Centre via sky bridge. The store opened in February 2016 with a second Toronto area location in the Sherway Gardens shopping center opening in spring 2016. On February 22, 2018, Saks Fifth Avenue opened its third Canadian store in Calgary, Alberta.

Starting in 2015 Saks began a $250 million, three-year restoration of its Fifth Avenue flagship store. In October 2015, Saks announced it would debut a new location in Greenwich, Connecticut. In autumn 2015, Saks announced it would replace its existing store at the Houston Galleria with a new store.

In 2005, vendors filed against Saks alleging unlawful chargebacks. The U.S. Securities and Exchange Commission (SEC) investigated the complaint for years and, according to the "New York Times", "exposed a tangle of illicit tactics that let Saks... keep money it owed to clothing makers", inflating Saks' yearly earnings up to 43% and abusively collecting around $30 million from suppliers over seven years. Saks settled with the SEC in 2007, after firing three or more executives involved in the fraudulent activities.

In 2014, Saks fired transgender employee Leyth Jamal after she was allegedly "belittled by coworkers, forced to use the men's room and repeatedly referred to by male pronouns (he and him)". After Jamal submitted a lawsuit for unfair dismissal, the company stated in a motion to dismiss that "it is well settled that transsexuals are not protected by Title VII of the Civil Rights Act of 1964." In a court filing, the United States Department of Justice rebuked Saks' argument, stating that "discrimination against an individual based on gender identity is discrimination because of sex." The Human Rights Campaign removed the company from its list of "allies" during the controversy. The lawsuit was later settled amicably, with undisclosed terms.

In 2017, following the events of Hurricane Maria in Puerto Rico, Saks's San Juan store located in Mall of San Juan suffered major damages along with its neighboring anchor store Nordstrom. Taubman Centers, the company who owns the mall, filed a lawsuit against Saks for failing to provide an estimated reopening date and failing to restore damages after the hurricane due to a binding contract. Although Nordstrom reopened on November 9, 2018, on October 30, 2018, Saks Fifth Avenue announced that it would officially vacate The Mall Of San Juan.




</doc>
<doc id="29484" url="https://en.wikipedia.org/wiki?curid=29484" title="Seabee">
Seabee

United States Naval Construction Battalions, better known as the Navy Seabees, form the U.S. Naval Construction Force (NCF). The Seabee nickname is a heterograph of the first letters "C B" from the words Construction Battalion. Depending upon how the word is used "Seabee" can refer to one of three things: all the enlisted personnel in the USN's occupational field 7 (OF-7), all officers and enlisted assigned to the Naval Construction Force (NCF), or Construction Battalions either Mobile or Amphibious. Seabees serve outside the NCF as well. During WWII they served in both the Naval Combat Demolition Units and the Underwater Demolition Teams (UDTs). They were also elements of Cubs, Lions, Acorns and the United States Marine Corps. Today they can be found in many special task assignments, including the Naval Support Unit at the Department of State, Camp David, many base Public Works, under both Commanders of the Naval Surface Forces Atlantic/Pacific fleets and USN diving commands.
Naval Construction Battalions were conceived of as a replacement for civilian construction companies working for the U.S. Navy after the country was drawn into World War II with the attack on Pearl Harbor. At that time roughly 70,000 civilians were constructing on U.S. military installations overseas. International law made it illegal for civilian workers to resist an attack. To do so would classify them as guerrillas and could lead to summary execution. That is exactly what happened when the Japanese invaded Wake Island.

Adm. Moreell's concept model CB was a USMC trained battalion of construction tradesmen. A military equivalent of those civilian companies, capable of any type of construction, anywhere needed, under any conditions or circumstances. It was quickly realized that CBs were flexible, adaptable and could be utilized in every theater of operations. The use of USMC organization allowed for smooth co-ordination, integration or interface between NCF and Marine Corps elements. Additionally, CBs could be deployed individually or in multiples as the project scope and scale dictated. What distinguishes Seabees from Combat Engineers are the skill sets. Combat Engineering is but a sub-set in the Seabee toolbox. They have a storied legacy of creative field ingenuity, stretching from Normandy and Okinawa to Iraq and Afghanistan. Adm. Ernest King wrote to the Seabees on their second anniversary, "Your ingenuity and fortitude have become a legend in the naval service." Seabees believe that anything they are tasked with, they "Can Do" (the CB motto). They were unique at conception and remain so today. In the October 1944 issue of "Flying" magazine, the Seabees are described as "a phenomenon of World War II". In 2017, the Seabees celebrated their 75 years of service without having changed from Adm. Ben Moreell's conceptual model.

CB Conceptual Formation

In the early 1930s, the idea that the Twelfth Regiment(Public Works) pioneered in 1917 was still in the minds of many Navy Civil Engineers. The planners of the Bureau of Yards and Docks (BuDocks) began providing for "Navy Construction Battalions" in their contingency war plans. In 1934 Capt. Carl Carlson's version of the plan was circulated to the Navy Yards, with the idea of "Navy Construction Battalions" tentatively approved by Chief of Naval Operations, Adm. Standley. In 1935, RADM. Norman Smith, Chief of BuDocks, selected Captain Walter Allen, the War Plans Officer, to represent BuDocks on the War Plans Board. Capt. Allen presented the bureau's "Naval Construction Battalions concept" to the Board. His presentation was adopted for inclusion in the Rainbow war plans. The Seabees would name their first Training Center for Capt. Allen.

One flaw to the proposal was that CBs would have a dual command; military control would be exercised by fleet line Officers while construction operations would be controlled by Civil Engineer Corps officers. Another issue was no provision for the military organization or military training necessary to provide unit structure, discipline, and esprit de corps. The plans only allowed for battalions to be formed to build training stations throughout CONUS. Only afterwards could they be forward deployed.
RADM. Ben Moreell became BuDocks Chief in December 1937 and would become the lead proponent of the CB proposal. 

By summer of 1941 civilian contractors were working on large naval bases at Guam, Midway, Wake, Pearl Harbor, Iceland, Newfoundland, and Bermuda. BuDocks decided there was a need to improve the Navy's supervision of these projects through the creation of "Headquarters Construction Companies". These companies were to consist of two officers and 99 enlisted men, but would do no actual construction work. The companies would be staffed by draftsmen and surveyors to aid the officers in charge of construction as well as the construction inspectors. RADM. Chester Nimitz, Chief of the Bureau of Navigation, authorized the formation of the 1st Headquarters Construction Company, on October 31st 1941. Recruitment began in November. Company formation and boot training began December 7th at Naval Station Newport, Rhode Island. By the 16th, four additional companies had been authorized but, Pearl Harbor had happened changing plans, mission, and the ratings needed to accomplish it all.

On December 28th 1941, RADM Moreell requested authority to commission three Naval Construction Battalions. On January 5th, 1942 that happened and the Naval Bureau of Navigation authorized that recruitment begin for experienced construction tradesmen. In the meanwhile, the other four HQ Construction Companies previously requested had been approved and authorized. With the approval to form CBs now in hand BuDocks combined HQ Companies 2 & 3 to form the 1st Naval Construction Battalion at Charleston So. Carolina. They were deployed as the 2nd & 3rd Construction Detachments. But, before that, the 1st HQ Construction Company was used to commission the 1st Naval Construction Detachment sent to Bora Bora. In January 1942 those men were part of Operation Bobcat and are known in Seabee history as "Bobcats". HQ Companies 4 & 5 were combined to form the 2nd NCB and were deployed as the 4th and 5th Construction Detachments. This deployment pattern continued through CB 5. CB 6 was the first battalion to deploy full complement to the same deployment site.

Before all this could happen BuDocks had to address the dual command issue. Naval regs stated unit command was strictly limited to line officers. BuDocks deemed it essential that CBs be commanded by CEC officers trained in construction. The newly formed Bureau of Naval Personnel (BuPers) strongly opposed this. Adm. Moreell took the issue directly to the Secretary of the Navy, Frank Knox. Who, on 19 March 1942, gave complete authority to the Civil Engineer Corps over all men assigned to construction units. Almost 11,400 would become CEC with 7,960 doing CB service. Two weeks prior, on March 5, all construction battalion personnel were officially named "Seabees" by the Dept. of the Navy. Seabees have annually observed that date since 1955. From 1942 until then December 28 was the date the NCF had observed for its birthdate.

The first Seabees were construction tradesmen who were given advanced rank correlated to their trade experience and skill. They were the highest paid group the U.S. had in uniform during WWII. To recruit these men, age and physical standards were less rigid accepting men up to age 50 in the beginning. During 1942 the average of age was 37. Even so, all were put through the same physical training. In December 1942, President Franklin D. Roosevelt ordered that CB recruits come from the Selective Service System. Enlistees could request CB service with a written statement certifying that they were trade qualified. This lasted until October 1943 when voluntary enlistment in the Seabees ceased until December 1944. This period saw recruits of standard 
age and much less skilled at enlistment. By wars end 258,872 officers and enlisted had served in the Seabees, never reaching their authorized billet of 321,056. 

In 1942 recruits first three weeks were at Camp Allen. Camp Bradford at Little Creek, Va. replaced Camp Allen which in turn was replaced by Camp Peary. Finally, all training was moved in 1944 to Camp Endicott, Rhode Island. The first five battalions were sent directly overseas because war dictated or created urgent construction needs. Battalions that followed would be sent to an advance base depots/naval training centers (NTC) at Davisville, RI, Gulfport, MS. or Port Hueneme, CA. The Davisville advanced base depot became operational in June 1942 with the NTC, Camp Endicott, commissioned on 11 August. That camp would train over 100,000 Seabees. The Naval Training Centers had classes in over 60 trades. Camp Rousseau at Port Hueneme became operational first and would serve to stage 175,000 Seabees forward. The other CB Camps were Camp Parks, Livermore, Ca., and Camp Lee-Stephenson, Quoddy Village, Eastport, Maine.

CBs sent to the Pacific were attached to one of the four Amphibious Corps: I, III, and V were USMC while VII was U.S. Army.

Advance bases
The Navy wanted to enable open communications concerning advance base construction and development without being concerned the enemy had intercepted the transmissions. The Office of Naval Operations created a solution. Each base construction operation was given a code name as a numbered metaphor for the size/type of base the Seabees were to construct, i.e. Lion 1. That code was also used to identify the "unit" that would be the administration for that base. These were Lion, Cub, Oak and Acorn with a LION being a main Fleet Base numbered 1–6. CUBs were Secondary Fleet Bases 1/4 the size of a Lion (numbered 1–12)) OAK and ACORN were the names given airfield units of new or captured enemy fields (primary and secondary in size). Cubs were quickly adopted as the primary type airfield. The speed with which the Seabees were able to get a Cub operational led the Marines to consider them a tactical component that was to be utilized as quickly as the Seabees could make it so. Camp Bedilion shared a common fence-line with Camp Rousseau at Port Hueneme. It was home to the Acorn Assembly and Training Detachment responsible for training and organizing Acorn units. All Acorns had a CBMU attached. As the war progressed BuDocks realized that logistics required Advance Base Construction Depots (ABCDs) be built to get the job done. So CBs made them at Nouméa, Pearl Harbor, Brisbane, Milne Bay, Samar, Subic Bay, and Okinawa. When the code was first created BuDocks plans had two CBs tasked to construct a Lion. By 1944 an entire Regiment was being used. As the capabilities of the NCF became known, projects and plans grew so that the invasion of Okinawa saw four Construction Brigades of 55,000 Seabees deployed there. It was beyond combat engineering, Seabees built the infra-structure needed to take the war to Japan. By wars end CBs had served on six continents and had constructed over 300 advanced bases on as many islands. The Pacific saw them build 111 major airstrips, 441 piers, PT boat & seaplane bases, bridges, roads, com-centers, petroleum storage tanks for 100,000,000 gal., hospitals to tend 700,000, and barracks to bunk 1.5 million.

Atlantic
In the Atlantic CBs biggest job was the preparations for the Normandy landing. Several months later 3 Seabee maintainece units were called to facilitate the crossing of the Rhine: CBMUs 627, 628, and 629. For CBMU 629 it was front-line work. The three CBMUs were formed when the 114th CB was decommissioned.

USMC historian Gordon L. Rottman wrote "that one of the biggest contributions the Navy made to the Marine Corps during WWII was the creation of the Seabees". In turn, the Corps would be influential upon the CB organization and its history. The Bureau of Yards and Docks original request of 28 December 1941 was for the authorization of three naval construction battalions. When those three battalions were formed the seabees did not have a fully functional base of their own. So, upon leaving navy boot camp, recruits were sent to National Youth Administration camps in Illinois, New Jersey, New York and Virginia to receive military training from the Marine Corps. In 1942 the marines issued USMC duffel bags and uniforms to Battalions 18, 19 , and 25. The 18th and 019th CBs both claim to have been the first CB authorized to wear the USMC uniform. Both received their military training and USMC issue at Marine Training Center, New River, N.C. Marine Corps Base Camp Lejeune. How many other battalions received the USMC issue is not recorded but it is known that the 25th, 31st, 43rd, 76th, 121st and 133rd NCBs did also. The Marine Corps listed CBs on their Table of organization: "D-Series Division" for 1942, "E-Series Division" for 1943, and "Amphibious Corps" for 1944/45.

Starting with the 1st Naval Construction Detachment (a.k.a. Bobcats), the Marines redesignated them the 3rd Battalion 22nd Marines. They were the very first Seabees and that was only the beginning. They joined the 22nd Marines in September 1943 and were put through intensive combat training. Not long after, the 4th Construction Detachment had men attached to the 5th Marine Defense Battalion on Funafuti for two years.

In 1942 the Marine Corps wanted one CB for each Marine Division, but was told no because of war priorities. However, by autumn CBs 18, 19 and 25 were attached to the Marine Corps as combat engineers. Each battalion was attached to a composite engineer regiment and redesignated as the 3rd Battalion of that Regiment. (See 17th Marine Regiment, 18th Marine Regiment, 19th Marine Regiment, and 20th Marine Regiment.) In August 1942, C company 18th NCB was transferred to the C.B. replacement group, FMF, San Diego. The rest of the 18th embarked from the FMF Base Depot, Norfolk, VA, en route to Guadalcanal where they would replace the 6th CB with the 1st Marine Division. Late 1943 men of the 6th Special CB were tasked to the 4th Marines Advance Depot in the Russells. In November, the 14th CB was tasked to the 2nd Raider Bn on Guadalcanal. Earlier in June, the 24th CB was tasked to the 9th Marine Defense Bn on Rendova. The 33rd CB had 202 men tasked to the 1st Pioneers as shore party for the 5th Marines on Peleliu. So were 241 from the 73rd CB. Also attached to the 1st Pioneers was the entire 17th Special CB (colored). At Enogi Inlet on Munda, the 47th had a det support the 1st and 4th Marine Raiders. On Bougainville, the 3rd Marine Div. made Cmdr. Brockenbrough of the 71st CB shore party commander. The 71st was supported by dets from the 25th, 53rd, and the 75th CBs. At Cape Torokina the 75th had 100 men volunteer to support the assault of the 3rd Marines. At the same time the 53rd provided shore parties to the 2nd Raiders on green beach and the 3rd Raiders on Puruata Island. The 121st was formed at the CB Training Center of MTC Camp Lejuene. There it was attached to the 4th Marine Div. as 3rd Bn 20th Marines.

In 1944 the Marine Engineer Regiments were inactivated. Even so, Marine Divisions still had a CB tasked to them. For Iwo Jima, the 133rd and 31st CBs were attached to the 4th and 5th Marine Divisions. The 133rd was tasked to the 23rd Marines as their shore party. The 31st CB was a attached to the 5th Shore Party Regiment with their demolitions echelon attached to the 5th Marine Div. Overall command of the Iwo Jima shore parties was the 8th Marine Field Supply Depot and a chief and 25 heavy equipment operators from the 8th CB volunteered to augment the Depot. Okinawa saw the 58th, 71st, 130th, and 145th CBs attached to the 6th, 2nd, and 1st Marine Divisions.

From Iwo Jima the 5th Marine Div. returned to Camp Tarawa to have the 116th CB tasked to the Division. When Japan fell the 116th CB went with the 5th Marine Div. as part of the occupation force. V-J day found thousands of Japanese troops still in China and the III Marine Amphibious Corps was sent there to get them back to Japan. A number of the 33rd NCR were assigned to III Marine Amphib. Corps for this mission: the 83rd, 96th, 122nd CBs and the 33rd Special CB.

Seabee Battalions were also tasked individually to the four Amphibious Corps. The 19th CB started out with the I Marine Amphibious Corps (I MAC) prior to being made an element of the 17th Marines. The 53rd CB was attached to I MAC as Naval Construction Battalion 1st M.A.C.". When I MAC was redesignated III Amphibious Corps the battalion became an element of the 1st Provisional Marine Brigade until the brigade was deactivated. For Guam, III Amphibious Corps had the 2nd Special CB, 25th and 53rd CBs. Lt. Cmdr. Whelan of the 25th CB was shore party commander for the 3rd Marines on beaches Red 1 and Red 2. V Amphibious Corps (VAC) had the 23rd Special and 62nd CBs on Iwo Jima. On Tinian the 6th Construction Brigade was attached to V Amphibious Corps.

When the war ended the Seabees had an unique standing with the U.S. Marine Corps. Seabee historian William Bradford Huie wrote "that the two have a camaraderie unknown else-wheres in the United States military". Even though they are "Navy" the Seabees adopted USMC fatigues with a Seabee insignia in place of the EGA. A number of WWII CBs adapted USMC insignia for their units, these included CBs 5, 18, 19, 25, 31, 53, 71, 117 and the 6th Brigade. The insignia each modified were the seahorse (5th Marine Rgt), Corps Castle (2nd Engineer Bn), the Eagle, Globe and Anchor, a divisional shield (3rd Marine Div.), a spearhead (5th Marine Div.), the eagle globe and anchor, a divisional shield (3rd Marine Div.), the USMC Bulldog, and an alligator with three stars (V Amphibious Corps).

see Notes: 2.1 a–e

In early May 1943, a two-phase "Naval Demolition Project" was directed by the Chief of Naval Operations "to meet a present and urgent requirement". The first phase began at Amphibious Training Base (ATB) Solomons, Maryland with the establishment of Operational Naval Demolition Unit No. 1. Six Officers and eighteen enlisted men reported from NTC Camp Peary dynamiting and demolition school, for a four-week course on May 14. Those Seabees, lead by Lieutenant Fred Wise CEC, were immediately sent to participate in the invasion of Sicily.

Naval Combat Demolition Units (NCDUs) consisted of one junior CEC officer and five enlisted and were numbered 1–216. After that first group had been trained Lt. Commander Draper Kauffman was selected to command the program that had been set up in Camp Peary's "Area E" where the Seabee dynamiting and demolition school opened. Between May and mid-July six NCDU classes were the first men graduated from the school at Camp Peary before the program was moved to Fort Pierce. The first class at Fort Pierce began in mid-July. Despite the move to Fort Pierce, Camp Peary remained Kauffman's manpower source. "He would go up to Camp Peary's dynamite school, assemble the (Seabees) in the auditorium and say, 'I need volunteers for hazardous, prolonged and distant duty." Fort Pierce had two Seabee units assigned to the school, Construction Battalion Detachment (CBD) 1011 and Construction Battalion Maintenance Unit (CBMU) 570. Their jobs were the construction and maintenance of the various obstacles needed for the demolitions class to practice their training. The men in those classes referred to themselves as "Demolitioneers".

The Navy had 34 NCDUs in England for the Invasion of Normandy. All told they suffered 53 percent casualties on Normandy. While waiting for D-day the NCDUs trained with the 146th, 277th and 299th Combat Engineer Battalions. Each NCDU had 5 men from a combat engineer battalion attached to the team. In the beginning the first 10 NCDUs were split into 3 groups. The whole thing was a bit ad-hoc as they had no commanding officer, but the senior officer was the leader of group III, Lt Smith (CEC). He served in that capacity unofficially for the entire group. His group III did a lot of experimental demolitions work and developed the Hagensen Pack. As more teams arrived a NCDU Command was created for Normandy. Afterwards four NCDUs that were on Utah were joined by nine others for Operation Dragoon.

With Europe invaded Admiral Turner requisitioned all available NCDUs from Fort Pierce for integration into the UDTs for the Pacific. Thirty NCDUs had been sent to the Pacific prior to Normandy while three had gone to the 8th Fleet in the Mediterranean. NCDUs 1–10 were staged at Turner City, Florida Island in the Solomons during January 1944. NCDU 1 went briefly to the Aleutians in 1943. NCDUs 4 and 5 were the first to see combat by helping the 4th Marines at Green island and Emirau Island. A few were temporarily attached to UDTs. Later NCDUs 1–10 were combined to form Underwater Demolition Team Able. This team was disbanded with NCDUs 2 and 3 plus 19, 20, 21 and 24 being assigned to MacArthur's 7th Amphibious Force and were the only NCDUs remaining at the war's end. The other men from Team Able were assigned to numeric UDTs.

see Notes: 2.2 a-e

Prior to Operation Galvanic and Tarawa, V Amphibious Corps had identified coral as an issue for future amphibious operations. RADM. Kelly Turner, commander V Amphibious Corps had ordered a review to get a grip on the problem. VAC found that the only people having any applicable experience with the material were men in the Naval Construction Battalions. Lt. Thomas C. Crist, of C Co 10th CB, LLP was in Pearl Harbor from Canton Island where he had been involved in a lagoon coral head clearance project. His being in Pearl Harbor was pivotal in UDT history. While there he learned of the Adm. Turner's interest in coral blasting and made contact. The Admiral tasked Lt. Crist to develop a method for blasting coral under combat conditions and putting together a team to do it. Lt. Crist started by getting men he had blasted coral with in CB 10. By the end of November 1943 he had close to 30 officers and 150 enlisted gathered at Waipio Amphibious Operating Base on Oahu.

Earlier that month the Navy learned a hard lesson about underwater obstructions with coral and tides during the invasion of Tarawa. It prompted Adm. Turner to request the creation of nine Underwater Demolition Teams to deal with the problem. Six would be assigned to VAC in the Central Pacific while the other three would go to III Amphibious Corps in the South Pacific. Adm. Turner chose the terms "under water" to distinguish from the Fort Pierce program. UDTs 1 & 2 were completely Seabees according to the UDT Archives with Seabees making up the vast majority of the men in teams 1–9, 13 and 15. How many Seabees were in UDT 10 is not cited in the records nor is anything stated for UDT 12. Seabees were roughly 20% of UDT 11.
UDT officers were mostly CEC. At formation UDT 10 was assigned 5 officers and 24 enlisted, trained as OSS Operational Swimmers. The group was multi-service: Army, Coast Guard, Marine Corps and Navy (Maritime Unit: Operational Swimmer Group II). but, the OSS was not allowed to operate in the Pacific Theater. Adm. Nimitz needed swimmers and did approve their transfer from the OSS to his operational and administrative control. The MU men brought with them the swimfins they had trained with and the Seabees made them a part of UDT attire as quickly as the Supply dept. could get them. In the Seabee dominated teams the next largest group of UDT volunteers came from the joint Army-Navy Scouts and Raiders school that was also in Fort Pierce and the Navy's Bomb disposal School.

The first underwater demolition team commanders were Cmdr. E.D. Brewster (CEC) UDT 1 and Lt. Crist (CEC) UDT 2. (Lt. Crist was replaced because Adm. Conolly wanted Line Officers with combat experience) When Teams 1 and 2 were formed they were "provisional" with 180 men total. The core of these teams came from the Seabees Lt. Crist had already gathered in Hawaii with seven different CBs represented in UDT 2. They wore fatigues with life-vests and were not expected to leave their boats similar to the NCDUs. However, at Kwajalein Fort Pierce protocol was changed. Adm.Turner ordered daylight reconnaissance, and Ens. Lewis F. Luehrs and Seabee Chief Bill Acheson wore swim trunks under their fatigues. They stripped down, spent 45 minutes in the water in broad daylight. When they got out were taken directly to Adm. Turner's flagship, still in their trunks, to report. Adm. Turner concluded that daylight reconnaissance by individual swimmers was the way to get accurate information on coral and underwater obstacles for upcoming amphibious operations. This is what he reported to Adm. Nimitz. At Engebi Cmdr. Brewster was wounded and all of the men with Ens. Luehrs wore swim trunks under their greens. The success of those UDT 1 Seabees not following Fort Pierce protocol rewrote the UDT mission model and training regimen. Ensign Luehrs and Chief Acheson were each awarded a Silver Star for their exploit and they unintentionally created the UDT's "naked warriors" image. Diving masks were not common in 1944 and a few men had tried using goggles at Kwajalein. They were a rare item in the Hawaiian sports stores so Lt Crist and Seebee Chief Howard Roeder and put in a request to supply for them. A fortuitous observation spotted a diving mask ad in a magazine, which generated a priority dispatch to the States appropriating the store's entire stock.

Three days after requesting the creation of UDTs Adm. Turner also requested the creation of a "Naval Combat Demolition Training & Experimental Base" at Kihei, Hawaii. The actions of UDT 1 were immediately incorporated into the training, making it distinctly different from that at Fort Pierce. The first training officer was Lt. Crist. He held the position briefly from the decommissioning of UDTs 1 & 2 until when he was made Commander of UDT 3. When UDT 3 returned from Leyte in November 1944 the team became the training instructors of the Oahu school and Lt. Crist was again Officer in charge of training. Under Lt Crist the 2 month training course was broken into four 2 week blocks. With an overall emphasis on swimming and reconnaissance the teams also covered night ops, problems of control, small arms, bivouacking, small unit tactics, along with coral and lava blasting. The team would remain in these jobs until April 1945 when it was sent to Fort Priece to do the same job there. Lt Crist had been promoted to Lt. Cmdr and was sent back to Hawaii but his Team 3 Seabees would train teams 12-22. Teams 12, 13 and 14 all had men from Team Able. UDT 14 is called the first "all fleet team" even though Seabees from Team Able were attached and the Commander and XO were both CEC (Ltjg A.B. Onderdonk and Ltjg C.E. Emery). Seabees of UDTs 13 and 15 would be on the beach at Iwo Jima. They scouted prior to D-day, helped direct the first landing craft to the correct beaches on D-day and helped clear the beaches of debris on D-plus 2. UDT 15, formed completely of NCDUs made it a Seabee team too. After July 1944 new UDTs were completely USN with no Army or USMC. In 1945 CBMU 570 was tasked to support UDT coldwater training at ATB Oceanside, CA

On Guam UDT 8 requested permission to build a base. It was approved by Capt. Grayson AdComPhibsPac but disapproved by the Marine Island Commander. Team 8 took its needs to fellow Seabees in the battalions on Guam to appropriate everything needed to build their base. The curshed coral paving only got done the night before Admiral of the Fleet, Chester W. Nimitz, Commander in Chief, U.S. Pacific Fleet did a base inspection of Teams 8 and 10. The Admiral gave the UDT base a glowing review.

At wars end 34 teams had been formed with teams 1–21 having actually been deployed. The Seabees provided over half of the men in the teams that saw service. The U.S. Navy did not publicize the existence of the UDTs until post war and when they did they gave credit to Lt. Cmdr. Kauffman and the Seabees. During WWII the Navy did not have a rating for the UDTs nor did they have an insignia. Those men with the CB rating on their uniforms considered themselves Seabees that were doing underwater demolition. They did not call themselves "UDTs" or "Frogmen" but rather "Demolitioneers" which had carried over from the NCDUs and LtCdr Kauffmans recruiting them from the Seabee dynamiting and demolition school. UDTs had to meet the military's standard age guidelines, Seabees older could not volunteer. In preparation for the invasion of Japan the UDTs created a cold water training center and mid-1945 UDT men had to meet a new physical standard. UDT 9 lost 70% of the team to this change.
see Notes: 2.3 a-v

In February 1942 CNO Admiral Harold Rainsford Stark recommended African Americans for ratings in the construction trades. In April the Navy announced it would enlist African Americans in the Seabees. Even so, there were just two regular CBs that were "colored" units, the 34th and 80th NCBs. Both had white Southern officers and black enlisted. Both battalions experienced problems with that arrangement that led to the replacement of the officers.

The Navy had a huge need for cargo handlers. The lack of stevedores for unloading ships in combat zones was creating a problem. On 18 September 1942 authorization was granted for the formation of cargo handling CBs identified as "Special Construction Battalions". By wars end 41 Special CBs had been commissioned of which 15 were "colored". Special CBs were the first fully integrated units in the U.S. Navy. The wars end also brought the decommissioning of every one of those units. The Navy's contemporary version of these units are Navy Cargo Handling Battalions of the Navy Expeditionary Logistics Support Group (United States).

The actions of the 17th Special CB (colored) at Peleliu 15–18 September 1944 were noteworthy. On D-day at Peleliu, the 7th Marines were in a situation where they did not have enough men to man the lines and get the wounded to safety. Coming to their aid were the 2 companies of the 16th Marine Field Depot (colored) and the 17th Special CB. The Japanese mounted a counter-attack at 0200 hours on D-day night. By the time it was over, nearly the entire 17th had volunteered to carry ammunition to the front lines on the stretchers they brought the wounded back on. They volunteered to man the line where the wounded had been, man 37mm anti-tank guns that had lost their crews and volunteered for anything the Marines needed. The 17th remained with the 7th Marines until the right flank had been secured on D plus 3. According to the Military History Encyclopedia on the Web, "were it not for the Black Marine shore party personal the counterattack on the 7th Marines would not have been repulsed".

A Construction Battalion Detachment (CBD) was formed from "screening Camp Peary and the NCF for geologists, petroleum engineers, driller (oil), tool pushers, roustabouts and roughnecks" and later designated 1058. Many of the enlisted and officers were chosen for their arctic experience with CB 12 and CB 66. Once chosen the detachment was assembled at Camp Lee Stephenson. In 1944 Congress had earmarked $1,000,000 for Operation Pet 4 to determine if there was actually oil in NPR 4 (U.S. Navy Petroleum Reserve No. 4). NPR-4 had been created and placed in the oil reserve in 1923. The area is known today as the National Petroleum Reserve in Alaska. The detachment's mission had a list of stated tasks: a. do a detailed geologic study at Umiat and Cape Simpson, b. drill test and core holes, c. drill a deep well, d. complete aerial and overland pipeline surveys. e. build a base camp with runway, f. build a field camp with runway. In 1944 the base camp was constructed at Point Barrow. Four D-8s with twenty sleds of supplies were readied for the 330 mile trek to Umiat once the tundra had frozen to support them. After those supplies were delivered the Cats returned to get the heavy well equipment. During the summer of 1945 a 1,816' wildcat was drilled and designated Seabee#1 before being shut down by the cold. The well site was near four known seeps at Umiat in the very south-east of NPR 4. The rock in the area was from the Upper Cretaceous and a stratum of it was named the "Seabee Formation". On the coast the Seabees drilled test holes at Cape Simpson and Point Barrow. In March 1946 civilians took over the project. Some had been members of CBD 1058 and had been hired immediately upon discharge for the same job they had performed for the Navy." The Navy drew upon the cold weather experience it gained from CBD 1058 and applied it in Operation Highjump and Operation Deep Freeze. – Today Seabee #1 is a USGS monitor well.

Land surveys

Twice the Seabees have been tasked with large scale land surveys. The first was done by CBD 1058 for a proposed NPR 4 pipeline route to Fairbanks. The Trans-Alaskan pipeline follows a portion of their survey from roughly the arctic circle to Fairbanks. The second would be done by a Seabee team from MCB 10. That group was sent to Vietnam in 1956 to survey and map that country's entire road network. This work would be heavily drawn upon during the Vietnam War.

see Notes: 2a–2wa

When the Japanese signed the surrendered CB 114 was in the Aleutians. In September 1945 the battalion was ordered to send a detachment to the USSR to build a Naval Advance Base (a Fleet Weather Central). It was located outside Petropavlovsk-Kamchatsky on the Kamchatka Peninsula and code named TAMA. The original agreement gave the Seabees 3 weeks to complete the camp. Upon arrival the Russians told the Seabees they had 10 days and were amazed that the Seabees did it. It was one of two that Stalin agreed to. The other was near Khabarousk, Siberia in buildings provided by the Russians.

With the fall of Japan Operation Beleaguer was initiated for the repatriation of the remnants of the Japanese Army home left in China. Part of the 33rd CB Regiment was tasked; CBs 83, 96, 122 and 32nd Special. These units landed at Tsingtao and Tangku in November 1945 attached to the 6th Marine Division. CB 42 and A Co. 33rd Special landed at Shanghai attached to Naval Advance Base Unit 13. With the war over, the discharge of all men eligible for discharge left only enough men for one CB and the two Specials. The men were consolidated in the 96th with the other units decommissioned. In December the 96th started construction of airfields at Tsingtao and Chinwangtao in support of III Marine Amphibious Corps operations. On 20 May 1946 orders were issued for CB III Marine Amphibious Corps to inactivate 96 CB on 1 August. Prior to that the 6th Marine Division was renamed the 3rd Marine Brigade. It existed only until 10 June. At that time the 96th CB was transferred to the 4th Marines of the 1st Marine Division and were deactivated from them in August. An unknown CBMU was supposed to take over the maintenance of the airfields, but Operation Beleaguer was close to completing its mission.

In early 1946 the 53rd NCB was still attached to III Marine Amphibious Corps. The unit was deployed for the preparations of Operation Crossroads and the nuclear testing at Bikini Atoll. The battalion was assigned to Task Group 1.8 and designated TU 1.8.6. 53's project list included observation and communication towers, general base facilities, and dredging the lagoon. On 3 August the battalion was decommissioned with the men transferred to CBD 1156 that was commissioned on Bikini. The TU 1.8.6 designation continued with them. The Battalion remained on the atoll for nine days after the second nuclear test when it was detached from the Marine Corps and deactivated there.

UDT 3 was designated TU 1.1.3 for the operation. On 27 April 1946, 7 officers and 51 enlisted embarked the USS Begor (APD-127) at the Seabee's base, Port Hueneme, for transit to Bikini. Afterwards, in 1948, the displaced bikinians put in a request to the U.S. Navy to blast a channel access to the island Kili where they had been relocated. This was given to the Seabee detachment on Kwajelin who placed a request for UDT 3. The King of the Bikinians was so pleased he held a going-away feast for the UDTs.

In January 1947, CBs 104 and 105 were reactivated. The 121st CB was decommission
ed December 31st, 1947 and re-designated CBD 1504. CBMU 650 was at Attu in 1947. The Seabees were officially organized in the Naval Reserve during that month also. The 30th NCR was home-ported on Guam. It comprised Construction Battalion Detachments 1501-13, and NCB 103. In 1949, when the 103rd was made a Mobile Construction Battalion CBs 104 and 105 were made Amphibious Construction Battalions. Between 1949 and 1968 CBs were organized into these two types of battalions MCBs or ACBs. In June 1950 the Naval Construction Force numbered approximately 2,800 active duty. All of the 30th's CBDS, formed in 1947, were decommissioned by January 1953.

The outbreak of the Korean War in June 1950 led to a call-up of more than 10,000 men from the Seabee Naval Reserve program. Seabees landed at Inchon with the assault troops dealing with enormous tides and enemy fire while installing causeways. Their actions there and elsewheres illustrated their necessity. During the Korea the authorized size of a CB was 550 men. When the truce was declared there was no CB demobilization as there had been at the end of WWII.

During the Korean War, the U.S. realized the need an air station in this region. Cubi Point in the Philippines was selected. Civilian contractors were approached for bids. After seeing the Zambales Mountains and the maze of jungle, they claimed it could not be done. The Navy then turned to the Seabees. The first to arrive was CBD 1802 to do the surveying. MCB 3 arrived on 2 October 1951 to get the project going and was joined by MCB 5 in November. Over the next five years, MCBs 2, 7, 9, 11 and CBD 1803 all contributed to the effort. They leveled a mountain to make way for a nearly runway. NAS Cubi Point turned out to be one of the largest earth-moving projects in the world, equivalent to the construction of the Panama Canal. Seabees there moved of dry fill plus another 15 million that was hydraulic fill. The $100 million facility was commissioned on 25 July 1956, and comprised an air station and an adjacent pier that was capable of docking the Navy's largest carriers. Adjusted-for-inflation, today's price-tag for what the Seabees built at Cubi Point would be $906,871,323.53.

Seabee Teams also called Civic Action Teams or CAT 

The first Seabees to be referred to as Seabee Teams were CBD 1802 and CBD 1803. Someone in the U.S. State Department learned of these men and adopted the idea as a way for making "good use" of the Seabees in the Cold War. They could be sent as "U.S. Good Will Ambassadors" to third world nations as a means to combat the spread of Communism and promote "Good Will", a military version of the Peace Corps. These 13 man teams would construct schools, drill wells or build clinics creating a positive image or rapport for the U.S. in the developing world. They were utilized by the United States Agency for International Development and were in S.E. Asia by the mid 1950s. Then in the early sixties the U.S. Army Special Forces were being sent into rural areas of South Vietnam to develop a self-defense force to counter the Communist threat and making use of the Seabee teams at these same places made perfect sense to the CIA. So twelve "Seabee teams with Secret Clearances were sent to Vietnam to assist the U.S. Army's Special Forces in the CIA funded Civilian Irregular Defense Group program (CIDG)" in the years 1963–1965. By 1965 the U.S. Army had enough engineers in theater to end the Seabees involvement with the Special Forces. At first they were called Seabee Technical Assistance Teams or STAT and were limited to two teams in the country at a time. Teams after STAT 1104 were renamed Seabee Teams and by 1969 there were 17 teams in theater, concurrently. In total, 128 teams were sent to Vietnam. As a group Seabee Teams received many awards for heroism. Teams were sent to other nations as well. The Royal Thai government requested STATs in 1963 and ever since the Seabees have continued to deploy teams.

Construction Civic Action Details or CCAD
CCADs or "See-Kads" are larger civic action units of 20–25 Seabees with the same purpose as Seabee Teams. The CCAD designation is not found in the record prior to 2013 and there is no explanation stating why these construction crews are called "details" and not "detachments".

Operation Highjump

On 2 and 5 December 1946, 166 Seabees sailed from Port Hueneme on the USS Yancey and USS Merrick assigned to Operation Highjump. They were part of Admiral Richard E. Byrd's Antarctic expedition. The U.S. Navy was in charge with "Classified" orders "to do all it could to establish a basis for a (U.S.) land claim in Antarctica". The Navy sent the Seabees to do the job starting with the construction of Little America (exploration base) IV as well as a runway for aerial mapping flights. This Operation was vastly larger than IGY Operation Deep Freeze that followed.

Operation Deep Freeze

In 1955, Seabees began deploying yearly to the continent of Antarctica as participants in Operation Deep Freeze. Their mission was to build and expand scientific bases located on the frozen continent and further establish a land claim for the U.S. (Fig. 19). The first "wintering over" party included 200 Seabees who distinguished themselves by constructing a ice runway on McMurdo Sound. Despite a blizzard that undid the entire project, the airstrip was completed in time for the advance party of Deep Freeze II to become the first to fly into the South Pole by plane. MCB 1 was assigned for Deep Freeze II.

Despite the adverse conditions, Seabees added to their list of accomplishments such things as snow-compacted roads, underground storage, laboratories, and living areas. One of the most notable achievements was MCB 1s construction of Antarctica's first nuclear power plant in 1962, which got them a Navy Unit Commendation. Another, in 1975, was the construction of the Buckminster Fuller Geodesic dome at Amundsen-Scott South Pole Station by NMCB 71. with a diameter of and high. This became a symbolic icon of the United States Antarctic Program until it was replaced by the new elevated station. When that happened the Dome was disassembled and sent to CBC Port Hueneme for storage.

Notes: 7a-7b

Seabees deployed to Vietnam twice in the 1950s. First in June 1954 as elements of Operation Passage to Freedom and then two years later to map and survey the nation's roads. Seabee teams 501 and 502 arrived on 25 January 1963 and are regarded as the first Seabees of the Vietnam War. They were sent to Dam Pau and Tri Ton to build camps for the Special Forces.In 1964 ACB 1 was the first CB in the theatre. Beginning in 1965 Naval Construction Regiments (NCRs) deployed throughout Vietnam. Seabees supported the Marines at Khe Sanh and Chu Lai combat base in addition to building numerous aircraft-support facilities, roads, and bridges. They also worked with and taught construction skills to the Vietnamese. In June 1965, Construction Mechanic 3rd Class Marvin G. Shields of Seabee Team 1104 was at the Battle of Dong Xoai. He was posthumously awarded the Medal of Honor and is the only Seabee to be awarded the medal. Seabee "Civic Action Teams" continued throughout the Vietnam War and often engaging enemy forces alongside Marines and Army special forces. Teams typically built schools, clinics, or drilled wells. In 1966 Seabees repaired the airfield at Khe Sahn in four days, with 3,900 feet of 60-foot-wide aluminum matting. General Westmoreland "called it one of the most outstanding military engineering feats in Vietnam." MCB 121 suffered the first CEC loss in 1967 and MCB 4 had a detail at Con Thien whose actions were a near repeat of Dong Xoai. 

In 1968 the Marine Corps requested that the Navy make a name change to the CBs. The Marines were using "MCB" for Marine Corps Base while the Navy was using "MCB" for Mobile Construction Battalions. The Navy then added "Naval" to MCB creating the NMCBs that now exist. During that year the 30th Naval Construction Regiment had five battalions in the Da Nang area and two at Chu Lai. The 32d NCR had three battalions tasked around Phu Bai and one at Dong Ha. In May 1968 two reserve battalions were activated 12 and 22, which brought the total number of battalions involved in Vietnam to 21. Both ACBs were in theater as well as Construction Battalion Maintenance Units (CBMUs) 301 and 302. NMCB 10 had an unusual "tasking" supporting the 101st Airborne in 1968. During 1969 the Seabees deployed topped out at 29,000, from there they drew-down began. The last battalion withdrew the end of 1971 and the last Seabee teams were out a year later. When it was over they had sent 137 Seabee teams, built 15 CB camps, and deployed 22 battalions. CBMU 302 became the largest CB ever at over 1400 men and was homeported at Cam Rahn Bay. On 23 April 1975 it was announced that U.S. involvement in Vietnam was over. 
thst day saw NMCB 4 start construction of a temporary camp for Operation New Life on Guam. It was on the site of a WWII Japanese airfield on Orote peninsula. In seven days 2000 squad tents were put up and 3500 when done.

During Vietnam the Seabees had a few uniform variations. One was the stenciling of the unit number across the back of the field jacket M-65(e.g., "MCB 1"). Another was the collar and cover devices for E4 E6 enlisted. The Navy authorized that the "crow" be replaced by the rating insignia for each trade. These devices were made in gold and black (subdued). Nametags on the fatigues started out white with a multicolored seabee until 1968. The NAVCATs became the only Seabees to ever be authorized to wear a shoulder patch.

NAVCATs Naval Construction Action Teams

CBMU 302 had 23 NAVCATS total with 15 active at its peak. Teams were numbered 1-23. They were Vice Admiral Elmo Zumwalt's, then Navy commander in Vietnam, wholesale expansion of the Seabee Team concept that he submitted in November 1968 to General Creighton Abrams commander of Military Assistance Command, Vietnam.

Agent Orange
Many Seabees were exposed to the defoliant herbicide while in Vietnam. NCBC Gulfport was the largest storage depot in the United States for agent orange. From there it was shipped to Vietnam. In 1968 the NCBC received 68,000 barrels to forward. Long term barrel storage began in 1969. That lasted until 1977. The site covered 30 acres and was still being cleaned up in 2013.

In 1960 MCB 103 built a Project Mercury telemetry and ground instrumentation station on Canton island.

On 28 January 1969 a detachment of 50 men from Amphibious Construction Battalion 2 augmented by an additional 17 Seabee divers from both the Atlantic and Pacific fleets as well as the 21st NCR began the installation of the Tektite habitat in Great Lameshur Bay at Lameshur, U.S. Virgin Islands. The Tektite program was funded by NASA and was the first scientists-in-the-sea program sponsored by the U.S. government. The Seabees also constructed a 12-hut base camp at Viers that is used today as the Virgin Islands Environmental Resource Station. The Tektite project was a product of the Cold War and caused the U.S. Navy to realize it needed a permanent Underwater Construction capability. "It was this project that led to the formation the Seabee Underwater Construction Teams".

see Notes: 10a-10c

As the Cold War wound down, new challenges were presented by the increased incidence of terrorism. This was in addition to numerous ongoing support missions like the Guam and Okinawa as well as the Navy and Marine Corps had Bases in Japan, Philippines, Puerto Rico, Cuba and Guatemala that still required Seabee support. Even though the Cold war had wound down Cold War Facilities still required the Seabees for Polaris and Poseidon submarines at Holy Loch, Scotland, Rota, Spain, Naples, Italy, and Souda Bay, Crete.
In 1971, the Seabees began their second huge peacetime construction Diego Garcia on a small atoll in the Indian Ocean. This project began in 1971 and was completed in 1987 at a cost of $200 million. Because of the extended time-frame, it is difficult to inflation-adjust that cost into today's dollars. The complex accommodates the Navy's largest ships and cargo planes. This base proved invaluable when Iraq invaded Kuwait in August 1990 and Operations Desert Shield and Desert Storm were launched.

CN Carmella Jones became the first female Seabee when she cross-rated to Equipment Operator during the summer of 1972.

Seabee construction efforts led to the expansion and improvement of Naval Air Facility, Sigonella, Sicily, turning this into a major base for the United States Sixth Fleet aviation activities.

There were combat roles as well. In 1983, a truck bomb demolished the barracks the Marines had secured in Beirut, Lebanon. After moving to the Beirut International Airport and setting up quarters there, Druse militia artillery began harassing the Marines. After consultations with the theater commander, Marine amphibious command and combat engineers, the forward-deployed battalion, NMCB-1 in Rota, Spain, sent in a 70-man AirDet working party with heavy equipment. Construction of artillery-resistant quarters went on from December 1983 until the Marines' withdrawal in February 1984. Only one casualty occurred when an equipment operator using a bulldozer to clear fields of fire was wounded by an RPG attack. Seabee EO2 Kirt May received the first Purple Heart awarded to a Seabee since Vietnam.

Robert Stethem was executed by the Lebanese Shia militia Hezbollah when they hijacked TWA Flight 847 in 1985. Stethem was a Steelworker Second Class (SW2), a Seabee diver and member of Underwater Construction Team One. The is named in his honor. On 24 August 2010, on board USS "Stethem" in Yokosuka, Kanagawa, Japan, Stethem was posthumously made an honorary Master Chief Constructionman (CUCM) by the Master Chief Petty Officer of the Navy and awarded the Prisoner of War Medal.

During the Persian Gulf War, more than 5,000 Seabees (4,000 active and 1,000 reservists) served in the Middle East. In August 1990 the First Marine Expeditionary Force (I MEF) was initially assigned NMCBs 4, 5, 7, and 40. The first Seabees in theater were a Detachment from ABC 1 soon to be joined by a Detachment from ACB 2. Shortly after them CBUs 411 and 415 arrived in Saudi Arabia. Mid September the Air-Dets for the four CBs arrived to build air fields for Marine Air Groups 11, 13, 16, and 25 of the 3rd Marine Air Wing. NMCB 7 was the first to arrive. Camps were constructed for both the 1st and 2nd Marine Divisions as well as Hq complexes for MEF I and II. Overall, in Saudi Arabia, Seabees built ten camps, fourteen galleys, and 6 million ft² (600,000 m²) of runways and parking aprons as well as over 200 helo zones. They built and maintained two 500-bed Fleet Hospitals near Al-Jubayl. The 3rd NCR was activated to provide a command structure. NMCBs 24 and 74 were also deployed in support of the Marine Corps. A desert camp was constructed at Ras Al Mishab, near the Kuwaiti border, and named "Camp Nomad" where it supported MAG 26.

Seabees were deployed from the beginning of the invasion of Afghanistan in 2001 and Iraq in 2003. They provided the required construction skills to rebuild the infrastructure of both Afghanistan and Iraq. All active and reserve NMCBs and NCRs were deployed to get the job done. NMCB 133 deployed to FOB Camp Rhino and Kandahar Airfield where a detention facility was constructed. One of their most high-profile tasks in Iraq was the removal of statues of Saddam Hussein in Baghdad. In Afghanistan, the Seabees' main task was the construction of multiple forward operating bases.

Since 2002, Seabees have provided critical and tactical construction skills in an effort to win the hearts and minds of locals in the Philippines. Their efforts have begun to deter the rising influence of radical terrorists in the southern Philippines, most notably the Abu Sayyaf's jungle training area. Seabees work along with Army, Marines, and Air Force under Joint Special Operations Task Force-Philippines.

see Notes:


At present, there are six active-duty Naval Mobile Construction Battalions (NMCBs) in the United States Navy, split between the Pacific Fleet and the Atlantic Fleet.

30th Naval Construction Regiment, Hq Guam Pacific Fleet Homeport for the Pacific Fleet Battalions is Port Hueneme, CA.

22nd Naval Construction Regiment, Hq Gulfport, Mississippi Atlantic Fleet Homeport for the Atlantic Fleet battalions is Gulfport MS.

NCF Reserve
From the early 1960s through 1991, reserve battalions were designated as Reserve Naval Mobile Construction Battalions (RNMCBs). After 1991, the word "reserve" was dropped, signifying the integration of reserve units with the active units of the NCF.

Detachment: This is a construction crew that is sent to smaller construction projects "detached" from the battalion's "main body" deployment site. 

Battalion: The battalion is the basic unit typically having a Hq Company plus four Construction Companies: A, B, C, & D. Seabee battalions are constituted in such a way as to be self-sustaining in the field. 

Regiment: During the rapid build-up of the Seabees during World War II, the number of battalions in a given area increased and larger construction programs were undertaken. This necessitated a higher command echelon to plan, coordinate, and assign the work of several battalions in one area. As a result, Naval Construction Regiments (NCRs) were established in December 1942.

Division: The Global War on Terror lead to the creation of a single command for global Seabee operations. On August 9, 2002, the 1st Naval Construction Division (1NCD) was commissioned at NAB Little Creek, Va. Since January 2006, 1NCD was a subordinate unit of Navy Expeditionary Combat Command (NECC). First Naval Construction Division (1NCD) was decommissioned May 31, 2013. The 1NCD staff was integrated into the NECC. Some 1NCD functions were transferred to the newly formed Naval Construction Groups (NCGs) in Gulfport, MS, and Port Hueneme, CA.

Naval Construction Groups: In 2013, the Seabee Readiness Groups (SRGs) were decommissioned, and re-formed into Naval Construction Groups 1 and 2. They are regimental-level command groups, tasked with administrative and tactical control of Seabee Battalions, as well as conducting pre-deployment training of NCF units in the NCGs' respective home port locations. Currently, Naval Construction Group Two (NCG-2) is based at CBC Gulfport, and Naval Construction Group One (NCG-1) is based at CBC Port Hueneme.

Seabee Engineering Reconnaissance Teams (SERTs)
Seabee Engineer Reconnaissance Teams are ten-person teams, developed during Operation Iraqi Freedom (OIF). SERTs are divided into three elements: liaison, security, and a reconnaissance. The liaison (LNO) element has a CEC officer and two petty officers who are communications specialists. They are responsible for communications transferring engineering assessments, intelligence, and in receiving engineering reach-back solutions from higher echelons. The reconnaissance element has a CEC officer, who is the Officer-in-Charge (OIC), a Builder or Steelworker chief petty officer who has some bridge construction experience, and petty officers of OF-7 ratings. The OIC is normally CEC with a civil/structural engineering background. The unit has a corpsman or medically-trained member, with the rest of the team selected for being the best of their trades in their battalion. All are qualified Seabee Combat Warfare Specialists. The UCTs proved the SERT concept was viable leading to it being adopted throughout OIF.

Amphibious Construction Battalions (PHIBCBs)
ACBs (also abbreviated as PHIBCB) trace their linage to the pontoon assembly CBs formed during World War II. On 31 October 1950, MCBs 104 and 105 were re-designated ACB 1 and ACB 2, and assigned to Naval Beach Groups. ACBs report to surface TYCOMs. Additionally, an ACB has a different personnel-mix to an NMCB, with half the enlisted personnel being Seabee rates and the other half being fleet rates.

Construction Battalion Maintaince Units:

When first organized during World War II, these units consisted of approximately one fourth of the personnel of an NCB, and were intended to take over the maintenance of bases on which major construction had been completed. Today, CBMU's provide public works support at Naval Support Activities, Forward Operating Bases, and Fleet Hospital/Expeditionary Medical Facilities during wartime or contingency operations for a Marine Expeditionary Force (MEF), Marine Expeditionary Group (MEG), or NSW. They also provide disaster recovery support to Naval Regional Commanders in CONUS.

NAVFAC Engineering & Expeditionary Warfare Center Ocean Facilities Department supports the Fleet through the support it gives the Underwater Construction Teams". UCTs deploy worldwide to conduct underwater construction, inspection, repair, and demolition operations of ocean facilities, to include repair of battle damage. They maintain a capability to support a Fleet Marine Force amphibious assault, subsequent combat service support ashore, and self-defense for their camp and facilities under construction. UCT1 is home ported at Virginia Beach, Virginia, while UCT2 is at Port Hueneme, California.

Underwater Construction Team (UCT):
"NAVFAC Engineering & Expeditionary Warfare Center Ocean Facilities Department supports the Fleet through the support it gives the Underwater Construction Teams". UCTs deploy worldwide to conduct underwater construction, inspection, repair, and demolition operations of ocean facilities, to include repair of battle damage. They maintain a capability to support a Fleet Marine Force amphibious assault, subsequent combat service support ashore, and self-defense for their camp and facilities under construction. UCT1 is home ported at Virginia Beach, Virginia, while UCT2 is at Port Hueneme, California.

Public Works: U.S. Naval Bases: These units have CEC officers leading them and enlisted Seabees for the various crews. About one-third of new Seabees are assigned to Public Works Departments (PWD) at naval installations both within the United States and overseas. While stationed at a Public Works Department, a Seabee has the opportunity to get specialized training and extensive experience in one or more facets of their rating. Some bases have civilians that augment the Seabees, but the department is a military organization.


Notes: 14.1 a-d

Naval Intelligence: NAVFACs

The Navy built 22 Naval Facilities (NAVFACs) for its Sound Surveillance System (SOSUS) to track Soviet submarines. They were in service 1954-79 with Seabees staffing the Public works at each Facility. In the 1980s technology reduced the number of tracking stations to 11 with advent of the Integrated Underwater Surveillance System (IUSS). NAVFAC tracking facilities were finally undone by further advances in tech, the end of the Cold War and disclosures by John Walker to the Soviets.

The Seabees have also been tasked building Naval Communication facilities. One at Nea Makri Greece was built by MCB 6 in 1962 and later upgraded by NMCB 133. Naval Communications Station Sidi Yahya is another going back to WWII another is NavCommSta Guam. It started out on the island as the Joint Communications Agency (JCA) in 1945.

Camp David is officially known as Naval Support Facility Thurmont, because it is technically a military installation. The staffing is primarily provided by the CEC, Seabees, and Marines of the U.S. Navy and Marine Corps. "In the early 1950s, the first Seabee BUs, UTs and CEs took over routine maintenance and repairs of the base. Although there have been vast changes made at the Camp over the years, Seabees continue to staff base public works while keeping the grounds in an impeccable condition." Additional Naval rates were added to oversee base administrative functions. "Selectees undergo a single scope background investigation to determine if they are eligible for a Top Secret Sensitive Compartmentalized Information (TS/SCI) Yankee White (YW) clearance. All personnel assigned to duty in Presidential support activities are required to have a "Yankee White" clearance. The tour lasts 36 months." When the base has a larger construction project a regular Naval Construction Battalion will send a detachment to take care of the job. CBs 5 and 133 have drawn these assignments.

In 1964, at the height of the Cold War, Seabees were assigned to the State Department because listening devices were found in the Embassy of the United States in Moscow. Those initial Seabees were "Naval Mobile Construction Battalion FOUR, Detachment November". The U.S. had just constructed a new embassy in Warsaw. After what had been found in Moscow Seabees were dispatched and found many "bugs" there also. This led to the creation of the Naval Support Unit in 1966 as well as the decision to make it permanent two years later. That year William Darrah, a Seabee of the support unit, is credited with saving the U.S. Embassy in Prague, Czechoslovakia from a potentially disastrous fire. In 1986, "as a result of reciprocal expulsions ordered by Washington and Moscow" Seabees were sent to "Moscow and Leningrad to help keep the embassy and the consulate functioning".

The Support Unit has a limited number of special billets for select NCOs, E-5 and above. These Seabees are assigned to the Department of State and attached to Diplomatic Security. Those chosen can be assigned to the Regional Security Officer of a specific embassy or be part of a team traveling from one embassy to the next. Duties include the installation of alarm systems, CCTV cameras, electromagnetic locks, safes, vehicle barriers, and securing compounds. They can also assist with the security engineering in sweeping embassies (electronic counter-intelligence). They are tasked with new construction or renovations in security sensitive areas and supervise private contractors in non-sensitive areas. Due to Diplomatic protocol the Support Unit is required to wear civilian clothes most of the time they are on duty and receive a supplemental clothing allowance for this. The information regarding this assignment is very scant, but State Department records in 1985 indicate Department security had 800 employees, plus 1,200 Marines and 115 Seabees. That Seabee number is roughly the same today.

Notes: 9a-9c and 5a

Combat Service Support Detachments (CSSD) have several hundred Seabees assigned in support of Naval Special Warfare (NSW) units based out of Coronado, CA, and Virginia Beach, VA. Seabees provide the field support for power generation/distribution, logistical movement, vehicle repair, construction/maintenance of encampments, water purification or facilities. Seabees assigned to support NSW receive extra training in first aid, small arms, driving, and specialized equipment. and are expected to qualify as Expeditionary Warfare Specialists. Seabees assigned to NSW are eligible to receive the following Naval Enlisted Classifications upon filling the requirements: 5306 – Naval Special Warfare (Combat Service Support) or 5307 – Naval Special Warfare (Combat Support). They also can apply for selection to support the NSW Development Group.

The trainees begin "A" School (trade school) upon completion of boot camp, spending about 75% of the twelve weeks immersed in hands-on training. The remaining 25% is spent in classroom instruction. From "A" School, new Seabees most often report to a NMCB command for their first tour of duty. For training, the new Seabees attend a four-week course of Expeditionary Combat Skills (ECS) at the NCBC in Gulfport, Ms, or Port Hueneme, Ca. ECS is also being taught to all personnel who report to a unit in the Navy Expeditionary Combat Command. ECS is a basic combat-skills course in learning map reading and land navigation, battlefield first aid, formulating defensive plans, conducting reconnaissance, and other combat-related skills. Half of each course is spent at a shooting range learning basic rifle-marksmanship and qualifying with a M16A2 or M16A3 rifle and the M9 service pistol. Those that are posted Alfa Company of an NMCB may be assigned to a crew-served weapon, such as the MK 19 40 mm grenade launcher, the M2HB .50-caliber machine gun, or the M240 machine gun. Many reserve units still field variants of the M60 machine gun. Until 2012, Seabees wore the U.S. Woodland camouflage uniform or the legacy tri-color Desert Camouflage Uniform, the last members of the entire U.S. military to do so, but have now transitioned to the Navy Working Uniform NWU Type III. Seabees use ALICE field gear, as well as some units working with Marines using USMC-issue Improved Load Bearing Equipment (ILBE) gear.

WWII training

Camp Endicott had roughly 45 different vocational schools plus additional specialized classes. These included Air compressors, Arc welding, BAR, Bridge building, Bulldozer operation, Camouflage, Carpentry, Concrete construction, Crane operation, Dam building, Diving, Diesel engines, Distillation and water purification, Dock building, Drafting, Drilling, Dry docks, Dynamite and demolition, Electricity, Electric motors, First aid, Fire fighting, Gasoline Engines, Generators, Grading roads and airfields, Ice makers, Ignition systems, Judo, Hut and tents, Lubrication, Machine gun, Marine engines, Marston Matting, Mosquito control, Photography, Pile driving, Pipe-fitting/plumbing, Pontoons, Power-shovel operation, Pumps, Radio, Refrigeration, Rifle, Riveting, Road building, Road Scrapers, Sheet metal, Soil testing, Steelworking, Storage tanks wood or steel, Tire repair, Tractor operation, Transformers, Vulcanizing, Water front, and Well-drilling.

Current rates: The current ratings were adopted by the Navy in 1948.

The Seabee "constructionman" ranks of E-1 through E-3 are designated by sky-blue stripes on uniforms. The color was adopted in 1899 as a uniform trim color designating the Civil Engineer Corps, but was later given up. Its continued use is a bit of Naval Heritage in the NCF.

At E9 the ratings are reduced to three: EQCM for equipment operators and construction mechanics, CUCM for builders, steelworkers and engineering aids, UCCM for construction electricians and utilitiesmen.

Diver : is a qualification that the various rates can obtain with three grades: Basic Underwater Construction Technician/ NEC 5932 (2nd Class Diver), Advanced Underwater Construction Technician/ NEC 5931 (1st Class Diver), and Master Underwater Construction Technician/ NEC 5933 (Master diver). Seabee divers are attached to five principal commands outside the NCF:

On 1 March 1942 the RADM Moreell recommended that an insignia be created to promote "esprit de corps" in the new CBs to ID their equipment as the Air corps did to ID squadrons. It was not intended for uniforms. Frank J. Iafrate, a civilian file clerk at Quonset Point Advance Naval Base, Davisville, Rhode Island, who created the original "Disney Style" Seabee. In early 1942 his design was sent to RADM Moreell who made a single request. That the Seabee being set inside a letter Q, for Quonset Point, be changed to a hawser rope and it would officially adopted. 

The Seabees had a second Logo. It was of a shirtless constructionman holding a sledge hammer with a rifle strapped across his back standing upon the words "Construimus Batuimus USN". The figure was on a shield with a blue field across the top and vertical red and white stripes. A small CEC logo is left of the figure and a small anchor is to the right. This logo was incorporated into many CB Unit insignias. 

During World War II, artists working for Disney Onsignia Department designed logos for about ten Seaabee units including the: 60th NCB, ,78th NCB 112th NCB , and the 133rd NCB. There are two Disney published Seabee logos that are not identified with any unit. Disney did not create the original Seabee insignia.

The end of WWII brought the decommissioning of nearly all of the CBs. They had been in existence less than four years when this happened and the Navy had not created a Historical Branch or Archive for the NCF. So, there was no central archive for Seabee history. As time passed, first with Korea and then Vietnam, Construction Battalions were reactivated with the units having no idea what the WWII insignia had been so they made new ones.

The military qualification badge for the Seabees is known as the Seabee combat warfare specialist insignia (SCW). It was created in 1993 for issue to both officers and enlisted personnel that fulfill the training requirements. Only members attached to a qualifying NCF unit are eligible for the SCW pin. The qualifying units include: Naval Mobile Construction Battalions (NMCB), Amphibious Construction Battalions (ACB), Naval Construction Force Support Units (NCFSU), Underwater Construction Teams (UCT), and, since the end of 2008, Naval Construction Regiments (NCR).
The Fleet Marine Force Insignia, also known as the Fleet Marine Force pin or FMF pin, are three military badges of the United States Navy which are issued to those U.S. Navy officers and sailors who are trained and qualified to perform duties in support of the United States Marine Corps. Those Seabees that draw an assignment with the Fleet Marine Force can earn the Fleet Marine Force Insignia, also known as the Fleet Marine Force pin or FMF pin. the United States Navy has authorized these badges for U.S. Navy officers and sailors who are trained and qualified to perform duties in support of the United States Marine Corps. There are currently three classes of the Fleet Marine Force pin: enlisted, officer, and chaplain. For the requirements, see: Fleet Marine Force Warfare Specialist (EFMFWS) Program per OPNAV Instruction 1414.4B.

The Peltier Award is given to the "best of type" active duty Naval Construction Battalions. It was instituted by Rear Admiral Eugene J. Peltier CEC and has been given annually since 1960. He is a former head of the Bureau of Yards and Docks (1959–1962).

see: Seabee (barge)

The first ship of a series of six "Seabee" ships, was the SS "Cape Mendocino" (T-AKR-5064), followed by and (Fig. 37), three of which were operated by Lykes Brothers Steamship Company. These vessels were originally operated under the names 'Doctor Lykes', 'Tillie Lykes' and 'Almeria Lykes' - the first two being operated between the US Gulf and North Europe, with the third being operated to the Mediterranean. "The U.S. Navy's Naval Construction Force, or SeaBees, primarily use the Seabee barges. The barges are loaded with cargo and floated to and from a mother ship, which allows loading and unloading of containerized cargo off-shore. Seabee barge ships are equipped with a stern cargo elevator for loading the barges from the water onto the vessel; loaded barges can then be moved toward the vessel's bow using an internal track system. The barges are stowed on internal decks and are not stacked. The "Sea Bee" vessels had three decks and could transport 38 lighters (12 on the lower decks and 14 on the upper deck). Seabee barges are larger and heavier than their counterpart, LASH barges." The dual function of the ship is noteworthy, as it had storage tanks with a capacity of nearly 36000 m³ volume built into its sides and unusually large double hull, allowing it to be used also as a product tanker. The ships were later purchased by Military Sealift Command.

The U.S. Navy Seabee Museum is located at Naval Base Ventura County, Port Hueneme, Ca. outside the main gate for public access. In July 2011 the new facility opened with galleries, grand hall, theater, storage, and research areas.

The Seabee Heritage Center is located in Building 446 at NCBC Gulfport, Ms. The Heritage Center is the Atlantic Coast Annex of the Seabee Museum in Port Hueneme. Opened in 1995, the museum annex commemorates the history and achievements of the Atlantic Coast Naval Construction Force (Seabees) and the Navy's Civil Engineer Corps. Exhibits at the Gulfport Annex are provided by the Seabee Museum in Port Hueneme.

The Seabee Museum and Memorial Park in Davisville, Rhode Island was opened in the late 1990s. A Fighting Seabee Statue is located there.


WWII

2a. The Seabee NTCs were named for former heads of the Civil Engineer Corps and the Bureau of Yards and Docks: Rear Admiral Mordecai T. "Endicott", 1898–1907, Rear Admiral Harry H. "Rousseau", 1907–1907, Rear Admiral Richard C. "Hollyday", 1907–1912, Rear Admiral Charles W. "Parks", 1918–1921. Camp Peary was named for Rear Admiral Robert Peary CEC. They also named a Training Center for their first CEC killed in combat, Lt. Irwin W. Lee and Lt. (jg) George W. Stephenson of the 24th CB.

2b.   Presidential Unit Citation USN/USMC : The 6th CB with the 1st Marine Div. on Guadalcanal.

2c.   Presidential Unit Citation USN/USMC : The 18th CB with the 2nd Marine Div. on Tarawa

2d.   Presidential Unit Citation USN/USMC : The 33rd CBs shore party detachment with the 1st Marine Div. on Peleliu

2e.   Presidential Unit Citation USN/USMC : The 73rd CBs shore party detachment with the 1st Marine Div. on Peleliu

2f.   U.S. Army Distinguished Unit Citation : 40th CB plus 12 men from the 78th CB with the 1st Cavalry Div. on Los Negros

2g.   Presidential Unit Citation USN/USMC : The 121st CB with the 4th Marine Div. on Saipan and Tinian

2h. WWII U.S.N. CB awards for valor were listed each month in "All Hands" along with the rest of the Navy.

2i. "On 13 February 1945: CNO Fleet Admiral Ernest J. King approved the retention of construction battalions as a permanent and integral part of the postwar Navy." 

Marine Corps, Seabees outside the NCF

2.1a. When the 18th, 19th and 25th CBs were transferred to the Marine Corps they each were reduced by one company plus 1/5th of Hq Co to match the organization and size of a USMC battalion. B Co from the 25th CB and C Co from the 18th CB were used in the formation of the 53rd CB. The other company was used to form the 121st CB.

2.1b. Due to Seabees being given advanced rank upon enlistment, enlisted Marines referred to construction battalions as "sergeant's battalions". USMC sergeants do not pull guard duty, so the ranked Seabees would not be assigned. The NCOs of the 18th wore USMC chevrons and not USN "crows" on their uniforms.

2.1c. USN insignia on USMC issue.

2.1d. Seabees were shore party for the Marines on Bougainville, Peleliu, Guam, Purata Island, Roi-Namur, Saipan, Iwo Jima, and Okinawa. The Marines deployed them as combat engineers at Cape Gloucester, Tarawa, and Tinian. The 23rd Marine Regiment had Seabees as shore party three times: Roi-Namur, Saipan, and Iwo Jima.

2.1e. The first Marines assigned to a CB were 50 men attached to CBD 1010 on Guam. The 2nd Separate Marine Engineer Battalion quickly followed being operationally attached to the 5th Construction Brigade and assigned to the 27th NCR with two former USMC CBs; the 25th and the 53rd. The Marine Engineers were assigned to get Orote Field operational. In mid-August 1944 the 1st Separate Marine Engineer Battalion was operationally attached to the 6th Construction Brigade on Tinian and assigned to the 30th NCR. Earlier to Guam and Tinian the Marine Corps had provided a 100-man detail to the 71st CB on Bougainville.

NCDUs, Seabees outside the NCF

2.2a. The NCDUs at Normandy were numbers: 11, 22-30, 41-46, 127-8, 130-42

2.2b. Camp Peary's "Area E" = explosives

2.2c. The Joint Army Navy Experimental Testing (JANET) site for beach obstacle removal, Project DM-361, was located at the former Seabee base, Camp Bradford a few months after the NCDU program moved.

2.2d. 14 NCDUs were combined to create UDT 9, composed almost completely of Seabees NCDUs 200 – 216 were combined to create UDT 15.

2.2e.   Presidential Unit Citation USN/USMC : Naval Combat Demolition Force O for Omaha beach at Normandy. The following NCDUs made up this group: 11, 22-24, 27, 41-46 (Fig. 12), 128-131, 133, 137, 138, 140-142.

2.2f.   Navy Unit Commendation: Naval Combat Demolition Force U for Utah beach at Normandy.

UDTs, Seabees outside the NCF

2.3a. The Naval Special Warfare Command building at the U.S.N. Seal base at Fort Pierce is named for LTJG Frank Kaine CEC commander of NCDU 2.

2.3b. General Donovan the head of the OSS approached General MacArthur and Admiral Nimitz about using OSS men in the Pacific with Europe invaded. Gen. MacArthur had no interest at all. Adm Nimitz looked at Donovan's list and also said no except he could use the swimmers from the Maritime Unit. He was only interested in them for their being swimmers not OSS. He viewed the Seabee's Marine Corps training as adequate. The OSS Maritime Unit attached to UDT 10 is referred to as both Group A and Group II in documents.

2.3c. Seabees outside the NCF, made naval history. For the Marianas operations of Kwajelein, Roi-Namur, Siapan, Tinian, Eniwetok, and Guam, Admiral Turner recommended over sixty Silver Stars and over three hundred Bronze Stars with Vs for the Seabees and other service members of UDTs 1-7 That was unpresendented in U.S. Naval/Marine Corps history. For UDTs 5 and 7 every officer received a silver star and all the enlisted received bronze stars with Vs for Operation Forager (Tinian). 
this was repeated for UDTs 3 and 4 with Operation Forager at Guam. Admiral Conolly felt the commanders of teams 3 and 4 (Lt. Crist and Lt. W.G. Carberry) should have received Navy Crosses.

2.3d. Seabees in the WWII NCF were awarded 5 Navy Crosses, 33 Silver Stars and over 2,000 Purple Hearts. Marine Corps policy is to not keep records on non-USMC personnel so the number of Marine Corps Commendations Seabees received is also not known. However, on Iwo Jima the 8th CB received 26 and the 133rd CB received 29.

2.3e. UDT 3 at formation had 11 CEC, 4 USN, 1 USMC Officers Nearly all of the men from UDTs 1 and 2 were used to form UDTs 3 and 4. 
UDT 7's officers went through "indoctrination" in "Area E" at Camp Peary.

2.3f. UDT 13 the 13th CB used the same nickname "Black Cats" 

2.3g. Sources state early UDTs were also recruited from the Marine Corps, Army and Navy's joint Scouts and Raiders (not USMC Raiders), Navy Bomb Disposal as well as regular Navy. 

Seabee North Slope Oil Exploration 1944

2w. Seabee Creek was named by men of CBD 1058 and runs into the Colville River at Umiat, Alaska.

2x. USN geologists with the project would discover the large Aupuk Gas Seep near the Aupuk Creek fork with the Colville River.

Cold War: Korea – Seabee Teams

5a. 1 October 1965 two Atlantic Fleet Seabee Teams were assigned to "Project Demo" of the U.S. State Department tasked to de-bug American embassies behind the iron curtain and repair the removal damage (MCB 11).

Cold War: Antarctica

6a. The Seabees built the following bases: McMurdo Station, South Pole Station, Byrd Station, Palmer Station, Siple Station, Ellsworth Station, Brockton Station, Eights Station, Plateau Station, Hallett Station, and *Little America IV and Little America V ..

6b. Seabee Heights is a geologic feature of the Transantarctic mountains. They overlook the Beardmore Glacier traverse route Seabee sled trains took to resupply inland stations. Seabee Hook is located near the site of Hallett Station on the Ross sea.

Cold War: Vietnam

8a. Commander Naval Construction Battalion U.S. Pacific Fleet, Tân Sơn Nhất, Republic of Vietnam, Completion Report 1963–1972.

8c. Military training for battalions during this period lasted six weeks. The first two weeks were at home port – Davisville Naval Construction Battalion Center, CBC Gulfport or CBC Port Hueneme. The last four weeks were with the Marines at Camp Lejuene or Marine Corps Base Camp Pendleton.

8d.  Presidential Unit Citation USN/USMC : Detachments from MCBs 5, 10, 53 and CBMU 301 for supporting the 26th Marines during the 11 week Battle of Khe Sanh Jan–Feb 1968.

8e. Cold War projects: 1961 floating dry dock for Polaris submarines at Holy Loch, Scotland. 1963 U.S. Naval Communications Listening Station Nea Makri, Greece.

Teketite

10a. NASA Cold War project: 1960 Project Mercury tracking facilities on Canton Island (MCB 10 Det)

Iraq Afghanistan

13a.   Presidential Unit Citation USN/USMC : 30th NCR, NMCBs 4, 5, 74, 133, Air-Det 22nd NCR, Air-Det UCT 2, NCF Support Unit 2 in support of the First Marine Expeditionary Force (I MEF Engineer Group) in November 2003 added later upon review were: NMCBs 7, 15 as well as Air-Det NMCB 21, Air-Det NMCB 25, and CBMU 303 Det. (per: CMC MARADMIN 507/03)

13b. In 2015 ACB 1 moved the Orion (spacecraft) Boilerplate (spaceflight) test article for NASA at San Diego, CA.

Cold War: CIA

14.1b. When CBD 1510 was transferred to CBD 1504 the men had been designated for function similar to what was done with Acorns: Aviation and OTA. The Navy's use of "OTA" denotes the assignment to the CIA in that "Other Transaction Authority (OTA) is the term commonly used to refer to the (10 U.S.C. 2371b) authority of the Department of Defense (DoD) to carry out certain prototype, research and production projects."

14.1d. In 2007 the Naval Expeditionary Combat Command (NECC) authorized funding forty Naval Intelligence billets in the NCF. The goal was to have organic NCF Intelligence personnel in the Division, Regiment, and Battalion. Historically a units training officer would become the intelligence officer when the unit left CONUS.

14.1e. The CIA had a station at the Subic Bay Naval Base.

14.1f. CIA redacted memorandum dated 14 June 1968 discusses the use on Naval Construction Personal/Seabees on a project.

Seabee insignia

18a. WWII Naval Construction Battalion Logos

18b. CBs sponsored many B-29s on Tinian tagging the aircraft with Seabee unit insignia as nose art.

Naval Support Unit

18.2 a. In 1977 the U.S. Embassy in Moscow suffered a severe fire prompting the construction of a new one in 1979. At the construction site of the new embassy twenty to thirty Seabees were assigned to oversee 800 plus Russian construction workers. This prompted the Russians to embed bugs in construction materials prior to delivery to the construction site. The success of the KGB in bugging the new embassy only reinforced the State Departments need for the Seabees.

SEABEE Barge Carriers

20a. Unusual Hull Design Requirements of the SEABEE Barge Carriers.


Other U.S. military construction/engineering organizations:





</doc>
<doc id="29485" url="https://en.wikipedia.org/wiki?curid=29485" title="Skyscraper">
Skyscraper

A skyscraper is a continuously habitable high-rise building that has over 40 floors and is taller than approximately . Historically, the term first referred to buildings with 10 to 20 floors in the 1880s. The definition shifted with advancing construction technology during the 20th century. Skyscrapers may host offices, residential spaces, and retail spaces. For buildings above a height of , the term supertall skyscrapers can be used, while skyscrapers reaching beyond are classified as megatall skyscrapers.

One common feature of skyscrapers is having a steel framework that supports curtain walls. These curtain walls either bear on the framework below or are suspended from the framework above, rather than resting on load-bearing walls of conventional construction. Some early skyscrapers have a steel frame that enables the construction of load-bearing walls taller than of those made of reinforced concrete.

Modern skyscrapers' walls are not load-bearing, and most skyscrapers are characterised by large surface areas of windows made possible by steel frames and curtain walls. However, skyscrapers can have curtain walls that mimic conventional walls with a small surface area of windows. Modern skyscrapers often have a tubular structure, and are designed to act like a hollow cylinder to resist wind, seismic, and other lateral loads. To appear more slender, allow less wind exposure and transmit more daylight to the ground, many skyscrapers have a design with setbacks, which in some cases is also structurally required.

, only nine cities have more than 100 skyscrapers that are or taller: Hong Kong (355), New York City (284), Shenzhen (235), Dubai (199), Shanghai (163), Tokyo (155), Chongqing (127), Chicago (126), and Guangzhou (115).

The term "skyscraper" was first applied to buildings of steel framed construction of at least 10 stories in the late 19th century, a result of public amazement at the tall buildings being built in major American cities like Chicago, New York City, Philadelphia, Detroit, and St. Louis. The first steel-frame skyscraper was the Home Insurance Building (originally 10 stories with a height of ) in Chicago, Illinois in 1885. Some point to Philadelphia's 10-story Jayne Building (1849–50) as a proto-skyscraper, or to New York's seven-floor Equitable Life Building (New York City), built in 1870, for its innovative use of a kind of skeletal frame, but such designation depends largely on what factors are chosen. Even the scholars making the argument find it to be purely academic.

The structural definition of the word "skyscraper" was refined later by architectural historians, based on engineering developments of the 1880s that had enabled construction of tall multi-story buildings. This definition was based on the steel skeleton—as opposed to constructions of load-bearing masonry, which passed their practical limit in 1891 with Chicago's Monadnock Building.

The Council on Tall Buildings and Urban Habitat defines skyscrapers as those buildings which reach or exceed in height. Others in the United States and Europe also draw the lower limit of a skyscraper at .

The Emporis Standards Committee defines a high-rise building as "a multi-story structure between 35–100 meters tall, or a building of unknown height from 12–39 floors" and a skyscraper as "a multi-story building whose architectural height is at least ." Some structural engineers define a highrise as any vertical construction for which wind is a more significant load factor than earthquake or weight. Note that this criterion fits not only high-rises but some other tall structures, such as towers.

The word "skyscraper" often carries a connotation of pride and achievement. The skyscraper, in name and social function, is a modern expression of the age-old symbol of the world center or "axis mundi": a pillar that connects earth to heaven and the four compass directions to one another.

The tallest building in ancient times was the Great Pyramid of Giza in ancient Egypt, built in the 26th century BC. It was not surpassed in height for thousands of years, the Lincoln Cathedral having exceeded it in 1311–1549, before its central spire collapsed. The latter in turn was not surpassed until the Washington Monument in 1884. However, being uninhabited, none of these structures actually comply with the modern definition of a skyscraper.

High-rise apartments flourished in classical antiquity. Ancient Roman insulae in imperial cities reached 10 and more stories. Beginning with Augustus (r. 30 BC-14 AD), several emperors attempted to establish limits of 20–25 m for multi-story buildings, but met with only limited success. Lower floors were typically occupied by shops or wealthy families, the upper rented to the lower classes. Surviving Oxyrhynchus Papyri indicate that seven-story buildings existed in provincial towns such as in 3rd century AD Hermopolis in Roman Egypt.

The skylines of many important medieval cities had large numbers of high-rise urban towers, built by the wealthy for defense and status. The residential Towers of 12th century Bologna numbered between 80 and 100 at a time, the tallest of which is the high Asinelli Tower. A Florentine law of 1251 decreed that all urban buildings be immediately reduced to less than 26 m. Even medium-sized towns of the era are known to have proliferations of towers, such as the 72 up to 51 m height in San Gimignano.

The medieval Egyptian city of Fustat housed many high-rise residential buildings, which Al-Muqaddasi in the 10th century described as resembling minarets. Nasir Khusraw in the early 11th century described some of them rising up to 14 stories, with roof gardens on the top floor complete with ox-drawn water wheels for irrigating them. Cairo in the 16th century had high-rise apartment buildings where the two lower floors were for commercial and storage purposes and the multiple stories above them were rented out to tenants. An early example of a city consisting entirely of high-rise housing is the 16th-century city of Shibam in Yemen. Shibam was made up of over 500 tower houses, each one rising 5 to 11 stories high, with each floor being an apartment occupied by a single family. The city was built in this way in order to protect it from Bedouin attacks. Shibam still has the tallest mudbrick buildings in the world, with many of them over high.

An early modern example of high-rise housing was in 17th-century Edinburgh, Scotland, where a defensive city wall defined the boundaries of the city. Due to the restricted land area available for development, the houses increased in height instead. Buildings of 11 stories were common, and there are records of buildings as high as 14 stories. Many of the stone-built structures can still be seen today in the old town of Edinburgh. The oldest iron framed building in the world, although only partially iron framed, is The Flaxmill (also locally known as the "Maltings"), in Shrewsbury, England. Built in 1797, it is seen as the "grandfather of skyscrapers", since its fireproof combination of cast iron columns and cast iron beams developed into the modern steel frame that made modern skyscrapers possible. In 2013 funding was confirmed to convert the derelict building into offices.

In 1857, Elisha Otis introduced the safety elevator, allowing convenient and safe passenger movement to upper floors. Another crucial development was the use of a steel frame instead of stone or brick, otherwise the walls on the lower floors on a tall building would be too thick to be practical. An early development in this area was Oriel Chambers in Liverpool, England. It was only five floors high. Further developments led to the world's first skyscraper, the ten-story Home Insurance Building in Chicago, built in 1884–1885. While its height is not considered very impressive today, it was at that time. The building of tall buildings in the 1880s gave the skyscraper its first architectural movement the Chicago School, which developed what has been called the Commercial Style.

The architect, Major William Le Baron Jenney, created a load-bearing structural frame. In this building, a steel frame supported the entire weight of the walls, instead of load-bearing walls carrying the weight of the building. This development led to the "Chicago skeleton" form of construction. In addition to the steel frame, the Home Insurance Building also utilized fireproofing, elevators, and electrical wiring, key elements in most skyscrapers today.

Burnham and Root's Rand McNally Building in Chicago, 1889, was the first all-steel framed skyscraper, while Louis Sullivan's Wainwright Building in St. Louis, Missouri, 1891, was the first steel-framed building with soaring vertical bands to emphasize the height of the building and is therefore considered to be the first early skyscraper.

In 1889, the Mole Antonelliana in Italy was 167 m (549 ft) tall.

Most early skyscrapers emerged in the land-strapped areas of Chicago and New York City toward the end of the 19th century. A land boom in Melbourne, Australia between 1888 and 1891 spurred the creation of a significant number of early skyscrapers, though none of these were steel reinforced and few remain today. Height limits and fire restrictions were later introduced. London builders soon found building heights limited due to a complaint from Queen Victoria, rules that continued to exist with few exceptions.

Concerns about aesthetics and fire safety had likewise hampered the development of skyscrapers across continental Europe for the first half of the twentieth century. Some notable exceptions are the tall 1898 Witte Huis "(White House)" in Rotterdam; the Royal Liver Building in Liverpool, completed in 1911 and high; the tall 1924 Marx House in Düsseldorf, Germany; the Kungstornen "(Kings' Towers)" in Stockholm, Sweden, which were built 1924–25, the Edificio Telefónica in Madrid, Spain, built in 1929; the Boerentoren in Antwerp, Belgium, built in 1932; the Prudential Building in Warsaw, Poland, built in 1934; and the Torre Piacentini in Genoa, Italy, built in 1940.

After an early competition between Chicago and New York City for the world's tallest building, New York took the lead by 1895 with the completion of the tall American Surety Building, leaving New York with the title of the world's tallest building for many years.

Modern skyscrapers are built with steel or reinforced concrete frameworks and curtain walls of glass or polished stone. They use mechanical equipment such as water pumps and elevators. Since the 1960s, according to the CTHUB, the skyscraper has been reoriented away from a symbol for North American corporate power to instead communicate a city or nation's place in the world.

Skyscraper construction entered a three-decades-long era of stagnation in 1930 due to the Great Depression and then World War II. Shortly after the war ended, the Soviet Union began construction on a series of skyscrapers in Moscow. Seven, dubbed the "Seven Sisters," were built between 1947 and 1953; and one, the Main building of Moscow State University, was the tallest building in Europe for nearly four decades (1953–1990). Other skyscrapers in the style of Socialist Classicism were erected in East Germany (Frankfurter Tor), Poland (PKiN), Ukraine (Hotel Ukrayina), Latvia (Academy of Sciences) and other Eastern Bloc countries. Western European countries also began to permit taller skyscrapers during the years immediately following World War II. Early examples include Edificio España (Spain) Torre Breda (Italy).

From the 1930s onward, skyscrapers began to appear in various cities in East and Southeast Asia as well as in Latin America. Finally, they also began to be constructed in cities of Africa, the Middle East, South Asia and Oceania (mainly Australia) from the late 1950s on.

Skyscraper projects after World War II typically rejected the classical designs of the early skyscrapers, instead embracing the uniform international style; many older skyscrapers were redesigned to suit contemporary tastes or even demolished—such as New York's Singer Building, once the world's tallest skyscraper.

German architect Ludwig Mies van der Rohe became one of the world's most renowned architects in the second half of the 20th century. He conceived of the glass façade skyscraper and, along with Norwegian Fred Severud, he designed the Seagram Building in 1958, a skyscraper that is often regarded as the pinnacle of the modernist high-rise architecture.

Skyscraper construction surged throughout the 1960s. The impetus behind the upswing was a series of transformative innovations which made it possible for people to live and work in "cities in the sky".

In the early 1960s structural engineer Fazlur Rahman Khan, considered the "father of tubular designs" for high-rises, discovered that the dominating rigid steel frame structure was not the only system apt for tall buildings, marking a new era of skyscraper construction in terms of multiple structural systems. His central innovation in skyscraper design and construction was the concept of the "tube" structural system, including the "framed tube", "trussed tube", and "bundled tube". His "tube concept", using all the exterior wall perimeter structure of a building to simulate a thin-walled tube, revolutionized tall building design. These systems allow greater economic efficiency, and also allow skyscrapers to take on various shapes, no longer needing to be rectangular and box-shaped. The first building to employ the tube structure was the Chestnut De-Witt apartment building, this building is considered to be a major development in modern architecture. These new designs opened an economic door for contractors, engineers, architects, and investors, providing vast amounts of real estate space on minimal plots of land. Over the next fifteen years, many towers were built by Fazlur Rahman Khan and the "Second Chicago School", including the hundred story John Hancock Center and the massive Willis Tower. Other pioneers of this field include Hal Iyengar and William LeMessurier.

Many buildings designed in the 70s lacked a particular style and recalled ornamentation from earlier buildings designed before the 50s. These design plans ignored the environment and loaded structures with decorative elements and extravagant finishes. This approach to design was opposed by Fazlur Khan and he considered the designs to be whimsical rather than rational. Moreover, he considered the work to be a waste of precious natural resources. Khan's work promoted structures integrated with architecture and the least use of material resulting in the least carbon emission impact on the environment. The next era of skyscrapers will focus on the environment including performance of structures, types of material, construction practices, absolute minimal use of materials/natural resources, embodied energy within the structures, and more importantly, a holistically integrated building systems approach.

Modern building practices regarding supertall structures have led to the study of "vanity height". Vanity height, according to the CTBUH, is the distance between the highest floor and its architectural top (excluding antennae, flagpole or other functional extensions). Vanity height first appeared in New York City skyscrapers as early as the 1920s and 1930s but supertall buildings have relied on such uninhabitable extensions for on average 30% of their height, raising potential definitional and sustainability issues. The current era of skyscrapers focuses on sustainability, its built and natural environments, including the performance of structures, types of materials, construction practices, absolute minimal use of materials and natural resources, energy within the structure, and a holistically integrated building systems approach. LEED is a current green building standard.

Architecturally, with the movements of Postmodernism, New Urbanism and New Classical Architecture, that established since the 1980s, a more classical approach came back to global skyscraper design, that remains popular today. Examples are the Wells Fargo Center, NBC Tower, Parkview Square, 30 Park Place, the Messeturm, the iconic Petronas Towers and Jin Mao Tower.

Other contemporary styles and movements in skyscraper design include organic, sustainable, neo-futurist, structuralist, high-tech, deconstructivist, blob, digital, streamline, novelty, critical regionalist, vernacular, Neo Art Deco and neo-historist, also known as revivalist.

3 September is the global commemorative day for skyscrapers, called "Skyscraper Day".

New York City developers competed among themselves, with successively taller buildings claiming the title of "world's tallest" in the 1920s and early 1930s, culminating with the completion of the Chrysler Building in 1930 and the Empire State Building in 1931, the world's tallest building for forty years. The first completed tall World Trade Center tower became the world's tallest building in 1972. However, it was overtaken by the Sears Tower (now Willis Tower) in Chicago within two years. The tall Sears Tower stood as the world's tallest building for 24 years, from 1974 until 1998, until it was edged out by Petronas Twin Towers in Kuala Lumpur, which held the title for six years.

The design and construction of skyscrapers involves creating safe, habitable spaces in very tall buildings. The buildings must support their weight, resist wind and earthquakes, and protect occupants from fire. Yet they must also be conveniently accessible, even on the upper floors, and provide utilities and a comfortable climate for the occupants. The problems posed in skyscraper design are considered among the most complex encountered given the balances required between economics, engineering, and construction management.

One common feature of skyscrapers is a steel framework from which curtain walls are suspended, rather than load-bearing walls of conventional construction. Most skyscrapers have a steel frame that enables them to be built taller than typical load-bearing walls of reinforced concrete. Skyscrapers usually have a particularly small surface area of what are conventionally thought of as walls. Because the walls are not load-bearing most skyscrapers are characterized by surface areas of windows made possible by the concept of steel frame and curtain wall. However, skyscrapers can also have curtain walls that mimic conventional walls and have a small surface area of windows.

The concept of a skyscraper is a product of the industrialized age, made possible by cheap fossil fuel derived energy and industrially refined raw materials such as steel and concrete. The construction of skyscrapers was enabled by steel frame construction that surpassed brick and mortar construction starting at the end of the 19th century and finally surpassing it in the 20th century together with reinforced concrete construction as the price of steel decreased and labour costs increased.

The steel frames become inefficient and uneconomic for supertall buildings as usable floor space is reduced for progressively larger supporting columns. Since about 1960, tubular designs have been used for high rises. This reduces the usage of material (more efficient in economic terms – Willis Tower uses a third less steel than the Empire State Building) yet allows greater height. It allows fewer interior columns, and so creates more usable floor space. It further enables buildings to take on various shapes.

Elevators are characteristic to skyscrapers. In 1852 Elisha Otis introduced the safety elevator, allowing convenient and safe passenger movement to upper floors. Another crucial development was the use of a steel frame instead of stone or brick, otherwise the walls on the lower floors on a tall building would be too thick to be practical. Today major manufacturers of elevators include Otis, ThyssenKrupp, Schindler, and KONE.

Advances in construction techniques have allowed skyscrapers to narrow in width, while increasing in height. Some of these new techniques include mass dampers to reduce vibrations and swaying, and gaps to allow air to pass through, reducing wind shear.

Good structural design is important in most building design, but particularly for skyscrapers since even a small chance of catastrophic failure is unacceptable given the high price. This presents a paradox to civil engineers: the only way to assure a lack of failure is to test for all modes of failure, in both the laboratory and the real world. But the only way to know of all modes of failure is to learn from previous failures. Thus, no engineer can be absolutely sure that a given structure will resist all loadings that could cause failure, but can only have large enough margins of safety such that a failure is acceptably unlikely. When buildings do fail, engineers question whether the failure was due to some lack of foresight or due to some unknowable factor.

The load a skyscraper experiences is largely from the force of the building material itself. In most building designs, the weight of the structure is much larger than the weight of the material that it will support beyond its own weight. In technical terms, the dead load, the load of the structure, is larger than the live load, the weight of things in the structure (people, furniture, vehicles, etc.). As such, the amount of structural material required within the lower levels of a skyscraper will be much larger than the material required within higher levels. This is not always visually apparent. The Empire State Building's setbacks are actually a result of the building code at the time (1916 Zoning Resolution), and were not structurally required. On the other hand, John Hancock Center's shape is uniquely the result of how it supports loads. Vertical supports can come in several types, among which the most common for skyscrapers can be categorized as steel frames, concrete cores, tube within tube design, and shear walls.

The wind loading on a skyscraper is also considerable. In fact, the lateral wind load imposed on supertall structures is generally the governing factor in the structural design. Wind pressure increases with height, so for very tall buildings, the loads associated with wind are larger than dead or live loads.

Other vertical and horizontal loading factors come from varied, unpredictable sources, such as earthquakes.

By 1895, steel had replaced cast iron as skyscrapers' structural material. Its malleability allowed it to be formed into a variety of shapes, and it could be riveted, ensuring strong connections. The simplicity of a steel frame eliminated the inefficient part of a shear wall, the central portion, and consolidated support members in a much stronger fashion by allowing both horizontal and vertical supports throughout. Among steel's drawbacks is that as more material must be supported as height increases, the distance between supporting members must decrease, which in turn increases the amount of material that must be supported. This becomes inefficient and uneconomic for buildings above 40 stories tall as usable floor spaces are reduced for supporting column and due to more usage of steel.

A new structural system of framed tubes was developed by Fazlur Rahman Khan in 1963. The framed tube structure is defined as "a three dimensional space structure composed of three, four, or possibly more frames, braced frames, or shear walls, joined at or near their edges to form a vertical tube-like structural system capable of resisting lateral forces in any direction by cantilevering from the foundation." Closely spaced interconnected exterior columns form the tube. Horizontal loads (primarily wind) are supported by the structure as a whole. Framed tubes allow fewer interior columns, and so create more usable floor space, and about half the exterior surface is available for windows. Where larger openings like garage doors are required, the tube frame must be interrupted, with transfer girders used to maintain structural integrity. Tube structures cut down costs, at the same time allowing buildings to reach greater heights. Concrete tube-frame construction was first used in the DeWitt-Chestnut Apartment Building, completed in Chicago in 1963, and soon after in the John Hancock Center and World Trade Center.

The tubular systems are fundamental to tall building design. Most buildings over 40-stories constructed since the 1960s now use a tube design derived from Khan's structural engineering principles, examples including the construction of the World Trade Center, Aon Center, Petronas Towers, Jin Mao Building, and most other supertall skyscrapers since the 1960s. The strong influence of tube structure design is also evident in the construction of the current tallest skyscraper, the Burj Khalifa.

Khan pioneered several other variations of the tube structure design. One of these was the concept of X-bracing, or the trussed tube, first employed for the John Hancock Center. This concept reduced the lateral load on the building by transferring the load into the exterior columns. This allows for a reduced need for interior columns thus creating more floor space. This concept can be seen in the John Hancock Center, designed in 1965 and completed in 1969. One of the most famous buildings of the structural expressionist style, the skyscraper's distinctive X-bracing exterior is actually a hint that the structure's skin is indeed part of its 'tubular system'. This idea is one of the architectural techniques the building used to climb to record heights (the tubular system is essentially the spine that helps the building stand upright during wind and earthquake loads). This X-bracing allows for both higher performance from tall structures and the ability to open up the inside floorplan (and usable floor space) if the architect desires.

The John Hancock Center was far more efficient than earlier steel-frame structures. Where the Empire State Building (1931), required about 206 kilograms of steel per square metre and 28 Liberty Street (1961) required 275, the John Hancock Center required only 145. The trussed tube concept was applied to many later skyscrapers, including the Onterie Center, Citigroup Center and Bank of China Tower.

An important variation on the tube frame is the bundled tube, which uses several interconnected tube frames. The Willis Tower in Chicago used this design, employing nine tubes of varying height to achieve its distinct appearance. The bundled tube structure meant that "buildings no longer need be boxlike in appearance: they could become sculpture."

The invention of the elevator was a precondition for the invention of skyscrapers, given that most people would not (or could not) climb more than a few flights of stairs at a time. The elevators in a skyscraper are not simply a necessary utility, like running water and electricity, but are in fact closely related to the design of the whole structure: a taller building requires more elevators to service the additional floors, but the elevator shafts consume valuable floor space. If the service core, which contains the elevator shafts, becomes too big, it can reduce the profitability of the building. Architects must therefore balance the value gained by adding height against the value lost to the expanding service core.

Many tall buildings use elevators in a non-standard configuration to reduce their footprint. Buildings such as the former World Trade Center Towers and Chicago's John Hancock Center use sky lobbies, where express elevators take passengers to upper floors which serve as the base for local elevators. This allows architects and engineers to place elevator shafts on top of each other, saving space. Sky lobbies and express elevators take up a significant amount of space, however, and add to the amount of time spent commuting between floors.

Other buildings, such as the Petronas Towers, use double-deck elevators, allowing more people to fit in a single elevator, and reaching two floors at every stop. It is possible to use even more than two levels on an elevator, although this has never been done. The main problem with double-deck elevators is that they cause everyone in the elevator to stop when only people on one level need to get off at a given floor.

Buildings with sky lobbies include the World Trade Center, Petronas Twin Towers, Willis Tower and Taipei 101. The 44th-floor sky lobby of the John Hancock Center also featured the first high-rise indoor swimming pool, which remains the highest in America.

Skyscrapers are usually situated in city centers where the price of land is high. Constructing a skyscraper becomes justified if the price of land is so high that it makes economic sense to build upward as to minimize the cost of the land per the total floor area of a building. Thus the construction of skyscrapers is dictated by economics and results in skyscrapers in a certain part of a large city unless a building code restricts the height of buildings.

Skyscrapers are rarely seen in small cities and they are characteristic of large cities, because of the critical importance of high land prices for the construction of skyscrapers. Usually only office, commercial and hotel users can afford the rents in the city center and thus most tenants of skyscrapers are of these classes.

Today, skyscrapers are an increasingly common sight where land is expensive, as in the centers of big cities, because they provide such a high ratio of rentable floor space per unit area of land.

One problem with skyscrapers is car parking. In the largest cities most people commute via public transport, but for smaller cities a lot of parking spaces are needed. Multi-storey car parks are impractical to build very tall, so a lot of land area is needed.

There may be a correlation between skyscraper construction and great income inequality but this has not been conclusively proven.

The amount of steel, concrete, and glass needed to construct a single skyscraper is large, and these materials represent a great deal of embodied energy. Skyscrapers are thus energy intensive buildings, but skyscrapers have a long lifespan, for example the Empire State Building in New York City, United States completed in 1931 and is still in active use.

Skyscrapers have considerable mass, which means that they must be built on a sturdier foundation than would be required for shorter, lighter buildings. Building materials must also be lifted to the top of a skyscraper during construction, requiring more energy than would be necessary at lower heights. Furthermore, a skyscraper consumes a lot of electricity because potable and non-potable water has to be pumped to the highest occupied floors, skyscrapers are usually designed to be mechanically ventilated, elevators are generally used instead of stairs, and natural lighting cannot be utilized in rooms far from the windows and the windowless spaces such as elevators, bathrooms and stairwells.

Skyscrapers can be artificially lit and the energy requirements can be covered by renewable energy or other electricity generation with low greenhouse gas emissions. Heating and cooling of skyscrapers can be efficient, because of centralized HVAC systems, heat radiation blocking windows and small surface area of the building. There is Leadership in Energy and Environmental Design (LEED) certification for skyscrapers. For example, the Empire State Building received a gold Leadership in Energy and Environmental Design rating in September 2011 and the Empire State Building is the tallest LEED certified building in the United States, proving that skyscrapers can be environmentally friendly. Also the 30 St Mary Axe in London, the United Kingdom is an environmentally friendly skyscraper.

In the lower levels of a skyscraper a larger percentage of the building cross section must be devoted to the building structure and services than is required for lower buildings:

In low-rise structures, the support rooms (chillers, transformers, boilers, pumps and air handling units) can be put in basements or roof space—areas which have low rental value. There is, however, a limit to how far this plant can be located from the area it serves. The farther away it is the larger the risers for ducts and pipes from this plant to the floors they serve and the more floor area these risers take. In practice this means that in highrise buildings this plant is located on 'plant levels' at intervals up the building.

At the beginning of the 20th century, New York City was a center for the Beaux-Arts architectural movement, attracting the talents of such great architects as Stanford White and Carrere and Hastings. As better construction and engineering technology became available as the century progressed, New York City and Chicago became the focal point of the competition for the tallest building in the world. Each city's striking skyline has been composed of numerous and varied skyscrapers, many of which are icons of 20th-century architecture:

Momentum in setting records passed from the United States to other nations with the opening of the Petronas Twin Towers in Kuala Lumpur, Malaysia, in 1998. The record for the world's tallest building has remained in Asia since the opening of Taipei 101 in Taipei, Taiwan, in 2004. A number of architectural records, including those of the world's tallest building and tallest free-standing structure, moved to the Middle East with the opening of the Burj Khalifa in Dubai, United Arab Emirates.

This geographical transition is accompanied by a change in approach to skyscraper design. For much of the twentieth century large buildings took the form of simple geometrical shapes. This reflected the "international style" or modernist philosophy shaped by Bauhaus architects early in the century. The last of these, the Willis Tower and World Trade Center towers in New York, erected in the 1970s, reflect the philosophy. Tastes shifted in the decade which followed, and new skyscrapers began to exhibit postmodernist influences. This approach to design avails itself of historical elements, often adapted and re-interpreted, in creating technologically modern structures. The Petronas Twin Towers recall Asian pagoda architecture and Islamic geometric principles. Taipei 101 likewise reflects the pagoda tradition as it incorporates ancient motifs such as the ruyi symbol. The Burj Khalifa draws inspiration from traditional Islamic art. Architects in recent years have sought to create structures that would not appear equally at home if set in any part of the world, but that reflect the culture thriving in the spot where they stand.

The following list measures height of the roof. The more common gauge is the "highest architectural detail"; such ranking would have included Petronas Towers, built in 1996.

Many skyscrapers were never built due to financial problems, politics and culture. The Chicago Spire was to be the tallest building in the Western Hemisphere, but it was on hold due to the global financial crisis of 2008. One year later, the project was cancelled.

Proposals for such structures have been put forward, including the Burj Mubarak Al Kabir in Kuwait and Azerbaijan Tower in Baku. Kilometer-plus structures present architectural challenges that may eventually place them in a new architectural category. The first building under construction and planned to be over one kilometre tall is the Jeddah Tower.

Several wooden skyscraper designs have been designed and built. A 14-story housing project in Bergen, Norway known as 'Treet' or 'The Tree' became the world's tallest wooden apartment block when it was completed in late 2015. The Tree's record was eclipsed by Brock Commons, an 18-story wooden dormitory at the University of British Columbia in Canada, when it was completed in September 2016.

A 40-story residential building 'Trätoppen' has been proposed by architect Anders Berensson to be built in Stockholm, Sweden. Trätoppen would be the tallest building in Stockholm, though there are no immediate plans to begin construction. The tallest currently-planned wooden skyscraper is the 70-story W350 Project in Tokyo, to be built by the Japanese wood products company Sumitomo Forestry Co. to celebrate its 350th anniversary in 2041. An 80-story wooden skyscraper, the River Beech Tower, has been proposed by a team including architects Perkins + Will and the University of Cambridge. The River Beech Tower, on the banks of the Chicago River in Chicago, Illinois, would be 348 feet shorter than the W350 Project despite having 10 more stories.

Wooden skyscrapers are estimated to be around a quarter of the weight of an equivalent reinforced-concrete structure as well as reducing the building carbon footprint by 60–75%. Buildings have been designed using cross-laminated timber (CLT) which gives a higher rigidity and strength to wooden structures. CLT panels are prefabricated and can therefore speed up building time.





</doc>
<doc id="29486" url="https://en.wikipedia.org/wiki?curid=29486" title="Sagas of Icelanders">
Sagas of Icelanders

The Sagas of Icelanders (), also known as family sagas, are one genre of Icelandic sagas. They are prose narratives mostly based on historical events that mostly took place in Iceland in the ninth, tenth, and early eleventh centuries, during the so-called Saga Age. They are the best-known specimens of Icelandic literature.

They are focused on history, especially genealogical and family history. They reflect the struggle and conflict that arose within the societies of the early generations of Icelandic settlers.

Eventually many of these Icelandic sagas were recorded, mostly in the thirteenth and fourteenth centuries. The 'authors', or rather recorders of these sagas are largely unknown. One saga, "Egils saga", is believed by some scholars to have been written by Snorri Sturluson, a descendant of the saga's hero, but this remains uncertain. The standard modern edition of Icelandic sagas is known as Íslenzk fornrit.

Among the several literary reviews of the sagas is that by Sigurður Nordal's "Sagalitteraturen", which divides the sagas into five chronological groups distinguished by the state of literary development:


A small number of sagas are thought to have existed and now to be lost. One example is the supposed "Gauks saga Trandilssonar".





</doc>
<doc id="29489" url="https://en.wikipedia.org/wiki?curid=29489" title="Staind">
Staind

Staind ( ) is an American rock band formed in Springfield, Massachusetts, in 1995. The original lineup consisted of lead vocalist and rhythm guitarist Aaron Lewis, lead guitarist Mike Mushok, bassist and backing vocalist Johnny April, and drummer Jon Wysocki. The lineup has been stable outside of the 2011 departure of Wysocki, who was replaced by Sal Giancarelli. Staind has recorded seven studio albums: "Tormented" (1996), "Dysfunction" (1999), "Break the Cycle" (2001), "14 Shades of Grey" (2003), "Chapter V" (2005), "The Illusion of Progress" (2008), and "Staind" (2011). The band's activity became more sporadic after their self-titled release, with Lewis pursuing a solo country music career and Mushok subsequently joining the band Saint Asonia, but they have continued to tour on and off in the following years. As of 2016, Lewis has reiterated that the band has not broken up, and will possibly create another album, but that his current focus is on his solo career. As of 2019, the band is on hiatus. Staind has sold over 15 million records worldwide. Many of their singles have reached high positions on US rock and all-format charts as well, including "It's Been Awhile", "Fade", "Price to Play", "So Far Away", and "Right Here".

In 1993, vocalist Aaron Lewis and guitarist Mike Mushok met at a Christmas party in Springfield, Massachusetts. Mushok introduced drummer Jon Wysocki while Lewis brought in bassist Johnny April to form the band in 1995. Their first public performance was in February 1995, playing a heavy, dark, and introspective style of metal. Extensive touring in the Northeast helped Staind acquire a regional following over the next few years.

The band started covering Korn, Rage Against the Machine, Pearl Jam, Tool, and Alice in Chains, among others, and played at local clubs (most commonly playing at Club Infinity) for a year and a half. Staind self-released their debut album, "Tormented", in November 1996, citing Tool, Faith No More, and Pantera as their influences. In October 1997, Staind acquired a concert slot through Aaron's cousin Justin Cantor with Limp Bizkit. Just prior to the performance, Limp Bizkit frontman Fred Durst was appalled by Staind's grotesque album cover and unsuccessfully attempted to remove them from the bill. Durst thought that Staind were Theistic Satanists. After being persuaded to let them perform, however, Durst was so impressed that he signed them to Flip Records by February 1998.

On April 13, 1999, Staind released their major label debut "Dysfunction" on Flip Records. The album, which was co-produced by Fred Durst and Terry Date (who also produced acts like Soundgarden, Deftones, and Pantera), received comparisons to alternative metal giants Tool and Korn. In particular, Aaron Lewis was lauded for his vocals, which were likened to those of Pearl Jam's Eddie Vedder.

The album achieved slow success, reaching the No. 1 spot on Billboard's Heatseeker Charts almost six months after its debut. In the same week, the record jumped to No. 74 on Billboard's Top 200 Album Charts. The nine-track LP (with one hidden track, "Excess Baggage") produced three singles, "Just Go", "Mudshovel", and "Home". "Mudshovel" and "Home" both received radio play, cracking the Top 20 of Billboard's Modern Rock and Mainstream Rock charts. In promotion of "Dysfunction", Staind went on several tours, including the Family Values Tour with acts like Limp Bizkit and The Crystal Method, as well as opening for Sevendust's headlining tour.

Staind toured with Limp Bizkit for the Family Values Tour during the fall of 1999, where Aaron Lewis performed an early version of "Outside" with Fred Durst at the Mississippi Coast Coliseum. Staind released their third studio album, "Break the Cycle", on May 22, 2001. Propelled by the success of the first single, "It's Been Awhile", the album debuted at No. 1 on Billboard's Top 200 Album charts, selling 716,000 copies in its first week. The record's first-week sales were the second highest of any album that year, behind Creed's "Weathered".

"Break the Cycle" saw the band retaining the nu metal sound from their previous album. Despite this, the album saw the band going further into a post-grunge sound which is evident in the smash hit song "It's Been Awhile", and the song led critics to compare the band to several other post-grunge bands at the time. The record spawned the singles "It's Been Awhile" (which hit the Billboard Top 10), "Fade", "Outside", "For You", and the acoustic ballad "Epiphany". "It's Been Awhile" spent a total of 16 and 14 weeks on top of the modern and mainstream rock charts respectively, making it one of the highest joint numbers of all time. In 2001, "Break the Cycle" sold four million copies worldwide, making it one of the best selling albums that year. "Break the Cycle" would go on to sell seven million copies worldwide, making this Staind's bestselling album.

In early 2003, Staind embarked on a worldwide tour to promote the release of the follow-up to "Break the Cycle", "14 Shades of Grey", which sold two million copies and debuted at number 1 on the Billboard 200. The album saw a departure from their previous nu metal sound as it mostly contained a lighter and more melodic post-grunge sound. "14 Shades of Grey" produced two mainstream hits, "Price to Play" and "So Far Away", which spent 14 weeks on top of the rock chart. In addition, two other singles were released: "How About You" and "Zoe Jane". The band's appearance at the Reading Festival during their 2003 tour had another impromptu acoustic set, this time due to equipment failure. The singles "So Far Away" and "Price to Play" came with two unreleased tracks, "Novocaine" and "Let It Out", which were released for the special edition of the group's subsequent album "Chapter V", which came out in late 2005. In 2003, Staind unsuccessfully sued their logo designer Jon Stainbrook in New York Federal Court for attempting to re-use the logo he had sold to the band. They re-opened the case in mid-2005.

Staind's fifth album, titled "Chapter V", was released on August 9, 2005, and became their third consecutive number one. The album opened to sales of 185,000 and has since been certified platinum in the U.S. The first single, "Right Here", was the biggest success from the album, garnering much mainstream radio play and peaking at number 1 on the mainstream rock chart. "Falling" was released as the second single, followed by "Everything Changes" and "King of All Excuses". Staind went on the road when the album came out, doing live shows and promoting it for a full year, including participating in the Fall Brawl tour with P.O.D., Taproot, and Flyleaf; they also had a solo tour across Europe and a mini-promotional tour in Australia for the first time. Other live shows included a cover of Pantera's "This Love", a tribute to Dimebag Darrell. Staind appeared on "The Howard Stern Show" on August 10, 2005 to promote "Chapter V". They performed acoustic renditions of the single "Right Here" and Beetlejuice's song "This is Beetle". In early November 2005, Staind released the limited edition 2-CD/DVD set of "Chapter V". On September 6, 2006, they performed an acoustic show in the Hiro Ballroom, New York City, that was recorded for their singles collection. The band played sixteen songs, including three covers: Tool's "Sober", Pink Floyd's "Comfortably Numb", and Alice in Chains's "Nutshell".

The collection "" was released on November 14, 2006. It included all the band's singles, the three covers performed at the New York show, and a remastered version of "Come Again" from Staind's first independent release "Tormented".

On August 19, 2008, Staind released their sixth album, "The Illusion of Progress". Prior to the album's release, the track "This Is It" was available for download on the iTunes Store, as well as for "Rock Band". The album debuted at No. 3 on the US Billboard 200, No. 1 on the Top Modern Rock/Alternative Albums Chart, No. 1 on the Top Digital Albums Chart, and also No. 1 on the Top Internet Albums Chart, with first-week sales of 91,800 units. The first single on the album, "Believe", topped Billboard's Top 10 Modern Rock Tracks on September 5, 2008. The band also supported Nickelback on their 2008 European tour. The second single was "All I Want", and came out on November 24. The single also became Staind's 13th top 20 hit on the rock charts. In Europe, the second single was "The Way I Am", released on January 26, 2009. The final single released from the album, "This Is It", was sent to radio stations across the country on May 4, 2009. The track was also included on the successful "" released in late June 2009. The same year, Staind embarked on a fall tour with the newly reunited Creed.

In March 2010, Aaron Lewis stated the band would start working on their seventh studio album by the end of the year. Lewis had finished recording his country music solo EP and had started a nonprofit organization to reopen his daughter's elementary school in Worthington, Massachusetts. Guitarist Mike Mushok stated in a Q&A session with fans that the band was looking to make a heavy record, but still "explore some of the things we did on the last record and take them somewhere new for us". In a webisode posted on the band's website, Lewis stated that eight songs were written and that "every one of them is as heavy or heavier than the heaviest song on the last record".

In December 2010, Staind posted three webisodes from the studio, which featured the band members discussing the writing and recording process of their new album. They announced that as of April 20, they had completed the recording of their seventh and would release it later that year.

On May 20, 2011, Staind announced that original drummer Jon Wysocki had left the band. Drummer Will Hunt filled in for a few dates, while Wysocki's drum tech Sal Giancarelli filled in for the rest of the tour. Three days later, it was reported that Staind's new album would be a self-titled release. It was released on September 13, 2011. The first single, "Not Again", was released to active radio stations on July 18. The song "The Bottom" appeared on the "" soundtrack. On June 30, Staind released a song called "Eyes Wide Open" from their new record. "Eyes Wide Open" would later be released on November 29 as the album's second single.

In November 2011, the band announced through their YouTube page that Sal Giancarelli was now an official member. The band continued to perform into 2012, embarking on an April and May tour with Godsmack and Halestorm, and they played the Uproar Festival in August and September with Shinedown and a number of other artists.

It was announced in July 2012 that the band was to be taking a hiatus. In an interview with Billboard, Aaron Lewis stated that "We're not breaking up. We're not gonna stop making music. We're just going to take a little hiatus that really hasn't ever been taken in our career. We put out seven records in 14 years. We've been pretty busy." Lewis also had plans to release his first solo album "The Road". During this time, Mike Mushok auditioned, and was selected, to play guitar for former Metallica bassist Jason Newsted's new band Newsted. He featured on their debut album "Heavy Metal Music".

Staind played their first show in two years at the Welcome To Rockville Festival on April 27, 2014. They also played the Carolina Rebellion and Rock on the Range festivals in May 2014.

In late 2014, the band went on another hiatus. Aaron Lewis continued to play solo shows and work on his next solo album. He also confirmed that the hiatus would last "for a while". Mike Mushok teamed up with former Three Days Grace singer Adam Gontier, former Finger Eleven drummer Rich Beddoe, and Eye Empire bassist Corey Lowery to form Saint Asonia.

On August 4, 2017, the band performed for the first time since November 2014 for an acoustic performance at Aaron Lewis' 6th annual charity golf tournament and concert when bassist Johnny April and drummer Sal Giancarelli joined Aaron Lewis and Mike Mushok to perform "Outside", "Something to Remind You", and "It's Been Awhile". Three days later, Lewis announced that Staind would never tour extensively again.

In April 2019, the band announced they would reform in September 2019 for some live performances. The band will play at Epicenter Festival on May 3rd 2020 at Charlotte Motor Speedway.

The topics of Staind's lyrics cover issues of depression, relationships, death, addiction, finding one's self, betrayal, and Lewis' thoughts about becoming a father in the song "Zoe Jane" from "14 Shades of Grey", as well as reflecting on his upbringing in the song "The Corner" from "The Illusion of Progress". Also from "14 Shades of Grey", the track titled "Layne" was written about Alice in Chains frontman Layne Staley in response to his death in 2002. The song is also about Staley's legacy and the effect his music had on the members of Staind, especially Aaron Lewis. Staind has been categorized as nu metal, alternative metal, heavy metal, hard rock, and post-grunge.

In 2001, "Rolling Stone" outlined the band's relationship to the nu metal label:
Staind's influences include Pantera, The Doors, Suicidal Tendencies, Kiss, Van Halen, Slayer, Led Zeppelin, Sepultura, Whitesnake, the Beatles, Alice in Chains, Faith No More, Deftones, Black Sabbath, Pearl Jam, Tool, Rage Against the Machine, Nirvana, Stone Temple Pilots, Helmet, James Taylor, Korn, and Crosby, Stills & Nash.

Current line-up

Former members

Touring musicians

Timeline

Studio albums



</doc>
<doc id="29490" url="https://en.wikipedia.org/wiki?curid=29490" title="Saddam Hussein">
Saddam Hussein

Saddam Hussein Abd al-Majid al-Tikriti (; Arabic: ""; 28 April 1937 – 30 December 2006) was the fifth President of Iraq from 16 July 1979 until 9 April 2003. A leading member of the revolutionary Arab Socialist Ba'ath Party, and later, the Baghdad-based Ba'ath Party and its regional organization the Iraqi Ba'ath Party—which espoused Ba'athism, a mix of Arab nationalism and socialism—Saddam played a key role in the 1968 coup (later referred to as the 17 July Revolution) that brought the party to power in Iraq.

As vice president under the ailing General Ahmed Hassan al-Bakr, and at a time when many groups were considered capable of overthrowing the government, Saddam created security forces through which he tightly controlled conflicts between the government and the armed forces. In the early 1970s, Saddam nationalized oil and foreign banks leaving the system eventually insolvent mostly due to the Iran–Iraq War, the Gulf War, and UN sanctions. Through the 1970s, Saddam cemented his authority over the apparatus of government as oil money helped Iraq's economy to grow at a rapid pace. Positions of power in the country were mostly filled with Sunni Arabs, a minority that made up only a fifth of the population.

Saddam formally rose to power in 1979, although he had already been the "de facto" head of Iraq for several years. He suppressed several movements, particularly Shi'a and Kurdish movements which sought to overthrow the government or gain independence, respectively, and maintained power during the Iran–Iraq War and the Gulf War. Whereas some in the Arab world lauded Saddam for opposing the United States and attacking Israel, he was widely condemned for the brutality of his dictatorship. The total number of Iraqis killed by the security services of Saddam's government in various purges and genocides is conservatively estimated to be 250,000. Saddam's invasions of Iran and Kuwait also resulted in hundreds of thousands of deaths.

In 2003, a coalition led by the United States invaded Iraq to depose Saddam, in which U.S. President George W. Bush and British Prime Minister Tony Blair erroneously accused him of possessing weapons of mass destruction and having ties to al-Qaeda. Saddam's Ba'ath party was disbanded and the country's first ever set of democratic elections were held. Following his capture on 13 December 2003, the trial of Saddam took place under the Iraqi Interim Government. On 5 November 2006, Saddam was convicted by an Iraqi court of crimes against humanity related to the 1982 killing of 148 Iraqi Shi'a, and sentenced to death by hanging. He was executed on 30 December 2006.

Before he was born, cancer killed both Saddam's father and brother. These deaths so depressed Saddam's mother (Sabha) that she attempted to abort her pregnancy and commit suicide. When her son was born, Sabha "would have nothing to do with him," and Saddam was taken in by an uncle.

His mother remarried, and Saddam gained three half-brothers through this marriage. His stepfather, Ibrahim al-Hassan, treated Saddam harshly after his return. At about age 10, Saddam fled the family and returned to live in Baghdad with his uncle Kharaillah Talfah, who became a father figure to Saddam. Talfah, the father of Saddam's future wife, was a devout Sunni Muslim and a veteran of the 1941 Anglo-Iraqi War between Iraqi nationalists and the United Kingdom, which remained a major colonial power in the region. Talfah later became the mayor of Baghdad during Saddam's time in power, until his notorious corruption compelled Saddam to force him out of office.

Later in his life relatives from his native Tikrit became some of his closest advisors and supporters. Under the guidance of his uncle he attended a nationalistic high school in Baghdad. After secondary school Saddam studied at an Iraqi law school for three years, dropping out in 1957 at the age of 20 to join the revolutionary pan-Arab Ba'ath Party, of which his uncle was a supporter. During this time, Saddam apparently supported himself as a secondary school teacher. Ba'athist ideology originated in Syria and the Ba'ath Party had a large following in Syria at the time, but in 1955 there were fewer than 300 Ba'ath Party members in Iraq and it is believed that Saddam's primary reason for joining the party as opposed to the more established Iraqi nationalist parties was his familial connection to Ahmed Hassan al-Bakr and other leading Ba'athists through his uncle.
Revolutionary sentiment was characteristic of the era in Iraq and throughout the Middle East. In Iraq progressives and socialists assailed traditional political elites (colonial-era bureaucrats and landowners, wealthy merchants and tribal chiefs, and monarchists). Moreover, the pan-Arab nationalism of Gamal Abdel Nasser in Egypt profoundly influenced young Ba'athists like Saddam. The rise of Nasser foreshadowed a wave of revolutions throughout the Middle East in the 1950s and 1960s, with the collapse of the monarchies of Iraq, Egypt, and Libya. Nasser inspired nationalists throughout the Middle East by fighting the British and the French during the Suez Crisis of 1956, modernizing Egypt, and uniting the Arab world politically.

In 1958, a year after Saddam had joined the Ba'ath party, army officers led by General Abd al-Karim Qasim overthrew Faisal II of Iraq in the 14 July Revolution.

Of the 16 members of Qasim's cabinet, 12 were Ba'ath Party members; however, the party turned against Qasim due to his refusal to join Gamal Abdel Nasser's United Arab Republic. To strengthen his own position within the government, Qasim created an alliance with the Iraqi Communist Party, which was opposed to any notion of pan-Arabism. Later that year, the Ba'ath Party leadership was planning to assassinate Qasim. Saddam was a leading member of the operation. At the time, the Ba'ath Party was more of an ideological experiment than a strong anti-government fighting machine. The majority of its members were either educated professionals or students, and Saddam fit the bill. The choice of Saddam was, according to journalist Con Coughlin, "hardly surprising." The idea of assassinating Qasim may have been Nasser's, and there is speculation that some of those who participated in the operation received training in Damascus, which was then part of the UAR. However, "no evidence has ever been produced to implicate Nasser directly in the plot." Saddam himself is not believed to have received any training outside of Iraq, as he was a late addition to the assassination team.

The assassins planned to ambush Qasim at Al-Rashid Street on 7 October 1959: one man was to kill those sitting at the back of the car, the rest killing those in front. During the ambush it is claimed that Saddam began shooting prematurely, which disorganised the whole operation. Qasim's chauffeur was killed, and Qasim was hit in the arm and shoulder. The assassins believed they had killed him and quickly retreated to their headquarters, but Qasim survived. At the time of the attack the Ba'ath Party had fewer than 1,000 members. Saddam's role in the failed assassination became a crucial part of his public image for decades. Kanan Makiya recounts:
The man and the myth merge in this episode. His biography—and Iraqi television, which stages the story ad nauseam—tells of his familiarity with guns from the age of ten; his fearlessness and loyalty to the party during the 1959 operation; his bravery in saving his comrades by commandeering a car at gunpoint; the bullet that was gouged out of his flesh under his direction in hiding; the iron discipline that led him to draw a gun on weaker comrades who would have dropped off a seriously wounded member of the hit team at a hospital; the calculating shrewdness that helped him save himself minutes before the police broke in leaving his wounded comrades behind; and finally the long trek of a wounded man from house to house, city to town, across the desert to refuge in Syria.

Some of the plotters (including Saddam) quickly managed to leave the country for Syria, the spiritual home of Ba'athist ideology. There Saddam was given full membership in the party by Michel Aflaq. Some members of the operation were arrested and taken into custody by the Iraqi government. At the show trial, six of the defendants were given death sentences; for unknown reasons the sentences were not carried out. Aflaq, the leader of the Ba'athist movement, organised the expulsion of leading Iraqi Ba'athist members, such as Fuad al-Rikabi, on the grounds that the party should not have initiated the attempt on Qasim's life. At the same time, Aflaq secured seats in the Iraqi Ba'ath leadership for his supporters, one of them being Saddam. Saddam moved from Syria to Egypt itself in February 1960, and he continued to live there until 1963, graduating from high school in 1961 and unsuccessfully pursuing a law degree.

Army officers with ties to the Ba'ath Party overthrew Qasim in the Ramadan Revolution coup of February 1963. Ba'athist leaders were appointed to the cabinet and Abdul Salam Arif became president. Arif dismissed and arrested the Ba'athist leaders later that year in the November 1963 Iraqi coup d'état. Being exiled in Egypt at the time, Saddam played no role in the 1963 coup or the brutal anti-communist purge that followed; although he returned to Iraq after the coup, Saddam remained "on the fringes of the newly installed Ba'thi administration and [had] to content himself with the minor position of a member of the Party's central bureau for peasants," in the words of Efraim Karsh and Inari Rautsi Unlike during the Qasim years, Saddam remained in Iraq following Arif's anti-Ba'athist purge in November 1963, and became involved in planning to assassinate Arif. In marked contrast to Qasim, Saddam knew that he faced no death penalty from Arif's government and knowingly accepted the risk of being arrested rather than fleeing to Syria again. Saddam was arrested in October 1964 and served approximately two years in prison before escaping in 1966. In 1966, Ahmed Hassan al-Bakr appointed him Deputy Secretary of the Regional Command. Saddam, who would prove to be a skilled organiser, revitalised the party. He was elected to the Regional Command, as the story goes, with help from Michel Aflaq—the founder of Ba'athist thought. In September 1966, Saddam initiated an extraordinary challenge to Syrian domination of the Ba'ath Party in response to the Marxist takeover of the Syrian Ba'ath earlier that year, resulting in the Party's formalized split into two separate factions. Saddam then created a Ba'athist security service, which he alone controlled.

In July 1968, Saddam participated in a bloodless coup led by Ahmed Hassan al-Bakr that overthrew Abdul Rahman Arif, Salam Arif's brother and successor. While Saddam's role in the coup was not hugely significant (except in the official account), Saddam planned and carried out the subsequent purge of the non-Ba'athist faction led by Prime Minister Abd ar-Razzaq an-Naif, whose support had been essential to the coup's success. According to a semi-official biography, Saddam personally led Naif at gunpoint to the plane that escorted him out of Iraq. Arif was given refuge in London and then Istanbul. Al-Bakr was named president and Saddam was named his deputy, and deputy chairman of the Ba'athist Revolutionary Command Council. According to biographers, Saddam never forgot the tensions within the first Ba'athist government, which formed the basis for his measures to promote Ba'ath party unity as well as his resolve to maintain power and programs to ensure social stability. Although Saddam was al-Bakr's deputy, he was a strong behind-the-scenes party politician. Al-Bakr was the older and more prestigious of the two, but by 1969 Saddam clearly had become the moving force behind the party.

In the late 1960s and early 1970s, as vice chairman of the Revolutionary Command Council, formally al-Bakr's second-in-command, Saddam built a reputation as a progressive, effective politician. At this time, Saddam moved up the ranks in the new government by aiding attempts to strengthen and unify the Ba'ath party and taking a leading role in addressing the country's major domestic problems and expanding the party's following.

After the Ba'athists took power in 1968, Saddam focused on attaining stability in a nation riddled with profound tensions. Long before Saddam, Iraq had been split along social, ethnic, religious, and economic fault lines: Sunni versus Shi'ite, Arab versus Kurd, tribal chief versus urban merchant, nomad versus peasant. The desire for stable rule in a country rife with factionalism led Saddam to pursue both massive repression and the improvement of living standards.

Saddam actively fostered the modernization of the Iraqi economy along with the creation of a strong security apparatus to prevent coups within the power structure and insurrections apart from it. Ever concerned with broadening his base of support among the diverse elements of Iraqi society and mobilizing mass support, he closely followed the administration of state welfare and development programs.

At the center of this strategy was Iraq's oil. On 1 June 1972, Saddam oversaw the seizure of international oil interests, which, at the time, dominated the country's oil sector. A year later, world oil prices rose dramatically as a result of the 1973 energy crisis, and skyrocketing revenues enabled Saddam to expand his agenda.
Within just a few years, Iraq was providing social services that were unprecedented among Middle Eastern countries. Saddam established and controlled the "National Campaign for the Eradication of Illiteracy" and the campaign for "Compulsory Free Education in Iraq," and largely under his auspices, the government established universal free schooling up to the highest education levels; hundreds of thousands learned to read in the years following the initiation of the program. The government also supported families of soldiers, granted free hospitalization to everyone, and gave subsidies to farmers. Iraq created one of the most modernized public-health systems in the Middle East, earning Saddam an award from the United Nations Educational, Scientific and Cultural Organization (UNESCO).

With the help of increasing oil revenues, Saddam diversified the largely oil-based Iraqi economy. Saddam implemented a national infrastructure campaign that made great progress in building roads, promoting mining, and developing other industries. The campaign helped Iraq's energy industries. Electricity was brought to nearly every city in Iraq, and many outlying areas. Before the 1970s, most of Iraq's people lived in the countryside and roughly two-thirds were peasants. This number would decrease quickly during the 1970s as global oil prices helped revenues to rise from less than a half billion dollars to tens of billions of dollars and the country invested into industrial expansion.

The oil revenue benefited Saddam politically. According to "The Economist", "Much as Adolf Hitler won early praise for galvanising German industry, ending mass unemployment and building autobahns, Saddam earned admiration abroad for his deeds. He had a good instinct for what the "Arab street" demanded, following the decline in Egyptian leadership brought about by the trauma of Israel's six-day victory in the 1967 war, the death of the pan-Arabist hero, Gamal Abdul Nasser, in 1970, and the "traitorous" drive by his successor, Anwar Sadat, to sue for peace with the Jewish state. Saddam's self-aggrandising propaganda, with himself posing as the defender of Arabism against Jewish or Persian intruders, was heavy-handed, but consistent as a drumbeat. It helped, of course, that his mukhabarat (secret police) put dozens of Arab news editors, writers and artists on the payroll."

In 1972, Saddam signed a 15-year Treaty of Friendship and Cooperation with the Soviet Union. According to historian Charles R. H. Tripp, the treaty upset "the U.S.-sponsored security system established as part of the Cold War in the Middle East. It appeared that any enemy of the Baghdad regime was a potential ally of the United States." In response, the U.S. covertly financed Kurdish rebels led by Mustafa Barzani during the Second Iraqi–Kurdish War; the Kurds were defeated in 1975, leading to the forcible relocation of hundreds of thousands of Kurdish civilians.

Saddam focused on fostering loyalty to the Ba'athists in the rural areas. After nationalizing foreign oil interests, Saddam supervised the modernization of the countryside, mechanizing agriculture on a large scale, and distributing land to peasant farmers. The Ba'athists established farm cooperatives and the government also doubled expenditures for agricultural development in 1974–1975. Saddam's welfare programs were part of a combination of "carrot and stick" tactics to enhance support for Saddam. The state-owned banks were put under his thumb. Lending was based on cronyism. Development went forward at such a fevered pitch that two million people from other Arab countries and even Yugoslavia worked in Iraq to meet the growing demand for labor.

In 1976, Saddam rose to the position of general in the Iraqi armed forces, and rapidly became the strongman of the government. As the ailing, elderly al-Bakr became unable to execute his duties, Saddam took on an increasingly prominent role as the face of the government both internally and externally. He soon became the architect of Iraq's foreign policy and represented the nation in all diplomatic situations. He was the "de facto" leader of Iraq some years before he formally came to power in 1979. He slowly began to consolidate his power over Iraq's government and the Ba'ath party. Relationships with fellow party members were carefully cultivated, and Saddam soon accumulated a powerful circle of support within the party.

In 1979, al-Bakr started to make treaties with Syria, also under Ba'athist leadership, that would lead to unification between the two countries. Syrian President Hafez al-Assad would become deputy leader in a union, and this would drive Saddam to obscurity. Saddam acted to secure his grip on power. He forced the ailing al-Bakr to resign on 16 July 1979, and formally assumed the presidency.

Saddam convened an assembly of Ba'ath party leaders on 22 July 1979. During the assembly, which he ordered videotaped, Saddam claimed to have found a fifth column within the Ba'ath Party and directed Muhyi Abdel-Hussein to read out a confession and the names of 68 alleged co-conspirators. These members were labelled "disloyal" and were removed from the room one by one and taken into custody. After the list was read, Saddam congratulated those still seated in the room for their past and future loyalty. The 68 people arrested at the meeting were subsequently tried together and found guilty of treason. 22 were sentenced to execution. Other high-ranking members of the party formed the firing squad. By 1 August 1979, hundreds of high-ranking Ba'ath party members had been executed.

Iraqi society fissures along lines of language, religion and ethnicity. The Ba'ath Party, secular by nature, adopted Pan-Arab ideologies which in turn were problematic for significant parts of the population. Following the Iranian Revolution of 1979, Iraq faced the prospect of régime change from two Shi'ite factions (Dawa and SCIRI) which aspired to model Iraq on its neighbour Iran as a Shia theocracy. A separate threat to Iraq came from parts of the ethnic Kurdish population of northern Iraq which opposed being part of an Iraqi state and favoured independence (an ongoing ideology which had preceded Ba'ath Party rule). To alleviate the threat of revolution, Saddam afforded certain benefits to the potentially hostile population. Membership in the Ba'ath Party remained open to all Iraqi citizens regardless of background. However, repressive measures were taken against its opponents.

The major instruments for accomplishing this control were the paramilitary and police organizations. Beginning in 1974, Taha Yassin Ramadan (himself a Kurdish Ba'athist), a close associate of Saddam, commanded the People's Army, which had responsibility for internal security. As the Ba'ath Party's paramilitary, the People's Army acted as a counterweight against any coup attempts by the regular armed forces. In addition to the People's Army, the Department of General Intelligence was the most notorious arm of the state-security system, feared for its use of torture and assassination. Barzan Ibrahim al-Tikriti, Saddam's younger half-brother, commanded Mukhabarat. Foreign observers believed that from 1982 this department operated both at home and abroad in its mission to seek out and eliminate Saddam's perceived opponents.

Saddam was notable for using terror against his own people. "The Economist" described Saddam as "one of the last of the 20th century's great dictators, but not the least in terms of egotism, or cruelty, or morbid will to power." Saddam's regime brought about the deaths of at least 250,000 Iraqis and committed war crimes in Iran, Kuwait, and Saudi Arabia. Human Rights Watch and Amnesty International issued regular reports of widespread imprisonment and torture.

As a sign of his consolidation of power, Saddam's personality cult pervaded Iraqi society. He had thousands of portraits, posters, statues and murals erected in his honor all over Iraq. His face could be seen on the sides of office buildings, schools, airports, and shops, as well as on Iraqi currency. Saddam's personality cult reflected his efforts to appeal to the various elements in Iraqi society. This was seen in his variety of apparel: he appeared in the costumes of the Bedouin, the traditional clothes of the Iraqi peasant (which he essentially wore during his childhood), and even Kurdish clothing, but also appeared in Western suits fitted by his favorite tailor, projecting the image of an urbane and modern leader. Sometimes he would also be portrayed as a devout Muslim, wearing full headdress and robe, praying toward Mecca.

He also conducted two show elections, in 1995 and 2002. In the 1995 referendum, conducted on 15 October, he reportedly received 99.96% of the votes in a 99.47% turnout, getting only 3,052 negative votes among an electorate of 8.4 million. In the October 15, 2002 referendum he officially achieved 100% of approval votes and 100% turnout, as the electoral commission reported the next day that every one of the 11,445,638 eligible voters cast a "Yes" vote for the president.

He erected statues around the country, which Iraqis toppled after his fall.

Iraq's relations with the Arab world have been extremely varied. Relations between Iraq and Egypt violently ruptured in 1977, when the two nations broke relations with each other following Iraq's criticism of Egyptian President Anwar Sadat's peace initiatives with Israel. In 1978, Baghdad hosted an Arab League summit that condemned and ostracized Egypt for accepting the Camp David Accords. However, Egypt's strong material and diplomatic support for Iraq in the war with Iran led to warmer relations and numerous contacts between senior officials, despite the continued absence of ambassadorial-level representation. Since 1983, Iraq has repeatedly called for restoration of Egypt's "natural role" among Arab countries.
Saddam developed a reputation for liking expensive goods, such as his diamond-coated Rolex wristwatch, and sent copies of them to his friends around the world. To his ally Kenneth Kaunda Saddam once sent a Boeing 747 full of presents—rugs, televisions, ornaments.

Saddam enjoyed a close relationship with Russian intelligence agent Yevgeny Primakov that dated back to the 1960s; Primakov may have helped Saddam to stay in power in 1991.

Saddam visited only two Western countries. The first visit took place in December 1974, when the dictator of Spain, Francisco Franco, invited him to Madrid and he visited Granada, Córdoba and Toledo. In September 1975 he met with Prime Minister Jacques Chirac in Paris, France.

Several Iraqi leaders, Lebanese arms merchant Sarkis Soghanalian and others have claimed that Saddam financed Chirac's party. In 1991 Saddam threatened to expose those who had taken largesse from him: "From Mr. Chirac to Mr. Chevènement, politicians and economic leaders were in open competition to spend time with us and flatter us. We have now grasped the reality of the situation. If the trickery continues, we will be forced to unmask them, all of them, before the French public." France armed Saddam and it was Iraq's largest trade partner throughout Saddam's rule. Seized documents show how French officials and businessmen close to Chirac, including Charles Pasqua, his former interior minister, personally benefitted from the deals with Saddam.

Because Saddam Hussein rarely left Iraq, Tariq Aziz, one of Saddam's aides, traveled abroad extensively and represented Iraq at many diplomatic meetings. In foreign affairs, Saddam sought to have Iraq play a leading role in the Middle East. Iraq signed an aid pact with the Soviet Union in 1972, and arms were sent along with several thousand advisers. However, the 1978 crackdown on Iraqi Communists and a shift of trade toward the West strained Iraqi relations with the Soviet Union; Iraq then took on a more Western orientation until the Gulf War in 1991.

After the oil crisis of 1973, France had changed to a more pro-Arab policy and was accordingly rewarded by Saddam with closer ties. He made a state visit to France in 1975, cementing close ties with some French business and ruling political circles. In 1975 Saddam negotiated an accord with Iran that contained Iraqi concessions on border disputes. In return, Iran agreed to stop supporting opposition Kurds in Iraq. Saddam led Arab opposition to the Camp David Accords between Egypt and Israel (1979).

Saddam initiated Iraq's nuclear enrichment project in the 1980s, with French assistance. The first Iraqi nuclear reactor was named by the French "Osirak." Osirak was destroyed on 7 June 1981 by an Israeli air strike (Operation Opera).

Nearly from its founding as a modern state in 1920, Iraq has had to deal with Kurdish separatists in the northern part of the country. Saddam did negotiate an agreement in 1970 with separatist Kurdish leaders, giving them autonomy, but the agreement broke down. The result was brutal fighting between the government and Kurdish groups and even Iraqi bombing of Kurdish villages in Iran, which caused Iraqi relations with Iran to deteriorate. However, after Saddam had negotiated the 1975 treaty with Iran, the Shah withdrew support for the Kurds, who suffered a total defeat.

In early 1979, Iran's Shah Mohammad Reza Pahlavi was overthrown by the Islamic Revolution, thus giving way to an Islamic republic led by the Ayatollah Ruhollah Khomeini. The influence of revolutionary Shi'ite Islam grew apace in the region, particularly in countries with large Shi'ite populations, especially Iraq. Saddam feared that radical Islamic ideas—hostile to his secular rule—were rapidly spreading inside his country among the majority Shi'ite population.

There had also been bitter enmity between Saddam and Khomeini since the 1970s. Khomeini, having been exiled from Iran in 1964, took up residence in Iraq, at the Shi'ite holy city of An Najaf. There he involved himself with Iraqi Shi'ites and developed a strong, worldwide religious and political following against the Iranian Government, which Saddam tolerated. However, when Khomeini began to urge the Shi'ites there to overthrow Saddam and under pressure from the Shah, who had agreed to a rapprochement between Iraq and Iran in 1975, Saddam agreed to expel Khomeini in 1978 to France. However this turned out to be an imminent failure and a political catalyst, for Khomeini had access to more media connections and also collaborated with a much larger Iranian community under his support which he used to his advantage.

After Khomeini gained power, skirmishes between Iraq and revolutionary Iran occurred for ten months over the sovereignty of the disputed Shatt al-Arab waterway, which divides the two countries. During this period, Saddam Hussein publicly maintained that it was in Iraq's interest not to engage with Iran, and that it was in the interests of both nations to maintain peaceful relations. However, in a private meeting with Salah Omar al-Ali, Iraq's permanent ambassador to the United Nations, he revealed that he intended to invade and occupy a large part of Iran within months. Later (probably to appeal for support from the United States and most Western nations), he would make toppling the Islamic government one of his intentions as well.
Iraq invaded Iran, first attacking Mehrabad Airport of Tehran and then entering the oil-rich Iranian land of Khuzestan, which also has a sizable Arab minority, on 22 September 1980 and declared it a new province of Iraq. With the support of the Arab states, the United States, and Europe, and heavily financed by the Arab states of the Persian Gulf, Saddam Hussein had become "the defender of the Arab world" against a revolutionary Iran. The only exception was the Soviet Union, who initially refused to supply Iraq on the basis of neutrality in the conflict, although in his memoirs, Mikhail Gorbachev claimed that Leonid Brezhnev refused to aid Saddam over infuriation of Saddam's treatment of Iraqi communists. Consequently, many viewed Iraq as "an agent of the civilized world." The blatant disregard of international law and violations of international borders were ignored. Instead Iraq received economic and military support from its allies, who overlooked Saddam's use of chemical warfare against the Kurds and the Iranians, in addition to Iraq's efforts to develop nuclear weapons.

In the first days of the war, there was heavy ground fighting around strategic ports as Iraq launched an attack on Khuzestan. After making some initial gains, Iraq's troops began to suffer losses from human wave attacks by Iran. By 1982, Iraq was on the defensive and looking for ways to end the war.

At this point, Saddam asked his ministers for candid advice. Health Minister Dr. Riyadh Ibrahim suggested that Saddam temporarily step down to promote peace negotiations. Initially, Saddam Hussein appeared to take in this opinion as part of his cabinet democracy. A few weeks later, Dr. Ibrahim was sacked when held responsible for a fatal incident in an Iraqi hospital where a patient died from intravenous administration of the wrong concentration of potassium supplement.

Dr. Ibrahim was arrested a few days after he started his new life as a sacked minister. He was known to have publicly declared before that arrest that he was "glad that he got away alive." Pieces of Ibrahim's dismembered body were delivered to his wife the next day.

Iraq quickly found itself bogged down in one of the longest and most destructive wars of attrition of the 20th century. During the war, Iraq used chemical weapons against Iranian forces fighting on the southern front and Kurdish separatists who were attempting to open up a northern front in Iraq with the help of Iran. These chemical weapons were developed by Iraq from materials and technology supplied primarily by West German companies as well as using dual-use technology imported following the Reagan administration's lifting of export restrictions. The United States also supplied Iraq with "satellite photos showing Iranian deployments." In a US bid to open full diplomatic relations with Iraq, the country was removed from the US list of State Sponsors of Terrorism. Ostensibly, this was because of improvement in the regime's record, although former United States Assistant Secretary of Defense Noel Koch later stated, "No one had any doubts about [the Iraqis'] continued involvement in terrorism ... The real reason was to help them succeed in the war against Iran." The Soviet Union, France, and China together accounted for over 90% of the value of Iraq's arms imports between 1980 and 1988.

Saddam reached out to other Arab governments for cash and political support during the war, particularly after Iraq's oil industry severely suffered at the hands of the Iranian navy in the Persian Gulf. Iraq successfully gained some military and financial aid, as well as diplomatic and moral support, from the Soviet Union, China, France, and the United States, which together feared the prospects of the expansion of revolutionary Iran's influence in the region. The Iranians, demanding that the international community should force Iraq to pay war reparations to Iran, refused any suggestions for a cease-fire. Despite several calls for a ceasefire by the United Nations Security Council, hostilities continued until 20 August 1988.

On 16 March 1988, the Kurdish town of Halabja was attacked with a mix of mustard gas and nerve agents, killing 5,000 civilians, and maiming, disfiguring, or seriously debilitating 10,000 more. ("see Halabja poison gas attack") The attack occurred in conjunction with the 1988 al-Anfal Campaign designed to reassert central control of the mostly Kurdish population of areas of northern Iraq and defeat the Kurdish peshmerga rebel forces. The United States now maintains that Saddam ordered the attack to terrorize the Kurdish population in northern Iraq, but Saddam's regime claimed at the time that Iran was responsible for the attack which some including the U.S. supported until several years later.

The bloody eight-year war ended in a stalemate. There were hundreds of thousands of casualties with estimates of up to one million dead. Neither side had achieved what they had originally desired and the borders were left nearly unchanged. The southern, oil rich and prosperous Khuzestan and Basra area (the main focus of the war, and the primary source of their economies) were almost completely destroyed and were left at the pre-1979 border, while Iran managed to make some small gains on its borders in the Northern Kurdish area. Both economies, previously healthy and expanding, were left in ruins.

Saddam borrowed tens of billions of dollars from other Arab states and a few billions from elsewhere during the 1980s to fight Iran, mainly to prevent the expansion of Shi'a radicalism. However, this had proven to completely backfire both on Iraq and on the part of the Arab states, for Khomeini was widely perceived as a hero for managing to defend Iran and maintain the war with little foreign support against the heavily backed Iraq and only managed to boost Islamic radicalism not only within the Arab states, but within Iraq itself, creating new tensions between the Sunni Ba'ath Party and the majority Shi'a population. Faced with rebuilding Iraq's infrastructure and internal resistance, Saddam desperately re-sought cash, this time for postwar reconstruction.

The Al-Anfal Campaign was a genocidal campaign against the Kurdish people (and many others) in Kurdish regions of Iraq led by the government of Saddam Hussein and headed by Ali Hassan al-Majid. The campaign takes its name from Surat al-Anfal in the Qur'an, which was used as a code name by the former Iraqi Ba'athist administration for a series of attacks against the "peshmerga" rebels and the mostly Kurdish civilian population of rural Northern Iraq, conducted between 1986 and 1989 culminating in 1988. This campaign also targeted Shabaks and Yazidis, Assyrians, Turkoman people and Mandeans and many villages belonging to these ethnic groups were also destroyed. Human Rights Watch estimates that between 50,000 and 100,000 people were killed. Some Kurdish sources put the number higher, estimating that 182,000 Kurds were killed.

The end of the war with Iran served to deepen latent tensions between Iraq and its wealthy neighbor Kuwait. Saddam urged the Kuwaitis to waive the Iraqi debt accumulated in the war, some $30 billion, but they refused.

Saddam pushed oil-exporting countries to raise oil prices by cutting back production; Kuwait refused, however. In addition to refusing the request, Kuwait spearheaded the opposition in OPEC to the cuts that Saddam had requested. Kuwait was pumping large amounts of oil, and thus keeping prices low, when Iraq needed to sell high-priced oil from its wells to pay off a huge debt.

Saddam had always argued that Kuwait was historically an integral part of Iraq, and that Kuwait had only come into being through the maneuverings of British imperialism; this echoed a belief that Iraqi nationalists had voiced for the past 50 years. This belief was one of the few articles of faith uniting the political scene in a nation rife with sharp social, ethnic, religious, and ideological divides.

The extent of Kuwaiti oil reserves also intensified tensions in the region. The oil reserves of Kuwait (with a population of 2 million next to Iraq's 25) were roughly equal to those of Iraq. Taken together, Iraq and Kuwait sat on top of some 20 percent of the world's known oil reserves; as an article of comparison, Saudi Arabia holds 25 percent.

Saddam complained to the U.S. State Department that Kuwait had slant drilled oil out of wells that Iraq considered to be within its disputed border with Kuwait. Saddam still had an experienced and well-equipped army, which he used to influence regional affairs. He later ordered troops to the Iraq–Kuwait border.

As Iraq-Kuwait relations rapidly deteriorated, Saddam was receiving conflicting information about how the U.S. would respond to the prospects of an invasion. For one, Washington had been taking measures to cultivate a constructive relationship with Iraq for roughly a decade. The Reagan administration gave Iraq roughly $4 billion in agricultural credits to bolster it against Iran. Saddam's Iraq became "the third-largest recipient of U.S. assistance."

Reacting to Western criticism in April 1990 Saddam threatened to destroy half of Israel with chemical weapons if it moved against Iraq. In May 1990 he criticized U.S. support for Israel warning that "the United States cannot maintain such a policy while professing friendship towards the Arabs." In July 1990 he threatened force against Kuwait and the UAE saying "The policies of some Arab rulers are American ... They are inspired by America to undermine Arab interests and security." The U.S. sent aerial planes and combat ships to the Persian Gulf in response to these threats.

U.S. ambassador to Iraq April Glaspie met with Saddam in an emergency meeting on 25 July 1990, where the Iraqi leader attacked American policy with regards to Kuwait and the United Arab Emirates:

Glaspie replied:

Saddam stated that he would attempt last-ditch negotiations with the Kuwaitis but Iraq "would not accept death."

U.S. officials attempted to maintain a conciliatory line with Iraq, indicating that while George H. W. Bush and James Baker did not want force used, they would not take any position on the Iraq–Kuwait boundary dispute and did not want to become involved.

Later, Iraq and Kuwait met for a final negotiation session, which failed. Saddam then sent his troops into Kuwait. As tensions between Washington and Saddam began to escalate, the Soviet Union, under Mikhail Gorbachev, strengthened its military relationship with the Iraqi leader, providing him military advisers, arms and aid.

On 2 August 1990, Saddam invaded Kuwait, initially claiming assistance to "Kuwaiti revolutionaries," thus sparking an international crisis. On 4 August an Iraqi-backed "Provisional Government of Free Kuwait" was proclaimed, but a total lack of legitimacy and support for it led to an 8 August announcement of a "merger" of the two countries. On 28 August Kuwait formally became the 19th Governorate of Iraq. Just two years after the 1988 Iraq and Iran truce, "Saddam Hussein did what his Gulf patrons had earlier paid him to prevent." Having removed the threat of Iranian fundamentalism he "overran Kuwait and confronted his Gulf neighbors in the name of Arab nationalism and Islam."

When later asked why he invaded Kuwait, Saddam first claimed that it was because Kuwait was rightfully Iraq's 19th province and then said "When I get something into my head I act. That's just the way I am." After Saddam's seizure of Kuwait in August 1990, a UN coalition led by the United States drove Iraq's troops from Kuwait in February 1991. The ability for Saddam Hussein to pursue such military aggression was from a "military machine paid for in large part by the tens of billions of dollars Kuwait and the Gulf states had poured into Iraq and the weapons and technology provided by the Soviet Union, Germany, and France."

Shortly before he invaded Kuwait, he shipped 100 new Mercedes 200 Series cars to top editors in Egypt and Jordan. Two days before the first attacks, Saddam reportedly offered Egypt's Hosni Mubarak 50 million dollars in cash, "ostensibly for grain."

U.S. President George H. W. Bush responded cautiously for the first several days. On one hand, Kuwait, prior to this point, had been a virulent enemy of Israel and was the Persian Gulf monarchy that had the most friendly relations with the Soviets. On the other hand, Washington foreign policymakers, along with Middle East experts, military critics, and firms heavily invested in the region, were extremely concerned with stability in this region. The invasion immediately triggered fears that the world's price of oil, and therefore control of the world economy, was at stake. Britain profited heavily from billions of dollars of Kuwaiti investments and bank deposits. Bush was perhaps swayed while meeting with British prime minister Margaret Thatcher, who happened to be in the U.S. at the time.

Cooperation between the United States and the Soviet Union made possible the passage of resolutions in the United Nations Security Council giving Iraq a deadline to leave Kuwait and approving the use of force if Saddam did not comply with the timetable. U.S. officials feared Iraqi retaliation against oil-rich Saudi Arabia, since the 1940s a close ally of Washington, for the Saudis' opposition to the invasion of Kuwait. Accordingly, the U.S. and a group of allies, including countries as diverse as Egypt, Syria and Czechoslovakia, deployed a massive number of troops along the Saudi border with Kuwait and Iraq in order to encircle the Iraqi army, the largest in the Middle East.

Saddam's officers looted Kuwait, stripping even the marble from its palaces to move it to Saddam's own palace.

During the period of negotiations and threats following the invasion, Saddam focused renewed attention on the Palestinian problem by promising to withdraw his forces from Kuwait if Israel would relinquish the occupied territories in the West Bank, the Golan Heights, and the Gaza Strip. Saddam's proposal further split the Arab world, pitting U.S.- and Western-supported Arab states against the Palestinians. The allies ultimately rejected any linkage between the Kuwait crisis and Palestinian issues.

Saddam ignored the Security Council deadline. Backed by the Security Council, a U.S.-led coalition launched round-the-clock missile and aerial attacks on Iraq, beginning 16 January 1991. Israel, though subjected to attack by Iraqi missiles, refrained from retaliating in order not to provoke Arab states into leaving the coalition. A ground force consisting largely of U.S. and British armoured and infantry divisions ejected Saddam's army from Kuwait in February 1991 and occupied the southern portion of Iraq as far as the Euphrates.

On 6 March 1991, Bush announced "What is at stake is more than one small country, it is a big idea—a new world order, where diverse nations are drawn together in common cause to achieve the universal aspirations of mankind: peace and security, freedom, and the rule of law."

In the end, the out-numbered and under-equipped Iraqi army proved unable to compete on the battlefield with the highly mobile coalition land forces and their overpowering air support. Some 175,000 Iraqis were taken prisoner and casualties were estimated at over 85,000. As part of the cease-fire agreement, Iraq agreed to scrap all poison gas and germ weapons and allow UN observers to inspect the sites. UN trade sanctions would remain in effect until Iraq complied with all terms. Saddam publicly claimed victory at the end of the war.

Iraq's ethnic and religious divisions, together with the brutality of the conflict that this had engendered, laid the groundwork for postwar rebellions. In the aftermath of the fighting, social and ethnic unrest among Shi'ite Muslims, Kurds, and dissident military units threatened the stability of Saddam's government. Uprisings erupted in the Kurdish north and Shi'a southern and central parts of Iraq, but were ruthlessly repressed. Uprisings in 1991, led to the death of 100,000–180,000 people, mostly civilian.

The United States, which had urged Iraqis to rise up against Saddam, did nothing to assist the rebellions. The Iranians, despite the widespread Shi'ite rebellions, had no interest in provoking another war, while Turkey opposed any prospect of Kurdish independence, and the Saudis and other conservative Arab states feared an Iran-style Shi'ite revolution. Saddam, having survived the immediate crisis in the wake of defeat, was left firmly in control of Iraq, although the country never recovered either economically or militarily from the Gulf War.

Saddam routinely cited his survival as "proof" that Iraq had in fact won the war against the U.S. This message earned Saddam a great deal of popularity in many sectors of the Arab world. John Esposito, however, claims that "Arabs and Muslims were pulled in two directions. That they rallied not so much to Saddam Hussein as to the bipolar nature of the confrontation (the West versus the Arab Muslim world) and the issues that Saddam proclaimed: Arab unity, self-sufficiency, and social justice." As a result, Saddam Hussein appealed to many people for the same reasons that attracted more and more followers to Islamic revivalism and also for the same reasons that fueled anti-Western feelings.

As one U.S. Muslim observer noted: "People forgot about Saddam's record and concentrated on America ... Saddam Hussein might be wrong, but it is not America who should correct him." A shift was, therefore, clearly visible among many Islamic movements in the post war period "from an initial Islamic ideological rejection of Saddam Hussein, the secular persecutor of Islamic movements, and his invasion of Kuwait to a more populist Arab nationalist, anti-imperialist support for Saddam (or more precisely those issues he represented or championed) and the condemnation of foreign intervention and occupation."

Saddam, therefore, increasingly portrayed himself as a devout Muslim, in an effort to co-opt the conservative religious segments of society. Some elements of Sharia law were re-introduced, and the ritual phrase "Allahu Akbar" ("God is great"), in Saddam's handwriting, was added to the national flag. Saddam also commissioned the production of a "Blood Qur'an," written using 27 litres of his own blood, to thank God for saving him from various dangers and conspiracies.

The United Nations sanctions placed upon Iraq when it invaded Kuwait were not lifted, blocking Iraqi oil exports. During the late 1990s, the UN considered relaxing the sanctions imposed because of the hardships suffered by ordinary Iraqis. Studies dispute the number of people who died in south and central Iraq during the years of the sanctions. On 9 December 1996, Saddam's government accepted the Oil-for-Food Programme that the UN had first offered in 1992.

Relations between the United States and Iraq remained tense following the Gulf War. The U.S. launched a missile attack aimed at Iraq's intelligence headquarters in Baghdad 26 June 1993, citing evidence of repeated Iraqi violations of the "no fly zones" imposed after the Gulf War and for incursions into Kuwait. U.S. officials continued to accuse Saddam of violating the terms of the Gulf War's cease fire, by developing weapons of mass destruction and other banned weaponry, and violating the UN-imposed sanctions. Also during the 1990s, President Bill Clinton maintained sanctions and ordered air strikes in the "Iraqi no-fly zones" (Operation Desert Fox), in the hope that Saddam would be overthrown by political enemies inside Iraq. Western charges of Iraqi resistance to UN access to suspected weapons were the pretext for crises between 1997 and 1998, culminating in intensive U.S. and British missile strikes on Iraq, 16–19 December 1998. After two years of intermittent activity, U.S. and British warplanes struck harder at sites near Baghdad in February 2001. Former CIA case officer Robert Baer reports that he "tried to assassinate" Saddam in 1995, amid "a decade-long effort to encourage a military coup in Iraq."

Saddam continued involvement in politics abroad. Video tapes retrieved after show his intelligence chiefs meeting with Arab journalists, including a meeting with the former managing director of Al-Jazeera, Mohammed Jassem al-Ali, in 2000. In the video Saddam's son Uday advised al-Ali about hires in Al-Jazeera: "During your last visit here along with your colleagues we talked about a number of issues, and it does appear that you indeed were listening to what I was saying since changes took place and new faces came on board such as that lad, Mansour." He was later sacked by Al-Jazeera.

In 2002, Austrian prosecutors investigated Saddam government's transactions with Fritz Edlinger that possibly violated Austrian money laundering and embargo regulations. Fritz Edlinger, president of the "General Secretary of the Society for Austro-Arab relations" (GÖAB) and a former member of Socialist International's Middle East Committee, was an outspoken supporter of Saddam Hussein. In 2005, an Austrian journalist revealed that Fritz Edlinger's GÖAB had received $100,000 from an Iraqi front company as well as donations from Austrian companies soliciting business in Iraq.

In 2002, a resolution sponsored by the European Union was adopted by the Commission for Human Rights, which stated that there had been no improvement in the human rights crisis in Iraq. The statement condemned President Saddam Hussein's government for its "systematic, widespread and extremely grave violations of human rights and international humanitarian law." The resolution demanded that Iraq immediately put an end to its "summary and arbitrary executions ... the use of rape as a political tool and all enforced and involuntary disappearances."

Many members of the international community, especially the U.S., continued to view Saddam as a bellicose tyrant who was a threat to the stability of the region. In his January 2002 state of the union address to Congress, President George W. Bush spoke of an "axis of evil" consisting of Iran, North Korea, and Iraq. Moreover, Bush announced that he would possibly take action to topple the Iraqi government, because of the threat of its weapons of mass destruction. Bush stated that "The Iraqi regime has plotted to develop anthrax, and nerve gas, and nuclear weapons for over a decade ... Iraq continues to flaunt its hostility toward America and to support terror."

After the passing of United Nations Security Council Resolution 1441, which demanded that Iraq give "immediate, unconditional and active cooperation" with UN and IAEA inspections, Saddam allowed U.N. weapons inspectors led by Hans Blix to return to Iraq. During the renewed inspections beginning in November 2002, Blix found no stockpiles of WMD and noted the "proactive" but not always "immediate" Iraqi cooperation as called for by UN Security Council Resolution 1441.

With war still looming on 24 February 2003, Saddam Hussein took part in an interview with CBS News reporter Dan Rather. Talking for more than three hours, he denied possessing any weapons of mass destruction, or any other weapons prohibited by UN guidelines. He also expressed a wish to have a live televised debate with George W. Bush, which was declined. It was his first interview with a U.S. reporter in over a decade. CBS aired the taped interview later that week. Saddam Hussein later told an FBI interviewer that he once left open the possibility that Iraq possessed weapons of mass destruction in order to appear strong against Iran.

The Iraqi government and military collapsed within three weeks of the beginning of the U.S.-led 2003 invasion of Iraq on 20 March. By the beginning of April, U.S.-led forces occupied much of Iraq. The resistance of the much-weakened Iraqi Army either crumbled or shifted to guerrilla tactics, and it appeared that Saddam had lost control of Iraq. He was last seen in a video which purported to show him in the Baghdad suburbs surrounded by supporters. When Baghdad fell to U.S.-led forces on 9 April, marked symbolically by the toppling of his statue, Saddam was nowhere to be found.

In April 2003, Saddam's whereabouts remained in question during the weeks following the fall of Baghdad and the conclusion of the major fighting of the war. Various sightings of Saddam were reported in the weeks following the war, but none was authenticated. At various times Saddam released audio tapes promoting popular resistance to his ousting.

Saddam was placed at the top of the "U.S. list of most-wanted Iraqis." In July 2003, his sons Uday and Qusay and 14-year-old grandson Mustapha were killed in a three-hour gunfight with U.S. forces.

On 13 December 2003, in Operation Red Dawn, Saddam Hussein was captured by American forces after being found hiding in a hole in the ground near a farmhouse in ad-Dawr, near Tikrit. Following his capture, Saddam was transported to a U.S. base near Tikrit, and later taken to the American base near Baghdad. Documents obtained and released by the National Security Archive detail FBI interviews and conversations with Hussein while he was in U.S. custody. On 14 December, U.S. administrator in Iraq Paul Bremer confirmed that Saddam Hussein had indeed been captured at a farmhouse in ad-Dawr near Tikrit. Bremer presented video footage of Saddam in custody.

Saddam was shown with a full beard and hair longer than his familiar appearance. He was described by U.S. officials as being in good health. Bremer reported plans to put Saddam on trial, but claimed that the details of such a trial had not yet been determined. Iraqis and Americans who spoke with Saddam after his capture generally reported that he remained self-assured, describing himself as a "firm, but just leader."

British tabloid newspaper "The Sun" posted a picture of Saddam wearing white briefs on the front cover of a newspaper. Other photographs inside the paper show Saddam washing his trousers, shuffling, and sleeping. The United States government stated that it considered the release of the pictures a violation of the Geneva Convention, and that it would investigate the photographs. During this period Saddam was interrogated by FBI agent George Piro.

The guards at the Baghdad detention facility called their prisoner "Vic," which stands for 'Very Important Criminal', and let him plant a small garden near his cell. The nickname and the garden are among the details about the former Iraqi leader that emerged during a March 2008 tour of the Baghdad prison and cell where Saddam slept, bathed, and kept a journal and wrote poetry in the final days before his execution; he was concerned to ensure his legacy and how the history would be told. The tour was conducted by U.S. Marine Maj. Gen. Doug Stone, overseer of detention operations for the U.S. military in Iraq at the time.

On 30 June 2004, Saddam Hussein, held in custody by U.S. forces at the U.S. base "Camp Cropper," along with 11 other senior Ba'athist leaders, were handed over to the interim Iraqi government to stand trial for crimes against humanity and other offences.

A few weeks later, he was charged by the Iraqi Special Tribunal with crimes committed against residents of Dujail in 1982, following a failed assassination attempt against him. Specific charges included the murder of 148 people, torture of women and children and the illegal arrest of 399 others.
Among the many challenges of the trial were:

On 5 November 2006, Saddam Hussein was found guilty of crimes against humanity and sentenced to death by hanging. Saddam's half brother, Barzan Ibrahim, and Awad Hamed al-Bandar, head of Iraq's Revolutionary Court in 1982, were convicted of similar charges. The verdict and sentencing were both appealed, but subsequently affirmed by Iraq's Supreme Court of Appeals.

Saddam was hanged on the first day of Eid ul-Adha, 30 December 2006, despite his wish to be executed by firing squad (which he argued was the lawful military capital punishment citing his military position as the commander-in-chief of the Iraqi military). The execution was carried out at Camp Justice, an Iraqi army base in Kadhimiya, a neighborhood of northeast Baghdad.

Saudi Arabia condemned Iraqi authorities for carrying on with the execution on a holy day. A presenter from the Al—Ikhbariya television station officially stated "There is a feeling of surprise and disapproval that the verdict has been applied during the holy months and the first days of Eid al-Adha. Leaders of Islamic countries should show respect for this blessed occasion ... not demean it." 

Video of the execution was recorded on a mobile phone and his captors could be heard insulting Saddam. The video was leaked to electronic media and posted on the Internet within hours, becoming the subject of global controversy. It was later claimed by the head guard at the tomb where his remains lay that Saddam's body had been stabbed six times after the execution. Saddam's demeanor while being led to the gallows have been discussed by two witnesses, Iraqi Judge Munir Haddad and Iraqi national security adviser Mowaffak al-Rubaie. The accounts of the two witnesses are contradictory as Haddad describes Saddam as being strong in his final moments whereas al-Rubaie says Saddam was clearly afraid.

Not long before the execution, Saddam's lawyers released his last letter.

A second unofficial video, apparently showing Saddam's body on a trolley, emerged several days later. It sparked speculation that the execution was carried out incorrectly as Saddam Hussein had a gaping hole in his neck.

Saddam was buried at his birthplace of Al-Awja in Tikrit, Iraq, on 31 December 2006. He was buried 3 km (2 mi) from his sons Uday and Qusay Hussein. His tomb was reported to have been destroyed in March 2015. Before it was destroyed, a Sunni tribal group reportedly removed his body to a secret location, fearful of what might happen.



In August 1995, Raghad and her husband Hussein Kamel al-Majid and Rana and her husband, Saddam Kamel al-Majid, defected to Jordan, taking their children with them. They returned to Iraq when they received assurances that Saddam would pardon them. Within three days of their return in February 1996, both of the Kamel brothers were attacked and killed in a gunfight with other clan members who considered them traitors.

In August 2003, Saddam's daughters Raghad and Rana received sanctuary in Amman, Jordan, where they are currently staying with their nine children. That month, they spoke with CNN and the Arab satellite station Al-Arabiya in Amman. When asked about her father, Raghad told CNN, "He was a very good father, loving, has a big heart." Asked if she wanted to give a message to her father, she said: "I love you and I miss you." Her sister Rana also remarked, "He had so many feelings and he was very tender with all of us."

With the intention of discrediting Saddam Hussein with his supporters, the CIA was considering making a video in which he would be seen having sex with a teenager.

In 1979, Rev. Jacob Yasso of Chaldean Sacred Heart Church congratulated Saddam Hussein on his presidency. In return, Rev. Yasso said that Saddam Hussein donated US$250,000 to his church, which is made up of at least 1,200 families of Middle Eastern descent. In 1980, Detroit Mayor Coleman Young allowed Rev. Yasso to present the key to the city of Detroit to Saddam Hussein. At the time, Saddam then asked Rev. Yasso, "I heard there was a debt on your church. How much is it?" After the inquiry, Saddam then donated another $200,000 to Chaldean Sacred Heart Church. Rev. Yasso said that Saddam made donations to Chaldean churches all over the world, and even went on record as saying "He's very kind to Christians."






</doc>
<doc id="29491" url="https://en.wikipedia.org/wiki?curid=29491" title="Sonja Henie">
Sonja Henie

Sonja Henie (8 April 1912 – 12 October 1969) was a Norwegian figure skater and film star. She was a three-time Olympic Champion (1928, 1932, 1936) in Ladies' Singles, a ten-time World Champion (1927–1936) and a six-time European Champion (1931–1936). Henie won more Olympic and World titles than any other ladies' figure skater. At the height of her acting career, she was one of the highest-paid stars in Hollywood and starred in a series of box-office hits, including "Thin Ice" (1937), "My Lucky Star" (1938), "Second Fiddle" (1939) and "Sun Valley Serenade" (1941).

Henie was born in 1912 in Kristiania (now Oslo) Norway; she was the only daughter of Wilhelm Henie (1872–1937), a prosperous Norwegian furrier, and his wife, Selma Lochmann-Nielsen (1888–1961). In addition to the income from the fur business, both of Henie's parents had inherited wealth. Wilhelm Henie had been a one-time World Cycling Champion and the Henie children were encouraged to take up a variety of sports at a young age. Henie initially showed talent at skiing, then followed her older brother, Leif, to take up figure skating. As a girl Henie also was a nationally ranked tennis player, and a skilled swimmer and equestrienne. Once Henie began serious training as a figure skater, her formal schooling ended. She was educated by tutors, and her father hired the best experts in the world, including the famous Russian ballerina, Tamara Karsavina, to transform his daughter into a sporting celebrity.

Henie won her first major competition, the senior Norwegian championships, at the age of 10. She then placed eighth in a field of eight at the 1924 Winter Olympics, at the age of eleven. During the 1924 program, she skated over to the side of the rink several times to ask her coach for directions, but by the next Olympiad, she needed no such assistance.

Henie won the first of an unprecedented ten consecutive World Figure Skating Championships in 1927 at the age of fourteen. The results of 1927 World Championships, where Henie won in 3–2 decision (or 7 vs. 8 ordinal points) over the defending Olympic and World Champion Herma Szabo of Austria, was controversial, as three of the five judges that gave Henie first-place ordinals were Norwegian (1 + 1 + 1 + 2 + 2 = 7 points) while Szabo received first-place ordinals from an Austrian and a German Judge (1 + 1 + 2 + 2 + 2 = 8 points). Henie went on to win first of her three Olympic gold medals the following year, became one of the youngest figure skating Olympic champions. She defended her Olympic titles in 1932 and in 1936, and her world titles annually until 1936. She also won six consecutive European championships from 1931 to 1936. Henie's unprecedented three Olympic gold medals haven't been matched by any ladies' single skater since; neither are her achievements as ten-time consecutive World Champion. While Irina Slutskaya of Russia won her seventh European Championship in 2006 to become the most successful ladies' skater in European Championships, Henie retains record of most consecutive titles, sharing it with Katarina Witt of Eastern Germany/Germany (1983–1988).

Towards the end of her career, she began to be strongly challenged by younger skaters including Cecilia Colledge, Megan Taylor, and Hedy Stenuf. However, she held off these competitors and went on to win her third Olympic title at the 1936 Winter Olympics, albeit in very controversial circumstances with Cecilia Colledge finishing a very close second. Indeed, after the school figures section at the 1936 Olympic competition, Colledge and Henie were virtually neck and neck with Colledge trailing by just a few points. As Sandra Stevenson recounted in her article in "The Independent" of 21 April 2008, "the closeness [of the competition] infuriated Henie, who, when the result for that section was posted on a wall in the competitors' lounge, swiped the piece of paper and tore it into little pieces. The draw for the free skating [then] came under suspicion after Henie landed the plum position of skating last, while Colledge had to perform second of the 26 competitors. The early start was seen as a disadvantage, with the audience not yet whipped into a clapping frenzy and the judges known to become freer with their higher marks as the event proceeded. Years later, a fairer, staggered draw was adopted to counteract this situation".

During her competitive career, Henie traveled widely and worked with a variety of foreign coaches. At home in Oslo, she trained at Frogner Stadium, where her coaches included Hjørdis Olsen and Oscar Holte. During the latter part of her competitive career she was coached primarily by the American Howard Nicholson in London. In addition to traveling to train and compete, she was much in demand as a performer at figure skating exhibitions in both Europe and North America. Henie became so popular with the public that police had to be called out for crowd control on her appearances in various disparate cities such as Prague and New York City. It was an open secret that, in spite of the strict amateurism requirements of the time, Wilhelm Henie demanded "expense money" for his daughter's skating appearances. Both of Henie's parents had given up their own pursuits in Norway—leaving Leif to run the fur business—in order to accompany Sonja on her travels and act as her managers.

Henie is credited with being the first figure skater to adopt the short skirt costume in figure skating, wear white boots, and make use of dance choreography. Her innovative skating techniques and glamorous demeanor transformed the sport permanently and confirmed its acceptance as a legitimate sport in the Winter Olympics.

After the 1936 World Figure Skating Championships, Henie gave up her amateur status and took up a career as a professional performer in acting and live shows. While still a girl, Henie had decided that she wanted to move to California and become a movie star when her competitive days were over, without considering that her thick accent might hinder her acting ambitions.

In 1936, following a successful ice show in Los Angeles orchestrated by her father to launch her film career, Hollywood studio chief Darryl Zanuck signed her to a long term contract at Twentieth Century Fox, which made her one of the highest-paid actresses of the time. After the success of her first film, "One in a Million" (1936), Henie's position was assured and she became increasingly demanding in her business dealings with Zanuck. Henie also insisted on having total control of the skating numbers in her films such as "Second Fiddle" (1939).

Henie tried to break the musical comedy mould with the anti-Nazi film "Everything Happens at Night" (1939) and "It's a Pleasure" (1945), a skating variation of the often-told "A Star Is Born" tale about alcoholic-star-in-decline-helps-newcomer-up. It was her only film shot in Technicolor, but it was not as huge at the box office as her other films and also proved her limitations as a dramatic actress in her one only dramatic film.

When Zanuck realized her limits in the pseudo-dramatic vein, he cast her in more musical comedies; "Sun Valley Serenade" (1941) with Glenn Miller, John Payne, The Nicholas Brothers, and hit songs such as "In the Mood", "Chattanooga Choo Choo", "It Happened in Sun Valley", and "I Know Why (And So Do You)", followed by "Iceland" (1942) with Jack Oakie, Payne, and the hit song "There Will Never Be Another You", and finally "Wintertime" (1943) with Cesar Romero, Carole Landis, Cornel Wilde, and Oakie. Sonja had by now developed a comedy flair and these films were all among the top box-office hits for 20th Century-Fox the respective years.

In addition to her film career at Fox from 1936 to 1943, Henie formed a business arrangement with Arthur Wirtz, who produced her touring ice shows under the name of "Hollywood Ice Revue". Wirtz also acted as Henie's financial advisor. At the time, figure skating and ice shows were not yet an established form of entertainment in the United States. Henie's popularity as a film actress attracted many new fans and instituted skating shows as a popular new entertainment. Throughout the 1940s, Henie and Wirtz produced lavish musical ice skating extravaganzas at Rockefeller Center's Center Theatre attracting millions of ticket buyers.

At the height of her fame, Henie brought as much as $2 million per year from her shows and touring activities. She also had numerous lucrative endorsement contracts, and deals to market skates, clothing, jewelry, dolls, and other merchandise branded with her name. These activities made her one of the wealthiest women in the world in her time.

Henie broke off her arrangement with Wirtz in 1950 and for the next three seasons produced her own tours under the name "Sonja Henie Ice Revue". It was an ill-advised decision to set herself up in competition with Wirtz, whose shows now featured the new Olympic champion Barbara Ann Scott. Since Wirtz controlled the best arenas and dates, Henie was left playing smaller venues and markets already saturated by other touring ice shows such as Ice Capades. The collapse of a section of bleachers during a show in Baltimore, Maryland, in 1952 compounded the tour's legal and financial woes.

In 1953, Henie formed a new partnership with Morris Chalfen to appear in his European "Holiday On Ice" tour, which proved to be a great success. She produced her own show at New York's Roxy Theatre in January 1956. However, a subsequent South American tour in 1956 was a disaster. Henie was drinking heavily at that time and could no longer keep up with the demands of touring, and this marked her retirement from skating. She did try to make a film series at her own expense; a series that would serve as a travelogue to several cities. Paris and London were mentioned, but only "Hello London" (1958) was made with her own backing, co-starring Michael Wilding and special guest star Stanley Holloway. While her ice show numbers were still worth watching, the film received few distributors and poor reviews, ending her film career. 

Her autobiography "Mitt livs eventyr", was published in 1938 which was translated and released as "Wings on My Feet" in 1940, and which was republished in a revised edition in 1954. At the time of her death, Henie was planning a comeback for a television special that would have aired in January 1970. She was to have danced to "Lara's Theme" from "Doctor Zhivago".

Henie's connections with Adolf Hitler and other high-ranking Nazi officials made her the subject of controversy before, during, and after World War II. During her amateur skating career, she performed often in Germany and was a favorite of German audiences and of Hitler personally. As a wealthy celebrity, she moved in the same social circles as royalty and heads of state and made Hitler's acquaintance as a matter of course. During the shooting of "Second Fiddle" (1939), she greeted the then Crown-Prince couple of Norway Olav and Märtha during their US tour. Through the years, her shows and later art exhibitions drew the attention of such people as Princess Margaret, Countess of Snowdon and Gustaf VI Adolf of Sweden and she met with them.

Controversy appeared first when Henie greeted Hitler with a Nazi salute at the 1936 Winter Olympics in Garmisch-Partenkirchen and after the Games she accepted an invitation to lunch with Hitler at his resort home in nearby Berchtesgaden, where Hitler presented Henie with an autographed photo with a lengthy inscription. She was strongly denounced in the Norwegian press for this. In her revised 1954 biography, she states that no Norwegian judge was in the panel for the 1936 Olympics – as she was entitled to as a Norwegian. She therefore made the most of it and won her third Olympic medal. When she – as a gold medal winner – passed Hitler's tribune with silver medalist Cecilia Colledge and bronze medalist Vivi-Anne Hultén, neither she nor the others honored Hitler with the Nazi salute. The 1936 European Figure Skating Championships also took place in Berlin and neither Henie, Colledge, nor Megan Taylor paid obeisance to Hitler. 

In her film "Everything Happens at Night" (1939), Ray Milland and Robert Cummings star as rival reporters hot on the trail of Dr. Hugo Norden (Maurice Moscovich). Norden, a Nobel Prize winner, was supposedly murdered by the Gestapo, but is rumored to be in hiding and writing anonymous dispatches advocating world peace. When Geoffrey and Ken track Dr. Norden to a small village in the Swiss Alps, they soon find themselves competing over the affections of beautiful Louise (Henie), who has a deeper connection to the missing Nobel laureate than the reporters realize. When Geoffrey and Ken get so distracted by romance that they begin to neglect their assignments, it almost leads to disaster as the Gestapo sets out to silence Dr. Norden once and for all. Released on 22 December 1939, it was banned in Nazi Germany.

Through her 1940 marriage to Dan Topping she had become an American citizen. As such, she was not eligible to speak Norway's cause and with producer Alexander Korda, who was set to produce the propaganda film That Hamilton Woman, could have faced deportation. The Senate Subcommittee (Senate Foreign Relations) dealt with such matters. After the bombing of Pearl Harbor, when America was no longer neutral, Henie pulled in uniform and visited and gave money to Little Norway. All Norwegians got free tickets to her shows during the war and she paid and held parties for them.

During the occupation of Norway by Nazi Germany, German troops saw Hitler's autographed photo prominently displayed at the piano in the Henie family home in Landøya, Asker. As a result, none of Henie's properties in Norway were confiscated or damaged by the Germans. Henie became a naturalized citizen of the United States in 1940. Like many Hollywood stars, she supported the U.S. war effort through USO and similar activities. After the Japanese attack, she invited the boys from Little Norway to her ice shows, gave the mechanics a plane as well a substantial sum of money to their educational fund. But her first rejection before the US entered the war was never to be forgotten. For this, she was condemned by many Norwegians and Norwegian-Americans. After the war, Henie was mindful that many of her countrymen considered her to be a quisling. However, she made a triumphant return to Norway with the Holiday on Ice tour in 1953 and 1955. The Norwegian Royal Family attended both events and indeed attended her funeral in 1969. The Royal Family were very mindful of whom they supported after the war and Norwegians looked to them as role models in that respect. Her complex reputation and legacy continues to stimulate debate amongst Norwegians, writers and historians.

Henie was married three times, to Dan Topping (1940–1946), Winthrop Gardiner Jr. (1949–1956), and the Norwegian shipping magnate and art patron Niels Onstad (1956–1969) (her death). After her retirement in 1956, Henie and Onstad settled in Oslo and accumulated a large collection of modern art that formed the basis for the Henie Onstad Kunstsenter at Høvikodden in Bærum near Oslo.

Henie was diagnosed with chronic lymphocytic leukemia in the mid-1960s. She died of the disease at age 57 in 1969 during a flight from Paris to Oslo. Generally regarded as one of the greatest figure skaters in history, she is buried with Onstad in Oslo on the hilltop overlooking the Henie Onstad Art Centre.








</doc>
<doc id="29493" url="https://en.wikipedia.org/wiki?curid=29493" title="Science &amp; Environmental Policy Project">
Science &amp; Environmental Policy Project

The Science & Environmental Policy Project (SEPP) is an advocacy group financed by private contributions based in Arlington, Virginia in the United States. It was founded in 1990 by atmospheric physicist S. Fred Singer. SEPP disputes the prevailing scientific views of climate change and ozone depletion. SEPP also questioned the science used to establish the dangers of secondhand smoke, arguing the risks are overstated.

SEPP's former Chairman of the Board of Directors is listed as Rockefeller University president emeritus Frederick Seitz, a former president of the National Academy of Sciences, now deceased.

SEPP listed the following key issues in 2010: 

On September 2, 1997, Singer said that "The possibility that global temperatures could rise because of an increase in carbon dioxide in the atmosphere is a concern that needs to be monitored...But there has been no indication in the last century that we've seen anything other than natural climate fluctuations. Both greenhouse theory and computer models predict that global warming should be more rapid in the polar regions than anywhere else," he says, "but in July the Antarctic experienced the coldest weather on record."

SEPP was the author of the Leipzig Declaration, which was based on the conclusions drawn from a November 1995 conference in Leipzig, Germany, which SEPP organized with the European Academy for Environmental Affairs.

SEPP's critics offer the following rebuttals to its claims:


In 2008, The Science and Environmental Policy Project completed the organization of the Nongovernmental International Panel on Climate Change (NIPCC) as the culmination of a process that began in 2003. The NIPCC calls itself "an international coalition of scientists convened to provide an independent examination of the evidence available on the causes and consequences of climate change in the published, peer-reviewed literature – examined without bias and selectivity."

The 2008 NIPCC document titled "Nature, Not Human Activity Rules the Climate: Summary for Policymakers of the Report of the Nongovernmental International Panel of Climate Change", published by The Heartland Institute, was released in February–March 2008. Singer served as General Editor and also holds the copyright.

Unnamed climate scientists from NASA, Stanford University and Princeton who were contacted by ABC News dismissed the same report as "fabricated nonsense.". In response, Singer objected to the ABC News piece, calling it "an appalling display of bias, unfairness, journalistic misbehavior, and a breakdown of ethical standards" which used "prejudicial language, distorted facts, libelous insinuations, and anonymous smears."


In 2004 Singer was coauthor of two papers published in Geophysical Research Letters:

Scientific criticism of SEPP's views:



</doc>
<doc id="29494" url="https://en.wikipedia.org/wiki?curid=29494" title="Abbey of Saint Gall">
Abbey of Saint Gall

The Abbey of Saint Gall () is a dissolved abbey (747–1805) in a Catholic religious complex in the city of St. Gallen in Switzerland. The Carolingian-era monastery has existed since 719 and became an independent principality between 9th and 13th centuries, and was for many centuries one of the chief Benedictine abbeys in Europe. It was founded by Saint Othmar on the spot where Saint Gall had erected his hermitage. The library of the Abbey is one of the richest medieval libraries in the world. The city of St. Gallen originated as an adjoining settlement of the abbey. Following the secularization of the abbey around 1800 the former Abbey church became a Cathedral in 1848. Since 1983 the whole remaining abbey precinct has been a UNESCO World Heritage Site.

Around 613 Gallus, according to tradition an Irish monk and disciple and companion of Saint Columbanus, established a hermitage on the site that would become the monastery. He lived in his cell until his death in 646, and was buried there in Arbon. Afterwards, the people venerated him as a saint and prayed at his tomb for his intercession in times of danger.

Following Gallus' death, Charles Martel appointed Otmar as custodian of St Gall's relics. Several different dates are given for the foundation of the monastery, including 719, 720, 747 and the middle of the 8th century. During the reign of Pepin the Short, in the 8th century, Othmar founded the Carolingian style Abbey of St Gall, where arts, letters and sciences flourished. The abbey grew fast and many Alemannic noblemen became monks. At the end of abbot Otmar's reign, the "Professbuch" mentions 53 names. Two monks of the Abbey of St Gall, Magnus von Füssen and Theodor, founded the monasteries in Kempten and Füssen in the Allgäu. With the increase in the number of monks the abbey grew stronger also economically. Much land in Thurgau, Zürichgau and in the rest of Alemannia as far as the Neckar was transferred to the abbey due to "Stiftungen". Under abbot Waldo of Reichenau (740–814) copying of manuscripts was undertaken and a famous library was gathered. Numerous Anglo-Saxon and Irish monks came to copy manuscripts. At Charlemagne's request Pope Adrian I sent distinguished chanters from Rome, who propagated the use of the Gregorian chant. In 744, the Alemannic nobleman Beata sells several properties to the abbey in order to finance his journey to Rome.

In the subsequent century, St Gall came into conflict with the nearby Bishopric of Constance which had recently acquired jurisdiction over the Abbey of Reichenau on Lake Constance. It was not until Emperor Louis the Pious (ruled 814–840) confirmed in 813 the imperial immediacy ("Reichsunmittelbarkeit") of the abbey, that this conflict ceased. The abbey became an Imperial Abbey ("Reichsabtei"). King Louis the German confirmed in 833 the immunity of the abbey and allowed the monks the free choice of their abbot. In 854 finally, the Abbey of St Gall reached its full autonomy by King Louis the German releasing the abbey from the obligation to pay tithes to the Bishop of Constance.

From this time until the 10th century, the abbey flourished. It was home to several famous scholars, including Notker of Liège, Notker the Stammerer, Notker Labeo and Hartker (who developed the antiphonal liturgical books for the abbey). During the 9th century a new, larger church was built and the library was expanded. Manuscripts on a wide variety of topics were purchased by the abbey and copies were made. Over 400 manuscripts from this time have survived and are still in the library today.

Between 924 and 933 the Magyars threatened the abbey and the books had to be removed to Reichenau for safety. Not all the books were returned.

On 26 April 937 a fire broke out and destroyed much of the abbey and the adjoining settlement, though the library was undamaged. About 954 they started to protect the monastery and buildings by a surrounding wall. Around 971/974 abbot Notker (about whom almost nothing is known; nephew of Notker Physicus) finalized the walling and the adjoining settlements started to become the town of St Gall. In 1006, the abbey was the northernmost place where a sighting of the 1006 supernova was recorded.

The death of abbot Ulrich II on 9 December 1076 terminated the cultural silver age of the monastery.

In 1207, abbot Ulrich von Sax becomes a Prince ("Reichsfürst", or simply "Fürst") of the Holy Roman Empire by King Philip of Swabia. The abbey became a Princely Abbey ("Reichsabtei"). As the abbey became more involved in local politics, it entered a period of decline. 
The city of St. Gallen proper progressively freed itself from the rule of the abbot, acquiring Imperial immediacy, and by the late 15th century was recognized as a Free imperial city.
By about 1353 the guilds, headed by the cloth-weavers guild, gained control of the civic government. In 1415 the city bought its liberty from the German king King Sigismund.
During the 14th century Humanists were allowed to carry off some of the rare texts from the abbey library.

In the late 14th and early 15th centuries, the farmers of the abbot's personal estates (known as "Appenzell", from meaning "cell (i.e. estate) of the abbot") began seeking independence. In 1401, the first of the Appenzell Wars broke out, and following the Appenzell victory at Stoss in 1405 they became allies of the Swiss Confederation in 1411. During the Appenzell Wars, the town of St. Gallen often sided with Appenzell against the abbey. So when Appenzell allied with the Swiss, the town of St. Gallen followed just a few months later. The abbot became an ally of several members of the Swiss Confederation (Zürich, Lucerne, Schwyz and Glarus) in 1451. While Appenzell and St. Gallen became full members of the Swiss Confederation in 1454. Then, in 1457 the town of St. Gallen became officially free from the abbot.

In 1468 the abbot, Ulrich Rösch, bought the County of Toggenburg from the representatives of its counts, after the family died out in 1436. In 1487 he built a monastery at Rorschach on Lake Constance, to which he planned to move. However, he encountered stiff resistance from the St. Gallen citizenry, other clerics, and the Appenzell nobility in the Rhine Valley who were concerned about their holdings. The town of St. Gallen wanted to restrict the increase of power in the abbey and simultaneously increase the power of the town. The mayor of St. Gallen, Ulrich Varnbüler, established contact with farmers and Appenzell residents (led by the fanatical Hermann Schwendiner) who were seeking an opportunity to weaken the abbot. Initially, he protested to the abbot and the representatives of the four sponsoring Confederate cantons (Zürich, Lucerne, Schwyz, and Glarus) against the construction of the new abbey in Rorschach. Then on July 28, 1489 he had armed troops from St. Gallen and Appenzell destroy the buildings already under construction. When the abbot complained to the Confederates about the damages and demanded full compensation, Varnbüler responded with a counter suit and in cooperation with Schwendiner rejected the arbitration efforts of the non-partisan Confederates. He motivated the clerics from Wil to Rorschach to discard their loyalty to the abbey and spoke against the abbey at the town meeting at Waldkirch, where the popular league was formed. He was confident that the four sponsoring cantons would not intervene with force, due to the prevailing tensions between the Confederation and the Swabian League. He was strengthened in his resolve by the fact that the people of St. Gallen elected him again to the highest magistrate in 1490.

However, in early 1490 the four cantons decided to carry out their duty to the abbey and to invade the St. Gallen canton with an armed force. The people of Appenzell and the local clerics submitted to this force without noteworthy resistance, while the city of St. Gallen braced for a fight to the finish. However, when they learned that their compatriots had given up the fight, they lost confidence; the end result was that they concluded a peace pact that greatly restricted the city's powers and burdened the city with serious penalties and reparations payments. Varnbüler and Schwendiner fled to the court of King Maximilian and lost all their property in St. Gallen and Appenzell. However, the abbot's reliance on the Swiss to support him reduced his position almost to that of a "subject district".

The town adopted the Reformation in 1524, while the abbey remained Catholic, which damaged relations between the town and abbey. Both the abbot and a representative of the town were admitted to the Swiss Tagsatzung or Diet as the closest associates of the Confederation.

In the 16th century the abbey was raided by Calvinist groups, which scattered many of the old books. In 1530, abbot Diethelm began a restoration that stopped the decline and led to an expansion of the schools and library.

Under abbot Pius (1630–74) a printing press was started. In 1712 during the Toggenburg war, also called the second war of Villmergen, the Abbey of St. Gall was pillaged by the Swiss. They took most of the books and manuscripts to Zürich and Bern. For security, the abbey was forced to request the protection of the townspeople of St. Gallen. Until 1457 the townspeople had been serfs of the abbey, but they had grown in power until they were protecting the abbey.

Following the disturbances, the abbey was still the largest religious city-state in Switzerland, with over 77,000 inhabitants. A final attempt to expand the abbey resulted in the demolition of most of the medieval monastery. The new structures, including the cathedral by architect Peter Thumb (1681-1766), were designed in the late Baroque style and constructed between 1755 and 1768. The large and ornate new abbey did not remain a monastery for very long. In 1798 the Prince-Abbot's secular power was suppressed, and the abbey was secularized. The monks were driven out and moved into other abbeys. The abbey became a separate See in 1846, with the abbey church as its cathedral and a portion of the monastic buildings for the bishop.

The Abbey library of Saint Gall is recognized as one of the richest medieval libraries in the world. It is home to one of the most comprehensive collections of early medieval books in the German-speaking part of Europe. , the library consists of over 160,000 books, of which 2100 are handwritten. Nearly half of the handwritten books are from the Middle Ages and 400 are over 1000 years old. Lately the "Stiftsbibliothek" has launched a project for the digitisation of the priceless manuscript collection, which currently (December 2009) contains 355 documents that are available on the "Codices Electronici Sangallenses" webpage.

The library interior is exquisitely realised in the Rococo style with carved polished wood, stucco and paint used to achieve its overall effect. It was designed by the architect Peter Thumb and is open to the public. In addition it holds exhibitions as well as concerts and other events.

One of the more interesting documents in the Stiftsbibliothek is a copy of Priscian's "Institutiones grammaticae" which contains the poem "Is acher in gaíth in-nocht..." written in Old Irish.

The library also preserves a unique 9th-century document, known as the Plan of St. Gall, the only surviving major architectural drawing from the roughly 700-year period between the fall of the Western Roman Empire and the 13th century. The Plan drawn was never actually built, and was so named because it was kept at the famous medieval monastery library, where it remains to this day. The plan was an ideal of what a well-designed and well-supplied monastery should have, as envisioned by one of the synods held at Aachen for the reform of monasticism in the Frankish empire during the early years of emperor Louis the Pious (between 814 and 817).

A late 9th-century drawing of St. Paul lecturing an agitated crowd of Jews and gentiles, part of a copy of a Pauline epistles produced at and still held by the monastery, was included in a medieval-drawing show at the Metropolitan Museum of Art in New York the summer of 2009. A reviewer noted that the artist had "a special talent for depicting hair, ... with the saint's beard ending in curling droplets of ink."

St. Gall is noted for its early use of the neume, the basic element of Western and Eastern systems of musical notation prior to the invention of five-line staff notation. The earliest extant manuscripts are from the 9th or 10th century.

In 1983, the Convent of St. Gall was inscribed on the UNESCO World Heritage List as "a perfect example of a great Carolingian monastery".

There were a total of 73 ruling abbots (including six anti-abbots) during 719 and 1805.
A complete collection of abbots' biographies was published 
by Henggeler (1929). A table of abbots' names complete with their coats of arms was printed by Beat Jakob Anton Hiltensperger in 1778.





</doc>
<doc id="29498" url="https://en.wikipedia.org/wiki?curid=29498" title="Secondary education">
Secondary education

Secondary education covers two phases on the International Standard Classification of Education scale. Level 2 or lower secondary education (less common junior secondary education) is considered the second and final phase of basic education, and level 3 (upper) secondary education is the stage before tertiary education. Every country aims to provide basic education, but the systems and terminology remain unique to them. Secondary education typically takes place after six years of primary education and is followed by higher education, vocational education or employment. Like primary education, in most countries secondary education is compulsory, at least until the age of 16. Children typically enter the lower secondary phase around age 11. Compulsory education sometimes extends to age 19.

Since 1989, education has been seen as a basic human right for a child; Article 28, of the Convention on the Rights of the Child states that primary education should be free and compulsory while different forms of secondary education, including general and vocational education, should be available and accessible to every child. The terminology has proved difficult, and there was no universal definition before ISCED divided the period between primary education and university into junior secondary education and upper secondary education.

In classical and medieval times secondary education was provided by the church for the sons of nobility and to boys preparing for universities and the priesthood. As trade required navigational and scientific skills the church reluctantly expanded the curriculum and widened the intake. With the Reformation the state wrestled the control of learning from the church, and with Comenius and John Locke education changed from being repetition of Latin text to building up knowledge in the child. Education was for the few. Up to the middle of the 19th century, secondary schools were organised to satisfy the needs of different social classes with the labouring classes getting 4 years, the merchant class 5 years and the elite getting 7 years. The rights to a secondary education were codified after 1945, and countries are still working to achieve the goal of mandatory and free secondary education for all youth under 19.

The 1997 International Standard Classification of Education (ISCED) describes seven levels that can be used to compare education internationally. Within a country these can be implemented in different ways, with different age levels and local denominations. The seven levels are:

Within this system, Levels 1 and 2 – that is, primary education and lower secondary – together form basic education. Beyond that, national governments may attach the label of secondary education to Levels 2 through 4 together, Levels 2 and 3 together, or Level 2 alone. These level definitions were put together for statistical purposes, and to allow the gathering of comparative data nationally and internationally. They were approved by the UNESCO General Conference at its 29th session in November 1997. Though they may be dated, they do provide a universal set of definitions and remain unchanged in the 2011 update.

The start of lower secondary education is characterised by the transition from the single-class-teacher, who delivers all content to a cohort of pupils, to one where content is delivered by a series of subject specialists. Its educational aim is to complete provision of basic education (thereby completing the delivery of basic skills) and to lay the foundations for lifelong learning.

Lower secondary education is likely to show these criteria:

The end of lower secondary education often coincides with the end of compulsory education in countries where that exists.

(Upper) secondary education starts on the completion of basic education, which also is defined as completion of lower secondary education. The educational focus is varied according to the student's interests and future direction. Education at this level is usually voluntary.

(Upper) secondary education is likely to show these criteria:

More subjects may be dropped, and increased specialism occurs. Completion of (upper) secondary education provides the entry requirements to Level 5 tertiary education, the entry requirements to technical or vocational education (Level 4, non tertiary course), or direct entry into the workplace.

In 2012 the ISCED published a further work on education levels where it codified particular paths and redefined the tertiary levels. Lower secondary education and (upper) secondary education could last between 2 and 5 years, and the transition between two often would be when students were allowed some subject choice.

Terminology for secondary schools varies by country, and the exact meaning of any of these varies. Secondary schools may also be called "academies", "colleges", "gymnasiums", "high schools", "lyceums", "middle schools", "preparatory schools", "sixth-form colleges", "upper schools", or "vocational schools", among other names. For further information about nomenclature, see the section below by country.

A form of education for adolescents became necessary in all societies that had an alphabet and engaged in commerce. In Western Europe, formal secondary education can be traced back to the Athenian educational reforms of 320BC. Though their civilisation was eclipsed and they were enslaved, Hellenistic Athenian teachers were valued in the Roman system. The Roman and Hellenistic schools of rhetoric taught the seven liberal arts and sciences – "grammar, rhetoric, logic, arithmetic, geometry, music" and "astronomy" – which were regarded as a preparation for the study at a tertiary level of theology, law and medicine. Boys would have been prepared to enter these schools by private tutors at home. Girls would have only received tuition at home.
When the Romans retreated, all traces of civilisation were erased.

England provides a good case study. When Augustine of Canterbury brought Christianity there in 597, no schools existed. He needed trained priests to conduct church services and boys to sing in the choir. He had to create both the grammar schools that taught Latin, to enable the English to study for the priesthood, and song schools (choir schools) that trained the 'sons of gentlefolk' to sing in cathedral choirs. In the case of Canterbury (597) and Rochester (604), both still exist. Bede in his Ecclesiastical history (732) tells that the Canterbury school taught more than the 'intended reading and understanding of Latin', but 'the rules of metric, astronomy and the computus as well as the works of the saints' Even at this stage, there was tension, as the church was worried that knowledge of Latin would give the student access to non-Christian texts that it would not wish them to read.

Over the centuries leading to the renaissance and reformation the church was the main provider of secondary education. Various invasions and schisms within the controlling church challenged the focus of the schools, and the curriculum and language of instruction waxed and waned. From 1100, With the growth of the towns, grammar schools 'free' of the church were founded, and some church grammar schools were handed over to the laïty. Universities were founded that didn't just train students for the priesthood.

Whereas in mainland Europe the renaissance preceded the reformation, local conditions in England caused the reformation to come first. The reformation was about allowing the laïty to interpret the Bible in their own way without the intervention of priests, and preferably in the vernacular. This stimulated the foundation of free Grammar schools- who searched for a less constrained curriculum. Colonialisation required navigation, mensuration, languages and administrative skills. The laïty wanted these taught to their sons. After Gutenberg1455 had mastered moveable metal type printing and Tyndale had translated the Bible into English (1525), Latin became a skill reserved for the catholic church and sons conservative nobility. Schools started to be set up for the sons of merchants in Europe and the colonies too- for example Boston Latin Grammar School (1635).

Comenius (1592–1670), a Moravian protestant proposed a new model of education- where ideas were developed from the familiar to the theoretical rather than through repetition, where languages were taught in the vernacular and supported universal education. In his "Didactica Magna" (Great Didactic), he outlined a system of schools that is the exact counterpart of many western school systems: kindergarten, elementary school, secondary school, six-form college, university.
Locke's Some Thoughts Concerning Education (1693) stressed the importance of a broader intellectual training, moral development and physical hardening. .

The grammar schools of the period can be categorised in three groups: the nine leading schools, seven of them boarding institutions which maintained the traditional curriculum of the classics, and mostly served 'the aristocracy and the squirearchy' ; most of the old endowed grammar schools serving a broad social base in their immediate localities which also stuck to the old curriculum; the grammar schools situated in the larger cities, serving the families of merchants and tradesmen who embraced change.

During the 18th century their social base widened and their curriculum developed, particularly in mathematics and the natural sciences. But this was not universal education and was self-selecting by wealth The industrial revolution changed that. Industry required an educated workforce where all workers needed to have completed a basic education. In France, Louis XIV, wrestled the control of education from the Jesuits, Condorcet set up Collèges for universal lower secondary education throughout the country, then Napoleon set up a regulated system of Lycee. In England, Robert Peel's Factory Act of 1802 required an employer to provide instruction in reading, writing and arithmetic during at least the first four years of the seven years of apprenticeship. The state had accepted responsibility for the basic education of the poor.
The provision of school places remained inadequate, so an Order in Council dated 10 April 1839 created the Committee of the Privy Council on Education.

There was considerable opposition to the idea that children of all classes should receive basic education, all the initiatives such as industrial schools and Sunday schools were initially a private or church initiative. With the Great Exhibition of 1851, it became clear just how far behind the English education system had fallen. 

Three reports were commissioned to examine the education of upper, middle and labouring class children. The Clarendon Commission sought to improve the nine Great Public Schools. The Taunton Commission looked at the 782 endowed grammar schools (private and public). They found varying quality and a patchy geographical coverage, with two thirds of all towns not having any secondary school. There was no clear conception of the purpose of secondary education. There were only thirteen girls' schools and their tuition was superficial, unorganised and unscientific. They recommended a system of first-grade schools targeted at a leaving age of 18 as preparation for upper and upper-middle class boys entering university, second-grade targeted at a leaving age of 16 for boys preparing for the army or the newer professions, and third-grade targeted at a leaving age of 14 for boys of small tenant farmers, small tradesmen, and superior artisans. This resulted in the 1869 Endowed Schools Act which advocated that girls should enjoy the same education as boys.

The Newcastle Commission inquired "into the state of public education in England and to consider and report what measures, if any, are required for the extension of sound and cheap elementary instruction to all classes of the people". It produced 1861 Newcastle Report and this led to the 1870 Elementary Education Act (Forster Act).

The school boards set up by the 1870 Elementary Education Act (Forster Act) and were stopped from providing secondary education by the Cockerton Judgement of 1899. The school leaving age at this time was 10. The Judgement prompted the 1902 Education Act (Balfour Act). Compulsory education was extended to 12. The new Local Education Authorities (LEA)s that were formed from the school boards; started to open Higher Grade Elementary Schools (ISCED Level2) or county schools to supplement the endowed grammar schools. These LEAs were allowed to build second-grade secondary schools that in the main became the future secondary modern schools. 

In the ""1904 Regulations for Secondary Schools"", the Board of Education determined that secondary schools should offer a:
a four year subject-based course leading to a certificate in English language and literature, geography, history, a foreign language, mathematics, science, drawing, manual work, physical training, and, for girls, housewifery. 

The Education Act 1918 (Fisher Act) extended compulsory full-time education to 14, and recommended compulsory part-time education from 14–18.
The Hadlow report, "Education the Adolescent" (1926) proposed that there should be a break point at eleven, establishing primary schools and secondary schools.

The United Nations, founded in 1947, was committed to education for all but the definition was difficult to formulate.
The Universal Declaration of Human Rights (1948) declared that elementary and fundamental education, which it didn't define, was a right to be enjoyed by all. The Education Act 1944 (Butler Act) made sweeping changes to the funding of state education using the tripartite system, but wasn't allowed to tackle private schools. It introduced the GCE 'O'level at 16, and the 'A' at 18, but only raised the school leaving age until 15, making the exam inaccessible to the majority. But one year of ISCED Level 3 (Upper) secondary education was mandatory and free. 

In 1972 the school leaving was raised to 16. The Education and Skills Act 2008, when it came into force in the 2013 academic year, initially required participation in some form of education or training until the school year in which the child turned 17, followed by the age being raised to the young person's 18th birthday in 2015. This was referred to as raising the "participation age" to distinguish it from the school leaving age which remains at 16. Thus the UK is following the ISCED Level 3 (Upper) secondary education guideline.

The United Nations was strong in its commitment to education for all but fell into linguistic difficulty defining that right.

“Article I: Purposes and functions
1. The purpose of the Organization is to contribute to peace and security by promoting collaboration among the nations through education, science and culture in order to further universal respect for justice, for the rule of law and for the human rights and fundamental freedoms which are affirmed for the peoples of the world, without distinction of race, sex, language or religion, by the Charter of the United Nations.”

The Universal Declaration of Human Rights (1948) declared that elementary and fundamental education was a right to be enjoyed by all, but again could not define either elementary and fundamental education.
Article 26 :(1) Everyone has the right to education. Education shall be free, at least in the elementary and fundamental stages. Elementary education shall be compulsory. Technical and professional education shall be made generally available and higher education shall be equally accessible to all on the basis of merit.
It was assumed that elementary education was basic education, the entitlement for children- and fundamental education was a right for the working man, but for a lawyer the definition is neither qualitative (stating what education means) or quantitative saying when it starts and when it is completed. The term secondary is not defined or mentioned. Together this has enabled countries to terminate free, compulsory, basic education at 11 or only continue education past eleven to boys.

Article 28, of the Convention on the Rights of the Child (1989) stated that primary education should be free and compulsory while different forms of secondary education, including general and vocational education, should be available and accessible to every
child. Free education should be provided and financial assistance offered in case of need. 
In 1990, at Jomtien again tried to define the content basic education and how it should be delivered. ‘Basic education’ is defined as ‘action designed to meet ‘basic learning needs’. ‘primary schooling’ is considered as ‘the main delivery system of basic education’.
addressing the basic learning needs of all means: early childhood care and development opportunities; relevant, quality primary schooling or equivalent out-of-school education for children; and literacy, basic knowledge and life skills training for youth and adults.’

The assumption being made that basic knowledge and life skills training for youth was the function of secondary education. This was codified by the ISCED documents. The Dakar Framework for Action 2010 goal 2 states: Ensuring that by 2015 all children, particularly girls, children in difficult circumstances and those belonging to ethnic minorities, have access to and complete free and compulsory (primary in the sense basic) education of good quality. The Dakar Framework for Action 2010 goal 5 states: Eliminating gender disparities in primary and secondary education by 2005, and achieving gender equality in education by 2015, with a focus on ensuring girls’ full and equal access to and achievement in basic education of good quality. 

Malala Yousafzai, Nobel Peace Prize winner in a said in a 2017 interview that:
“My goal is to make sure every child, girl and boy, they get the opportunity to go to school." “It is their basic human right, so I will be working on that and I will never stop until I see the last child going to school.” 
UNESCO believes that in order to prepare young people for life and work in a rapidly changing world, secondary-level education systems need to be re-oriented to impart a broad repertoire of life-skills. These skills should include the key generic competencies, non occupation-specific practical capabilities, ICT, the ability to learn independently, to work in teams, entrepreneurship and civic responsibility.

They may be best instilled through a shared foundational learning period and by deferring the directing of students into academic and vocational streams for as long as possible, and then there should be flexibility to ensure the free movement of students between the streams depending on their aptitudes and inclinations. Accreditation in one stream should have equal recognition in the other as well as for access to higher education. This will equip young people with multiple skills so that they are prepared to enter and re-enter the workforce several times in their working lives, as wage employees or self-employed entrepreneurs, and to re-train themselves when their skills become obsolete.

It recognizes that there is no single model that will suit all countries, or even all communities in a given country. Secondary-level education policy should be under continuous review to keep in step with scientific and technological, economic and societal change.

Each country has developed the form of education most appropriate for them. There is an attempt to compare the effectiveness by using the results from the PISA that, each third year, assesses the scholastic performance on mathematics, science, and reading of a representative sample of 5000 fifteen year olds from each country.




</doc>
<doc id="29500" url="https://en.wikipedia.org/wiki?curid=29500" title="Serotonin syndrome">
Serotonin syndrome

Serotonin syndrome (SS) is a group of symptoms that may occur with the use of certain serotonergic medications or drugs. The degree of symptoms can range from mild to severe. Symptoms include high body temperature, agitation, increased reflexes, tremor, sweating, dilated pupils, and diarrhea. Body temperature can increase to greater than . Complications may include seizures and extensive muscle breakdown.
Serotonin syndrome is typically caused by the use of two or more serotonergic medications or drugs. This may include selective serotonin reuptake inhibitor (SSRI), serotonin norepinephrine reuptake inhibitor (SNRI), monoamine oxidase inhibitor (MAOI), tricyclic antidepressants (TCAs), amphetamines, pethidine (meperidine), tramadol, dextromethorphan, buspirone, L-tryptophan, 5-HTP, St. John's wort, triptans, ecstasy (MDMA), metoclopramide, ondansetron, or cocaine. It occurs in about 15% of SSRI overdoses. It is a predictable consequence of excess serotonin on the central nervous system (CNS). Onset of symptoms is typically within a day of the extra serotonin.
Diagnosis is based on a person's symptoms and history of medication use. Other conditions that can produce similar symptoms such as neuroleptic malignant syndrome, malignant hyperthermia, anticholinergic toxicity, heat stroke, and meningitis should be ruled out. No laboratory tests can confirm the diagnosis.
Initial treatment consists of discontinuing medications which may be contributing. In those who are agitated, benzodiazepines may be used. If this is not sufficient, a serotonin antagonist such as cyproheptadine may be used. In those with a high body temperature active cooling measures may be needed. The number of cases of serotonin syndrome that occur each year is unclear. With appropriate treatment the risk of death is less than one percent. The high-profile case of Libby Zion, who is generally accepted to have died from serotonin syndrome, resulted in changes to graduate medical education in New York State.

Symptom onset is usually rapid, often occurring within minutes of elevated serotonin levels. Serotonin syndrome encompasses a wide range of clinical findings. Mild symptoms may consist of increased heart rate, shivering, sweating, dilated pupils, myoclonus (intermittent jerking or twitching), as well as overresponsive reflexes. However, many of these symptoms may be side effects of the drug or drug interaction causing excessive levels of serotonin; not an effect of elevated serotonin itself. Tremor is a common side effect of MDMA's action on dopamine, whereas hyperreflexia is symptomatic of exposure to serotonin agonists. Moderate intoxication includes additional abnormalities such as hyperactive bowel sounds, high blood pressure and hyperthermia; a temperature as high as . The overactive reflexes and clonus in moderate cases may be greater in the lower limbs than in the upper limbs. Mental changes include hypervigilance or insomnia and agitation. Severe symptoms include severe increases in heart rate and blood pressure that may lead to shock. Temperature may rise to above in life-threatening cases. Other abnormalities include metabolic acidosis, rhabdomyolysis, seizures, kidney failure, and disseminated intravascular coagulation; these effects usually arising as a consequence of hyperthermia.

The symptoms are often described as a clinical triad of abnormalities:


A large number of medications and street drugs can cause serotonin syndrome when taken alone at high doses or in combination with other serotonergic drugs. The table below lists some of these drugs.
Many cases of serotonin toxicity occur in people who have ingested drug combinations that synergistically increase synaptic serotonin. It may also occur due to an overdose of a single serotonergic agent. The combination of MAOIs with precursors such as L-tryptophan or 5-HTP pose a particularly acute risk of life-threatening serotonin syndrome. The case of combination of MAOIs with tryptamine agonists (commonly known as ayahuasca) can present similar dangers as their combination with precursors, but this phenomenon has been described in general terms as the "cheese effect". Many MAOIs irreversibly inhibit monoamine oxidase. It can take at least four weeks for this enzyme to be replaced by the body in the instance of irreversible inhibitors. With respect to tricyclic antidepressants only clomipramine and imipramine have a risk of causing SS.

Many medications may have been incorrectly thought to cause serotonin syndrome. For example, some case reports have implicated atypical antipsychotics in serotonin syndrome, but it appears based on their pharmacology that they are unlikely to cause the syndrome. It has also been suggested that mirtazapine has no significant serotonergic effects, and is therefore not a dual action drug. Bupropion has also been suggested to cause serotonin syndrome, although as there is no evidence that it has any significant serotonergic activity, it is thought unlikely to produce the syndrome. In 2006 the United States Food and Drug Administration issued an alert suggesting that the combined use of SSRIs or SNRIs and triptan medications or sibutramine could potentially lead to severe cases of serotonin syndrome. This has been disputed by other researchers as none of the cases reported by the FDA met the Hunter criteria for serotonin syndrome. The condition has however occurred in surprising clinical situations, and because of phenotypic variations among individuals, it has been associated with unexpected drugs, including mirtazapine.

The relative risk and severity of serotonergic side effects and serotonin toxicity, with individual drugs and combinations, is complex. Serotonin syndrome has been reported in patients of all ages, including the elderly, children, and even newborn infants due to in utero exposure. The serotonergic toxicity of SSRIs increases with dose, but even in over-dose it is insufficient to cause fatalities from serotonin syndrome in healthy adults. Elevations of central nervous system serotonin will typically only reach potentially fatal levels when drugs with different mechanisms of action are mixed together. Various drugs, other than SSRIs, also have clinically significant potency as serotonin reuptake inhibitors, (e.g. tramadol, amphetamine, and MDMA) and are associated with severe cases of the syndrome.

Serotonin is a neurotransmitter involved in multiple biologically complex processes including aggression, pain, sleep, appetite, anxiety, depression, migraine, and vomiting. In humans the effects of excess serotonin were first noted in 1960 in patients receiving a monoamine oxidase inhibitor (MAOI) and tryptophan. The syndrome is caused by increased serotonin in the central nervous system. It was originally suspected that agonism of 5-HT receptors in central grey nuclei and the medulla was responsible for the development of the syndrome. Further study has determined that overstimulation of primarily the 5-HT receptors appears to contribute substantially to the condition. The 5-HT receptor may still contribute through a pharmacodynamic interaction in which increased synaptic concentrations of a serotonin agonist saturate all receptor subtypes. Additionally, noradrenergic CNS hyperactivity may play a role as CNS norepinephrine concentrations are increased in serotonin syndrome and levels appear to correlate with the clinical outcome. Other neurotransmitters may also play a role; NMDA receptor antagonists and GABA have been suggested as affecting the development of the syndrome. Serotonin toxicity is more pronounced following supra-therapeutic doses and overdoses, and they merge in a continuum with the toxic effects of overdose.

A postulated "spectrum concept" of serotonin toxicity emphasises the role that progressively increasing serotonin levels play in mediating the clinical picture as side effects merge into toxicity. The dose-effect relationship is the effects of progressive elevation of serotonin, either by raising the dose of one drug, or combining it with another serotonergic drug which may produce large elevations in serotonin levels. Some experts prefer the terms serotonin toxicity or serotonin toxidrome, to more accurately reflect that it is a form of poisoning.

There is no specific test for serotonin syndrome. Diagnosis is by symptom observation and investigation of the person's history. Several criteria have been proposed. The first evaluated criteria were introduced in 1991 by Harvey Sternbach. Researchers later developed the Hunter Toxicity Criteria Decision Rules, which have better sensitivity and specificity, 84% and 97%, respectively, when compared with the gold standard of diagnosis by a medical toxicologist. As of 2007, Sternbach's criteria were still the most commonly used.

The most important symptoms for diagnosing serotonin syndrome are tremor, extreme aggressiveness, akathisia, or clonus (spontaneous, inducible and ocular). Physical examination of the patient should include assessment of deep-tendon reflexes and muscle rigidity, the dryness of the mucosa of the mouth, the size and reactivity of the pupils, the intensity of bowel sounds, skin color, and the presence or absence of sweating. The patient's history also plays an important role in diagnosis, investigations should include inquiries about the use of prescription and over-the-counter drugs, illicit substances, and dietary supplements, as all these agents have been implicated in the development of serotonin syndrome. To fulfill the Hunter Criteria, a patient must have taken a serotonergic agent and meet one of the following conditions:

Serotonin toxicity has a characteristic picture which is generally hard to confuse with other medical conditions, but in some situations it may go unrecognized because it may be mistaken for a viral illness, anxiety disorders, neurological disorder, anticholinergic
poisoning, sympathomimetic toxicity, or worsening psychiatric condition. The condition most often confused with serotonin syndrome is neuroleptic malignant syndrome (NMS). The clinical features of neuroleptic malignant syndrome and serotonin syndrome share some features which can make differentiating them difficult. In both conditions, autonomic dysfunction and altered mental status develop. However, they are actually very different conditions with different underlying dysfunction (serotonin excess vs dopamine blockade). Both the time course and the clinical features of NMS differ significantly from those of serotonin toxicity. Serotonin toxicity has a rapid onset after the administration of a serotonergic drug and responds to serotonin blockade such as drugs like chlorpromazine and cyproheptadine. Dopamine receptor blockade (NMS) has a slow onset, typically evolves over several days after administration of a neuroleptic drug, and responds to dopamine agonists such as bromocriptine.

Differential diagnosis may become difficult in patients recently exposed to both serotonergic and neuroleptic drugs. Bradykinesia and extrapyramidal "lead pipe" rigidity are classically present in NMS, whereas serotonin syndrome causes hyperkinesia and clonus; these distinct symptoms can aid in differentiation.

Management is based primarily on stopping the usage of the precipitating drugs, the administration of serotonin antagonists such as cyproheptadine, and supportive care including the control of agitation, the control of autonomic instability, and the control of hyperthermia. Additionally, those who ingest large doses of serotonergic agents may benefit from gastrointestinal decontamination with activated charcoal if it can be administered within an hour of overdose. The intensity of therapy depends on the severity of symptoms. If the symptoms are mild, treatment may only consist of discontinuation of the offending medication or medications, offering supportive measures, giving benzodiazepines for myoclonus, and waiting for the symptoms to resolve. Moderate cases should have all thermal and cardiorespiratory abnormalities corrected and can benefit from serotonin antagonists. The serotonin antagonist cyproheptadine is the recommended initial therapy, although there have been no controlled trials demonstrating its efficacy for serotonin syndrome. Despite the absence of controlled trials, there are a number of case reports detailing apparent improvement after people have been administered cyproheptadine. Animal experiments also suggest a benefit from serotonin antagonists. Cyproheptadine is only available as tablets and therefore can only be administered orally or via a nasogastric tube; it is unlikely to be effective in people administered activated charcoal and has limited use in severe cases. Cyproheptadine can be stopped when the person is no longer experiencing symptoms and the half life of serotonergic medications already passed.

Additional pharmacological treatment for severe case includes administering atypical antipsychotic drugs with serotonin antagonist activity such as olanzapine. Critically ill people should receive the above therapies as well as sedation or neuromuscular paralysis. People who have autonomic instability such as low blood pressure require treatment with direct-acting sympathomimetics such as epinephrine, norepinephrine, or phenylephrine. Conversely, hypertension or tachycardia can be treated with short-acting antihypertensive drugs such as nitroprusside or esmolol; longer acting drugs such as propranolol should be avoided as they may lead to hypotension and shock. The cause of serotonin toxicity or accumulation is an important factor in determining the course of treatment. Serotonin is catabolized by monoamine oxidase A in the presence of oxygen, so if care is taken to prevent an unsafe spike in body temperature or metabolic acidosis, oxygenation will assist in dispatching the excess serotonin. The same principle applies to alcohol intoxication. In cases of serotonin syndrome caused by monoamine oxidase inhibitors oxygenation will not help to dispatch serotonin. In such instances, hydration is the main concern until the enzyme is regenerated.

Specific treatment for some symptoms may be required. One of the most important treatments is the control of agitation due to the extreme possibility of injury to the person themselves or caregivers, benzodiazepines should be administered at first sign of this. Physical restraints are not recommended for agitation or delirium as they may contribute to mortality by enforcing isometric muscle contractions that are associated with severe lactic acidosis and hyperthermia. If physical restraints are necessary for severe agitation they must be rapidly replaced with pharmacological sedation. The agitation can cause a large amount of muscle breakdown. This breakdown can cause severe damage to the kidneys through a condition called rhabdomyolysis.

Treatment for hyperthermia includes reducing muscle overactivity via sedation with a benzodiazepine. More severe cases may require muscular paralysis with vecuronium, intubation, and artificial ventilation. Suxamethonium is not recommended for muscular paralysis as it may increase the risk of cardiac dysrhythmia from hyperkalemia associated with rhabdomyolysis. Antipyretic agents are not recommended as the increase in body temperature is due to muscular activity, not a hypothalamic temperature set point abnormality.

Upon the discontinuation of serotonergic drugs, most cases of serotonin syndrome resolve within 24 hours, although in some cases delirium may persist for a number of days. Symptoms typically persist for a longer time frame in patients taking drugs which have a long elimination half-life, active metabolites, or a protracted duration of action.

Cases have reported muscle pain and weakness persisting for months, and antidepressant discontinuation may contribute to ongoing features. Following appropriate medical management, serotonin syndrome is generally associated with a favorable prognosis.

Epidemiological studies of serotonin syndrome are difficult as many physicians are unaware of the diagnosis or they may miss the syndrome due to its variable manifestations. In 1998 a survey conducted in England found that 85% of the general practitioners that had prescribed the antidepressant nefazodone were unaware of serotonin syndrome. The incidence may be increasing as a larger number of pro-serotonergic drugs (drugs which increase serotonin levels) are now being used in clinical practice. One postmarketing surveillance study identified an incidence of 0.4 cases per 1000 patient-months for patients who were taking nefazodone. Additionally, around 14 to 16 percent of persons who overdose on SSRIs are thought to develop serotonin syndrome.

The most widely recognized example of serotonin syndrome was the death of Libby Zion in 1984. Zion was a freshman at Bennington College at her death on March 5, 1984, at age 18. She died within 8 hours of her emergency admission to the New York Hospital Cornell Medical Center. She had an ongoing history of depression, and came to the Manhattan hospital on the evening of March 4, 1984, with a fever, agitation and "strange jerking motions" of her body. She also seemed disoriented at times. The emergency room physicians were unable to diagnose her condition definitively but admitted her for hydration and observation. Her death was caused by a combination of pethidine and phenelzine. A medical intern prescribed the pethidine. The case influenced graduate medical education and residency work hours. Limits were set on working hours for medical postgraduates, commonly referred to as interns or residents, in hospital training programs, and they also now require closer senior physician supervision.




</doc>
<doc id="29501" url="https://en.wikipedia.org/wiki?curid=29501" title="Sustainable development">
Sustainable development

Sustainable development is the organizing principle for meeting human development goals while simultaneously sustaining the ability of natural systems to provide the natural resources and ecosystem services based upon which the economy and society depend. The desired result is a state of society where living conditions and resources are used to continue to meet human needs without undermining the integrity and stability of the natural system. Sustainable development can be defined as development that meets the needs of the present without compromising the ability of future generations to meet their own needs.
While the modern concept of sustainable development is derived mostly from the 1987 Brundtland Report, it is also rooted in earlier ideas about sustainable forest management and twentieth-century environmental concerns. As the concept developed, it has shifted its focus more towards the economic development, social development and environmental protection for future generations. It has been suggested that "the term 'sustainability' should be viewed as humanity's target goal of human-ecosystem equilibrium, while 'sustainable development' refers to the holistic approach and temporal processes that lead us to the end point of sustainability". Modern economies are endeavoring to reconcile ambitious economic development and obligations of preserving natural resources and ecosystems, as the two are usually seen as of conflicting nature. Instead of holding climate change commitments and other sustainability measures as a remedy to economic development, turning and leveraging them into market opportunities will do greater good. The economic development brought by such organized principles and practices in an economy is called Managed Sustainable Development (MSD).

The concept of sustainable development has been, and still is, subject to criticism, including the question of what is to be sustained in sustainable development. It has been argued that there is no such thing as a sustainable use of a non-renewable resource, since any positive rate of exploitation will eventually lead to the exhaustion of earth's finite stock; this perspective renders the Industrial Revolution as a whole unsustainable. It has also been argued that the meaning of the concept has opportunistically been stretched from 'conservation management' to 'economic development', and that the Brundtland Report promoted nothing but a business as usual strategy for world development, with an ambiguous and insubstantial concept attached as a public relations slogan (see below). 

Sustainability can be defined as the practice of maintaining world processes of productivity indefinitely—natural or human-made—by replacing resources used with resources of equal or greater value without degrading or endangering natural biotic systems. Sustainable development ties together concern for the carrying capacity of natural systems with the social, political, and economic challenges faced by humanity. Sustainability Science is the study of the concepts of sustainable development and environmental science. There is an additional focus on the present generations' responsibility to regenerate, maintain and improve planetary resources for use by future generations.

Sustainable development has its roots in ideas about sustainable forest management which were developed in Europe during the 17th and 18th centuries. In response to a growing awareness of the depletion of timber resources in England, John Evelyn argued that "sowing and planting of trees had to be regarded as a national duty of every landowner, in order to stop the destructive over-exploitation of natural resources" in his 1662 essay "Sylva". In 1713 Hans Carl von Carlowitz, a senior mining administrator in the service of Elector Frederick Augustus I of Saxony published "Sylvicultura economics", a 400-page work on forestry. Building upon the ideas of Evelyn and French minister Jean-Baptiste Colbert, von Carlowitz developed the concept of managing forests for sustained yield. His work influenced others, including Alexander von Humboldt and Georg Ludwig Hartig, eventually leading to the development of a science of forestry. This, in turn, influenced people like Gifford Pinchot, first head of the US Forest Service, whose approach to forest management was driven by the idea of wise use of resources, and Aldo Leopold whose land ethic was influential in the development of the environmental movement in the 1960s.

Following the publication of Rachel Carson's "Silent Spring" in 1962, the developing environmental movement drew attention to the relationship between economic growth and development and environmental degradation. Kenneth E. Boulding in his influential 1966 essay "The Economics of the Coming Spaceship Earth" identified the need for the economic system to fit itself to the ecological system with its limited pools of resources. One of the first uses of the term sustainable in the contemporary sense was by the Club of Rome in 1972 in its classic report on the "Limits to Growth", written by a group of scientists led by Dennis and Donella Meadows of the Massachusetts Institute of Technology. Describing the desirable "state of global equilibrium", the authors wrote: "We are searching for a model output that represents a world system that is sustainable without sudden and uncontrolled collapse and capable of satisfying the basic material requirements of all of its people."

Following the Club of Rome report, an MIT research group prepared ten days of hearings on "Growth and Its Implication for the Future" (Roundtable Press, 1973) for the US Congress, the first hearings ever held on sustainable development. William Flynn Martin, David Dodson Gray, and Elizabeth Gray prepared the hearings under the Chairmanship of Congressman John Dingell.

In 1980 the International Union for the Conservation of Nature published a world conservation strategy that included one of the first references to sustainable development as a global priority and introduced the term "sustainable development". Two years later, the United Nations World Charter for Nature raised five principles of conservation by which human conduct affecting nature is to be guided and judged. In 1987 the United Nations World Commission on Environment and Development released the report "Our Common Future", commonly called the Brundtland Report. The report included what is now one of the most widely recognised definitions of sustainable development.

Since the Brundtland Report, the concept of sustainable development has developed beyond the initial intergenerational framework to focus more on the goal of "socially inclusive and environmentally sustainable economic growth". In 1992, the UN Conference on Environment and Development published the Earth Charter, which outlines the building of a just, sustainable, and peaceful global society in the 21st century. The action plan Agenda 21 for sustainable development identified information, integration, and participation as key building blocks to help countries achieve development that recognises these interdependent pillars. It emphasises that in sustainable development everyone is a user and provider of information. It stresses the need to change from old sector-centered ways of doing business to new approaches that involve cross-sectoral co-ordination and the integration of environmental and social concerns into all development processes. Furthermore, Agenda 21 emphasises that broad public participation in decision making is a fundamental prerequisite for achieving sustainable development.

Under the principles of the United Nations Charter the Millennium Declaration identified principles and treaties on sustainable development, including economic development, social development and environmental protection. Broadly defined, sustainable development is a systems approach to growth and development and to manage natural, produced, and social capital for the welfare of their own and future generations. The term sustainable development as used by the United Nations incorporates both issues associated with land development and broader issues of human development such as education, public health, and standard of living.

A 2013 study concluded that sustainability reporting should be reframed through the lens of four interconnected domains: ecology, economics, politics and culture.

Education for Sustainable
Development (ESD) is defined as education that encourages changes in knowledge, skills, values and attitudes to enable a more sustainable and equitable society. ESD aims to empower and equip current and future generations to meet the needs using a balanced and integrated approach to the economic, social and environmental dimensions of sustainable development.

The concept of ESD was born from the need for education to address the growing and changing environmental challenges facing the planet. In order to do this, education must change to provide the knowledge, skills, values and attitudes that empower learners to contribute to sustainable development. At the same time, education must be strengthened in all agendas, programmes, and activities that promote sustainable development. Sustainable development must be integrated into education and education must be integrated into sustainable development. ESD promotes the integration of these critical sustainability issues in local and global contexts into the curriculum to prepare learners to understand and respond to the changing world. ESD aims to produce learning outcomes that include core competencies such as critical and systematic thinking, collaborative decision-making, and taking responsibility for the present and future generations. Since traditional single-directional delivery of knowledge is not sufficient to inspire learners to take action as responsible citizens, ESD entails rethinking the learning environment, physical and virtual. The learning environment itself must adapt and apply a whole-institution approach to embed the philosophy of sustainable development. Building the capacity of educators and policy support at international, regional, national and local levels helps drive changes in learning institutions. Empowered youth and local communities interacting with education institutions become key actors in advancing sustainable development.

The launch of the UN Decade of Education for Sustainable Development (2005-2014) started a global movement to reorient education to address the challenges of sustainable development. Building on the achievement of the Decade, stated in the Aichi-Nagoya Declaration on ESD, UNESCO endorsed the Global Action Programme on ESD (GAP) in the 37th session of its General Conference. Acknowledged by UN General Assembly Resolution A/RES/69/211 and launched at the UNESCO World Conference on ESD in 2014, the GAP aims to scale-up actions and good practices. UNESCO has a major role, along with its partners, in bringing about key achievements to ensure the principles of ESD are promoted through formal, non-formal and informal education.

International recognition of ESD as the key enabler for sustainable development is growing steadily. The role of ESD was recognized in three major UN summits on sustainable development: the 1992 UN Conference on Environment and Development (UNCED) in Rio de Janeiro, Brazil; the 2002 World Summit on Sustainable Development (WSSD) in Johannesburg, South Africa; and the 2012 UN Conference on Sustainable Development (UNCSD) in Rio de Janeiro. Other key global agreements such as the Paris Agreement (Article 12) also recognize the importance of ESD. Today, ESD is arguably at the heart of the 2030 Agenda for Sustainable Development and its 17 Sustainable Development Goals (SDGs) (United Nations, 2015). The SDGs recognize that all countries must stimulate action in the following key areas - people, planet, prosperity, peace and partnership - in order to tackle the global challenges that are crucial for the survival of humanity. ESD is explicitly mentioned in Target 4.7 of SDG4, which aims to ensure that all learners acquire the knowledge and skills needed to promote sustainable development and is understood as an important means to achieve all the other 16 SDGs (UNESCO, 2017).

Sustainable development can be thought of in terms of three spheres, dimensions, domains or pillars, i.e. the environment, the economy and society. The three-sphere framework was initially proposed by the economist Rene Passet in 1979. It has also been worded as "economic, environmental and social" or "ecology, economy and equity". This has been expanded by some authors to include a fourth pillar of culture, institutions or governance, or alternatively reconfigured as four domains of the social - ecology, economics, politics and culture, thus bringing economics back inside the social, and treating ecology as the intersection of the social and the natural.

The ecological stability of human settlements is part of the relationship between humans and their natural, social and built environments. Also termed human ecology, this broadens the focus of sustainable development to include the domain of human health. Fundamental human needs such as the availability and quality of air, water, food and shelter are also the ecological foundations for sustainable development; addressing public health risk through investments in ecosystem services can be a powerful and transformative force for sustainable development which, in this sense, extends to all species.

Environmental sustainability concerns the natural environment and how it endures and remains diverse and productive. Since natural resources are derived from the environment, the state of air, water, and the climate are of particular concern. The IPCC Fifth Assessment Report outlines current knowledge about scientific, technical and socio-economic information concerning climate change, and lists options for adaptation and mitigation. Environmental sustainability requires society to design activities to meet human needs while preserving the life support systems of the planet. This, for example, entails using water sustainably, utilizing renewable energy, and sustainable material supplies (e.g. harvesting wood from forests at a rate that maintains the biomass and biodiversity).

An unsustainable situation occurs when natural capital (the sum total of nature's resources) is used up faster than it can be replenished. Sustainability requires that human activity only uses nature's resources at a rate at which they can be replenished naturally. Inherently the concept of sustainable development is intertwined with the concept of carrying capacity. Theoretically, the long-term result of environmental degradation is the inability to sustain human life. Such degradation on a global scale should imply an increase in human death rate until population falls to what the degraded environment can support. If the degradation continues beyond a certain tipping point or critical threshold it would lead to eventual extinction for humanity.

Integral elements for a sustainable development are research and innovation activities. A telling example is the European environmental research and innovation policy, which aims at defining and implementing a transformative agenda to greening the economy and the society as a whole so to achieve a truly sustainable development. Research and innovation in Europe is financially supported by the programme Horizon 2020, which is also open to participation worldwide. A promising direction towards sustainable development is to design systems that are flexible and reversible.

Pollution of the public resources is really not a different action, it just is a reverse tragedy of the commons, in that instead of taking something out, something is put into the commons. When the costs of polluting the commons are not calculated into the cost of the items consumed, then it becomes only natural to pollute, as the cost of pollution is external to the cost of the goods produced and the cost of cleaning the waste before it is discharged exceeds the cost of releasing the waste directly into the commons. So, the only way to solve this problem is by protecting the ecology of the commons by making it, through taxes or fines, more costly to release the waste directly into the commons than would be the cost of cleaning the waste before discharge.

So, one can try to appeal to the ethics of the situation by doing the right thing as an individual, but in the absence of any direct consequences, the individual will tend to do what is best for the person and not what is best for the common good of the public. Once again, this issue needs to be addressed. Because, left unaddressed, the development of the commonly owned property will become impossible to achieve in a sustainable way. So, this topic is central to the understanding of creating a sustainable situation from the management of the public resources that are used for personal use.

Sustainable agriculture consists of environment friendly methods of farming that allow the production of crops or livestock without damage to human or natural systems. It involves preventing adverse effects to soil, water, biodiversity, surrounding or downstream resources—as well as to those working or living on the farm or in neighbouring areas. The concept of sustainable agriculture extends intergenerationally, passing on a conserved or improved natural resource, biotic, and economic base rather than one which has been depleted or polluted. Elements of sustainable agriculture include permaculture, agroforestry, mixed farming, multiple cropping, and crop rotation. It involves agricultural methods that do not undermine the environment, smart farming technologies that enhance a quality environment for humans to thrive and reclaiming and transforming deserts into farmlands(Herman Daly, 2017). 

Numerous sustainability standards and certification systems exist, including organic certification, Rainforest Alliance, Fair Trade, UTZ Certified, Bird Friendly, and the Common Code for the Coffee Community (4C).

It has been suggested that because of rural poverty and overexploitation, environmental resources should be treated as important economic assets, called natural capital. Economic development has traditionally required a growth in the gross domestic product. This model of unlimited personal and GDP growth may be over. Sustainable development may involve improvements in the quality of life for many but may necessitate a decrease in resource consumption.
According to ecological economist , ecological economics is defined by its focus on nature, justice, and time. Issues of intergenerational equity, irreversibility of environmental change, uncertainty of long-term outcomes, and sustainable development guide ecological economic analysis and valuation.

As early as the 1970s, the concept of sustainability was used to describe an economy "in equilibrium with basic ecological support systems". Scientists in many fields have highlighted "The Limits to Growth", and economists have presented alternatives, for example a 'steady-state economy', to address concerns over the impacts of expanding human development on the planet. In 1987 the economist Edward Barbier published the study "The Concept of Sustainable Economic Development", where he recognised that goals of environmental conservation and economic development are not conflicting and can be reinforcing each other.

A World Bank study from 1999 concluded that based on the theory of genuine savings, policymakers have many possible interventions to increase sustainability, in macroeconomics or purely environmental. Several studies have noted that efficient policies for renewable energy and pollution are compatible with increasing human welfare, eventually reaching a golden-rule steady state.

The study, "Interpreting Sustainability in Economic Terms", found three pillars of sustainable development, interlinkage, intergenerational equity, and dynamic efficiency.

But Gilbert Rist points out that the World Bank has twisted the notion of sustainable development to prove that economic development need not be deterred in the interest of preserving the ecosystem. He writes: "From this angle, 'sustainable development' looks like a cover-up operation. ... The thing that is meant to be sustained is really 'development', not the tolerance capacity of the ecosystem or of human societies."

The World Bank, a leading producer of environmental knowledge, continues to advocate the win-win prospects for economic growth and ecological stability even as its economists express their doubts. Herman Daly, an economist for the Bank from 1988 to 1994, writes:
When authors of "WDR" '92 [the highly influential 1992 "World Development Report" that featured the environment] were drafting the report, they called me asking for examples of "win-win" strategies in my work. What could I say? None exists in that pure form; there are trade-offs, not "win-wins." But they want to see a world of "win-wins" based on articles of faith, not fact. I wanted to contribute because "WDR"s are important in the Bank, [because] task managers read [them] to find philosophical justification for their latest round of projects. But they did not want to hear about how things really are, or what I find in my work...

A meta review in 2002 looked at environmental and economic valuations and found a lack of "sustainability policies". A study in 2004 asked if we consume too much. A study concluded in 2007 that knowledge, manufactured and human capital (health and education) has not compensated for the degradation of natural capital in many parts of the world. It has been suggested that intergenerational equity can be incorporated into a sustainable development and decision making, as has become common in economic valuations of climate economics. A meta review in 2009 identified conditions for a strong case to act on climate change, and called for more work to fully account of the relevant economics and how it affects human welfare. According to free-market environmentalist John Baden "the improvement of environment quality depends on the market economy and the existence of legitimate and protected property rights". They enable the effective practice of personal responsibility and the development of mechanisms to protect the environment. The State can in this context "create conditions which encourage the people to save the environment".

Misum, Mistra Center for Sustainable Markets, based at Stockholm School of Economics, aims to provide policy research and advice to Swedish and international actors on Sustainable Markets. Misum is a cross-disciplinary and multi-stakeholder knowledge center dedicated to sustainability and sustainable markets and contains three research platforms: Sustainability in Financial Markets (Mistra Financial Systems), Sustainability in Production and Consumption and Sustainable Socio-Economic Development.

The total environment includes not just the biosphere of earth, air, and water, but also human interactions with these things, with nature, and what humans have created as their surroundings.

As countries around the world continue to advance economically, they put a strain on the ability of the natural environment to absorb the high level of pollutants that are created as a part of this economic growth. Therefore, solutions need to be found so that the economies of the world can continue to grow, but not at the expense of the public good. In the world of economics the amount of environmental quality must be considered as limited in supply and therefore is treated as a scarce resource. This is a resource to be protected. One common way to analyze possible outcomes of policy decisions on the scarce resource is to do a cost-benefit analysis. This type of analysis contrasts different options of resource allocation and, based on an evaluation of the expected courses of action and the consequences of these actions, the optimal way to do so in the light of different policy goals can be elicited.

Benefit-cost analysis basically can look at several ways of solving a problem and then assigning the best route for a solution, based on the set of consequences that would result from the further development of the individual courses of action, and then choosing the course of action that results in the least amount of damage to the expected outcome for the environmental quality that remains after that development or process takes place. Further complicating this analysis are the interrelationships of the various parts of the environment that might be impacted by the chosen course of action. Sometimes it is almost impossible to predict the various outcomes of a course of action, due to the unexpected consequences and the amount of unknowns that are not accounted for in the benefit-cost analysis.

Sustainable energy is clean and can be used over a long period of time. Unlike fossil fuels and biofuels that provide the bulk of the worlds energy, renewable energy sources like hydroelectric, solar and wind energy produce far less pollution. Solar energy is commonly used on public parking meters, street lights and the roof of buildings. Wind power has expanded quickly, its share of worldwide electricity usage at the end of 2014 was 3.1%. Most of California's fossil fuel infrastructures are sited in or near low-income communities, and have traditionally suffered the most from California's fossil fuel energy system. These communities are historically left out during the decision-making process, and often end up with dirty power plants and other dirty energy projects that poison the air and harm the area. These toxicants are major contributors to health problems in the communities. As renewable energy becomes more common, fossil fuel infrastructures are replaced by renewables, providing better social equity to these communities.
Overall, and in the long run, sustainable development in the field of energy is also deemed to contribute to economic sustainability and national security of communities, thus being increasingly encouraged through investment policies.

Main article: Green manufacturing and Distributed manufacturing

One of the core concepts in sustainable development is that technology can be used to assist people to meet their developmental needs. Technology to meet these sustainable development needs is often referred to as appropriate technology, which is an ideological movement (and its manifestations) originally articulated as intermediate technology by the economist E. F. Schumacher in his influential work "Small Is Beautiful" and now covers a wide range of technologies. Both Schumacher and many modern-day proponents of appropriate technology also emphasise the technology as people-centered. Today appropriate technology is often developed using open source principles, which have led to Open-Source Appropriate Technology (OSAT) and thus many of the plans of the technology can be freely found on the Internet. OSAT has been proposed as a new model of enabling innovation for sustainable development.

Transportation is a large contributor to greenhouse gas emissions. It is said that one-third of all gases produced are due to transportation. Motorized transport also releases exhaust fumes that contain particulate matter which is hazardous to human health and a contributor to climate change.

Sustainable transport has many social and economic benefits that can accelerate local sustainable development. According to a series of reports by the Low Emission Development Strategies Global Partnership (LEDS GP), sustainable transport can help create jobs, improve commuter safety through investment in bicycle lanes and pedestrian pathways, make access to employment and social opportunities more affordable and efficient. It also offers a practical opportunity to save people's time and household income as well as government budgets, making investment in sustainable transport a 'win-win' opportunity.

Some Western countries are making transportation more sustainable in both long-term and short-term implementations. An example is the modification in available transportation in Freiburg, Germany. The city has implemented extensive methods of public transportation, cycling, and walking, along with large areas where cars are not allowed.

Since many Western countries are highly automobile-oriented, the main transit that people use is personal vehicles. About 80% of their travel involves cars. Therefore, California, is one of the highest greenhouse gases emitters in the United States. The federal government has to come up with some plans to reduce the total number of vehicle trips in order to lower greenhouse gases emission. Such as:


Other states and nations have built efforts to translate knowledge in behavioral economics into evidence-based sustainable transportation policies.

The most broadly accepted criterion for corporate sustainability constitutes a firm's efficient use of natural capital. This eco-efficiency is usually calculated as the economic value added by a firm in relation to its aggregated ecological impact. This idea has been popularised by the World Business Council for Sustainable Development (WBCSD) under the following definition: "Eco-efficiency is achieved by the delivery of competitively priced goods and services that satisfy human needs and bring quality of life, while progressively reducing ecological impacts and resource intensity throughout the life-cycle to a level at least in line with the earth's carrying capacity" (DeSimone and Popoff, 1997: 47).

Similar to the eco-efficiency concept but so far less explored is the second criterion for corporate sustainability. Socio-efficiency describes the relation between a firm's value added and its social impact. Whereas, it can be assumed that most corporate impacts on the environment are negative (apart from rare exceptions such as the planting of trees) this is not true for social impacts. These can be either positive (e.g. corporate giving, creation of employment) or negative (e.g. work accidents, mobbing of employees, human rights abuses). Depending on the type of impact socio-efficiency thus either tries to minimise negative social impacts (i.e. accidents per value added) or maximise positive social impacts (i.e. donations per value added) in relation to the value added.

Both eco-efficiency and socio-efficiency are concerned primarily with increasing economic sustainability. In this process they instrumentalise both natural and social capital aiming to benefit from win-win situations. However, as Dyllick and Hockerts point out the business case alone will not be sufficient to realise sustainable development. They point towards eco-effectiveness, socio-effectiveness, sufficiency, and eco-equity as four criteria that need to be met if sustainable development is to be reached.

CASI Global, New York "CSR & Sustainability together lead to sustainable development. CSR as in corporate social responsibility is not what you do with your profits, but is the way you make profits. This means CSR is a part of every department of the company value chain and not a part of HR / independent department. Sustainability as in effects towards Human resources, Environment and Ecology has to be measured within each department of the company." CASI Global

At the present time, sustainable development can reduce poverty. Sustainable development reduces poverty through financial (among other things, a balanced budget), environmental (living conditions), and social (including equality of income) means.

In sustainable architecture the recent movements of New Urbanism and New Classical architecture promote a sustainable approach towards construction, that appreciates and develops smart growth, architectural tradition and classical design. This in contrast to modernist and International Style architecture, as well as opposing to solitary housing estates and suburban sprawl, with long commuting distances and large ecological footprints. Both trends started in the 1980s. (Sustainable architecture is predominantly relevant to the economics domain while architectural landscaping pertains more to the ecological domain.)

A study concluded that social indicators and, therefore, sustainable development indicators, are scientific constructs whose principal objective is to inform public policy-making. The International Institute for Sustainable Development has similarly developed a political policy framework, linked to a sustainability index for establishing measurable entities and metrics. The framework consists of six core areas:

The United Nations Global Compact Cities Programme has defined sustainable political development in a way that broadens the usual definition beyond states and governance. The political is defined as the domain of practices and meanings associated with basic issues of social power as they pertain to the organisation, authorisation, legitimation and regulation of a social life held in common. This definition is in accord with the view that political change is important for responding to economic, ecological and cultural challenges. It also means that the politics of economic change can be addressed. They have listed seven subdomains of the domain of politics:


This accords with the Brundtland Commission emphasis on development that is guided by human rights principles (see above).

Working with a different emphasis, some researchers and institutions have pointed out that a fourth dimension should be added to the dimensions of sustainable development, since the triple-bottom-line dimensions of economic, environmental and social do not seem to be enough to reflect the complexity of contemporary society. In this context, the Agenda 21 for culture and the United Cities and Local Governments (UCLG) Executive Bureau lead the preparation of the policy statement "Culture: Fourth Pillar of Sustainable Development", passed on 17 November 2010, in the framework of the World Summit of Local and Regional Leaders – 3rd World Congress of UCLG, held in Mexico City. This document inaugurates a new perspective and points to the relation between culture and sustainable development through a dual approach: developing a solid cultural policy and advocating a cultural dimension in all public policies. The Circles of Sustainability approach distinguishes the four domains of economic, ecological, political and cultural sustainability.

Other organizations have also supported the idea of a fourth domain of sustainable development. The Network of Excellence "Sustainable Development in a Diverse World", sponsored by the European Union, integrates multidisciplinary capacities and interprets cultural diversity as a key element of a new strategy for sustainable development. The Fourth Pillar of Sustainable Development Theory has been referenced by executive director of IMI Institute at UNESCO Vito Di Bari in his manifesto of art and architectural movement Neo-Futurism, whose name was inspired by the 1987 United Nations’ report Our Common Future. The Circles of Sustainability approach used by Metropolis defines the (fourth) cultural domain as practices, discourses, and material expressions, which, over time, express continuities and discontinuities of social meaning.

Recently, human-centered design and cultural collaboration have been popular frameworks for sustainable development in marginalized communities. These frameworks involve open dialogue which entails sharing, debating, and discussing, as well as holistic evaluation of the site of development. Especially when working on sustainable development in marginalized communities, cultural emphasis is a crucial factor in project decisions, since it largely affects aspects of their lives and traditions. Collaborators utilize articulation theory in co-designing. This allows for them to understand each other's thought process and their comprehension of the sustainable projects. By using the method of co-design, the beneficiaries' holistic needs are being considered. Final decisions and implementations are made with respect to sociocultural and ecological factors.

The user-oriented framework relies heavily on user participation and user feedback in the planning process. Users are able to provide new perspective and ideas, which can be considered in a new round of improvements and changes. It is said that increased user participation in the design process can garner a more comprehensive understanding of the design issues, due to more contextual and emotional transparency between researcher and participant. A key element of human centered design is applied ethnography, which was a research method adopted from cultural anthropology. This research method requires researchers to be fully immersed in the observation so that implicit details are also recorded.

Many communities express environmental concerns, so life cycle analysis is often conducted when assessing the sustainability of a product or prototype. The assessment is done in stages with meticulous cycles of planning, design, implementation, and evaluation. The decision to choose materials is heavily weighted on its longevity, renewability, and efficiency. These factors ensure that researchers are conscious of community values that align with positive environmental, social, and economic impacts.

The United Nations Conference on Sustainable Development (UNCSD; also known as Rio 2012) was the third international conference on sustainable development, which aimed at reconciling the economic and environmental goals of the global community. An outcome of this conference was the development of the Sustainable Development Goals that aim to promote sustainable progress and eliminate inequalities around the world. However, few nations met the World Wide Fund for Nature's definition of sustainable development criteria established in 2006. Although some nations are more developed than others, all nations are constantly developing because each nation struggles with perpetuating disparities, inequalities and unequal access to fundamental rights and freedoms.

In 2007 a report for the U.S. Environmental Protection Agency stated: "While much discussion and effort has gone into sustainability indicators, none of the resulting systems clearly tells us whether our society is sustainable. At best, they can tell us that we are heading in the wrong direction, or that our current activities are not sustainable. More often, they simply draw our attention to the existence of problems, doing little to tell us the origin of those problems and nothing to tell us how to solve them." Nevertheless, a majority of authors assume that a set of well defined and harmonised indicators is the only way to make sustainability tangible. Those indicators are expected to be identified and adjusted through empirical observations (trial and error).

The most common critiques are related to issues like data quality, comparability, objective function and the necessary resources. However a more general criticism is coming from the project management community: How can a sustainable development be achieved at global level if we cannot monitor it in any single project?

The Cuban-born researcher and entrepreneur Sonia Bueno suggests an alternative approach that is based upon the integral, long-term cost-benefit relationship as a measure and monitoring tool for the sustainability of every project, activity or enterprise. Furthermore, this concept aims to be a practical guideline towards sustainable development following the principle of conservation and increment of value rather than restricting the consumption of resources.

Reasonable qualifications of sustainability are seen U.S. Green Building Council's (USGBC) Leadership in Energy and Environmental Design (LEED). This design incorporates some ecological, economic, and social elements. The goals presented by LEED design goals are sustainable sites, water efficiency, energy consumption and atmospheric emission reduction, material and resources efficiency, and indoor environmental quality. Although amount of structures for sustainability development is many, these qualification has become a standard for sustainable building.

Recent research efforts created also the SDEWES Index to benchmark the performance of cities across aspects that are related to energy, water and environment systems. The SDEWES Index consists of 7 dimensions, 35 indicators, and close to 20 sub-indicators. It is currently applied to 58 cities.

The sustainable development debate is based on the assumption that societies need to manage three types of capital (economic, social, and natural), which may be non-substitutable and whose consumption might be irreversible. Leading ecological economist and steady-state theorist Herman Daly, for example, points to the fact that natural capital can not necessarily be substituted by economic capital. While it is possible that we can find ways to replace some natural resources, it is much more unlikely that they will ever be able to replace eco-system services, such as the protection provided by the ozone layer, or the climate stabilizing function of the Amazonian forest. In fact natural capital, social capital and economic capital are often complementarities. A further obstacle to substitutability lies also in the multi-functionality of many natural resources. Forests, for example, not only provide the raw material for paper (which can be substituted quite easily), but they also maintain biodiversity, regulate water flow, and absorb CO2.

Another problem of natural and social capital deterioration lies in their partial irreversibility. The loss of biodiversity, for example, is often definitive. The same can be true for cultural diversity. For example, with globalisation advancing quickly the number of indigenous languages is dropping at alarming rates. Moreover, the depletion of natural and social capital may have non-linear consequences. Consumption of natural and social capital may have no observable impact until a certain threshold is reached. A lake can, for example, absorb nutrients for a long time while actually increasing its productivity. However, once a certain level of algae is reached lack of oxygen causes the lake's ecosystem to break down suddenly.

If the degradation of natural and social capital has such important consequence the question arises why action is not taken more systematically to alleviate it. Cohen and Winn point to four types of market failure as possible explanations: First, while the benefits of natural or social capital depletion can usually be privatised, the costs are often externalised (i.e. they are borne not by the party responsible but by society in general). Second, natural capital is often undervalued by society since we are not fully aware of the real cost of the depletion of natural capital. Information asymmetry is a third reason—often the link between cause and effect is obscured, making it difficult for actors to make informed choices. Cohen and Winn close with the realization that contrary to economic theory many firms are not perfect optimisers. They postulate that firms often do not optimise resource allocation because they are caught in a "business as usual" mentality.

"Main page: Education for sustainable development"

Education must be revisited in light of a renewed vision of sustainable human and social development that is both equitable and viable. This vision of sustainability must take into consideration the social, environmental and economic dimensions of human development and the various ways in which these relate to education: ‘An empowering education is one that builds the human resources we need to be productive, to continue to learn, to solve problems, to be creative, and to live together and with nature in peace and harmony. When nations ensure that such an education is accessible to all throughout their lives, a quiet revolution is set in motion: education becomes the engine of sustainable development and the key to a better world.’

It has been argued that since the 1960s, the concept of sustainable development has changed from "conservation management" to "economic development", whereby the original meaning of the concept has been stretched somewhat.

In the 1960s, the international community realised that many African countries needed national plans to safeguard wildlife habitats, and that rural areas had to confront the limits imposed by soil, climate and water availability. This was a strategy of conservation management. In the 1970s, however, the focus shifted to the broader issues of the provisioning of basic human needs, community participation as well as appropriate technology use throughout the developing countries (and not just in Africa). This was a strategy of economic development, and the strategy was carried even further by the Brundtland Commission's report on "Our Common Future" when the issues went from regional to international in scope and application. In effect, the conservationists were crowded out and superseded by the developers.

But shifting the focus of sustainable development from conservation to development has had the imperceptible effect of stretching the original forest management term of sustainable yield from the use of renewable resources only (like forestry), to now also accounting for the use of non-renewable resources (like minerals). This stretching of the term has been questioned. Thus, environmental economist Kerry Turner has argued that literally, there can be no such thing as overall "sustainable development" in an industrialised world economy that remains heavily dependent on the extraction of earth's finite stock of exhaustible mineral resources: "It makes no sense to talk about the sustainable use of a non-renewable resource (even with substantial recycling effort and reduction in use rates). Any positive rate of exploitation will eventually lead to exhaustion of the finite stock."

In effect, it has been argued that the industrial revolution as a whole is unsustainable. 

One critic has argued that the Brundtland Commission promoted nothing but a business as usual strategy for world development, with the ambiguous and insubstantial concept of "sustainable development" attached as a public relations slogan: The report on "Our Common Future" was largely the result of a political bargaining process involving many special interest groups, all put together to create a common appeal of political acceptability across borders. After World War II, the notion of "development" had been established in the West to imply the projection of the American model of society onto the rest of the world. In the 1970s and 1980s, this notion was broadened somewhat to also imply human rights, basic human needs and finally, ecological issues. The emphasis of the report was on helping poor nations out of poverty and meeting the basic needs of their growing populations—as usual. This issue demanded more economic growth, also in the rich countries, who would then import more goods from the poor countries to help them out—as usual. When the discussion switched to , the obvious dilemma was left aside by calling for economic growth with improved resource efficiency, or what was termed "a change in the "quality" of growth". However, most countries in the West had experienced such improved resource efficiency since the early-20th century already and as usual; only, this improvement had been more than offset by continuing industrial expansion, to the effect that world resource consumption was now higher than ever before—and these two historical trends were completely ignored in the report. Taken together, the policy of perpetual economic growth for the entire planet remained virtually intact. Since the publication of the report, the ambiguous and insubstantial slogan of "sustainable development" has marched on worldwide.




</doc>
<doc id="29507" url="https://en.wikipedia.org/wiki?curid=29507" title="Scientific American">
Scientific American

Scientific American (informally abbreviated SciAm or sometimes SA) is an American popular science magazine. Many famous scientists, including Albert Einstein, have contributed articles to it. It is the oldest continuously published monthly magazine in the United States (though it only became monthly in 1921).

"Scientific American" was founded by inventor and publisher Rufus M. Porter in 1845 as a four-page weekly newspaper. Throughout its early years, much emphasis was placed on reports of what was going on at the U.S. Patent Office. It also reported on a broad range of inventions including perpetual motion machines, an 1860 device for buoying vessels by Abraham Lincoln, and the universal joint which now can be found in nearly every automobile manufactured. Current issues include a "this date in history" section, featuring excerpts from articles originally published 50, 100, and 150 years earlier. Topics include humorous incidents, wrong-headed theories, and noteworthy advances in the history of science and technology.

Porter sold the publication to Alfred Ely Beach and Orson Desaix Munn a mere ten months after founding it. Until 1948, it remained owned by Munn & Company. Under Munn's grandson, Orson Desaix Munn III, it had evolved into something of a "workbench" publication, similar to the twentieth-century incarnation of "Popular Science".

In the years after World War II, the magazine fell into decline. In 1948, three partners who were planning on starting a new popular science magazine, to be called "The Sciences", purchased the assets of the old "Scientific American" instead and put its name on the designs they had created for their new magazine. Thus the partnerspublisher Gerard Piel, editor Dennis Flanagan, and general manager Donald H. Miller, Jr.essentially created a new magazine. Miller retired in 1979, Flanagan and Piel in 1984, when Gerard Piel's son Jonathan became president and editor; circulation had grown fifteen-fold since 1948. In 1986, it was sold to the Holtzbrinck group of Germany, which has owned it since.

In the fall of 2008, "Scientific American" was put under the control of Nature Publishing Group, a division of Holtzbrinck.

Donald Miller died in December 1998, Gerard Piel in September 2004 and Dennis Flanagan in January 2005. Mariette DiChristina became editor-in-chief after John Rennie stepped down in June 2009, and stepped down herself in September 2019.

"Scientific American" published its first foreign edition in 1890, the Spanish-language "La America Cientifica". Publication was suspended in 1905, and another 63 years would pass before another foreign-language edition appeared: In 1968, an Italian edition, "Le Scienze", was launched, and a Japanese edition, ', followed three years later. A new Spanish edition, "Investigación y Ciencia" was launched in Spain in 1976, followed by a French edition, ', in France in 1977, and a German edition, ', in Germany in 1978. A Russian edition "V Mire Nauki" was launched in the Soviet Union in 1983, and continues in the present-day Russian Federation. "Kexue" (科学, "Science" in Chinese), a simplified Chinese edition launched in 1979, was the first Western magazine published in the People's Republic of China. Founded in Chongqing, the simplified Chinese magazine was transferred to Beijing in 2001. Later in 2005, a newer edition, "Global Science" (环球科学), was published instead of "Kexue", which shut down due to financial problems. A traditional Chinese edition, known as ', was introduced to Taiwan in 2002. The Hungarian edition "Tudomány" existed between 1984 and 1992. In 1986, an Arabic edition, "", was published. In 2002, a Portuguese edition was launched in Brazil.

Today, "Scientific American" publishes 18 foreign-language editions around the globe: Arabic, Brazilian Portuguese, Simplified Chinese, Traditional Chinese, Czech, Dutch, French, German, Greek, Hebrew, Italian, Japanese, Korean, Lithuanian (discontinued after 15 issues), Polish, Romanian, Russian, and Spanish.

From 1902 to 1911, "Scientific American" supervised the publication of the "Encyclopedia Americana", which during some of that period was known as "The Americana".

It originally styled itself "The Advocate of Industry and Enterprise" and "Journal of Mechanical and other Improvements". On the front page of the first issue was the engraving of "Improved Rail-Road Cars". The masthead had a commentary as follows:

The commentary under the illustration gives the flavor of its style at the time:
Also in the first issue is commentary on Signor Muzio Muzzi's proposed device for aerial navigation.



The Scientific American 50 award was started in 2002 to recognize contributions to science and technology during the magazine's previous year. The magazine's 50 awards cover many categories including agriculture, communications, defence, environment, and medical diagnostics. The complete list of each year's winners appear in the December issue of the magazine, as well as on the magazine's web site.

In March 1996, "Scientific American" launched its own website that includes articles from current and past issues, online-only features, daily news, weird science, special reports, trivia, "Scidoku" and more. The website introduced a paywall in April 2019, with readers able to view a few articles for free each month.

Notable features have included:

From 1990 to 2005 "Scientific American" produced a television program on PBS called "Scientific American Frontiers" with hosts Woodie Flowers and Alan Alda.

From 1983 to 1997, "Scientific American" has produced an encyclopedia set of volumes from their publishing division, the Scientific American Library. These books were not sold in retail stores, but as a Book of the Month Club selection priced from $24.95 to $32.95. Topics covered dozens of areas of scientific knowledge and included in-depth essays on: The Animal Mind; Atmosphere, Climate, and Change; Beyond the Third Dimension; Cosmic Clouds; Cycles of Life • Civilization and the Biosphere; The Discovery Of Subatomic Particles; Diversity and the Tropical Rain Forest; Earthquakes and Geological Discovery; Exploring Planetary Worlds; Gravity's Fatal Attraction; Fire; Fossils And The History Of Life; From Quarks to the Cosmos; A Guided Tour Of The Living Cell; Human Diversity; Perception; The Solar System; Sun and Earth; The Science of Words (Linguistics); The Science Of Musical Sound; The Second Law (of Thermodynamics); Stars; Supercomputing and the Transformation of Science.

Scientific American launched a publishing imprint in 2010 in partnership with Farrar, Straus and Giroux.


In April 1950, the U.S. Atomic Energy Commission ordered "Scientific American" to cease publication of an issue containing an article by Hans Bethe that appeared to reveal classified information about the thermonuclear hydrogen bomb. Subsequent review of the material determined that the AEC had overreacted. The incident was important for the "new" "Scientific American"'s history, as the AEC's decision to burn 3000 copies of an early press-run of the magazine containing the offending material appeared to be "book burning in a free society" when publisher Gerard Piel leaked the incident to the press.

In its January 2002 issue, "Scientific American" published a series of criticisms of the Bjørn Lomborg book "The Skeptical Environmentalist". Cato Institute fellow Patrick J. Michaels said the attacks came because the book "threatens billions of taxpayer dollars that go into the global change kitty every year." Journalist Ronald Bailey called the criticism "disturbing" and "dishonest", writing, "The subhead of the review section, 'Science defends itself against "The Skeptical Environmentalist",' gives the show away: Religious and political views need to defend themselves against criticism, but science is supposed to be a process for determining the facts."

The May 2007 issue featured a column by Michael Shermer calling for a United States pullout from the Iraq War. In response, "Wall Street Journal" online columnist James Taranto jokingly called "Scientific American" "a liberal political magazine".

The publisher was criticized in 2009 when it notified collegiate libraries that yearly subscription prices for the magazine would increase by nearly 500% for print and 50% for online access to $1500 yearly.

An editorial in the September 2016 issue of Scientific American attacked U.S. presidential candidate Donald Trump for alleged "anti-science" attitudes and rhetoric. This marked the first time that the publication forayed into commenting on U.S. presidential politics.



In 2013, Danielle N. Lee, a female scientist who blogged at "Scientific American", was called a "whore" in an email by an editor at the science website "Biology Online" after refusing to write professional content without compensation. When Lee, outraged about the email, wrote a rebuttal on her "Scientific American" blog, the editor-in-chief of "Scientific American", Mariette DiChristina, removed the post, sparking an outrage by supporters of Lee. While DiChristina cited legal reasons for removing the blog, others criticized her for censoring Lee. The editor at Biology Online was fired after the incident.

The controversy widened in the ensuing days. The magazine's blog editor, Bora Zivkovic, was the subject of allegations of sexual harassment by another blogger, Monica Byrne. Although the alleged incident had occurred about a year earlier, editor Mariette DiChristina informed readers that the incident had been investigated and resolved to Ms. Byrne's satisfaction. However, the incident involving Dr. Lee had prompted Ms. Byrne to reveal the identity of Zivkovic, following the latter's support of Dr. Lee. Zivkovic responded on Twitter and his own blog, admitting the incident with Ms. Byrne had taken place. His blog post apologized to Ms. Byrne, and referred to the incident as "singular", stating that his behavior was not "engaged in before or since."

Due to the allegations, Zivkovic resigned from the board of Science Online, the popular science blogging conference that he helped establish. Following Zivkovic's admission, several prominent female bloggers, including other bloggers for the magazine, wrote their own accounts that contradicted Zivkovic's assertions, alleging additional incidents of sexual harassment. A day after these new revelations, Zivkovic resigned his position at "Scientific American", according to a press release from the magazine.




</doc>
<doc id="29511" url="https://en.wikipedia.org/wiki?curid=29511" title="Siouxsie and the Banshees">
Siouxsie and the Banshees

Siouxsie and the Banshees were a British rock band, formed in London in 1976 by vocalist Siouxsie Sioux and bass guitarist Steven Severin. They have been widely influential, both over their contemporaries and with later acts. "Mojo" rated guitarist John McGeoch in their list of "100 Greatest Guitarists of All Time" for his work on "Spellbound". "The Times" cited the group as "one of the most audacious and uncompromising musical adventurers of the post-punk era".

Initially associated with the punk scene, the band rapidly evolved to create "a form of post-punk discord full of daring rhythmic and sonic experimentation". Their debut album "The Scream" was released in 1978 to critical acclaim. In 1980, they changed their musical direction and became "almost a different band" with "Kaleidoscope", which peaked at number 5 in the UK Albums Chart. With "Juju" (1981) which also reached the top 10, they became an influence on the emerging gothic scene. In 1988, the band made a breakthrough in North America with the multifaceted album "Peepshow", which received critical praise. With substantial support from alternative rock radio stations, they achieved a mainstream hit in the US in 1991 with the single "Kiss Them for Me".

During their career, Siouxsie and the Banshees released 11 studio albums and 30 singles. The band experienced several line-up changes, with Siouxsie and Severin being the only constant members. They disbanded in 1996, with Siouxsie and drummer Budgie continuing to record music as the Creatures, a second band they had formed in the early 1980s. In 2004, Siouxsie began a solo career.

Siouxsie Sioux and Steven Severin met at a Roxy Music concert in September 1975, at a time when glam rock had faded and there was nothing new coming through with which they could identify. From February 1976, Siouxsie, Severin and some friends began to follow an unsigned band, the Sex Pistols. Journalist Caroline Coon dubbed them the "Bromley Contingent", as most of them came from the Bromley region of South London, a label Severin came to despise. "There was no such thing, it was just a bunch of people drawn together by the way they felt and they looked". They were all inspired by the Sex Pistols and their uncompromising attitude. When they learned that one of the bands scheduled to play the 100 Club Punk Festival, organised by Sex Pistols manager Malcolm McLaren, were pulling out from the bill at the last minute, Siouxsie suggested that she and Severin play, even though they had no band name or additional members. Two days later, the pair appeared at the festival held in London on 20 September 1976. With two borrowed musicians at their side, Marco Pirroni on guitar and John Simon Ritchie (already commonly known as Sid Vicious) on drums, their set consisted of a 20-minute improvisation based on "The Lord's Prayer".

While the band intended to split up after the gig, they were asked to play again. Two months later, Siouxsie and Severin recruited drummer Kenny Morris and guitarist Peter Fenton. After playing several gigs in early 1977, they realised that Fenton did not fit in because he was "a real rock guitarist". John McKay finally took his place in July. Their first live appearance on television took place in November on Manchester's Granada, on Tony Wilson's TV show "So It Goes". They then recorded their first John Peel session for BBC radio and appeared on the front cover of UK weekly "Sounds" magazine the following month.

While the band sold out venues in London in early 1978, they still had problems getting the right recording contract that could give them "complete artistic control". Polydor finally offered this guarantee and signed them in June. Their first single, "Hong Kong Garden", featuring a xylophone motif, reached the top 10 in the UK shortly after. A "NME" review hailed it as "a bright, vivid narrative, something like snapshots from the window of a speeding Japanese train, power charged by the most original, intoxicating guitar playing I heard in a long, long time".

The band released their debut album, "The Scream", in November 1978. Nick Kent of "NME" said of the record: "The band sounds like some unique hybrid of the Velvet Underground mated with much of the ingenuity of "Tago Mago"-era Can, if any parallel can be drawn". At the end of the article, he added this remark: "Certainly, the traditional three-piece sound has never been used in a more unorthodox fashion with such stunning results".

The Banshees' second album, "Join Hands", was released in 1979. In "Melody Maker", Jon Savage described "Poppy Day" as "a short, powerful evocation of the Great War graveyards", and "Record Mirror" described the whole record as a dangerous work that "should be heard". The Banshees embarked on a major tour to promote the album. A few dates into the tour in September, Morris and McKay left an in-store signing after an argument and quit the band. In need of replacements to fulfill tour dates, the Banshees' manager called drummer Budgie, formerly with the Slits, and asked him to audition. Budgie was hired, but Siouxsie and Severin had no success auditioning guitarists. Robert Smith of the Cure offered his services in case they could not find a guitarist (his group were already the support band on the tour), so the band held him to it after seeing too many "rock virtuosos". The tour resumed in September and after the last concert, Smith returned to the Cure.

Drummer Budgie became a permanent member, and the band entered the studios to record the single "Happy House" with guitarist John McGeoch, formerly of Magazine. Their third album, "Kaleidoscope", released in 1980, saw the Banshees exploring new musical territories with the use of other instruments like synthesizers, sitars and drum machines. The group initially had a concept of making each song sound completely different, without regard to whether or not the material could be performed in concert. "Melody Maker" described the result as "a kaleidoscope of sound and imagery, new forms, and content, flashing before our eyes". "Kaleidoscope" was a commercial success, peaking at number 5 in the UK albums chart. This lineup, featuring McGeoch on guitar, toured the United States for the first time in support of the album, playing their first shows in New York City in November 1980.
For "Juju" (1981), the band took a different approach and practised the songs in concert first before recording them. "Juju", according to Severin, became an unintentional concept album that "drew on darker elements". "Sounds" hailed it as "intriguing, intense, brooding and powerfully atmospheric". The album later peaked at number 7 in the UK albums chart and became one of their biggest sellers. McGeoch's guitar contributions on "Juju" would be later praised by Johnny Marr of the Smiths.

During the 1981 accompanying tour, Siouxsie and Budgie secretly became a couple. At the same time, they also began a drum-and-voice duo called the Creatures, releasing their first EP, "Wild Things".

The Banshees followed in 1982 with the psychedelic "A Kiss in the Dreamhouse". The record, featuring strings on several numbers, was an intentional contrast to their previous work, with Severin later describing it as a "sexy album". The British press greeted it enthusiastically. Richard Cook finished his "NME" review with this sentence: "I promise...this music will take your breath away". At that time, McGeoch was struggling with alcohol problems, and was hospitalised on his return to a promotional trip from Madrid. The band fired him shortly thereafter. Severin asked Robert Smith to take over guitarist duties again; Smith accepted and rejoined the group in November 1982.

During 1983, the band members worked on several side projects; Siouxsie and Budgie composed the first Creatures album, "Feast", while Severin and Smith recorded as the Glove. Smith then insisted on documenting his time with the Banshees, so the group released a cover version of the Beatles' "Dear Prudence" in September 1983. It became their biggest hit, reaching number 3 on the UK Singles Chart. They also released a live album, "Nocturne", and completed their sixth studio album, "Hyæna". Shortly before its release in May 1984, Smith left the group, citing health issues due to an overloaded schedule, being in two bands at once.

Ex-Clock DVA guitarist John Valentine Carruthers replaced him. The Banshees then reworked four numbers from their repertoire, augmented by a string section, for "The Thorn" EP. "NME" praised the project at its release: "The power of a classical orchestra is the perfect foil for the band's grindingly insistent sounds". The new Banshees lineup spent much of 1985 working on a new record, "Tinderbox". The group finished the song "Cities in Dust" before the album, so they rushed its release as a single prior to their longest tour of the UK. "Tinderbox" was finally released in April 1986. "Sounds" magazine noted: ""Tinderbox" is a refreshing slant on the Banshees' disturbing perspective and restores their vivid shades to pop's pale palette". Due to the length of time spent working on "Tinderbox", the group desired spontaneity and decided to record an album of cover songs, "Through the Looking Glass", in 1987. "Mojo" magazine later praised their version of "Strange Fruit". After the album's release, the band realised Carruthers was no longer fitting in and decided to work on new material as a trio.

Following a lengthy break, the band recruited multi-instrumentalist Martin McCarrick and guitarist Jon Klein. The quintet recorded "Peepshow" in 1988, with non-traditional rock instrumentation including cello and accordion. "Q" magazine praised the album in its 5-star review: ""Peepshow" takes place in some distorted fairground of the mind where weird and wonderful shapes loom". The first single, "Peek-a-Boo", was seen by critics as a "brave move" with horns and dance elements. "Sounds" wrote: "The snare gets slapped, Siouxsie's voice meanders all around your head and it all comes magically together". "Peek-a-Boo" was their first real breakthrough in the United States. After the tour, the band decided to take a break, with Siouxsie and Budgie recording as the Creatures and releasing their most critically acclaimed album to date, "Boomerang", and Severin and McCarrick working on material together.

In 1991, Siouxsie and the Banshees returned with the single "Kiss Them for Me", mixing strings over a dance rhythm laced with exotica. The group collaborated with the then unknown Asian Tabla player Talvin Singh, who also sang during the bridge. The single received glowing reviews and later peaked in the "Billboard" Hot 100 at number 23, allowing them to reach a new audience. The album "Superstition" followed shortly afterwards, and the group toured the US as second headliners of the inaugural Lollapalooza tour. The following year, the Banshees were asked to compose "Face to Face" as a single for the film "Batman Returns", at director Tim Burton's request.

In 1993, the Banshees recorded new songs based on string arrangements, but quickly stopped the sessions to play festivals abroad. On their return home, they hired former Velvet Underground member John Cale to produce the rest of the record. At its release, 1995's "The Rapture" was described by "Melody Maker" as "a fascinating, transcontinental journey through danger and exotica". A few weeks after its release, Polydor dropped the band from its roster and Klein was replaced on the band's last tour in 1995 by ex-Psychedelic Furs guitarist Knox Chandler. In April 1996, the Banshees disbanded after 20 years of working together. Siouxsie and Budgie announced that they would carry on recording as the Creatures. In 1999, they released the album "Anima Animus" to critical acclaim.

In 2002, Universal Music kicked off the band's remastered back catalogue by releasing "The Best of Siouxsie and the Banshees". In April, Siouxsie, Severin, Budgie and Chandler reunited briefly for the Seven Year Itch tour, which spawned the "Seven Year Itch" live album and DVD in 2003. The day after their last concert in Tokyo, Japan, Siouxsie and Budgie stayed in town on their own and entered into a recording studio as the Creatures. Their fourth and final studio album, "Hái!", came out a few months later.

In 2004, "Downside Up", a box set that collected all of the Banshees' B-sides and "The Thorn" EP, was released. "The Times" wrote in its review: "for here is a group that never filled B-sides with inferior, throwaway tracks. Rather they saw them as an outlet for some of their most radical and challenging work".

In 2006, the band's first four records were remastered and compiled with previously unreleased bonus tracks. Several recordings made for the John Peel radio show from 1978 to 1986 were also compiled on "". AllMusic described the first session as "a fiery statement of intent" and qualified the other performances as "excellent". The second batch of remasters, concerning the 1982–1986 era, was issued in April 2009. It included four other reissues (including their highly regarded "A Kiss in the Dreamhouse" from 1982). The "At the BBC" box set, containing a DVD with all of the band's UK live television performances and three CDs with in-concert recordings, was also released in June of the same year.

In April 2014, their debut single "Hong Kong Garden" was reissued on double 7-inch vinyl. It was announced that this would be part of a three-year plan with Universal. In late October, their last four studio albums (1987's "Through the Looking Glass", 1988's "Peepshow", 1991's "Superstition" and 1995's "The Rapture") were reissued on CD in remastered versions with bonus tracks. Siouxsie and Severin curated a compilation CD called "It's a Wonderfull Life" for the monthly magazine "Mojo", issued in September with Siouxsie on the front cover. On this CD, the pair honoured several composers of film music and classical music that had inspired them.

In 2015, after releasing another compilation called "Spellbound: The Collection", which included singles, album tracks and B-sides, the band reissued 1979's "Join Hands" on vinyl for Record Store Day, with different cover artwork. A CD box set titled "Classic Album Selection Volume One" was released in January 2016, containing their first six albums newly remastered by Kevin Metcalfe. "Classic Album Selection Volume Two", including the other last six albums, followed in April. A vinyl picture disc edition of "The Scream" was released in November.

A vinyl reissue series on Polydor of all of the band's albums, remastered from the original ¼" tapes in 2018 by Miles Showell and cut at half speed at Abbey Road Studios, began in August with "Tinderbox", "Juju", "Through the Looking Glass" and "Join Hands". The second batch of vinyl reissues, released in September, included "Superstition", "A Kiss in the Dreamhouse" and "The Scream". The third batch, arriving in December, contained "The Rapture", "Peepshow", "Kaleidoscope" and "Hyæna". A blue vinyl edition of "The Scream", limited to 1,000 copies, was also released in November for the 40th anniversary of the album, on indie only record stores' websites.

Siouxsie and the Banshees have been described as developing "a form of post-punk discord full of daring rhythmic and sonic experimentation". "The Times" wrote that "The Banshees stand proudly [... as] one of the most audacious and uncompromising musical adventurers of the post-punk era". With some of their darkest material, the band also helped spawn the gothic scene. The band is also considered a new wave act.

They were also one of the first alternative bands; music historian Peter Buckley pointed out that they were at "the very front of the alternative-rock scene". In 1988, "Peek-a-Boo" was the very first track to top the US Modern Rock chart after "Billboard" launched this chart in the first week of September to list the most played songs on alternative and college radio stations. Simon Goddard wrote that the "Banshees - Mk II would become one of the biggest alternative pop groups of the 1980s". "Spin" described them as "alternative rockers" in 1991 when referring to their presence in the top 40 chart. Noting the band's participation in the first Lollapalooza festival, journalist Jim Gerr saw them as one of the "elements of the alternative rock community". "Mojo" retrospectively presented them as one of "alternative rock's iconic groups".

Siouxsie and the Banshees have had an impact on many later genres including post-punk, new wave, synth pop, gothic rock, alternative music, shoegazing and trip-hop, influencing a wide range of musicians including Joy Division, the Cure, the Smiths, Depeche Mode, PJ Harvey, Radiohead, Tricky and LCD Soundsystem.

Joy Division's Peter Hook, who saw the group in concert in Manchester in 1977, said: "Siouxsie and the Banshees were one of our big influences [...] The Banshees first LP was one of my favourite ever records, the way the guitarist and the drummer played was a really unusual way of playing". Joy Division producer Martin Hannett saw a difference between the Banshees' first main lineup and the other bands of 1977: "Any harmonies you got were stark, to say the least, except for the odd exception, like Siouxsie. They were interesting". The Cure's leader, Robert Smith, declared in 2003: "Siouxsie and the Banshees and Wire were the two bands I really admired. They meant something." He also pinpointed what the 1979 "Join Hands" tour brought him musically. "On stage that first night with the Banshees, I was blown away by how powerful I felt playing that kind of music. It was so different to what we were doing with the Cure. Before that, I'd wanted us to be like the Buzzcocks or Elvis Costello, the punk Beatles. Being a Banshee really changed my attitude to what I was doing".

The two songwriters of the Smiths cited the band; singer Morrissey said that "Siouxsie and the Banshees were excellent", and that "they were one of the great groups of the late 1970s, early 1980s". He also said in 1994, "If you study modern groups, those who gain press coverage and chart action, none of them are as good as Siouxsie and the Banshees at full pelt. That's not dusty nostalgia, that's fact". When asked "who do you regret not going to see live", guitarist Johnny Marr replied "Siouxsie and the Banshees mk 1. But mk 2 were even better". Marr mentioned his liking for John McGeoch and his contribution to the single "Spellbound". Marr qualified it as "clever" with a "really good picky thing going on which is very un-rock'n'roll". Smiths' historian Goddard wrote that Marr "praise[d] the McGeoch-era Banshees as a significant inspiration". U2 cited Siouxsie and the Banshees as a major influence and selected "Christine" for a "Mojo" compilation. The Edge was the presenter of an award given to Siouxsie at the "Mojo" ceremony in 2005. In December 1981, Dave Gahan of Depeche Mode named the Banshees as one of his three favourite bands along with Sparks and Roxy Music. Gahan later hailed the single "Candyman" at its release, saying, "This is a great Banshees record[...], I like their sound". Jim Reid of the Jesus and Mary Chain selected "Jigsaw Feeling" from "The Scream" as being among his favourite songs. Thurston Moore of Sonic Youth cited "Hong Kong Garden" in his top 25 all-time favourite songs, and Kevin Shields of My Bloody Valentine also mentioned them as being among his early influences. Dave Navarro of Jane's Addiction once noted a parallel between his band and the Banshees: "There are so many similar threads: melody, use of sound, attitude, sex-appeal. I always saw Jane's Addiction as the masculine Siouxsie and the Banshees". Primal Scream's Bobby Gillespie liked the group's ability to produce pop songs while transmitting something subversive. He said, "They were outsiders bringing outsider subjects to the mainstream. We’re not trying to rip off the Banshees, but that's kind of where we’re coming from".

The Banshees have been hailed by other acts. Thom Yorke related that seeing Siouxsie on stage in concert in 1985 inspired him to become a performer. Radiohead cited McGeoch-era Siouxsie records when mentioning the recording of the song "There There", and rehearsed Banshees' material prior to their 2008 tour. Jeff Buckley, who took inspiration from several female vocalists, covered "Killing Time" (from the "Boomerang" album) on various occasions. Buckley also owned all the Banshees' albums in his record collection. Suede singer Brett Anderson named "Juju" as one of his favourite records in 2011, and also cited three other albums by the band on his website: "The Scream", "Kaleidoscope" and "Tinderbox". Red Hot Chili Peppers performed "Christine" in concert, and their guitarist John Frusciante cited the Banshees in interviews. Garbage singer Shirley Manson stated, "I learned how to sing listening to "The Scream" and "Kaleidoscope". Today, I can see and hear the Banshees' influence all over the place". Siouxsie has also been praised by other female singers including PJ Harvey and Courtney Love. PJ Harvey has stated, "It's hard to beat Siouxsie Sioux, in terms of live performance. She is so exciting to watch, so full of energy and human raw quality", and selected Siouxsie's album "Anima Animus" in her top 10 albums of 1999. The band had a strong effect on two important trip hop acts. Tricky covered "Tattoo" to open his second album, "Nearly God"; the original 1983 proto-trip-hop version of that song aided Tricky in the creation of his style. Massive Attack, sampled "Metal Postcard" on the song "Superpredators", recorded prior to their "Mezzanine" album. Air's Jean-Benoît Dunckel cited the group as one of his three main influences. Billy Corgan of the Smashing Pumpkins cited the Banshees as an important influence on his music. Faith No More covered "Switch" in concert and cited "The Scream" as one of their influences.

The Banshees continue to influence younger musicians. Singer James Murphy was marked by certain Banshees albums during his childhood. His band LCD Soundsystem covered "Slowdive" as a B-side to the single "Disco Infiltrator". The Beta Band sampled "Painted Bird" on their track "Liquid Bird" from the "Heroes to Zeros" album. TV on the Radio said that they have always tried to make a song that begins like "Kiss Them for Me" where all of a sudden, there's an "element of surprise" with "a giant drum coming in". Santigold based one of her songs around the music of "Red Light". "'My Superman' is an interpolation of 'Red Light'". Indie folk group DeVotchKa covered the ballad "The Last Beat of My Heart" at the suggestion of Arcade Fire singer Win Butler; it was released on the "Curse Your Little Heart" EP. Gossip named the Banshees as one of their major influences during the promotion of their single "Heavy Cross". British indie band Bloc Party took inspiration from "Peek-a-Boo" and their singer Kele Okereke stated about that Banshees' single: "it sounded like nothing else on this planet. This is [...] a pop song that they put out in the middle of their career [...] to me it sounded like the most current but most futuristic bit of guitar-pop music I've heard". The Weeknd sampled different parts of "Happy House" for his song "House of Balloons", and also used the chorus of the initial version.






</doc>
<doc id="29513" url="https://en.wikipedia.org/wiki?curid=29513" title="Simula">
Simula

Simula is the name of two simulation programming languages, Simula I and Simula 67, developed in the 1960s at the Norwegian Computing Center in Oslo, by Ole-Johan Dahl and Kristen Nygaard. Syntactically, it is a fairly faithful superset of ALGOL 60, also influenced by the design of Simscript.

Simula 67 introduced objects, classes, inheritance and subclasses, virtual procedures, coroutines, and discrete event simulation, and features garbage collection. Also other forms of subtyping (besides inheriting subclasses) were introduced in Simula derivatives.

Simula is considered the first object-oriented programming language. As its name suggests, Simula was designed for doing simulations, and the needs of that domain provided the framework for many of the features of object-oriented languages today.

Simula has been used in a wide range of applications such as simulating VLSI designs, process modeling, protocols, algorithms, and other applications such as typesetting, computer graphics, and education. The influence of Simula is often understated, and Simula-type objects are reimplemented in C++, Object Pascal, Java, C# and several other languages. Computer scientists such as Bjarne Stroustrup, creator of C++, and James Gosling, creator of Java, have acknowledged Simula as a major influence.

The following account is based on Jan Rune Holmevik's historical essay.

Kristen Nygaard started writing computer simulation programs in 1957. Nygaard saw a need for a better way to describe the heterogeneity and the operation of a system. To go further with his ideas on a formal computer language for describing a system, Nygaard realized that he needed someone with more computer programming skills than he had. Ole-Johan Dahl joined him on his work January 1962. The decision of linking the language up to ALGOL 60 was made shortly after. By May 1962 the main concepts for a simulation language were set. "SIMULA I" was born, a special purpose programming language for simulating discrete event systems.

Kristen Nygaard was invited to visit the Eckert–Mauchly Computer Corporation late May 1962 in connection with the marketing of their new UNIVAC 1107 computer. At that visit Nygaard presented the ideas of Simula to Robert Bemer, the director of systems programming at Univac. Bemer was a sworn ALGOL fan and found the Simula project compelling. Bemer was also chairing a session at the second international conference on information processing hosted by IFIP. He invited Nygaard, who presented the paper "SIMULA -- An Extension of ALGOL to the Description of Discrete-Event Networks".

The Norwegian Computing Center got a UNIVAC 1107 August 1963 at a considerable discount, on which Dahl implemented the SIMULA I under contract with UNIVAC. The implementation was based on the UNIVAC ALGOL 60 compiler. SIMULA I was fully operational on the UNIVAC 1107 by January 1965. In the following couple of years Dahl and Nygaard spent a lot of time teaching Simula. Simula spread to several countries around the world and SIMULA I was later implemented on Burroughs B5500 computers and the Russian URAL-16 computer.

In 1966 C. A. R. Hoare introduced the concept of record class construct, which Dahl and Nygaard extended with the concept of prefixing and other features to meet their requirements for a generalized process concept. Dahl and Nygaard presented their paper on Class and Subclass declarations at the IFIP Working Conference on simulation languages in Oslo, May 1967. This paper became the first formal definition of Simula 67. In June 1967 a conference was held to standardize the language and initiate a number of implementations. Dahl proposed to unify the type and the class concept. This led to serious discussions, and the proposal was rejected by the board. SIMULA 67 was formally standardized on the first meeting of the SIMULA Standards Group (SSG) in February 1968.

Simula was influential in the development of Smalltalk and later object-oriented programming languages. It also helped inspire the actor model of concurrent computation although Simula only supports coroutines and not true concurrency.

In the late sixties and the early seventies there were four main implementations of Simula:


These implementations were ported to a wide range of platforms. The TOPS-10 implemented the concept of public, protected, and private member variables and procedures, that later was integrated into Simula 87. Simula 87 is the latest standard and is ported to a wide range of platforms. There are mainly four implementations:


In November 2001 Dahl and Nygaard were awarded the IEEE John von Neumann Medal by the Institute of Electrical and Electronics Engineers "For the introduction of the concepts underlying object-oriented programming through the design and implementation of SIMULA 67". In April 2002 they received the 2001 A. M. Turing Award by the Association for Computing Machinery (ACM), with the citation: "For ideas fundamental to the emergence of object oriented programming, through their design of the programming languages Simula I and Simula 67." Unfortunately neither Dahl nor Nygaard could make it to the ACM Turing Award Lecture, scheduled to be delivered at the November 2002 OOPSLA conference in Seattle, as they died in June and August of that year, respectively.

Simula Research Laboratory is a research institute named after the Simula language, and Nygaard held a part-time position there from the opening in 2001. The new Computer Science building at the University of Oslo is named Ole Johan Dahl's House, in Dahl's honour, and the main auditorium is named Simula.

The empty computer file is the minimal program in Simula, measured by the size of the source code.
It consists of one thing only; a dummy statement.

However, the minimal program is more conveniently represented as an empty block:

It begins executing and immediately terminates.
The language does not have any return value from the program itself.

An example of a Hello world program in Simula:

Simula is case-insensitive.

A more realistic example with use of classes, subclasses and virtual procedures:

The above example has one super class (Glyph) with two subclasses (Char and Line).
There is one virtual procedure with two implementations.
The execution starts by executing the main program.
Simula does not have the concept of abstract classes since classes with pure virtual procedures can be instantiated. This means that in the above example all classes can be instantiated. Calling a pure virtual procedure will however produce a run-time error.

Simula supports call by name so the Jensen's Device can easily be implemented.
However, the default transmission mode for simple parameter is call by value, contrary to ALGOL which used call by name.
The source code for the Jensen's Device must therefore specify call by name for the parameters when compiled by a Simula compiler.

Another much simpler example is the summation function formula_1 which can be implemented as follows:

The above code uses call by name for the controlling variable (k) and the expression (u).
This allows the controlling variable to be used in the expression.

Note that the Simula standard allows for certain restrictions on the controlling variable
in a for loop. The above code therefore uses a while loop for maximum portability.

The following:

formula_2

can then be implemented as follows:

Simula includes a simulation package for doing discrete event simulations. This simulation package is based on Simula's object oriented features and its coroutine concept.

Sam, Sally, and Andy are shopping for clothes. They have to share one fitting room. Each one of them is browsing the store for about 12 minutes and then uses the fitting room exclusively for about three minutes, each following a normal distribution. A simulation of their fitting room experience is as follows:

The main block is prefixed with codice_1 for enabling simulation. The simulation package can be used on any block and simulations can even be nested when simulating someone doing simulations.

The fitting room object uses a queue (codice_2) for getting access to the fitting room. When someone requests the fitting room and it's in use they must wait in this queue (codice_3). When someone leaves the fitting room the first one (if any) is released from the queue (codice_4) and accordingly removed from the door queue (codice_5).

Person is a subclass of Process and its activity is described using hold (time for browsing the store and time spent in the fitting room) and calls procedures in the fitting room object for requesting and leaving the fitting room.

The main program creates all the objects and activates all the person objects to put them into the event queue. The main program holds for 100 minutes of simulated time before the program terminates.



</doc>
<doc id="29515" url="https://en.wikipedia.org/wiki?curid=29515" title="SNOBOL">
SNOBOL

SNOBOL ("StriNg Oriented and symBOlic Language") is a series of computer programming languages developed between 1962 and 1967 at AT&T Bell Laboratories by David J. Farber, Ralph E. Griswold and Ivan P. Polonsky, culminating in SNOBOL4. It was one of a number of text-string-oriented languages developed during the 1950s and 1960s; others included COMIT and TRAC.

SNOBOL4 stands apart from most programming languages of its era by having patterns as a first-class data type ("i.e." a data type whose values can be manipulated in all ways permitted to any other data type in the programming language) and by providing operators for pattern concatenation and alternation. SNOBOL4 patterns are a type of object and admit various manipulations, much like later object-oriented languages such as JavaScript whose patterns are known as regular expressions. In addition SNOBOL4 strings generated during execution can be treated as programs and either interpreted or compiled and executed (as in the eval function of other languages).

SNOBOL4 was quite widely taught in larger US universities in the late 1960s and early 1970s and was widely used in the 1970s and 1980s as a text manipulation language in the humanities.

In the 1980s and 1990s its use faded as newer languages such as AWK and Perl made string manipulation by means of regular expressions fashionable. SNOBOL4 patterns subsume BNF grammars, which are equivalent to context-free grammars and more powerful than regular expressions. 
The "regular expressions" in current versions of AWK and Perl are in fact extensions of regular expressions in the traditional sense, but regular expressions, unlike SNOBOL4 patterns, are not recursive, which gives a distinct computational advantage to SNOBOL4 patterns. (Recursive expressions did appear in Perl 5.10, though, released in December 2007.)

One of the designers of SNOBOL, Ralph Griswold, designed successors to SNOBOL4 called SL5 and Icon, which combined the backtracking of SNOBOL4 pattern matching with more standard ALGOL-like structuring, as well as adding some features of their own.

The initial SNOBOL language was created as a tool to be used by its authors to work with the symbolic manipulation of polynomials. It was written in assembly language for the IBM 7090. It had a simple syntax, only one datatype, the string, no functions, and no declarations and very little error control. However, despite its simplicity and its "personal" nature its use began to spread to other groups. As a result, the authors decided to extend it and tidy it up. They rewrote it and added functions, both standard and user-defined, and released the result as SNOBOL3. SNOBOL2 did exist but it was a short-lived intermediate development version without user-defined functions and was never released. SNOBOL3 became quite popular and was rewritten for other computers than the IBM 7090 by other programmers. As a result, several incompatible dialects arose.

As SNOBOL3 became more popular the authors received more and more requests for extensions to the language. They also began to receive complaints about incompatibility and bugs in versions that they hadn't written. To address this and to take advantage of the new computers being introduced in the late 1960s, the decision was taken to develop SNOBOL4 with many extra datatypes and features but based on a virtual machine to allow improved portability across computers. The SNOBOL4 language translator was still written in assembly language. However the macro features of the assembler were used to define the virtual machine instructions of the SNOBOL Implementation Language, the SIL. This very much improved the portability of the language by making it relatively easy to port the virtual machine which hosted the translator by recreating its virtual instructions on any machine which included a macro assembler or indeed a high level language.

SNOBOL4 supports a number of built-in data types, such as integers and limited precision real numbers, strings, patterns, arrays, and tables (associative arrays), and also allows the programmer to define additional data types and new functions. SNOBOL4's programmer-defined data type facility was advanced at the time—it is similar to the records of the earlier COBOL and the later Pascal programming languages.

All SNOBOL command lines are of the form

Each of the five elements is optional. In general, the "subject" is matched against the "pattern". If the "object" is present, any matched portion is replaced by the "object" via rules for replacement. The "transfer" can be an absolute branch or a conditional branch dependent upon the success or failure of the subject evaluation, the pattern evaluation, the pattern match, the object evaluation or the final assignment. It can also be a transfer to code created and compiled by the program itself during a run.

A SNOBOL pattern can be very simple or extremely complex. A simple pattern is just a text string (e.g. "ABCD"), but a complex pattern may be a large structure describing, for example, the complete grammar of a computer language. It is possible to implement a language interpreter in SNOBOL almost directly from a Backus–Naur form expression of it, with few changes. Creating a macro assembler and an interpreter for a completely theoretical piece of hardware could take as little as a few hundred lines, with a new instruction being added with a single line.

Complex SNOBOL patterns can do things that would be impractical or impossible using the more primitive regular expressions used in most other pattern-matching languages. Some of this power derives from the so-called "SPITBOL extensions" (which have since been incorporated in basically all modern implementations of the original SNOBOL 4 language too), although it is possible to achieve the same power without them. Part of this power comes from the side effects that it is possible to produce during the pattern matching operation, including saving numerous intermediate/tentative matching results and the ability to invoke user-written functions during the pattern match which can perform nearly any desired processing, and then influence the ongoing direction the interrupted pattern match takes, or even to indeed change the pattern itself during the matching operation. Patterns can be saved like any other first-class data item, and can be concatenated, used within other patterns, and used to create very complex and sophisticated pattern expressions. It is possible to write, for example, a SNOBOL4 pattern which matches "a complete name and international postal mailing address", which is well beyond anything that is practical to even attempt using regular expressions.

SNOBOL4 pattern-matching uses a backtracking algorithm similar to that used in the logic programming language Prolog, which provides pattern-like constructs via DCGs. This algorithm makes it easier to use SNOBOL as a logic programming language than is the case for most languages.

SNOBOL stores variables, strings and data structures in a single garbage-collected heap.

SNOBOL rivals APL for its distinctiveness in format and programming style, both being radically unlike more "standard" procedural languages such as BASIC, Fortran, or C.

The "Hello, World!" program might be as follows...
END
A simple program to ask for a user's name and then use it in an output sentence...

END
To choose between three possible outputs...
MEH OUTPUT = "Hi, " Username :(END)
LOVE OUTPUT = "How nice to meet you, " Username :(END)
HATE OUTPUT = "Oh. It's you, " Username
END
To continue requesting input until no more is forthcoming...
AGAIN NameCount = NameCount + 1
GETINPUT OUTPUT = "Please give me name " NameCount + 1 
END

The classic implementation was on the PDP-10; it has been used to study compilers, formal grammars, and artificial intelligence, especially machine translation and machine comprehension of natural languages. The original implementation was on an IBM 7090 at Bell Labs, Holmdel, N.J. SNOBOL4 was specifically designed for portability; the first implementation was started on an IBM 7094 in 1966 but completed on an IBM 360 in 1967. It was rapidly ported to many other platforms.

It is normally implemented as an interpreter because of the difficulty in implementing some of its very high-level features, but there is a compiler, the SPITBOL compiler, which provides nearly all the facilities that the interpreter provides.

The Gnat Ada Compiler comes with a package (GNAT.Spitbol) which implements all of the Spitbol string manipulation semantics. This can be called from within an Ada program.

The file editor for the Michigan Terminal System (MTS) provided pattern matching based on SNOBOL4 patterns.

Several implementations are currently available. Macro SNOBOL4 in C written by Phil Budne is a free, open source implementation, capable of running on almost any platform. Catspaw, Inc provided a commercial implementation of the SNOBOL4 language for many different computer platforms, including DOS, Macintosh, Sun, RS/6000, and others, and these implementations are now available free from Catspaw. Minnesota SNOBOL4, by Viktors Berstis, the closest PC implementation to the original IBM mainframe version (even including Fortran-like FORMAT statement support) is also free.

Although SNOBOL itself has no structured programming features, a SNOBOL preprocessor called Snostorm was designed and implemented during the 1970s by Fred G. Swartz for use under the Michigan Terminal System (MTS) at the University of Michigan. Snostorm was used at the eight to fifteen sites that ran MTS. It was also available at University College London (UCL) between 1982 and 1984.

Snocone by Andrew Koenig adds block-structured constructs to the SNOBOL4 language. Snocone is a self-contained programming language, rather than a proper superset of SNOBOL4.

The SPITBOL implementation also introduced a number of features which, while not using traditional structured programming keywords, nevertheless can be used to provide many of the equivalent capabilities normally thought of as "structured programming", most notably nested if/then/else type constructs. These features have since been added to most recent SNOBOL4 implementations. After many years as a commercial product, in April 2009 SPITBOL was released as free software under the GNU General Public License.

According to Dave Farber, he, Griswold and Polonsky "finally arrived at the name Symbolic EXpression Interpreter SEXI."

Common backronyms of "SNOBOL" are 'String Oriented Symbolic Language' or (as a quasi-initialism) 'StriNg Oriented symBOlic Language'.





</doc>
<doc id="29518" url="https://en.wikipedia.org/wiki?curid=29518" title="Statistical physics">
Statistical physics

Statistical physics is a branch of physics that uses methods of probability theory and statistics, and particularly the mathematical tools for dealing with large populations and approximations, in solving physical problems. It can describe a wide variety of fields with an inherently stochastic nature. Its applications include many problems in the fields of physics, biology, chemistry, neuroscience, and even some social sciences, such as sociology and linguistics. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.

In particular, statistical mechanics develops the phenomenological results of thermodynamics from a probabilistic examination of the underlying microscopic systems. Historically, one of the first topics in physics where statistical methods were applied was the field of mechanics, which is concerned with the motion of particles or objects when subjected to a force.

Statistical mechanics provides a framework for relating the microscopic properties of individual atoms and molecules to the macroscopic or bulk properties of materials that can be observed in everyday life, therefore explaining thermodynamics as a natural result of statistics, classical mechanics, and quantum mechanics at the microscopic level. Because of this history, statistical physics is often considered synonymous with statistical mechanics or statistical thermodynamics.

One of the most important equations in statistical mechanics (akin to formula_1 in Newtonian mechanics, or the Schrödinger equation in quantum mechanics) is the definition of the partition function formula_2, which is essentially a weighted sum of all possible states formula_3 available to a system.

where formula_5 is the Boltzmann constant, formula_6 is temperature and formula_7 is energy of state formula_3. Furthermore, the probability of a given state, formula_3, occurring is given by

Here we see that very-high-energy states have little probability of occurring, a result that is consistent with intuition.

A statistical approach can work well in classical systems when the number of degrees of freedom (and so the number of variables) is so large that the exact solution is not possible, or not really useful. Statistical mechanics can also describe work in non-linear dynamics, chaos theory, thermal physics, fluid dynamics (particularly at high Knudsen numbers), or plasma physics.

Although some problems in statistical physics can be solved analytically using approximations and expansions, most current research utilizes the large processing power of modern computers to simulate or approximate solutions. A common approach to statistical problems is to use a Monte Carlo simulation to yield insight into the properties of a complex system.

Quantum statistical mechanics is statistical mechanics applied to quantum mechanical systems. In quantum mechanics a statistical ensemble (probability distribution over possible quantum states) is described by a density operator "S", which is a non-negative, self-adjoint, trace-class operator of trace 1 on the Hilbert space "H" describing the quantum system. This can be shown under various mathematical formalisms for quantum mechanics. One such formalism is provided by quantum logic.

A significant contribution (at different times) in development of statistical physics was given by Satyendra Nath Bose, James Clerk Maxwell, Ludwig Boltzmann, J. Willard Gibbs, Marian Smoluchowski, Albert Einstein, Enrico Fermi, Richard Feynman, Lev Landau, Vladimir Fock, Werner Heisenberg, Nikolay Bogolyubov, Benjamin Widom, Lars Onsager, Benjamin and Jeremy Chubb (also inventors of the titanium sublimation pump), Humb, Manoo, and others. Statistical physics is studied in the nuclear center at Los Alamos. Also, Pentagon has organized a large department for the study of turbulence at Princeton University. Work in this area is also being conducted by Saclay (Paris), Max Planck Institute, Netherlands Institute for Atomic and Molecular Physics and other research centers.

Statistical physics allowed us to explain and quantitatively describe superconductivity, superfluidity, turbulence, phlogistine, antiphlogistine, ludige, collective phenomena in solids and plasma, and the structural features of liquid. It underlies the modern astrophysics. It is statistical physics that helped us to create such intensively developing study of liquid crystals and to construct a theory of phase transition and critical phenomena. Many experimental studies of matter are entirely based on the statistical description of a system. These include the scattering of cold neutrons, X-ray, visible light, and more.
Statistical physics plays a major role in Physics of Solid State Physics, Materials Science, Nuclear Physics, Astrophysics, Chemistry, Biology and Medicine (e.g. study of the spread of infectious diseases), Information Theory and Technique but also in those areas of technology owing to their development in the evolution of Modern Physics. It still has important applications in theoretical sciences such as Sociology and Linguistics and is useful for researchers in higher education, corporate governance, and industry.


Thermal and Statistical Physics (lecture notes, Web draft 2001) by Mallett M., Blumler P.
by Harald J W Müller-Kirsten (University of Kaiserslautern, Germany)


</doc>
<doc id="29519" url="https://en.wikipedia.org/wiki?curid=29519" title="Side effect (computer science)">
Side effect (computer science)

In computer science, an operation, function or expression is said to have a side effect if it modifies some state variable value(s) outside its local environment, that is to say has an observable effect besides returning a value (the main effect) to the invoker of the operation. State data updated "outside" of the operation may be maintained "inside" a stateful object or a wider stateful system within which the operation is performed. 
Example side effects include modifying a non-local variable, modifying a static local variable, modifying a mutable argument passed by reference, performing I/O or calling other side-effect functions. In the presence of side effects, a program's behaviour may depend on history; that is, the order of evaluation matters. Understanding and debugging a function with side effects requires knowledge about the context and its possible histories.

The degree to which side effects are used depends on the programming paradigm. Imperative programming is commonly used to produce side effects, to update a system's state. By contrast, Declarative programming is commonly used to report on the state of system, without side effects. 

In functional programming, side effects are rarely used. The lack of side effects makes it easier to do formal verifications of a program. Functional languages such as Standard ML, Scheme and Scala do not restrict side effects, but it is customary for programmers to avoid them. The functional language Haskell expresses side effects such as I/O and other stateful computations using monadic actions.

Assembly language programmers must be aware of "hidden" side effects—instructions that modify parts of the processor state which are not mentioned in the instruction's mnemonic. A classic example of a hidden side effect is an arithmetic instruction that implicitly modifies condition codes (a hidden side effect) while it explicitly modifies a register (the overt effect). One potential drawback of an instruction set with hidden side effects is that, if many instructions have side effects on a single piece of state, like condition codes, then the logic required to update that state sequentially may become a performance bottleneck. The problem is particularly acute on some processors designed with pipelining (since 1990) or with out-of-order execution. Such a processor may require additional control circuitry to detect hidden side effects and stall the pipeline if the next instruction depends on the results of those effects.

Absence of side effects is a necessary, but not sufficient, condition for referential transparency. Referential transparency means that an expression (such as a function call) can be replaced with its value. This requires that the expression is pure, that is to say the expression must be deterministic (always give the same value for the same input) and side-effect free.

Side effects caused by the time taken for an operation to execute are usually ignored when discussing side effects and referential transparency. There are some cases, such as with hardware timing or testing, where operations are inserted specifically for their temporal side effects e.g. codice_1 or codice_2. These instructions do not change state other than taking an amount of time to complete.

A function codice_3 with side effects is said to be idempotent under sequential composition codice_4 if, when called twice with the same list of arguments, the second call has no side effects (assuming no other procedures were called between the end of the first call and the start of the second call).

For instance, consider the following Python code:

Here, codice_5 is idempotent because the second call to codice_5 (with the same argument) does not change the visible program state: codice_7 was already set to 5 in the first call, and is again set to 5 in the second call, thus keeping the same value. Note that this is distinct from idempotence under function composition codice_8. For example, the absolute value is idempotent under function composition:

One common demonstration of side effect behavior is that of the assignment operator in C++. For example, assignment returns the right operand and has the side effect of assigning that value to a variable. This allows for syntactically clean multiple assignment:

Because the operator right associates, this equates to

Where the result of assigning 3 into codice_9 then gets assigned into codice_10. This presents a potential hangup for novice programmers who may confuse

with



</doc>
<doc id="29523" url="https://en.wikipedia.org/wiki?curid=29523" title="List of science fiction editors">
List of science fiction editors

This is a list of science fiction editors, editors working for book and magazine publishing companies who have edited science fiction. Many have also edited works of fantasy and other related genres, all of which have been sometimes grouped under the name speculative fiction. 

Editors on this list should fulfill the conditions for in science fiction or related genres. Evidence for notability includes an existing wiki-biography, or evidence that one could be written. Borderline cases should be discussed on the article's talk page.




























</doc>
<doc id="29525" url="https://en.wikipedia.org/wiki?curid=29525" title="Square-free integer">
Square-free integer

In mathematics, a square-free integer (or squarefree integer) is an integer which is divisible by no perfect square other than 1. That is, its prime factorization has exactly one factor for each prime that appears in it. For example, is square-free, but is not, because 18 is divisible by . The smallest positive square-free numbers are

The radical of an integer is its largest square-free factor. An integer is square-free if and only if it is equal to its radical.

Any arbitrary positive integer can be represented in a unique way as the product of a powerful number (that is an integer such that is divisible by the square of every prime factor) and a square-free integer, which are coprime. The square-free factor, called the "square-free part" of the number, is the largest square-free divisor of that is coprime with . The square-free part of an integer may be smaller than the largest square-free divisor.

Any arbitrary positive integer can be represented in a unique way as the product of a square and a square-free integer:
In this factorization, is the largest divisor of such that is a divisor of .

In summary, there are three square-free factors that are naturally associated to every integer: the square-free part, the above factor , and the largest square-free factor. Each is a factor of the next one. All are easily deduced from a prime factorization: if
is the prime factorization of , where formula_3 are distinct prime numbers, then the square-free part is
The square-free factor such the quotient is a square is 
and the largest square-free factor is 

For example, if formula_7 the square-free part is , the square-free factor such that the quotient is a square is , and the largest square-free factor is .

No algorithm is known for computing any of these square-free factors which is faster than computing the complete prime factorization. In particular, there is no known polynomial-time algorithm for computing the square-free part of an integer, nor even for determining whether an integer is square-free. In contrast, polynomial-time algorithms are known for primality testing. This is a major difference between the arithmetic of the integers, and the arithmetic of the univariate polynomials, as polynomial-time algorithms are known for square-free factorization of polynomials (in short, the largest square-free factor of a polynomial is its quotient by the greatest common divisor of the polynomial and its formal derivative).

A positive integer "n" is square-free if and only if in the prime factorization of "n", no prime factor occurs with an exponent larger than one. Another way of stating the same is that for every prime factor "p" of "n", the prime "p" does not evenly divide "n" / "p". Also "n" is square-free if and only if in every factorization "n" = "ab", the factors "a" and "b" are coprime. An immediate result of this definition is that all prime numbers are square-free.

A positive integer "n" is square-free if and only if all abelian groups of order "n" are isomorphic, which is the case if and only if any such group is cyclic. This follows from the classification of finitely generated abelian groups.

A integer "n" is square-free if and only if the factor ring Z / "nZ (see modular arithmetic) is a product of fields. This follows from the Chinese remainder theorem and the fact that a ring of the form Z / "kZ is a field if and only if "k" is a prime.

For every positive integer "n", the set of all positive divisors of "n" becomes a partially ordered set if we use divisibility as the order relation. This partially ordered set is always a distributive lattice. It is a Boolean algebra if and only if "n" is square-free.

A positive integer "n" is square-free if and only if "μ"("n") ≠ 0, where "μ" denotes the Möbius function.

The absolute value of the Möbius function is the indicator function for the square-free integers – that is, is equal to 1 if is square-free, and 0 if it is not. The Dirichlet series of this indicator function is
where is the Riemann zeta function. This follows from the Euler product
where the products are taken over the prime numbers.

Let "Q"("x") denote the number of square-free integers between 1 and "x" ( shifting index by 1). For large "n", 3/4 of the positive integers less than "n" are not divisible by 4, 8/9 of these numbers are not divisible by 9, and so on. Because these ratios satisfy the multiplicative property (this follows from Chinese remainder theorem), we obtain the approximation:

This argument can be made rigorous for getting the estimate (using big O notation)

"Sketch of a proof:" the above characterization gives
observing that the last summand is zero for formula_13, it results that

By exploiting the largest known zero-free region of the Riemann zeta function Arnold Walfisz improved the approximation to
for some positive constant .

Under the Riemann hypothesis, the error term can be reduced to

Recently (2015) the error term has been further reduced to

The asymptotic/natural density of square-free numbers is therefore

Therefore over 3/5 of the integers are square-free.

Likewise, if "Q"("x","n") denotes the number of "n"-free integers (e.g. 3-free integers being cube-free integers) between 1 and "x", one can show

Since a multiple of 4 must have a square factor 4=2, it cannot occur that four consecutive integers are all square-free. On the other hand, there exist infinitely many integers "n" for which 4"n" +1, 4"n" +2, 4"n" +3 are all square-free. Otherwise, observing that 4"n" and at least one of 4"n" +1, 4"n" +2, 4"n" +3 among four could be non-square-free for sufficiently large "n", half of all positive integers minus finitely many must be non-square-free and therefore
contrary to the above asymptotic estimate for formula_21.

There exist sequences of consecutive non-square-free integers of arbitrary length. Indeed, if "n" satisfies a simultaneous congruence
for distinct primes formula_23, then each formula_24 is divisible by "p". On the other hand, the above-mentioned estimate formula_25 implies that, for some constant "c", there always exists a square-free integer between "x" and formula_26 for positive "x". Moreover, an elementary argument allows us to replace formula_26 by formula_28 The ABC conjecture would allow formula_29.

The table shows how formula_32 and formula_33 compare at powers of 10.

formula_34 , also denoted as formula_35.
formula_36 changes its sign infinitely often as formula_37 tends to infinity.

The absolute value of formula_36 is astonishingly small compared with formula_37.

If we represent a square-free number as the infinite product

then we may take those formula_41 and use them as bits in a binary number with the encoding

The square-free number 42 has factorization 2 × 3 × 7, or as an infinite product 2 · 3  · 5 · 7 · 11 · 13 ··· Thus the number 42 may be encoded as the binary sequence ...001011 or 11 decimal. (The binary digits are reversed from the ordering in the infinite product.)

Since the prime factorization of every number is unique, so also is every binary encoding of the square-free integers.

The converse is also true. Since every positive integer has a unique binary representation it is possible to reverse this encoding so that they may be decoded into a unique square-free integer.

Again, for example, if we begin with the number 42, this time as simply a positive integer, we have its binary representation 101010. This decodes to 2 · 3 · 5 · 7 · 11 · 13 = 3 × 7 × 13 = 273.

Thus binary encoding of squarefree numbers describes a bijection between the nonnegative integers and the set of positive squarefree integers.

The central binomial coefficient

is never squarefree for "n" > 4. This was proven in 1985 for all sufficiently large integers by András Sárközy, and for all integers > 4 in 1996 by Olivier Ramaré and Andrew Granville.

The multiplicative function formula_44 is defined
to map positive integers "n" to "t"-free numbers by reducing the
exponents in the prime power representation modulo "t":
The value set of formula_46, in particular, are the
square-free integers. Their Dirichlet generating functions are

OEIS representatives are ("t"=2), ("t"=3) and ("t"=4).



</doc>
<doc id="29530" url="https://en.wikipedia.org/wiki?curid=29530" title="Sentinel (comics)">
Sentinel (comics)

The Sentinels are a fictional variety of mutant-hunting robots appearing in American comic books published by Marvel Comics. They are typically depicted as antagonists to the X-Men.

The Sentinels played a large role in the 1990s "X-Men" animated series and have been featured in several X-Men video games. The Sentinels are featured prominently in the 2014 film "" while simulated versions made brief appearances in the 2006 film "" and the 2016 film "". In 2009, The Sentinels were ranked as IGN's 38th Greatest Comic Book Villain of All Time.

Created by Stan Lee and Jack Kirby, they first appeared in "The X-Men" #14 (November 1965).

Sentinels are programmed to locate mutants and capture or kill them. Though several types of Sentinels have been introduced, the typical Sentinel is three stories tall, is capable of flight, projects energy blasts, and can detect mutants.

Sentinels are designed to hunt mutants. While many are capable of tactical thought, only a handful are self-aware.

Sentinels are technologically advanced, and have exhibited a wide variety of abilities. They are armed (primarily with energy weapons and restraining devices), capable of flight, and can detect mutants at long range. They possess vast physical strength, and their bodies are highly resistant to damage. Some are able to alter their physical forms or re-assemble and reactivate themselves after they have been destroyed.

Some Sentinel variants have the ability to learn from their experiences, developing their defenses during an engagement. Several groups of Sentinels have been created and/or led by a single massive Sentinel called Master Mold. Some Sentinels are also equipped with an inconspicuous logic loop in case they should go rogue to convince them that they are mutants as demonstrated in the Tri-Sentinel.

There are different types of Sentinels that appear in the comics:



The following are alternative versions of the Sentinels, which appear outside of regular Marvel canon.

In the "Age of Apocalypse" timeline, Bolivar Trask created the Sentinels with his wife Moira. These Sentinels are equipped with several body-mounted gun turrets, and their primary directive is to protect humans rather than to hunt mutants. They are capable of cooperating with mutants in order to further this mission. Later, the Sentinels are adapted by Weapon Omega to serve a reverse purpose, and now aid in the hunting of the human race.

In the "Days of Future Past" timeline, which takes place in an alternate future, the "Omega Sentinels" have advanced technologically and become the "de facto" rulers of the United States. The most powerful among them is Nimrod.

In the joke comic "Fred Hembeck Destroys the Marvel Universe", the X-Men are killed by silent, black, man-sized "Ninja Sentinels".

In the "Here Comes Tomorrow" future timeline, a Sentinel named Rover is Tom Skylark's companion and protector. After more than 150 years of being active, Rover has become self-aware and, possibly, capable of emotion.

In the "House of M" storyline, Magneto is victorious in a mutant/human war. The Sentinels are adapted by Sebastian Shaw, now the director of S.H.I.E.L.D., to serve a reverse purpose, and now aid in the hunting of sapien rebels.

In the MC2 timeline, Wild Thing encounters a Prime Sentinel that has accidentally been activated by a faulty microwave.

In the alternate reality of "X-Men: Ronin", the story is played out in Japan. A police unit called "Sentinel Force" designs, builds and pilots the robots. These are aesthetically similar to regular Sentinels, but each is subtly different from the others.

In the comic crossover "X-Men/Star Trek: Second Contact", the X-Men work with the crew of the "Enterprise"-E to battle Kang the Conqueror. An away team composed of Captain Picard, Deanna Troi, Nightcrawler and Colossus encounter an approximation of the "Days of Future Past" timeline, in which the Sentinels have merged with the Borg.

The Ultimate Marvel version of Sentinels were created by Bolivar Trask, were already in action in the "Ultimate X-Men" story arc, hunting down and killing mutants on the streets, in a program apparently openly and publicly acknowledged by the U.S. government. Later on, there were also the New Sentinels that were sixty of S.H.I.E.L.D.'s top agents in Sentinel battle armor and they were described to have enough hardware to take on a fleet of the old Sentinel models. A new breed of Sentinel robots, created by Trask under the Fenris twins' orders, was later created. After the events of the Ultimatum Wave, Nimrod Sentinels was deployed to hunt, capture or kill mutants that refused to turn themselves in. William Stryker, Jr., using Sentinel tech, later displayed an ability to summon a fleet of Sentinels after being attacked by the Shroud.




Sentinels have appeared as major antagonists in almost every video game to feature the X-Men:


Several different toys of Sentinels have been made since their introduction. One is the X-Men Classics 10" Sentinel by Toybiz. A "Build-A-Figure" version of the character was made in wave ten of the "Marvel Legends" line. The most recent Sentinel toy is made by Hasbro as part of the Marvel Universe line. Along with a large, unposeable statue, two Minimates figures have been made of the Sentinels. The first, a classic version, came with Rachel Summers in either her Phoenix or Marvel Girl guises. The second, based on "", comes with a red-haired "First Appearance" figure of Ryu.
In 2014, The Lego group released a set in the Marvel Super Heroes line titled "X-Men vs. the Sentinel", featuring the sentinel as a buildable figure, also including the Blackbird, Magneto, Wolverine, Storm, and Cyclops.




</doc>
<doc id="29532" url="https://en.wikipedia.org/wiki?curid=29532" title="Sebastian Shaw">
Sebastian Shaw

Sebastian Shaw is the name of:



</doc>
<doc id="29533" url="https://en.wikipedia.org/wiki?curid=29533" title="Savage Land">
Savage Land

The Savage Land is a hidden prehistoric land appearing in American comic books published by Marvel Comics. It is a tropical preserve hidden in Antarctica. It has appeared in many story arcs in "Uncanny X-Men" as well as related books.

The Savage Land first appeared as The Land Where Time Stands Still in "Marvel Mystery Comics" #22 (August 1941), in the tale "Khor, the Black Sorcerer" by Joe Simon, Jack Kirby, and Syd Shores.
It gained its familiar form and moniker in "X-Men" #10 (March 1965) courtesy of Stan Lee and Jack Kirby.

The Savage Land was created by the alien Nuwali at the behest of the other-dimensional, nigh-omnipotent aliens known as the Beyonders who sought to observe the process of evolution under relatively controlled conditions and had the Nuwali set up a number of game preserves on several planets. One of these planets was Earth during the Triassic period where the Nuwali chose a valley in Antarctica surrounded by active volcanoes, where they installed a number of advanced technological devices in order to maintain a tropical climate. The aliens then stocked the area with all manner of Earth life over the following several millennia. They also brought over the Man-Apes, earlier hominid ancestors of "Homo sapiens".

The Beyonders eventually grew bored with the experiment, and the Nuwali stopped maintaining the Savage Land during the Late Pleistocene (the Ice Age era). However, the self-maintaining technology that allowed the pocket of tropical climate was left running, and many species which became extinct in other areas of the Earth continued to thrive within the Savage Land.

Later on, a group of human survivors from Atlantis sailed to Antarctica before the "Great Cataclysm" which sank Atlantis into the ocean. There, they discovered a cavern where they found an immense climate-controlling device and harnessed the technology used to keep the Savage Land's volcanoes working. They named their location "Pangea", which is Atlantean for "paradise".

They mastered genetic engineering, which had been used on the Man-Apes when the Nuwali were still maintaining the Savage Land area. They used their genetic engineering techniques to transform other Savage Land inhabitants like the Golden People, the Lizard Men, the Reptile Men, the Tubantis, and others. The Atlanteans then forced them to work for them until these animal people revolted. After a time of war, the animal people demanded civil rights and the Atlanteans used technology to expand the Savage Land's surface area for the animal people to live in. When the Great Cataclysm struck, the Atlantean empire fell and thanks to the machines, the Savage Land locations were spared from sinking into the sea.

In more recent years, the Savage Land was rediscovered by Lord Robert Plunder, who took back a sample of the metal known as "anti-metal" or "Antarctic vibranium" with him. This mysterious metal had the ability to produce vibrations which would liquefy all other metals. Fleeing from those who sought to steal this discovery, Plunder took his eldest son Kevin with him for a second trip into the Savage Land. Unfortunately, the elder Plunder was killed by a local tribe of Man-Apes.

Kevin survived, thanks to the timely intervention of the orphaned sabretooth tiger later known as Zabu. He grew to adulthood in the Savage Land, becoming the adventurer known as Ka-Zar. Ka-Zar had many team-ups with the X-Men, who first revealed the Savage Land's existence, Spider-Man, and many other superheroes who had visited the Savage Land. He later met and married Shanna the She-Devil.

The Savage Land's existence is common knowledge throughout the world. At one time, there were press junkets, sponsored by the oil company Roxxon. "Daily Bugle" photographer Peter Parker was sent and helped uncover Roxxon's unethical and dangerous manipulation of the local resources.

At one point, Spider-Man teamed up with Ka-Zar to save Gwen Stacy from Kraven the Hunter and Gog at the time when her class and J. Jonah Jameson were visiting the Savage Land.

Many villains have threatened the Savage Land, including Sauron, Garokk, Magneto, and Thanos.

In issue #257 of "The Avengers" the Savage Land was decimated by an evil alien named Terminus (or one of his pawns) when he destroyed the machines that maintained the tropical climate. Many of the Savage Land's native people were saved from the ensuing destruction by M'rin: The Warlord of The Skies who took them into her own native dimension to safety. Ka-Zar, Shanna, and Zabu wandered until the High Evolutionary (with help from the X-Men, M'rin and Garokk) restored the region and its creatures, allowing them to return to the Savage Land with their newborn son. The other natives who had taken refuge in M'rin's dimension returned as well.

Sometime after that, Spider-Man had Devil Dinosaur and Moon-Boy emigrated to the Savage Land after rescuing them from Ringmaster.

Evidence in the pages of the "New Avengers" suggests that S.H.I.E.L.D. is operating in the Savage Land, mining vibranium while using the indigenous population as slave labor, but these operations have been classified, and the operation was apparently decimated by a missile strike from the Helicarrier during an attack by the New Avengers. The team only survived thanks to Iron Man's force field.

The Savage Land is featured in the limited series "Claws", serving as a place of revenge for Wolverine and Black Cat on Arcade and White Rabbit. After defeating the two villains, the heroes left them stranded.

In "", Cyclops and Emma Frost were vacationing there until Archangel contacted them about San Francisco looking like the 1960s.

Alyosha Kravinoff fled to the Savage Land after Punisher sabotaged his zoo.

During the "Secret Invasion" storyline, Ka-Zar and Shanna discover Skrulls mining for vibranium. The New Avengers and the Mighty Avengers head toward the Savage Land where a downed Skrull ship was sighted. Luke Cage opens the downed Skrull ship and a large group of Marvel superheroes with older appearances and costumes come out, speaking as if they believe themselves to be authentic. They soon break out into a fight where the Spider-Man from the ship is killed by a Tyrannosaurus and regresses to a Skrull. The Hawkeye from the ship is killed by Ronin and regresses to a Skrull. This causes the superheroes from the ship to scatter into the jungle. The New Avengers' Spider-Man is knocked away by a Tyrannosaurus and ends up confronting Ka-Zar, Shanna, Zabu, and Sauron as well as some of the other locals (ranging from the Sun People, the Swamp Men, and the Tree People). At the point where Spider-Man accuses Ka-Zar and Shanna for being Skrulls, the Captain America from the ship attacks who thinks the same thing for Spider-Man. When the Captain America is hit by a dart coated in some type of poison, it regresses to a Skrull named Pit'o Nilli and is then killed by Shanna. The ship's Beast is trapped underground with Wonder Man. The two try to escape together, but Beast betrays Wonder Man as the two are about to return to the surface. During this, Iron Man uses an abandoned scientific facility nearby to try and recreate his original armor. When it came to the confrontation with both Avengers teams, the Savage Land natives, and the heroes from the ship, Mister Fantastic and Abigail Brand used a laser to identify the heroes from the ship as Skrulls. Ka-Zar joined the Avengers teams into fighting the Skrulls in New York while Shanna and the other Savage Land natives hunted down the remaining Skrulls hiding out in the Savage Land.

After the events of "Second Coming" during the "Heroic Age" storyline, Cyclops takes some time off to go hunting in the Savage Land during which he encounters Steve Rogers. Steve Rogers suggests to Cyclops that he brings the X-Men out of the shadows and into the light as heroes. Steve Rogers also arranges to have the president award Scott the Presidential Medal of Freedom which sways the people of San Francisco to welcome the X-Men back.

Around the same time following their defeat after the hunt for "spiders" in the "Grim Hunt" storyline, the Kravinoff Family are also currently residing in the Savage Land.

It is later revealed that Miek and the other Imperials and Natives from Sakaar that came with Hulk back in "World War Hulk" had settled in the Savage Land constructing a village called New Imperia.

During the "Avengers vs. X-Men" storyline, Captain America ends up fighting Gambit in the Savage Land.

As part of the "Marvel NOW!" event, some of The Garden's evolution seeds had fallen into the Savage Land. While working to get it under control, the Avengers find that A.I.M. is also there where they test the extracted formula from one of the pods and tests it on their intern Dr. Jema. The formula puts a strain on Dr. Jema just as the Avengers arrive.

As part of the "All-New, All-Different Marvel", Magneto led a new team of X-Men to protect mutant-kind at all costs with their base in the Savage Land.

The Marvel Universe's version of the United Nations considers the Savage Land an international wildlife preserve and forbids any commercial exploitation of its resources.

There are some famous locations in the Savage Land:


There are many types of races in the Savage Land and Pangea. The Nuwali transported primitive man now known as the Man-Apes, which unlike the rest of the world thrived until the 21st century. The next arrivals were the Ancient Atlanteans who added the region as part of their empire. They used the Nuwali technology to mutate the Man-Apes into various Beast-Men to perform certain tasks. These slaves rebelled after the great Cataclysm and made Pangea their home. Many Atlanteans remained and their decedents became the various human tribes, with some clinging to the old ways and technology but most forget and resort to more primitive hunter-gatherer societies. Among the Savage Land races are:


In the "Age of Apocalypse" reality, the Savage Land houses Avalon, a secret haven for humans and mutants. A method to reach it exists, but it will only cost the refugee everything they own and even then, there is no guarantee of arriving alive. It is led by Destiny, a pacifist Juggernaut and Douglas Ramsey, the latter of whom provides a field that allows everybody to understand each other despite speaking different languages. Avalon was eventually found by Apocalypse forces and destroyed by the Shadow King who mind-controlled its inhabitants into killing each other. He was defeated, but casualties were high.

During the "Age of Ultron" storyline, the superhero resistance against Ultron had relocated to the Savage Land to come up with a plan to defeat Ultron.

In "Marvel Zombies Return", the Savage Land, like everywhere else on Earth, has been eaten by the superhuman zombies, with the surviving zombies musing that the Savage Land was their 'number one' meal in the aftermath, as it contained such an abundance of food that they were actually full for a full hour after eating there, as opposed to the usual ravenous hunger they feel. It is also the location of the final battle between the zombies and 'New Avengers'- three zombies who have beaten their hunger and the cyborg James Rhodes- at the storyline's conclusion, with Rhodes using one of his fingers to lure the zombies into an ambush.

In the "Earth X" universe, the Savage Land is where Magneto built his sanctuary called Sentinel City.

In the "House of M" reality created by an insane Scarlet Witch, the Savage Land was known as "Pangea." It is also known that Kevin Plunder has been granted political asylum in the United States for his human rights activism in this prehistoric land.

In the alternate future depicted in Marvel 2099, an alien attack floods much of the earth rendering the Savage Land the only habitable space. Thousands of refugees (including Miguel O'Hara and most of X-Nation and X-Men) make new homes here. It is not without its own dangers.

In the "Transformers" Marvel comics continuity, shortly after the "Ark"' spacecraft crashed on Earth 4 million years before the present day, the computer aboard the Ark detected Shockwave landing on the prehistoric Savage Land. The "Ark" used the last of its capabilities to revive the five Autobot warriors by scanning the Savage Land's dominant lifeform: dinosaurs, and rebuild them into the Dinobots. The Dinobots fought Shockwave, a battle that ended in permanent stalemate when Snarl brought down the mountain that Shockwave stood upon, knocking all of them into a tar pit. They remained deactivated until the year 1984. Since the Dinobots' alt-mode forms resemble creatures that were long-extinct by 4 million years ago, the Savage Land provided author Simon Furman a way to explain this within the canon timeline.

During the "Spider-Geddon" storyline, an alternate unidentified Earth has a version of Spider-Man that lives in the Savage Land and was raised by a tribe of giant spiders following an airplane crash. It was mentioned by Ka-Zar the Hunter to Wilson Fisk that his father killed the last of the Man-Apes.

In the Ultimate Marvel universe, the Savage Land is a large island somewhere in the southern hemisphere. It was originally said to have been created by Magneto, using theories and methods developed by Professor X, as the site for genetic experiments. Magneto's goal there was to create a new human race who would be less trouble to rule than the current one, that he decided to restart evolution from scratch, and control the process to his own specifications. As a result of this, at its current level of advancement, it has dinosaurs and that Magneto has shown no further interest in advancing the evolution of the Savage Land. It has remained in its dinosaur state since the departure of Professor X. This story is later revealed as false (see below).

Magneto's original base was on the Savage Land. When it was destroyed in the first arc of "Ultimate X-Men", the computer controlling the base gained self-awareness, and hijacked the genetic experiment project to create an army of nanotech-enhanced, zombie-like thralls. It planned to take over the world, but was stopped by Wolverine, Cyclops, and Kitty Pryde.

The Savage Land is now the home of Longshot, who managed to get there in a small boat which launched from Genosha. Longshot recently aided Magneto in breaking out of prison, and the two may be planning something.

In "Ultimates 3", it is revealed that the dinosaurs were conjured by Scarlet Witch as a result of her reality warping abilities and not by Magneto's creation. The aboriginal inhabitants were wiped out and only a small tribe of survivors including Ka-Zar and Shanna remain.

The inhabitants help the Ultimates remove the last of Magneto's forces as of "Ultimatum".

The Savage Land appears in a "What If" story where the Savage Land was terraforming and has taken over New York. Both Ka-Zar and Parnival sacrifice themselves to return New York to normal, with Shanna the only survivor of his "family."

Additionally, in the "What If" issues involving alternative outcomes to the Age of Ultron, a group composed of Wolverine, the Hulk, Peter Parker and a Ghost Rider venture to the Savage Land in order to prevent a Master Mold under the control of a future version of Ezekiel Stane from unleashing a wave of Stark armors on the world.







</doc>
<doc id="29536" url="https://en.wikipedia.org/wiki?curid=29536" title="Stephen Schneider">
Stephen Schneider

Stephen Henry Schneider (February 11, 1945 – July 19, 2010) was Professor of Environmental Biology and Global Change at Stanford University, a Co-Director at the Center for Environment Science and Policy of the Freeman Spogli Institute for International Studies and a Senior Fellow in the Stanford Woods Institute for the Environment. Schneider served as a consultant to federal agencies and White House staff in the Richard Nixon, Jimmy Carter, Ronald Reagan, George H. W. Bush, Bill Clinton, George W. Bush and Barack Obama administrations.

Schneider's research included modeling of the atmosphere, climate change, and the effect of global climate change on biological systems. Schneider was the founder and editor of the journal "Climatic Change" and authored or co-authored over 450 scientific papers and other publications. He was a Coordinating Lead Author in Working Group II Intergovernmental Panel on Climate Change (IPCC) Third Assessment Report and was engaged as a co-anchor of the Key Vulnerabilities Cross-Cutting Theme for the Fourth Assessment Report (AR4) at the time of his death. During the 1980s, Schneider emerged as a leading public advocate of sharp reductions of greenhouse gas emissions to combat global warming. In 2006 Professor Schneider was an Adelaide Thinker in Residence advising the South Australian Government of Premier Mike Rann on climate change and renewable energy policies. In ten years South Australia went from zero to 31% of its electricity generation coming from renewables.

An annual award for outstanding climate science communication was created in Schneider's honor after his death, by the Commonwealth Club of California. The Stephen Schneider Memorial Lecture of the American Geophysical Union honors Schneider's life and work. 

Schneider grew up on Long Island, New York. He studied engineering at Columbia University, receiving his bachelor's degree in mechanical engineering in 1966. In 1971, he earned a Ph.D. in mechanical engineering and plasma physics. Schneider studied the role of greenhouse gases and suspended particulate material on climate as a postdoctoral fellow at NASA's Goddard Institute for Space Studies.

In 1971, Schneider was second author on a "Science" paper with S. Ichtiaque Rasool titled "Atmospheric Carbon Dioxide and Aerosols: Effects of Large Increases on Global Climate" ("Science" 173, 138–141). This paper used a one-dimensional radiative transfer model to examine the competing effects of cooling from aerosols and warming from CO. The paper concluded that:

Carbon dioxide was predicted to have only a minor role. However, the model was very simple and the calculation of the CO effect was lower than other estimates by a factor of about three, as noted in a footnote to the paper.

The story made headlines in the "New York Times". Shortly afterwards, Schneider became aware that he had overestimated the cooling effect of aerosols, and underestimated the warming effect of CO by a factor of about three. He had mistakenly assumed that measurements of air particles he had taken near the source of pollution applied worldwide. He also found that much of the effect was due to natural aerosols which would not be affected by human activities, so the cooling effect of changes in industrial pollution would be much less than he had calculated. Having found that recalculation showed that global warming was the more likely outcome, he published a retraction of his earlier findings in 1974.

In a 1976 book "The Genesis Strategy" he discusses both long-term warming due to carbon dioxide and short-term cooling due to aerosols, and advocated for adopting policies that are resilient to future changes in climate.

Schneider was a frequent contributor to commercial and noncommercial print and broadcast media on climate and environmental issues, e.g., "Nova", "Planet Earth", "Nightline", "Today Show", "The Tonight Show", Bill Maher's shows, "Good Morning America", "Dateline", The Discovery Channel, as well as appearances on the British, Canadian and Australian Broadcasting Corporations.

Schneider commented about the frustrations and difficulties involved with assessing and communicating scientific ideas. In a January 2002 "Scientific American" article, he wrote:
In 1989, Schneider addressed the challenge scientists face trying to communicate complex, important issues without adequate time during media interviews. This citation sometimes was used by his critics to accuse him of supporting misuse of science for political goals:

For the original, together with Schneider's commentary on its misrepresentation, see also American Physical Society, "APS News" August/September 1996.


Schneider was married to the biologist Terry Root. Schneider was a survivor of an aggressive cancer, mantle cell lymphoma. He documented his struggle to conquer the condition, including applying his own knowledge of science to design his own treatment regime, in a self-published 2005 book, "The Patient from Hell". He died unexpectedly on July 19, 2010 after suffering a pulmonary embolism while returning from a scientific meeting in , Sweden.






</doc>
<doc id="29537" url="https://en.wikipedia.org/wiki?curid=29537" title="Scientific misconduct">
Scientific misconduct

Scientific misconduct is the violation of the standard codes of scholarly conduct and ethical behavior in the publication of professional scientific research. A "Lancet" review on "Handling of Scientific Misconduct in Scandinavian countries" provides the following sample definitions: (reproduced in The COPE report 1999.)

The consequences of scientific misconduct can be damaging for perpetrators and journal audience and for any individual who exposes it. In addition there are public health implications attached to the promotion of medical or other interventions based on false or fabricated research findings.

Three percent of the 3,475 research institutions that report to the US Department of Health and Human Services' Office of Research Integrity, indicate some form of scientific misconduct. However the ORI will only investigate allegations of impropriety where research was funded by federal grants. They routinely monitor such research publication for red flags and their investigation is subject to a statute of limitations. Other private organizations like the Committee of Medical Journal Editors (COJE) can only police their own members.

The validity of the methods and results of scientific papers are often scrutinized in journal clubs. In this venue, members can decide amongst themselves with the help of peers if a scientific paper's ethical standards are met.

According to David Goodstein of Caltech, there are motivators for scientists to commit misconduct, which are briefly summarised here.

The U.S. National Science Foundation defines three types of research misconduct: fabrication, falsification, and plagiarism.
Other types of research misconduct are also recognized:

Compared to other forms of scientific misconduct, image fraud (manipulation of images to distort their meaning) is of particular interest since it can frequently be detected by external parties. In 2006, the "Journal of Cell Biology" gained publicity for instituting tests to detect photo manipulation in papers that were being considered for publication. This was in response to the increased usage of programs such as Adobe Photoshop by scientists, which facilitate photo manipulation. Since then more publishers, including the Nature Publishing Group, have instituted similar tests and require authors to minimize and specify the extent of photo manipulation when a manuscript is submitted for publication. However, there is little evidence to indicate that such tests are applied rigorously. One Nature paper published in 2009 has subsequently been reported to contain around 20 separate instances of image fraud.

Although the type of manipulation that is allowed can depend greatly on the type of experiment that is presented and also differ from one journal to another, in general the following manipulations are not allowed:

All authors of a scientific publication are expected to have made reasonable attempts to check findings submitted to academic journals for publication.

Simultaneous submission of scientific findings to more than one journal or duplicate publication of findings is usually regarded as misconduct, under what is known as the Ingelfinger rule, named after the editor of the New England Journal of Medicine 1967-1977, Franz Ingelfinger.

Guest authorship (where there is stated authorship in the absence of involvement, also known as gift authorship) and ghost authorship (where the real author is not listed as an author) are commonly regarded as forms of research misconduct. In some cases coauthors of faked research have been accused of inappropriate behavior or research misconduct for failing to verify reports authored by others or by a commercial sponsor. Examples include the case of Gerald Schatten who co-authored with Hwang Woo-Suk, the case of Professor Geoffrey Chamberlain named as guest author of papers fabricated by Malcolm Pearce, (Chamberlain was exonerated from collusion in Pearce's deception) – and the coauthors with Jan Hendrik Schön at Bell Laboratories. More recent cases include that of Charles Nemeroff, then the editor-in-chief of "Neuropsychopharmacology", and a well-documented case involving the drug Actonel.

Authors are expected to keep all study data for later examination even after publication. The failure to keep data may be regarded as misconduct. Some scientific journals require that authors provide information to allow readers to determine whether the authors might have commercial or non-commercial conflicts of interest. Authors are also commonly required to provide information about ethical aspects of research, particularly where research involves human or animal participants or use of biological material. Provision of incorrect information to journals may be regarded as misconduct. Financial pressures on universities have encouraged this type of misconduct. The majority of recent cases of alleged misconduct involving undisclosed conflicts of interest or failure of the authors to have seen scientific data involve collaborative research between scientists and biotechnology companies.

In general, defining whether an individual is guilty of misconduct requires a detailed investigation by the individual's employing academic institution. Such investigations require detailed and rigorous processes and can be extremely costly. Furthermore, the more senior the individual under suspicion, the more likely it is that conflicts of interest will compromise the investigation. In many countries (with the notable exception of the United States) acquisition of funds on the basis of fraudulent data is not a legal offence and there is consequently no regulator to oversee investigations into alleged research misconduct. Universities therefore have few incentives to investigate allegations in a robust manner, or act on the findings of such investigations if they vindicate the allegation.

Well publicised cases illustrate the potential role that senior academics in research institutions play in concealing scientific misconduct. A King's College (London) internal investigation showed research findings from one of their researchers to be 'at best unreliable, and in many cases spurious' but the college took no action, such as retracting relevant published research or preventing further episodes from occurring. It was only 10 years later, when an entirely separate form of misconduct by the same individual was being investigated by the General Medical Council, that the internal report came to light.

In a more recent case an internal investigation at the National Centre for Cell Science (NCCS), Pune determined that there was evidence of misconduct by Dr. Gopal Kundu, but an external committee was then organised which dismissed the allegation, and the NCCS issued a memorandum exonerating the authors of all charges of misconduct. Undeterred by the NCCS exoneration, the relevant journal ("Journal of Biological Chemistry") withdrew the paper based on its own analysis.

Some academics believe that scientific colleagues who suspect scientific misconduct should consider taking informal action themselves, or reporting their concerns. This question is of great importance since much research suggests that it is very difficult for people to act or come forward when they see unacceptable behavior, unless they have help from their organizations. A "User-friendly Guide," and the existence of a confidential organizational ombudsman may help people who are uncertain about what to do, or afraid of bad consequences for their speaking up.

Journals are responsible for safeguarding the research record and hence have a critical role in dealing with suspected misconduct. This is recognised by the Committee on Publication Ethics (COPE) which has issued clear guidelines on the form (e.g. retraction) that concerns over the research record should take.

Evidence emerged in 2012 that journals learning of cases where there is strong evidence of possible misconduct, with issues potentially affecting a large portion of the findings, frequently fail to issue an expression of concern or correspond with the host institution so that an investigation can be undertaken. In one case the Journal of Clinical Oncology issued a Correction despite strong evidence that the original paper was invalid. 
In another case, Nature allowed a Corrigendum to be published despite clear evidence of image fraud. Subsequent Retraction of the paper required the actions of an independent whistleblower.

The cases of Joachim Boldt and Yoshitaka Fujii in anaesthesiology focussed attention on the role that journals play in perpetuating scientific fraud as well as how they can deal with it. In the Boldt case, the Editors-in-Chief of 18 specialist journals (generally anaesthesia and intensive care) made a joint statement regarding 88 published clinical trials conducted without Ethics Committee approval. In the Fujii case, involving nearly 200 papers, the journal Anesthesia & Analgesia, which published 24 of Fujii's papers, has accepted that its handling of the issue was inadequate. Following publication of a Letter to the Editor from Kranke and colleagues in April 2000, along with a non-specific response from Dr. Fujii, there was no follow-up on the allegation of data manipulation and no request for an institutional review of Dr. Fujii's research. Anesthesia & Analgesia went on to publish 11 additional manuscripts by Dr. Fujii following the 2000 allegations of research fraud, with Editor Steven Shafer stating in March 2012 that subsequent submissions to the Journal by Dr. Fujii should not have been published without first vetting the allegations of fraud. In April 2012 Shafer led a group of editors to write a joint statement, in the form of an ultimatum made available to the public, to a large number of academic institutions where Fujii had been employed, offering these institutions the chance to attest to the integrity of the bulk of the allegedly fraudulent papers.

The consequences of scientific fraud vary based on the severity of the fraud, the level of notice it receives, and how long it goes undetected. For cases of fabricated evidence, the consequences can be wide-ranging, with others working to confirm (or refute) the false finding, or with research agendas being distorted to address the fraudulent evidence. The Piltdown Man fraud is a case in point: The significance of the bona-fide fossils that were being found was muted for decades because they disagreed with Piltdown Man and the preconceived notions that those faked fossils supported. In addition, the prominent paleontologist Arthur Smith Woodward spent time at Piltdown each year until he died, trying to find more Piltdown Man remains. The misdirection of resources kept others from taking the real fossils more seriously and delayed the reaching of a correct understanding of human evolution. (The Taung Child, which should have been the death knell for the view that the human brain evolved first, was instead treated very critically because of its disagreement with the Piltdown Man evidence.)

In the case of Prof Don Poldermans, the misconduct occurred in reports of trials of treatment to prevent death and myocardial infarction in patients undergoing operations. The trial reports were relied upon to issue guidelines that applied for many years across North America and Europe.

In the case of Dr Alfred Steinschneider, two decades and tens of millions of research dollars were lost trying to find the elusive link between infant sleep apnea, which Steinschneider said he had observed and recorded in his laboratory, and sudden infant death syndrome (SIDS), of which he stated it was a precursor. The cover was blown in 1994, 22 years after Steinschneider's 1972 "Pediatrics" paper claiming such an association, when Waneta Hoyt, the mother of the patients in the paper, was arrested, indicted and convicted on five counts of second-degree murder for the smothering deaths of her five children. While that in itself was bad enough, the paper, presumably written as an attempt to save infants' lives, ironically was ultimately used as a defense by parents suspected in multiple deaths of their own children in cases of Münchausen syndrome by proxy. The 1972 "Pediatrics" paper was cited in 404 papers in the interim and is still listed on Pubmed without comment.

The potentially severe consequences for individuals who are found to have engaged in misconduct also reflect on the institutions that host or employ them and also on the participants in any peer review process that has allowed the publication of questionable research. This means that a range of actors in any case may have a motivation to suppress any evidence or suggestion of misconduct. Persons who expose such cases, commonly called whistleblowers, find themselves open to retaliation by a number of different means. These negative consequences for exposers of misconduct have driven the development of whistle blowers charters – designed to protect those who raise concerns. A whistleblower is almost always alone in their fight – their career becomes completely dependent on the decision about alleged misconduct. If the accusations prove false, their career is completely destroyed, but even in case of positive decision the career of the whistleblower can be under question: their reputation of "troublemaker" will prevent many employers from hiring them. There is no international body where a whistleblower could give their concerns. If a university fails to investigate suspected fraud or provides a fake investigation to save their reputation the whistleblower has no right of appeal.

With the advancement of the internet, there are now several tools available to aid in the detection of plagiarism and multiple publication within biomedical literature. One tool developed in 2006 by researchers in Dr. Harold Garner's laboratory at the University of Texas Southwestern Medical Center at Dallas is Déjà vu, an open-access database containing several thousand instances of duplicate publication. All of the entries in the database were discovered through the use of text data mining algorithm eTBLAST, also created in Dr. Garner's laboratory. The creation of Déjà vu and the subsequent classification of several hundred articles contained therein have ignited much discussion in the scientific community concerning issues such as ethical behavior, journal standards, and intellectual copyright. Studies on this database have been published in journals such as "Nature" and "Science", among others.

Other tools which may be used to detect fraudulent data include error analysis. Measurements generally have a small amount of error, and repeated measurements of the same item will generally result in slight differences in readings. These differences can be analyzed, and follow certain known mathematical and statistical properties. Should a set of data appear to be too faithful to the hypothesis, i.e., the amount of error that would normally be in such measurements does not appear, a conclusion can be drawn that the data may have been forged. Error analysis alone is typically not sufficient to prove that data have been falsified or fabricated, but it may provide the supporting evidence necessary to confirm suspicions of misconduct.

Kirby Lee and Lisa Bero suggest, "Although reviewing raw data can be difficult, time-consuming and expensive, having such a policy would hold authors more accountable for the accuracy of their data and potentially reduce scientific fraud or misconduct."

Andrew Wakefield, who claimed links between the MMR vaccine, autism and inflammatory bowel disease, was found guilty of dishonesty in his research and banned from medicine by the UK General Medical Council following an investigation by Brian Deer of the London Sunday Times.





</doc>
<doc id="29538" url="https://en.wikipedia.org/wiki?curid=29538" title="Set (card game)">
Set (card game)

Set (stylized as SET) is a real-time card game designed by Marsha Falco in 1974 and published by Set Enterprises in 1991. The deck consists of 81 unique cards that vary in four features across three possibilities for each kind of feature: number of shapes (one, two, or three), shape (diamond, squiggle, oval), shading (solid, striped, or open), and color (red, green, or purple). Each possible combination of features (e.g. a card with [three] [striped] [green] [diamonds]) appears as a card precisely "once" in the deck. 

In the game, certain combinations of three cards are said to make up a set. For each one of the four categories of features — color, number, shape, and shading — the three cards must display that feature as a) either all the same, or b) all different. Put another way: For each feature the three cards must "avoid" having two cards showing one version of the feature and the remaining card showing a different version. 

For example, 3 solid red diamonds, 2 solid green squiggles, and 1 solid purple oval form a set, because the shadings of the three cards are all the same, while the numbers, the colors, and the shapes among the three cards are all different.

For any "set", the number of features that are all the same and the number of features that are all different may break down as 0 the same + 4 different; or 1 the same + 3 different; or 2 the same + 2 different; or 3 the same + 1 different. (It cannot break down as 4 features the same + 0 different as the cards would be identical, and there are no identical cards in the Set deck.)

The game evolved out of a coding system that the designer used in her job as a geneticist. "Set" won American Mensa's "Mensa Select" award in 1991 and placed 9th in the 1995 "Deutscher Spiele Preis".

Several games can be played with these cards, all involving the concept of a "set". A set consists of three cards satisfying "all" of these conditions:

The rules of "Set" are summarized by: If you can sort a group of three cards into "two of ____ and one of ____", then it is not a set.

For example, these three cards form a set:

Given any two cards from the deck, there is one and only one other card that forms a set with them.

In the standard Set game, the dealer lays out cards on the table until either twelve are laid down or someone sees a set and calls "Set!". The player who called "Set" takes the cards in the set, and the dealer continues to deal out cards until twelve are on the table. A player who sees a set among the twelve cards calls "Set" and takes the three cards, and the dealer lays three more cards on the table. (To call out "set" and not pick one up quickly enough results in a penalty.) It is possible that there is no set among the twelve cards; in this case, the dealer deals out three more cards to make fifteen dealt cards, or eighteen or more, as necessary. This process of dealing by threes and finding sets continues until the deck is exhausted and there are no more sets on the table. At this point, whoever has collected the most sets wins.

Variants were included with the Set game that involve different mechanics to find sets, as well as different player interaction. Additional variants continue to be created by avid players of the game.



</doc>
<doc id="29539" url="https://en.wikipedia.org/wiki?curid=29539" title="Silver Star">
Silver Star

The Silver Star Medal is the United States Armed Forces's third-highest personal decoration for valor in combat. The Silver Star Medal is awarded primarily to members of the United States Armed Forces for gallantry in action against an enemy of the United States.

The Silver Star Medal (SSM) is the successor award to the "Citation Star" ( silver star) which was established by an Act of Congress on July 9, 1918, during World War I. On July 19, 1932, the Secretary of War approved the conversion of the "Citation Star" to the SSM with the original "Citation Star" incorporated into the center of the medal.

Authorization for the Silver Star Medal was placed into law by an Act of Congress for the U.S. Navy on August 7, 1942, and an Act of Congress for the U.S. Army on December 15, 1942. The current statutory authorization for the medal is Title 10 of the United States Code, for the U.S. Army, for the U.S. Air Force, and for the U.S. Navy.

The U.S. Army and Air Force award the medal as the "Silver Star". The U.S. Navy, Marine Corps, and Coast Guard continue to award the medal as the "Silver Star Medal". Since 21 December 2016, the Department of Defense (DoD) refers to the decoration as the Silver Star Medal.

The Silver Star Medal is awarded for gallantry, so long as the action does not justify the award of one of the next higher valor awards: the Distinguished Service Cross, the Navy Cross, the Air Force Cross, or the Coast Guard Cross. The gallantry displayed must have taken place while in action against an enemy of the United States, while engaged in military operations involving conflict with an opposing foreign force, or while serving with friendly foreign forces engaged in an armed conflict against an opposing armed force in which the United States is not a belligerent party.

The Silver Star Medal is awarded for singular acts of valor or heroism over a brief period, such as one or two days of a battle.

Air Force pilots and combat systems officers and Navy/Marine Corps naval aviators and flight officers flying fighter aircraft, are often considered eligible to receive the Silver Star upon becoming an ace (i.e., having five or more confirmed aerial kills), which entails the pilot and, in multi-seat fighters, the weapons system officer or radar intercept officer, intentionally and successfully risking his life multiple times under combat conditions and emerging victorious. However, during the Vietnam War, the last conflict to produce U.S. fighter aces: an Air Force pilot and two navigators/weapon systems officers (who were later retrained as Air Force pilots), a naval aviator and a naval flight officer/radar intercept officer who had achieved this distinction, were eventually awarded the Air Force Cross and Navy Cross, respectively, in addition to SSMs previously awarded for earlier aerial kills.


The Silver Star Medal is a gold five-pointed star, in circumscribing diameter with a laurel wreath encircling rays from the center and a diameter silver star superimposed in the center. The pendant is suspended from a rectangular shaped metal loop with rounded corners. The reverse has the inscription "FOR GALLANTRY IN ACTION". The ribbon is wide and consists of the following stripes: Old Glory red (center stripe); proceeding outward in pairs white; ultramarine blue; white; and ultramarine blue.

Second and subsequent awards of the Silver Star Medal are denoted by bronze or silver oak leaf clusters in the Army and Air Force and by gold or silver inch stars in the Navy, Marine Corps, and Coast Guard.

The Department of Defense does not keep extensive records for the Silver Star Medal. Independent groups estimate that between 100,000 and 150,000 SSMs have been awarded since the decoration was established. Colonel David Hackworth who was awarded ten SSMs while serving in the Army during the Korean War and Vietnam War, is likely to be the person awarded the most SSMs. Donald H. Russell, a civilian Vought F4U Corsair technical support engineer attached to a Marine Corps fighter wing, received the SSM for his actions aboard after the carrier was attacked by a Japanese dive bomber in March 1945.
Wyatt Waldron was a Marine with 3rd Battalion 4th Marines and received the Silver Star Medal, the Purple Heart and two other medals of Valor during his three combat deployments.In the fall of 1944,the American president’s close adviser Harry Hopkins, the US Ambassador in Moscow W. Averell Harriman and a military attaché presented Silver Star to Alexey Voloshin, who was the first to cross the Dnieper with his battery. 

Three Army nurses that served in World War I were cited in 1919 and 1920 with Citation Stars for gallantry in attending to the wounded while under artillery fire in July 1918. In 2007, it was discovered that they had never been awarded their Citation Stars. The three nurses (Army nurses served without rank until 1920) were awarded the Silver Star Medal posthumously:


An unknown number of servicewomen received the award in World War II. Four Army nurses serving in Italy during the war—First Lieutenant Mary Roberts, Second Lieutenant Elaine Roe, Second Lieutenant Rita Virginia Rourke, and Second Lieutenant Ellen Ainsworth (posthumous)—became the first women recipients of the Silver Star, all cited for their bravery in evacuating the 33rd Field Hospital at Anzio on February 10, 1944. Later that same year, Corporal Magdalena Leones, a Filipino American, received the medal for clandestine activities on Luzon; , she is the only female Asian American to receive a Silver Star.

The next known servicewomen to receive the Silver Star is Army National Guard Sergeant Leigh Ann Hester in 2005, for gallantry during an insurgent ambush on a convoy in Iraq and Army Specialist Monica Lin Brown in March 2008, for extraordinary heroism as a combat medic in the War in Afghanistan.

Notable recipients include:

















</doc>
<doc id="29540" url="https://en.wikipedia.org/wiki?curid=29540" title="Single UNIX Specification">
Single UNIX Specification

The Single UNIX Specification (SUS) is the collective name of a family of standards for computer operating systems, compliance with which is required to qualify for using the "UNIX" trademark. The core specifications of the SUS are developed and maintained by the Austin Group, which is a joint working group of IEEE, ISO JTC 1 SC22 and The Open Group. If an operating system is submitted to The Open Group for certification, and passes conformance tests, then it is deemed to be compliant with a UNIX standard such as UNIX 98 or UNIX 03.

Very few BSD and Linux-based operating systems are submitted for compliance with the Single UNIX Specification, although system developers generally aim for compliance with POSIX standards, which form the core of the Single UNIX Specification.

The SUS emerged from a mid-1980s project to standardize operating system interfaces for software designed for variants of the Unix operating system. The need for standardization arose because enterprises using computers wanted to be able to develop programs that could be used on the computer systems of different manufacturers without reimplementing the programs. Unix was selected as the basis for a standard system interface partly because it was manufacturer-neutral.

In 1988, these standards became IEEE 1003 (also registered as ISO/IEC 9945), or POSIX, which loosely stands for Portable Operating System Interface.

In the early 1990s, a separate effort known as the Common API Specification or Spec 1170 was initiated by several major vendors, who formed the COSE alliance in the wake of the Unix wars. This specification became more popular because it was available at no cost, whereas the IEEE charged a substantial fee for access to the POSIX specification. Management over these specifications was assigned to X/Open who also received the Unix trademark from Novell in 1993. Unix International (UI) merged into Open Software Foundation (OSF) in 1994 only to merge with X/Open to form The Open Group in 1996.

This was a repackaging of the X/Open Portability Guide (XPG), Issue 4, Version 2.

In 1995, the Open Group released the Single UNIX Specification Version 1, 1995 Edition.

This specification consisted of:
and was at the core of the UNIX 95 brand.

In 1997, the Open Group released the Single UNIX Specification Version 2.

This specification consisted of:
and was at the core of the UNIX 98 brand.

Beginning in 1998, a joint working group known as the Austin Group began to develop the combined standard that would be known as the Single UNIX Specification Version 3 and as POSIX:2001 (formally: IEEE Std 1003.1-2001). It was released on January 30, 2002.

This standard consisted of:
and is at the core of the UNIX 03 brand.

In 2004, a new edition of the POSIX:2001 standard was released, incorporating two technical corrigenda. It is called POSIX:2004 (formally: IEEE Std 1003.1-2004).

In December 2008, the Austin Group published a new major revision, known as POSIX:2008 (formally: IEEE Std 1003.1-2008). This is the core of the Single UNIX Specification, Version 4 (SUSv4).

This standard consists of:

The Technical Corrigendum 1 is mostly targeting internationalization and it introduces a role-based access model. It was published in 2012 for the Unix Base specification and it is registered as the 2013 Edition of POSIX 2008. A trademark "UNIX V7" (not to be confused with V7 UNIX, the version of Research Unix from 1979) has been created to mark compliance with SUS Version 4.

The Technical Corrigendum 2 has been published in September 2016, leading into "IEEE Std 1003.1-2008, 2016 Edition" and "Single UNIX Specification, Version 4, 2016 Edition".

In January 2018 an "administrative rollup" edition, susv4-2018, was released. It incorporates Single UNIX Specification version 4 TC1 and TC2, and is technically identical to the 2016 edition.

SUSv3 totals some 3700 pages, which are thematically divided into four main parts:


The standard user command line and scripting interface is the POSIX shell, an extension of the Bourne Shell based on an early version of the Korn Shell. Other user-level programs, services and utilities include awk, echo, ed, vi, and hundreds of others. Required program-level services include basic I/O (file, terminal, and network) services. A test suite accompanies the standard. It is called PCTS or the POSIX Certification Test Suite.

Additionally, SUS includes CURSES (XCURSES) specification, which specifies 372 functions and 3 header files. All in all, SUSv3 specifies 1742 interfaces.

Note that a system need not include source code derived in any way from AT&T Unix to meet the specification. For instance, IBM OS/390, now z/OS, qualifies as a "Unix" despite having no code in common.

There are three official marks for conforming systems

Older UNIX standards (superseded)

AIX 5L V5.2 with some updates, AIX 5L V5.3 and AIX 6.1, are registered as UNIX 03 compliant. AIX 5L V5.2 is registered as UNIX 98 compliant.

EulerOS 2.0 for the x86-64 architecture was certified as UNIX 03 compliant. The UNIX 03 conformance statement shows that the standard C compiler is from the GNU Compiler Collection (gcc), and that the system is a Linux distribution of the Red Hat family.

HP-UX 11i V3 Release B.11.31 is registered as UNIX 03 compliant. Previous releases are registered as UNIX 95.

HP-UX 11i features also provide partial conformance to the UNIX 98 specification.

Apple's macOS (previously known as Mac OS X or OS X) is a UNIX 03 registered product,
first becoming registered with Mac OS X 10.5 "Leopard" on October 26, 2007 (when run on Macs with Intel processors). All newer versions of macOS (except Mac OS X 10.7 "Lion") have been registered.

Solaris 11.4 and later are registered as UNIX v7 compliant; Solaris is the only system to be registered as v7 compliant . Solaris 11 and Solaris 10 are registered as UNIX 03 compliant on 32-bit and 64-bit x86 (X86-64) and SPARC systems. Solaris 8 and 9 are registered as UNIX 98 compliant on 32-bit x86 and SPARC systems; 64-bit x86 systems are not supported.

Solaris 2.5.1 was also registered as UNIX 95 compliant on the PReP PowerPC platform in 1996, but the product was withdrawn before more than a few dozen copies had been sold.

IBM z/OS 1.2 and higher is registered as UNIX 95 compliant.
z/OS 1.9, released on September 28, 2007, and subsequent releases "better align" with UNIX 03.

The last Reliant UNIX versions were registered as UNIX 95 compliant (XPG4 hard branding).

Inspur K-UX 2.0 and 3.0 for the x86-64 architecture were certified as UNIX 03 compliant. The UNIX 03 conformance statement for Inspur K-UX 2.0 and 3.0 shows that the standard C compiler is from the GNU Compiler Collection (gcc), and that the system is a Linux distribution of the Red Hat family.

UnixWare 7.1.3 is registered as UNIX 95 compliant.
SCO OpenServer 5 is registered as UNIX 93 compliant.

Tru64 UNIX V5.1A and later are registered as UNIX 98 compliant.

Other operating systems registered as UNIX 95 or UNIX 93 compliant:

Developers and vendors of Unix-like operating systems such as Linux, FreeBSD, and MINIX, typically do not certify their distributions and do not install full POSIX utilities by default. Sometimes, SUS compliance can be improved by installing additional packages, but very few Linux systems can be configured to be completely conformant.

Darwin, the open source subset of macOS, has behavior that can be set to comply with UNIX 03.

FreeBSD previously had a "C99 and POSIX Conformance Project" which aimed for compliance with a subset of the Single UNIX Specification, and documentation where there were differences.

For Linux, the Linux Standard Base was formed in 2001 as an attempt to standardize the internal structures of Linux-based systems for increased compatibility. It is based on the POSIX specifications, the Single UNIX Specification, and other open standards, and also extends them in several areas; but there are some conflicts between the LSB and The POSIX standards. However, although these standards are commonly accepted, few Linux distributions actually go through certification as LSB compliant.




</doc>
<doc id="29544" url="https://en.wikipedia.org/wiki?curid=29544" title="Scientific Revolution">
Scientific Revolution

The Scientific Revolution was a series of events that marked the emergence of modern science during the early modern period, when developments in mathematics, physics, astronomy, biology (including human anatomy) and transformed the views of society about nature. The Scientific Revolution took place in Europe towards the end of the Renaissance period and continued through the late 18th century, influencing the intellectual social movement known as the Enlightenment. While its dates are debated, the publication in 1543 of Nicolaus Copernicus's "De revolutionibus orbium coelestium" ("On the Revolutions of the Heavenly Spheres") is often cited as marking the beginning of the Scientific Revolution.

The concept of a scientific revolution taking place over an extended period emerged in the eighteenth century in the work of Jean Sylvain Bailly, who saw a two-stage process of sweeping away the old and establishing the new. The beginning of the Scientific Revolution, the 'Scientific Renaissance', was focused on the recovery of the knowledge of the ancients; this is generally considered to have ended in 1632 with publication of Galileo's "Dialogue Concerning the Two Chief World Systems". The completion of the Scientific Revolution is attributed to the "grand synthesis" of Isaac Newton's 1687 "Principia". The work formulated the laws of motion and universal gravitation thereby completing the synthesis of a new cosmology. By the end of the 18th century, the Age of Enlightenment that followed Scientific Revolution had given way to the "Age of Reflection."

Great advances in science have been termed "revolutions" since the 18th century. In 1747, the French mathematician Alexis Clairaut wrote that "Newton was said in his own life to have created a revolution". The word was also used in the preface to Lavoisier's 1789 work announcing the discovery of oxygen. "Few revolutions in science have immediately excited so much general notice as the introduction of the theory of oxygen ... Lavoisier saw his theory accepted by all the most eminent men of his time, and established over a great part of Europe within a few years from its first promulgation."

In the 19th century, William Whewell described the revolution in science itself – the scientific method – that had taken place in the 15th-16th century. "Among the most conspicuous of the revolutions which opinions on this subject have undergone, is the transition from an implicit trust in the internal powers of man's mind to a professed dependence upon external observation; and from an unbounded reverence for the wisdom of the past, to a fervid expectation of change and improvement." This gave rise to the common view of the Scientific Revolution today:

The Scientific Revolution is traditionally assumed to start with the Copernican Revolution (initiated in 1543) and to be complete in the "grand synthesis" of Isaac Newton's 1687 "Principia". Much of the change of attitude came from Francis Bacon whose "confident and emphatic announcement" in the modern progress of science inspired the creation of scientific societies such as the Royal Society, and Galileo who championed Copernicus and developed the science of motion.

In the 20th century, Alexandre Koyré introduced the term "scientific revolution", centering his analysis on Galileo. The term was popularized by Butterfield in his "Origins of Modern Science". Thomas Kuhn's 1962 work "The Structure of Scientific Revolutions" emphasized that different theoretical frameworks—such as Einstein's relativity theory and Newton's theory of gravity, which it replaced—cannot be directly compared without meaning loss.

The period saw a fundamental transformation in scientific ideas across mathematics, physics, astronomy, and biology in institutions supporting scientific investigation and in the more widely held picture of the universe. The Scientific Revolution led to the establishment of several modern sciences. In 1984, Joseph Ben-David wrote:

Many contemporary writers and modern historians claim that there was a revolutionary change in world view. In 1611 the English poet, John Donne, wrote:
Mid-20th-century historian Herbert Butterfield was less disconcerted, but nevertheless saw the change as fundamental:
The history professor Peter Harrison attributes Christianity to having contributed to the rise of the Scientific Revolution:

The Scientific Revolution was built upon the foundation of ancient Greek learning and science in the Middle Ages, as it had been elaborated and further developed by Roman/Byzantine science and medieval Islamic science. Some scholars have noted a direct tie between "particular aspects of traditional Christianity" and the rise of science. 
The "Aristotelian tradition" was still an important intellectual framework in the 17th century, although by that time natural philosophers had moved away from much of it. Key scientific ideas dating back to classical antiquity had changed drastically over the years, and in many cases been discredited. The ideas that remained, which were transformed fundamentally during the Scientific Revolution, include:

It is important to note that ancient precedent existed for alternative theories and developments which prefigured later discoveries in the area of physics and mechanics; but in light of the limited number of works to survive translation in a period when many books were lost to warfare, such developments remained obscure for centuries and are traditionally held to have had little effect on the re-discovery of such phenomena; whereas the invention of the printing press made the wide dissemination of such incremental advances of knowledge commonplace. Meanwhile, however, significant progress in geometry, mathematics, and astronomy was made in medieval times.

It is also true that many of the important figures of the Scientific Revolution shared in the general Renaissance respect for ancient learning and cited ancient pedigrees for their innovations. Nicolaus Copernicus (1473–1543), Galileo Galilei (1564–1642), Kepler (1571–1630) and Newton (1642–1727), all traced different ancient and medieval ancestries for the heliocentric system. In the Axioms Scholium of his "Principia," Newton said its axiomatic three laws of motion were already accepted by mathematicians such as Huygens (1629–1695), Wallace, Wren and others. While preparing a revised edition of his "Principia", Newton attributed his law of gravity and his to a range of historical figures.

Despite these qualifications, the standard theory of the history of the Scientific Revolution claims that the 17th century was a period of revolutionary scientific changes. Not only were there revolutionary theoretical and experimental developments, but that even more importantly, the way in which scientists worked was radically changed. For instance, although intimations of the concept of inertia are suggested sporadically in ancient discussion of motion,
the salient point is that Newton's theory differed from ancient understandings in key ways, such as an external force being a requirement for violent motion in Aristotle's theory.

Under the scientific method as conceived in the 17th century, natural and artificial circumstances were set aside as a research tradition of systematic experimentation was slowly accepted by the scientific community. The philosophy of using an inductive approach to obtain knowledge—to abandon assumption and to attempt to observe with an open mind—was in contrast with the earlier, Aristotelian approach of deduction, by which analysis of known facts produced further understanding. In practice, many scientists and philosophers believed that a healthy mix of both was needed—the willingness to question assumptions, yet also to interpret observations assumed to have some degree of validity.

By the end of the Scientific Revolution the qualitative world of book-reading philosophers had been changed into a mechanical, mathematical world to be known through experimental research. Though it is certainly not true that Newtonian science was like modern science in all respects, it conceptually resembled ours in many ways. Many of the hallmarks of modern science, especially with regard to its institutionalization and professionalization, did not become standard until the mid-19th century.

The Aristotelian scientific tradition's primary mode of interacting with the world was through observation and searching for "natural" circumstances through reasoning. Coupled with this approach was the belief that rare events which seemed to contradict theoretical models were aberrations, telling nothing about nature as it "naturally" was. During the Scientific Revolution, changing perceptions about the role of the scientist in respect to nature, the value of evidence, experimental or observed, led towards a scientific methodology in which empiricism played a large, but not absolute, role.

By the start of the Scientific Revolution, empiricism had already become an important component of science and natural philosophy. Prior thinkers, including the early-14th-century nominalist philosopher William of Ockham, had begun the intellectual movement toward empiricism.

The term British empiricism came into use to describe philosophical differences perceived between two of its founders Francis Bacon, described as empiricist, and René Descartes, who was described as a rationalist. Thomas Hobbes, George Berkeley, and David Hume were the philosophy's primary exponents, who developed a sophisticated empirical tradition as the basis of human knowledge.

An influential formulation of empiricism was John Locke's "An Essay Concerning Human Understanding" (1689), in which he maintained that the only true knowledge that could be accessible to the human mind was that which was based on experience. He wrote that the human mind was created as a "tabula rasa", a "blank tablet," upon which sensory impressions were recorded and built up knowledge through a process of reflection.

The philosophical underpinnings of the Scientific Revolution were laid out by Francis Bacon, who has been called the father of empiricism. His works established and popularised inductive methodologies for scientific inquiry, often called the "Baconian method", or simply the scientific method. His demand for a planned procedure of investigating all things natural marked a new turn in the rhetorical and theoretical framework for science, much of which still surrounds conceptions of proper methodology today.

Bacon proposed a great reformation of all process of knowledge for the advancement of learning divine and human, which he called "Instauratio Magna" (The Great Instauration). For Bacon, this reformation would lead to a great advancement in science and a progeny of new inventions that would relieve mankind's miseries and needs. His "Novum Organum" was published in 1620. He argued that man is "the minister and interpreter of nature", that "knowledge and human power are synonymous", that "effects are produced by the means of instruments and helps", and that "man while operating can only apply or withdraw natural bodies; nature internally performs the rest", and later that "nature can only be commanded by obeying her". Here is an abstract of the philosophy of this work, that by the knowledge of nature and the using of instruments, man can govern or direct the natural work of nature to produce definite results. Therefore, that man, by seeking knowledge of nature, can reach power over it—and thus reestablish the "Empire of Man over creation", which had been lost by the Fall together with man's original purity. In this way, he believed, would mankind be raised above conditions of helplessness, poverty and misery, while coming into a condition of peace, prosperity and security.

For this purpose of obtaining knowledge of and power over nature, Bacon outlined in this work a new system of logic he believed to be superior to the old ways of syllogism, developing his scientific method, consisting of procedures for isolating the formal cause of a phenomenon (heat, for example) through eliminative induction. For him, the philosopher should proceed through inductive reasoning from fact to axiom to physical law. Before beginning this induction, though, the enquirer must free his or her mind from certain false notions or tendencies which distort the truth. In particular, he found that philosophy was too preoccupied with words, particularly discourse and debate, rather than actually observing the material world: "For while men believe their reason governs words, in fact, words turn back and reflect their power upon the understanding, and so render philosophy and science sophistical and inactive."

Bacon considered that it is of greatest importance to science not to keep doing intellectual discussions or seeking merely contemplative aims, but that it should work for the bettering of mankind's life by bringing forth new inventions, having even stated that "inventions are also, as it were, new creations and imitations of divine works". He explored the far-reaching and world-changing character of inventions, such as the printing press, gunpowder and the compass.

Bacon first described the experimental method.
William Gilbert was an early advocate of this method. He passionately rejected both the prevailing Aristotelian philosophy and the Scholastic method of university teaching. His book "De Magnete" was written in 1600, and he is regarded by some as the father of electricity and magnetism. In this work, he describes many of his experiments with his model Earth called the terrella. From these experiments, he concluded that the Earth was itself magnetic and that this was the reason compasses point north.

"De Magnete" was influential not only because of the inherent interest of its subject matter, but also for the rigorous way in which Gilbert described his experiments and his rejection of ancient theories of magnetism. According to Thomas Thomson, "Gilbert['s]... book on magnetism published in 1600, is one of the finest examples of inductive philosophy that has ever been presented to the world. It is the more remarkable, because it preceded the "Novum Organum" of Bacon, in which the inductive method of philosophizing was first explained."

Galileo Galilei has been called the "father of modern observational astronomy", the "father of modern physics", the "father of science", and "the Father of Modern Science". His original contributions to the science of motion were made through an innovative combination of experiment and mathematics.

Galileo was one of the first modern thinkers to clearly state that the laws of nature are mathematical. In "The Assayer" he wrote "Philosophy is written in this grand book, the universe ... It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures;..." His mathematical analyses are a further development of a tradition employed by late scholastic natural philosophers, which Galileo learned when he studied philosophy. He ignored Aristotelianism. In broader terms, his work marked another step towards the eventual separation of science from both philosophy and religion; a major development in human thought. He was often willing to change his views in accordance with observation. In order to perform his experiments, Galileo had to set up standards of length and time, so that measurements made on different days and in different laboratories could be compared in a reproducible fashion. This provided a reliable foundation on which to confirm mathematical laws using inductive reasoning.

Galileo showed an appreciation for the relationship between mathematics, theoretical physics, and experimental physics. He understood the parabola, both in terms of conic sections and in terms of the ordinate (y) varying as the square of the abscissa (x). Galilei further asserted that the parabola was the theoretically ideal trajectory of a uniformly accelerated projectile in the absence of friction and other disturbances. He conceded that there are limits to the validity of this theory, noting on theoretical grounds that a projectile trajectory of a size comparable to that of the Earth could not possibly be a parabola, but he nevertheless maintained that for distances up to the range of the artillery of his day, the deviation of a projectile's trajectory from a parabola would be only very slight.

Scientific knowledge, according to the Aristotelians, was concerned with establishing true and necessary causes of things. To the extent that medieval natural philosophers used mathematical problems, they limited social studies to theoretical analyses of local speed and other aspects of life. The actual measurement of a physical quantity, and the comparison of that measurement to a value computed on the basis of theory, was largely limited to the mathematical disciplines of astronomy and optics in Europe.

In the 16th and 17th centuries, European scientists began increasingly applying quantitative measurements to the measurement of physical phenomena on the Earth. Galileo maintained strongly that mathematics provided a kind of necessary certainty that could be compared to God's: "...with regard to those few [mathematical propositions] which the human intellect does understand, I believe its knowledge equals the Divine in objective certainty..."

Galileo anticipates the concept of a systematic mathematical interpretation of the world in his book "Il Saggiatore":

Aristotle recognized four kinds of causes, and where applicable, the most important of them is the "final cause". The final cause was the aim, goal, or purpose of some natural process or man-made thing. Until the Scientific Revolution, it was very natural to see such aims, such as a child's growth, for example, leading to a mature adult. Intelligence was assumed only in the purpose of man-made artifacts; it was not attributed to other animals or to nature.

In "mechanical philosophy" no field or action at a distance is permitted, particles or corpuscles of matter are fundamentally inert. Motion is caused by direct physical collision. Where natural substances had previously been understood organically, the mechanical philosophers viewed them as machines. As a result, Isaac Newton's theory seemed like some kind of throwback to "spooky action at a distance". According to Thomas Kuhn, Newton and Descartes held the teleological principle that God conserved the amount of motion in the universe:

Gravity, interpreted as an innate attraction between every pair of particles of matter, was an occult quality in the same sense as the scholastics' "tendency to fall" had been... By the mid eighteenth century that interpretation had been almost universally accepted, and the result was a genuine reversion (which is not the same as a retrogression) to a scholastic standard. Innate attractions and repulsions joined size, shape, position and motion as physically irreducible primary properties of matter.

Newton had also specifically attributed the inherent power of inertia to matter, against the mechanist thesis that matter has no inherent powers. But whereas Newton vehemently denied gravity was an inherent power of matter, his collaborator Roger Cotes made gravity also an inherent power of matter, as set out in his famous preface to the "Principia's" 1713 second edition which he edited, and contradicted Newton himself. And it was Cotes's interpretation of gravity rather than Newton's that came to be accepted.

The first moves towards the institutionalization of scientific investigation and dissemination took the form of the establishment of societies, where new discoveries were aired, discussed and published. The first scientific society to be established was the Royal Society of London. This grew out of an earlier group, centred around Gresham College in the 1640s and 1650s. According to a history of the College:

The scientific network which centred on Gresham College played a crucial part in the meetings which led to the formation of the Royal Society.

These physicians and natural philosophers were influenced by the "new science", as promoted by Francis Bacon in his "New Atlantis", from approximately 1645 onwards. A group known as "The Philosophical Society of Oxford" was run under a set of rules still retained by the Bodleian Library.

On 28 November 1660, the 1660 committee of 12 announced the formation of a "College for the Promoting of Physico-Mathematical Experimental Learning", which would meet weekly to discuss science and run experiments. At the second meeting, Robert Moray announced that the King approved of the gatherings, and a Royal charter was signed on 15 July 1662 creating the "Royal Society of London", with Lord Brouncker serving as the first President. A second Royal Charter was signed on 23 April 1663, with the King noted as the Founder and with the name of "the Royal Society of London for the Improvement of Natural Knowledge"; Robert Hooke was appointed as Curator of Experiments in November. This initial royal favour has continued, and since then every monarch has been the patron of the Society.

The Society's first Secretary was Henry Oldenburg. Its early meetings included experiments performed first by Robert Hooke and then by Denis Papin, who was appointed in 1684. These experiments varied in their subject area, and were both important in some cases and trivial in others. The society began publication of "Philosophical Transactions" from 1665, the oldest and longest-running scientific journal in the world, which established the important principles of scientific priority and peer review.

The French established the Academy of Sciences in 1666. In contrast to the private origins of its British counterpart, the Academy was founded as a government body by Jean-Baptiste Colbert. Its rules were set down in 1699 by King Louis XIV, when it received the name of 'Royal Academy of Sciences' and was installed in the Louvre in Paris.

As the Scientific Revolution was not marked by any single change, the following new ideas contributed to what is called the Scientific Revolution. Many of them were revolutions in their own fields.


For almost five millennia, the geocentric model of the Earth as the center of the universe had been accepted by all but a few astronomers. In Aristotle's cosmology, Earth's central location was perhaps less significant than its identification as a realm of imperfection, inconstancy, irregularity and change, as opposed to the "heavens" (Moon, Sun, planets, stars), which were regarded as perfect, permanent, unchangeable, and in religious thought, the realm of heavenly beings. The Earth was even composed of different material, the four elements "earth", "water", "fire", and "air", while sufficiently far above its surface (roughly the Moon's orbit), the heavens were composed of different substance called "aether". The heliocentric model that replaced it involved not only the radical displacement of the earth to an orbit around the sun, but its sharing a placement with the other planets implied a universe of heavenly components made from the same changeable substances as the Earth. Heavenly motions no longer needed to be governed by a theoretical perfection, confined to circular orbits.

Copernicus' 1543 work on the heliocentric model of the solar system tried to demonstrate that the sun was the center of the universe. Few were bothered by this suggestion, and the pope and several archbishops were interested enough by it to want more detail. His model was later used to create the calendar of Pope Gregory XIII. However, the idea that the earth moved around the sun was doubted by most of Copernicus' contemporaries. It contradicted not only empirical observation, due to the absence of an observable stellar parallax, but more significantly at the time, the authority of Aristotle.

The discoveries of Johannes Kepler and Galileo gave the theory credibility. Kepler was an astronomer who, using the accurate observations of Tycho Brahe, proposed that the planets move around the sun not in circular orbits, but in elliptical ones. Together with his other laws of planetary motion, this allowed him to create a model of the solar system that was an improvement over Copernicus' original system. Galileo's main contributions to the acceptance of the heliocentric system were his mechanics, the observations he made with his telescope, as well as his detailed presentation of the case for the system. Using an early theory of inertia, Galileo could explain why rocks dropped from a tower fall straight down even if the earth rotates. His observations of the moons of Jupiter, the phases of Venus, the spots on the sun, and mountains on the moon all helped to discredit the Aristotelian philosophy and the Ptolemaic theory of the solar system. Through their combined discoveries, the heliocentric system gained support, and at the end of the 17th century it was generally accepted by astronomers.

This work culminated in the work of Isaac Newton. Newton's "Principia" formulated the laws of motion and universal gravitation, which dominated scientists' view of the physical universe for the next three centuries. By deriving Kepler's laws of planetary motion from his mathematical description of gravity, and then using the same principles to account for the trajectories of comets, the tides, the precession of the equinoxes, and other phenomena, Newton removed the last doubts about the validity of the heliocentric model of the cosmos. This work also demonstrated that the motion of objects on Earth and of celestial bodies could be described by the same principles. His prediction that the Earth should be shaped as an oblate spheroid was later vindicated by other scientists. His laws of motion were to be the solid foundation of mechanics; his law of universal gravitation combined terrestrial and celestial mechanics into one great system that seemed to be able to describe the whole world in mathematical formulae.

As well as proving the heliocentric model, Newton also developed the theory of gravitation. In 1679, Newton began to consider gravitation and its effect on the orbits of planets with reference to Kepler's laws of planetary motion. This followed stimulation by a brief exchange of letters in 1679–80 with Robert Hooke, who had been appointed to manage the Royal Society's correspondence, and who opened a correspondence intended to elicit contributions from Newton to Royal Society transactions. Newton's reawakening interest in astronomical matters received further stimulus by the appearance of a comet in the winter of 1680–1681, on which he corresponded with John Flamsteed. After the exchanges with Hooke, Newton worked out proof that the elliptical form of planetary orbits would result from a centripetal force inversely proportional to the square of the radius vector (see Newton's law of universal gravitation – History and "De motu corporum in gyrum"). Newton communicated his results to Edmond Halley and to the Royal Society in "De motu corporum in gyrum", in 1684. This tract contained the nucleus that Newton developed and expanded to form the "Principia".

The "Principia" was published on 5 July 1687 with encouragement and financial help from Edmond Halley. In this work, Newton stated the three universal laws of motion that contributed to many advances during the Industrial Revolution which soon followed and were not to be improved upon for more than 200 years. Many of these advancements continue to be the underpinnings of non-relativistic technologies in the modern world. He used the Latin word "gravitas" (weight) for the effect that would become known as gravity, and defined the law of universal gravitation.

Newton's postulate of an invisible force able to act over vast distances led to him being criticised for introducing "occult agencies" into science. Later, in the second edition of the "Principia" (1713), Newton firmly rejected such criticisms in a concluding General Scholium, writing that it was enough that the phenomena implied a gravitational attraction, as they did; but they did not so far indicate its cause, and it was both unnecessary and improper to frame hypotheses of things that were not implied by the phenomena. (Here Newton used what became his famous expression "hypotheses non fingo").

The writings of Greek physician Galen had dominated European medical thinking for over a millennium. The Flemish scholar Vesalius demonstrated mistakes in the Galen's ideas. Vesalius dissected human corpses, whereas Galen dissected animal corpses. Published in 1543, Vesalius' "De humani corporis fabrica" was a groundbreaking work of human anatomy. It emphasized the priority of dissection and what has come to be called the "anatomical" view of the body, seeing human internal functioning as an essentially corporeal structure filled with organs arranged in three-dimensional space. This was in stark contrast to many of the anatomical models used previously, which had strong Galenic/Aristotelean elements, as well as elements of astrology.

Besides the first good description of the sphenoid bone, he showed that the sternum consists of three portions and the sacrum of five or six; and described accurately the vestibule in the interior of the temporal bone. He not only verified the observation of Etienne on the valves of the hepatic veins, but he described the vena azygos, and discovered the canal which passes in the fetus between the umbilical vein and the vena cava, since named ductus venosus. He described the omentum, and its connections with the stomach, the spleen and the colon; gave the first correct views of the structure of the pylorus; observed the small size of the caecal appendix in man; gave the first good account of the mediastinum and pleura and the fullest description of the anatomy of the brain yet advanced. He did not understand the inferior recesses; and his account of the nerves is confused by regarding the optic as the first pair, the third as the fifth and the fifth as the seventh.

Before Vesalius, the anatomical notes by Alessandro Achillini demonstrate a detailed description of the human body and compares what he has found during his dissections to what others like Galen and Avicenna have found and notes their similarities and differences. Niccolò Massa was an Italian anatomist who wrote an early anatomy text "Anatomiae Libri Introductorius" in 1536, described the cerebrospinal fluid and was the author of several medical works. Jean Fernel was a French physician who introduced the term "physiology" to describe the study of the body's function and was the first person to describe the spinal canal.

Further groundbreaking work was carried out by William Harvey, who published "De Motu Cordis" in 1628. Harvey made a detailed analysis of the overall structure of the heart, going on to an analysis of the arteries, showing how their pulsation depends upon the contraction of the left ventricle, while the contraction of the right ventricle propels its charge of blood into the pulmonary artery. He noticed that the two ventricles move together almost simultaneously and not independently like had been thought previously by his predecessors.

In the eighth chapter, Harvey estimated the capacity of the heart, how much blood is expelled through each pump of the heart, and the number of times the heart beats in a half an hour. From these estimations, he demonstrated that according to Gaelen's theory that blood was continually produced in the liver, the absurdly large figure of 540 pounds of blood would have to be produced every day. Having this simple mathematical proportion at hand—which would imply a seemingly impossible role for the liver—Harvey went on to demonstrate how the blood circulated in a circle by means of countless experiments initially done on serpents and fish: tying their veins and arteries in separate periods of time, Harvey noticed the modifications which occurred; indeed, as he tied the veins, the heart would become empty, while as he did the same to the arteries, the organ would swell up.

This process was later performed on the human body (in the image on the left): the physician tied a tight ligature onto the upper arm of a person. This would cut off blood flow from the arteries and the veins. When this was done, the arm below the ligature was cool and pale, while above the ligature it was warm and swollen. The ligature was loosened slightly, which allowed blood from the arteries to come into the arm, since arteries are deeper in the flesh than the veins. When this was done, the opposite effect was seen in the lower arm. It was now warm and swollen. The veins were also more visible, since now they were full of blood.

Various other advances in medical understanding and practice were made. French physician Pierre Fauchard started dentistry science as we know it today, and he has been named "the father of modern dentistry". Surgeon Ambroise Paré (c. 1510–1590) was a leader in surgical techniques and battlefield medicine, especially the treatment of wounds, and Herman Boerhaave (1668–1738) is sometimes referred to as a "father of physiology" due to his exemplary teaching in Leiden and his textbook "Institutiones medicae" (1708).

Chemistry, and its antecedent alchemy, became an increasingly important aspect of scientific thought in the course of the 16th and 17th centuries. The importance of chemistry is indicated by the range of important scholars who actively engaged in chemical research. Among them were the astronomer Tycho Brahe, the chemical physician Paracelsus, Robert Boyle, Thomas Browne and Isaac Newton. Unlike the mechanical philosophy, the chemical philosophy stressed the active powers of matter, which alchemists frequently expressed in terms of vital or active principles—of spirits operating in nature.

Practical attempts to improve the refining of ores and their extraction to smelt metals were an important source of information for early chemists in the 16th century, among them Georg Agricola (1494–1555), who published his great work "De re metallica" in 1556. His work describes the highly developed and complex processes of mining metal ores, metal extraction and metallurgy of the time. His approach removed the mysticism associated with the subject, creating the practical base upon which others could build.

English chemist Robert Boyle (1627–1691) is considered to have refined the modern scientific method for alchemy and to have separated chemistry further from alchemy. Although his research clearly has its roots in the alchemical tradition, Boyle is largely regarded today as the first modern chemist, and therefore one of the founders of modern chemistry, and one of the pioneers of modern experimental scientific method. Although Boyle was not the original discover, he is best known for Boyle's law, which he presented in 1662: the law describes the inversely proportional relationship between the absolute pressure and volume of a gas, if the temperature is kept constant within a closed system.

Boyle is also credited for his landmark publication "The Sceptical Chymist" in 1661, which is seen as a cornerstone book in the field of chemistry. In the work, Boyle presents his hypothesis that every phenomenon was the result of collisions of particles in motion. Boyle appealed to chemists to experiment and asserted that experiments denied the limiting of chemical elements to only the classic four: earth, fire, air, and water. He also pleaded that chemistry should cease to be subservient to medicine or to alchemy, and rise to the status of a science. Importantly, he advocated a rigorous approach to scientific experiment: he believed all theories must be tested experimentally before being regarded as true. The work contains some of the earliest modern ideas of atoms, molecules, and chemical reaction, and marks the beginning of the history of modern chemistry.


Important work was done in the field of optics. Johannes Kepler published "Astronomiae Pars Optica" ("The Optical Part of Astronomy") in 1604. In it, he described the inverse-square law governing the intensity of light, reflection by flat and curved mirrors, and principles of pinhole cameras, as well as the astronomical implications of optics such as parallax and the apparent sizes of heavenly bodies. "Astronomiae Pars Optica" is generally recognized as the foundation of modern optics (though the law of refraction is conspicuously absent).

Willebrord Snellius (1580–1626) found the mathematical law of refraction, now known as Snell's law, in 1621. Subsequently René Descartes (1596–1650) showed, by using geometric construction and the law of refraction (also known as Descartes' law), that the angular radius of a rainbow is 42° (i.e. the angle subtended at the eye by the edge of the rainbow and the rainbow's centre is 42°). He also independently discovered the law of reflection, and his essay on optics was the first published mention of this law.

Christiaan Huygens (1629–1695) wrote several works in the area of optics. These included the "Opera reliqua" (also known as "Christiani Hugenii Zuilichemii, dum viveret Zelhemii toparchae, opuscula posthuma") and the "Traité de la lumière".

Isaac Newton investigated the refraction of light, demonstrating that a prism could decompose white light into a spectrum of colours, and that a lens and a second prism could recompose the multicoloured spectrum into white light. He also showed that the coloured light does not change its properties by separating out a coloured beam and shining it on various objects. Newton noted that regardless of whether it was reflected or scattered or transmitted, it stayed the same colour. Thus, he observed that colour is the result of objects interacting with already-coloured light rather than objects generating the colour themselves. This is known as Newton's theory of colour. From this work he concluded that any refracting telescope would suffer from the dispersion of light into colours. The interest of the Royal Society encouraged him to publish his notes "On Colour" (later expanded into "Opticks"). Newton argued that light is composed of particles or "corpuscles" and were refracted by accelerating toward the denser medium, but he had to associate them with waves to explain the diffraction of light.

In his "Hypothesis of Light" of 1675, Newton posited the existence of the ether to transmit forces between particles. In 1704, Newton published "Opticks", in which he expounded his corpuscular theory of light. He considered light to be made up of extremely subtle corpuscles, that ordinary matter was made of grosser corpuscles and speculated that through a kind of alchemical transmutation "Are not gross Bodies and Light convertible into one another, ...and may not Bodies receive much of their Activity from the Particles of Light which enter their Composition?"

Dr. William Gilbert, in "De Magnete", invented the New Latin word "electricus" from "" ("elektron"), the Greek word for "amber". Gilbert undertook a number of careful electrical experiments, in the course of which he discovered that many substances other than amber, such as sulphur, wax, glass, etc., were capable of manifesting electrical properties. Gilbert also discovered that a heated body lost its electricity and that moisture prevented the electrification of all bodies, due to the now well-known fact that moisture impaired the insulation of such bodies. He also noticed that electrified substances attracted all other substances indiscriminately, whereas a magnet only attracted iron. The many discoveries of this nature earned for Gilbert the title of "founder of the electrical science". By investigating the forces on a light metallic needle, balanced on a point, he extended the list of electric bodies, and found also that many substances, including metals and natural magnets, showed no attractive forces when rubbed. He noticed that dry weather with north or east wind was the most favourable atmospheric condition for exhibiting electric phenomena—an observation liable to misconception until the difference between conductor and insulator was understood.

Robert Boyle also worked frequently at the new science of electricity, and added several substances to Gilbert's list of electrics. He left a detailed account of his researches under the title of "Experiments on the Origin of Electricity". Boyle, in 1675, stated that electric attraction and repulsion can act across a vacuum. One of his important discoveries was that electrified bodies in a vacuum would attract light substances, this indicating that the electrical effect did not depend upon the air as a medium. He also added resin to the then known list of electrics.

This was followed in 1660 by Otto von Guericke, who invented an early electrostatic generator. By the end of the 17th century, researchers had developed practical means of generating electricity by friction with an electrostatic generator, but the development of electrostatic machines did not begin in earnest until the 18th century, when they became fundamental instruments in the studies about the new science of electricity. The first usage of the word "electricity" is ascribed to Sir Thomas Browne in his 1646 work, "Pseudodoxia Epidemica". In 1729 Stephen Gray (1666–1736) demonstrated that electricity could be "transmitted" through metal filaments.

As an aid to scientific investigation, various tools, measuring aids and calculating devices were developed in this period.

John Napier introduced logarithms as a powerful mathematical tool. With the help of the prominent mathematician Henry Briggs their logarithmic tables embodied a computational advance that made calculations by hand much quicker. His Napier's bones used a set of numbered rods as a multiplication tool using the system of lattice multiplication. The way was opened to later scientific advances, particularly in astronomy and dynamics.

At Oxford University, Edmund Gunter built the first analog device to aid computation. The 'Gunter's scale' was a large plane scale, engraved with various scales, or lines. Natural lines, such as the line of chords, the line of sines and tangents are placed on one side of the scale and the corresponding artificial or logarithmic ones were on the other side. This calculating aid was a predecessor of the slide rule. It was William Oughtred (1575–1660) who first used two such scales sliding by one another to perform direct multiplication and division, and thus is credited as the inventor of the slide rule in 1622.

Blaise Pascal (1623–1662) invented the mechanical calculator in 1642. The introduction of his Pascaline in 1645 launched the development of mechanical calculators first in Europe and then all over the world. Gottfried Leibniz (1646–1716), building on Pascal's work, became one of the most prolific inventors in the field of mechanical calculators; he was the first to describe a pinwheel calculator, in 1685, and invented the Leibniz wheel, used in the arithmometer, the first mass-produced mechanical calculator. He also refined the binary number system, foundation of virtually all modern computer architectures.

John Hadley (1682–1744) was the inventor of the octant, the precursor to the sextant (invented by John Bird), which greatly improved the science of navigation.

Denis Papin (1647–1712) was best known for his pioneering invention of the steam digester, the forerunner of the steam engine. The first working steam engine was patented in 1698 by the English inventor Thomas Savery, as a "...new invention for raising of water and occasioning motion to all sorts of mill work by the impellent force of fire, which will be of great use and advantage for drayning mines, serveing townes with water, and for the working of all sorts of mills where they have not the benefitt of water nor constant windes." The invention was demonstrated to the Royal Society on 14 June 1699 and the machine was described by Savery in his book "The Miner's Friend; or, An Engine to Raise Water by Fire" (1702), in which he claimed that it could pump water out of mines. Thomas Newcomen (1664–1729) perfected the practical steam engine for pumping water, the Newcomen steam engine. Consequently, Thomas Newcomen can be regarded as a forefather of the Industrial Revolution.

Abraham Darby I (1678–1717) was the first, and most famous, of three generations of the Darby family who played an important role in the Industrial Revolution. He developed a method of producing high-grade iron in a blast furnace fueled by coke rather than charcoal. This was a major step forward in the production of iron as a raw material for the Industrial Revolution.

Refracting telescopes first appeared in the Netherlands in 1608, apparently the product of spectacle makers experimenting with lenses. The inventor is unknown but Hans Lippershey applied for the first patent, followed by Jacob Metius of Alkmaar. Galileo was one of the first scientists to use this new tool for his astronomical observations in 1609.

The reflecting telescope was described by James Gregory in his book "Optica Promota" (1663). He argued that a mirror shaped like the part of a conic section, would correct the spherical aberration that flawed the accuracy of refracting telescopes. His design, the "Gregorian telescope", however, remained un-built.

In 1666, Isaac Newton argued that the faults of the refracting telescope were fundamental because the lens refracted light of different colors differently. He concluded that light could not be refracted through a lens without causing chromatic aberrations. From these experiments Newton concluded that no improvement could be made in the refracting telescope. However, he was able to demonstrate that the angle of reflection remained the same for all colors, so he decided to build a reflecting telescope. It was completed in 1668 and is the earliest known functional reflecting telescope.

50 years later, John Hadley developed ways to make precision aspheric and parabolic objective mirrors for reflecting telescopes, building the first parabolic Newtonian telescope and a Gregorian telescope with accurately shaped mirrors. These were successfully demonstrated to the Royal Society.

The invention of the vacuum pump paved the way for the experiments of Robert Boyle and Robert Hooke into the nature of vacuum and atmospheric pressure. The first such device was made by Otto von Guericke in 1654. It consisted of a piston and an air gun cylinder with flaps that could suck the air from any vessel that it was connected to. In 1657, he pumped the air out of two conjoined hemispheres and demonstrated that a team of sixteen horses were incapable of pulling it apart. The air pump construction was greatly improved by Robert Hooke in 1658.

Evangelista Torricelli (1607–1647) was best known for his invention of the mercury barometer. The motivation for the invention was to improve on the suction pumps that were used to raise water out of the mines. Torricelli constructed a sealed tube filled with mercury, set vertically into a basin of the same substance. The column of mercury fell downwards, leaving a Torricellian vacuum above.

Surviving instruments from this period, tend to be made of durable metals such as brass, gold, or steel, although examples such as telescopes made of wood, pasteboard, or with leather components exist. Those instruments that exist in collections today tend to be robust examples, made by skilled craftspeople for and at the expense of wealthy patrons. These may have been commissioned as displays of wealth. In addition, the instruments preserved in collections may not have received heavy use in scientific work; instruments that had visibly received heavy use were typically destroyed, deemed unfit for display, or excluded from collections altogether. It is also postulated that the scientific instruments preserved in many collections were chosen because they were more appealing to collectors, by virtue of being more ornate, more portable, or made with higher-grade materials.

Intact air pumps are particularly rare. The pump at right included a glass sphere to permit demonstrations inside the vacuum chamber, a common use. The base was wooden, and the cylindrical pump was brass. Other vacuum chambers that survived were made of brass hemispheres.

Instrument makers of the late seventeenth and early eighteenth century were commissioned by organizations seeking help with navigation, surveying, warfare, and astronomical observation. The increase in uses for such instruments, and their widespread use in global exploration and conflict, created a need for new methods of manufacture and repair, which would be met by the Industrial Revolution.

People and key ideas that emerged from the 16th and 17th centuries:

The idea that modern science took place as a kind of revolution has been debated among historians. A weakness of the idea of scientific revolution is the lack of a systematic approach to the question of knowledge in the period comprehended between the 14th and 17th centuries, leading to misunderstandings on the value and role of modern authors. From this standpoint, the continuity thesis is the hypothesis that there was no radical discontinuity between the intellectual development of the Middle Ages and the developments in the Renaissance and early modern period and has been deeply and widely documented by the works of scholars like Pierre Duhem, John Hermann Randall, Alistair Crombie and William A. Wallace, who proved the preexistence of a wide range of ideas used by the followers of the Scientific Revolution thesis to substantiate their claims. Thus, the idea of a scientific revolution following the Renaissance is—according to the continuity thesis—a myth. Some continuity theorists point to earlier intellectual revolutions occurring in the Middle Ages, usually referring to either a European Renaissance of the 12th century or a medieval Muslim scientific revolution, as a sign of continuity.

Another contrary view has been recently proposed by Arun Bala in his dialogical history of the birth of modern science. Bala proposes that the changes involved in the Scientific Revolution—the mathematical realist turn, the mechanical philosophy, the atomism, the central role assigned to the Sun in Copernican heliocentrism—have to be seen as rooted in multicultural influences on Europe. He sees specific influences in Alhazen's physical optical theory, Chinese mechanical technologies leading to the perception of the world as a machine, the Hindu-Arabic numeral system, which carried implicitly a new mode of mathematical atomic thinking, and the heliocentrism rooted in ancient Egyptian religious ideas associated with Hermeticism.

Bala argues that by ignoring such multicultural impacts we have been led to a Eurocentric conception of the Scientific Revolution. However, he clearly states: "The makers of the revolution—Copernicus, Kepler, Galileo, Descartes, Newton, and many others—had to selectively appropriate relevant ideas, transform them, and create new auxiliary concepts in order to complete their task... In the ultimate analysis, even if the revolution was rooted upon a multicultural base it is the accomplishment of Europeans in Europe." Critics note that lacking documentary evidence of transmission of specific scientific ideas, Bala's model will remain "a working hypothesis, not a conclusion".

A third approach takes the term "Renaissance" literally as a "rebirth". A closer study of Greek philosophy and Greek mathematics demonstrates that nearly all of the so-called revolutionary results of the so-called scientific revolution were in actuality restatements of ideas that were in many cases older than those of Aristotle and in nearly all cases at least as old as Archimedes. Aristotle even explicitly argues against some of the ideas that were espoused during the Scientific Revolution, such as heliocentrism. The basic ideas of the scientific method were well known to Archimedes and his contemporaries, as demonstrated in the well-known discovery of buoyancy. Atomism was first thought of by Leucippus and Democritus. Lucio Russo claims that science as a unique approach to objective knowledge was born in the Hellenistic period (c. 300 BC), but was extinguished with the advent of the Roman Empire. This approach to the Scientific Revolution reduces it to a period of relearning classical ideas that is very much an extension of the Renaissance. This view does not deny that a change occurred but argues that it was a reassertion of previous knowledge (a renaissance) and not the creation of new knowledge. It cites statements from Newton, Copernicus and others in favour of the Pythagorean worldview as evidence.

In more recent analysis of the Scientific Revolution during this period, there has been criticism of not only the Eurocentric ideologies spread, but also of the dominance of male scientists of the time. Science as we know it today, and the original theories that we base modern science on, was built by males, regardless of the input women might have made. The incorporation of women's work in the sciences during this time tends to be obscured. Scholars have tried to look into the participation of women in the 17th century in science, and even with sciences as simple as domestic knowledge women were making advances. With the limited history provided from texts of the period we are not completely aware if women were helping these scientists develop the ideas they did. Another idea to consider is the way this period influenced even the women scientists of the periods following it. Annie Jump Cannon was an astronomer who benefitted from the laws and theories developed from this period; she made several advances in the century following the Scientific Revolution. It was an important period for the future of science, including the incorporation of women into fields using the developments made.




</doc>
<doc id="29545" url="https://en.wikipedia.org/wiki?curid=29545" title="Salian dynasty">
Salian dynasty

The Salian dynasty (; also known as the Frankish dynasty after the family's Salian Frankish origin and position as dukes of Franconia) was a dynasty in the High Middle Ages. The dynasty provided four German Kings (1024–1125), all of whom went on to be crowned Holy Roman Emperor (1027–1125); as such, the term "Salic dynasty" is also used to refer to the Holy Roman Empire of the time as a separate term.

After the death of the last Saxon of the Ottonian Dynasty in 1024, the elective titles of King of the Germans and then three years later Holy Roman Emperor both passed to the first monarch of the Salian dynasty in the person of Conrad II, the only son of Count Henry of Speyer and Adelheid of Alsace (both territories in the Franconia of the day). He was elected German King in 1024 and crowned Holy Roman Emperor on 26 March 1027.

The four Salian kings of the dynasty—Conrad II, Henry III, Henry IV, and Henry V—ruled the Holy Roman Empire from 1027 to 1125, and firmly established their monarchy as a major European power. They achieved the development of a permanent administrative system based on a class of public officials answerable to the crown.

Werner of Worms and his son Duke Conrad the Red of Lorraine, who died in 955, founded the ancestral dynasty. Conrad the Red married Liutgarde, a daughter of Emperor Otto I. Their son Otto I, Duke of Carinthia ruled Carinthia from 978 to 1004.

Duke Otto had three sons: Bruno, who became Pope Gregory V; Conrad; and Henry, count of Speyer. Henry was the father of the first Salian Emperor Conrad II.

Pope Leo IX (in office 1049 to 1054) also had family ties to the dynasty, since his grandfather Hugo III was the brother of Adelheid, the grandmother of Henry III.

After the death of the last Saxon Emperor Henry II the first Salian regent Conrad II was elected by the majority of the Prince-electors and was crowned German king in Mainz on 8 September 1024. Early in 1026 Conrad went to Milan, where Ariberto, archbishop of Milan, crowned him king of Italy. When Rudolph III, King of Burgundy died 1032, Conrad II also claimed this kingship on the basis of an inheritance Henry II had extorted from the former in 1006. Despite some opposition, the Burgundian and Provençal nobles paid homage to Conrad in Zürich in 1034. This Kingdom of Burgundy would become known as the Kingdom of Arles under Conrad's successors.

Already in 1028 Conrad II had his son Henry III elected and anointed king of Germany. Henry's tenure led to an overstatement of previously unknown sacral kingship. So during this reign Speyer Cathedral was expanded to be the largest church in Western Christendom. Henry's conception of a legitimate power of royal disposition in the duchies was successful against the dukes, and thus secured royal control. However, in Lorraine, this led to years of conflict, from which Henry emerged as the winner. But also in southern Germany a powerful opposition group was formed in the years 1052–1055. 1046 Henry ended the papal schism, freed the Papacy from dependence on the Roman nobility, and laid the basis for its universal applicability. His early death in 1056 was long regarded as a disaster for the Empire.

The early Salians owed much of their success to their alliance with the Church, a policy begun by Otto I, which gave them the material support they needed to subdue rebellious dukes. In time, however, the Church came to regret this close relationship. The alliance broke down in 1075 during what came to be known as the Investiture Controversy (or "Investiture Dispute"), a struggle in which the reformist Pope, Gregory VII, demanded that Emperor Henry IV renounce his rights over the Church in Germany. The pope also attacked the concept of monarchy by divine right and gained the support of significant elements of the German nobility interested in limiting imperial absolutism. More important, the pope forbade ecclesiastical officials under pain of excommunication to support Henry as they had so freely done in the past. In the end, Henry IV journeyed to Canossa in northern Italy in 1077 to do penance and to receive absolution from the pope. However, he resumed the practice of lay investiture (appointment of religious officials by civil authorities) and arranged the election of an antipope (Antipope Clement III) in 1080.

The monarch's struggle with the papacy resulted in a war that ravaged through the Holy Roman Empire from 1077 until the Concordat of Worms in 1122. The reign of the last ruler of the Salian dynasty Henry V coincided with the final phase of the great Investiture Controversy, which had pitted pope against emperor. By the settlement of the Concordat of Worms, Henry V surrendered to the demands of the second generation of Gregorian reformers. This agreement stipulated that the pope would appoint high church officials but gave the German king the right to veto the papal choices. Imperial control of Italy was lost for a time, and the imperial crown became dependent on the political support of competing aristocratic factions. Feudalism also became more widespread as freemen sought protection by swearing allegiance to a lord. These powerful local rulers, having thereby acquired extensive territories and large military retinues, took over administration within their territories and organized it around an increasing number of castles. The most powerful of these local rulers came to be called princes rather than dukes.

According to the laws of the feudal system of the Holy Roman Empire, the king had no claims on the vassals of the other princes, only on those living within his family's territory. Lacking the support of the formerly independent vassals and weakened by the increasing hostility of the Church, the monarchy lost its pre-eminence. Thus the Investiture Contest strengthened local power in the Holy Roman Empire – in contrast to the trend in France and England, where centralized royal power grew. The Investiture Contest had an additional effect. The long struggle between emperor and pope hurt the Holy Roman Empire's intellectual life, in this period largely confined to monasteries, and the empire no longer led or even kept pace with developments occurring in France and Italy. For instance, no universities were founded in the Holy Roman Empire until the fourteenth century.

The first Hohenstaufen king Conrad III was a grandson of the Salian Henry IV, Holy Roman Emperor. (Agnes, Henry IV's daughter and Henry V's sister, was the heiress of Salian dynasty's lands: her first marriage produced the royal and imperial Hohenstaufen dynasty and her second marriage the ducal Babenberg potentates of Duchy of Austria which was elevated much due to such connections Privilegium Minus.)


Their regnal dates as emperor take into account elections and subsequent coronations.




</doc>
<doc id="29549" url="https://en.wikipedia.org/wiki?curid=29549" title="Self-replication">
Self-replication

Self-replication is any behavior of a dynamical system that yields construction of an identical or similar copy of itself. Biological cells, given suitable environments, reproduce by cell division. During cell division, DNA is replicated and can be transmitted to offspring during reproduction. Biological viruses can replicate, but only by commandeering the reproductive machinery of cells through a process of infection. Harmful prion proteins can replicate by converting normal proteins into rogue forms. Computer viruses reproduce using the hardware and software already present on computers. Self-replication in robotics has been an area of research and a subject of interest in science fiction. Any self-replicating mechanism which does not make a perfect copy (mutation) will experience genetic variation and will create variants of itself. These variants will be subject to natural selection, since some will be better at surviving in their current environment than others and will out-breed them.

Early research by John von Neumann established that replicators have several parts:


Exceptions to this pattern may be possible, although none have yet been achieved. For example, scientists have come close to constructing RNA that can be copied in an "environment" that is a solution of RNA monomers and transcriptase. In this case, the body is the genome, and the specialized copy mechanisms are external. The requirement for an outside copy mechanism has not yet been overcome, and such systems are more accurately characterized as "assisted replication" than "self-replication".

However, the simplest possible case is that only a genome exists. Without some specification of the self-reproducing steps, a genome-only system is probably better characterized as something like a crystal.

Recent research has begun to categorize replicators, often based on the amount of support they require.


The design space for machine replicators is very broad. A comprehensive study to date by Robert Freitas and Ralph Merkle has identified 137 design dimensions grouped into a dozen separate categories, including: (1) Replication Control, (2) Replication Information, (3) Replication Substrate, (4) Replicator Structure, (5) Passive Parts, (6) Active Subunits, (7) Replicator Energetics, (8) Replicator Kinematics, (9) Replication Process, (10) Replicator Performance, (11) Product Structure, and (12) Evolvability.

In computer science a quine is a self-reproducing computer program that, when executed, outputs its own code. For example, a quine in the Python programming language is:

A more trivial approach is to write a program that will make a copy of any stream of data that it is directed to, and then direct it at itself. In this case the program is treated as both executable code, and as data to be manipulated. This approach is common in most self-replicating systems, including biological life, and is simpler as it does not require the program to contain a complete description of itself.

In many programming languages an empty program is legal, and executes without producing errors or other output. The output is thus the same as the source code, so the program is trivially self-reproducing.

In geometry a self-replicating tiling is a tiling pattern in which several congruent tiles may be joined together to form a larger tile that is similar to the original. This is an aspect of the field of study known as tessellation. The "sphinx" hexiamond is the only known self-replicating pentagon. For example, four such concave pentagons can be joined together to make one with twice the dimensions. Solomon W. Golomb coined the term rep-tiles for self-replicating tilings.

In 2012, Lee Sallows identified rep-tiles as a special instance of a self-tiling tile set or setiset. A setiset of order "n" is a set of "n" shapes that can be assembled in "n" different ways so as to form larger replicas of themselves. Setisets in which every shape is distinct are called 'perfect'. A rep-"n" rep-tile is just a setiset composed of "n" identical pieces.

One form of natural self-replication that isn't based on DNA or RNA occurs in clay crystals. Clay consists of a large number of small crystals, and clay is an environment that promotes crystal growth. Crystals consist of a regular lattice of atoms and are able to grow if e.g. placed in a water solution containing the crystal components; automatically arranging atoms at the crystal boundary into the crystalline form. Crystals may have irregularities where the regular atomic structure is broken, and when crystals grow, these irregularities may propagate, creating a form of self-replication of crystal irregularities. Because these irregularities may affect the probability of a crystal breaking apart to form new crystals, crystals with such irregularities could even be considered to undergo evolutionary development.

It is a long-term goal of some engineering sciences to achieve a clanking replicator, a material device that can self-replicate. The usual reason is to achieve a low cost per item while retaining the utility of a manufactured good. Many authorities say that in the limit, the cost of self-replicating items should approach the cost-per-weight of wood or other biological substances, because self-replication avoids the costs of labor, capital and distribution in conventional manufactured goods.

A fully novel artificial replicator is a reasonable near-term goal.
A NASA study recently placed the complexity of a clanking replicator at approximately that of Intel's Pentium 4 CPU. That is, the technology is achievable with a relatively small engineering group in a reasonable commercial time-scale at a reasonable cost.

Given the currently keen interest in biotechnology and the high levels of funding in that field, attempts to exploit the replicative ability of existing cells are timely, and may easily lead to significant insights and advances.

A variation of self replication is of practical relevance in compiler construction, where a similar bootstrapping problem occurs as in natural self replication. A compiler (phenotype) can be applied on the compiler's own source code (genotype) producing the compiler itself. During compiler development, a modified (mutated) source is used to create the next generation of the compiler. This process differs from natural self-replication in that the process is directed by an engineer, not by the subject itself.

An activity in the field of robots is the self-replication of machines. Since all robots (at least in modern times) have a fair number of the same features, a self-replicating robot (or possibly a hive of robots) would need to do the following:


On a nano scale, assemblers might also be designed to self-replicate under their own power. This, in turn, has given rise to the "grey goo" version of Armageddon, as featured in such science fiction novels as "Bloom", "Prey", and "Recursion".

The Foresight Institute has published guidelines for researchers in mechanical self-replication. The guidelines recommend that researchers use several specific techniques for preventing mechanical replicators from getting out of control, such as using a broadcast architecture.

For a detailed article on mechanical reproduction as it relates to the industrial age see mass production.

Research has occurred in the following areas:


The goal of self-replication in space systems is to exploit large amounts of matter with a low launch mass. For example, an autotrophic self-replicating machine could cover a moon or planet with solar cells, and beam the power to the Earth using microwaves. Once in place, the same machinery that built itself could also produce raw materials or manufactured objects, including transportation systems to ship the products. Another model of self-replicating machine would copy itself through the galaxy and universe, sending information back.

In general, since these systems are autotrophic, they are the most difficult and complex known replicators. They are also thought to be the most hazardous, because they do not require any inputs from human beings in order to reproduce.

A classic theoretical study of replicators in space is the 1980 NASA study of autotrophic clanking replicators, edited by Robert Freitas.

Much of the design study was concerned with a simple, flexible chemical system for processing lunar regolith, and the differences between the ratio of elements needed by the replicator, and the ratios available in regolith. The limiting element was Chlorine, an essential element to process regolith for Aluminium. Chlorine is very rare in lunar regolith, and a substantially faster rate of reproduction could be assured by importing modest amounts.

The reference design specified small computer-controlled electric carts running on rails. Each cart could have a simple hand or a small bull-dozer shovel, forming a basic robot.

Power would be provided by a "canopy" of solar cells supported on pillars. The other machinery could run under the canopy.

A "casting robot" would use a robotic arm with a few sculpting tools to make plaster molds. Plaster molds are easy to make, and make precise parts with good surface finishes. The robot would then cast most of the parts either from non-conductive molten rock (basalt) or purified metals. An electric oven melted the materials.

A speculative, more complex "chip factory" was specified to produce the computer and electronic systems, but the designers also said that it might prove practical to ship the chips from Earth as if they were "vitamins".

Nanotechnologists in particular believe that their work will likely fail to reach a state of maturity until human beings design a self-replicating assembler of nanometer dimensions .

These systems are substantially simpler than autotrophic systems, because they are provided with purified feedstocks and energy. They do not have to reproduce them. This distinction is at the root of some of the controversy about whether molecular manufacturing is possible or not. Many authorities who find it impossible are clearly citing sources for complex autotrophic self-replicating systems. Many of the authorities who find it possible are clearly citing sources for much simpler self-assembling systems, which have been demonstrated. In the meantime, a Lego-built autonomous robot able to follow a pre-set track and assemble an exact copy of itself, starting from four externally provided components, was demonstrated experimentally in 2003 .

Merely exploiting the replicative abilities of existing cells is insufficient, because of limitations in the process of protein biosynthesis (also see the listing for RNA).
What is required is the rational design of an entirely novel replicator with a much wider range of synthesis capabilities.

In 2011, New York University scientists have developed artificial structures that can self-replicate, a process that has the potential to yield new types of materials. They have demonstrated that it is possible to replicate not just molecules like cellular DNA or RNA, but discrete structures that could in principle assume many different shapes, have many different functional features, and be associated with many different types of chemical species.

For a discussion of other chemical bases for hypothetical self-replicating systems, see alternative biochemistry.





</doc>
<doc id="29550" url="https://en.wikipedia.org/wiki?curid=29550" title="Shmuel Yosef Agnon">
Shmuel Yosef Agnon

Shmuel Yosef Agnon () (July 17, 1888 – February 17, 1970) was a Nobel Prize laureate writer and was one of the central figures of modern Hebrew fiction. In Hebrew, he is known by the acronym Shai Agnon (). In English, his works are published under the name S. Y. Agnon.

Agnon was born in Polish Galicia, then part of the Austro-Hungarian Empire, and later immigrated to Mandatory Palestine, and died in Jerusalem, Israel.

His works deal with the conflict between the traditional Jewish life and language and the modern world. They also attempt to recapture the fading traditions of the European "shtetl" (village). In a wider context, he also contributed to broadening the characteristic conception of the narrator's role in literature. Agnon shared the Nobel Prize with the poet Nelly Sachs in 1966.

Shmuel Yosef Halevi Czaczkes (later Agnon) was born in Buczacz (Polish spelling, pronounced "Buchach") or Butschatsch (German spelling), Polish Galicia (then within the Austro-Hungarian Empire), now Buchach, Ukraine. Officially, his date of birth on the Hebrew calendar was 18 Av 5648 (July 26), but he always said his birthday was on the Jewish fast day of Tisha B'Av, the Ninth of Av.

His father, Shalom Mordechai Halevy, was ordained as a rabbi, but worked in the fur trade, and had many connections among the Hasidim, His mother's side had ties to the Mitnagdim.

He did not attend school and was schooled by his parents. In addition to studying Jewish texts, Agnon studied writings of the Haskalah, and was also tutored in German. At the age of eight, he began to write in Hebrew and Yiddish, At the age of 15, he published his first poem – a Yiddish poem about the Kabbalist Joseph della Reina. He continued to write poems and stories in Hebrew and Yiddish, which were published in Galicia.
In 1908, he moved to Jaffa in Ottoman Palestine. The first story he published there was "Agunot" ("Forsaken Wives"), which appeared that same year in the journal "Ha`omer." He used the pen name "Agnon," derived from the title of the story, which he adopted as his official surname in 1924. In 1910, "Forsaken Wives" was translated into German. In 1912, at the urging of Yosef Haim Brenner, he published a novella, "Vehaya Ha'akov Lemishor" ("The Crooked Shall Be Made Straight").

In 1913, Agnon moved to Germany, where he met Esther Marx (1889-1973). They married in 1920 and had two children. In Germany he lived in Berlin and Bad Homburg vor der Höhe (1921–24). Salman Schocken, a businessman and later also publisher, became his literary patron and freed him from financial worries. From 1931 on, his work was published by Schocken Books, and his short stories appeared regularly in the newspaper "Haaretz", also owned by the Schocken family. In Germany, he continued to write short stories and collaborated with Martin Buber on an anthology of Hasidic stories. Many of his early books appeared in Buber's "Jüdischer Verlag" (Berlin). The mostly assimilated, secular German Jews, Buber and Franz Rosenzweig among them, considered Agnon to be a legitimate relic, being a religious man, familiar with Jewish scripture. Gershom Scholem called him "the Jews' Jew".

In 1924, a fire broke out in his home, destroying his manuscripts and rare book collection. This traumatic event crops up occasionally in his stories. Later that year, Agnon returned to Palestine and settled with his family in the Jerusalem neighborhood of Talpiot. In 1929, his library was destroyed again during anti-Jewish riots.

When his novel "Hachnasat Kalla" ("The Bridal Canopy") appeared in 1931 to great critical acclaim, Agnon's place in Hebrew literature was assured. In 1935, he published "Sippur Pashut" ("A Simple Story"), a novella set in Buchach at the end of the 19th century. Another novel, "Tmol Shilshom" ("Only Yesterday"), set in Eretz Yisrael (Israel) of the early 20th century, appeared in 1945.

Agnon's writing has been the subject of extensive academic research. Many leading scholars of Hebrew literature have published books and papers on his work, among them Baruch Kurzweil, Dov Sadan, Nitza Ben-Dov, Dan Miron, Dan Laor and Alan Mintz. Agnon writes about Jewish life, but with his own unique perspective and special touch. In his Nobel acceptance speech, Agnon claimed "Some see in my books the influences of authors whose names, in my ignorance, I have not even heard, while others see the influences of poets whose names I have heard but whose writings I have not read." He went on to detail that his primary influences were the stories of the Bible. Agnon acknowledged that he was also influenced by German literature and culture, and European literature in general, which he read in German translation. A collection of essays on this subject, edited in part by Hillel Weiss, with contributions from Israeli and German scholars, was published in 2010: "Agnon and Germany: The Presence of the German World in the Writings of S.Y. Agnon". The budding Hebrew literature also influenced his works, notably that of his friend, Yosef Haim Brenner. In Germany, Agnon also spent time with the Hebraists Hayim Nahman Bialik and Ahad Ha'am.

The communities he passed through in his life are reflected in his works:

Nitza Ben-Dov writes about Agnon's use of allusiveness, free-association and imaginative dream-sequences, and discusses how seemingly inconsequential events and thoughts determine the lives of his characters.

Some of Agnon's works, such as "The Bridal Canopy", "And the Crooked Shall Be Made Straight", and "The Doctor's Divorce", have been adapted for theatre. A play based on Agnon's letters to his wife, "Esterlein Yakirati", was performed at the Khan Theater in Jerusalem.

Agnon's writing often used words and phrases that differed from what would become established modern Hebrew. His distinct language is based on traditional Jewish sources, such as the Torah and the Prophets, Midrashic literature, the Mishnah, and other Rabbinic literature. Some examples include:

Bar-Ilan University has made a computerized concordance of his works in order to study his language.

Agnon was twice awarded the Bialik Prize for literature (1934 and 1950). He was also twice awarded the Israel Prize, for literature (1954 and 1958).

In 1966, he was awarded the Nobel Prize in Literature "for his profoundly characteristic narrative art with motifs from the life of the Jewish people". The prize was shared with German Jewish author Nelly Sachs. In his speech at the award ceremony, Agnon introduced himself in Hebrew: "As a result of the historic catastrophe in which Titus of Rome destroyed Jerusalem and Israel was exiled from its land, I was born in one of the cities of the Exile. But always I regarded myself as one who was born in Jerusalem".

In later years, Agnon's fame was such that when he complained to the municipality that traffic noise near his home was disturbing his work, the city closed the street to cars and posted a sign that read: "No entry to all vehicles, writer at work!"

 
Agnon died in Jerusalem on February 17, 1970. His daughter, Emuna Yaron, has continued to publish his work posthumously. Agnon's archive was transferred by the family to the National Library in Jerusalem. His home in Talpiot, built in 1931 in the Bauhaus style, was turned into a museum, "Beit Agnon." The study where he wrote many of his works was preserved intact. Agnon's image, with a list of his works and his Nobel Prize acceptance speech, appeared on the fifty-shekel bill, second series, in circulation from 1985 to 2014. The main street in Jerusalem's Givat Oranim neighborhood is called Sderot Shai Agnon, and a synagogue in Talpiot, a few blocks from his home, is named after him. Agnon is also memorialized in Buchach, now in Ukraine, where he was born. There is an extensive (relative to the size of the museum) exhibition in the Historical Museum in Buchach and, just a few yards away, a bust of Agnon is mounted on a pedestal in a plaza across the street from the house where he lived. The house itself is preserved and marked as the home where Agnon lived from birth till the age of (approximately) 19; the street that runs in front of the house is named "Agnon Street" (in Ukrainian).

Agnotherapy is a method developed in Israel to help elderly people express their feelings.

After Agnon's death, the former mayor of Jerusalem Mordechai Ish-Shalom initiated the opening of his home to the public. In the early 1980s, the kitchen and family dining room were turned into a lecture and conference hall, and literary and cultural evenings were held there. In 2005, the Agnon House Association in Jerusalem renovated the building, which reopened in January 2009. The house was designed by the German-Jewish architect Fritz Korenberg, who was also his neighbor.






In 1977 the Hebrew University published "Yiddish Works", a collection of stories and poems that Agnon wrote in Yiddish during 1903–1906.





</doc>
<doc id="29551" url="https://en.wikipedia.org/wiki?curid=29551" title="Steve Ditko">
Steve Ditko

Stephen J. Ditko (; November 2, 1927 – c. June 29, 2018) was an American comics artist and writer best known as the artist and co-creator, with Stan Lee, of the Marvel Comics superheroes Spider-Man and Doctor Strange.

Ditko studied under Batman artist Jerry Robinson at the Cartoonist and Illustrators School in New York City. He began his professional career in 1953, working in the studio of Joe Simon and Jack Kirby, beginning as an inker and coming under the influence of artist Mort Meskin. During this time, he then began his long association with Charlton Comics, where he did work in the genres of science fiction, horror, and mystery. He also co-created the superhero Captain Atom in 1960.

During the 1950s, Ditko also drew for Atlas Comics, a forerunner of Marvel Comics. He went on to contribute much significant work to Marvel. In 1966, after being the exclusive artist on "The Amazing Spider-Man" and the "Doctor Strange" feature in "Strange Tales", Ditko left Marvel for reasons he never specified.

Ditko continued to work for Charlton and also DC Comics, including a revamp of the long-running character the Blue Beetle, and creating or co-creating the Question, the Creeper, Shade the Changing Man, and Hawk and Dove. Ditko also began contributing to small independent publishers, where he created Mr. A, a hero reflecting the influence of Ayn Rand's philosophy of Objectivism. Ditko largely declined to give interviews, saying he preferred to communicate through his work.

Ditko was inducted into the comics industry's Jack Kirby Hall of Fame in 1990, and into the Will Eisner Award Hall of Fame in 1994.

Ditko was born on November 2, 1927 in Johnstown, Pennsylvania, the son of first-generation American Carpatho-Rusyn immigrants from the former Czechoslovakia (now Slovakia), father Stephen Ditko, an artistically talented master carpenter at a steel mill, and mother Anna, a homemaker. The second-oldest child in a working-class family, he was preceded by sister Anna Marie, and followed by sister Elizabeth and brother Patrick. Inspired by his father's love of newspaper comic strips, particularly Hal Foster's "Prince Valiant", Ditko found his interest in comics accelerated by the introduction of the superhero Batman in 1939, and by Will Eisner's "The Spirit", which appeared in a tabloid-sized comic-book insert in Sunday newspapers.

Ditko in junior high school was part of a group of students who crafted wooden models of German airplanes to aid civilian World War II aircraft-spotters. Upon graduating from Johnstown High School in 1945, he enlisted in the U.S. Army on October 26, 1945, and did military service in postwar Germany, where he drew comics for an Army newspaper.

Following his discharge, Ditko learned that his idol, Batman artist Jerry Robinson, was teaching at the Cartoonists and Illustrators School (later the School of Visual Arts) in New York City. Moving there in 1950, he enrolled in the art school under the G.I. Bill. Robinson found the young student "a very hard worker who really focused on his drawing" and someone who "could work well with other writers as well as write his own stories and create his own characters", and he helped Ditko acquire a scholarship for the following year. "He was in my class for two years, four or five days a week, five hours a night. It was very intense." Robinson, who invited artists and editors to speak with his class, once brought in Stan Lee, then editor of Marvel Comics' 1950s precursor Atlas Comics and, "I think that was when Stan first saw Steve's work."

Ditko began professionally illustrating comic books in early 1953, drawing writer Bruce Hamilton's science-fiction story "Stretching Things" for the Key Publications imprint Stanmor Publications, which sold the story to Ajax/Farrell, where it finally found publication in "Fantastic Fears" #5 (cover-dated Feb. 1954). Ditko's first published work was his second professional story, the six-page "Paper Romance" in "Daring Love" #1 (Oct. 1953), published by the Key imprint Gillmor Magazines.

Shortly afterward, Ditko found work at the studio of writer-artists Joe Simon and Jack Kirby, who had created Captain America and other characters. Beginning as an inker on backgrounds, Ditko was soon working with and learning from Mort Meskin, an artist whose work he had long admired. "Meskin was fabulous," Ditko once recalled. "I couldn't believe the ease with which he drew: strong compositions, loose pencils, yet complete; detail without clutter. I loved his stuff". Ditko's known assistant work includes aiding inker Meskin on the Jack Kirby pencil work of Harvey Comics' "Captain 3-D" #1 (Dec. 1953). For his own third published story, Ditko penciled and inked the six-page "A Hole in His Head" in "Black Magic" vol. 4, #3 (Dec. 1953), published by Simon & Kirby's Crestwood Publications imprint Prize Comics.

Ditko then began a long association with the Derby, Connecticut publisher Charlton Comics, a low-budget division of a company best known for song-lyric magazines. Beginning with the cover of "The Thing!" #12 (Feb. 1954) and the eight-page vampire story "Cinderella" in that issue, Ditko would continue to work intermittently for Charlton until the company's demise in 1986, producing science fiction, horror and mystery stories, as well as co-creating Captain Atom, with writer Joe Gill, in "Space Adventures" #33 (March 1960). He first went on hiatus from the company, and comics altogether, in mid-1954, when he contracted tuberculosis and returned to his parents' home in Johnstown to recuperate.

After he recovered and moved back to New York City in late 1955, Ditko began drawing for Atlas Comics, the 1950s precursor of Marvel Comics, beginning with the four-page "There'll Be Some Changes Made" in "Journey into Mystery" #33 (April 1956); this debut tale would be reprinted in Marvel's "Curse of the Weird" #4 (March 1994). Ditko would go on to contribute a large number of stories, many considered classic, to Atlas/Marvel's "Strange Tales" and the newly launched "Amazing Adventures", "Strange Worlds", "Tales of Suspense" and "Tales to Astonish", issues of which would typically open with a Kirby-drawn monster story, followed by one or two twist-ending thrillers or sci-fi tales drawn by Don Heck, Paul Reinman, or Joe Sinnott, all capped by an often-surreal, sometimes self-reflexive short by Ditko and writer-editor Stan Lee.

These Lee-Ditko short stories proved so popular that "Amazing Adventures" was reformatted to feature such stories exclusively beginning with issue #7 (Dec. 1961), when the comic was rechristened "Amazing Adult Fantasy", a name intended to reflect its more "sophisticated" nature, as likewise the new tagline "The magazine that respects your intelligence". Lee in 2009 described these "short, five-page filler strips that Steve and I did together", originally "placed in any of our comics that had a few extra pages to fill", as "odd fantasy tales that I'd dream up with O. Henry-type endings." Giving an early example of what would later be known as the "Marvel Method" of writer-artist collaboration, Lee said, "All I had to do was give Steve a one-line description of the plot and he'd be off and running. He'd take those skeleton outlines I had given him and turn them into classic little works of art that ended up being far cooler than I had any right to expect."

After Marvel Comics editor-in-chief Stan Lee obtained permission from publisher Martin Goodman to create a new "ordinary teen" superhero named "Spider-Man", Lee originally approached his leading artist, Jack Kirby. Kirby told Lee about his own 1950s character conception, variously called the Silver Spider and Spiderman, in which an orphaned boy finds a magic ring that gives him super powers. Comics historian Greg Theakston says Lee and Kirby "immediately sat down for a story conference" and Lee afterward directed Kirby to flesh out the character and draw some pages. "A day or two later", Kirby showed Lee the first six pages, and, as Lee recalled, "I hated the way he was doing it. Not that he did it badly — it just wasn't the character I wanted; it was too heroic".

Lee turned to Ditko, who developed a visual motif Lee found satisfactory, although Lee would later replace Ditko's original cover with one penciled by Kirby. Ditko said, "The Spider-Man pages Stan showed me were nothing like the (eventually) published character. In fact, the only drawings of Spider-Man were on the splash <nowiki>[</nowiki>i.e., page 1] and at the end [where] Kirby had the guy leaping at you with a web gun... Anyway, the first five pages took place in the home, and the kid finds a ring and turns into Spider-Man."

Ditko also recalled that, "One of the first things I did was to work up a costume. A vital, visual part of the character. I had to know how he looked ... before I did any breakdowns. For example: A clinging power so he wouldn't have hard shoes or boots, a hidden wrist-shooter versus a web gun and holster, etc. ... I wasn't sure Stan would like the idea of covering the character's face but I did it because it hid an obviously boyish face. It would also add mystery to the character..."

Much earlier, in a rare contemporaneous account, Ditko described his and Lee's contributions in a mail interview with Gary Martin published in "Comic Fan" #2 (Summer 1965): "Stan Lee thought the name up. I did costume, web gimmick on wrist & spider signal". He added he would continue drawing Spider-Man "[i]f nothing better comes along." That same year, he expressed to the fanzine "Voice of Comicdom", regarding a poll of "Best Liked" fan-created comics, "It seems a shame, since comics themselves have so little variety of stories and styles that you would deliberately restrict your own creative efforts to professional comics['] shallow range. What is 'Best Liked' by most readers is what they are most familiar in seeing and any policy based on readers likes has to end up with a lot of look-a-like (sic) strips. You have a great opportunity to show everyone a whole new range of ideas, unlimited types of stories and styles—why FLUB it!"

From 1958 to 1968, Ditko shared a Manhattan studio at 43rd Street and Eighth Avenue with noted fetish artist Eric Stanton, an art-school classmate. When either artist was under deadline pressure, it was not uncommon for them to pitch in and help the other with his assignment. Ditko biographer Blake Bell, without citing sources, said, "At one time in history, Ditko denied ever touching Stanton's work, even though Stanton himself said they would each dabble in each other's art; mainly spot-inking", and the introduction to one book of Stanton's work says, "Eric Stanton drew his pictures in India ink, and they were then hand-coloured by Ditko". In a 1988 interview with Theakston, Stanton recalled that although his contribution to Spider-Man was "almost nil", he and Ditko had "worked on storyboards together and I added a few ideas. But the whole thing was created by Steve on his own... I think I added the business about the webs coming out of his hands".

Spider-Man debuted in "Amazing Fantasy" #15 (Aug. 1962), the final issue of that science-fiction/fantasy anthology series. When the issue proved to be a top seller, Spider-Man was given his own series, "The Amazing Spider-Man". Lee and Ditko's collaboration on the series saw the creation of many of the character's best known antagonists including Doctor Octopus in issue #3 (July 1963); the Sandman in #4 (Sept. 1963); the Lizard in #6 (Nov. 1963); Electro in #9 (March 1964); and the Green Goblin in #14 (July 1964). Ditko eventually desired credit for the plotting he was contributing under the Marvel Method. Lee concurred, and starting with #25 (June 1965), Ditko received plot credit for the stories.

One of the most celebrated issues of the Lee-Ditko run is #33 (Feb. 1966), the third part of the story arc "If This Be My Destiny...!", and featuring the dramatic scene of Spider-Man, through force of will and thoughts of family, escaping from being pinned by heavy machinery. Comics historian Les Daniels noted, "Steve Ditko squeezes every ounce of anguish out of Spider-Man's predicament, complete with visions of the uncle he failed and the aunt he has sworn to save." Peter David observed, "After his origin, this two-page sequence from "Amazing Spider-Man" #33 is perhaps the best-loved sequence from the Stan Lee/Steve Ditko era." Steve Saffel stated the "full page Ditko image from "The Amazing Spider-Man" #33 is one of the most powerful ever to appear in the series and influenced writers and artists for many years to come." Matthew K. Manning wrote that "Ditko's illustrations for the first few pages of this Lee story included what would become one of the most iconic scenes in Spider-Man's history." The story was chosen as #15 in the 100 Greatest Marvels of All Time poll of Marvel's readers in 2001. Editor Robert Greenberger wrote in his introduction to the story, "These first five pages are a modern-day equivalent to Shakespeare as Parker's soliloquy sets the stage for his next action. And with dramatic pacing and storytelling, Ditko delivers one of the great sequences in all comics."

Ditko created In a 1963 letter to Jerry Bails, Marvel writer-editor Stan Lee called the character Ditko's idea, saying, "The first story is nothing great, but perhaps we can make something of him-- 'twas Steve's idea and I figured we'd give it a chance, although again, we had to rush the first one too much. Little sidelight: Originally decided to call him Mr. Strange, but thought the 'Mr.' bit too similar to Mr. Fantastic..." the supernatural hero Doctor Strange in "Strange Tales" #110 (July 1963). Ditko in the 2000s told a visiting fan that Lee gave Dr. Strange the first name "Stephen".

Though often overshadowed by his Spider-Man work, Ditko's Doctor Strange artwork has been equally acclaimed for its surrealistic mystical landscapes and increasingly psychedelic visuals that helped make the feature a favorite of college students. "People who read 'Doctor Strange' thought people at Marvel must be heads [i.e. drug users]," recalled then-associate editor and former Doctor Strange writer Roy Thomas in 1971, "because they had had similar experiences high on mushrooms. But ... I don't use hallucinogens, nor do I think any artists do."

Eventually Lee & Ditko would take Strange into ever-more-abstract realms. In an epic 17-issue story arc in "Strange Tales" #130–146 (March 1965 – July 1966), Lee and Ditko introduced the cosmic character Eternity, who personified the universe and was depicted as a silhouette whose outlines are filled with the cosmos. As historian Bradford W. Wright describes,

The cartoonist and fine artist Seth in 2003 described Ditko's style as: 

In addition to Dr. Strange, Ditko in the 1960s also drew comics starring the Hulk and Iron Man. He penciled and inked the final issue of "The Incredible Hulk" (#6, March 1963), then continued to collaborate with writer-editor Lee on relaunched a Hulk feature in the omnibus "Tales to Astonish", beginning with issue #60 (Oct. 1964). Ditko, inked by George Roussos, penciled the feature through #67 (May 1965). Ditko designed the Hulk's primary antagonist, the Leader, in #62 (Dec. 1964).

Ditko also penciled the Iron Man feature in "Tales of Suspense" #47–49 (Nov. 1963 – Jan. 1964), with various inkers. The first of these debuted the initial version of Iron Man's modern red-and-golden armor, though whether Ditko or cover-penciler and principal character designer Jack Kirby designed the costume is uncertain.

Whichever feature he drew, Ditko's idiosyncratic, cleanly detailed, instantly recognizable art style, emphasizing mood and anxiety, found great favor with readers. The character of Spider-Man and his troubled personal life meshed well with Ditko's own interests, which Lee eventually acknowledged by giving the artist plotting credits on the latter part of their 38-issue run. But after four years on the title, Ditko left Marvel; he and Lee had not been on speaking terms for some time, with art and editorial changes handled through intermediaries. The details of the rift remain uncertain, even to Lee, who confessed in 2003, "I never really knew Steve on a personal level." Ditko later claimed it was Lee who broke off contact and disputed the long-held belief that the disagreement was over the true identity of the Green Goblin: "Stan never knew what he was getting in my Spider-Man stories and covers until after [production manager] Sol Brodsky took the material from me ... so there couldn't have been any disagreement or agreement, no exchanges ... no problems between us concerning the Green Goblin or anything else from before issue #25 to my final issues". Spider-Man successor artist John Romita, in a 2010 deposition, recalled that Lee and Ditko "ended up not being able to work together because they disagreed on almost everything, cultural, social, historically, everything, they disagreed on characters. ..." A friendly farewell was given to Ditko in the "Bullpen Bulletins" of comics cover-dated July 1966, including "Fantastic Four" #52: "Steve recently told us he was leaving for personal reasons. After all these years, we're sorry to see him go, and we wish the talented guy success with his future endeavors."

Regardless, said Lee in 2007, "Quite a few years ago I met him up at the Marvel offices when I was last in New York. And we spoke; he's a hell of a nice guy and it was very pleasant. ... I haven't heard from him since that meeting."

Back at Charlton—where the page rate was low but creators were allowed greater freedom—Ditko worked on such characters as the Blue Beetle (1967–1968), the Question (1967–1968), and Captain Atom (1965–1967), returning to the character he'd co-created in 1960. In addition, in 1966 and 1967, he drew 16 stories, most of them written by Archie Goodwin, for Warren Publishing's horror-comic magazines "Creepy" and "Eerie", generally using an ink-wash technique.

In 1967, Ditko gave his Objectivist ideas ultimate expression in the form of Mr. A, published in Wally Wood's independent title "witzend" # 3. Ditko's hard line against criminals was controversial and he continued to produce Mr. A stories and one-pagers until the end of the 1970s. Ditko returned to Mr. A in 2000 and in 2009.

Ditko moved to DC Comics in 1968, where he co-created the Creeper in "Showcase" #73 (April 1968) with Don Segall, under editor Murray Boltinoff. DC Comics writer and executive Paul Levitz observed that Ditko's art on the "Creeper" stories made "them look unlike anything else being published by DC at the time." Ditko co-created the team Hawk and Dove in "Showcase" #75 (June 1968), with writer Steve Skeates. Around this time, he penciled the lead story, written and inked by Wally Wood, in Wood's early mature-audience, independent-comics publication "Heroes, Inc. Presents Cannon" (1969).

Ditko's stay at DC was short—he would work on all six issues of the Creeper's own title, "Beware the Creeper" (June 1968 – April 1969), though leaving midway through the final one—and the reasons for his departure uncertain. But while at DC, Ditko recommended Charlton staffer Dick Giordano to the company, who would go on to become a top DC penciller, inker, editor, and ultimately, in 1981, the managing editor.

From this time up through the mid-1970s, Ditko worked exclusively for Charlton and various small press/independent publishers. Frank McLaughlin, Charlton's art director during this period, describes Ditko as living "in a local hotel in Derby for a while. He was a very happy-go-lucky guy with a great sense of humor at that time, and always supplied the [female] color separators with candy and other little gifts".

For Charlton in 1974 he did Liberty Belle backup stories in "E-Man" and conceived Killjoy. Ditko produced much work for Charlton's science-fiction and horror titles, as well as for former Marvel publisher Martin Goodman's start-up line Atlas/Seaboard Comics, where he co-created the superhero the Destructor with writer Archie Goodwin, and penciled all four issues of the namesake series (Feb.–Aug. 1975), the first two of which were inked by Wally Wood. Ditko worked on the second and third issues of "Tiger-Man" and the third issue of "Morlock 2001", with Bernie Wrightson inking.

Ditko returned to DC Comics in 1975, creating a short-lived title, "Shade, the Changing Man" (1977–1978). Shade was later revived, without Ditko's involvement, in DC's mature-audience imprint Vertigo. With writer Paul Levitz, he co-created the four-issue sword and sorcery series "Stalker" (1975–1976). Ditko and writer Gerry Conway produced the first issue of a two-issue "Man-Bat" series. He also revived the Creeper and did such various other jobs as a short Demon backup series in 1979 and stories in DC's horror and science-fiction anthologies. Editor Jack C. Harris hired Ditko as guest artist on several issues of "The Legion of Super-Heroes", a decision which garnered a mixed reaction from the title's readership. Ditko also drew the Prince Gavyn version of Starman in "Adventure Comics" #467–478 (1980). He then decamped to do work for a variety of publishers, briefly contributing to DC again in the mid-1980s, with four pinups of his characters for "Who's Who: The Definitive Directory of the DC Universe" and a pinup for "Superman" #400 (Oct. 1984) and its companion portfolio.

Ditko returned to Marvel in 1979, taking over Jack Kirby's "Machine Man", drawing "The Micronauts" and Captain Universe, and continuing to freelance for the company into the late 1990s. Starting in 1984, he penciled the last two years of the space-robot series "Rom". A Godzilla story by Ditko and Marv Wolfman was changed into a Dragon Lord story published in "Marvel Spotlight". Ditko and writer Tom DeFalco introduced the Speedball character in "The Amazing Spider-Man Annual" #22 (1988) and Ditko drew a ten-issue series based on the character.

In 1982, he also began freelancing for the early independent comics label Pacific Comics, beginning with "Captain Victory and the Galactic Rangers" #6 (Sept. 1982), in which he introduced the superhero Missing Man, with Mark Evanier scripting to Ditko's plot and art. Subsequent Missing Man stories appeared in "Pacific Presents" #1–3 (Oct. 1982 – March 1984), with Ditko scripting the former and collaborating with longtime friend Robin Snyder on the script for the latter two. Ditko also created The Mocker for Pacific, in "Silver Star" #2 (April 1983).

For Eclipse Comics, he contributed a story featuring his character Static (no relation to the later Milestone Comics character) in "Eclipse Monthly" #1–3 (Aug.–Oct. 1983), introducing supervillain the Exploder in #2. With writer Jack C. Harris, Ditko drew the backup feature "The Faceless Ones" in First Comics' "Warp" #2–4 (April–June 1983). Working with that same writer and others, Ditko drew a handful of the Fly, Fly-Girl and Jaguar stories for "The Fly" #2–8 (July 1983 – Aug. 1984), for Archie Comics' short-lived 1980s superhero line; in a rare latter-day instance of Ditko inking another artist, he inked penciler Dick Ayers on the Jaguar story in "The Fly" #9 (Oct. 1984). Western Publishing in 1982 announced a series by Ditko and Harris would appear in a new science-fiction comic, "Astral Frontiers", but that title never materialized.

In the early 1990s Ditko worked for Jim Shooter's newly founded company Valiant Comics, drawing, among others, issues of "Magnus, Robot Fighter", "Solar, Man of the Atom" and "X-O-Manowar". In 1992 Ditko worked with writer Will Murray to produce one of his last original characters for Marvel Comics, the superheroine Squirrel Girl, who debuted in "Marvel Super-Heroes" vol. 2, #8, a.k.a. "Marvel Super-Heroes Winter Special" (Jan. 1992).

In 1993, he did the Dark Horse Comics one-shot "The Safest Place in the World". For the Defiant Comics series "Dark Dominion," he drew issue #0, which was released as a set of trading cards. In 1995, he pencilled a four-issue series for Marvel based on the "Phantom 2040" animated TV series. This included a poster that was inked by John Romita Sr. "Steve Ditko's Strange Avenging Tales" was announced as a quarterly series from Fantagraphics Books, although it only ran one issue (Feb. 1997) due to publicly unspecified disagreements between Ditko and the publisher.

"The New York Times" assessed in 2008 that, "By the '70s he was regarded as a slightly old-fashioned odd-ball; by the '80s he was a commercial has-been, picking up wretched work-for-hire gigs. ...following the example of [Ayn] Rand's John Galt, Ditko hacked out moneymaking work, saving his care for the crabbed Objectivist screeds he published with tiny presses. And boy, could Ditko hack: seeing samples of his Transformers coloring book and his Big Boy comic is like hearing Orson Welles sell frozen peas."

Ditko retired from mainstream comics in 1998. His later work for Marvel and DC included such established superheroes as the Sub-Mariner (in "Marvel Comics Presents") and newer, licensed characters such as the "Mighty Morphin Power Rangers". The last mainstream character he created was Marvel's Longarm in "Shadows & Light" #1 (Feb. 1998), in a self-inked, 12-page Iron Man story "A Man's Reach...", scripted by Len Wein. His final mainstream work was a five-page New Gods story for DC Comics, "Infinitely Gentle Infinitely Suffering", inked by Mick Gray and believed to be intended for the 2000–2002 "Orion" series but not published until the 2008 trade paperback "Tales of the New Gods".

Since then, Ditko's solo work has been published intermittently by Robin Snyder, who was his editor at Charlton, Archie Comics, and Renegade Press in the 1980s. The Snyder publications have included a number of original books as well as reprints such as "Static", "The Missing Man", "The Mocker" and, in 2002, "Avenging World", a collection of stories and essays spanning 30 years.

In 2008, Ditko and Snyder released "The Avenging Mind", a 32-page essay publication featuring several pages of new artwork; and "Ditko, Etc...", a 32-page comic book composed of brief vignettes and editorial cartoons. Releases have continued in that format, with stories introducing such characters as the Hero, Miss Eerie, the Cape, the Madman, the Grey Negotiator, the !? and the Outline. He said in 2012 of his self-published efforts, "I do those because that's all they'll let me do."

In addition to the new material, Ditko and Snyder have reprinted earlier Ditko material. In 2010 they published a new edition of the 1973 "Mr. A" comic and a selection of Ditko covers in "The Cover Series". In 2011 they published a new edition of the 1975 comic "...Wha...!? Ditko's H. Series".

Two "lost" stories drawn by Ditko in 1978 have been published by DC in hardcover collections of the artist's work. A Creeper story scheduled for the never published "Showcase" #106 appears in "The Creeper by Steve Ditko" (2010) and an unpublished "Shade, the Changing Man" story appears in "The Steve Ditko Omnibus Vol. 1" (2011). A Hulk and the Human Torch story written by Jack C. Harris and drawn by Ditko in the 1980s was published by Marvel as "Incredible Hulk and the Human Torch: From the Marvel Vault" #1 in August 2011.

As of 2012, Ditko continued to work in Manhattan's Midtown West neighborhood. He mostly declined to give interviews or make public appearances, explaining in 1969 that, "When I do a job, it's not my personality that I'm offering the readers but my artwork. It's not what I'm like that counts; it's what I did and how well it was done. I produce a product, a comic art story. Steve Ditko is the brand name." However, he did contribute numerous essays to Robin Snyder's fanzine "The Comics". Ditko was an ardent supporter of Objectivism.

He had a nephew who became an artist, also named Steve Ditko. As far as it is known, he never married and had no surviving children at the time of his death. Will Eisner stated that Ditko had a son out of wedlock, but this may have been a confused reference to the nephew.

Ditko said in 2012 that he had made no income on the four "Spider-Man" films released to that time. However, a neighbor of Ditko's stated that he received royalty checks. Those involved with creating the "Doctor Strange" film purposely declined to contact him during production, believing they would not be welcome.

Ditko was found unresponsive in his apartment in New York City on June 29, 2018. Police said he had died within the previous two days. He was pronounced dead at age 90, with the cause of death initially deemed as a result of a myocardial infarction, brought on by arteriosclerotic and hypertensive cardiovascular disease.

The final words of Ditko's last essay, published posthumously in "Down Memory Lane" in February 2019, quoted an "old toast" and were appropriately cantankerous: "Here's to those who wish me well, and those that don't can go to hell."


In September 2007, presenter Jonathan Ross hosted a one-hour documentary for BBC Four titled "In Search of Steve Ditko". The program covers Ditko's work at Marvel, DC, and Charlton Comics and at Wally Wood's "witzend", as well as his following of Objectivism. It includes testimonials by Alan Moore, Mark Millar, Jerry Robinson and Stan Lee, among others. Ross, accompanied by writer Neil Gaiman, met Ditko briefly at his New York office, but he declined to be filmed, interviewed or photographed. He did, however, give the two a selection of some comic books. At the end of the show, Ross said he had since spoken to Ditko on the telephone and, as a joke, that he was now on first name terms with him.

As penciller (generally but not exclusively self-inked), unless otherwise noted

Farrell Publications


Harvey Comics


Key Publications


Prize Comics


Charlton Comics


Marvel Comics


St. John Publications


DC Comics

ACG


Dell Publishing


Warren Publishing

Tower Comics


Independent

Atlas/Seaboard

CPL Gang


Star*Reach Productions

M W Communications


Pacific Comics

New Media Publishing


First Comics


Eclipse Comics

Epic Comics

Archie Comics


Deluxe Comics


Renegade Press

Globe Communications


Ace Comics


3-D- Zone


Valiant Comics


Marvel UK


Dark Horse Comics

Defiant Comics


Topps Comics


Yoe! Studio

Fantagraphics Books


AC Comics


Robin Snyder



</doc>
<doc id="29555" url="https://en.wikipedia.org/wiki?curid=29555" title="List of tourist attractions in Sardinia">
List of tourist attractions in Sardinia

This is a list of the most famous tourist destinations of Sardinia. Minor islands are included from Olbia, clockwise — industrial sites are not included.





</doc>
<doc id="29556" url="https://en.wikipedia.org/wiki?curid=29556" title="List of people from Sardinia">
List of people from Sardinia

Sardinia is the second-largest island in the Mediterranean Sea, with a population of about 1.6 million people. The list includes notable natives of Sardinia, as well as those who were born elsewhere but spent a large part of their active life in Sardinia. People of Sardinian heritage and descent are in a separate section of this article. 









Emanuela Loi

























</doc>
<doc id="29558" url="https://en.wikipedia.org/wiki?curid=29558" title="Gavinus">
Gavinus

Saint Gavinus () is a Christian saint who is greatly celebrated in Sardinia, Italy, as one of the Martyrs of Torres (), along with his companions SS Protus and Januarius.

He was probably a Roman soldier martyred for the Christian faith during the persecution of Diocletian in 304 in the city of Porto Torres (), according to the legend on the orders of the governor ("preside") of Sardinia and Corsica, a certain Barbarus.

The well-known Romanesque church of Gavoi is dedicated to him, as is the town of San Gavino Monreale, and a number of communes in Corsica.

The 11th-century Basilica of San Gavino in Porto Torres, Sassari, is also dedicated to this saint. It was built by Comita or Gomida, Judge of Torres, and contains the relics, discovered in 1614, not only of Saint Gavinus, but also of his companions, Saints Protus and Januarius.

His feast day is given in the Roman Martyrology as 30 May.




</doc>
<doc id="29559" url="https://en.wikipedia.org/wiki?curid=29559" title="Sienna">
Sienna

Sienna (from , "Siena earth") is an earth pigment containing iron oxide and manganese oxide. In its natural state, it is yellow-brown and is called raw sienna. When heated, it becomes a reddish brown and is called burnt sienna. It takes its name from the city-state of Siena, where it was produced during the Renaissance. Along with ochre and umber, it was one of the first pigments to be used by humans, and is found in many cave paintings. Since the Renaissance, it has been one of the brown pigments most widely used by artists.

The first recorded use of "sienna" as a colour name in English was in 1760.

Like the other earth colours, such as yellow ochre and umber, sienna is a clay containing iron oxide, called limonite, which in its natural state has a yellowish colour. In addition to iron oxide, natural or raw sienna also contains about five percent of manganese oxide, which makes it darker than ochre. When heated, the iron oxide is dehydrated and turns partially to haematite, which gives it a reddish-brown colour. 
Sienna is lighter in shade than raw umber, which is also clay with iron oxide, but which has a higher content of manganese (5 to 20 percent) which makes it greenish brown or dark brown. When heated, raw umber becomes burnt umber, a very dark brown.
The pigment sienna was known and used, in its natural form, by the ancient Romans. It was mined near Arcidosso, formerly under Sienese control, now in the province of Grosseto, on Monte Amiata in southern Tuscany. It was called "terra rossa" (red earth), "terra gialla", or terra di Siena"." During the Renaissance, it was noted by the most widely read author about painting techniques, Giorgio Vasari, under the name terra rossa. It became, along with umber and yellow ochre, one of the standard browns used by artists from the 16th to 19th centuries, including Caravaggio (1571-1610) and Rembrandt (1606-1669), who used all the earth colours, including ochre, sienna and umber, in his palette.

By the 1940s, the traditional sources in Italy were nearly exhausted. Much of today's sienna production is carried out in the Italian islands of Sardinia and Sicily, while other major deposits are found in the Appalachian Mountains, where it is often found alongside the region's iron deposits. It is also still produced in the French Ardennes, in the small town of Bonne Fontaine near Ecordal.

In the 20th century, pigments began to be produced using synthetic iron oxide rather than the natural earth. The labels on paint tubes indicate whether they contain natural or synthetic ingredients. PY-43 indicates natural raw sienna, PR-102 indicates natural burnt sienna.

There is no single agreed standard for the colour of sienna, and the name is used today for a wide variety of hues and shades. They vary by country and colour list, and there are many proprietary variations offered by paint companies. The colour box at the top of the article shows one variation from the ISCC-NBS colour list.

Raw sienna is a yellowish-brown natural earth pigment, composed primarily of iron oxide hydroxide. The box shows the colour of the pigment in its natural, or raw state. It contains a large quantity of iron oxide and a small quantity (about five percent) of manganese oxide.

This kind of pigment is known as yellow ochre, yellow earth, limonite, or terra gialla. The pigment name for natural raw sienna from the Colour Index International, shown on the labels of oil paints, is PY-43.

This box at rights shows a variation of raw sienna from the Italian Ferrario 1919 colour list. 
Burnt sienna contains a large proportion of anhydrous iron oxide. It is made by heating raw sienna, which dehydrates the iron oxide, changing it partially to haematite, giving it rich reddish-brown colour.

The pigment is also known as red earth, red ochre, and terra rossa. On the Colour Index International, the pigment is known as PR-102.

This version is from the Italian Ferrario 1919 colour list.

The first recorded use of "burnt sienna" as a colour name in English was in 1853.

This variation of burnt sienna is from the Maerz and Paul "A Dictionary of Color" from 1930. It is considerably lighter than most other versions of burnt sienna. It was a mix of burnt orange and raw sienna.

This infobox shows the colour dark sienna. This variation is from the ISCC-NBS colour list. A similar dark sienna paint was frequently used on Bob Ross' TV show, "The Joy of Painting".

The web colour sienna is defined by the list of X11 colours used in web browsers and web design.


</doc>
<doc id="29564" url="https://en.wikipedia.org/wiki?curid=29564" title="Super Bowl XXXVI">
Super Bowl XXXVI

Super Bowl XXXVI was an American football game between the National Football Conference (NFC) champion St. Louis Rams and the American Football Conference (AFC) champion New England Patriots to decide the National Football League (NFL) champion for the 2001 season. The Patriots defeated the Rams by the score of 20–17. It was New England's first Super Bowl championship, and the franchise's first league championship of any kind. The game was also notable for snapping the AFC East's long streak of not being able to win a Super Bowl championship, as the division's teams had lost eight Super Bowls in total (prior to the Patriots victory in XXXVI). It would be the last time the Rams reached a Super Bowl during their time in St. Louis; the team would return to Super Bowl LIII in 2019 as the Los Angeles Rams 17 years later, where they would again face the Patriots, only to lose 13–3.

The game was played at the Louisiana Superdome in New Orleans, on February 3, 2002. Following the September 11, 2001 attacks earlier in the season, the NFL postponed a week of regular-season games and moved the league's playoff schedule back. As a result, Super Bowl XXXVI was rescheduled from the original date of January 27 to February 3, becoming the first Super Bowl played in February. The pregame ceremonies and the halftime show headlined by the Irish rock band U2 honored the victims of 9/11. Due to heightened security measures following the terrorist attacks, this was the first Super Bowl designated as a National Special Security Event (NSSE) by the Office of Homeland Security (OHS). The Department of Homeland Security (DHS), which replaced the OHS in 2003, later established the practice of naming each subsequent Super Bowl an NSSE. Additionally, it was the last Super Bowl to be played in New Orleans before Hurricane Katrina slammed the city on August 29, 2005; the first since then was Super Bowl XLVII in 2013.

This game marked the Rams' third Super Bowl appearance in franchise history and the second in three seasons. St. Louis posted an NFL-best 14–2 regular season record, led by quarterback Kurt Warner and "The Greatest Show on Turf" offense. The Patriots clinched their third Super Bowl berth after posting an 11–5 regular season record, led by second-year quarterback Tom Brady and a defense that ended the regular season ranked sixth in scoring.

Although the Rams out-gained the Patriots 427–267 in total yards, New England built a 17–3 third-quarter lead off three Rams turnovers. After a holding penalty in the fourth quarter negated a Patriots fumble return for a touchdown, Warner scored a 2-yard touchdown run and threw a 26-yard touchdown pass to tie the game, 17–17, with 1:30 remaining. Without any timeouts, Brady led his team down the field to set up kicker Adam Vinatieri's game-winning 48-yard field goal as time expired. Brady, who completed 16 of 27 passes for 145 yards and a touchdown, was named Super Bowl MVP.

After the Rams’ 1999 season that had culminated in a gripping victory over the Tennessee Titans in Super Bowl XXXIV, their offense again dominated the league in 2000, leading the NFL in passing, scoring, and total yards. However, the Rams had one of the worst defenses in the league, ranking last in points allowed (471). This, along with injury problems and a coaching change from championship-winning coach Dick Vermeil – who resigned just 48 hours after the game – to Mike Martz, caused the Rams to slip to a 10–6 record in 2000. The season ended with a disappointing loss to the New Orleans Saints in the wild card round of the playoffs.

After signing several new defensive players in the off-season, and hiring new defensive coordinator Lovie Smith, the Rams finished the 2001 season with the NFL's best regular season record at 14–2. They led the league in both total offensive yards (6,930) and scoring (503). This was the Rams' third consecutive season with over 500 points, an NFL record. On defense, they only allowed 271 points, improving their 31st ranking in 2000 to 7th in 2001.

The Rams' 1999–2001 offense, nicknamed "The Greatest Show on Turf", is widely considered one of the best in NFL history. The team possessed an incredible amount of offensive talent at nearly every position. In 2001, quarterback Kurt Warner was awarded his second NFL Most Valuable Player Award after throwing for 4,830 yards and 36 touchdowns, with 22 interceptions, and earned a league high 101.4 passer rating. Wide receivers Torry Holt and Isaac Bruce each amassed over 1,100 receiving yards, combining for 142 receptions, 2,469 yards, and 13 touchdowns. Wide receiver Ricky Proehl caught 40 passes for 563 yards and 5 touchdowns. Tight end Ernie Conwell caught 38 passes for 431 yards and 4 touchdowns. Wide receiver Az-Zahir Hakim caught 39 passes for 374 yards, and added another 333 yards returning punts.

Running back Marshall Faulk won NFL Offensive Player of the Year Award for the third year in a row in 2001. He rushed for 1,382 yards, caught 83 passes for 765 yards, scored 21 touchdowns, and became the first NFL player ever to gain more than 2,000 combined rushing and receiving yards for 4 consecutive seasons. Running back Trung Canidate was also a major contributor, rushing for 441 yards, catching 17 passes for 154 yards, returning kickoffs for 748 yards, and scoring 6 touchdowns. The Rams offensive line was led by guard Adam Timmerman and offensive tackle Orlando Pace, who was selected to the Pro Bowl for the third consecutive year.

The Rams' defense ranked third in the league in fewest yards allowed (4,733). The line was anchored by Pro Bowl defensive end Leonard Little, who led the team with 14.5 sacks and recovered a fumble, and defensive end Grant Wistrom, who recorded 9 sacks, 2 interceptions, and 1 fumble recovery. The Rams linebackers unit was led by London Fletcher, who had 4.5 sacks, 2 interceptions, and 4 forced fumbles. St. Louis also had an outstanding secondary, led by Dré Bly (6 interceptions, 150 return yards, and 2 touchdowns), Pro Bowl selection Aeneas Williams (4 interceptions, 69 return yards, 2 touchdowns), and Dexter McCleon (4 interceptions, 66 yards).

The Rams also bested the Patriots in a nationally televised ESPN Sunday night game on November 18 at Foxboro Stadium. Although the Patriots jumped out to an early lead, a critical turnover before the end of the first half that led to a Rams score proved costly. In the second half, the Rams wore New England down and won 24–17. The Rams lost four of their defensive players with injuries. The Patriots' physical play led Rams coach Mike Martz to say after the game that the Patriots were "a Super Bowl–caliber team."

The Patriots' chances for a Super Bowl appearance seemed bleak shortly after the season had begun. Before the season even started, quarterbacks coach Dick Rehbein died of a heart attack at the age of 45. The Patriots, coached by Bill Belichick, lost their first two games, and moreover, in their second loss at home to the New York Jets, starting quarterback Drew Bledsoe suffered a sheared blood vessel on a hit by Jets linebacker Mo Lewis that caused him to miss several weeks. His replacement was second-year quarterback Tom Brady, a sixth-round draft pick who had thrown only 3 passes in 2000. Also, midway through the season, wide receiver Terry Glenn, the team's leading receiver in 2000, was benched due to off-the-field problems. He had been suspended for the first four games for failing a drug test and after serving it he played in just four more before injuries and disputes with the coaching staff caused Belichick to deactivate him for good.
Upon assuming the role of starting quarterback, Brady enjoyed immediate success in the regular season, leading New England to a 44–13 win over the Indianapolis Colts in his first start and eventually to an 11–5 record. He completed 63.9 percent of his passes for 2,843 yards and 18 touchdowns with 12 interceptions and was selected to the Pro Bowl. Veteran Pro Bowl wide receiver Troy Brown was the main receiving threat, recording 101 receptions for 1,199 yards and 5 touchdowns, while also adding another 413 yards and 2 touchdowns returning punts. His 14.2 yards per punt return average led the NFL. Wide receiver David Patten also was productive, catching 51 passes for 749 yards and 4 touchdowns. Running back Antowain Smith provided the team with a stable running game, rushing for 1,157 yards, catching 19 passes for 192 yards, and scoring 13 touchdowns.

New England was outstanding on defense as well. Up front, linemen Bobby Hamilton (7 sacks, 1 fumble recovery) and rookie Richard Seymour excelled at pressuring quarterbacks and stuffing the run. Behind them, the Patriots had three outstanding linebackers: Mike Vrabel (2 interceptions, 3 sacks), Willie McGinest (5 sacks), and Tedy Bruschi (2 interceptions). The secondary also featured outstanding talent such as defensive back Otis Smith, who led the team with 5 interceptions for 181 yards and 2 touchdowns. Cornerback Ty Law intercepted 3 passes, returning them for 91 yards and 2 touchdowns. Safety Lawyer Milloy had 2 interceptions during the season, and was selected along with Law to represent the New England defense in the Pro Bowl. The defense ended the season ranked 6th in scoring, but 24th in total yards allowed. Following their loss to the Rams at home, the Patriots dropped to 5–5, but did not lose again the rest of the season to clinch a first-round bye in the AFC playoffs.

Coincidentally, this was the third straight time that the New England Patriots' Super Bowl appearance would be at the Superdome, meaning they joined the Dallas Cowboys as the only teams to play three different Super Bowls in one stadium; the Cowboys had played three at the old Miami Orange Bowl in the 1970s. In their maiden Super Bowl appearance in Super Bowl XX, the Patriots lost 46–10 – the biggest margin of victory in a Super Bowl to that point – to a Chicago Bears team coached by Mike Ditka and including Mike Singletary and Walter Payton. The Patriots returned to the Superdome 11 years later for Super Bowl XXXI but lost 35–21 to a Green Bay Packers team including Brett Favre, Reggie White and Desmond Howard and coached by Mike Holmgren. Milloy, Law, Vinatieri, Bledsoe, McGinest, Bruschi and Otis Smith were among the players who had played in that game, while Belichick had been assistant head coach to Bill Parcells. The Patriots did not appear in a Super Bowl hosted by another city until the team played in Super Bowl XXXVIII two years later in Houston, Texas.

The Rams began their postseason run with a 45–17 win over the Green Bay Packers in the NFC divisional round. Expected to be a close shootout between Warner and Packers quarterback Brett Favre, the Rams defense dominated the Packers by intercepting a playoff record 6 passes from Favre and returning 3 of them for touchdowns. The Rams offense also racked up 24 points on 2 touchdown passes by Warner, a touchdown run by Faulk, and a field goal by Jeff Wilkins, helping St. Louis put the game away by the end of the third quarter.

One week later, the Rams advanced to the Super Bowl with a 29–24 win over the Philadelphia Eagles in the NFC Championship Game. Philadelphia managed to build a 17–13 halftime lead, but St. Louis scored 16 consecutive second half points (2 touchdown runs by Faulk and a Wilkins field goal) to earn the win, limiting the Eagles to only one touchdown pass in the second half. Warner finished the game with 22 of 33 pass completions for 212 yards and a touchdown, with no interceptions, while Faulk rushed for 159 yards and 2 touchdowns.

In the AFC Divisional Round, the Patriots defeated the Oakland Raiders 16–13 during a raging New England snowstorm in the last game ever played at Foxboro Stadium. The signature moment of the game was a controversial ruling by referee Walt Coleman in the fourth quarter that caused this game to be commonly known as the "Tuck Rule Game." While the Patriots possessed the ball, trailing the Raiders 13–10 with under two minutes left in regulation and no time outs, Brady was sacked by defensive back Charles Woodson, and appeared to fumble the ball. The fumble was recovered by Raiders linebacker Greg Biekert, presumably ending the game with a Raiders victory. After reviewing the play using instant replay, Coleman reversed the call on the field pursuant to the "tuck rule", where a loose ball is ruled an incomplete pass if lost while "tucking" the ball. Most of the controversy centered on whether Brady was still trying to tuck the ball away when he lost control. Brady then led his team to the Raiders 27-yard line, where kicker Adam Vinatieri made a 45-yard field goal which barely cleared the crossbar to send the game into overtime. The Patriots won the toss in overtime and won on another Vinatieri field goal from 23 yards; per the overtime rules in place at that time. Oakland's offense never regained possession.

In the AFC Championship Game, the Patriots traveled to Heinz Field to face the Pittsburgh Steelers, who were coming off a 27–10 win over the previous season's Super Bowl champion Baltimore Ravens. New England scored first with a 55-yard punt return touchdown by Brown, but in the second quarter, Brady was knocked out of the game with a sprained ankle. He was replaced by Bledsoe in Bledsoe's first game action since being injured in September. Upon entering the game, Bledsoe quickly moved the Patriots down the field and threw an 11-yard touchdown pass to Patten to give the Patriots a 14–3 halftime lead. Early in the second half, the Steelers moved from their own 32 to the New England 16, where they lined up for a field goal by Kris Brown. However, Brandon Mitchell blocked the kick, Brown picked up the ball at the 40 and ran 11 yards before lateraling to Antwan Harris, who took it 49 yards for a touchdown that made the score 21–3. But Pittsburgh scored two third-quarter touchdowns to make the score 21–17. The Patriots ended the comeback attempt by scoring a field goal in the fourth quarter and intercepting 2 passes from Steelers quarterback Kordell Stewart in the final 3 minutes of the game.

New Orleans had been preparing for Super Bowl XXXVI ever since the city was awarded the game on October 28, 1998 during the NFL's meetings in Kansas City, Missouri, beating out San Diego as host city. However, the September 11, 2001 terrorist attacks led the league to postpone its September 16 games and play them a week after the scheduled conclusion of the regular season. This caused the playoffs and Super Bowl to be delayed by one week. Rescheduling Super Bowl XXXVI from January 27 to February 3 proved extraordinarily difficult. In addition to rescheduling the game itself, all related events and activities had to be accommodated. This marked the first time in NFL history that the Super Bowl was played in February; all subsequent Super Bowls (excluding Super Bowl XXXVII in 2003) after that have been played in February.

Historically, the NFL made allowance for an open weekend between the Conference Championship games and the Super Bowl. However, there wasn't one scheduled for 2001, due to the NFL's decision beginning in the 1999 season to move the opening week of games to the weekend after Labor Day. Because the date of the Super Bowl had been set through 2003, the bye week prior to the Super Bowl did not return until 2004.

The NFL and New Orleans officials worked diligently to put together a deal to reschedule the game. The league considered a number of options, including shortening the regular season, shortening the playoffs, condensing the three playoff rounds in two weeks, and moving the game to the Rose Bowl in Pasadena, California. It was eventually decided to make every effort to maintain a full regular season and playoff, and push the Super Bowl back to February 3. Also, due to the Super Bowl being sent back a week, the first week of New Orleans Mardi Gras parades rolled one week earlier than normal.

One of the most significant logistical challenges was accommodating the National Automobile Dealers Association (NADA) Convention, which was originally slated to occupy the Superdome on February 3. On October 3, 2001, the NFL announced its intentions to hold the game on February 3, even though no agreement had been reached with NADA. Several weeks later, the three parties came to an accord in which the NADA agreed to move its convention date to the original Super Bowl week in exchange for financial and other considerations, including promotional spots shown during selected regular season NFL games. This agreement permitted the NFL to move the game back to February 3, and allowed for a full standard playoff tournament.
The original logo for Super Bowl XXXVI had a style that reflected the host city, and was distributed on some memorabilia items during 2001. However, after the 9/11 attacks, a new logo reflecting American patriotism was designed, featuring the shape of the 48 contiguous states and the American flag colors of red, white, and blue.

Janet Jackson was originally scheduled to perform during the Halftime Show, but allowed U2 to perform to tribute the events of September 11.

This was the final Super Bowl played on the first-generation AstroTurf surface. From 2000 to 2005, NFL stadiums phased out the short-pile AstroTurf in favor of natural grass or other, newer artificial surfaces which closely simulate grass, like FieldTurf.

Prior to Super Bowl XXXVI, Superdome officials considered installing natural grass for the game. The proposed installation method was comparable to what had been used at the Silverdome during the 1994 FIFA World Cup, and at Giants Stadium from 2000 to 2002. The plan called for large trays of grass to be grown and cultivated outdoors, then brought inside the dome and placed on the field for the game. In the end, cost and quality concerns prompted stadium and league officials to abandon the project.

The Rams entered as 14-point favorites. This was partly because Rams quarterback Kurt Warner statistically had his best year of his career, with a quarterback rating of 101.4, a 68.7 percent completion rate, and threw for 4,830 yards. Many had believed that the Patriots' Cinderella story was simply a fluke, especially after beating the veteran Oakland Raiders in a controversial playoff game in which a recovered fumble by the Raiders was reversed by the tuck rule.

There had been speculation on whether longtime starter Drew Bledsoe might start the game. As stated above, Bledsoe replaced an injured Brady against the Steelers in the AFC Championship game. Eventually, though, Brady was named starter.

The game was broadcast in the United States by Fox; the telecast was presented in a 480p enhanced-definition format marketed as "Fox Widescreen". While promoted as having better quality than standard-definition, and being the first U.S. sporting event produced in a widescreen format with the same production as the main feed for standard-definition viewers (rather than using a separate production for the widescreen feed), it was not true high definition, but still matched the aspect ratio of HDTV sets.

The game was called by play-by-play announcer Pat Summerall and color commentator John Madden. Pam Oliver and Ron Pitts served as sideline reporters. This was Summerall's 26th and final Super Bowl broadcast on television or radio. It was also the eighth and final Super Bowl telecast (and final NFL telecast of any kind) for the Summerall and Madden announcing team. The two had become the NFL's most famous broadcast duo since they were paired together in 1981 on CBS. After this game, Summerall retired from broadcasting and Madden moved to ABC. As a result, Madden was the first person to announce Super Bowls on different networks in consecutive years when he called Super Bowl XXXVII on ABC with Al Michaels.

James Brown hosted all the events with help from his fellow "Fox NFL Sunday" cast members Terry Bradshaw, Howie Long, and Cris Collinsworth. Jillian Barberie served as the weather and entertainment reporter during the pre-game show.

Memorable television commercials that aired during the game included Sony Pictures' trailer for "Spider-Man", Budweiser's "Picking a Card", and Super Bowl Ad Meter commercial of the year winners Bud Light "Satin Sheets." The best commercial of the year from Adbowl M&M's "Chocolate on our Pillow or Hotel Check In" and EA Sports' Madden NFL 2002 which aired during the game three days after Madden NFL 2002 start selling in Japan by Electronic Arts Square.

Before the game, an ensemble of singers featured Barry Manilow, Yolanda Adams, James Ingram, Wynonna Judd, and Patti LaBelle performing Manilow's song "Let Freedom Ring."

In a video segment, past and present NFL players read excerpts from the Declaration of Independence, which has become a part of all subsequent Super Bowls carried by Fox Sports. Super Bowls XXXIX, XLII, and XLV used different active and former players (and a player's widow) reading the Declaration for each version. Former U.S. presidents Gerald Ford, Jimmy Carter, George H. W. Bush, and Bill Clinton appeared in another videotaped segment and recited some of the speeches by Abraham Lincoln. Because Ronald Reagan had Alzheimer's disease, his wife Nancy appeared on the segment in place of him.

Singers Mary J. Blige and Marc Anthony, along with the Boston Pops Orchestra, performed "America the Beautiful". Paul McCartney then sang his post-9/11 song "Freedom". Afterwards, singer Mariah Carey, accompanied by the Boston Pops Orchestra, performed the national anthem.

George H. W. Bush became the first president, past or present, to participate in a Super Bowl coin toss in person (Ronald Reagan participated in the Super Bowl XIX coin toss via satellite from the White House in 1985). Bush was joined by former Dallas Cowboys Hall of Fame quarterback Roger Staubach. Staubach played at the United States Naval Academy and was the Most Valuable Player of Super Bowl VI, which was played 30 years prior at New Orleans' Tulane Stadium.

As was customary at the time, the Rams' individual offensive starters were introduced first, as the Rams were considered the visitors. However, when it came time to introduce the Patriots' starters, Pat Summerall, making the public address announcement, revealed that the Patriots chose "to be introduced as a team." According to David Halberstam's book, "The Education of a Coach", Belichick was given a choice by the NFL to introduce either the offense or defense. Belichick chose neither, asking that the team be introduced all at once in the spirit of unity. Although this was initially rejected by the NFL, Belichick held his ground and the NFL honored his request. The full team introduction demonstrated solidarity, and struck a chord with the audience in the wake of the 9/11 attacks. Since the next Super Bowl game, both Super Bowl participants have been introduced collectively as a team, a precedent which has continued.

The halftime show featured a three-song set from Irish rock band U2, who had just completed their successful Elevation Tour. After a rendition of "Beautiful Day", the band played "MLK" and "Where the Streets Have No Name" as the names of the victims from the September 11 attacks were projected onto a sheet behind the stage. While singing "Where the Streets Have No Name", the group's lead singer Bono replaced the lyrics "take shelter from the poison rain" with "dance in the Louisiana rain", "high on a desert plain" with "where there's no sorrow or pain", and the final line "it's all I can do" with "it's all we can do". At the conclusion of the song, Bono opened his jacket to reveal an American flag printed into the lining. U2's halftime show captivated the audience as a poignant tribute to those who had been lost in the attacks. In 2009, SI.com ranked it as the best halftime show in Super Bowl history, while it was rated the second-greatest by "Askmen.com".

Janet Jackson was originally selected to perform at the Halftime Show, but she instead allowed U2 to perform a tribute to the events of September 11 and due to traveling concerns following the tragedy. She performed again for the Super Bowl halftime two years later, when her highly controversial Super Bowl Halftime Show performance incident occurred.

The Rams scored first midway through the first quarter, with quarterback Kurt Warner completing 6-of-7 passes for 43 yards on a 48-yard, 10-play drive to set up a 50-yard field goal by kicker Jeff Wilkins. At the time, the field goal was the third longest in Super Bowl history. While the rest of the quarter was scoreless, the Patriots were stifling the typically high powered Rams offense by playing physical man coverage with the Rams receivers, forcing them into long drives that would end in punts or field goal attempts.

Early in the second quarter, the Rams drove to New England's 34-yard line, but Warner threw an incompletion on third down, and Wilkins' subsequent 52-yard field goal attempt sailed wide left.

With 8:49 left in the second quarter, a blitz by linebacker Mike Vrabel led Warner to be intercepted by Patriots defensive back Ty Law on a pass that was intended for wide receiver Isaac Bruce, Law then scored on a 47-yard return to give the Patriots a 7–3 lead. With less than two minutes left in the first half, Warner completed a pass to receiver Ricky Proehl at the Rams 40-yard line, but New England defensive back Antwan Harris tackled him, and forced a fumble which was recovered by Patriots defensive back Terrell Buckley. Patriots quarterback Tom Brady started off the Patriots drive with a 16-yard completion to Troy Brown and finished it with an 8-yard touchdown pass to receiver David Patten with 31 seconds left in the half. By halftime, New England owned a surprising 14–3 lead. It was the first time in the entire 2001 season that the Rams fell behind by more than eight points in a game.

The Patriots received the opening kickoff of the second half, but could only reach the St. Louis 43-yard line before being forced to punt. Aided by a 20-yard reception by wide receiver Az-Zahir Hakim, a 22-yard reception by Bruce, and a defensive pass interference penalty on Patriots defensive back Otis Smith, the Rams advanced to the New England 41-yard line. However, on the next play, Vrabel and defensive lineman Richard Seymour sacked Warner for a 9-yard loss. Warner then threw two consecutive incomplete passes, which resulted in the Rams punting.

Later in the third quarter, Smith intercepted a pass intended for Rams wide receiver Torry Holt after Holt slipped while coming off the line of scrimmage, and returned the ball 30 yards to the Rams 33-yard line. Though St. Louis' defense did not give up a touchdown to the Patriots, kicker Adam Vinatieri made a 37-yard field goal to increase New England's lead to 17–3.

The Rams responded by driving to the Patriots' 3-yard line on their ensuing drive. On fourth-and-goal, the Rams attempted to score a touchdown. Warner went back to pass and finding no one open scrambled to his right trying to run the ball in for a touchdown. Warner fumbled the ball while being tackled by linebacker Roman Phifer, which was recovered by defensive back Tebucky Jones who returned it 97 yards for a touchdown that would have increased the Patriots lead to 23–3. However, the play was nullified by a holding penalty on linebacker Willie McGinest, who illegally hugged Rams running back Marshall Faulk and prevented him from becoming an eligible receiver. This gave the Rams a first down on the 1-yard line. On second down, Warner scored on a 2-yard touchdown run to cut the Patriots' lead to 17–10.

After Warner's touchdown, the Rams defense forced the Patriots to a three-and-out. St. Louis then drove from own 7-yard line to the New England 36-yard line, aided by a 30-yard reception by Proehl. However, McGinest sacked Warner for a 16-yard loss on second down, pushing the Rams back to their 46-yard line. St. Louis punted after Warner's third down pass was incomplete.

The Rams forced New England to another three-and-out, and got the ball back on their own 45-yard line with 1:51 left in the game. Warner threw three consecutive completions: an 18-yard pass to Hakim, an 11-yard one to wide receiver Yo Murphy, and finally a 26-yard touchdown completion to Proehl that tied the game 17–17 with 1:30 left in the fourth quarter.
The Patriots had no timeouts left for their ensuing drive, which led Fox color commentator John Madden to initially suggest that the Patriots should run out the clock and attempt to win in overtime. Instead, New England attempted to get the winning score in regulation on the final drive. Bill Belichick conferred with offensive coordinator Charlie Weis and they agreed to go for it. Belichick later stated, "With a quarterback like Brady, going for the win is not that dangerous, because he's not going to make a mistake." Brady opened the drive with three dump-off completions to running back J. R. Redmond, who got out of bounds on the last one and moved the ball to their 41-yard line with 33 seconds left. At this point, Madden admitted on the air that he now liked what the Patriots were doing. After an incomplete pass, Brady completed a 23-yard pass underneath the Rams' zone defense to wide receiver Troy Brown—who also got out of bounds—and followed it up with a 6-yard completion to tight end Jermaine Wiggins to advance to the Rams' 30-yard line. Brady then spiked the ball with seven seconds left, which set up Vinatieri's 48-yard field goal attempt. Vinatieri, who had never missed a field goal indoors, made the kick as time ran out, marking the first time in Super Bowl history that a game was won by a score on the final play.

Warner finished the game with 28 completions out of 44 passes for 365 yards, 1 touchdown, and 2 interceptions, and rushed 3 times for 6 yards and a touchdown. Warner's 365 passing yards were the second highest total in Super Bowl history behind his own record of 414 yards set in Super Bowl XXXIV. Hakim was the top receiver of the game with 5 catches for 90 yards, and also rushed once for 5 yards. Faulk led the team with 76 rushing yards, and also caught 4 passes for 54 yards.

Patriots running back Antowain Smith was the top rusher of the game with 92 yards, and caught a pass for 4 yards. Troy Brown was the Patriots leading receiver with 6 catches for 89 yards. Brown also had a 15-yard kickoff return, and a 4-yard punt return, which gave him 108 total yards. Although the Rams outgained the Patriots 427–267 in total yards, New England forced three turnovers that were converted into 17 points. The Patriots, on the other hand, committed no turnovers.


Sources: NFL.com Super Bowl XXXVI, Super Bowl XXXVI Play Finder NE, Super Bowl XXXVI Play Finder StL

Completions/attempts
Carries
Long gain
Receptions
Times targeted

The following records were set in Super Bowl XXXVI, according to the official NFL.com boxscore, the 2016 NFL Record & Fact Book and the ProFootball reference.com game summary.

Records tied

Source:

Four hours after the game ended, Tom Brady visited Bill Belichick's hotel room where, as per team rules, he had to get his coach's permission to miss the team flight and instead travel to Walt Disney World in Orlando. Belichick gave him a perplexed look, and after a few seconds of dead silence, responded, "Of course you can go. How many times do you win the Super Bowl?"

The game heralded the Patriots dynasty, being the first of nine Super Bowl appearances under the duo of head coach Belichick and quarterback Brady. The Patriots finished the 2002 NFL season 9–7, missing the playoffs. But they went on to win Super Bowl XXXVIII, Super Bowl XXXIX, thus winning three Super Bowls in four years. Then, they won their fourth, fifth, and sixth Super Bowls (Super Bowl XLIX, Super Bowl LI, and Super Bowl LIII) a decade after their third. Brady also won three more Super Bowl MVP awards in Super Bowl XXXVIII, Super Bowl XLIX, and Super Bowl LI, making him the only player to be named Super Bowl MVP four times. Super Bowl XXXVI later became part of the wider 2007 New England Patriots videotaping controversy, also known as "Spygate". In addition to other videotaping allegations, the "Boston Herald" reported, citing an unnamed source, that the Patriots had also taped the Rams' walkthrough practice prior to the game. After further investigations, the league determined that no tape of the Rams' Super Bowl walkthrough was made, and the "Herald" later issued an apology in 2008 for their article about the alleged walkthrough tape. Nevertheless, the Patriots finished the 2007 regular season with a perfect 16–0 record, but failed to record an undefeated 19–0 championship season after losing Super Bowl XLII to the New York Giants. And at the conclusion of the 2015 NFL season, the Patriots held the NFL's best record since Spygate, compiling a 96–32 record from 2008 to 2015.

The Patriots' win in this Super Bowl, beyond just serving as a springboard to five more championships, also became the starting point for a decade of success in Boston sports, with the city's teams winning seven championships in the four major North American sports leagues (the NFL, the NBA, the NHL and MLB), including at least one in each league. Over the next fifteen years, in addition to the Patriots' five additional Super Bowls:

Following the Bruins winning the 2011 Stanley Cup Finals, "Boston Globe" columnist Dan Shaughnessy ranked all seven championships from the past decade and ranked the Patriots winning Super Bowl XXXVI as the second-greatest Boston sports championship of the decade behind only the Red Sox winning the 2004 World Series.

After the Patriots won their first championship in franchise history, it started a run of a team in American sports from NCAA and the four major sports winning their first (or next) franchise championship with a wait of 17 years or more between titles. This streak is still continuing in 2019 after the University of Virginia won their first NCAA Men's Basketball National Championship, the St. Louis Blues won their first Stanley Cup championship in their 52-year franchise history (which was coincidentally against another New England team, the Boston Bruins), the Toronto Raptors won their first NBA Championship in their 24-year franchise history, and the Washington Nationals won their first World Series in their 15th season in Washington, D.C. and their 51st season overall, including their time as the Montreal Expos.

Brady, Malloy and Vinatieri also provided the team's commentary in the 2001 Patriots' episode of "", narrated by actor Martin Sheen.

Beginning with the Rams' appearance in Super Bowl XXXVI, 10 different NFC teams appeared in the Super Bowl over the next 10 years. This trend was broken when the New York Giants earned a trip to Super Bowl XLVI after participating in Super Bowl XLII four years earlier (the Giants defeated the Patriots in both games).

This game was regarded as a "Super Bowl hangover" for the Rams because they lost a Super Bowl where they were heavy favorites, with a win potentially ushering in a Rams dynasty, but instead the loss signaled the beginning of the end of The Greatest Show on Turf era. Due to injuries to Kurt Warner and Marshall Faulk, the Rams finished with a 7–9 record the following year and missed the postseason. They qualified for the playoffs only two more times (2003 and 2004), and only won one more playoff game (the 2004–05 wild card game) during the remainder of their tenure in St. Louis. Head coach Mike Martz was fired after missing most of the 2005 season due to illness. Warner suffered a concussion on opening day in 2003, and was later demoted to backup quarterback for the rest of that season. He then signed with the New York Giants in 2004 as a caretaker quarterback, eventually losing the starting job to rookie quarterback Eli Manning. Warner later joined the Arizona Cardinals in 2005 and eventually gained the starting job where led that team to their first Super Bowl appearance, XLIII, following the 2008 season.

Super Bowl XXXVI ended up being the last Super Bowl that the Rams participated while based in St. Louis; they relocated back to Los Angeles in 2016. The Rams would once again reach the Super Bowl in the 2018 season.

The Patriots and Rams rematched 17 years later in Super Bowl LIII (2019); again featuring the head coach-quarterback tandem of Bill Belichick and Tom Brady who, in addition to Dante Scarnecchia, are the only active personnel left from Super Bowl XXXVI. Coincidentally like XXXVI, LIII also featured one of the league's top offenses (Rams) against one of the top defenses (Patriots). The Patriots won their record-tying sixth Super Bowl by defeating the Rams 13-3, with Brady remarking that LIII had a "throwback feel" to XXXVI.




</doc>
<doc id="29570" url="https://en.wikipedia.org/wiki?curid=29570" title="Scansano">
Scansano

Scansano is a town and comune, of medieval origin, in the province of Grosseto, Tuscany, central Italy. The area which Scansano lies within is called Maremma.

Scansano area is home to the production of Morellino di Scansano, a type of wine.

The municipality is formed by the municipal seat of Scansano and the villages ("frazioni") of Baccinello, Montorgiali, Murci, Pancole, Poggioferro, Polveraia, Pomonte and Preselle.

<BR>


</doc>
<doc id="29574" url="https://en.wikipedia.org/wiki?curid=29574" title="List of maritime explorers">
List of maritime explorers

This is a list of maritime explorers.

The list includes explorers which had contributed, and continue to contribute to human knowledge of the planet's geography, weather, biodiversity, human cultures, the expansion of trade, or established communication between diverse populations...



</doc>
<doc id="29577" url="https://en.wikipedia.org/wiki?curid=29577" title="Wednesday Morning, 3 A.M.">
Wednesday Morning, 3 A.M.

Wednesday Morning, 3 A.M. is the debut studio album by American folk rock duo Simon & Garfunkel. Following their early gig as "Tom and Jerry", Columbia Records signed the two in late 1963. It was produced by Tom Wilson and engineered by Roy Halee. The cover and the label include the subtitle "exciting new sounds in the folk tradition". Recorded in March 1964, the album was released on October 19.

The album was initially unsuccessful, so Paul Simon moved to London, England, and Art Garfunkel continued his studies at Columbia University in his native New York City, before reuniting with Simon in late 1965. "Wednesday Morning, 3 A.M." was re-released in January 1966 (to capitalize on their newly found radio success because of the overdubbing of the song "The Sound of Silence" in June 1965, adding electric guitars, bass guitar and a drum kit), and reached  30 on the Billboard 200. It was belatedly released in the UK two years later (in 1968) in both mono and stereo formats.

The song "He Was My Brother" was dedicated to Andrew Goodman, who was their friend and a classmate of Simon at Queens College. Andrew Goodman volunteered in Freedom Summer during 1964 and was abducted and killed in the murders of Chaney, Goodman, and Schwerner.

The album is included in its entirety as part of the Simon & Garfunkel box sets "Collected Works" and "The Columbia Studio Recordings (1964–1970)".

The album was produced by Tom Wilson and engineered by Roy Halee between March 10-31, 1964.

“Benedictus” was arranged and adapted from Orlando di Lasso's "Missa Octavi toni", a Renaissance setting of the ordinary of the mass. The text, in Latin, is "benedictus qui venit in nomine Domini" (KJV: "Blessed be he that cometh in the name of the Lord" ). The song is arranged for two voices with cello and sparse guitar accompaniment.

The album's cover photo was shot at the Fifth Avenue / 53rd Street subway station in New York City. In several concerts, Art Garfunkel related that during the photo session, several hundred pictures were taken that were unusable due to the "old familiar suggestion" on the wall in the background, which inspired Paul Simon to write the song "A Poem on the Underground Wall" for the duo's later "Parsley, Sage, Rosemary and Thyme" album.

The album was initially unsuccessful, having been released in the shadow of the British Invasion. This resulted in Paul Simon moving to England and Art Garfunkel continuing his studies at Columbia University in New York City. Following the success of "The Sound of Silence," the album peaked at  30 on the "Billboard" album chart in 1966.




</doc>
<doc id="29578" url="https://en.wikipedia.org/wiki?curid=29578" title="Sheldon Rampton">
Sheldon Rampton

Sheldon Rampton (born August 4, 1957) is an American editor and author. He was editor of "PR Watch", and is the author of several books that criticize the public relations industry and what he sees as other forms of corporate and government propaganda.

Rampton was born in Long Beach, California. At the age of one, his family moved to Las Vegas, Nevada, where his father worked as a musician. Raised as a member of The Church of Jesus Christ of Latter-day Saints (LDS Church), he spent two years in Japan as a Latter-day Saint missionary from 1976 to 1978. Upon returning to the United States, however, he left the LDS Church, influenced in part by Mormon feminist Sonia Johnson.

Upon graduation in 1982, Rampton worked as a newspaper reporter before becoming a peace activist. During the 1980s and 1990s, he worked closely with the Wisconsin Coordinating Council on Nicaragua (WCCN), which opposed the Reagan administration's military interventions in Central America and works to promote economic development, human rights, and mutual friendship between the people of the United States and Nicaragua. At WCCN, Rampton helped establish the Nicaraguan Credit Alternatives Fund (NICA Fund) in 1992, which channels loans from US investors to support microcredit and other "alternative credit" programs in Nicaragua.

In 1995, Rampton teamed with John Stauber as co-editors of PR Watch, a publication of the Center for Media and Democracy (CMD). They were described as liberal, and their writings are regarded by some members of the public relations industry as one-sided and hostile, but their work drew wide attention. ActivistCash, a website hosted by Washington lobbyist Richard Berman, has castigated them as "self-anointed watchdogs," "scare-mongers," "reckless" and "left-leaning." Rampton and Stauber have in turn argued that the ActivistCash critique contains a number of "demonstrably false" claims. According to a review in the Denver Post, their 2001 book, "Toxic Sludge Is Good for You," offered "a sardonic, wide-ranging look at the public relations industry."

Rampton is also a contributor to the Wikipedia open content project, and was the person who coined the name "Wikimedia" which later became the name of the foundation that manages Wikipedia and its sister projects. Inspired by Wikipedia's collaborative writing model, Rampton founded Disinfopedia (now known as SourceWatch), another CMD project, to complement his PR Watch work to expose what Rampton perceives as deceptive and misleading public relations campaigns.

After leaving the Center for Media and Democracy in 2009, Rampton became a website developer, joining an open government initiative led by New York State Senate chief information officer Andrew Hoppin. In 2010, Hoppin and Rampton co-founded NuCivic, an open source software company, which they sold in December 2014 to GovDelivery, a software services company now known as Granicus. Rampton currently works as a software engineer at Granicus.




</doc>
<doc id="29579" url="https://en.wikipedia.org/wiki?curid=29579" title="Miller test">
Miller test

The Miller test, also called the three-prong obscenity test, is the United States Supreme Court's test for determining whether speech or expression can be labeled obscene, in which case it is not protected by the First Amendment to the United States Constitution and can be prohibited.

The Miller test was developed in the 1973 case "Miller v. California". It has three parts:

The work is considered obscene only if "all three" conditions are satisfied.

The first two prongs of the Miller test are held to the standards of the community, and the last prong is held to what is reasonable to a person of the United States as a whole. The national reasonable person standard of the third prong acts as a check on the community standard of the first two prongs, allowing protection for works that in a certain community might be considered obscene but on a national level might have redeeming value.

For legal scholars, several issues are important. One is that the test allows for community standards rather than a national standard. What offends the average person in Manhattan, Kansas, may differ from what offends the average person in Manhattan, New York. The relevant community, however, is not defined.

Another important issue is that the Miller Test asks for an interpretation of what the "average" person finds offensive, rather than what the more sensitive persons in the community are offended by, as obscenity was defined by the previous test, the Hicklin test, stemming from the English precedent.

In practice, pornography showing genitalia and sexual acts is not "ipso facto" obscene according to the Miller test. For instance, in 2000, a jury in Provo, Utah, took only a few minutes to clear Larry Peterman, owner of a Movie Buffs video store, in Utah County, Utah. He had been charged with distributing obscene material for renting pornographic videos which were displayed in a screened-off area of the store clearly marked as adult-only. The Utah County region had often boasted of being one of the most socially conservative areas in the United States. However, researchers had shown that guests at the local Marriott Hotel were disproportionately large consumers of pay-per-view pornographic material, accessing far more material than the store was distributing.

Because it allows for community standards and demands "serious" value, Justice Douglas worried in his dissent that this test would make it easier to suppress speech and expression. "Miller" replaced a previous test asking whether the speech or expression was "utterly without redeeming social value". As used, however, the test generally makes it difficult to outlaw any form of expression. Many works decried as pornographic have been successfully argued to have some artistic or literary value, most publicly in the context of the National Endowment for the Arts in the 1990s.

The advent of the Internet has made the "community standards" part of the test even more difficult to judge; as material published on a web server in one place can be read by a person residing anywhere else, there is a question as to which jurisdiction should apply. In "United States of America v. Extreme Associates", a pornography distributor from North Hollywood, California, was judged to be held accountable to the community standards applying in western Pennsylvania, where the Third Circuit made its ruling, because the materials were available via Internet in that area. The United States Court of Appeals for the Ninth Circuit has ruled in "United States v. Kilbride" that a "national community standard" should be used for the Internet, but this has yet to be upheld at the national level.



</doc>
<doc id="29580" url="https://en.wikipedia.org/wiki?curid=29580" title="Set-top box">
Set-top box

A set-top box (STB), also colloquially known as a cable box is an information appliance device that generally contains a TV-tuner input and displays output to a television set and an external source of signal, turning the source signal into content in a form that can then be displayed on the television screen or other display device. They are used in cable television, satellite television, and over-the-air television systems, as well as other uses.

According to the "Los Angeles Times", the cost to a cable provider in the United States for a set-top box is between $150 for a basic box to $250 for a more sophisticated box. In 2016, the average pay-TV subscriber paid $231 per year to lease their set-top box from a cable service provider.

The signal source might be an Ethernet cable, a satellite dish, a coaxial cable (see cable television), a telephone line (including DSL connections), broadband over power lines (BPL), or even an ordinary VHF or UHF antenna. Content, in this context, could mean any or all of video, audio, Internet web pages, interactive video games, or other possibilities. Satellite and microwave-based services also require specific external receiver hardware, so the use of set-top boxes of various formats has never completely disappeared. Set-top boxes can also enhance source signal quality.

Before the All-Channel Receiver Act of 1962 required US television receivers to be able to tune the entire VHF and UHF range (which in North America was NTSC-M channels 2 through 83 on 54 to 890 MHz), a set-top box known as a UHF converter would be installed at the receiver to shift a portion of the UHF-TV spectrum onto low-VHF channels for viewing. As some 1960s-era 12-channel TV sets remained in use for many years, and Canada and Mexico were slower than the US to require UHF tuners to be factory-installed in new TVs, a market for these converters continued to exist for much of the 1970s.

Cable television represented a possible alternative to deployment of UHF converters as broadcasts could be frequency-shifted to VHF channels at the cable head-end instead of the final viewing location. However, most cable systems could not accommodate the full 54-890 MHz VHF/UHF frequency range and the twelve channels of VHF space were quickly exhausted on most systems. Adding any additional channels therefore needed to be done by inserting the extra signals into cable systems on nonstandard frequencies, typically either below VHF channel 7 (midband) or directly above VHF channel 13 (superband).

These frequencies corresponded to non-television services (such as two-way radio) over-the-air and were therefore not on standard TV receivers. Before cable-ready TV sets became common in the late 1980s, an electronic tuning device called a cable converter box was needed to receive the additional analog cable TV channels and transpose or convert the selected channel to analog radio frequency (RF) for viewing on a regular TV set on a single channel, usually VHF channel 3 or 4. The box allowed an analog non-cable-ready television set to receive analog encrypted cable channels and was a prototype topology for later date digital encryption devices. Newer televisions were then converted to be analog cypher cable-ready, with the standard converter built-in for selling premium television (aka pay per view). Several years later and slowly marketed, the advent of digital cable continued and increased the need for various forms of these devices. Block conversion of the entire affected frequency band onto UHF, while less common, was used by some models to provide full VCR compatibility and the ability to drive multiple TV sets, albeit with a somewhat nonstandard channel numbering scheme.

Newer television receivers greatly reduced the need for external set-top boxes, although cable converter boxes continue to be used to descramble premium cable channels according to carrier-controlled access restrictions, and to receive digital cable channels, along with using interactive services like video on demand, pay per view, and home shopping through television.

Set-top boxes were also made to enable closed captioning on older sets in North America, before this became a mandated inclusion in new TV sets. Some have also been produced to mute the audio (or replace it with noise) when profanity is detected in the captioning, where the offensive word is also blocked. Some also include a V-chip that allows only programs of some television content ratings. A function that limits children's time watching TV or playing video games may also be built in, though some of these work on main electricity rather than the video signal.

The transition to digital terrestrial television after the turn of the millennium left many existing television receivers unable to tune and display the new signal directly. In the United States, where analog shutdown was completed in 2009 for full-service broadcasters, a federal subsidy was offered for coupon-eligible converter boxes with deliberately limited capability which would restore signals lost to digital transition.

Professional set-top boxes are referred to as IRDs or integrated receiver/decoders in the professional broadcast audio/video industry. They are designed for more robust field handling and rack mounting environments. IRDs are capable of outputting uncompressed serial digital interface signals, unlike consumer STBs which usually don't, mostly because of copyright reasons.

Hybrid set-top boxes, such as those used for Smart TV programming, enable viewers to access multiple TV delivery methods (including terrestrial, cable, internet, and satellite); like IPTV boxes, they include video on demand, time-shifting TV, Internet applications, video telephony, surveillance, gaming, shopping, TV-centric electronic program guides, and e-government. By integrating varying delivery streams, hybrids (sometimes known as "TV-centric") enable pay-TV operators more flexible application deployment, which decreases the cost of launching new services, increases speed to market, and limits disruption for consumers.

As examples, Hybrid Broadcast Broadband TV (HbbTV) set-top boxes allow traditional TV broadcasts, whether from terrestrial (DTT), satellite, or cable providers, to be brought together with video delivered over the Internet and personal multimedia content. Advanced Digital Broadcast (ADB) launched its first hybrid DTT/IPTV set-top box in 2005, which provided Telefónica with the digital TV platform for its Movistar TV service by the end of that year. In 2009, ADB provided Europe's first three-way hybrid digital TV platform to Polish digital satellite operator n, which enables subscribers to view integrated content whether delivered via satellite, terrestrial, or internet.

UK based Inview Technology has over 8M STBs deployed in the UK for Teletext and an original push VOD service for Top Up TV.

In IPTV networks, the set-top box is a small computer providing two-way communications on an IP network and decoding the video streaming media. IP set-top boxes have a built-in home network interface that can be Ethernet, Wireless (802.11 g,n,ac), or one of the existing wire home networking technologies such as HomePNA or the ITU-T G.hn standard, which provides a way to create a high-speed (up to 1Gbit/s) local area network using existing home wiring (power lines, phone lines, and coaxial cables).

In the US and Europe, telephone companies use IPTV (often on ADSL or optical fiber networks) as a means to compete with traditional local cable television monopolies.

This type of service is distinct from Internet television, which involves third-party content over the public Internet not controlled by the local system operator.

Electronic program guides and interactive program guides provide users of television, radio, and other media applications with continuously updated menus displaying broadcast programming or scheduling information for current and upcoming programming. Some guides, such as ITV, also feature backward scrolling to promote their catch-up content.

This feature allows the user to choose preferred channels, making them easier and quicker to access; this is handy with the wide range of digital channels on offer. The concept of favourite channels is superficially similar to that of the "bookmark" function offered in many Web browsers.

The timer allows the user to program and enable the box to switch between channels at certain times: this is handy to record from more than one channel while the user is out. The user still needs to program the VCR or DVD recorder.

Some models have controls on the box, as well as on the remote control. This is useful should the user lose the remote or if the batteries age.

Some remote controls can also control some basic functions of various brands of TVs. This allows the user to use just one remote to turn the TV on and off, adjust volume, or switch between digital and analog TV channels or between terrestrial and internet channels.

The parental lock or content filters allow users over 18 years old to block access to channels that are not appropriate for children, using a personal identification number. Some boxes simply block all channels, while others allow the user to restrict access to chosen channels not suitable for children below certain ages.

As complexity and potential programming faults of the set-top box increase, software such as MythTV, Select-TV and Microsoft's Media Center have developed features comparable to those of set-top boxes, ranging from basic DVR-like functionality to DVD copying, home automation, and housewide music or video playback.

Almost all modern set-top boxes feature automatic firmware update processes. The firmware update is typically provided by the service provider.

With the advent of flat-panel televisions, set-top boxes are now deeper in profile than the tops of most modern TV sets. Because of this, set-top boxes are often placed beneath televisions, and the term set-top box has become something of a misnomer, possibly helping the adoption of the term "digibox". Additionally, newer set-top boxes that sit at the edge of IP-based distribution networks are often called net-top boxes or NTBs, to differentiate between IP and RF inputs. The Roku LT is around the size of a pack of cards and delivers Smart TV to conventional sets.

The distinction between external tuner or demodulator boxes (traditionally considered to be "set-top boxes") and storage devices (such as VCR, DVD, or disc-based PVR units) is also blurred by the increasing deployment of satellite and cable tuner boxes with hard disk, network or USB interfaces built-in.

Devices with the capabilities of computer terminals, such as the WebTV thin client, also fall into the grey area that could invite the term "NTB".

In Europe, a set-top box does not necessarily contain a tuner of its own. A box connected to a television (or VCR) SCART connector is fed with the baseband television signal from the set's tuner, and can have the television display the returned processed signal instead.
This SCART feature had been used for connection to analogue decoding equipment by pay TV operators in Europe, and in the past was used for connection to teletext equipment before the decoders became built-in. The outgoing signal could be of the same nature as the incoming signal, or RGB component video, or even an "insert" over the original signal, due to the "fast switching" feature of SCART.

In case of analogue pay-TV, this approach avoided the need for a second remote control. The use of digital television signals in more modern pay-TV schemes requires that decoding take place before the digital-to-analogue conversion step, rendering the video outputs of an analogue SCART connector no longer suitable for interconnection to decryption hardware. Standards such as DVB's Common Interface and ATSC's CableCARD therefore use a PCMCIA-like card inserted as part of the digital signal path as their alternative to a tuner-equipped set-top box.

In June 2011 a report from the American National Resources Defense Council brought attention to the energy efficiency of set-top boxes, and the US Department of Energy announced plans to consider the adoption of energy efficiency standards for set-top boxes. In November 2011, the National Cable & Telecommunications Association announced a new energy efficiency initiative that commits the largest American cable operators to the purchase of set-top boxes that meet Energy Star standards and the development of sleep modes that will use less energy when the set-top box is not being used to watch or record video.



</doc>
<doc id="29582" url="https://en.wikipedia.org/wiki?curid=29582" title="Scatology">
Scatology

In medicine and biology, scatology or coprology is the study of feces.

Scatological studies allow one to determine a wide range of biological information about a creature, including its diet (and thus where it has been), health and diseases such as tapeworms. The word derives from the Greek ( ) meaning "dung, feces"; "coprology" derives from the Greek of similar meaning.

A comprehensive study of scatology was documented by John Gregory Bourke under the title " Rites of All Nations" (1891). An abbreviated version of the work (with a foreword by Sigmund Freud), was published as "The Portable Scatalog" in 1994.

In psychology, a scatology is an obsession with excretion or excrement, or the study of such obsessions.

In sexual fetishism, scatology (usually abbreviated "scat") refers to coprophilia, when a person is sexually aroused by fecal matter, whether in the use of feces in various sexual acts, watching someone defecating, or simply seeing the feces. Entire subcultures in sexuality are devoted to this fetish.

In literature, "scatological" is a term to denote the literary trope of the grotesque body. It is used to describe works that make particular reference to excretion or excrement, as well as to toilet humor. Well-known for his scatological tropes is the late medieval fictional character of Till Eulenspiegel. Another common example is John Dryden's "Mac Flecknoe", a poem that employs extensive scatological imagery to ridicule Dryden's contemporary Thomas Shadwell. In German literature in particular is a wealth of scatological texts and references, which includes such books as Collofino's "Non Olet". A case which has provoked an unusual amount of comment in the academic literature is Mozart's scatological humour. Smith, in his review of English literature's representations of scatology from the Middle Ages to the 18th century, notes two attitudes towards scatology. One of these emphasises the merry and the carnivalesque. This is found in Chaucer and Shakespeare. The other attitude is one of self-disgust and misanthropy. This is found in the works of the Earl of Rochester and Jonathan Swift.




</doc>
<doc id="29586" url="https://en.wikipedia.org/wiki?curid=29586" title="Σ-algebra">
Σ-algebra

In mathematical analysis and in probability theory, a σ-algebra (also σ-field) on a set "X" is a collection Σ of subsets of "X" that
includes "X" itself, is closed under complement, and is closed under countable unions.

The definition implies that it also includes 
the empty subset and that it is closed under countable intersections.
The pair ("X", Σ) is called a measurable space or Borel space.

A σ-algebra is a type of algebra of sets. An algebra of sets needs only to be closed under the union or intersection of "finitely" many subsets, which is a weaker condition.

The main use of σ-algebras is in the definition of measures; specifically, the collection of those subsets for which a given measure is defined is necessarily a σ-algebra. This concept is important in mathematical analysis as the foundation for Lebesgue integration, and in probability theory, where it is interpreted as the collection of events which can be assigned probabilities. Also, in probability, σ-algebras are pivotal in the definition of conditional expectation.

In statistics, (sub) σ-algebras are needed for the formal mathematical definition of a sufficient statistic, particularly when the statistic is a function or a random process and the notion of conditional density is not applicable.

If one possible σ-algebra on "X" is where ∅ is the empty set. In general, a finite algebra is always a σ-algebra.

If {"A", "A", "A", …} is a countable partition of "X" then the collection of all unions of sets in the partition (including the empty set) is a σ-algebra.

A more useful example is the set of subsets of the real line formed by starting with all open intervals and adding in all countable unions, countable intersections, and relative complements and continuing this process (by transfinite iteration through all countable ordinals) until the relevant closure properties are achieved - the σ-algebra produced by this process is known as the Borel algebra on the real line, and can also be conceived as the smallest (i.e. "coarsest") σ-algebra containing all the open sets, or equivalently containing all the closed sets. It is foundational to measure theory, and therefore modern probability theory, and a related construction known as the Borel hierarchy is of relevance to descriptive set theory.

There are at least three key motivators for σ-algebras: defining measures, manipulating limits of sets, and managing partial information characterized by sets.

A measure on "X" is a function that assigns a non-negative real number to subsets of "X"; this can be thought of as making precise a notion of "size" or "volume" for sets. We want the size of the union of disjoint sets to be the sum of their individual sizes, even for an infinite sequence of disjoint sets.

One would like to assign a size to "every" subset of "X", but in many natural settings, this is not possible. For example, the axiom of choice implies that, when the size under consideration is the ordinary notion of length for subsets of the real line, then there exist sets for which no size exists, for example, the Vitali sets. For this reason, one considers instead a smaller collection of privileged subsets of "X". These subsets will be called the measurable sets. They are closed under operations that one would expect for measurable sets; that is, the complement of a measurable set is a measurable set and the countable union of measurable sets is a measurable set. Non-empty collections of sets with these properties are called σ-algebras.

Many uses of measure, such as the probability concept of almost sure convergence, involve limits of sequences of sets. For this, closure under countable unions and intersections is paramount. Set limits are defined as follows on σ-algebras.

In much of probability, especially when conditional expectation is involved, one is concerned with sets that represent only part of all the possible information that can be observed. This partial information can be characterized with a smaller σ-algebra which is a subset of the principal σ-algebra; it consists of the collection of subsets relevant only to and determined only by the partial information. A simple example suffices to illustrate this idea.

Imagine you and another person are betting on a game that involves flipping a coin repeatedly and observing whether it comes up Heads ("H") or Tails ("T"). Since you and your opponent are each infinitely wealthy, there is no limit to how long the game can last. This means the sample space Ω must consist of all possible infinite sequences of "H" or "T":

However, after "n" flips of the coin, you may want to determine or revise your betting strategy in advance of the next flip. The observed information at that point can be described in terms of the 2 possibilities for the first "n" flips. Formally, since you need to use subsets of Ω, this is codified as the σ-algebra

Observe that then

where formula_8 is the smallest σ-algebra containing all the others.

Let "X" be some set, and let formula_9 represent its power set. Then a subset formula_10 is called a "σ"-algebra if it satisfies the following three properties:


From these properties, it follows that the σ-algebra is also closed under countable intersections (by applying De Morgan's laws).

It also follows that the empty set ∅ is in Σ, since by (1) "X" is in Σ and (2) asserts that its complement, the empty set, is also in Σ. Moreover, since } satisfies condition (3) as well, it follows that } is the smallest possible σ-algebra on "X". The largest possible σ-algebra on "X" is 2:=formula_9.

Elements of the "σ"-algebra are called measurable sets. An ordered pair , where "X" is a set and Σ is a "σ"-algebra over "X", is called a measurable space. A function between two measurable spaces is called a measurable function if the preimage of every measurable set is measurable. The collection of measurable spaces forms a category, with the measurable functions as morphisms. Measures are defined as certain types of functions from a "σ"-algebra to [0, ∞].

A σ-algebra is both a π-system and a Dynkin system (λ-system). The converse is true as well, by Dynkin's theorem (below).

This theorem (or the related monotone class theorem) is an essential tool for proving many results about properties of specific σ-algebras. It capitalizes on the nature of two simpler classes of sets, namely the following.

Dynkin's π-λ theorem says, if "P" is a π-system and "D" is a Dynkin system that contains "P" then the σ-algebra σ("P") generated by "P" is contained in "D". Since certain π-systems are relatively simple classes, it may not be hard to verify that all sets in "P" enjoy the property under consideration while, on the other hand, showing that the collection "D" of all subsets with the property is a Dynkin system can also be straightforward. Dynkin's π-λ Theorem then implies that all sets in σ("P") enjoy the property, avoiding the task of checking it for an arbitrary set in σ("P").

One of the most fundamental uses of the π-λ theorem is to show equivalence of separately defined measures or integrals. For example, it is used to equate a probability for a random variable "X" with the Lebesgue-Stieltjes integral typically associated with computing the probability:

where "F"("x") is the cumulative distribution function for "X", defined on R, while formula_13 is a probability measure, defined on a σ-algebra Σ of subsets of some sample space Ω.

Suppose formula_14 is a collection of σ-algebras on a space "X".



Suppose "Y" is a subset of "X" and let ("X", Σ) be a measurable space.

A "σ"-algebra Σ is just a "σ"-ring that contains the universal set "X". A "σ"-ring need not be a "σ"-algebra, as for example measurable subsets of zero Lebesgue measure in the real line are a "σ"-ring, but not a "σ"-algebra since the real line has infinite measure and thus cannot be obtained by their countable union. If, instead of zero measure, one takes measurable subsets of finite Lebesgue measure, those are a ring but not a "σ"-ring, since the real line can be obtained by their countable union yet its measure is not finite.

"σ"-algebras are sometimes denoted using calligraphic capital letters, or the Fraktur typeface. Thus may be denoted as formula_23 or formula_24.

A separable σ-algebra (or separable σ-field) is a σ-algebra formula_25 that is a separable space when considered as a metric space with metric formula_26 for formula_27 and a given measure formula_28 (and with formula_29 being the symmetric difference operator). Note that any σ-algebra generated by a countable collection of sets is separable, but the converse need not hold. For example, the Lebesgue σ-algebra is separable (since every Lebesgue measurable set is equivalent to some Borel set) but not countably generated (since its cardinality is higher than continuum).

A separable measure space has a natural pseudometric that renders it separable as a pseudometric space. The distance between two sets is defined as the measure of the symmetric difference of the two sets. Note that the symmetric difference of two distinct sets can have measure zero; hence the pseudometric as defined above need not to be a true metric. However, if sets whose symmetric difference has measure zero are identified into a single equivalence class, the resulting quotient set can be properly metrized by the induced metric. If the measure space is separable, it can be shown that the corresponding metric space is, too.

Let "X" be any set.

A stopping time formula_30 can define a formula_31-algebra formula_32, the
so-called formula_33-Algebra of τ-past, which in a filtered probability space describes the information up to the random time formula_30 in the sense that, if the filtered probability space is interpreted as a random experiment, the maximum information that can be found out about the experiment from arbitrarily often repeating it until the time formula_30 is formula_32.

Let "F" be an arbitrary family of subsets of "X". Then there exists a unique smallest σ-algebra which contains every set in "F" (even though "F" may or may not itself be a σ-algebra). It is, in fact, the intersection of all σ-algebras containing "F". (See intersections of σ-algebras above.) This σ-algebra is denoted σ("F") and is called the σ-algebra generated by "F".

If "F" is empty, then σ("F") = }. Otherwise σ("F") consists of all the subsets of "X" that can be made from elements of "F" by a countable number of complement, union and intersection operations.

For a simple example, consider the set "X" = {1, 2, 3}. Then the σ-algebra generated by the single subset {1} is </nowiki>}}. By an abuse of notation, when a collection of subsets contains only one element, "A", one may write σ("A") instead of σ({"A"}); in the prior example σ({1}) instead of σ(<nowiki></nowiki>). Indeed, using to mean is also quite common.

There are many families of subsets that generate useful σ-algebras. Some of these are presented here.

If "f" is a function from a set "X" to a set "Y" and "B" is a σ-algebra of subsets of "Y", then the σ-algebra generated by the function "f", denoted by σ("f"), is the collection of all inverse images "f"("S") of the sets "S" in "B". i.e.

A function "f" from a set "X" to a set "Y" is measurable with respect to a σ-algebra Σ of subsets of "X" if and only if σ("f") is a subset of Σ.

One common situation, and understood by default if "B" is not specified explicitly, is when "Y" is a metric or topological space and "B" is the collection of Borel sets on "Y".

If "f" is a function from "X" to R then σ("f") is generated by the family of subsets which are inverse images of intervals/rectangles in R:

A useful property is the following. Assume "f" is a measurable map from ("X", Σ) to ("S", Σ) and "g" is a measurable map from ("X", Σ) to ("T", Σ). If there exists a measurable map "h" from ("T", Σ) to ("S", Σ) such that "f"("x") = "h"("g"("x")) for all "x", then σ("f") ⊂ σ("g"). If "S" is finite or countably infinite or, more generally, ("S", Σ) is a standard Borel space (e.g., a separable complete metric space with its associated Borel sets), then the converse is also true. Examples of standard Borel spaces include R with its Borel sets and R with the cylinder σ-algebra described below.

An important example is the Borel algebra over any topological space: the σ-algebra generated by the open sets (or, equivalently, by the closed sets). Note that this σ-algebra is not, in general, the whole power set. For a non-trivial example that is not a Borel set, see the Vitali set or Non-Borel sets.

On the Euclidean space R, another σ-algebra is of importance: that of all Lebesgue measurable sets. This σ-algebra contains more sets than the Borel σ-algebra on R and is preferred in integration theory, as it gives a complete measure space.

Let formula_39 and formula_40 be two measurable spaces. The σ-algebra for the corresponding product space formula_41 is called the product σ-algebra and is defined by

Observe that formula_43 is a π-system.

The Borel σ-algebra for R is generated by half-infinite rectangles and by finite rectangles. For example,

For each of these two examples, the generating family is a π-system.

Suppose

is a set of real-valued functions on formula_46. Let formula_47 denote the Borel subsets of R. A cylinder subset of is a finitely restricted set defined as

Each

is a π-system that generates a σ-algebra formula_50. Then the family of subsets

is an algebra that generates the cylinder σ-algebra for . This σ-algebra is a subalgebra of the Borel σ-algebra determined by the product topology of formula_52 restricted to .

An important special case is when formula_46 is the set of natural numbers and is a set of real-valued sequences. In this case, it suffices to consider the cylinder sets

for which

is a non-decreasing sequence of σ-algebras.

Suppose formula_56 is a probability space. If formula_57 is measurable with respect to the Borel σ-algebra on R then is called a random variable ("n = 1") or random vector ("n" > 1). The σ-algebra generated by is

Suppose formula_56 is a probability space and formula_60 is the set of real-valued functions on formula_46. If formula_62 is measurable with respect to the cylinder σ-algebra formula_63 (see above) for , then is called a stochastic process or random process. The σ-algebra generated by is

the σ-algebra generated by the inverse images of cylinder sets.




</doc>
<doc id="29587" url="https://en.wikipedia.org/wiki?curid=29587" title="Second Battle of El Alamein">
Second Battle of El Alamein

The Second Battle of El Alamein (23 October – 11 November 1942) was a battle of the Second World War that took place near the Egyptian railway halt of El Alamein. The First Battle of El Alamein and the Battle of Alam el Halfa had prevented the Axis from advancing further into Egypt.

In August 1942, General Claude Auchinleck had been sacked as Commander-in-Chief Middle East Command and his successor, Lieutenant-General William Gott was killed on his way to replace him as commander of the Eighth Army. Lieutenant-General Bernard Montgomery was appointed and led the Eighth Army offensive.

The Allied victory was the beginning of the end of the Western Desert Campaign, eliminating the Axis threat to Egypt, the Suez Canal and the Middle Eastern and Persian oil fields. The battle revived the morale of the Allies, being the first big success against the Axis since Operation Crusader in late 1941. The battle coincided with the Allied invasion of French North Africa in Operation Torch on 8 November, the Battle of Stalingrad and the Guadalcanal Campaign.

Panzer Army Africa (/, "Generalfeldmarschall" Erwin Rommel), composed of German and Italian tank and infantry units, had advanced into Egypt after its success at the Battle of Gazala (26 May – 21 June 1942). The Axis advance menaced British control of the Suez Canal, the Middle East and its oil resources. General Claude Auchinleck withdrew the Eighth Army to within of Alexandria where the Qattara Depression was south of El Alamein on the coast. The depression was impassable and meant that any attack had to be frontal; Axis attacks in the First Battle of El Alamein (1–27 July) were defeated.

Eighth Army counter-attacks in July also failed, as the Axis forces dug in and regrouped. Auchinleck called off the attacks at the end of July to rebuild the army. In early August, the Prime Minister, Winston Churchill and General Sir Alan Brooke, the Chief of the Imperial General Staff (CIGS), visited Cairo and replaced Auchinleck as Commander-in-chief Middle East Command with General Harold Alexander. Lieutenant-General William Gott was made commander of the Eighth Army but was killed when his transport aircraft was shot down by "Luftwaffe" fighters; Lieutenant-General Bernard Montgomery was flown from Britain to replace him.

Lacking reinforcements and depending on small, underdeveloped ports for supplies, aware of a huge Allied reinforcement operation for the Eighth Army, Rommel decided to attack first. The two armoured divisions of the "Afrika Korps" and the reconnaissance units of led the attack but were repulsed at the Alam el Halfa ridge and Point 102 on 30 August 1942 during the Battle of Alam el Halfa and the Axis forces retired to their start lines. The short front line and secure flanks favoured the Axis defence and Rommel had time to develop the Axis defences, sowing extensive minefields with mines and miles of barbed wire. Alexander and Montgomery intended to establish a superiority of force sufficient to achieve a breakthrough and exploit it to destroy . Earlier in the Western Desert Campaign, neither side had been able to exploit a local victory sufficiently to defeat its opponent before it had withdrawn and transferred the problem of over-extended supply lines to the victor.

The British had an intelligence advantage because Ultra and local sources exposed the Axis order of battle, its supply position and intentions. A reorganisation of military intelligence in Africa in July had also improved the integration of information received from all sources and the speed of its dissemination. With rare exceptions, intelligence identified the supply ships destined for North Africa, their location or routing and in most cases their cargoes, allowing them to be attacked. By 25 October, was down to three days' supply of fuel, only two days' worth of which were east of Tobruk. Harry Hinsley, the official historian of British intelligence wrote in 1981 that "The Panzer Army... did not possess the operational freedom of movement that was absolutely essential in consideration of the fact that the British offensive can be expected to start any day". Submarine and air transport somewhat eased the shortage of ammunition and by late October, there was sixteen days' supply at the front. After six more weeks, the Eighth Army was ready; and began the offensive against the and of the .

Montgomery's plan was for a main attack to the north of the line and a secondary attack to the south, involving XXX Corps (Lieutenant-General Oliver Leese) and XIII Corps (Lieutenant-General Brian Horrocks), while X Corps (Lieutenant-General Herbert Lumsden) was to exploit the success. With Operation Lightfoot, Montgomery intended to cut two corridors through the Axis minefields in the north. One corridor was to run south-west through the 2nd New Zealand Division sector towards the centre of Miteirya Ridge, while the second was to run west, passing north of the west end of the Miteirya Ridge across the 9th Australian and 51st (Highland) Division sectors. Tanks would then pass through and defeat the German armour. Diversions at Ruweisat Ridge in the centre and also the south of the line would keep the rest of the Axis forces from moving northwards. Montgomery expected a 12-day battle in three stages: the break-in, the dogfight and the final breaking of the enemy.

For the first night of the offensive, Montgomery planned for four infantry divisions of XXX Corps to advance on a front to the Oxalic Line, over-running the forward Axis defences. Engineers would clear and mark the two lanes through the minefields, through which the armoured divisions from X Corps would pass to gain the Pierson Line. They would rally and consolidate their position just west of the infantry positions, blocking an Axis tank counter-attack. The British tanks would then advance to "Skinflint", astride the north–south Rahman Track deep in the Axis defensive system, to challenge the Axis armour. The infantry battle would continue as the Eighth Army infantry "crumbled" the deep Axis defensive fortifications (three successive lines of fortification had been constructed) and destroy any tanks that attacked them.

The Commonwealth forces practised a number of deceptions in the months before the battle to confuse the Axis command as to the whereabouts of the forthcoming battle and when the battle was likely to occur. This operation was code-named Operation Bertram. In September, they dumped waste materials (discarded packing cases, etc.) under camouflage nets in the northern sector, making them appear to be ammunition or ration dumps. The Axis naturally noticed these but, as no offensive action immediately followed and the "dumps" did not change in appearance, they were subsequently ignored. This allowed Eighth Army to build up supplies in the forward area unnoticed by the Axis, by replacing the rubbish with ammunition, petrol or rations at night. Meanwhile, a dummy pipeline was built, hopefully leading the Axis to believe the attack would occur much later than it, in fact, did and much further south. To further the illusion, dummy tanks consisting of plywood frames placed over jeeps were constructed and deployed in the south. In a reverse feint, the tanks destined for battle in the north were disguised as supply trucks by placing removable plywood superstructures over them.

As a preliminary, the 131st (Queen's) Infantry Brigade of the 44th (Home Counties) Infantry Division, supported by tanks from the 4th Armoured Brigade, launched Operation Braganza attacking the 185th Airborne Division Folgore on the night of 29/30 September in an attempt to capture the Deir el Munassib area. The Italian paratroopers repelled the attack, killing or capturing over 300 of the attackers. It was wrongly assumed that "Fallschirmjäger" (German paratroopers) had manned the defences and been responsible for the British reverse. The "Afrika Korps" war diary notes that the Italian paratroops "bore the brunt of the attack. It fought well and inflicted heavy losses on the enemy."

With the failure of the offensive at the Battle of Alam el Halfa, the Axis forces went onto the defensive but losses had not been excessive. The Axis supply line from Tripoli was extremely long and captured Allied supplies and equipment had been exhausted, but Rommel decided to advance into Egypt.

The Eighth Army was being supplied with men and materials from the United Kingdom, India, Australia and New Zealand, as well as with trucks and the new Sherman tanks from the United States. Rommel continued to request equipment, supplies and fuel but the priority of the German war effort was the Eastern Front and very limited supplies reached North Africa. Rommel was ill and in early September, arrangements were made for him to return to Germany on sick leave and for "General der Panzertruppe" Georg Stumme to transfer from the Russian front to take his place. Before he left for Germany on 23 September, Rommel organised the defence and wrote a long appreciation of the situation to "Oberkommando der Wehrmacht" (OKW armed forces high command), once again setting out the essential needs of the Panzer Army.
Rommel knew that the British Commonwealth forces would soon be strong enough to attack. His only hope now relied on the German forces fighting in the Battle of Stalingrad quickly to defeat the Red Army, then move south through the Trans-Caucasus and threaten Iran (Persia) and the Middle East. If successful, large numbers of British and Commonwealth forces would have to be sent from the Egyptian front to reinforce the Ninth Army in Iran, leading to the postponement of any offensive against his army. Rommel hoped to convince OKW to reinforce his forces for the eventual link-up between and the German armies fighting in southern Russia, enabling them finally to defeat the British and Commonwealth armies in North Africa and the Middle East.

In the meantime, the dug in and waited for the attack by the Eighth Army or the defeat of the Red Army at Stalingrad. Rommel added depth to his defences by creating at least two belts of mines about apart, connected at intervals to create boxes (Devil's gardens) which would restrict enemy penetration and deprive British armour of room for manoeuvre. The front face of each box was lightly held by battle outposts and the rest of the box was unoccupied but sowed with mines and explosive traps and covered by enfilading fire. The main defensive positions were built to a depth of at least behind the second mine belt. The Axis laid around half a million mines, mostly Teller anti-tank mines with some smaller anti-personnel types (such as the S-mine). (Many of these mines were British, and had been captured at Tobruk). To lure enemy vehicles into the minefields, the Italians dragged an axle and tyres through the fields using a long rope to create what appeared to be well-used tracks.
Rommel did not want the British armour to break out into the open because he had neither the strength of numbers nor fuel to match them in a battle of manoeuvre. The battle had to be fought in the fortified zones; a breakthrough had to be defeated quickly. Rommel stiffened his forward lines by alternating German and Italian infantry formations. Because the Allied deception confused the Axis as to the point of attack, Rommel departed from his usual practice of holding his armoured strength in a concentrated reserve and split it into a northern group (15th Panzer and "Littorio" Division) and a southern group (21st Panzer and "Ariete" Division), each organised into battle groups to be able to make a quick armoured intervention wherever the blow fell and prevent narrow breakthroughs from being enlarged. A significant proportion of his armoured reserve was dispersed and held unusually far forward. The 15th Panzer Division had 125 operational tanks (16 Pz.IIs, 43 Pz.III Ausf H, 43 Pz.III Ausf J, 6 Pz.IV Ausf D, 15 Pz.IV Ausf F) while the 21st Panzer Division had 121 operational combat vehicles (12 Pz.IIs, 38 Pz.III Ausf H, 43 Pz.III Ausf J, 2 Pz.IV Ausf D, 15 Pz.IV Ausf F).

Rommel held the 90th Light Division further back and kept the "Trieste" Motorised Division in reserve near the coast. Rommel hoped to move his troops faster than the Allies, to concentrate his defences at the most important point () but lack of fuel meant that once the had concentrated, it would not be able to move again because of lack of fuel. The British were well aware that Rommel would be unable to mount a defence based on his usual manoeuvre tactics but no clear picture emerged of how he would fight the battle and British plans seriously underestimated the Axis defences and the fighting power of the .

Prior to the main barrage, there was a diversion by the 24th Australian Brigade, which involved the 15th Panzer Division being subjected to heavy fire for a few minutes. Then at 21:40 (Egyptian Summer Time) on 23 October on a calm, clear evening under the bright sky of a full moon, Operation Lightfoot began with a 1,000-gun barrage. The fire plan had been arranged so that the first rounds from the 882 guns from the field and medium batteries would land along the front at the same time. After twenty minutes of general bombardment, the guns switched to precision targets in support of the advancing infantry. The shelling plan continued for five and a half hours, by the end of which each gun had fired about 600 rounds, about 529,000 shells.

Operation Lightfoot alluded to the infantry attacking first. Anti-tank mines would not be tripped by soldiers stepping on them since they were too light. As the infantry advanced, engineers had to clear a path for the tanks coming behind. Each gap was to be wide, which was just enough to get tanks through in single file. The engineers had to clear a route through the Devil's Gardens. It was a difficult task that was not achieved because of the depth of the Axis minefields.
At 22:00, the four infantry divisions of XXX Corps began to move. The objective was to establish a bridgehead before dawn at the imaginary line in the desert where the strongest enemy defences were situated, on the far side of the second mine belt. Once the infantry reached the first minefields, the mine sweepers, including Reconnaissance Corps troops and sappers, moved in to create a passage for the armoured divisions of X Corps. Progress was slower than planned but at 02:00, the first of the 500 tanks crawled forward. By 04:00, the lead tanks were in the minefields, where they stirred up so much dust that there was no visibility at all, traffic jams developed and tanks bogged down. Only about half of the infantry attained their objectives and none of the tanks broke through.
The 7th Armoured Division (with one Free French Brigade under command) from XIII Corps (Lieutenant-General Brian Horrocks) made a secondary attack to the south. The main attack aimed to achieve a breakthrough, engage and pin down the 21st Panzer Division and the "Ariete" Armoured Division around Jebel Kalakh, while the Free French on the far left were to secure Qaret el Himeimat and the el Taqa plateau. The right flank of the attack was to be protected by 44th Infantry Division with the 131st Infantry Brigade. The attack met determined resistance, mainly from the 185 Airborne Division "Folgore", part of the Ramcke Parachute Brigade and the Keil Group. The minefields were deeper than anticipated and clearing paths through them was impeded by Axis defensive fire. By dawn on 24 October, paths still had not been cleared through the second minefield to release 22nd and 4th Light Armoured Brigades into the open to make their planned turn north into the rear of enemy positions west of Deir el Munassib.

Further north along the XIII Corps front, the 50th Infantry Division achieved a limited and costly success against determined resistance from the "Pavia" Division, "Brescia" Division and elements of the 185th Airborne Division "Folgore". The 4th Indian Infantry Division, on the far left of the XXX Corps front at Ruweisat Ridge, made a mock attack and two small raids intended to deflect attention to the centre of the front.

Dawn aerial reconnaissance showed little change in Axis disposition, so Montgomery gave his orders for the day: the clearance of the northern corridor should be completed and the New Zealand Division supported by 10th Armoured should push south from Miteirya Ridge. 9th Australian Division, in the north, should plan a crumbling operation for that night, while in the southern sector, 7th Armoured should continue to try to break through the minefields with support, if necessary, from 44th Division. "Panzer" units counter-attacked the 51st Highland Division just after sunrise, only to be stopped in their tracks.
The morning of Saturday 24 October brought disaster for the German headquarters. The reports that Stumme had received that morning showed the attacks had been on a broad front but that such penetration as had occurred should be containable by local units. He went forward himself to observe the state of affairs and, finding himself under fire, suffered a heart attack and died. Temporary command was given to Major-General Wilhelm Ritter von Thoma. Hitler had already decided that Rommel should leave his sanatorium and return to North Africa. Rommel flew to Rome early on 25 October to press the "Commando Supremo" for more fuel and ammunition and then on to North Africa to resume command that night of the Panzer Army Africa, which that day was renamed the German-Italian Panzer Army ("Deutsch-Italienische Panzerarmee").

There was little activity during the day pending more complete clearance of paths through the minefields. The armour was held at "Oxalic". Artillery and the Allied Desert Air Force, making over 1,000 sorties, attacked Axis positions all day to aid the 'crumbling' of the Axis forces. By 16:00 there was little progress.

At dusk, with the sun at their backs, Axis tanks from the 15th Panzer Division and Italian "Littorio" Division swung out from the Kidney feature (also known to the Germans and Italians as Hill 28), often wrongly called a ridge as it was actually a depression, to engage the 1st Armoured Division and the first major tank battle of El Alamein began. Over 100 tanks were involved and half were destroyed by dark. Neither position was altered.

At around 10:00, Axis aircraft had destroyed a convoy of 25 Allied vehicles carrying petrol and ammunition, setting off a night-long blaze; Lumsden wanted to call off the attack, but Montgomery made it clear that his plans were to be carried out. The thrust that night by 10th Armoured Division from Miteirya Ridge failed. The lifting of mines on the Miteirya Ridge and beyond took far longer than planned and the leading unit, 8th Armoured Brigade, was caught on their start line at 22:00—zero hour—by an air attack and were scattered. By the time they had reorganised they were well behind schedule and out of touch with the creeping artillery barrage. By daylight the brigade was out in the open taking considerable fire from well sited tanks and anti-tank guns. Meanwhile 24th Armoured Brigade had pushed forward and reported at dawn they were on the Pierson line, although it turned out that, in the dust and confusion, they had mistaken their position and were well short.

The attack in the XIII Corps sector to the south fared no better. 44th Division's 131st Infantry Brigade cleared a path through the mines, but when 22nd Armoured Brigade passed through, they came under heavy fire and were repulsed, with 31 tanks disabled. Allied air activity that night focused on Rommel's northern armoured group, where of bombs were dropped. To prevent a recurrence of 8th Armoured Brigade's experience from the air, attacks on Axis landing fields were also stepped up.

The initial thrust had ended by Sunday. The Allies had advanced through the minefields in the west to make a wide and deep inroad. They now sat atop Miteirya Ridge in the south-east. Axis forces were firmly entrenched in most of their original battle positions and the battle was at a standstill. Montgomery decided that the planned advance southward from Miteirya Ridge by the New Zealanders would be too costly and instead decided that XXX Corps—while keeping firm hold of Miteirya—should strike northward toward the coast with 9th Australian Division. Meanwhile, 1st Armoured Division—on the Australians' left—should continue to attack west and north-west, and activity to the south on both Corps fronts would be confined to patrolling. The battle would be concentrated at the Kidney feature and Tel el Eisa until a breakthrough occurred.
By early morning, the Axis forces launched a series of attacks using 15th Panzer and "Littorio" divisions. The Panzer Army was probing for a weakness, but without success. When the sun set the Allied infantry went on the attack. Around midnight, 51st Division launched three attacks, but no one knew exactly where they were. Pandemonium and carnage ensued, resulting in the loss of over 500 Allied troops, and leaving only one officer among the attacking forces. While the 51st Highland Division was operating around Kidney Ridge, the Australians were attacking Point 29 (sometimes shown on Axis maps as "28") a high Axis artillery observation post south-west of Tel el Eisa, to surround the Axis coastal salient containing the German 164th Light Division and large numbers of Italian infantry.

This was the new northern thrust Montgomery had devised earlier in the day, and was to be the scene of heated battle for some days. The Australian 26th Brigade attacked at midnight, supported by artillery and 30 tanks of 40th Royal Tank Regiment. They took the position and 240 prisoners. Fighting continued in this area for the next week, as the Axis tried to recover the small hill that was so important to their defence. Night bombers dropped of bombs on targets in the battlefield and on the "Stuka" base at Sidi Haneish, while night fighters flew patrols over the battle area and the Axis forward landing grounds. In the south, the 4th Armoured Brigade and 69th Infantry Brigade attacked the 187th Paratroopers Infantry Regiment "Folgore" at Deir Munassib, but lost about 20 tanks gaining only the forward positions.

Rommel, on his return to North Africa on the evening of 25 October, assessed the battle. Casualties, particularly in the north, as a result of incessant artillery and air attack, had been severe. The Italian "Trento" Division had lost 50 per cent of its infantry and most of its artillery, the 164th Light Division had lost two battalions. The 15th Panzer and "Littorio" divisions had prevented the Allied tanks from breaking through but this had been a costly defensive success, the 15th Panzer Division being reduced to 31 tanks remaining. Most other units were also under strength, the men were on half rations, a large number were sick and had only enough fuel for three days.

Rommel was convinced by this time that the main assault would come in the north and determined to retake Point 29. He ordered a counter-attack against it by the 15th Panzer Division and the 164th Light Division, with part of the Italian XX Corps to begin at 15:00 but under constant artillery and air attack this came to nothing. According to Rommel this attack did meet some success, with the Italians recapturing part of "Hill 28",

The bulk of the Australian 2/17th Battalion, which had defended the position, was forced to retreat. Rommel reversed his policy of distributing his armour across the front, ordering the 90th Light Division forward from Ed Daba and 21st Panzer Division north along with one third of the "Ariete" Division and half the artillery from the southern sector to join the 15th Panzer Division and the "Littorio" Division. The move could not be reversed because of the fuel shortage. The "Trieste" Division was ordered from Fuka to replace the 90th Light Division at Ed Daba but the 21st Panzer Division and the "Ariete" Division made slow progress during the night under constant attack from DAF bombers.

At the Kidney feature, the British failed to take advantage of the missing tanks. Each time they tried to move forward they were stopped by anti-tank guns. The Allied offensive was stalled. Churchill railed, "Is it really impossible to find a general who can win a battle?" Three Vickers Wellington torpedo bombers of 38 Squadron destroyed the oil tanker "Tergestea" at Tobruk during the night. Bristol Beaufort torpedo bombers of 42 Squadron, attached to No. 47 Squadron, sank the tanker "Proserpina" at Tobruk, removing the last hope for refuelling the .

By 26 October, XXX Corps had completed the capture of the bridgehead west of the second mine belt, the tanks of X Corps, established just beyond the infantry, had failed to break through the Axis anti-tank defences. Montgomery decided that over the next two days, while continuing the process of attrition, he would thin out his front line to create a reserve for another attack. The reserve was to include the 2nd New Zealand Division (with the 9th Armoured Brigade under command), the 10th Armoured Division and the 7th Armoured Division.
The attacks in the south, which lasted three days and caused considerable losses without achieving a breakthrough, were suspended.

By this time, the main battle was concentrated around Tel el Aqqaqir and the Kidney feature at the end of 1st Armoured Division's path through the minefield. A mile north-west of the feature was Outpost Woodcock and roughly the same distance south-west lay Outpost Snipe. An attack was planned on these areas using two battalions from 7th Motor Brigade. At 23:00 on 26 October 2 Battalion, The Rifle Brigade would attack Snipe and 2nd Battalion King's Royal Rifle Corps (KRRC) would attack Woodcock. The plan was for 2nd Armoured Brigade to pass round the north of Woodcock the following dawn and 24th Armoured Brigade round the south of Snipe. The attack was to be supported by all the available artillery of both X and XXX Corps.

Both battalions had difficulty finding their way in the dark and dust. At dawn, the KRRC had not reached its objective and had to find cover and dig in some distance from Woodcock. 2nd Rifle Brigade had had better fortune and after following the shell bursts of the supporting artillery dug in when they concluded they had reached their objective having encountered little opposition.

At 06:00, the 2nd Armoured Brigade commenced its advance and ran into such stiff opposition that, by noon, it had still not linked with the KRRC. The 24th Armoured Brigade started a little later and was soon in contact with the Rifle Brigade (having shelled them in error for a while). Some hours of confused fighting ensued involving tanks from the "Littorio" and troops and anti-tank guns from 15th Panzer which managed to keep the British armour at bay in spite of the support of the Rifle Brigade battle group's anti-tank guns. Rommel had decided to make two counter-attacks using his fresh troops. 90th Light Division was to make a fresh attempt to capture Point 29 and 21st Panzer were targeted at Snipe (the "Ariete" detachment had returned south).

At Snipe, mortar and shellfire was constant all day long. At 16:00, Rommel launched his major attack. German and Italian tanks moved forward. Against them the Rifle Brigade had 13 6-pounder anti-tank guns along with six more from the supporting 239th Anti-Tank Battery, RA. Although on the point of being overrun more than once they held their ground, destroying 22 German and 10 Italian tanks. The Germans gave up but in error the British battle group was withdrawn without being replaced that evening. Its CO, Lieutenant-Colonel Victor Buller Turner, was awarded the Victoria Cross. Only one anti-tank gun—from 239 Battery—was brought back.

When it was discovered that neither Woodcock nor Snipe was in Eighth Army hands, 133rd Lorried Infantry Brigade was sent to capture them. By 01:30 on 28 October, the 4th battalion Royal Sussex Regiment judged they were on Woodcock and dug in. At dawn, 2nd Armoured Brigade moved up in support but before contact could be made 4th Royal Sussex were counter-attacked and overrun with many losses. Meanwhile, the Lorried Brigade's two other battalions had moved on Snipe and dug in, only to find out the next day that they were in fact well short of their objective.

Further north, the 90th Light Division's attack on Point 29 during the afternoon of 27 October failed under heavy artillery and bombing which broke up the attack before it had closed with the Australians. The action at Snipe was an episode of the Battle of El Alamein described by the regiment's historian as the most famous day of the regiment's war. Lucas-Phillips, in his "Alamein" records that:

On 28 October, 15th and 21st Panzer made a determined attack on the X Corps front but were halted by sustained artillery, tank and anti-tank gun fire. In the afternoon, they paused to regroup to attack again but they were bombed for two and a half hours and were prevented from even forming up. This proved to be Rommel's last attempt to take the initiative and as such his defeat here represented a turning point in the battle.

At this point, Montgomery ordered the X Corps formations in the Woodcock-Snipe area to go over to defence while he focused his army's attack further to the north. Late on 27 October, the British 133rd Brigade was sent forward to recover lost positions but the next day, a good part of this force was overrun by German and Italian tanks from the Littorio and supporting 12th Bersaglieri Regiment and several hundred British soldiers were captured. On the night of 28/29 October, the 9th Australian Division was ordered to make a second set-piece attack. The 20th Australian Infantry Brigade with 40th R.T.R. in support would push north-west from Point 29 to form a base for 26th Australian Infantry Brigade with 46th R.T.R. in support, to attack north-east to an Axis location south of the railway known as Thompson's Post and then over the railway to the coast road, where they would advance south-east to close on the rear of the Axis troops in the coastal salient. An attack by the third brigade would then be launched on the salient from the south-east.

The 20th Brigade took its objectives with little trouble but 26th Brigade had more trouble. Because of the distances involved, the troops were riding on 46th R.T.R. Valentine tanks as well as carriers, which mines and anti-tank guns soon brought to grief, forcing the infantry to dismount. The infantry and tanks lost touch with each other in fighting with the 125th "Panzergrenadier" Regiment and a battalion of 7th Bersaglieri Regiment sent to reinforce the sector and the advance came to a halt. The Australians suffered 200 casualties in that attack and suffered 27 killed and 290 wounded. The German and Italian forces that had participated in the counter-attack formed an outpost and held on until the arrival of German reinforcements on 1 November.
It became clear that there were no longer enough hours of darkness left to reform, continue the attack and see it to its conclusion, so the operation was called off. By the end of these engagements in late October, the British had 800 tanks still in operation, while the "Panzerarmee" day report for 28 October (intercepted and read by Eighth Army the following evening) recorded 81 serviceable German tanks and 197 Italian. With the help of signals intelligence information the "Proserpina" (carrying 4,500 tonnes of fuel) and "Tergestea" (carrying 1,000 tonnes of fuel and 1,000 tonnes of ammunition) had been destroyed on 26 October and the tanker "Luisiano" (carrying 2,500 tonnes of fuel) had been sunk off the west coast of Greece by a torpedo from a Wellington bomber on 28 October. Rommel told his commanders, "It will be quite impossible for us to disengage from the enemy. There is no gasoline for such a manoeuvre. We have only one choice and that is to fight to the end at Alamein."

These actions by the Australians and British had alerted Montgomery that Rommel had committed his reserve in the form of 90th Light Division to the front and that its presence in the coastal sector suggested that Rommel was expecting the next major Eighth Army offensive in this sector. Montgomery determined therefore that it would take place further south on a front south of Point 29. The attack was to take place on the night of 31 October/1 November, as soon as he had completed the reorganisation of his front line to create the reserves needed for the offensive (although in the event it was postponed by 24 hours). To keep Rommel's attention on the coastal sector, Montgomery ordered the renewal of the 9th Australian Division operation on the night of 30/31 October.

The night of 30 October saw a continuation of previous Australian plans, their third attempt to reach the paved road. Although not all the objectives were achieved, by the end of the night they were astride the road and the railway, making the position of the Axis troops in the salient precarious. Rommel brought up a battlegroup from "21. Panzer-Division" and on 31 October, launched four successive attacks against "Thompson's Post". The fighting was intense and often hand-to-hand, but no ground was gained by the Axis forces. One of the Australians killed was Sergeant William Kibby (2/48th Infantry Battalion) who, for his heroic actions from the 23rd until his death on the 31st – including a lone attack on a machine-gun position at his own initiative – was awarded the Victoria Cross.

Again, on Sunday, 1 November Rommel tried to dislodge the Australians, but the brutal, desperate fighting resulted in nothing but lost men and equipment. He did however regain contact with "Panzergrenadier-Regiment 125" in the nose of the salient, and the supporting "10° battaglione Bersaglieri" – that fought well according to German and Allied sources; the "Bersaglieri" had resisted several Australian attacks even though they were (in the words of military historian Niall Barr) "surrounded on all sides, short of ammunition, food and water, [and] unable to evacuate their many wounded".

By now, it had become obvious to Rommel that the battle was lost. His fuel state continued to be critical: on 1 November, two more supply ships—the "Tripolino" and the "Ostia"—had been torpedoed and sunk from the air north-west of Tobruk. The shortage forced him to rely increasingly on fuel flown in from Crete on the orders of Albert Kesselring, Luftwaffe "Oberbefehlshaber Süd" ("OB Süd", Supreme Commander South), despite the restrictions imposed by heavy bombing of the airfields in Crete and the Desert Air Force's efforts to intercept the transport aircraft.

Rommel began to plan a retreat anticipating retiring to Fuka, some west, as he had only 90 tanks remaining in stark contrast with the Allies' 800. Large amounts of fuel arrived at Benghazi after the German forces had started to retreat, but little of it reached the front, a fact Kesselring tried to change by delivering it more closely to the fighting forces.

This phase of the battle began at 01:00 on 2 November, with the objective of destroying enemy armour, forcing the enemy to fight in the open, reducing the Axis stock of petrol, attacking and occupying enemy supply routes, and causing the disintegration of the enemy army. The intensity and the destruction in Supercharge were greater than anything witnessed so far during this battle. The objective of this operation was Tel el Aqqaqir, the base of the Axis defence roughly north-west of the Kidney feature and situated on the Rahman lateral track.

The initial thrust of Supercharge was to be carried out by the 2nd New Zealand Division. Lieutenant-General Bernard Freyberg, had tried to free them of this task, as they had lost 1,405 men in just three days, at El Ruweisat Ridge in July. However, in addition to its own 5th New Zealand Infantry Brigade and 28th (Maori) Infantry Battalion, the division was to have had placed under its command 151st (Durham) Brigade from 50th Division, 152nd (Seaforth and Camerons) Brigade from 51st Division and the 133rd Royal Sussex Lorried Infantry Brigade. In addition, the division was to have British 9th Armoured Brigade under command.

As in Operation Lightfoot, it was planned that two infantry brigades (the 151st on the right and 152nd on the left) each this time supported by a regiment of tanks—the 8th and 50th Royal Tank Regiments—would advance and clear a path through the mines. Once they reached their objectives, distant, 9th Armoured Brigade would pass through supported by a heavy artillery barrage and break open a gap in the Axis defences on and around the Rahman track, some further forward, which the 1st Armoured Division, following behind, would pass through into the open to take on Rommel's armoured reserves. Rommel had ordered 21st Panzer Division from the front line on 31 October to form a mobile counterattacking force. The division had left behind a "panzergrenadier" regiment which would bolster the "Trieste" Division which had been ordered forward to replace it. Rommel had also interspersed formations from the "Trieste" and 15th Panzer Divisions to "corset" his weaker forces in the front line. On 1 November the two German armoured divisions had 102 effective tanks to face Supercharge and the "Littorio" and "Trieste" Divisions had 65 tanks between them.

Supercharge started with a seven-hour aerial bombardment focused on Tel el Aqqaqir and Sidi Abd el Rahman, followed by a four and a half hour barrage of 360 guns firing 15,000 shells. The two assault brigades started their attack at 01:05 on 2 November and gained most of their objectives to schedule and with moderate losses. On the right of the main attack 28th (Maori) battalion captured positions to protect the right flank of the newly formed salient and 133rd Lorried Infantry did the same on the left. New Zealand engineers cleared five lines through the mines allowing the Royal Dragoons armoured car regiment to slip out into the open and spend the day raiding the Axis communications.

The 9th Armoured Brigade had started its approach march at 20:00 on 1 November from El Alamein railway station with around 130 tanks and arrived at its start line with only 94 runners (operational tanks). The brigade was to have started its attack towards Tel el Aqqaqir at 05:45 behind a barrage; the attack was postponed for 30 minutes while the brigade regrouped on Currie's orders. At 06:15, 30 minutes before dawn, the three regiments of the brigade advanced towards the gun line.

Brigadier Currie had tried to get the brigade out of doing this job, stating that he believed the brigade would be attacking on too wide a front with no reserves and that they would most likely have 50 percent losses.

The reply came from Freyberg that Montgomery

The German and Italian anti-tank guns (mostly Pak38 and Italian 47 mm guns, along with 24 of the formidable 88 mm flak guns) opened fire upon the charging tanks silhouetted by the rising sun. German tanks, which had penetrated between the Warwickshire Yeomanry and Royal Wiltshire Yeomanry, also caused many casualties. British tanks attacking the "Folgore" sector were fought off with petrol bombs and mortar fire as well as with the obsolete Italian 47 mm cannons. The Axis gun screen started to inflict a steady amount of damage upon the advancing tanks but was unable to stop them; over the course of the next 30 minutes, around 35 guns were destroyed and several hundred prisoners taken. The 9th Armoured Brigade had started the attack with 94 tanks and was reduced to only 14 operational tanks and of the 400 tank crew involved in the attack, 230 were killed, wounded or captured.

After the Brigade's action, Brigadier Gentry of 6th New Zealand Brigade went ahead to survey the scene. On seeing Brigadier Currie asleep on a stretcher, he approached him saying, "Sorry to wake you John, but I'd like to know where your tanks are?" Currie waved his hand at a group of tanks around him and replied "There they are". Gentry said "I don't mean your headquarters tanks, I mean your armoured regiments. Where are they?" Currie waved his arm and again replied, "There are my armoured regiments, Bill".

The brigade had sacrificed itself upon the gun line and caused great damage but had failed to create the gap for the 1st Armoured Division to pass through; however, soon after dawn 1st Armoured Division started to deploy and the remains of 9th Armoured Brigade came under its command. 2nd Armoured Brigade came up behind the 9th, and by mid-morning 8th Armoured Brigade had come up on its left, ordered to advance to the south-west. In heavy fighting during the day the British armour made little further progress. At 11:00 on 2 November, the remains of 15th Panzer, 21st Panzer and "Littorio" Armoured Divisions counter-attacked 1st Armoured Division and the remains of 9th Armoured Brigade, which by that time had dug in with a screen of anti-tank guns and artillery together with intensive air support. The counter-attack failed under a blanket of shells and bombs, resulting in a loss of some 100 tanks.
Although X Corps had failed in its attempt to break out, it had succeeded in its objective of finding and destroying enemy tanks. Although tank losses were approximately equal, this represented only a portion of the total British armour, but most of Rommel's tanks; the "Afrika Korps" strength of tanks fit for battle fell by 70 while in addition to the losses of the 9th Armoured Brigade, the 2nd and 8th Armoured Brigades lost 14 tanks in the fighting, with another 40 damaged or broken down. The fighting was later termed the "Hammering of the Panzers". In the late afternoon and early evening, the 133rd Lorried and 151st Infantry Brigades—by this time back under command of 51st Infantry Division—attacked respectively the Snipe and Skinflint (about a mile west of Snipe) positions in order to form a base for future operations. The heavy artillery concentration which accompanied their advance suppressed the opposition from the "Trieste" Division and the operation succeeded with few casualties.

On the night of 2 November, Montgomery once again reshuffled his infantry in order to bring four brigades (5th Indian, 151st, 5th New Zealand and 154th) into reserve under XXX Corps to prepare for the next thrust. He also reinforced X Corps by moving 7th Armoured Division from army reserve and sending 4th Light Armoured Brigade from XIII Corps in the south. General von Thoma's report to Rommel that night said he would have at most 35 tanks available to fight the next day and his artillery and anti-tank weapons had been reduced to ⅓ of their strength at the start of the battle. Rommel concluded that to forestall a breakthrough and the resulting destruction of his whole army he must start withdrawing to the planned position at Fuka. He called up "Ariete" from the south to join the mobile Italian XX Corps around Tel el Aqqaqir. His mobile forces (XX Corps, "Afrika Korps", 90th Light Division and 19th "Flak" Division) were ordered to make a fighting withdrawal while his other formations were to withdraw as best they could with the limited transport available.

At 20:30 on 2 November, Lumsden decided that one more effort by his X Corps would see the gun screen on the Rahman track defeated and ordered 7th Motor Brigade to seize the track along a front north of Tell el Aqqaqir. The 2nd and 8th Armoured Brigades would then pass through the infantry to a distance of about . On the morning of 3 November 7 Armoured Division would pass through and swing north heading for the railway at Ghazal station. 7th Motor Brigade set off at 01:15 on 3 November, but having received its orders late, had not had the chance to reconnoitre the battle area in daylight. This combined with stiff resistance led to the failure of their attack. As a consequence, the orders for the armour were changed and 2nd Armoured Brigade was tasked to support the forward battalion of 133rd Lorried Brigade (2nd King's Royal Rifle Corps) and 8th Armoured Brigade was to push south-west. Fighting continued throughout 3 November, but 2nd Armoured was held off by elements of the "Afrika Korps" and tanks of the "Littorio" Division. Further south, 8th Armoured Brigade was held off by anti-tank units helped later by tanks of the arriving "Ariete" Division.

On 2 November, Rommel signalled to Hitler that

and at 13.30 on 3 November Rommel received a reply,

Rommel thought the order (similar to one that had been given at the same time by Benito Mussolini through ),

Rommel ordered the Italian X and XXI Corps and the 90th Light Division to hold while the withdrew approximately west during the night of 3 November. The Italian XX Corps and the Division conformed to their position and Rommel replied to Hitler confirming his determination to hold the battlefield. The Desert Air Force continued its bombing and in its biggest day of the battle it flew 1,208 sorties and dropped of bombs.

On the night of 3/4 November, Montgomery ordered three of the infantry brigades in reserve to advance on the Rahman track as a prelude to an armoured break-out. At 17:45, the 152nd Infantry Brigade with the 8th RTR in support, attacked about south of Tel el Aqqaqir. The 5th Indian Infantry Brigade was to attack the track further south during the early hours of 4 November; at 06:15, the 154th Infantry Brigade was to attack Tel el Aqqaqir. The 152nd Infantry Brigade was mistakenly told the Axis had withdrawn from their objectives and unexpectedly met determined resistance. Communications failed and the forward infantry elements ended up digging in well short of their objective. By the time the 5th Indian Brigade set off, the defenders had begun to withdraw and their objective was taken virtually unopposed. By the time the 154th Brigade moved into some artillery-fire, the Axis had left.

On 4 November, the Eighth Army plan for pursuit began at dawn; no fresh units were available and the 1st and 7th Armoured divisions were to turn northwards to roll up the Axis units still in the forward lines. The 2nd New Zealand Division with two lorried infantry brigades and the 9th Armoured and 4th Light Armoured brigades under command, was to head west along desert tracks to the escarpment above Fuka, about away. The New Zealanders got off to a slow start because its units were dispersed after the recent fighting and took time to concentrate. Paths through the minefields were congested and had deteriorated, which caused more delays. By dark, the division was only west of the Rahman track, the 9th Armoured Brigade was still at the track and the 6th New Zealand Brigade even further back.

The plan to trap the 90th Light Division with the 1st and 7th Armoured divisions misfired. The 1st Armoured Division came into contact with the remnants of 21st Panzer Division and had to spend most of the day pushing them back . The 7th Armoured Division was held up by the Armoured Division, which was destroyed conducting a determined resistance. In his diary, Rommel wrote

The Armoured Division and the Motorised Division were also destroyed. Berlin radio claimed that in this sector the "British were made to pay for their penetration with enormous losses in men and material. The Italians fought to the last man." The British took many prisoners, since the remnants of Italian infantry divisions were not motorised and could not escape from encirclement. Private Sid Martindale, 1st Battalion Argyll & Sutherland Highlanders, wrote about the "Bologna" Division, which had taken the full weight of the British armoured attack:

The and the remnants of the Division tried to fight their way out and marched into the desert without water, food or transport before surrendering, exhausted and dying from dehydration. It was reported that Colonel Arrigo Dall'Olio, commanding the 40th Infantry Regiment of the Division, surrendered saying, "We have ceased firing not because we haven't the desire but because we have spent every round". In a symbolic act of defiance, no one in 40th Infantry Regiment raised their hands. Harry Zinder of "Time" magazine noted that the Italians fought better than had been expected and commented that for the Italians

By late morning on 4 November, Rommel realised his situation was desperate,

Rommel telegraphed Hitler for permission to fall back on Fuka. As further Allied blows fell, Thoma was captured and reports came in from the and divisions that they were encircled. At 17:30, unable to wait any longer for a reply from Hitler, Rommel gave orders to retreat.

Due to lack of transport, most of the Italian infantry formations were abandoned. Any chance of getting them away with an earlier move had been spoiled by Hitler's insistence that Rommel hold his ground, obliging him to keep the non-motorised Italian units well forward until it was too late. To deepen the armoured thrusts, the 1st Armoured Division was directed at El Daba, down the coast and the 7th Armoured Division towards Galal, a further west along the railway. The New Zealand Division group had hoped to reach their objective by mid-morning on 5 November but was held up by artillery-fire when picking their way through what turned out to be a dummy minefield and the 15th Panzer Division got there first.

The 7th Armoured Division was ordered cross-country to cut the coast road at Sidi Haneish, west of the Rahman track, while the 1st Armoured Division, west of El Dada, was ordered to take a wide detour through the desert to Bir Khalda, west of the Rahman track, preparatory to turning north to cut the road at Mersa Matruh. Both moves failed, the 7th Armoured Division finished the day short of its objective. The 1st Armoured Division tried to make up time with a night march but in the darkness the armour became separated from their support vehicles and ran out of fuel at dawn on 6 November, short of Bir Khalda. The DAF continued to fly in support but because of the dispersion of X Corps, it was difficult to establish bomb lines, beyond which, aircraft were free to attack.

By 11:00 on 6 November, the "B" Echelon support vehicles began to reach the 1st Armoured Division but with only enough fuel to replenish two of the armoured regiments, which set off again hoping to be in time to cut off the Axis. The regiments ran out of fuel again, south-west of Mersa Matruh. A fuel convoy had set out from Alamein on the evening of 5 November but progress was slow as the tracks had become very cut up. By midday on 6 November, it began to rain and the convoy bogged from the rendezvous with the 1st Armoured Division "B". The 2nd New Zealand Division advanced toward Sidi Haneish while the 8th Armoured Brigade, 10th Armoured Division, had moved west from Galal to occupy the landing fields at Fuka and the escarpment. Roughly south-west of Sidi Haneish, the 7th Armoured Division encountered the 21st Panzer Division and the "Voss" Reconnaissance Group that morning. In a running fight, the 21st Panzer Division lost 16 tanks and numerous guns, narrowly escaping encirclement and reached Mersa Matruh that evening. It was again difficult to define bomb lines but US heavy bombers attacked Tobruk, sinking [] and later attacked Benghazi, sinking and setting the tanker (6,572 GRT), alight.

On 7 November, waterlogged ground and lack of fuel stranded the 1st and 7th Armoured divisions. The 10th Armoured Division, on the coast road and with ample fuel, advanced to Mersa Matruh while its infantry mopped up on the road west of Galal. Rommel intended to fight a delaying action at Sidi Barrani, west of Matruh, to gain time for Axis troops to get through the bottlenecks at Halfaya and Sollum. The last rearguards left Matruh on the night of 7/8 November but were only able to hold Sidi Barrani until the evening of 9 November. By the evening of 10 November, the 2nd New Zealand Division, heading for Sollum, had the 4th Light Armoured Brigade at the foot of the Halfaya Pass while 7th Armoured Division was conducting another detour to the south, to take Fort Capuzzo and Sidi Azeiz. On the morning of 11 November, the 5th New Zealand Infantry Brigade captured the pass, taking 600 Italian prisoners. By nightfall on 11 November, the Egyptian wall was clear but Montgomery was forced to order that the pursuit should temporarily be continued only by armoured cars and artillery, because of the difficulty in supplying larger formations west of Bardia.

El Alamein was an Allied victory, although Rommel did not lose hope until the end of the Tunisia Campaign. Churchill said,

The Allies frequently had numerical superiority in the Western Desert but never had it been so complete in quantity and quality. With the arrival of Sherman tanks, 6-pounder anti-tank guns and Spitfires in the Western Desert, the Allies gained a comprehensive superiority. Montgomery envisioned the battle as an attrition operation, similar to those fought in the First World War and accurately predicted the length of the battle and the number of Allied casualties. Allied artillery was superbly handled and Allied air support was excellent, in contrast to the "Luftwaffe" and "Regia Aeronautica", which offered little or no support to ground forces, preferring to engage in air-to-air combat. Air supremacy had a huge effect on the battle. Montgomery wrote,

Historians debate the reasons Rommel decided to advance into Egypt. In 1997, Martin van Creveld wrote that Rommel had been advised by the German and Italian staffs that his army could not properly be supplied so far from the ports of Tripoli and Benghazi. Rommel pressed ahead with his advance to Alamein and as predicted, supply difficulties limited the attacking potential of the axis forces. According to Maurice Remy (2002), Hitler and Mussolini put pressure on Rommel to advance. Rommel had been very pessimistic, especially after the First Battle of El Alamein, and knew that as US supplies were en route to Africa and Axis ships were being sunk in the Mediterranean, the Axis was losing a race against time. On 27 August, Kesselring promised Rommel that supplies would arrive in time but Westphal pointed out that such an expectation would be unrealistic and the offensive should not begin until they had arrived. After a conversation with Kesselring on 30 August, Rommel decided to attack, "the hardest [decision] in my life".

Italian Defence ministry chief of staff Luigi Binelli Mantelli said: "The spirit of service and cohesion are fundamental elements for the operational capacity of the armed forces ... The (Folgore) Paratroopers have always shown this. El Alamein was a battle that was lost with great honour, facing up to overwhelmingly superior fire power with poor weapons but with great spirit and capacity to resist and to hold up high the honour of Italy". German General Erwin Rommel described the Italians as "extraordinary, courageous, disciplined, but badly commanded and equipped." He also said the famous phrase (written on a commemorative plaque at El Alamein):

Winston Churchill said in a speech to the House of Commons a month after El Alamein: "We must honour the men that were the Lions of the Folgore". British historian John Bierman said that the Italian tank regiment "fought with great audacity, just as the Ariete artillery regiment did". According to American historian John W. Gordon, the British special forces were so impressed by the methods and tactics of the Italian desert corps "that they actually copied them". On October 30, 2012, a historical park was unveiled on the battle site to show visitors key points in the Egyptian desert. Padua University has worked with 266 volunteers on the project, which also aims to protect the site from oil exploration and coastal building development. A major exhibition on the battle was staged in Milan in 2008.

In 2005, Niall Barr wrote that the casualties, was an estimate because of the chaos of the Axis retreat. British figures, based on Ultra intercepts, gave German casualties as and captured. Italian losses were and captured. By 11 November, the number of Axis prisoners had risen to In a note to "The Rommel Papers", Fritz Bayerlein (quoting figures obtained from ) instead estimated German losses in the battle as 1,100 killed, 3,900 wounded and 7,900 prisoners and Italian losses as 1,200 killed, 1,600 wounded and 20,000 prisoners.

According to the Italian official history, Axis losses during the battle were 4,000 to 5,000 killed or missing, 7,000 to 8,000 wounded and 17,000 prisoners; during the retreat the losses rose to 9,000 killed or missing, 15,000 wounded and 35,000 prisoners. According to General Giuseppe Rizzo, total Axis casualties included 25,000 men killed or wounded (including 5,920 Italians killed) and 30,000 prisoners (20,000 Italians and 10,724 Germans), 510 tanks and 2,000 field guns, anti-tank guns, anti-aircraft guns. Axis tank losses were on 4 November, only tanks were left out of the the beginning of the battle. About half of the tanks had been lost and most of the remainder were knocked out on the next day by the 7th Armoured Division. About guns were lost, along with and aircraft.

The Eighth Army had of whom had been killed, and missing; of the casualties were British, Australian, New Zealanders, South African, Indian and Allied forces. The Eighth Army lost from tanks, although by the end of the battle, had been repaired. The artillery lost and the DAF lost and aircraft.

The Eighth Army was surprised by the Axis withdrawal and confusion caused by redeployments between the three corps meant they were slow in pursuit, failing to cut off Rommel at Fuka and Mersa Matruh. The Desert Air Force failed to make a maximum effort to bomb a disorganised and retreating opponent, which on 5 November was within range and confined to the coast road. Supply shortages and a belief that the "Luftwaffe" were about to get strong reinforcements, led the DAF to be cautious, reduce the number of offensive sorties on 5 November and protect the Eighth Army.

The Axis made a fighting withdrawal to El Agheila but the Axis troops were exhausted and had received few replacements, while Montgomery had planned to transport material over great distances, to provide the Eighth Army with of supplies per day. Huge quantities of engineer stores had been collected to repair the coast road; the railway line from El Alamein to Fort Capuzzo, despite having been blown up in over 200 places, was quickly repaired. In the month after Eighth Army reached Capuzzo, the railway carried of supplies. Benghazi handled a day by the end of December, rather than the expected .

Montgomery paused for three weeks to concentrate his forces and prepare an assault on El Agheila to deny the Axis the possibility of a counter-attack . On 11 December, Montgomery launched the 51st (Highland) Division along the line of the coast road with the 7th Armoured Division on the inland flank. On 12 December the 2nd New Zealand Division started a deeper flanking manoeuvre to cut the Axis line of retreat on the coast road in the rear of the Mersa Brega position. The Highland Division made a slow and costly advance and 7th Armoured Division met stiff resistance from the "Ariete" Combat Group (the remains of the "Ariete" Armoured Division). The had lost roughly 75,000 men, 1,000 guns and 500 tanks since the Second Battle of Alamein and withdrew. By 15 December, the New Zealanders had reached the coast road but the firm terrain allowed Rommel to break his forces into smaller units and withdraw cross-country through the gaps between the New Zealand positions.

Rommel conducted a text-book retreat, destroying all equipment and infrastructure left behind and peppering the land behind him with mines and booby traps. The Eighth Army reached Sirte on 25 December but west of the port, were forced to pause to consolidate their strung out formations and to prepare an attack at Wadi Zemzem, near Buerat east of Tripoli. Rommel had, with the agreement of Field Marshal Bastico, sent a request to the Italian "Commando Supremo" in Rome to withdraw to Tunisia where the terrain would better suit a defensive action and where he could link with the Axis army forming there, in response to the Operation Torch landings. Mussolini replied on 19 December that the must resist to the last man at Buerat.

On 15 January 1943, the 51st (Highland) Division made a frontal attack while the 2nd New Zealand Division and the 7th Armoured Division drove around the inland flank of the Axis line. Weakened by the withdrawal of 21st Panzer Division to Tunisia to strengthen the 5th Panzer Army (Hans-Jürgen von Arnim), Rommel conducted a fighting retreat. The port of Tripoli, further west, was taken on 23 January as Rommel continued to withdraw to the Mareth Line, the French southern defensive position in Tunisia.

Rommel was by this time in contact with the Fifth Panzer Army, which had been fighting against the multi-national First Army in northern Tunisia, since shortly after Operation Torch. Hitler was determined to retain Tunisia and Rommel finally started to receive replacement men and materials. The Axis faced a war on two fronts, with the Eighth Army approaching from the east and the British, French and Americans from the west. The German-Italian Panzer Army was renamed the Italian First Army (General Giovanni Messe) and Rommel assumed command of the new Army Group Africa, responsible for both fronts. The two Allied armies were commanded by the 18th Army Group (General Harold Alexander). The failure of the First Army in the run for Tunis in December 1942 led to a longer North African campaign which ended when the Italian-German forces in North Africa capitulated in May 1943.





</doc>
<doc id="29588" url="https://en.wikipedia.org/wiki?curid=29588" title="Sextant">
Sextant

A sextant is a doubly reflecting navigation instrument that measures the angular distance between two visible objects. The primary use of a sextant is to measure the angle between an astronomical object and the horizon for the purposes of celestial navigation. The estimation of this angle, the altitude, is known as "sighting" or "shooting" the object, or "taking a sight". The angle, and the time when it was measured, can be used to calculate a position line on a nautical or aeronautical chart—for example, sighting the Sun at noon or Polaris at night (in the Northern Hemisphere) to estimate latitude. Sighting the height of a landmark can give a measure of "distance off" and, held horizontally, a sextant can measure angles between objects for a position on a chart. A sextant can also be used to measure the lunar distance between the moon and another celestial object (such as a star or planet) in order to determine Greenwich Mean Time and hence longitude. The principle of the instrument was first implemented around 1731 by John Hadley (1682–1744) and Thomas Godfrey (1704–1749), but it was also found later in the unpublished writings of Isaac Newton (1643–1727). Additional links can be found to Bartholomew Gosnold (1571–1607) indicating that the use of a sextant for nautical navigation predates Hadley's implementation. (This reference to Gosnold's use of a sextant, in a popular British travel magazine article, rather than in a nautical history journal, does not cite sources, and is very probably inaccurate.) In 1922, it was modified for aeronautical navigation by Portuguese navigator and naval officer .

This section discusses navigators' sextants. Most of what is said about these specific sextants applies equally to other types of sextants. Navigators' sextants were primarily used for ocean navigation.

Like the Davis quadrant, the sextant allows celestial objects to be measured relative to the horizon, rather than relative to the instrument. This allows excellent precision. Also, unlike the backstaff, the sextant allows direct observations of stars. This permits the use of the sextant at night when a backstaff is difficult to use. For solar observations, filters allow direct observation of the sun.

Since the measurement is relative to the horizon, the measuring pointer is a beam of light that reaches to the horizon. The measurement is thus limited by the angular accuracy of the instrument and not the sine error of the length of an alidade, as it is in a mariner's astrolabe or similar older instrument.

A sextant does not require a completely steady aim, because it measures a relative angle. For example, when a sextant is used on a moving ship, the image of both horizon and celestial object will move around in the field of view. However, the relative position of the two images will remain steady, and as long as the user can determine when the celestial object touches the horizon, the accuracy of the measurement will remain high compared to the magnitude of the movement.

The sextant is not dependent upon electricity (unlike many forms of modern navigation) or anything human-controlled (like GPS satellites). For these reasons, it is considered an eminently practical back-up navigation tool for ships.

The frame of a sextant is in the shape of a sector which is approximately of a circle (60°), hence its name ("sextāns, -antis" is the Latin word for "one sixth"). Both smaller and larger instruments are (or were) in use: the octant, quintant (or pentant) and the (doubly reflecting) quadrant span sectors of approximately of a circle (45°), of a circle (72°) and of a circle (90°), respectively. All of these instruments may be termed "sextants".

Attached to the frame are the "horizon mirror", an "index arm" which moves the "index mirror", a sighting telescope, sun shades, a graduated scale and a micrometer drum gauge for accurate measurements. The scale must be graduated so that the marked degree divisions register twice the angle through which the index arm turns. The scales of the octant, sextant, quintant and quadrant are graduated from below zero to 90°, 120°, 140° and 180° respectively. For example, the sextant shown alongside has a scale graduated from −10° to 142°, so that is basically a quintant: the frame is a sector of a circle subtending an angle of 76° (not 72°) at the pivot of the index arm.

The necessity for the doubled scale reading follows by consideration of the relations of the fixed ray (between the mirrors), the object ray (from the sighted object) and the direction of the normal perpendicular to the index mirror. When the index arm moves by an angle, say 20°, the angle between the fixed ray and the normal also increases by 20°. But the angle of incidence equals the angle of reflection so the angle between the object ray and the normal must also increase by 20°. The angle between the fixed ray and the object ray must therefore increase by 40°. This is the case shown in the graphic alongside.

There are two types of horizon mirrors on the market today. Both types give good results.

Traditional sextants have a half-horizon mirror, which divides the field of view in two. On one side, there is a view of the horizon; on the other side, a view of the celestial object. The advantage of this type is that both the horizon and celestial object are bright and as clear as possible. This is superior at night and in haze, when the horizon can be difficult to see. However, one has to sweep the celestial object to ensure that the lowest limb of the celestial object touches the horizon.

Whole-horizon sextants use a half-silvered horizon mirror to provide a full view of the horizon. This makes it easy to see when the bottom limb of a celestial object touches the horizon. Since most sights are of the sun or moon, and haze is rare without overcast, the low-light advantages of the half-horizon mirror are rarely important in practice.

In both types, larger mirrors give a larger field of view, and thus make it easier to find a celestial object. Modern sextants often have 5 cm or larger mirrors, while 19th-century sextants rarely had a mirror larger than 2.5 cm (one inch). In large part, this is because precision flat mirrors have grown less expensive to manufacture and to silver.

An artificial horizon is useful when the horizon is invisible, as occurs in fog, on moonless nights, in a calm, when sighting through a window or on land surrounded by trees or buildings. Professional sextants can mount an artificial horizon in place of the horizon-mirror assembly. An artificial horizon is usually a mirror that views a fluid-filled tube with a bubble.

Most sextants also have filters for use when viewing the sun and reducing the effects of haze. The filters usually consist of a series of progressively darker glasses that can be used singly or in combination to reduce haze and the sun's brightness. However, sextants with adjustable polarizing filters have also been manufactured, where the degree of darkness is adjusted by twisting the frame of the filter.

Most sextants mount a 1 or 3-power monocular for viewing. Many users prefer a simple sighting tube, which has a wider, brighter field of view and is easier to use at night. Some navigators mount a light-amplifying monocular to help see the horizon on moonless nights. Others prefer to use a lit artificial horizon.

Professional sextants use a click-stop degree measure and a worm adjustment that reads to a minute, 1/60 of a degree. Most sextants also include a vernier on the worm dial that reads to 0.1 minute. Since 1 minute of error is about a nautical mile, the best possible accuracy of celestial navigation is about . At sea, results within several nautical miles, well within visual range, are acceptable. A highly skilled and experienced navigator can determine position to an accuracy of about .

A change in temperature can warp the arc, creating inaccuracies. Many navigators purchase weatherproof cases so that their sextant can be placed outside the cabin to come to equilibrium with outside temperatures. The standard frame designs (see illustration) are supposed to equalise differential angular error from temperature changes. The handle is separated from the arc and frame so that body heat does not warp the frame. Sextants for tropical use are often painted white to reflect sunlight and remain relatively cool. High-precision sextants have an invar (a special low-expansion steel) frame and arc. Some scientific sextants have been constructed of quartz or ceramics with even lower expansions. Many commercial sextants use low-expansion brass or aluminium. Brass is lower-expansion than aluminium, but aluminium sextants are lighter and less tiring to use. Some say they are more accurate because one's hand trembles less. Solid brass frame sextants are less susceptible to wobbling in high winds or when the vessel is working in heavy seas, but as noted are substantially heavier. Sextants with aluminum frames and brass arcs have also been manufactured. Essentially, a sextant is intensely personal to each navigator, and he or she will choose whichever model has the features which suit them best.

Aircraft sextants are now out of production, but had special features. Most had artificial horizons to permit taking a sight through a flush overhead window. Some also had mechanical averagers to make hundreds of measurements per sight for compensation of random accelerations in the artificial horizon's fluid. Older aircraft sextants had two visual paths, one standard and the other designed for use in open-cockpit aircraft that let one view from directly over the sextant in one's lap. More modern aircraft sextants were periscopic with only a small projection above the fuselage. With these, the navigator pre-computed his sight and then noted the difference in observed versus predicted height of the body to determine his position.

A "sight" (or "measure") of the angle between the sun, a star, or a planet, and the horizon is done with the 'star telescope' fitted to the sextant using a visible horizon. On a vessel at sea even on misty days a sight may be done from a low height above the water to give a more definite, better horizon. Navigators hold the sextant by its handle in the right hand, avoiding touching the arc with the fingers.

For a sun sight, a filter is used to overcome the glare such as "shades" covering both index mirror and the horizon mirror designed to prevent eye damage. By setting the index bar to zero, the sun can be viewed through the telescope. Releasing the index bar (either by releasing a clamping screw, or on modern instruments, using the quick-release button), the image of the sun can be brought down to about the level of the horizon. It is necessary to flip back the horizon mirror shade to be able to see the horizon, and then the fine adjustment screw on the end of the index bar is turned until the bottom curve (the "lower limb") of the sun just touches the horizon. 'Swinging' the sextant about the axis of the telescope ensures that the reading is being taken with the instrument held vertically. The angle of the sight is then read from the scale on the arc, making use of the micrometer or vernier scale provided. The exact time of the sight must also be noted simultaneously, and the height of the eye above sea-level recorded.

An alternative method is to estimate the current altitude (angle) of the sun from navigation tables, then set the index bar to that angle on the arc, apply suitable shades only to the index mirror, and point the instrument directly at the horizon, sweeping it from side to side until a flash of the sun's rays are seen in the telescope. Fine adjustments are then made as above. This method is less likely to be successful for sighting stars and planets.

Star and planet sights are normally taken during nautical twilight at dawn or dusk, while both the heavenly bodies and the sea horizon are visible. There is no need to use shades or to distinguish the lower limb as the body appears as a mere point in the telescope. The moon can be sighted, but it appears to move very fast, appears to have different sizes at different times, and sometimes only the lower or upper limb can be distinguished due to its phase.

After a sight is taken, it is reduced to a position by looking at several mathematical procedures. The simplest sight reduction is to draw the equal-altitude circle of the sighted celestial object on a globe. The intersection of that circle with a dead-reckoning track, or another sighting, gives a more precise location.

Sextants can be used very accurately to measure other visible angles, for example between one heavenly body and another and between landmarks ashore. Used horizontally, a sextant can measure the apparent angle between two landmarks such as a lighthouse and a church spire, which can then be used to find the distance "off" or out to sea (provided the distance between the two landmarks is known). Used vertically, a measurement of the angle between the lantern of a lighthouse of known height and the sea level at its base can also be used for distance off.

Due to the sensitivity of the instrument it is easy to knock the mirrors out of adjustment. For this reason a sextant should be checked frequently for errors and adjusted accordingly.

There are four errors that can be adjusted by the navigator, and they should be removed in the following order.





</doc>
<doc id="29589" url="https://en.wikipedia.org/wiki?curid=29589" title="Single transferable vote">
Single transferable vote

The single transferable vote (STV) is a proportional voting system designed to achieve fairly-mixed proportional representation through voters casting single transferable votes in multi-seat organizations or constituencies (voting districts). There are various ways of counting votes under STV, as described below.

Under STV, each elector (voter) gets a single vote in an election electing multiple winners. Each elector marks their ballot vote for the most preferred candidate and also marks back-up preferences. The vote goes to the voter's first preference if possible; but if their first preference is eliminated, instead of being thrown away the vote is transferred to a back-up preference, with the vote being assigned to the voter's second, third, or lower choice or being apportioned fractionally to different candidates. 

The counting process works this way: Votes are totaled, and a quota (the minimum number of votes required to win a seat) is derived. If the elector's first-ranked candidate achieves the quota, the candidate is declared elected; and, in some STV systems, any surplus vote is transferred to other candidates in proportion to the next back-up preference marked on the ballots. If more candidates than seats remain, the candidate with the fewest votes is eliminated, with the votes in their favour being transferred to other candidates as determined by the voters' next back-up preference. These elections and eliminations, and vote transfers if applicable, continue until enough candidates exceed quota to fill all the seats or until there are only as many remaining candidates as there are unfilled seats, when the remaining candidates are declared elected. 

The specific method of transferring votes varies in different systems (see ).

Advocates for STV say that it enables votes to be cast for individual candidates rather than for parties and party machine-controlled party lists, andcompared to first-past-the-post (FPTP) votingreduces "wasted" votes (votes being wasted on losers and surplus votes being wasted on sure winners) by transferring them to other preferred candidates.

STV also provides approximately proportional representation, even in non-partisan elections, ensuring that minority factions have some representation. The key to STV's achievement of proportionality is that each elector (voter) only casts "one single vote", in an election "electing multiple winners". 

STV elections grow more proportional in direct relation to the number of seats to be elected in each constituencythe more seats to be won, the more the distribution of the seats in an STV election will be proportional. For example, in a three-seat STV election using a Hare Quota formula_1, a candidate needs only a third of the votes to win a seat. In a seven-seat STV election, any candidate who can get the support of about 14 percent of the vote (either first preferences alone or a combination of first preferences and transferred lower-ranked preferences) will win a seat.

Advocates for STV argue it is an improvement over winner-take-all non-proportional voting systems such as first-past-the-post, where vote splits commonly result in a majority of voters electing no one and the successful candidate having support from just a minority of the district voters. STV prevents in most cases one single party taking all the seats and in its thinning out of the candidates in the field prevents the election of an extreme candidate or party if it does not have enough overall general appeal.

STV is the system of choice of the Proportional Representation Society of Australia (which calls it quota-preferential proportional representation), the Electoral Reform Society in the United Kingdom and FairVote in the USA (which refers to STV as fair representation voting and instant-runoff voting as "ranked-choice voting", although there are other preferential voting methods that use ranked-choice ballots). 

Its critics contend that some voters find the mechanisms behind STV difficult to understand, but this does not make it more difficult for voters to rank the list of candidates in order of preference on an STV ballot paper (see ).

Others see a vote transfer process that is more time-consuming than in first-past-the-post elections where the result is known within a few hours and say it is not worth using STV just to have more proportional results. However, STV's supporters say that some winners are known in the same period as under FPTP, and that with delays under FPTP caused by mail-in or absentee ballots, any delays in an STV scenario are not noticeable or are no great hardship.

STV has had its widest adoption in the English-speaking world. , in government elections, STV is used for:
In British Columbia, Canada, a type of STV called BC-STV was recommended for provincial elections by the British Columbia Citizens' Assembly on Electoral Reform in 2004. In a 2005 provincial referendum, it received 57.69% support and passed in 77 of 79 electoral districts. It was not adopted, however, because it fell short of the 60% threshold requirement the BC Liberal government had set for the referendum to be binding. In a second referendum, on 12 May 2009, BC-STV was defeated 60.91% to 39.09%.

STV has also been used in several other jurisdictions, particularly in provincial elections in the cities of Edmonton and Calgary in Alberta (until 1959, when the Alberta provincial government changed it to first past the post). Less well known is STV use at the municipal level in western Canada – Calgary used STV for more than 50 years before it was changed to first past the post. For a more complete list, see "History and use of the single transferable vote".

When STV is used for single-winner elections, it is equivalent to the instant-runoff voting (alternative vote) method. STV used for multi-winner elections is sometimes called "proportional representation through the single transferable vote", or PR-STV. "STV" usually refers to the multi-winner version, as it does in this article. In the United States, it is sometimes called choice voting, preferential voting, or preference voting. ("Preferential voting" can also refer to a broader category, ranked voting systems.)

Hare-Clark is the name given to PR-STV elections in Tasmania and the Australian Capital Territory.

In STV, each voter ranks the candidates in order of preference, marking a '1' beside their most preferred candidate, a '2' beside their second most preferred, and so on as shown in the sample ballot on the right. As noted, this is a simplified example. In practice, the ballot would usually be organized in columns so that voters are informed of each candidate's party affiliations or whether they are standing as independents.

The most straightforward way to count a ranked ballot vote is simply to sequentially identify the candidate with the least support, eliminate that candidate, and transfer those votes to the next-named candidate on each ballot. This process is repeated until there are only as many candidates left as seats available. This method was used for a period of time in several local elections in South Australia. In effect, it is identical to instant-runoff voting, which is commonly used in leadership contests, except that the transfer process is terminated when there are still several candidates remaining, if all the seats have been filled. However, preferences for elected candidates are not transferred at any value, possibly penalising those who vote for a popular candidate.

In most STV elections, an additional step is taken that ensures that all elected candidates are elected with approximately equal numbers of votes. It can be shown that a candidate requires a minimum number of votes – the quota (or threshold) – to be elected. A number of different quotas can be used; the most common is the Droop quota, given by the floor function formula:

The Droop quota is an extension of requiring a 50% + 1 majority in single-winner elections. For example, at most 3 people can have 25% + 1 in 3-winner elections, 9 can have 10% + 1 in 9-winner elections, and so on.

If fractional votes can be submitted, then the Droop quota may be modified so that the fraction is not rounded down.
Major Frank Britton, of the Election Ballot Services at the Electoral Reform Society, observed that the final plus one of the Droop quota is not needed; the exact quota is then simply formula_2. Without fractional votes, the equivalent integer quota may be written:

So, the quota for one seat is fifty out of a hundred votes, not fifty-one.

An STV election count starts with a count of each voters' first choice, recording how many for each candidate, calculation of the total number of votes and the quota and then taking the following steps:


There are variations, such as how to transfer surplus votes from winning candidates and whether to transfer votes to already-elected candidates. When the number of votes transferred from the losing candidate with the fewest votes is too small to change the ordering of remaining candidates, more than one candidate can be eliminated simultaneously.

One simplistic formula for how to transfer surplus votes is:

however, this can produce fractional votes. See for a discussion of how this is handled.

If a candidate is eliminated and their votes are transferred to already victorious candidates, then the new excess votes for the victorious candidate (transferred from the eliminated candidate) will be transferred to the next preference of the victorious candidate, as happened with their initial excess. However, any votes which would transfer from the victorious candidate to one who was already eliminated must be reallocated. See for details.

Because votes cast for losing candidates and excess votes cast for winning candidates are transferred to voters' next choice candidates, STV is said to minimize wasted votes.

Suppose a food election is conducted to determine what three foods to serve at a party. There are 5 candidates, 3 of which will be chosen. The candidates are: "Oranges", "Pears", "Chocolate", "Strawberries", and "Hamburgers". The 20 guests at the party mark their ballots according to the table below. In this example, a second choice is made by only some of the voters.

First, the quota is calculated. Using the Droop quota, with 20 voters and 3 winners to be found, the number of votes required to be elected is:

When ballots are counted the election proceeds as follows:

Result: The winners are Chocolate, Oranges and Strawberries. This result differs from the one that would have occurred if the three winners were decided by first preference plurality rankings, in which case Pear would have been a winner as opposed to Strawberry for having a greater number of first preference votes.

STV systems primarily differ in how they transfer votes and in the size of the quota. For this reason some have suggested that STV can be considered a family of voting systems rather than a single system. 

The quota must be set so that no more candidates can reach quota than there are seats to be filled. The Droop quota is the most commonly used quota. It being relatively low means that the largest party is likely to take the majority of the seats in a district. The Hare quota, which was used in the original proposals by Thomas Hare, ensures greater representation to less-popular parties with in a district.

The easiest methods of transferring surpluses involve an element of randomness; partially random systems, such as the Hare system, are used in the Republic of Ireland (except Senate elections) and in Malta, among other places. The Gregory method (also known as Newland-Britain or Senatorial rules) eliminates randomness by allowing for the transfer of fractions of votes. Gregory is in use in Northern Ireland, the Republic of Ireland (Senate elections) and in Australia. Both Gregory and earlier methods have the problem that in some circumstances they do not treat all votes equally. For this reason Meek's method, Warren's method and the Wright system have been invented. While easier methods can usually be counted by hand, except in a very small election Meek and Warren require counting to be conducted by computer. The Wright system is a refinement of the Australian Senate system replacing the process of distribution and segmentation of preferences by a reiterative counting process where the count is reset and restarted on every exclusion. Meek is used in local body elections in New Zealand.

Meek in 1969 was the first to realize that computers make it possible to count votes in way that is conceptually simpler and closer to the original concept of STV. One advantage of Meek's method is that the quota is adjusted at each stage of counting when the number of votes decreases because some become non-transferable.

Meek also considered a variant on his system which allows for equal preferences to be expressed. This has subsequently (since 1998) been used by the John Muir Trust for electing its trustees.

The concept of transferable voting was first proposed by Thomas Wright Hill in 1819. The system remained unused in public elections until 1855, when Carl Andræ proposed a transferable vote system for elections in Denmark, and his system was used in 1856 to elect the Rigsraad and from 1866 it was also adapted for indirect elections to the second chamber, the Landsting, until 1915.

Although he was not the first to propose transferable votes, the English barrister Thomas Hare is generally credited with the conception of STV, and he may have independently developed the idea in 1857. Hare's view was that STV should be a means of "making the exercise of the suffrage a step in the elevation of the individual character, whether it be found in the majority or the minority." In Hare's original system, he further proposed that electors should have the opportunity of discovering which candidate their vote had ultimately counted for, to improve their personal connection with voting. At the time of Hare's original proposal, the UK did not use the secret ballot, so not only could the voter determine the ultimate role of their vote in the election, the elected MPs would have been able to determine who had voted for them. As Hare envisaged that the whole House of Commons be elected "at large" this would have replaced geographical constituencies with what Hare called "constituencies of interest" – those people who had actually voted for each MP. In modern elections, held by secret ballot, a voter can discover how their vote was distributed by viewing detailed election results. This is particularly easy to do using Meek's method, where only the final weightings of each candidate need to be published. The elected member is, however, unable to verify whom their supporters were.

The noted political essayist John Stuart Mill was a friend of Hare's and an early proponent of STV, praising it at length in his essay "Considerations on Representative Government", in which he writes: "Of all modes in which a national representation can possibly be constituted, this one affords the best security for the intellectual qualifications desirable in the representatives. At present... the only persons who can get elected are those who possess local influence, or make their way by lavish expenditure..." His contemporary, Walter Bagehot, also praised the Hare system for allowing everyone to elect an MP, even ideological minorities, but also argued that the Hare system would create more problems than it solved: "[the Hare system] is inconsistent with the extrinsic independence as well as the inherent moderation of a Parliament – two of the conditions we have seen, are essential to the bare possibility of parliamentary government."

Advocacy of STV spread throughout the British Empire, leading it to be sometimes known as "British Proportional Representation". In 1896, Andrew Inglis Clark was successful in persuading the Tasmanian House of Assembly to be the first parliament in the world elected by what became known as the "Hare-Clark electoral system", named after himself and Thomas Hare. H. G. Wells was a strong advocate, calling it "Proportional Representation".
The HG Wells formula for scientific voting, repeated, over many years, in his PR writings, to avoid misunderstanding, is Proportional Representation by the Single Transferable Vote in large constituencies.

STV in large constituencies permits an approach to the Hare-Mill-Wells ideal of mirror representation. The UK National Health Service used to elect, First Past The Post, all white male General Practitioners. In 1979, STV proportionally represented women, immigrants and specialists, to the General Medical Council.
In 1948, single transferable vote proportional representation on a state-by-state basis became the method for electing Senators to the Australian Senate. This change has led to the rise of a number of minor parties such as the Democratic Labor Party, Australian Democrats and Australian Greens who have taken advantage of this system to achieve parliamentary representation and the balance of power. From the 1984 election, group ticket voting was introduced in order to reduce a high rate of informal voting but in 2016, group tickets were abolished to avoid undue influence of preference deals amongst parties that were seen as distorting election results and a form of optional preferential voting was introduced.

Beginning in the 1970s, Australian states began to reform their upper houses to introduce proportional representation in line with the Federal Senate. The first was the South Australian Legislative Council in 1973, which initially used a party list system (replaced with STV in 1985), followed by the Single Transferable Vote being introduced for the New South Wales Legislative Council in 1978, the Western Australian Legislative Council in 1987 and the Victorian Legislative Council in 2003. The Single Transferable Vote was also introduced for the elections to the Australian Capital Territory Legislative Assembly after a 1992 referendum.

In the United States, the Proportional Representation League was founded in 1893 to promote STV, and their efforts resulted in its adoption by many city councils in the first half of the 20th century. More than twenty cities have used STV, including Cleveland, Cincinnati and New York City. As of January 2010, it is used to elect the city council and school committee in Cambridge, Massachusetts and the park board in Minneapolis, Minnesota. STV has also been adopted for student government elections at several American universities, including Carnegie Mellon, MIT, Oberlin, Reed, UC Berkeley, UC Davis, Vassar, UCLA, Whitman, and UT Austin. Legislation (HR 3057), was introduced in Congress in June 2017 that would establish STV for US House elections starting in 2022.

The degree of proportionality of STV election results depends directly on the district magnitude (i.e. the number of seats in each district). While Ireland originally had a median district magnitude of five (ranging from three to nine) in 1923, successive governments lowered this. Systematically lowering the number of representatives from a given district directly benefits larger parties at the expense of smaller ones.

Supposing that the Droop quota is used: in a nine-seat district, the quota or threshold is 10% (plus one vote); in a three-seat district, it would be 25% (plus one vote).

A parliamentary committee in 2010 discussed the "increasing trend towards the creation of three-seat constituencies in Ireland" and recommended not less than four-seaters, except where the geographic size of such a constituency would be disproportionately large.

STV provides proportionality by transferring votes to minimize waste, and therefore also minimizes the number of unrepresented or disenfranchised voters.

A frequent concern about STV is its complexity compared with plurality voting methods. Before the advent of computers, this complexity made ballot-counting more difficult than for some other voting methods.

The algorithm is complicated. In large elections with many candidates, a computer may be required. (This is because after several rounds of counting, there may be many different categories of previously transferred votes, each with a different permutation of early preferences and thus each with a different carried-forward weighting, all of which have to be kept track of.)

STV differs from other proportional representation systems in that candidates of one party can be elected on transfers from voters for other parties. Hence, STV may reduce the role of political parties in the electoral process and corresponding partisanship in the resulting government. A district only needs to have four members to be proportional for the major parties, but may under-represent smaller parties, even though they may well be more likely to be elected under STV than under first past the post. Also, while small parties seen as reasonable second preferences by others (such as the Green Party in Ireland) more easily get elected, parties seen as more extreme by others (such as Sinn Féin in Ireland) find it harder to attract second preferences and therefore find it harder to win seats.

As STV is a multi-member system, filling vacancies between elections can be problematic, and a variety of methods have been devised:

If there are not enough candidates to represent one of the priorities the electorate vote for (such as a party), all of them may be elected in the early stages, with votes being transferred to candidates with other views. On the other hand, putting up too many candidates might result in first preference votes being spread too thinly among them, and consequently several potential winners with broad second-preference appeal may be eliminated before others are elected and their second-preference votes distributed. In practice, the majority of voters express preference for candidates from the same party in order, which minimizes the impact of this potential effect of STV.

The outcome of voting under STV is proportional within a single election to the collective preference of voters, assuming voters have ranked their real preferences and vote along strict party lines (assuming parties and no individual independents participate in the election). However, due to other voting mechanisms usually used in conjunction with STV, such as a district or constituency system, an election using STV may not guarantee proportionality across all districts put together.

A number of methods of tactical or strategic voting exist that can be used in STV elections, but much less so than with First Past the Post. (In STV elections, most constituencies will be marginal, at least with regard to the allocation of a final seat.)

STV systems vary, both in ballot design and in whether or not voters are obliged to provide a full list of preferences. In jurisdictions such as Malta, Republic of Ireland and Northern Ireland, voters may rank as many or as few candidates as they wish. Consequently, voters sometimes, for example, rank only the candidates of a single party, or of their most preferred parties. A minority of voters, especially if they do not fully understand the system, may even "bullet vote", only expressing a first preference, or indicate a first preference for multiple candidates, especially when both STV and plurality are being used in concurrent elections. Allowing voters to rank only as many candidates as they wish grants them greater freedom, but can also lead to some voters ranking so few candidates that their vote eventually becomes "exhausted"–that is, at a certain point during the count, it can no longer be transferred and therefore loses an opportunity to influence the result.

The method can be confusing, and may cause some people to vote incorrectly with respect to their actual preferences. The ballots can also be long; having multiple pages also increases the chances of people missing the later opportunities to continue voting.

Some opponents argue that larger, multi-seat districts would require more campaign funds to reach the voters. Proponents argue that STV can lower campaign costs because like-minded candidates can share some expenses. Proponents reason that negative advertising is disincentivized in such a system, as its effect is diluted among a larger pool of candidates. In addition, unlike in at-large plurality elections, candidates do not have to secure the support of at least 50% of voters, allowing candidates to focus campaign spending primarily on supportive voters.

Academic analysis of voting systems such as STV generally centers on the voting system criteria that they pass. No preference voting system satisfies all the criteria in Arrow's impossibility theorem: in particular, STV fails to achieve independence of irrelevant alternatives (like most other vote-based ordering systems) and monotonicity.

The relative performance of political parties in STV systems is analysed in a different fashion from that used in other electoral schemes. For example, seeing which candidates are declared elected on first preference votes alone can be shown as follows:

The data can also be analysed to find the proportion of voters who express only a single preference, or those who express a minimum number of preferences, in order to assess party strength. Where parties nominate multiple candidates in an electoral district, analysis can also be done to assess their relative strength.

Other useful information can be found by analysing terminal transfersi.e., when the votes of a candidate are transferred and no other candidate from that party remains in the countespecially with respect to the first instance in which that occurs:

Another effect of STV is that candidates who did well on first preference votes may not be elected, and those who did poorly on first preferences can be elected, because of differences in second and later preferences. This can also be analysed:



</doc>
<doc id="29591" url="https://en.wikipedia.org/wiki?curid=29591" title="Stellarator">
Stellarator

A stellarator is a plasma device that relies primarily on external magnets to confine a plasma. In the future, scientists researching magnetic confinement fusion aim to use stellarator devices as a vessel for nuclear fusion reactions. The name refers to the possibility of harnessing the power source of the stars, including the sun. It is one of the earliest fusion power devices, along with the z-pinch and magnetic mirror.

The stellarator was invented by Lyman Spitzer of Princeton University in 1951, and much of its early development was carried out by his team at what became the Princeton Plasma Physics Laboratory (PPPL).

Lyman's Model A began operation in 1953 and demonstrated that stellarators could confine plasmas. Larger models followed, but these demonstrated poor performance, suffering from a problem known as pump-out that caused them to lose plasma at rates far worse than theoretical predictions. By the early 1960s, any hope of quickly producing a commercial machine faded, and attention turned to studying the fundamental theory of high-energy plasmas. By the mid-1960s, Spitzer was convinced that the stellarator was matching the Bohm diffusion rate, which suggested it would never be a practical fusion device.

The release of information on the USSR's tokamak design in 1968 indicated a leap in performance. This led to the Model C stellarator being converted to the Symmetrical Tokamak (ST) as a way to confirm or deny these results. ST confirmed them, and large-scale work on the stellarator concept ended as the tokamak got most of the attention. The tokamak ultimately proved to have similar problems to the stellarators, but for different reasons. Since the 1990s, this has led to renewed interest in the stellarator design. New methods of construction have increased the quality and power of the magnetic fields, improving performance. A number of new devices have been built to test these concepts. Major examples include Wendelstein 7-X in Germany, the Helically Symmetric Experiment (HSX) in the US, and the Large Helical Device in Japan.

In 1934, Mark Oliphant, Paul Harteck and Ernest Rutherford were the first to achieve fusion on Earth, using a particle accelerator to shoot deuterium nuclei into a metal foil containing deuterium, lithium or other elements. These experiments allowed them to measure the nuclear cross section of various reactions of fusion between nuclei, and determined that the tritium-deuterium reaction occurred at a lower energy than any other fuel, peaking at about 100,000 electronvolts (100 keV).

100 keV corresponds to a temperature of about a billion kelvins. Due to the Maxwell–Boltzmann statistics, a bulk gas at a much lower temperature will still contain some particles at these much higher energies. Because the fusion reactions release so much energy, even a small number of these reactions can release enough energy to keep the gas at the required temperature. In 1944, Enrico Fermi demonstrated that this would occur at a bulk temperature of about 50 million Celsius, still very hot but within the range of existing experimental systems. The key problem was "confining" such a plasma; no material container could withstand those temperatures. But because plasmas are electrically conductive, they are subject to electric and magnetic fields which provide a number of solutions.

In a magnetic field, the electrons and nuclei of the plasma circle the magnetic lines of force. One way to provide some confinement would be to place a tube of fuel inside the open core of a solenoid. A solenoid creates magnetic lines running down its center, and fuel would be held away from the walls by orbiting these lines of force. But such an arrangement does not confine the plasma along the length of the tube. The obvious solution is to bend the tube around into a torus (donut) shape, so that any one line forms a circle, and the particles can circle forever.

However, this solution does not actually work. For purely geometric reasons, the magnets ringing the torus are closer together on the inside curve, inside the "donut hole". Fermi noted this would cause the electrons to drift away from the nuclei, eventually causing them to separate and cause large voltages to develop. The resulting electric field would cause the plasma ring inside the torus to expand until it hit the walls of the reactor.

In the post-war era, a number of researchers began considering different ways to confine a plasma. George Paget Thomson of Imperial College London proposed a system now known as z-pinch, which runs a current through the plasma. Due to the Lorentz force, this current creates a magnetic field that pulls the plasma in on itself, keeping it away from the walls of the reactor. This eliminates the need for magnets on the outside, avoiding the problem Fermi noted. Various teams in the UK had built a number of small experimental devices using this technique by the late 1940s.

Another person working on controlled fusion reactors was Ronald Richter, a former German scientist who moved to Argentina after the war. His "thermotron" used a system of electrical arcs and mechanical compression (sound waves) for heating and confinement. He convinced Juan Perón to fund development of an experimental reactor on an isolated island near the Chilean border. Known as the Huemul Project, this was completed in 1951. Richter soon convinced himself fusion had been achieved in spite of other people working on the project disagreeing. The "success" was announced by Perón on 24 March 1951, becoming the topic of newspaper stories around the world.

While preparing for a ski trip to Aspen, Lyman Spitzer received a telephone call from his father, who mentioned an article on Huemul in "The New York Times". Looking over the description in the article, Spitzer concluded it could not possibly work; the system simply could not provide enough energy to heat the fuel to fusion temperatures. But the idea stuck with him, and he began considering systems that would work. While riding the ski lift, he hit upon the stellarator concept.

The basic concept was a way to modify the torus layout so that it addressed Fermi's concerns though the device's geometry. By twisting one end of the torus compared to the other, forming a figure-8 layout instead of a circle, the magnetic lines no longer travelled around the tube at a constant radius, instead they moved closer and further from the torus' center. A particle orbiting these lines would find itself constantly moving in and out across the minor axis of the torus. The drift upward while it travelled through one section of the reactor would be reversed after half an orbit and it would drift downward again. The cancellation was not perfect, but it appeared this would so greatly reduce the net drift rates that the fuel would remain trapped long enough to heat it to the required temperatures.

His 1958 description was simple and direct:
While working at Los Alamos in 1950, John Wheeler suggested setting up a secret research lab at Princeton University that would carry on theoretical work on H-bombs after he returned to the university in 1951. Spitzer was invited to join this program, given his previous research in interstellar plasmas.

But by the time of his trip to Aspen, Spitzer had lost interest in bomb design and he turned his attention full-time to fusion as a power source. Over the next few months, Spitzer produced a series of reports outlining the conceptual basis for the stellarator, as well as potential problems. The series is notable for its depth; it not only included a detailed analysis of the mathematics of the plasma and stability but also outlined a number of additional problems like heating the plasma and dealing with impurities.

With this work in hand, Spitzer began to lobby the Department of Energy (DOE) for funding to develop the system. He outlined a plan involving three stages. The first would see the construction of a Model A, whose purpose was to demonstrate that a plasma could be created and that its confinement time was better than a torus. If the A model was successful, the B model would attempt to heat the plasma to fusion temperatures. This would be followed by a C model, which would attempt to actually create fusion reactions at a large scale. This entire series was expected to take about a decade.

Around the same time, Jim Tuck had been introduced to the pinch concept while working at Clarendon Laboratory at Oxford University. He was offered a job in the US and eventually ended up at Los Alamos, where he acquainted the other researchers with the concept. When he heard Spitzer was promoting the stellarator, he also travelled to Washington to propose building a pinch device. He considered Spitzer's plans "incredibly ambitious." Nevertheless, Spitzer was successful in gaining $50,000 in funding from the DOE, while Tuck received nothing.

The Princeton program was officially created on 1 July 1951. Spitzer, an avid mountain climber, proposed the name "Project Matterhorn" because he felt "the work at hand seemed difficult, like the ascent of a mountain." Two sections were initially set up, S Section working on the stellarator under Spitzer, and B Section working on bomb design under Wheeler. Matterhorn was set up at Princeton's new Forrestal Campus, a plot of land the University purchased from the Rockefeller Institute for Medical Research when Rockefeller relocated to Manhattan. The land was located about from the main Princeton campus and already had sixteen laboratory buildings. Spitzer set up the top-secret S Section in a former rabbit hutch.

It was not long before the other labs began agitating for their own funding. Tuck had managed to arrange some funding for his Perhapsatron through some discretionary budgets at LANL, but other teams at LANL, Berkeley and Oak Ridge (ORNL) also presented their ideas. The DOE eventually organized a new department for all of these projects, becoming "Project Sherwood".

With the funding from the DOE, Spitzer began work by inviting James Van Allen to join the group and set up an experimental program. Allen suggested starting with a small "tabletop" device. This led to the Model A design, which began construction in 1952. It was made from pyrex tubes about in total length, and magnets capable of about 1,000 gauss. The machine began operations in early 1953 and clearly demonstrated improved confinement over the simple torus.

This led to the construction of the Model B, which had the problem that the magnets were not well mounted and tended to move around when they were powered to their maximum capacity of 50,000 gauss. A second design also failed for the same reason, but this machine demonstrated several-hundred-kilovolt X-rays that suggested good confinement. The lessons from these two designs led to the B-1, which used ohmic heating (see below) to reach plasma temperatures around 100,000 degrees. This machine demonstrated that impurities in the plasma caused large x-ray emissions that rapidly cooled the plasma. In 1956, B-1 was rebuilt with an ultra-high vacuum system to reduce the impurities but found that even at smaller quantities they were still a serious problem. Another effect noticed in the B-1 was that during the heating process, the particles would remain confined for only a few tenths of a millisecond, while once the field was turned off, any remaining particles were confined for as long as 10 milliseconds. This appeared to be due to "cooperative effects" within the plasma.

Meanwhile, a second machine known as B-2 was being built. This was similar to the B-1 machine but used pulsed power to allow it to reach higher magnetic energy and included a second heating system known as magnetic pumping. This machine was also modified to add an ultra-high vacuum system. Unfortunately, B-2 demonstrated little heating from the magnetic pumping, which was not entirely unexpected because this mechanism required longer confinement times, and this was not being achieved. As it appeared that little could be learned from this system in its current form, in 1958 it was sent to the Atoms for Peace show in Geneva. However, when the heating system was modified, the coupling increased dramatically, demonstrating temperatures within the heating section as high as .

Two additional machines were built to study pulsed operation. B-64 was completed in 1955, essentially a larger version of the B-1 machine but powered by pulses of current that produced up to 15,000 gauss. This machine included a "diverter", which removed impurities from the plasma, greatly reducing the x-ray cooling effect seen on earlier machines. B-64 included straight sections in the curved ends which gave it a squared-off appearance. This appearance led to its name, it was a "figure-8, squared", or 8 squared, or 64. This led to experiments in 1956 where the machine was re-assembled without the twist in the tubes, allowing the particles to travel without rotation.

B-65, completed in 1957, was built using the new "racetrack" layout. This was the result of the observation that adding helical coils to the curved portions of the device produced a field that introduced the rotation purely through the resulting magnetic fields. This had the added advantage that the magnetic field included "shear", which was known to improve stability. B-3, also completed in 1957, was a greatly enlarged B-2 machine with ultra-high vacuum and pulsed confinement up to 50,000 gauss and projected confinement times as long as 0.01 second. The last of the B-series machines was the B-66, completed in 1958, which was essentially a combination of the racetrack layout from B-65 with the larger size and energy of the B-3.

Unfortunately, all of these larger machines demonstrated a problem that came to be known as "pump out". This effect was causing plasma drift rates that were not only higher than classical theory suggested but also much higher than the Bohm rates. B-3's drift rate was a full three times that of the worst-case Bohm predictions, and failed to maintain confinement for more than a few tens of microseconds.

As early as 1954, as research continued on the B-series machines, the design of the Model C device was becoming more defined. It emerged as a large racetrack-layout machine with multiple heating sources and a diverter, essentially an even larger B-66. Construction began in 1958 and was completed in 1961. It could be adjusted to allow a plasma minor axis between and was in length. The toroidal field coils normally operated at 35,000 gauss.

By the time Model C began operations, information collected from previous machines was making it clear that it would not be able to produce large-scale fusion. Ion transport across the magnetic field lines was much higher than classical theory suggested. Greatly increased magnetic fields of the later machines did little to address this, and confinement times simply were not improving. Attention began to turn to a much greater emphasis on the theoretical understanding of the plasma. In 1961, Melvin B. Gottlieb took over the Matterhorn Project from Spitzer, and on 1 February the project was renamed as the Princeton Plasma Physics Laboratory (PPPL).

Continual modification and experimentation on the Model C slowly improved its operation, and the confinement times eventually increased to match that of Bohm predictions. New versions of the heating systems were used that slowly increased the temperatures. Notable among these was the 1964 addition of a small particle accelerator to accelerate fuel ions to high enough energy to cross the magnetic fields, depositing energy within the reactor when they collided with other ions already inside. This method of heating, now known as neutral beam injection, has since become almost universal on magnetic confinement fusion machines.

Model C spent most of its history involved in studies of ion transport. Through continual tuning of the magnetic system and the addition of the new heating methods, in 1969, Model C eventually reached electron temperatures of 400 eV. Through this period, a number of new potential stellarator designs emerged, using a single set of magnetic coils. The Model C used separate confinement and helical coils, but it was seen that these could be combined, and this led to the "torsitron" concept.

In 1968, scientists in the Soviet Union released the results of their tokamak machines, notably their newest example, T-3. The results were so startling that there was widespread scepticism. To address this, the Soviets invited a team of experts from the United Kingdom to test the machines for themselves. Their tests, made using a laser-based system developed for the ZETA reactor in England, verified the Soviet claims of electron temperatures of 1,000 eV. What followed was a "veritable stampede" of tokamak construction worldwide.

At first the US labs ignored the tokamak; Spitzer himself dismissed it out of hand as experimental error. However, as new results came in, especially the UK reports, Princeton found itself in the position of trying to defend the stellarator as a useful experimental machine while other groups from around the US were clamoring for funds to build tokamaks. In July 1969 Gottlieb had a change of heart, offering to convert the Model C to a tokamak layout. In December it was shut down and reopened in May as the Symmetric Tokamak (ST).

The ST immediately matched the performance being seen in the Soviet machines, besting the Model C's results by over ten times. From that point, PPPL was the primary developer of the tokamak approach in the US, introducing a series of machines to test various designs and modifications. The Princeton Large Torus of 1975 quickly hit several performance numbers that were required for a commercial machine, and it was widely believed the critical threshold of breakeven would be reached in the early 1980s. What was needed was larger machines and more powerful systems to heat the plasma to fusion temperatures.

Tokamaks are a type of pinch machine, differing from earlier designs primarily in the amount of current in the plasma: above a certain threshold known as the "safety factor", or "q", the plasma is much more stable. ZETA ran at a "q" around , while experiments on tokamaks demonstrated it needs to be at least 1. Machines following this rule showed dramatically improved performance. However, by the mid-1980s the easy path to fusion disappeared; as the amount of current in the new machines began to increase, a new set of instabilities in the plasma appeared. These could be addressed, but only by greatly increasing the power of the magnetic fields, requiring superconducting magnets and huge confinement volumes. The cost of such a machine was such that the involved parties banded together to begin the ITER project.

As the problems with the tokamak approach grew, interest in the stellarator approach reemerged. This coincided with the development of advanced computer aided planning tools that allowed the construction of complex magnets that were previously known but considered too difficult to design and build.

New materials and construction methods have increased the quality and power of the magnetic fields, improving performance. New devices have been built to test these concepts. Major examples include Wendelstein 7-X in Germany, the Helically Symmetric Experiment (HSX) in the US, and the Large Helical Device in Japan. W7X and LHD use superconducting magnetic coils.
The lack of an internal current eliminates some of the instabilities of the tokamak, meaning the stellarator should be more stable at similar operating conditions. On the downside, since no confinement is provided by the current found in a tokamak, the stellarator requires more powerful magnets to reach any given confinement. The stellarator is an inherently steady-state machine, which has several advantages from an engineering standpoint.

In 2019 a Hessian maxtrix was applied to simplify the math required to assess the error fields associated with important coil imperfections. Magnetic island size and quasi-symmetry are analytically differentiated over coil parameters. The eigenvectors of the Hessian matrix identify sensitive coil deviations. Each coil requires separate tolerance and some certain perturbation combinations, permitting greater coil tolerances, which could reduce time and cost.

Heating a gas increases the energy of the particles within it, so by heating a gas into the hundreds of millions of degrees, the majority of the particles within it would reach the energy required to fuse. According to the Maxwell–Boltzmann distribution, some of the particles will reach the required energies at much lower average temperatures. Because the energy released by the reaction is much greater than what it takes to start it, even a small number of reactions can heat surrounding fuel until it fuses as well. In 1944, Enrico Fermi calculated the D-T reaction would be self-sustaining at about .

Materials heated beyond a few tens of thousand degrees ionize into their electrons and nuclei, producing a gas-like state of matter known as plasma. According to the ideal gas law, like any hot gas, plasma has an internal pressure and thus wants to expand. For a fusion reactor, the challenge is to keep the plasma contained; any known substance would melt or sublime at these temperatures. But because a plasma is electrically conductive, it is subject to electric and magnetic fields. In a magnetic field, the electrons and nuclei orbit around the magnetic field lines, confining them to the area defined by the field.

A simple confinement system can be made by placing a tube inside the open core of a solenoid. The tube can be evacuated and then filled with the requisite gas and heated until it becomes a plasma. The plasma naturally wants to expand outwards to the walls of the tube, as well as move along it, towards the ends. The solenoid creates magnetic field lines running down the center of the tube, and the plasma particles orbit these lines, preventing their motion towards the sides. Unfortunately, this arrangement would not confine the plasma along the "length" of the tube, and the plasma would be free to flow out the ends.

The obvious solution to this problem is to bend the tube around into a torus (a ring or donut) shape. Motion towards the sides remains constrained as before, and while the particles remain free to move along the lines, in this case, they will simply circulate around the long axis of the tube. But, as Fermi pointed out, when the solenoid is bent into a ring, the electrical windings would be closer together on the inside than the outside. This would lead to an uneven field across the tube, and the fuel will slowly drift out of the center. Since the electrons and ions would drift in opposite directions, this would lead to a charge separation and electrostatic forces that would eventually overwhelm the magnetic force. Some additional force needs to counteract this drift, providing long-term "confinement".

Spitzer's key concept in the stellarator design is that the drift that Fermi noted could be canceled out through the physical arrangement of the vacuum tube. In a simple torus, particles on the inside edge of the tube, where the field was stronger, would drift up, while those on the outside would drift down (or vice versa). However, if the particle were made to alternate between the inside and outside of the tube, the drifts would cancel out. The cancellation is not perfect, leaving some net drift, but basic calculations suggested drift would be lowered enough to confine plasma long enough to heat it sufficiently.

Spitzer's suggestion for doing this was simple. Instead of a normal torus, the device would essentially be cut in half to produce two half-tori. They would then be joined with two straight sections between the open ends. The key was that they were connected to alternate ends so that the right half of one of the tori was connected to the left of the other. The resulting design resembled a figure-8 when viewed from above. Because the straight tubes could not pass through each other, the design did not lay flat, the tori at either end had to be tilted. This meant the drift cancellation was further reduced, but again, calculations suggested the system would work.

To understand how the system works to counteract drift, consider the path of a single particle in the system starting in one of the straight sections. If that particle is perfectly centered in the tube, it will travel down the center into one of the half-tori, exit into the center of the next tube, and so on. This particle will complete a loop around the entire reactor without leaving the center. Now consider another particle traveling parallel to the first, but initially located near the inside wall of the tube. In this case, it will enter the "outside" edge of the half-torus and begin to drift down. It exits that section and enters the second straight section, still on the outside edge of that tube. However, because the tubes are crossed, when it reaches the second half-torus it enters it on the "inside" edge. As it travels through this section it drifts back up.

This effect would reduce one of the primary causes of drift in the machine, but there were others to consider as well. Although the ions and electrons in the plasma would both circle the magnetic lines, they would do so in opposite directions, and at very high rotational speeds. This leads to the possibility of collisions between particles circling different lines of force as they circulate through the reactor, which due to purely geometric reasons, causes the fuel to slowly drift outward. This process eventually causes the fuel to either collide with the structure or cause a large charge separation between the ions and electrons. Spitzer introduced the concept of a "divertor", a magnet placed around the tube that pulled off the very outer layer of the plasma. This would remove the ions before they drifted too far and hit the walls. It would also remove any heavier elements in the plasma.

Using "classical" calculations the rate of diffusion through collisions was low enough that it would be much lower than the drift due to uneven fields in a normal toroid. But studies in 1949 demonstrated much higher losses and became known as Bohm diffusion. Spitzer spent considerable effort considering this issue, and concluded that the anomalous rate being seen by Bohm was due to instability in the plasma, which he believed could be addressed.

Practical complications make the original figure-8 device less than ideal. This led to alternative designs and additions.

One of the major concerns is that the magnetic fields in the system will only properly confine a particle of a given mass traveling at a given speed. Particles traveling faster or slower will not circulate in the desired fashion. Particles with very low speeds (corresponding to low temperatures) are not confined and can drift out to the tube walls. Those with too much energy may hit the outside walls of the curved sections. To address these concerns, Spitzer introduced the concept of a "divertor" that would connect to one of the straight sections. This was essentially a mass spectrometer that would remove particles that were moving too fast or too slow for proper confinement.

The physical limitation that the two straight sections cannot intersect means that the rotational transform within the loop is not a perfect 180 degrees, but typically closer to 135 degrees. This led to alternate designs in an effort to get the angle closer to 180. An early attempt was built into the Stellarator B-2, which placed both curved sections flat in relation to the ground, but at different heights. The formerly straight sections had additional curves inserted, two sections of about 45 degrees, so they now formed extended S-shapes. This allowed them to route around each other while being perfectly symmetrical in terms of angles.

A better solution to the need to rotate the particles was introduced in the Stellarator B-64 and B-65. These eliminated the cross-over and flattened the device into an oval, or as they referred to it, a racetrack. The rotation of the particles was introduced by placing a new set of magnetic coils on the half-torus on either end, the "corkscrew windings". The field from these coils mixes with the original confinement fields to produce a mixed field that rotates the lines of force through 180 degrees. This made the mechanical design of the reactor much simpler, but in practice, it was found that the mixed field was very difficult to produce in a perfectly symmetrical fashion.

Unlike the z-pinch designs being explored in the UK and other US labs, the stellarator has no induced electrical current within the plasma – at a macroscopic level, the plasma is neutral and unmoving, in spite of the individual particles within it rapidly circulating. In pinch machines, and the later tokamaks, the current itself is one of the primary methods of heating the plasma. In the stellarator, no such natural heating source is present.

Early stellarator designs used a system similar to those in the pinch devices to provide the initial heating to bring the gas to plasma temperatures. This consisted of a single set of windings from a transformer, with the plasma itself forming the secondary set. When energized with a pulse of current, the particles in the region are rapidly energized and begin to move. This brings additional gas into the region, quickly ionizing the entire mass of gas. This concept was referred to as "ohmic heating" because it relied on the resistance of the gas to create heat, in a fashion not unlike a conventional resistance heater. As the temperature of the gas increases, the conductivity of the plasma improves. This makes the ohmic heating process less and less effective, and this system is limited to temperatures of about 1 million kelvins.

To heat the plasma to higher temperatures, a second heat source was added, the "magnetic pumping" system. This consisted of radio-frequency source fed through a coil spread along the vacuum chamber. The frequency is chosen to be similar to the natural frequency of the particles around the magnetic lines of force, the "cyclotron frequency". This causes the particles in the area to gain energy, which causes them to orbit in a wider radius. Since other particles are orbiting their own lines nearby, at a macroscopic level, this change in energy appears as an increase in pressure. According to the ideal gas law, this results in an increase in temperature. Like the ohmic heating, this process also becomes less efficient as the temperature increases, but is still capable of creating very high temperatures. When the frequency is deliberately set close to that of the ion circulation, this is known as "ion-cyclotron resonance heating", although this name is not widely used.

There are several ways to heat the plasma (which must be done before ignition can occur).



Several different configurations of stellarator exist, including:


The goal of magnetic confinement devices is to minimise energy transport across a magnetic field. Toroidal devices are relatively successful because the magnetic properties seen by the particles are averaged as they travel around the torus. The strength of the field seen by a particle, however, generally varies, so that some particles will be trapped by the mirror effect. These particles will not be able to average the magnetic properties so effectively, which will result in increased energy transport. In most stellarators, these changes in field strength are greater than in tokamaks, which is a major reason that transport in stellarators tends to be higher than in tokamaks.

University of Wisconsin electrical engineering Professor David Anderson and research assistant John Canik proved in 2007 that the Helically Symmetric eXperiment (HSX) can overcome this major barrier in plasma research. The HSX is the first stellarator to use a quasisymmetric magnetic field. The team designed and built the HSX with the prediction that quasisymmetry would reduce energy transport. As the team's latest research showed, that is exactly what it does. "This is the first demonstration that quasisymmetry works, and you can actually measure the reduction in transport that you get," says Canik.

The newer Wendelstein 7-X in Germany was designed to be close to omnigeneity (a property of the magnetic field such that the mean radial drift is zero), which is a necessary but not sufficient condition for quasisymmetry; that is, all quasisymmetric magnetic fields are omnigenous, but not all omnigenous magnetic fields are quasisymmetric.





</doc>
<doc id="29592" url="https://en.wikipedia.org/wiki?curid=29592" title="SLA">
SLA

SLA may refer to:






</doc>
<doc id="29593" url="https://en.wikipedia.org/wiki?curid=29593" title="SYSTRAN">
SYSTRAN

SYSTRAN, founded by Dr. Peter Toma in 1968, is one of the oldest machine translation companies. SYSTRAN has done extensive work for the United States Department of Defense and the European Commission. 

SYSTRAN provides the technology for Yahoo! Babel Fish among others. It was used by Google's language tools until 2007. SYSTRAN is used by the Dashboard Translation widget in OS X.

Commercial versions of SYSTRAN can run on Microsoft Windows (including Windows Mobile), Linux, and Solaris. Historically, SYSTRAN systems used Rule-based machine translation (RbMT) technology. With the release of SYSTRAN Server 7 in 2010, SYSTRAN implemented a hybrid rule-based/Statistical machine translation (SMT) technology which was the first of its kind in the marketplace.

, the company had 59 employees of whom 26 are computational experts and 15 computational linguists. The number of employees decreased from 70 in 2006 to 59 in 2008.

With its origin in the Georgetown machine translation effort, SYSTRAN was one of the few machine translation systems to survive the major decrease of funding after the ALPAC Report of the mid-1960s. The company was established in La Jolla in California to work on translation of Russian to English text for the United States Air Force during the Cold War. Large numbers of Russian scientific and technical documents were translated using SYSTRAN under the auspices of the USAF Foreign Technology Division (later the National Air and Space Intelligence Center) at Wright-Patterson Air Force Base, Ohio. The quality of the translations, although only approximate, was usually adequate for understanding content.

The company was sold in 1986 to the Gachot family, based in Paris, and is now traded publicly on the French stock exchange. Its company headquarters is in Paris, while its U.S. headquarters is still in La Jolla.

During the dot-com boom, the international language industry started a new era, and SYSTRAN entered into agreements with a number of translation integrators, the most successful of these being WorldLingo.

In 2014, the company was acquired by CSLi (Korea).

Most of SYSTRAN's revenue comes from a few customers. 57.1% comes from the 10 main customers and the three largest customers account for 10.9%, 8.9%, and 8.9% of its revenues, respectively. Revenues had been declining in the early 2000s: 10.2 million euros in 2004, 10.1 million euros in 2005, 9.3 million euros in 2006, 8.8 million euros in 2007, and 7.6 million euros in 2008, before seeing a rebound in 2009 with 8.6 million euros.

The following is a list of the source and target languages with which SYSTRAN works. Many of the pairs are to or from English or French.



</doc>
<doc id="29594" url="https://en.wikipedia.org/wiki?curid=29594" title="Stephen I of Hungary">
Stephen I of Hungary

Stephen I, also known as King Saint Stephen ( ; ; ; 975 – 15 August 1038 AD), was the last Grand Prince of the Hungarians between 997 and 1000 or 1001, and the first King of Hungary from 1000 or 1001 until his death in 1038. The year of his birth is uncertain, but many details of his life suggest that he was born in or after 975 in Esztergom. At his birth, he was given the pagan name Vajk. The date of his baptism is unknown. He was the only son of Grand Prince Géza and his wife, Sarolt, who was descended from the prominent family of the "gyulas". Although both of his parents were baptized, Stephen was the first member of his family to become a devout Christian. He married Gisela of Bavaria, a scion of the imperial Ottonian dynasty.

After succeeding his father in 997, Stephen had to fight for the throne against his relative, Koppány, who was supported by large numbers of pagan warriors. He defeated Koppány mainly with the assistance of foreign knights, including Vecelin, Hont and Pázmány, but also with help from native lords. He was crowned on 25 December 1000 or 1 January 1001 with a crown sent by Pope Sylvester II. In a series of wars against semi-independent tribes and chieftainsincluding the Black Hungarians and his uncle, Gyula the Youngerhe unified the Carpathian Basin. He protected the independence of his kingdom by forcing the invading troops of Conrad II, Holy Roman Emperor, to withdraw from Hungary in 1030.

Stephen established at least one archbishopric, six bishoprics and three Benedictine monasteries; thus the Church in Hungary developed independently of the archbishops of the Holy Roman Empire. He encouraged the spread of Christianity with severe punishments for ignoring Christian customs. His system of local administration was based on counties organized around fortresses and administered by royal officials. Hungary, which enjoyed a lasting period of peace during his reign, became a preferred route for pilgrims and merchants traveling between Western Europe and the Holy Land or Constantinople.

He survived all of his children. He died on 15 August 1038 and was buried in his new basilica, built in Székesfehérvár and dedicated to the Holy Virgin. His death caused civil wars which lasted for decades. He was canonized by Pope Gregory VII, together with his son, Emeric, and Bishop Gerard of Csanád, in 1083. Stephen is a popular saint in Hungary and the neighboring territories. In Hungary, his feast day (celebrated on 20 August) is also a public holiday commemorating the foundation of the state, known as State Foundation Day.

Stephen's birth date is uncertain as it was not recorded in contemporaneous documents. Hungarian and Polish chronicles written centuries later give three different years: 967, 969 and 975. The unanimous testimony of his three late 11th-century or early 12th-century hagiographies and other Hungarian sources, which state that Stephen was "still an adolescent" in 997, substantiate the reliability of the later year (975). Stephen's "Lesser Legend" adds that he was born in Esztergom, which implies that he was born after 972 because his father, Géza, Grand Prince of the Hungarians, chose Esztergom as royal residence around that year. Géza promoted the spread of Christianity among his subjects by force, but never ceased worshipping pagan gods. Both his son's "Greater Legend" and the nearly contemporaneous Thietmar of Merseburg described Géza as a cruel monarch, suggesting that he was a despot who mercilessly consolidated his authority over the rebellious Hungarian lords.

Hungarian chronicles agree that Stephen's mother was Sarolt, daughter of Gyula, a Hungarian chieftain with jurisdiction either in Transylvania or in the wider region of the confluence of the rivers Tisza and Maros. Many historiansincluding Pál Engel and Gyula Kristópropose that her father was identical with "Gylas", who had been baptized in Constantinople around 952 and "remained faithful to Christianity", according to Byzantine chronicler John Skylitzes. However, this identification is not unanimously accepted; historian György Györffy states that it was not Sarolt's father, but his younger brother, who was baptized in the Byzantine capital. In contrast with all Hungarian sources, the "Polish-Hungarian Chronicle" and later Polish sources state that Stephen's mother was Adelhaid, an otherwise unknown sister of Duke Mieszko I of Poland, but the reliability of this report is not accepted by modern historians.

Stephen was born as Vajk, a name derived from the Turkic word "baj", meaning "hero", "master", "prince" or "rich". Stephen's "Greater Legend" narrates that he was baptized by the saintly Bishop Adalbert of Prague, who stayed in Géza's court several times between 983 and 994. However, Saint Adalbert's nearly contemporaneous "Legend", written by Bruno of Querfurt, does not mention this event. Accordingly, the date of Stephen's baptism is unknown: Györffy argues that he was baptized soon after birth, while Kristó proposes that he only received baptism just before his father's death in 997.

Stephen's official hagiography, written by Bishop Hartvic and sanctioned by Pope Innocent III, narrates that he "was fully instructed in the knowledge of the grammatical art" in his childhood. This implies that he studied Latin, though some scepticism is warranted as few kings of this era were able to write. His two other late 11th-century hagiographies do not mention any grammatical studies, stating only that he "was brought up by receiving an education appropriate for a little prince". Kristó says that the latter remark only refers to Stephen's physical training, including his participation in hunts and military actions. According to the "Illuminated Chronicle", one of his tutors was a Count Deodatus from Italy, who later founded a monastery in Tata.

According to Stephen's legends, Grand Prince Géza convoked an assembly of the Hungarian chieftains and warriors when Stephen "ascended to the first stage of adolescence", at the age of 14 or 15. Géza nominated Stephen as his successor and all those present took an oath of loyalty to the young prince. Györffy also writes, without identifying his source, that Géza appointed his son to rule the "Nyitra ducate" around that time. Slovak historians, including Ján Steinhübel and Ján Lukačka, accept Györffy's view and propose that Stephen administered Nyitra (now Nitra, Slovakia) from around 995.

Géza arranged Stephen's marriage, to Gisela, daughter of Henry II, Duke of Bavaria, in or after 995. This marriage established the first family link between a Hungarian ruler and a Western European ruling house, as Gisela was closely related to the Ottonian dynasty of Holy Roman Emperors. According to popular tradition preserved in the Scheyern Abbey in Bavaria, the ceremony took place at the Scheyern castle and was celebrated by Saint Adalbert. Gisela was accompanied to her new home by Bavarian knights, many of whom received land grants from her husband and settled in Hungary, helping to strengthen Stephen's military position. Györffy writes that Stephen and his wife "presumably" settled in Nyitra after their marriage.

Grand Prince Géza died in 997. Stephen convoked an assembly at Esztergom where his supporters declared him grand prince. Initially, he only controlled the northwestern regions of the Carpathian Basin; the rest of the territory was still dominated by tribal chieftains. Stephen's ascension to the throne was in line with the principle of primogeniture, which prescribed that a father was succeeded by his son. On the other hand, it contradicted the traditional idea of seniority, according to which Géza should have been succeeded by the most senior member of the Árpád dynasty, which was Koppány at that time. Koppány, who held the title Duke of Somogy, had for many years administered the regions of Transdanubia south of Lake Balaton.

Koppány proposed to Géza's widow, Sarolt, in accordance with the pagan custom of levirate marriage. He also announced his claim to the throne. Although it is not impossible that Koppány had already been baptized, in 972, most of his supporters were pagans, opponents of the Christianity represented by Stephen and his predominantly German retinue. A charter of 1002 for the Pannonhalma Archabbey writes of a war between "the Germans and the Hungarians" when referring to the armed conflicts between Stephen and Koppány. Even so, Györffy says that "Oszlar" ("Alan"), "Besenyő" ("Pecheneg"), "Kér" and other place names, referring to ethnic groups or Hungarian tribes in Transdanubia around the supposed borders of Koppány's duchy, suggest that significant auxiliary units and groups of Hungarian warriorswho had been settled there by Grand Prince Gézafought in Stephen's army.

Kristó states that the entire conflict between Stephen and Koppány was only a feud between two members of the Árpád dynasty, with no effect on other Hungarian tribal leaders. Koppány and his troops invaded the northern regions of Transdanubia, took many of Stephen's forts and plundered his lands. Stephen, who according to the "Illuminated Chronicle" "was for the first time girded with his sword", placed the brothers Hont and Pázmány at the head of his own guard and nominated Vecelin to lead the royal army. The latter was a German knight who had come to Hungary in the reign of Géza. Hont and Pázmány were, according to Simon of Kéza's "Gesta Hunnorum et Hungarorum" and the "Illuminated Chronicle", "knights of Swabian origin" who settled in Hungary either under Géza or in the first years of Stephen's reign. On the other hand, Lukačka and other Slovak historians say that Hont and Pázmány were "Slovak" noblemen who had joined Stephen during his rule in Nyitra.

Koppány was besieging Veszprém when he was informed of the arrival of Stephen's army. In the ensuing battle, Stephen won a decisive victory over his enemies. Koppány was killed on the battlefield. His body was quartered and its parts were displayed at the gates of the forts of Esztergom, Győr, Gyulafehérvár (Alba Iulia, Romania) and Veszprém in order to threaten all of those who were conspiring against the young monarch.

Stephen occupied Koppány's duchy and granted large estates to his own partisans. He also prescribed that Koppány's former subjects were to pay tithes to the Pannonhalma Archabbey, according to the deed of the foundation of this monastery which has been preserved in a manuscript containing interpolations. The same document declares that "there were no other bishoprics and monasteries in Hungary" at that time. On the other hand, the nearly contemporary Bishop Thietmar of Merseburg stated that Stephen "established bishoprics in his kingdom" before being crowned king. If the latter report is valid, the dioceses of Veszprém and Győr are the most probable candidates, according to historian Gábor Thoroczkay.

By ordering the display of one part of Koppány's quartered corpse in Gyulafehérvár, the seat of his maternal uncle, Gyula the Younger, Stephen asserted his claim to reign all lands dominated by Hungarian lords. He also decided to strengthen his international status by adopting the title of king. However, the exact circumstances of his coronation and its political consequences are subject to scholarly debate.

Thietmar of Merseburg writes that Stephen received the crown "with the favour and urging" of Emperor Otto III (r. 996–1002), implying that Stephen accepted the Emperor's suzerainty before his coronation. On the other hand, all of Stephen's legends emphasize that he received his crown from Pope Sylvester II (r. 999–1003). Kristó and other historians point out that Pope Sylvester and Emperor Otto were close allies, which implies that both reports are valid: Stephen "received the crown and consecration" from the Pope, but not without the Emperor's consent. Around 75 years after the coronation, Pope Gregory VII (r. 1075–1085), who claimed suzerainty over Hungary, declared that Stephen had "offered and devotedly surrendered" Hungary "to Saint Peter" (that is to the Holy See). In a contrasting report, Stephen's "Greater Legend" states that the King offered Hungary to the Virgin Mary. Modern historiansincluding Pál Engel, and Miklós Molnárwrite that Stephen always asserted his sovereignty and never accepted papal or imperial suzerainty. For instance, none of his charters were dated according to the years of the reign of the contemporary emperors, which would have been the case if he had been their vassal. Furthermore, Stephen declared in the preamble to his "First Book of Laws" that he governed his realm "by the will of God".

The exact date of Stephen's coronation is unknown. According to later Hungarian tradition, he was crowned on the first day of the second millennium, which may refer either to 25 December 1000 or to 1 January 1001. Details of Stephen's coronation preserved in his "Greater Legend" suggest that the ceremony, which took place in Esztergom or Székesfehérvár followed the rite of the coronation of the German kings. Accordingly, Stephen was anointed with consecrated oil during the ceremony. Stephen's portrait, preserved on his royal cloak from 1031, shows that his crown, like the Holy Roman Emperor's diadem, was a hoop crown decorated with gemstones.

Besides his crown, Stephen regarded a spear with a flag as an important symbol of his sovereignty. For instance, his first coins bear the inscription LANCEA REGIS ("the king's spear") and depict an arm holding a spear with flag. According to the contemporaneous Adémar de Chabannes, a spear had been given to Stephen's father by Emperor Otto III as a token of Géza's right to "enjoy the most freedom in the possession of his country". Stephen is styled in various ways"Ungarorum rex" ("king of the Hungarians"), "Pannoniorum rex" ("king of the Pannonians") or "Hungarie rex" ("king of Hungary")in his charters.

Although Stephen's power did not rely on his coronation, the ceremony granted him the internationally accepted legitimacy of a Christian monarch who ruled his realm "by the Grace of God". All his legends testify that he established an archbishopric with its see in Esztergom shortly after his coronation. This act ensured that the Church in Hungary became independent of the prelates of the Holy Roman Empire. The earliest reference to an archbishop of Esztergom, named Domokos, has been preserved in the deed of foundation of the Pannonhalma Archabbey from 1002. According to historian Gábor Thoroczkay, Stephen also established the Diocese of Kalocsa in 1001. Stephen invited foreign priests to Hungary to evangelize his kingdom. Associates of the late Adalbert of Prague, including Radla and Astrik, arrived in Hungary in the first years of his reign. The presence of an unnamed "Archbishop of the Hungarians" at the synod of 1007 of Frankfurt and the consecration of an altar in Bamberg in 1012 by Archbishop Astrik show that Stephen's prelates maintained a good relationship with the clergy of the Holy Roman Empire.

The transformation of Hungary into a Christian state was one of Stephen's principal concerns throughout his reign. Although the Hungarians' conversion had already begun in his father's reign, it was only Stephen who systematically forced his subjects to give up their pagan rituals. His legislative activity was closely connected with Christianity. For example, his "First Book of Laws" from the first years of his reign includes several provisions prescribing the observance of feast days and the confession before death. His other laws protected property rights and the interests of widows and orphans, or regulated the status of serfs.

Many Hungarian lords refused to accept Stephen's suzerainty even after his coronation. The new King first turned against his own uncle, Gyula the Younger, whose realm "was most wide and rich", according to the "Illuminated Chronicle". Stephen invaded Transylvania and seized Gyula and his family around 1002 or in 1003. The contemporary "Annals of Hildesheim" adds that Stephen converted his uncle's "country to the Christian faith by force" after its conquest. Accordingly, historians date the establishment of the Diocese of Transylvania to this period. If the identification, proposed by Kristó, Györffy and other Hungarian historians, of Gyula with one Prokuiwho was Stephen's uncle according to Thietmar of Merseburgis valid, Gyula later escaped from captivity and fled to Bolesław I the Brave, Duke of Poland (r. 992–1025).

About a hundred years later, the chronicler Gallus Anonymus also made mention of armed conflicts between Stephen and Boleslav, stating that the latter "defeated the Hungarians in battle and made himself master of all their lands as far as the Danube". Györffy says that the chronicler's report refers to the occupation of the valley of the river Moravaa tributary of the Danubeby the Poles in the 1010s. On the other hand, the "Polish-Hungarian Chronicle" states that the Polish duke occupied large territories north of the Danube and east of the Morava as far as Esztergom in the early 11th century. According to Steinhübel, the latter source proves that a significant part of the lands that now form Slovakia were under Polish rule between 1002 and 1030. In contrast with the Slovak historian, Györffy writes that this late chronicle "in which one absurdity follows another" contradicts all facts known from 11th-century sources.

The "Illuminated Chronicle" narrates that Stephen "led his army against Kean, Duke of the Bulgarians and Slavs whose lands are by their natural position most strongly fortified" following the occupation of Gyula's country. According to a number of historians, including Zoltán Lenkey and Gábor Thoroczkay, Kean was the head of a small state located in the southern parts of Transylvania and Stephen occupied his country around 1003. Other historians, including Györffy, say that the chronicle's report preserved the memory of Stephen's campaign against Bulgaria in the late 1010s.

Likewise, the identification of the "Black Hungarians"who were mentioned by Bruno of Querfurt and Adémar de Chabannes among the opponents of Stephen's proselytizing policyis uncertain. Györffy locates their lands to the east of the river Tisza; while Thoroczkay says they live in the southern parts of Transdanubia. Bruno of Querfurt's report of the Black Hungarians' conversion by force suggests that Stephen conquered their lands at the latest in 1009 when "the first mission of Saint Peter"a papal legate, Cardinal Azoarrived in Hungary. The latter attended the meeting in Győr where the royal charter determining the borders of the newly established Bishopric of Pécs was issued on 23 August 1009.

The Diocese of Eger was also set up around 1009. According to Thoroczkay, "it is very probable" that the bishopric's establishment was connected with the conversion of the Kabarsan ethnic group of Khazar origin and their chieftain. The head of the Kabarswho was either Samuel Aba or his father married Stephen's unnamed younger sister on this occasion. The Aba clan was the most powerful among the native families who joined Stephen and supported him in his efforts to establish a Christian monarchy. The reports by Anonymus, Simon of Kéza and other Hungarian chroniclers of the Bár-Kalán, Csák and other 13th-century noble families descending from Hungarian chieftains suggest that other native families were also involved in the process.

Stephen set up a territory-based administrative system, establishing counties. Each county, headed by a royal official known as a count or "ispán", was an administrative unit organized around a royal fortress. Most fortresses were earthworks in this period, but the castles at Esztergom, Székesfehérvár and Veszprém were built of stone. Forts serving as county seats also became the nuclei of Church organization. The settlements developing around them, where markets were held on each Sunday, were important local economic centers.

Stephen's brother-in-law, Henry II, became King of Germany in 1002 and Holy Roman Emperor in 1013. Their friendly relationship ensured that the western borders of Hungary experienced a period of peace in the first decades of the 11th century. Even when Henry II's discontented brother, Bruno, sought refuge in Hungary in 1004, Stephen preserved the peace with Germany and negotiated a settlement between his two brothers-in-law. Around 1009, he gave his younger sister in marriage to Otto Orseolo, Doge of Venice (r. 1008–1026), a close ally of the Byzantine Emperor, Basil II (r. 976–1025), which suggests that Hungary's relationship with the Byzantine Empire was also peaceful. On the other hand, the alliance between Hungary and the Holy Roman Empire brought her into a war with Poland lasting from around 1014 until 1018. The Poles occupied the Hungarian posts along the river Morava. Györffy and Kristó write that a Pecheneg incursion into Transylvania, the memory of which has been preserved in Stephen's legends, also took place in this period, because the Pechenegs were close allies of the Polish duke's brother-in-law, Grand Prince Sviatopolk I of Kiev (r. 1015–1019).

Poland and the Holy Roman Empire concluded the Peace of Bautzen in January 1018. Later in the same year, 500 Hungarian horsemen accompanied Boleslav of Poland to Kiev, suggesting that Hungary had been included in the peace treaty. The historian Ferenc Makk says that the Peace of Bautzen obliged Boleslav to hand over all the territories he had occupied in the Morava valley to Stephen. According to Leodvin, the first known Bishop of Bihar (r. 1050 – 1060), Stephen allied with the Byzantines and led a military expedition to assist them against "barbarians" in the Balkan Peninsula. The Byzantine and Hungarian troops jointly took "Cesaries" which Györffy identifies as the present-day town of Ohrid. Leodvin's report suggests that Stephen joined the Byzantines in the war ending with their conquest of Bulgaria in 1018. However, the exact date of his expedition is uncertain. Györffy argues that it was only in the last year of the war that Stephen led his troops against the Bulgarians.

Bishop Leodvin wrote that Stephen collected relics of a number of saints in "Cesaries" during his campaign in the Balkans, including Saint George and Saint Nicholas. He donated them to his new triple-naved basilica dedicated to the Holy Virgin in Székesfehérvár, where he also set up a cathedral chapter and his new capital. His decision was influenced by the opening, in 1018 or 1019, of a new pilgrimage route that bypassed his old capital, Esztergom. The new route connected Western Europe and the Holy Land through Hungary. Stephen often met the pilgrims, contributing to the spread of his fame throughout Europe. Abbot Odilo of Cluny, for example, wrote in a letter to Stephen that "those who have returned from the shrine of our Lord" testify to the king's passion "towards the honour of our divine religion". Stephen also established four hostels for pilgrims in Constantinople, Jerusalem, Ravenna and Rome.

In addition to pilgrims, merchants often used the safe route across Hungary when travelling between Constantinople and Western Europe. Stephen's legends refer to 60 wealthy Pechenegs who travelled to Hungary, but were attacked by Hungarian border guards. The king sentenced his soldiers to death in order to demonstrate his determination to preserve internal peace. Regular minting of coinage began in Hungary in the 1020s. Stephen's silver dinars bearing the inscriptions STEPHANUS REX ("King Stephen") and REGIA CIVITAS ("royal city") were popular in contemporary Europe, as demonstrated by counterfeited copies unearthed in Sweden.

Stephen convinced some pilgrims and merchants to settle in Hungary. Gerard, a Benedictine monk who arrived in Hungary from the Republic of Venice between 1020 and 1026, initially planned to continue his journey to the Holy Land, but decided to stay in the country after his meeting with the king. Stephen also established a number of Benedictine monasteriesincluding the abbeys at Pécsvárad, Zalavár and Bakonybélin this period.

The "Long Life of Saint Gerard" mentions Stephen's conflict with Ajtony, a chieftain in the region of the river Maros. Many historians date their clash to the end of the 1020s, although Györffy and other scholars put it at least a decade earlier. The conflict arose when Ajtony, who "had taken his power from the Greeks", according to Saint Gerard's legend, levied tax on the salt transported to Stephen on the river. The king sent a large army led by Csanád against Ajtony, who was killed in battle. His lands were transformed into a Hungarian county and the king set up a new bishopric at Csanád (Cenad, Romania), Ajtony's former capital, which was renamed after the commander of the royal army. According to the "Annales Posonienses", the Venetian Gerard was consecrated as the first bishop of the new diocese in 1030.

Stephen's brother-in-law, Emperor Henry, died on 13 July 1024. He was succeeded by a distant relative, Conrad II (r. 1024–1039), who adopted an offensive foreign policy. Conrad II expelled Doge Otto Orseolothe husband of Stephen's sisterfrom Venice in 1026. He also persuaded the Bavarians to proclaim his own son, Henry, as their duke in 1027, although Stephen's son, Emeric had a strong claim to the Duchy of Bavaria through his mother. Emperor Conrad planned a marriage alliance with the Byzantine Empire and dispatched one of his advisors, Bishop Werner of Strasbourg, to Constantinople. In the autumn of 1027, the bishop seemingly travelled as a pilgrim, but Stephen, who had been informed of his actual purpose, refused to let him enter into his country. Conrad II's biographer Wipo of Burgundy narrated that the Bavarians incited skirmishes along the common borders of Hungary and the Holy Roman Empire in 1029, causing a rapid deterioration in relations between the two countries.

Emperor Conrad personally led his armies to Hungary in June 1030 and plundered the lands west of the River Rába. However, according to the "Annals of Niederalteich", the emperor, suffering from consequences of the scorched earth tactics used by the Hungarian army, returned to Germany "without an army and without achieving anything, because the army was threatened by starvation and was captured by the Hungarians at Vienna". Peace was restored after Conrad had ceded the lands between the rivers Lajta and Fischa to Hungary in the summer of 1031.

Stephen's biographer, Hartvic, narrates that the King, whose children died one by one in infancy, "restrained the grief over their death by the solace on account of the love of his surviving son", Emeric. However, Emeric was wounded in a hunting accident and died in 1031. After the death of his son, the elderly King could never "fully regain his former health", according to the "Illuminated Chronicle". Kristó writes that the picture, which has been preserved in Stephen's legends, of the king keeping the vigils and washing the feet of paupers, is connected with Stephen's last years, following the death of his son.

Emeric's death jeopardized his father's achievements in establishing a Christian state, because Stephen's cousin, Vazulwho had the strongest claim to succeed himwas suspected of an inclination towards paganism. According to the "Annals of Altaich" Stephen disregarded his cousin's claim and nominated his sister's son, the Venetian Peter Orseolo, as his heir. The same source adds that Vazul was captured and blinded, and his three sons, Levente, Andrew and Béla, were expelled from Hungary. Stephen's legends refer to an unsuccessful attempt upon the elderly king's life by members of his court. According to Kristó, the legends refer to a plot in which Vazul participated and his mutilation was a punishment for this act. That Vazul's ears were filled with molten lead was only recorded in later sources, including the "Illuminated Chronicle".

In the view of some historians, provisions in Stephen's "Second Book of Laws" on the "conspiracy against the king and the kingdom" imply that the book was promulgated after Vazul's unsuccessful plot against Stephen. However, this view has not been universally accepted. Györffy states that the law book was issued, not after 1031, but around 1009. Likewise, the authenticity of the decree on tithes is debated: according to Györffy, it was issued during Stephen's reign, but Berend, Laszlovszky and Szakács argue that it "might be a later addition".

Stephen died on 15 August 1038. He was buried in the basilica of Székesfehérvár. His reign was followed by a long period of civil wars, pagan uprisings and foreign invasions. The instability ended in 1077 when Ladislaus, a grandson of Vazul, ascended the throne.

Stephen married Gisela, a daughter of Duke Henry the Wrangler of Bavaria, who was a nephew of Otto I, Holy Roman Emperor. Gisela's mother was Gisela of Burgundy, a member of the Welf dynasty. Born around 985, Gisela was younger than her husband, whom she survived. She left Hungary in 1045 and died as Abbess of the Niedernburg Abbey in Passau in Bavaria around 1060.

Although the "Illuminated Chronicle" states that Stephen "begot many sons", only two of them, Otto and Emeric, are known by name. Otto, who was named after Otto III, seems to have been born before 1002. He died as a child.

Emeric, who received the name of his maternal uncle, Emperor Henry II, was born around 1007. His "Legend" from the early 12th century describes him as a saintly prince who preserved his chastity even during his marriage. According to Györffy, Emeric's wife was a kinswoman of the Byzantine Emperor Basil II. His premature death led to the series of conflicts leading to Vazul's blinding and civil wars.

The following family tree presents Stephen's ancestors and his relatives who are mentioned in the article.

"*A Khazar, Pecheneg or Volga Bulgarian lady."<br>"**Györffy writes that she may have been a member of the Bulgarian Cometopuli dynasty."<br>"***Samuel Aba might have been the son of Stephen's sister instead of her husband."

Stephen has always been considered one of the most important statesmen in the history of Hungary. His main achievement was the establishment of a Christian state that ensured that the Hungarians survived in the Carpathian Basin, in contrast to the Huns, Avars and other peoples who had previously controlled the same territory. As Bryan Cartledge emphasizes, Stephen also gave his kingdom "forty years of relative peace and sound but unspectacular rule".

His successors, including those descended from Vazul, were eager to emphasize their devotion to Stephen's achievements. Although Vazul's son, Andrew I of Hungary, secured the throne due to a pagan uprising, he prohibited pagan rites and declared that his subjects should "live in all things according to the law which King St. Stephen had taught them", according to the 14th-century "Illuminated Chronicle". In medieval Hungary, communities that claimed a privileged status or attempted to preserve their own "liberties" often declared that the origin of their special status was to be attributed to King Saint Stephen. An example is a 1347 letter from the people of Táp telling the king about their grievances against the Pannonhalma Archabbey and stating that the taxes levied upon them by the abbot contradicted "the liberty granted to them in the time of King Saint Stephen".

Stephen's cult emerged after the long period of anarchy characterizing the rule of his immediate successors. However, there is no evidence that Stephen became an object of veneration before his canonization. For instance, the first member of the royal family to be named after him, Stephen II, was born in the early 12th century.

Stephen's canonization was initiated by Vazul's grandson, King Ladislaus I of Hungary, who had consolidated his authority by capturing and imprisoning his cousin, Solomon. According to Bishop Hartvic, the canonization was "decreed by apostolic letter, by order of the Roman see", suggesting that the ceremony was permitted by Pope Gregory VII. The ceremony started at Stephen's tomb, where on 15 August 1083 masses of believers began three days of fasting and praying. Legend tells that Stephen's coffin could not be opened until King Ladislaus held Solomon in captivity at Visegrád. The opening of Stephen's tomb was followed by the occurrence of healing miracles, according to Stephen's legends. Historian Kristó attributes the healings either to mass psychosis or deception. Stephen's legends also say that his "balsam-scented" remains were elevated from the coffin, which was filled with "rose-colored water", on 20 August. On the same day, Stephen's son, Emeric, and the bishop of Csanád, Gerard, were also canonized.

Stephen's first legend, the so-called "Greater Legend", was written between 1077 and 1083. It provided an idealized portrait of the king, one who dedicated himself and his kingdom to the Virgin Mary. However, Stephen's "Lesser Legend"composed around 1100, under King Colomanemphasized Stephen's severity. A third legend, also composed during King Coloman's reign by Bishop Hartvic, was based on the two existing legends. Sanctioned in 1201 by Pope Innocent III, Hartvic's work served as Stephen's official legend. Gábor Klaniczay wrote that Stephen's legends "opened a new chapter in the legends of holy rulers as a genre", suggesting that a monarch can achieve sainthood through actively using his royal powers. Stephen was the first triumphant "miles Christi" ("Christ's soldier") among the canonized monarchs. He was also a "confessor king", one who had not suffered martyrdom, whose cult was sanctioned, in contrast with earlier holy monarchs.

Stephen's cult spread beyond the borders of Hungary. Initially, he was primarily venerated in Scheyern and Bamberg, in Bavaria, but his relics were also taken to Aachen, Cologne, Montecassino and Namur. Upon the liberation of Buda from the Ottoman Turks, Pope Innocent XI expanded King Saint Stephen's cult to the entire Roman Catholic Church in 1686, and declared 2 September his feast day. As the feast of Saint Joachim was moved, in 1969, from 16 August, the day immediately following the day of Stephen's death, Stephen's feast was moved to that date. Stephen is venerated as the patron saint of Hungary, and regarded as the protector of kings, masons, stonecutters, stonemasons and bricklayers, and also of children suffering from severe illnesses. His canonization was recognized by Ecumenical Patriarch Bartholomew I of Constantinople in 2000. In the calendar of the Hungarian Catholic Church, Stephen's feast is observed on 20 August, the day on which his relics were translated. In addition, a separate feast day (30 May) is dedicated to his "Holy Dexter".

Stephen's intact dexter, or right hand (), became the subject of a cult. A cleric named Mercurius stole it, but it was discovered on 30 May 1084 in Bihar County. The theft of sacred relics, or "furta sacra", had by that time become a popular topic of saints' biographies. Bishop Hartvic described the discovery of Stephen's right hand in accordance with this tradition, referring to adventures and visions. An abbey erected in Bihar County (now Sâniob, Romania) was named after and dedicated to the veneration of the Holy Dexter.

The Holy Dexter was kept for centuries in the Szentjobb Abbey, except during the Mongol invasion of 1241 and 1242, when it was transferred to Ragusa (now Dubrovnik, Croatia). The relic was then taken to Székesfehérvár around 1420. Following the Ottoman occupation of the central territories of the Kingdom of Hungary in the mid-16th century, it was guarded in many places, including Bosnia, Ragusa and Vienna. It was returned to Hungary in 1771, when Queen Maria Theresa donated it to the cloister of the Sisters of Loreto in Buda. It was kept in Buda Castle's St. Sigismund Chapel between around 1900 and 1944, in a cave near Salzburg in 1944 and 1945, and again by the Sisters of Loreto in Buda, between 1945 and 1950. Finally, since 1950, the Holy Dexter has been in St. Stephen's Basilica in Budapest. An annual procession celebrating the relic was instituted in 1938, and continued until 1950, when the procession was forbidden by the Communist government. It was resumed in 1988.

According to Stephen's "Greater Legend", the king "himself compiled a book for his son on moral education". This work, now known as "Admonitions" or "De institutione morum", was preserved in manuscripts written in the Late Middle Ages. Although scholars debate whether it can actually be attributed to the king or a cleric, most of them agree that it was composed in the first decades of the 11th century.

The "Admonitions" argues that kingship is inseparably connected with the Catholic faith. Its author emphasized that a monarch is required to make donations to the Church and regularly consult his prelates, but is entitled to punish clergymen who do wrong. One of its basic ideas was that a sovereign has to cooperate with the "pillars of his rule", meaning the prelates, aristocrats, "ispáns" and warriors.

King St Stephen has been a popular theme in Hungarian poetry since the end of the 13th century. The earliest poems were religious hymns which portrayed the holy king as the apostle of the Hungarians. Secular poetry, especially poems written for his feast day, followed a similar pattern, emphasizing Stephen's role as the first king of Hungary. Poets described Stephen as the symbol of national identity and independence and of the ability of the Hungarian nation to survive historical cataclysms during the Communist regime between 1949 and 1989.

A popular hymn, still sung in the churches, was first recorded in the late . It hails King St. Stephen as "radiant star of Hungarians". Ludwig van Beethoven composed his "King Stephen Overture" for the inauguration of the Hungarian theatre in Pest in 1812. According to musician James M. Keller, "[t]he descending unisons that open the "King Stephen Overture" would seem to prefigure the opening of the "Ninth Symphony" [a]nd then a later theme, introduced by flutes and clarinets, seems almost to be a of the famous "Ode 'To Joy"' melody of the Ninth Symphony's finale". Hungarian composer Ferenc Erkel named his last complete opera from 1885, "István király" ("King Stephen"), after him. In 1938, Zoltán Kodály wrote a choral piece titled "Ének Szent István Királyhoz" ("Hymn to King Stephen"). In 1983, Levente Szörényi and János Bródy composed a rock opera—"István, a király" ("Stephen, the King")—about the early years of his reign. Seventeen years later, in 2000, Szörényi composed a sequel called "Veled, Uram!" ("You, Sir").





</doc>
<doc id="29598" url="https://en.wikipedia.org/wiki?curid=29598" title="Sprite">
Sprite

Sprite commonly refers to:

Sprite may also refer to:







</doc>
<doc id="29602" url="https://en.wikipedia.org/wiki?curid=29602" title="San Giovanni di Posada">
San Giovanni di Posada

San Giovanni di Posada (Latin: Portus Luguidonis or Portus Liquidonis) is a "frazione" and small village in Sardinia, Italy, on the Tyrrhenian coast of the island, in the territory of the "comune" of Posada.

Formerly known as Marina di Posada, it underwent rebuilding in the 1970s as an elegant residential village for holidays.

Its history goes back to the Roman harbour (named "Portus Luguidonis" - presumably located in the little bay in front of the ancient church of St. John), from where the Romans entered inner Sardinia. Through this harbour passed all the goods to or from Rome, but all the cargo was carried by small and light ships directed to Olbia (some 50km north), where bigger ships would have trafficked with Ostia. Traffic was supposedly intense, Sardinia bearing the "sobriquet" "the granary of Rome".

In the immediate surroundings, it is supposed there was a temple in honour of Feronia, an Etruscan deity, goddess of the waters; this would prove the presence of Etruscans in this area at the time of Nuragici people. A similar cult of Feronia is reported on the Italian mainland at least in two places: in Fiano Romano (near Rome), and in Terracina, some 120km south of Rome.

It is one of the main tourist destinations of Sardinia, has a long beach (more than 15km of white sand) and a system of rivers of biological importance. A part of this territory is going to be formally protected in the near future with the creation of a nature park ("Parco Fluviale").



</doc>
<doc id="29603" url="https://en.wikipedia.org/wiki?curid=29603" title="Scott Joplin">
Scott Joplin

Scott Joplin (November 24, 1868 – April 1, 1917) was an African-American composer and pianist. Joplin achieved fame for his ragtime compositions and was dubbed the King of Ragtime. During his brief career, he wrote 44 original ragtime pieces, one ragtime ballet, and two operas. One of his first and most popular pieces, the "Maple Leaf Rag", became ragtime's first and most influential hit, and has been recognized as the archetypal rag.

Joplin grew up in a musical family of railway laborers in Texarkana, Arkansas, and developed his own musical knowledge with the help of local teachers. While in Texarkana, Texas, he formed a vocal quartet and taught mandolin and guitar. During the late 1880s he left his job as a railroad laborer and travelled the American South as an itinerant musician. He went to Chicago for the World's Fair of 1893, which played a major part in making ragtime a national craze by 1897.

Joplin moved to Sedalia, Missouri, in 1894 and earned a living as a piano teacher. There he taught future ragtime composers Arthur Marshall, Scott Hayden and Brun Campbell. He began publishing music in 1895, and publication of his "Maple Leaf Rag" in 1899 brought him fame. This piece had a profound influence on writers of ragtime. It also brought Joplin a steady income for life, though he did not reach this level of success again and frequently had financial problems. In 1901 Joplin moved to St. Louis, where he continued to compose and publish, and regularly performed in the community. The score to his first opera "A Guest of Honor" was confiscated in 1903 with his belongings for non-payment of bills, and is now considered lost.

In 1907, Joplin moved to New York City to find a producer for a new opera. He attempted to go beyond the limitations of the musical form that made him famous, but without much monetary success. His second opera, "Treemonisha", was never fully staged during his lifetime.

In 1916, Joplin descended into dementia as a result of syphilis. He was admitted to Manhattan State Hospital a mental institution in January 1917, and died there three months later at the age of 48. Joplin's death is widely considered to mark the end of ragtime as a mainstream music format; over the next several years, it evolved with other styles into stride, jazz, and eventually big band swing.

Joplin's music was rediscovered and returned to popularity in the early 1970s with the release of a million-selling album recorded by Joshua Rifkin. This was followed by the Academy Award-winning 1973 film "The Sting" that featured several of Joplin's compositions, most notably "The Entertainer", whose performance by pianist Marvin Hamlisch received wide airplay. "Treemonisha" was finally produced in full, to wide acclaim, in 1972. In 1976, Joplin was posthumously awarded a Pulitzer Prize.
According to author Edward A. Berlin, "One tenacious myth tells us that Joplin was born in Texarkana, Texas, on November 24, 1868. The location is easily dispensed with: Texarkana was not established until 1873." But, based on a letter discovered by musicologist John Tennison in 2015 in the December 19, 1856 edition of "The Times-Picayune", it is clear that Texarkana was established as a place-name at least as early as 1856. Consequently, it appears possible that Joplin, born 12 years later, could have been born in Texarkana. Others, including genealogists, simply posit that Joplin was born in Linden, Texas, either in late 1867 or early 1868. This birth location is supported by the first census data in which he appears as a two-year old in 1870, Linden, Cass, Texas. Linden is 40 miles southwest of Texarkana.

He was the second of six children (the others being Monroe, Robert, William, Myrtle, and Ossie) born to Giles Joplin, an ex-slave from North Carolina, and Florence Givens, a freeborn African-American woman from Kentucky.

The Joplins "subsequently" moved to Texarkana, Arkansas, where Giles worked as a laborer for the railroad and Florence was a cleaner. Joplin's father had played the violin for plantation parties in North Carolina, and his mother sang and played the banjo. Joplin was given a rudimentary musical education by his family and from the age of seven, he was allowed to play the piano while his mother cleaned.

At some point in the early 1880s, Giles Joplin left the family for another woman. His wife Florence struggled to support her children through domestic work. Biographer Susan Curtis speculated that the mother's support of Joplin's musical education was critical to the parents' separation. Joplin's father wanted the boy to pursue practical employment that would supplement the family income.

According to a family friend, the young Joplin was serious and ambitious, studying music and playing the piano after school. While a few local teachers aided him, he received most of his music education from Julius Weiss, a German-born American Jewish music professor who had immigrated to Texas in the late 1860s and was employed as music tutor to a prominent local business family. Weiss, as described by "San Diego Jewish World" writer Eric George Tauber, "was no stranger to [receiving] race hatred... As a German Jew, he was often slapped and called a “Christ-killer." Nevertheless, Weiss had studied music at university in Germany and was listed in town records as a "Professor of music." Impressed by Joplin's talent, and realizing his family's dire straits, Weiss taught him free of charge. He tutored the 11-year-old Joplin until the boy was 16, during which time Weiss introduced him to folk and classical music, including opera. Weiss helped Joplin appreciate music as an "art as well as an entertainment," and helped his mother acquire a used piano. According to Weiss' wife, Lottie, Joplin never forgot Weiss. In his later years, after achieving fame as a composer, Joplin sent his former teacher "...gifts of money when he was old and ill," until Weiss died. At the age of 16, Joplin performed in a vocal quartet with three other boys in and around Texarkana, also playing piano. In addition he taught guitar and mandolin.

In the late 1880s, having performed at various local events as a teenager, Joplin chose to give up work as a laborer with the railroad and left Texarkana to become a traveling musician. Little is known about his movements at this time, although he is recorded in Texarkana in July 1891 as a member of the Texarkana Minstrels in a performance that happened to be raising money for a monument to Jefferson Davis, President of the Southern Confederacy. He soon discovered, however, that there were few opportunities for black pianists. Churches and brothels were among the few options for steady work. Joplin played pre-ragtime 'jig-piano' in various red-light districts throughout the mid-South, and some claim he was in Sedalia and St. Louis in Missouri during this time.

In 1893 Joplin was in Chicago for the World's Fair. While in Chicago, he formed his first band playing cornet and began arranging music for the group to perform. Although the World's Fair minimized the involvement of African-Americans, black performers still came to the saloons, cafés and brothels that lined the fair. The exposition was attended by 27 million Americans and had a profound effect on many areas of American cultural life, including ragtime. Although specific information is sparse, numerous sources have credited the Chicago World Fair with spreading the popularity of ragtime. Joplin found that his music, as well as that of other black performers, was popular with visitors. By 1897 ragtime had become a national craze in U.S. cities, and was described by the "St. Louis Dispatch" as "a veritable call of the wild, which mightily stirred the pulses of city bred people."

In 1894 Joplin arrived in Sedalia, Missouri. At first, Joplin stayed with the family of Arthur Marshall, at the time a 13-year-old boy but later one of Joplin's students and a rag-time composer in his own right. There is no record of Joplin having a permanent residence in the town until 1904, as Joplin was making a living as a touring musician.

There is little precise evidence known about Joplin's activities at this time, although he performed as a solo musician at dances and at the major black clubs in Sedalia, the Black 400 club and the Maple Leaf Club. He performed in the Queen City Cornet Band, and his own six-piece dance orchestra. A tour with his own singing group, the Texas Medley Quartet, gave him his first opportunity to publish his own compositions and it is known that he went to Syracuse, New York and Texas. Two businessmen from New York published Joplin's first two works, the songs "Please Say You Will", and "A Picture of her Face" in 1895. Joplin's visit to Temple, Texas, enabled him to have three pieces published there in 1896, including the "Great Crush Collision March", which commemorated a planned train crash on the Missouri–Kansas–Texas Railroad on September 15 that he may have witnessed. The March was described by one of Joplin's biographers as a "special... early essay in ragtime." While in Sedalia he was teaching piano to students who included future ragtime composers Arthur Marshall, Brun Campbell, and Scott Hayden. In turn, Joplin enrolled at the George R. Smith College, where he apparently studied "...advanced harmony and composition." The College records were destroyed in a fire in 1925, and biographer Edward A. Berlin notes that it was unlikely that a small college for African-Americans would be able to provide such a course.

In 1899, Joplin married Belle, the sister-in-law of collaborator Scott Hayden. Although there were hundreds of rags in print by the time the "Maple Leaf Rag" was published, Joplin was not far behind. His first published rag, "Original Rags", had been completed in 1897, the same year as the first ragtime work in print, the "Mississippi Rag" by William Krell. The "Maple Leaf Rag" was likely to have been known in Sedalia before its publication in 1899; Brun Campbell claimed to have seen the manuscript of the work in around 1898. The exact circumstances that led to the Maple Leaf Rag's publication are unknown, and a number of versions of the event contradict each other. After several unsuccessful approaches to publishers, Joplin signed a contract on August 10, 1899 with John Stillwell Stark, a retailer of musical instruments who later became his most important publisher. The contract stipulated that Joplin would receive a 1% royalty on all sales of the rag, with a minimum sales price of 25 cents. With the inscription "To the Maple Leaf Club" prominently visible along the top of at least some editions, it is likely that the rag was named after the Maple Leaf Club, although there is no direct evidence to prove the link, and there were many other possible sources for the name in and around Sedalia at the time.

There have been many claims about the sales of the "Maple Leaf Rag", for example that Joplin was the first musician to sell 1 million copies of a piece of instrumental music. Joplin's first biographer, Rudi Blesh wrote that during its first six months the piece sold 75,000 copies, and became "...the first great instrumental sheet music hit in America." However, research by Joplin's later biographer Edward A. Berlin demonstrated that this was not the case; the initial print-run of 400 took one year to sell, and under the terms of Joplin's contract with a 1% royalty would have given Joplin an income of $4 (or approximately $ at current prices). Later sales were steady, and would have given Joplin an income that would have covered his expenses. In 1909, estimated sales would have given him an income of $600 annually (approximately $16,968 in current prices).

The "Maple Leaf Rag" did serve as a model for the hundreds of rags to come from future composers, especially in the development of classic ragtime. After the publication of the "Maple Leaf Rag", Joplin was soon being described as "King of rag time writers", not least by himself on the covers of his own work, such as "The Easy Winners" and "Elite Syncopations".

After the Joplins moved to St. Louis in early 1900, they had a baby daughter who died only a few months after birth. Joplin's relationship with his wife was difficult, as she had no interest in music. They eventually separated and then divorced. About this time, Joplin collaborated with Scott Hayden in the composition of four rags. It was in St. Louis that Joplin produced some of his best-known works, including "The Entertainer", "March Majestic", and the short theatrical work "The Ragtime Dance".

In June 1904, Joplin married Freddie Alexander of Little Rock, Arkansas, the young woman to whom he had dedicated "The Chrysanthemum". She died on September 10, 1904, of complications resulting from a cold, ten weeks after their wedding. Joplin's first work copyrighted after Freddie's death, "Bethena", was described by one biographer as "...an enchantingly beautiful piece that is among the greatest of ragtime waltzes."

During this time, Joplin created an opera company of 30 people and produced his first opera "A Guest of Honor" for a national tour. It is not certain how many productions were staged, or even if this was an all-black show or a racially mixed production. During the tour, either in Springfield, Illinois, or Pittsburg, Kansas, someone associated with the company stole the box office receipts. Joplin could not meet the company's payroll or pay for its lodgings at a theatrical boarding house. It is believed that the score for "A Guest of Honor" was lost and perhaps destroyed because of non-payment of the company's boarding house bill.

In 1907, Joplin moved to New York City, which he believed was the best place to find a producer for a new opera. After his move to New York, Joplin met Lottie Stokes, whom he married in 1909. In 1911, unable to find a publisher, Joplin undertook the financial burden of publishing "Treemonisha" himself in piano-vocal format. In 1915, as a last-ditch effort to see it performed, he invited a small audience to hear it at a rehearsal hall in Harlem. Poorly staged and with only Joplin on piano accompaniment, it was "a miserable failure" to a public not ready for "crude" black musical forms—so different from the European grand opera of that time. The audience, including potential backers, was indifferent and walked out. Scott writes that "after a disastrous single performance ... Joplin suffered a breakdown. He was bankrupt, discouraged, and worn out." He concludes that few American artists of his generation faced such obstacles: ""Treemonisha" went unnoticed and unreviewed, largely because Joplin had abandoned commercial music in favor of art music, a field closed to African Americans." In fact, it was not until the 1970s that the opera received a full theatrical staging.

In 1914, Joplin and Lottie self-published his "Magnetic Rag" as the Scott Joplin Music Company, which he had formed the previous December. Biographer Vera Brodsky Lawrence speculates that Joplin was aware of his advancing deterioration due to syphilis and was "...consciously racing against time." In her sleeve notes on the 1992 Deutsche Grammophon release of "Treemonisha" she notes that he "...plunged feverishly into the task of orchestrating his opera, day and night, with his friend Sam Patterson standing by to copy out the parts, page by page, as each page of the full score was completed."

By 1916, Joplin was suffering from tertiary syphilis but more specifically it likely was neurosyphilis. In January 1917, he was admitted to Manhattan State Hospital, a mental institution. He died there on April 1 of syphilitic dementia at the age of 48 and was buried in a pauper's grave that remained unmarked for 57 years. His grave at Saint Michaels Cemetery in East Elmhurst was finally given a marker in 1974, the year "The Sting", which showcased his music, won for Best Picture at the Oscars.

The combination of classical music, the musical atmosphere present around Texarkana (including work songs, gospel hymns, spirituals and dance music) and Joplin's natural ability have been cited as contributing significantly to the invention of a new style that blended African-American musical styles with European forms and melodies, and first became celebrated in the 1890s: ragtime.

When Joplin was learning the piano, serious musical circles condemned ragtime because of its association with the vulgar and inane songs "...cranked out by the tune-smiths of Tin Pan Alley." As a composer Joplin refined ragtime, elevating it above the low and unrefined form played by the "...wandering honky-tonk pianists... playing mere dance music" of popular imagination. This new art form, the classic rag, combined Afro-American folk music's syncopation and 19th-century European romanticism, with its harmonic schemes and its march-like tempos. In the words of one critic, "Ragtime was basically... an Afro-American version of the polka, or its analog, the Sousa-style march." With this as a foundation, Joplin intended his compositions to be played exactly as he wrote them – without improvisation. Joplin wrote his rags as "classical" music in miniature form in order to raise ragtime above its "cheap bordello" origins and produced work that opera historian Elise Kirk described as, "... more tuneful, contrapuntal, infectious, and harmonically colorful than any others of his era."

Some speculate that Joplin's achievements were influenced by his classically trained German music teacher Julius Weiss, who may have brought a polka rhythmic sensibility from the old country to the 11-year old Joplin. As Curtis put it, "The educated German could open up the door to a world of learning and music of which young Joplin was largely unaware."

Joplin's first and most significant hit, the "Maple Leaf Rag", was described as the archetype of the classic rag, and influenced subsequent rag composers for at least 12 years after its initial publication thanks to its rhythmic patterns, melody lines, and harmony, though with the exception of Joseph Lamb, they generally failed to enlarge upon it.

The opera's setting is a former slave community in an isolated forest near Joplin's childhood town Texarkana in September 1884. The plot centers on an 18-year-old woman Treemonisha who is taught to read by a white woman, and then leads her community against the influence of conjurers who prey on ignorance and superstition. Treemonisha is abducted and is about to be thrown into a wasps' nest when her friend Remus rescues her. The community realizes the value of education and the liability of their ignorance before choosing her as their teacher and leader.

Joplin wrote both the score and the libretto for the opera, which largely follows the form of European opera with many conventional arias, ensembles and choruses. In addition the themes of superstition and mysticism evident in "Treemonisha" are common in the operatic tradition, and certain aspects of the plot echo devices in the work of the German composer Richard Wagner (of which Joplin was aware). A sacred tree Treemonisha sits beneath recalls the tree that Siegmund takes his enchanted sword from in "Die Walküre", and the retelling of the heroine's origins echos aspects of the opera "Siegfried". In addition, African-American folk tales also influence the story—the wasp nest incident is similar to the story of Br'er Rabbit and the briar patch.

"Treemonisha" is not a ragtime opera—because Joplin employed the styles of ragtime and other black music sparingly, using them to convey "racial character," and to celebrate the music of his childhood at the end of the 19th century. The opera has been seen as a valuable record of rural black music from late 19th century re-created by a "skilled and sensitive participant."

Berlin speculates about parallels between the plot and Joplin's own life. He notes that Lottie Joplin (the composer's third wife) saw a connection between the character Treemonisha's wish to lead her people out of ignorance, and a similar desire in the composer. In addition, it has been speculated that Treemonisha represents Freddie, Joplin's second wife, because the date of the opera's setting was likely to have been the month of her birth.

At the time of the opera's publication in 1911, the "American Musician and Art Journal" praised it as, "...an entirely new form of operatic art." Later critics have also praised the opera as occupying a special place in American history, with its heroine, "...a startlingly early voice for modern civil rights causes, notably the importance of education and knowledge to African American advancement." Curtis's conclusion is similar: "In the end, "Treemonisha" offered a celebration of literacy, learning, hard work, and community solidarity as the best formula for advancing the race." Berlin describes it as a "...fine opera, certainly more interesting than most operas then being written in the United States," but later states that Joplin's own libretto showed the composer, "...was not a competent dramatist," with the book not up to the quality of the music.

As Rick Benjamin, the founder and director of the Paragon Ragtime Orchestra, found out, Joplin succeeded in performing "Treemonisha" for paying audiences in Bayonne, New Jersey, in 1913. On 6 December 2011, the centenary of the Joplin piano score's publication, New World Records released an entirely new recording of "Treemonisha". August 1984 saw the German premiere of "Treemonisha" at the Stadttheater Gießen. In October 2013, Nicolás Isasi directed the premiere of "Treemonisha" in Argentina with a team of 60 young artists at the in Buenos Aires. Another performance in Germany, falsely labelling itself as the German premiere, occurred on 25 April 2015 at the Staatsschauspiel Dresden under direction and choreography of .

Joplin's skills as a pianist were described in glowing terms by a Sedalia newspaper in 1898, and fellow ragtime composers Arthur Marshall and Joe Jordan both said that he played the instrument well. However, the son of publisher John Stark stated that Joplin was a rather mediocre pianist and that he composed on paper, rather than at the piano. Artie Matthews recalled the "delight" the St. Louis players took in outplaying Joplin.

While Joplin never made an audio recording, his playing is preserved on seven piano rolls for use in mechanical player pianos. All seven were made in 1916. Of these, the six released under the Connorized label show evidence of significant editing to correct the performance to strict rhythm and add embellishments, probably by the staff musicians at Connorized. Berlin theorizes that by the time Joplin reached St. Louis, he may have experienced discoordination of the fingers, tremors, and an inability to speak clearly—all symptoms of the syphilis that killed him in 1917. Biographer Blesh described the second roll recording of "Maple Leaf Rag" on the UniRecord label from June 1916 as "...shocking... disorganized and completely distressing to hear." While there is disagreement among piano-roll experts as to how much of this is due to the relatively primitive recording and production techniques of the time, Berlin notes that the "Maple Leaf Rag" roll was likely to be the truest record of Joplin's playing at the time. The roll, however, may not reflect his abilities earlier in life.

Joplin and his fellow ragtime composers rejuvenated American popular music, fostering an appreciation for African-American music among European-Americans by creating exhilarating and liberating dance tunes. "Its syncopation and rhythmic drive gave it a vitality and freshness attractive to young urban audiences indifferent to Victorian proprieties ... Joplin's ragtime expressed the intensity and energy of a modern urban America."

Joshua Rifkin, a leading Joplin recording artist, wrote, "A pervasive sense of lyricism infuses his work, and even at his most high-spirited, he cannot repress a hint of melancholy or adversity ... He had little in common with the fast and flashy school of ragtime that grew up after him." Joplin historian Bill Ryerson adds that, "In the hands of authentic practitioners like Joplin, ragtime was a disciplined form capable of astonishing variety and subtlety ... Joplin did for the rag what Chopin did for the mazurka. His style ranged from tones of torment to stunning serenades that incorporated the bolero and the tango." Biographer Susan Curtis wrote that Joplin's music had helped to "revolutionise American music and culture" by removing Victorian restraint.

Composer and actor Max Morath found it striking that the vast majority of Joplin's work did not enjoy the popularity of the "Maple Leaf Rag", because while the compositions were of increasing lyrical beauty and delicate syncopation they remained obscure and unheralded during his lifetime. Joplin apparently realized that his music was ahead of its time: As music historian Ian Whitcomb mentions that Joplin, "...opined that "Maple Leaf Rag" would make him 'King of Ragtime Composers' but he also knew that he would not be a pop hero in his own lifetime. 'When I'm dead twenty-five years, people are going to recognize me,' he told a friend." Just over thirty years later he was recognized, and later historian Rudi Blesh wrote a large book about ragtime, which he dedicated to the memory of Joplin.

Although he was penniless and disappointed at the end of his life, Joplin set the standard for ragtime compositions and played a key role in the development of ragtime music. And as a pioneer composer and performer, he helped pave the way for young black artists to reach American audiences of both races. After his death, jazz historian Floyd Levin noted: "Those few who realized his greatness bowed their heads in sorrow. This was the passing of the king of all ragtime writers, the man who gave America a genuine native music."

The home Joplin rented in St. Louis from 1900 to 1903 was recognized as a National Historic Landmark in 1976 and was saved from destruction by the local African American community. In 1983, the Missouri Department of Natural Resources made it the first state historic site in Missouri dedicated to African American heritage. At first it focused entirely on Joplin and ragtime music, ignoring the urban milieu which shaped his musical compositions. A newer heritage project has expanded coverage to include the more complex social history of black urban migration and the transformation of a multi-ethnic neighborhood to the contemporary community. Part of this diverse narrative now includes coverage of uncomfortable topics of racial oppression, poverty, sanitation, prostitution, and sexually transmitted diseases.

After his death in 1917, Joplin's music and ragtime in general waned in popularity as new forms of musical styles, such as jazz and novelty piano, emerged. Even so, jazz bands and recording artists such as Tommy Dorsey in 1936, Jelly Roll Morton in 1939 and J. Russel Robinson in 1947 released recordings of Joplin compositions. "Maple Leaf Rag" was the Joplin piece found most often on 78 rpm records.

In the 1960s, a small-scale reawakening of interest in classical ragtime was underway among some American music scholars such as Trebor Tichenor, William Bolcom, William Albright and Rudi Blesh. Audiophile Records released a two record set, "The Complete Piano Works of Scott Joplin, The Greatest of Ragtime Composers", performed by Knocky Parker, in 1970.

In 1968, Bolcom and Albright interested Joshua Rifkin, a young musicologist, in the body of Joplin's work. Together, they hosted an occasional ragtime-and-early-jazz evening on WBAI radio. In November 1970, Rifkin released a recording called "" on the classical label Nonesuch. It sold 100,000 copies in its first year and eventually became Nonesuch's first million-selling record. The "Billboard" Best-Selling Classical LPs chart for September 28, 1974 has the record at number 5, with the follow-up "Volume 2" at number 4, and a combined set of both volumes at number 3. Separately both volumes had been on the chart for 64 weeks. In the top seven spots on that chart, six of the entries were recordings of Joplin's work, three of which were Rifkin's. Record stores found themselves for the first time putting ragtime in the classical music section. The album was nominated in 1971 for two Grammy Award categories: Best Album Notes and Best Instrumental Soloist Performance (without orchestra). Rifkin was also under consideration for a third Grammy for a recording not related to Joplin, but at the ceremony on March 14, 1972, Rifkin did not win in any category. He did a tour in 1974, which included appearances on BBC Television and a sell-out concert at London's Royal Festival Hall. In 1979, Alan Rich wrote in the magazine "New York" that by giving artists like Rifkin the opportunity to put Joplin's music on disk, Nonesuch Records "...created, almost alone, the Scott Joplin revival."

In January 1971, Harold C. Schonberg, music critic at "The New York Times", having just heard the Rifkin album, wrote a featured Sunday edition article entitled "Scholars, Get Busy on Scott Joplin!" Schonberg's call to action has been described as the catalyst for classical music scholars, the sort of people Joplin had battled all his life, to conclude that Joplin was a genius. Vera Brodsky Lawrence of the New York Public Library published a two-volume set of Joplin works in June 1971, entitled "The Collected Works of Scott Joplin", stimulating a wider interest in the performance of Joplin's music.

In mid-February 1973 under the direction of Gunther Schuller, the New England Conservatory Ragtime Ensemble recorded an album of Joplin's rags taken from the period collection "Standard High-Class Rags" called "Joplin: The Red Back Book". The album won a Grammy Award as Best Chamber Music Performance in that year, and went on to become "Billboard" magazine's Top Classical Album of 1974. The group subsequently recorded two more albums for Golden Crest Records: "More Scott Joplin Rags" in 1974 and "The Road From Rags To Jazz" in 1975.

In 1973, film producer George Roy Hill contacted Schuller and Rifkin separately, asking each man to write the score for a film project he was working on: "The Sting". Both men turned down the request because of previous commitments. Instead Hill found Marvin Hamlisch available, and brought him into the project as composer. Hamlisch lightly adapted Joplin's music for "The Sting", for which he won an Academy Award for Best Original Song Score and Adaptation on April 2, 1974. His version of "The Entertainer" reached number 3 on the "Billboard" Hot 100 and the American Top 40 music chart on May 18, 1974, prompting "The New York Times" to write, "The whole nation has begun to take notice." Thanks to the film and its score, Joplin's work became appreciated in both the popular and classical music world, becoming (in the words of music magazine "Record World"), the "classical phenomenon of the decade." Rifkin later said of the film soundtrack that Hamlisch lifted his piano adaptations directly from Rifkin's style and his band adaptations from Schuller's style. Schuller said Hamlisch, "...got the Oscar for music he didn't write (since it is by Joplin) and arrangements he didn't write, and 'editions' he didn't make. A lot of people were upset by that, but that's show biz!"

On October 22, 1971, excerpts from "Treemonisha" were presented in concert form at Lincoln Center with musical performances by Bolcom, Rifkin and Mary Lou Williams supporting a group of singers. Finally, on January 28, 1972, T.J. Anderson's orchestration of "Treemonisha" was staged for two consecutive nights, sponsored by the Afro-American Music Workshop of Morehouse College in Atlanta, with singers accompanied by the Atlanta Symphony Orchestra under the direction of Robert Shaw, and choreography by Katherine Dunham. Schonberg remarked in February 1972 that the "Scott Joplin Renaissance" was in full swing and still growing. In May 1975, "Treemonisha" was staged in a full opera production by the Houston Grand Opera. The company toured briefly, then settled into an eight-week run in New York on Broadway at the Palace Theatre in October and November. This appearance was directed by Gunther Schuller, and soprano Carmen Balthrop alternated with Kathleen Battle as the title character. An "original Broadway cast" recording was produced. Because of the lack of national exposure given to the brief Morehouse College staging of the opera in 1972, many Joplin scholars wrote that the Houston Grand Opera's 1975 show was the first full production.

1974 saw the Birmingham Royal Ballet under director Kenneth MacMillan create "Elite Syncopations", a ballet based on tunes by Joplin and other composers of the era. That year also brought the premiere by the Los Angeles Ballet of "Red Back Book", choreographed by John Clifford to Joplin rags from the collection of the same name, including both solo piano performances and arrangements for full orchestra.









</doc>
<doc id="29605" url="https://en.wikipedia.org/wiki?curid=29605" title="Syncopation">
Syncopation

In music, syncopation involves a variety of rhythms played together to make a piece of music, making part or all of a tune or piece of music off-beat. More simply, syncopation is "a disturbance or interruption of the regular flow of rhythm": a "placement of rhythmic stresses or accents where they wouldn't normally occur". It is the correlation of at least two sets of time intervals.

Syncopation is used in many musical styles, especially dance music: "All dance music makes use of syncopation, and it's often a vital element that helps tie the whole track together". In the form of a back beat, syncopation is used in virtually all contemporary popular music.

Syncopation can also occur when a strong harmony is placed on a weak beat, for instance, when a 7th-chord is placed on the second beat of measure or a dominant chord is placed at the fourth beat of a measure. The latter frequently occurs in tonal cadences in 18th- and early-19th-century music and is the usual conclusion of any section.

A hemiola can also be seen as one straight measure in three with one long chord and one short chord and a syncope in the measure thereafter, with one short chord and one long chord. Usually, the last chord in a hemiola is a (bi-)dominant, and as such a strong harmony on a weak beat, hence a syncope.

Technically, "syncopation occurs when a temporary displacement of the regular metrical accent occurs, causing the emphasis to shift from a strong accent to a weak accent". "Syncopation is", however, "very simply, a deliberate disruption of the two- or three-beat stress pattern, most often by stressing an off-beat, or a note that is not on the beat."

In the following example, there are two points of syncopation where the third beats are carried over (sustained) from the second beats. In the same way, the first beat of the second bar is carried over from the fourth beat of the first bar.

</score>

Though syncopation may be highly complex, dense or complex-looking rhythms often contain no syncopation. The following rhythm, though dense, stresses the regular downbeats, 1 and 4 (in ):

</score>

However, whether it's a placed rest or an accented note, any point in a piece of music that moves the listener's sense of the downbeat is a point of syncopation because it's shifting where the strong and weak accents are built.

The stress can shift by less than a whole beat, so it falls on an offbeat, as in the following example, where the stress in the first bar is shifted back by an eighth note (or quaver):

</score>

Whereas the notes are expected to fall "on" the beat:

</score>

Playing a note ever so slightly before, or after, a beat is another form of syncopation because this produces an unexpected accent:

</score>

It can be helpful to think of a rhythm in eighth notes and count it as "1-and-2-and-3-and-4-and". In general, emphasizing the "and" would be considered the off-beat.

Anticipated bass is a bass tone that comes syncopated shortly before the downbeat, which is used in Son montuno Cuban dance music. Timing can vary, but it usually falls on the 2+ and the 4 of the time, thus anticipating the third and first beats. This pattern is commonly known as the Afro-Cuban bass tumbao.

Richard Middleton suggests adding the concept of transformation to Narmour's prosodic rules which create rhythmic successions in order to explain or generate syncopations. "The syncopated pattern is heard 'with reference to', 'in light of', as a remapping of, its partner." He gives examples of various types of syncopation: Latin, backbeat, and before-the-beat. First however, one may listen to the audio example of stress on the "strong" beats, where expected: 

In the example below, the first two measures has an unsyncopated rhythm is shown in the first measure The third measure has a syncopated rhythm in which the first and fourth beat are provided as expected, but the accent unexpectedly lands in between the second and third beats, creating a familiar "Latin rhythm" known as tresillo.

</score>

The accent may be shifted from the first to the second beat in duple meter (and the third to fourth in quadruple), creating the backbeat rhythm:

</score>

Different crowds will "clap along" at concerts either on 1 and 3 or on 2 and 4, as above.

The phrasing of "Satisfaction" is a good example of syncopation. It is derived here from its theoretic unsyncopated form, a repeated trochee (¯ ˘ ¯ ˘). A backbeat transformation is applied to "I" and "can't", and then a before-the-beat transformation is applied to "can't" and "no".

This demonstrates how each syncopated pattern may be heard as a remapping, "with reference to" or "in light of", an unsyncopated pattern.

Syncopation has been an important element of European musical composition since at least the Middle Ages. Many Italian and French compositions of the music of the 14th-century Trecento make use of syncopation, as in of the following madrigal by Giovanni da Firenze. (See also hocket.)
The refrain "Deo Gratias" from the 15th-century anonymous English "Agincourt Carol" is also characterised by lively syncopation:
“The 15th-century carol repertory is one of the most substantial monuments of English medieval music... The early carols are rhythmically straightforward, in modern 6/8 time; later the basic rhythm is in 3/4, with many cross-rhythms... as in the famous Agincourt carol 'Deo gratias Anglia'. As in other music of the period, the emphasis is not on harmony, but on melody and rhythm.”

Composers of the musical High Renaissance Venetian School, such as Giovanni Gabrieli (1557–1612), exploited syncopation in both their secular madrigals and instrumental pieces and also in their choral sacred works, such as the motet "Domine, Dominus noster":

Denis Arnold (1979, p. 93) says: "the syncopations of this passage are of a kind which is almost a Gabrieli fingerprint, and they are typical of a general liveliness of rhythm common to Venetian music". The composer Igor Stravinsky (1959, p. 91), no stranger to syncopation himself, spoke of "those marvellous rhythmic inventions" that feature in Gabrieli's music.

J. S. Bach and Handel used syncopated rhythms as an inherent part of their compositions. One of the best-known examples of syncopation in music from the Baroque era was the "Hornpipe" from Handel’s "Water Music" (1733).

Christopher Hogwood (2005, p. 37) describes the Hornpipe as “possibly the most memorable movement in the collection, combining instrumental brilliance and rhythmic vitality… Woven amongst the running quavers are the insistent off-beat syncopations that symbolise confidence for Handel.” Bach's Brandenburg Concerto No. 4 features striking deviations from the established rhythmic norm in its first and third movements. According to Malcolm Boyd (1993, p. 53), each ritornello section of the first movement, "is clinched with an "Epilog" of syncopated antiphony":
Boyd (1993, p. 85) also hears the coda to the third movement as "remarkable… for the way the rhythm of the initial phrase of the fugue subject is expressed… with the accent thrown on to the second of the two minims (now staccato)":

Haydn, Mozart, Beethoven, and Schubert used syncopation to create variety especially in their symphonies. The opening movement of Beethoven's "Eroica" Symphony No. 3 exemplifies powerfully the uses of syncopation in a piece in triple time. After setting up a clear pattern of three beats to a bar at the outset, Beethoven disrupts it through syncopation in a number of ways:

(1) By displacing the rhythmic emphasis to a weak part of the beat, as in the first violin part in bars 7–9:

Taruskin (2010, p. 658) describes here how "the first violins, entering immediately after the C sharp, are made palpably to totter for two bars".

(2) By placing accents on normally weak beats, as in bars 25–26 and 28–35:
This "long sequence of syncopated sforzandi" recurs later during the development section of this movement, in a passage that Antony Hopkins (1981, p. 75) describes as "a rhythmic pattern that rides roughshod over the properties of a normal three-in-a bar".

(3) By inserting silences (rests) at points where a listener might expect strong beats, in the words of George Grove (1896, p. 61), "nine bars of discords given fortissimo on the weak beats of the bar":





</doc>
