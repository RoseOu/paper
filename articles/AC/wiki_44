<doc id="28916" url="https://en.wikipedia.org/wiki?curid=28916" title="Shetland">
Shetland

Shetland (, ), also called the Shetland Islands and formerly Zetland, is a subarctic archipelago in the Northern Isles of Scotland, situated in the Northern Atlantic, between Great Britain, the Faroe Islands and Norway.

The islands lie some to the northeast of Orkney, from the Scottish mainland and west of Norway. They form part of the division between the Atlantic Ocean to the west and the North Sea to the east. The total area is , and the population totalled 23,210 in 2011. The islands comprise the Shetland constituency of the Scottish Parliament. The local authority, Shetland Islands Council, is one of the 32 council areas of Scotland. The islands' administrative centre and only burgh is Lerwick, which has been the capital of Shetland since taking over from Scalloway in 1708.

The largest island, known as "Mainland", has an area of , making it the third-largest Scottish island and the fifth-largest of the British Isles. There are an additional 15 inhabited islands. The archipelago has an oceanic climate, a complex geology, a rugged coastline and many low, rolling hills.

Humans have lived in Shetland since the Mesolithic period. The early historic period was dominated by Scandinavian influences, especially from Norway. The islands became part of Scotland in the 15th century. When Scotland became part of the Kingdom of Great Britain in 1707, trade with northern Europe decreased. Fishing continues to be an important aspect of the economy up to the present day. The discovery of North Sea oil in the 1970s significantly boosted Shetland's economy, employment and public sector revenues.

The local way of life reflects the Scottish and Norse heritage of the isles, including the Up Helly Aa fire festival and a strong musical tradition, especially the traditional fiddle style. The islands have produced a variety of writers of prose and poetry, often in the distinct Shetland dialect of the Scots language. There are numerous areas set aside to protect the local fauna and flora, including a number of important sea bird nesting sites. The Shetland pony and Shetland Sheepdog are two well-known Shetland animal breeds. Other local breeds include the Shetland sheep, cow, goose, and duck. The Shetland pig, or grice, has been extinct since about 1930.

The islands' motto, which appears on the Council's coat of arms, is "". The Old Norse origin of this phrase is likely from the Norwegian provincial laws, such as the Frostathing Law. It is also mentioned in "Njáls saga", and means "By law shall land be built".

The name of Shetland is derived from the Old Norse words, ('hilt'), and ('land').

In AD 43 and 77 the Roman authors Pomponius Mela and Pliny the Elder referred to seven islands they respectively called and , both of which are assumed to be Shetland. Another possible early written reference to the islands is Tacitus' report in "Agricola" in AD 98. After describing the discovery and conquest of Orkney, he wrote that the Roman fleet had seen "Thule, too". In early Irish literature, Shetland is referred to as —"the Isles of Cats", which may have been the pre-Norse inhabitants' name for the islands. The Cat clan also occupied parts of the northern Scottish mainland (see Kingdom of Cat); and their name can be found in Caithness and in the Gaelic name for Sutherland (, meaning "among the Cats").

The oldest version of the modern name Shetland is , the Latinised adjectival form of the Old Norse name, recorded in a letter from Harald, Count of Shetland, in 1190, becoming "Hetland" in 1431 after various intermediate transformations. It is possible that the Pictish "cat" sound forms part of this Norse name. It then became in the 16th century.

As Norn was gradually replaced by Scots in the form of the Shetland dialect, became . The initial letter is the Middle Scots letter, "yogh", the pronunciation of which is almost identical to the original Norn sound, . When the use of the letter yogh was discontinued, it was often replaced by the similar-looking letter z (which at the time was usually rendered with a curled tail: ⟨ʒ⟩) hence , the form used in the name of the pre-1975 county council. This is also the source of the ZE postcode used for Shetland.

Most of the individual islands have Norse names, although the derivations of some are obscure and may represent pre-Norse, possibly Pictish or even pre-Celtic names or elements.

Shetland is around north of mainland Scotland, covers an area of and has a coastline long.

Lerwick, the capital and largest settlement, has a population of 6,958 and about half of the archipelago's total population of 23,167 people live within of the town.

Scalloway on the west coast, which was the capital until 1708, has a population of less than 1,000.

Only 16 of about 100 islands are inhabited. The main island of the group is known as Mainland. The next largest are Yell, Unst, and Fetlar, which lie to the north, and Bressay and Whalsay, which lie to the east. East and West Burra, Muckle Roe, Papa Stour, Trondra and Vaila are smaller islands to the west of Mainland. The other inhabited islands are Foula west of Walls, Fair Isle south-west of Sumburgh Head, and the Out Skerries to the east.

The uninhabited islands include Mousa, known for the Broch of Mousa, the finest preserved example in Scotland of an Iron Age broch; Noss to the east of Bressay, which has been a national nature reserve since 1955; St Ninian's Isle, connected to Mainland by the largest active tombolo in the UK; and Out Stack, the northernmost point of the British Isles. Shetland's location means that it provides a number of such records: Muness is the most northerly castle in the United Kingdom and Skaw the most northerly settlement.

The geology of Shetland is complex, with numerous faults and fold axes. These islands are the northern outpost of the Caledonian orogeny, and there are outcrops of Lewisian, Dalradian and Moine metamorphic rocks with histories similar to their equivalents on the Scottish mainland. There are also Old Red Sandstone deposits and granite intrusions. The most distinctive feature is the ophiolite in Unst and Fetlar which is a remnant of the Iapetus Ocean floor made up of ultrabasic peridotite and gabbro.

Much of Shetland's economy depends on the oil-bearing sediments in the surrounding seas. Geological evidence shows that in around 6100 BC a tsunami caused by the Storegga Slides hit Shetland, as well as the rest of the east coast of Scotland, and may have created a wave of up to high in the voes where modern populations are highest.

The highest point of Shetland is Ronas Hill at . The Pleistocene glaciations entirely covered the islands. During that period, the Stanes of Stofast, a 2000-tonne glacial erratic, came to rest on a prominent hilltop in Lunnasting.

Shetland has a national scenic area which, unusually, includes a number of discrete locations: Fair Isle, Foula, South West Mainland (including the Scalloway Islands), Muckle Roe, Esha Ness, Fethaland and Herma Ness. The total area covered by the designation is 41,833 ha, of which 26,347 ha is marine (i.e. below low tide).

In October 2018, legislation came into force in Scotland to prevent public bodies, without good reason, showing Shetland in a separate box in maps, as had often been the practice. The legislation requires the islands to be "displayed in a manner that accurately and proportionately represents their geographical location in relation to the rest of Scotland", so as make clear the islands' real distance from other areas.

Shetland has an oceanic temperate maritime climate (Köppen: "Cfb"), bordering on, but very slightly above average in summer temperatures, the subpolar variety, with long but cool winters and short mild summers. The climate all year round is moderate owing to the influence of the surrounding seas, with average night-time low temperatures a little above in January and February and average daytime high temperatures of near in July and August. The highest temperature on record was on the 6th of August 1910 and the lowest in the Januaries of 1952 and 1959. The frost-free period may be as little as three months. In contrast, inland areas of nearby Scandinavia on similar latitudes experience significantly larger temperature differences between summer and winter, with the average highs of regular July days comparable to Lerwick's all-time record heat that is around , further demonstrating the moderating effect of the Atlantic Ocean. In contrast, winters are considerably milder than those expected in nearby continental areas, even comparable to winter temperatures of many parts of England and Wales much further south.

The general character of the climate is windy and cloudy with at least of rain falling on more than 250 days a year. Average yearly precipitation is , with November and December the wettest months. Snowfall is usually confined to the period November to February, and snow seldom lies on the ground for more than a day. Less rain falls from April to August although no month receives less than . Fog is common during summer due to the cooling effect of the sea on mild southerly airflows.

Because of the islands' latitude, on clear winter nights the "northern lights" can sometimes be seen in the sky, while in summer there is almost perpetual daylight, a state of affairs known locally as the "simmer dim". Annual bright sunshine averages 1110 hours, and overcast days are common.

Due to the practice, dating to at least the early Neolithic, of building in stone on virtually treeless islands, Shetland is extremely rich in physical remains of the prehistoric eras and there are over 5,000 archaeological sites all told. A midden site at West Voe on the south coast of Mainland, dated to 4320–4030 BC, has provided the first evidence of Mesolithic human activity in Shetland. The same site provides dates for early Neolithic activity and finds at Scord of Brouster in Walls have been dated to 3400 BC. "Shetland knives" are stone tools that date from this period made from felsite from Northmavine.

Pottery shards found at the important site of Jarlshof also indicate that there was Neolithic activity there although the main settlement dates from the Bronze Age. This includes a smithy, a cluster of wheelhouses and a later broch. The site has provided evidence of habitation during various phases right up until Viking times. Heel-shaped cairns, are a style of chambered cairn unique to Shetland, with a particularly large example in Vementry.

Numerous brochs were erected during the Iron Age. In addition to Mousa there are significant ruins at Clickimin, Culswick, Old Scatness and West Burrafirth, although their origin and purpose is a matter of some controversy. The later Iron Age inhabitants of the Northern Isles were probably Pictish, although the historical record is sparse. Hunter (2000) states in relation to King Bridei I of the Picts in the sixth century AD: "As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.” In 2011, the collective site, "The Crucible of Iron Age Shetland", including Broch of Mousa, Old Scatness and Jarlshof, joined the UKs "Tentative List" of World Heritage Sites.

The expanding population of Scandinavia led to a shortage of available resources and arable land there and led to a period of Viking expansion, the Norse gradually shifting their attention from plundering to invasion. Shetland was colonised during the late 8th and 9th centuries, the fate of the existing indigenous population being uncertain. Modern Shetlanders have almost identical proportions of Scandinavian matrilineal and patrilineal genetic ancestry, suggesting that the islands were settled by both men and women in equal measure.

Vikings then used the islands as a base for pirate expeditions to Norway and the coasts of mainland Scotland. In response, Norwegian king Harald Hårfagre ("Harald Fair Hair") annexed the Northern Isles (comprising Orkney and Shetland) in 875. Rognvald Eysteinsson received Orkney and Shetland from Harald as an earldom as reparation for the death of his son in battle in Scotland, and then passed the earldom on to his brother Sigurd the Mighty.

The islands converted to Christianity in the late 10th century. King Olav Tryggvasson summoned the "jarl" Sigurd the Stout during a visit to Orkney and said, "I order you and all your subjects to be baptised. If you refuse, I'll have you killed on the spot and I swear I will ravage every island with fire and steel." Unsurprisingly, Sigurd agreed and the islands became Christian at a stroke. Unusually, from c. 1100 onwards the Norse "jarls" owed allegiance both to Norway and to the Scottish crown through their holdings as Earls of Caithness.

In 1194, when Harald Maddadsson was Earl of Orkney and Shetland, a rebellion broke out against King Sverre Sigurdsson of Norway. The ("Island Beardies") sailed for Norway but were beaten in the Battle of Florvåg near Bergen. After his victory King Sverre placed Shetland under direct Norwegian rule, a state of affairs that continued for nearly two centuries.

From the mid-13th century onwards Scottish monarchs increasingly sought to take control of the islands surrounding the mainland. The process was begun in earnest by Alexander II and was continued by his successor Alexander III. This strategy eventually led to an invasion of Scotland by Haakon Haakonsson, King of Norway. His fleet assembled in Bressay Sound before sailing for Scotland. After the stalemate of the Battle of Largs, Haakon retreated to Orkney, where he died in December 1263, entertained on his deathbed by recitations of the sagas. His death halted any further Norwegian expansion in Scotland and following this ill-fated expedition, the Hebrides and Mann were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth, although the Scots recognised continuing Norwegian sovereignty over Orkney and Shetland.

In the 14th century, Orkney and Shetland remained a Norwegian possession, but Scottish influence was growing. Jon Haraldsson, who was murdered in Thurso in 1231, was the last of an unbroken line of Norse jarls, and thereafter the earls were Scots noblemen of the houses of Angus and St Clair. On the death of Haakon VI in 1380, Norway formed a political union with Denmark, after which the interest of the royal house in the islands declined. In 1469, Shetland was pledged by Christian I, in his capacity as King of Norway, as security against the payment of the dowry of his daughter Margaret, betrothed to James III of Scotland. As the money was never paid, the connection with the Crown of Scotland became permanent. In 1470, William Sinclair, 1st Earl of Caithness ceded his title to James III, and the following year the Northern Isles were directly absorbed to the Crown of Scotland, an action confirmed by the Parliament of Scotland in 1472. Nonetheless, Shetland's connection with Norway has proved to be enduring.

From the early 15th century onward Shetlanders sold their goods through the Hanseatic League of German merchantmen. The Hansa would buy shiploads of salted fish, wool and butter, and import salt, cloth, beer and other goods. The late 16th century and early 17th century were dominated by the influence of the despotic Robert Stewart, Earl of Orkney, who was granted the islands by his half-sister Mary Queen of Scots, and his son Patrick. The latter commenced the building of Scalloway Castle, but after his imprisonment in 1609 the Crown annexed Orkney and Shetland again until 1643 when Charles I granted them to William Douglas, 7th Earl of Morton. These rights were held on and off by the Mortons until 1766, when they were sold by James Douglas, 14th Earl of Morton to Laurence Dundas.

The trade with the North German towns lasted until the 1707 Act of Union, when high salt duties prevented the German merchants from trading with Shetland. Shetland then went into an economic depression, as the local traders were not as skilled in trading salted fish. However, some local merchant-lairds took up where the German merchants had left off, and fitted out their own ships to export fish from Shetland to the Continent. For the independent farmers of Shetland this had negative consequences, as they now had to fish for these merchant-lairds.

Smallpox afflicted the islands in the 17th and 18th centuries (as it did all of Europe), but as vaccines became available after 1800, health improved. The islands were very badly hit by the potato famine of 1846 and the government introduced a Relief Plan for the islands under the command of Captain Robert Craigie of the Royal Navy who stayed in Lerwick to oversee the project 1847-1852. During this period Craigie also did much to improve and increase roads in the islands.

Population increased to a maximum of 31,670 in 1861. However, British rule came at price for many ordinary people as well as traders. The Shetlanders' nautical skills were sought by the Royal Navy. Some 3,000 served during the Napoleonic wars from 1800 to 1815 and press gangs were rife. During this period 120 men were taken from Fetlar alone, and only 20 of them returned home. By the late 19th century 90% of all Shetland was owned by just 32 people, and between 1861 and 1881 more than 8,000 Shetlanders emigrated. With the passing of the Crofters' Act in 1886 the Liberal prime minister William Gladstone emancipated crofters from the rule of the landlords. The Act enabled those who had effectively been landowners' serfs to become owner-occupiers of their own small farms. By this time fishermen from Holland, who had traditionally gathered each year off the coast of Shetland to fish for herring, triggered an industry in the islands that boomed from around 1880 until the 1920s when stocks of the fish began to dwindle. The production peaked in 1905 at more than a million barrels, of which 708,000 were exported.

During World War I many Shetlanders served in the Gordon Highlanders, a further 3,000 served in the Merchant Navy, and more than 1,500 in a special local naval reserve. The 10th Cruiser Squadron was stationed at Swarbacks Minn (the stretch of water to the south of Muckle Roe), and during a single year from March 1917 more than 4,500 ships sailed from Lerwick as part of an escorted convoy system. In total, Shetland lost more than 500 men, a higher proportion than any other part of Britain, and there were further waves of emigration in the 1920s and 1930s.

During World War II a Norwegian naval unit nicknamed the "Shetland Bus" was established by the Special Operations Executive in the autumn of 1940 with a base first at Lunna and later in Scalloway to conduct operations around the coast of Norway. About 30 fishing vessels used by Norwegian refugees were gathered and the Shetland Bus conducted covert operations, carrying intelligence agents, refugees, instructors for the resistance, and military supplies. It made over 200 trips across the sea, and Leif Larsen, the most highly decorated allied naval officer of the war, made 52 of them. Several RAF airfields and sites were also established at Sullom Voe and several lighthouses suffered enemy air attacks.

Oil reserves discovered in the later 20th century in the seas both east and west of Shetland have provided a much-needed alternative source of income for the islands. The East Shetland Basin is one of Europe's largest oil fields and as a result of the oil revenue and the cultural links with Norway, a small Home Rule movement developed briefly to recast the constitutional position of Shetland. It saw as its models the Isle of Man, as well as Shetland's closest neighbour, the Faroe Islands, an autonomous dependency of Denmark.

The population stood at 17,814 in 1961.

Today, the main revenue producers in Shetland are agriculture, aquaculture, fishing, renewable energy, the petroleum industry (crude oil and natural gas production), the creative industries and tourism.

Fishing remains central to the islands' economy today, with the total catch being in 2009, valued at over £73.2 million. Mackerel makes up more than half of the catch in Shetland by weight and value, and there are significant landings of haddock, cod, herring, whiting, monkfish and shellfish.

Oil and gas were first landed in 1978 at Sullom Voe, which has subsequently become one of the largest terminals in Europe. Taxes from the oil have increased public sector spending on social welfare, art, sport, environmental measures and financial development. Three quarters of the islands' workforce is employed in the service sector, and the Shetland Islands Council alone accounted for 27.9% of output in 2003. Shetland's access to oil revenues has funded the Shetland Charitable Trust, which in turn funds a wide variety of local programmes. The balance of the fund in 2011 was £217 million, i.e., about £9,500 per head.

In January 2007, the Shetland Islands Council signed a partnership agreement with Scottish and Southern Energy for the Viking Wind Farm, a 200-turbine wind farm and subsea cable. This renewable energy project would produce about 600 megawatts and contribute about £20 million to the Shetland economy per year. The plan met with significant opposition within the islands, primarily resulting from the anticipated visual impact of the development. The PURE project in Unst is a research centre which uses a combination of wind power and fuel cells to create a wind hydrogen system. The project is run by the Unst Partnership, the local community's development trust.

Farming is mostly concerned with the raising of Shetland sheep, known for their unusually fine wool.

Knitwear is important both to the economy and culture of Shetland, and the Fair Isle design is well known. However, the industry faces challenges due to plagiarism of the word "Shetland" by manufacturers operating elsewhere, and a certification trademark, "The Shetland Lady", has been registered.

Crofting, the farming of small plots of land on a legally restricted tenancy basis, is still practised and is viewed as a key Shetland tradition as well as an important source of income. Crops raised include oats and barley; however, the cold, windswept islands make for a harsh environment for most plants.

Shetland is served by a weekly local newspaper, "The Shetland Times" and the online "Shetland News" with radio service being provided by BBC Radio Shetland and the commercial radio station SIBC.

Shetland is a popular destination for cruise ships, and in 2010 the Lonely Planet guide named Shetland as the sixth best region in the world for tourists seeking unspoilt destinations. The islands were described as "beautiful and rewarding" and the Shetlanders as "a fiercely independent and self-reliant bunch". Overall visitor expenditure was worth £16.4 million in 2006, in which year just under 26,000 cruise liner passengers arrived at Lerwick Harbour. This business has grown substantially with 109 cruise ships already booked in for 2019, representing over 107,000 passenger visits. In 2009, the most popular visitor attractions were the Shetland Museum, the RSPB reserve at Sumburgh Head, Bonhoga Gallery at Weisdale Mill and Jarlshof. Geopark Shetland (now Shetland UNESCO Global Geopark) was established by the Amenity Trust in 2009 to boost sustainable tourism to the islands.


Transport between islands is primarily by ferry, and Shetland Islands Council operates various inter-island services. Shetland is also served by a domestic connection from Lerwick to Aberdeen on mainland Scotland. This service, which takes about 12 hours, is operated by NorthLink Ferries. Some services also call at Kirkwall, Orkney, which increases the journey time between Aberdeen and Lerwick by 2 hours. There are plans for road tunnels to some of the islands, especially Bressay and Whalsay; however, it is hard to convince the mainland government to finance them.

Sumburgh Airport, the main airport in Shetland, is located close to Sumburgh Head, south of Lerwick. Loganair operates flights to other parts of Scotland up to ten times a day, the destinations being Kirkwall, Aberdeen, Inverness, Glasgow and Edinburgh. Lerwick/Tingwall Airport is located west of Lerwick. Operated by Directflight Limited in partnership with Shetland Islands Council, it is devoted to inter-island flights from the Shetland Mainland to most of the inhabited islands.

Scatsta Airport near Sullom Voe allows frequent charter flights from Aberdeen to transport oilfield workers and this small terminal has the fifth largest number of international passengers in Scotland.

Public bus services are operated in Mainland, Whalsay, Burra, Unst and Yell.

The archipelago is exposed to wind and tide, and there are numerous sites of wrecked ships. Lighthouses are sited as an aid to navigation at various locations.

The Shetland Islands Council is the Local Government authority for all the islands and is based in Lerwick Town Hall.

Shetland is sub-divided into 18 community council areas and into 12 civil parishes that are used for statistical purposes.

In Shetland there are two high schools—Anderson and Brae—five junior high schools, and 24 primary schools.

In 2014 there were plans to close other junior high schools and require boarding at Anderson.

Shetland is also home to the North Atlantic Fisheries College, the Centre for Nordic Studies and Shetland College, which are all associated with the University of the Highlands and Islands.
The islands are represented by the Shetland football team who regularly compete in the Island Games. The islands' senior football league is the G&S Flooring Premier League.

The Reformation reached the archipelago in 1560. This was an apparently peaceful transition and there is little evidence of religious intolerance in Shetland's recorded history.

In the 2011 census, Shetland registered a higher proportion of people with no religion than the Scottish average. Nevertheless, a variety of religious denominations are represented in the islands.

The Methodist Church has a relatively high membership in Shetland, which is a District of the Methodist Church (with the rest of Scotland comprising a separate District).

The Church of Scotland has a Presbytery of Shetland that includes St. Columba's Church in Lerwick.

The Catholic population is served by the church of St. Margaret and the Sacred Heart in Lerwick. The Parish is part of the Diocese of Aberdeen.

The Scottish Episcopal Church (part of the Anglican Communion) has regular worship at St Magnus' Church, Lerwick, St Colman's Church, Burravoe, and the Chapel of Christ the Encompasser, Fetlar, the last of which is maintained by the Society of Our Lady of the Isles, the most northerly and remote Anglican religious order of nuns.

The Church of Jesus Christ of Latter-day Saints has a congregation in Lerwick. The former print works and offices of the local newspaper, The Shetland Times, has been converted into a chapel.

Shetland is represented in the House of Commons as part of the Orkney and Shetland constituency, which elects one Member of Parliament. Since 2001, the MP has been Alistair Carmichael. This seat has been held by the Liberal Democrats or their predecessors the Liberal Party since 1950, longer than any other seat in the UK.

In the Scottish Parliament the Shetland constituency elects one Member of the Scottish Parliament (MSP) by the first past the post system. Tavish Scott of the Scottish Liberal Democrats had held the seat since the creation of the Scottish Parliament in 1999. Beatrice Wishart MSP, also of the Scottish Liberal Democrats, was elected to replace Tavish Scott in August 2019. Shetland is within the Highlands and Islands electoral region.

The political composition of the Shetland Islands Council is 21 Independents and 1 Scottish National Party.

The Wir Shetland movement was set up in 2015 to campaign for greater autonomy. As of early 2018, however, the movement appears to be inactive.

Roy Grönneberg, who founded the local chapter of the Scottish National Party in 1966, designed the flag of Shetland in cooperation with Bill Adams to mark the 500th anniversary of the transfer of the islands from Norway to Scotland. The colours are identical to those of the flag of Scotland, but are shaped in the Nordic cross. After several unsuccessful attempts, including a plebiscite in 1985, the Lord Lyon King of Arms approved it as the official flag of Shetland in 2005.

After the islands were officially transferred from Norway to Scotland in 1472, several Scots families from the Scottish Lowlands emigrated to Shetland in the 16th and 17th centuries. Studies of the genetic makeup of the islands' population, however, indicate that Shetlanders are just under half Scandinavian in origin, and sizeable amounts of Scandinavian ancestry, both patrilineal and matrilineal, have been reported in Orkney (55%) and Shetland (68%). This combination is reflected in many aspects of local life. For example, almost every place name in use can be traced back to the Vikings. The Lerwick Up Helly Aa is one of several fire festivals held in Shetland annually in the middle of winter, starting on the last Tuesday of January. The festival is just over 100 years old in its present, highly organised form. Originally held to break up the long nights of winter and mark the end of Yule, the festival has become one celebrating the isles' heritage and includes a procession of men dressed as Vikings and the burning of a replica longship. 
Shetland also competes in the biennial International Island Games, which it hosted in 2005.

The cuisine of Shetland is based on locally produced lamb, beef and seafood, much of it organic. Inevitably, the real ale-producing Valhalla Brewery is the most northerly in Britain. The Shetland Black is a variety of blue potato with a dark skin and indigo-coloured flesh markings.

The Norn language was a form of Old Norse spoken in the Northern Isles, and continued to be spoken until the 18th century. It was gradually replaced in Shetland by an insular dialect of Scots, known as Shetlandic, which is in turn being replaced in some areas by Scottish English. Although Norn was spoken for hundreds of years, it is now extinct and few written sources remain, although influences remain in the Insular Scots dialects. Shetlandic is used in local radio and dialect writing, and is kept alive by organisations such as Shetland Forwirds, Isle Folk, and the Shetland Folk Society.

Shetland's culture and landscapes have inspired a variety of musicians, writers and film-makers. The Forty Fiddlers was formed in the 1950s to promote the traditional fiddle style, which is a vibrant part of local culture today. Notable exponents of Shetland folk music include Aly Bain, Fiddlers' Bid, and the late Tom Anderson and Peerie Willie Johnson. Thomas Fraser was a country musician who never released a commercial recording during his life, but whose work has become popular more than 20 years after his death in 1978.

The annual Shetland Folk Festival began in 1981 and is hosted on the first weekend of May.

Walter Scott's 1822 novel "The Pirate" is set in "a remote part of Shetland", and was inspired by his 1814 visit to the islands. The name "Jarlshof" meaning "Earl's Mansion" is a coinage of his. Robert Cowie, a doctor born in Lerwick published the 1874 work 

Hugh MacDiarmid, the Scots poet and writer, lived in Whalsay from the mid-1930s through 1942, and wrote many poems there, including a number that directly address or reflect the Shetland environment, such as "On A Raised Beach", which was inspired by a visit to West Linga. The 1975 novel "North Star" by Hammond Innes is largely set in Shetland and Raman Mundair's 2007 book of poetry "A Choreographer's Cartography" offers a British Asian perspective on the landscape. The "Shetland Quartet" by Ann Cleeves, who previously lived in Fair Isle, is a series of crime novels set around the islands. In 2013 her novel "Red Bones" became the basis of BBC crime drama television series "Shetland".

Vagaland, who grew up in Walls, was arguably Shetland's finest poet of the 20th century. Haldane Burgess was a Shetland historian, poet, novelist, violinist, linguist and socialist, and Rhoda Bulter (1929–94) is one of the best-known Shetland poets of recent times. Other 20th- and 21st-century poets and novelists include Christine De Luca, Robert Alan Jamieson who grew up in Sandness, the late Lollie Graham of Veensgarth, Stella Sutherland of Bressay, the late William J Tait from Yell and Laureen Johnson.

There are two monthly magazines in production: "Shetland Life" and "i'i' Shetland". The quarterly "The New Shetlander", founded in 1947, is said to be Scotland's longest-running literary magazine. For much of the later 20th century it was the major vehicle for the work of local writers — and others, including early work by George Mackay Brown.

Michael Powell made "The Edge of the World" in 1937, a dramatisation based on the true story of the evacuation of the last 36 inhabitants of the remote island of St Kilda on 29 August 1930. St Kilda lies in the Atlantic Ocean, west of the Outer Hebrides but Powell was unable to get permission to film there. Undaunted, he made the film over four months during the summer of 1936 in Foula and the film transposes these events to Shetland. Forty years later, the documentary "Return to the Edge of the World" was filmed, capturing a reunion of cast and crew of the film as they revisited the island in 1978.

A number of other films have been made on or about Shetland including "A Crofter's Life in Shetland" (1932) "A Shetland Lyric" (1934), "Devil's Gate" (2003) and "It's Nice Up North" (2006), a comedy documentary by Graham Fellows. The Screenplay film festival takes place annually in Mareel, a cinema, music and education venue.

The BBC One television series "Shetland", a crime drama, is set in the islands and is based on the book series by Ann Cleeves. The programme is filmed partly in Shetland and partly on the Scottish mainland.

Shetland has three national nature reserves, at the seabird colonies of Hermaness and Noss, and at Keen of Hamar to preserve the serpentine flora. There are a further 81 SSSIs, which cover 66% or more of the land surfaces of Fair Isle, Papa Stour, Fetlar, Noss and Foula. Mainland has 45 separate sites.

The landscape in Shetland is marked by the grazing of sheep and the harsh conditions have limited the total number of plant species to about 400. Native trees such as rowan and crab apple are only found in a few isolated places such as cliffs and loch islands. The flora is dominated by Arctic-alpine plants, wild flowers, moss and lichen. Spring squill, buck's-horn plantain, Scots lovage, roseroot and sea campion are abundant, especially in sheltered places. Shetland mouse-ear ("Cerastium nigrescens") is an endemic flowering plant found only in Shetland. It was first recorded in 1837 by botanist Thomas Edmondston. Although reported from two other sites in the nineteenth century, it currently grows only on two serpentine hills in the island of Unst. The nationally scarce oysterplant is found in several islands and the British Red Listed bryophyte "Thamnobryum alopecurum" has also been recorded. Listed marine algae include: "Polysiphonia fibrillosa" (Dillwyn) Sprengel, "Polysiphonia atlantica" Kapraun and J.Norris, "Polysiphonia brodiaei" (Dillwyn) Sprengel, "Polysiphonia elongata" (Hudson) Sprengel, "Polysiphonia elongella". Harvey The Shetland Monkeyflower is unique to Shetland and is a mutation of the Monkeyflower "(mimulus guttatus") introduced to Shetland in the 19th century.

Shetland has numerous seabird colonies. Birds found in the islands include Atlantic puffin, storm-petrel, red-throated diver, northern gannet and great skua (locally called the "bonxie"). Numerous rarities have also been recorded including black-browed albatross and snow goose, and a single pair of snowy owls bred in Fetlar from 1967 to 1975. The Shetland wren, Fair Isle wren and Shetland starling are subspecies endemic to Shetland. There are also populations of various moorland birds such as curlew, snipe and golden plover.

One of the early ornithologists that wrote about the wealth of birdlife in Shetland was Edmund Selous (1857-1934) in his book "The Bird Watcher in the Shetlands" (1905). He writes extensively about the gulls and terns, about the lesser or arctic skuas, the black guillemots and many other birds (and the seals) of the islands.

The geographical isolation and recent glacial history of Shetland have resulted in a depleted mammalian fauna and the brown rat and house mouse are two of only three species of rodent present in the islands. The Shetland field mouse is the third and the archipelago's fourth endemic subspecies, of which there are three varieties in Yell, Foula and Fair Isle. They are variants of "Apodemus sylvaticus" and archaeological evidence suggests that this species was present during the Middle Iron Age (around 200 BC to AD 400). It is possible that "Apodemus" was introduced from Orkney where a population has existed since at the least the Bronze Age.

There is a variety of indigenous breeds, of which the diminutive Shetland pony is probably the best known, as well as being an important part of the Shetland farming tradition. The first written record of the pony was in 1603 in the Court Books of Shetland and, for its size, it is the strongest of all the horse breeds. Others are the Shetland Sheepdog or "Sheltie", the endangered Shetland cattle and Shetland Goose and the Shetland sheep which is believed to have originated prior to 1000 AD. The Grice was a breed of semi-domesticated pig that had a habit of attacking lambs. It became extinct sometime between the middle of the nineteenth century and the 1930s.







</doc>
<doc id="28917" url="https://en.wikipedia.org/wiki?curid=28917" title="Soay, Inner Hebrides">
Soay, Inner Hebrides

Soay (, ) is an island just off the coast of Skye, in the Inner Hebrides of Scotland.

Soay lies to the west of Loch Scavaig on the south-west coast of Skye, from which it is separated by Soay Sound. Unlike its neighbours Skye and Rùm, Soay is low-lying, reaching at Beinn Bhreac. The dumb-bell shaped island is virtually cut in half by inlets that form Soay Harbour (N) and the main bay, Camas nan Gall (to the S). The main settlement, Mol-chlach, is on the shore of Camas nan Gall. It is normally reached by boat from Elgol. The island is part of the Cuillin Hills National Scenic Area, one of 40 in Scotland.

The name derives from Old Norse "Sauða-ey" meaning "Sheep Island". Camas nan Gall (G: "Bay of Foreigners") is probably named after the Norse invaders, after whom the Hebrides ("Na h-Innse Gall") are also named.

The population peaked at 158 in 1851, following eviction of crofters from Skye in the Highland Clearances.

In 1946, author Gavin Maxwell bought the island and established a factory to process shark oil from basking sharks. The enterprise was unsuccessful, lasting just three years. Maxwell wrote about it in his book "Harpoon at a Venture". After the failure of the business the island was sold on to Maxwell's business partner, Tex Geddes. The island had the first solar-powered telephone exchange in the world.

Previously mainly Scottish Gaelic-speaking, most of the population was evacuated to Mull on 20 June 1953, since when the island has been sparsely populated. In 2001 the population was 7. By 2003 this had dwindled to 2 and the usually resident population in 2011 were three people.

Local stamps were issued for Soay between 1965 and 1967, all on the Europa theme, some being overprinted to commemorate Sir Winston Churchill. As the stamps were produced without the owner's permission, they are regarded as bogus.




</doc>
<doc id="28918" url="https://en.wikipedia.org/wiki?curid=28918" title="Storytelling game">
Storytelling game

A storytelling game is a game where two or more persons collaborate on telling a spontaneous story. Usually, each player takes care of one or more characters in the developing story. Some games in the tradition of role-playing games require one participant to take the roles of the various supporting characters, as well as introducing non-character forces (for example, a flood), but other systems dispense with this figure and distribute this function among all players.

Since this person usually sets the ground and setting for the story, he or she is often referred to as the "storyteller" (often contracted to "ST") or "narrator". Any number of other alternate forms may be used, many of which are variations on the term "gamemaster"; these variants are especially common in storytelling games derived from or similar to role-playing games.

In contrast to improv theater, storytelling gamers describe the actions of their characters rather than acting them out, except during dialogue or, in some games, monologue. However, "live action" versions exist, which are very much akin to theater except in the crucial absence of a non-participating audience.

The most popular modern storytelling games originated as a subgenre of role-playing games, where the game rules and statistics are heavily de-emphasised in favor of creating a believable story and immersive experience for all involved. So while in a conventional game the announcement that one's character is going to leap over a seven-meters-wide canyon will be greeted with the request to roll a number of dice, a player in a storytelling game who wishes to have a character perform a similar feat will have to convince the others (especially the storyteller) why it is both probable and keeping within the established traits of their character to successfully do so. As such, these games are a subclass of diceless role-playing games.

Not all players find the storytelling style of role-playing satisfying. Many role-playing gamers are more comfortable in a system that gives them less freedom, but where they do not need to police themselves; others find it easier to enjoy a system where a more concrete framework of rules is already present. These three types of player are discussed by the GNS theory.

Some role-playing game systems which describe themselves as "storytelling games" nevertheless use randomisers rather than story in the arbitration of the rules, often in the form of a contest of Rock, Paper, Scissors or a card drawn from a deck of cards. Such "storytelling" games are instead simplified or streamlined forms of traditional role-playing games. Conversely, most modern role-playing games encourage gamemasters to ignore their gaming systems if it makes for a more enjoyable story, even though they may not describe themselves as "storytelling" games.

A growing number of websites utilize a bulletin board system, in which the gaming is akin to Collaborative Fiction but known as a "Literary Role-Playing Game". The players contribute to an ongoing story with defined parameters but no narrator or directing force. A 'moderator' may oversee the gamers to ensure that the rules, guidelines and parameters of the gaming "world" are being upheld, but otherwise the writers are free to interact as players in an improvisational play. Many of these "Literary RPGs" are fan-fiction based, such as (most prevalently) Tolkien's Middle-earth, Star Wars, Harry Potter, Twilight, any number of anime and manga sources, or they are simply based in thematic worlds such as the mythologies of Ancient Greece, fairy tales, the Renaissance or science fiction. Most often referred to as "Literary RPGs" and place a greater emphasis on writing skill and storytelling ability than on any sense of competition driven outcome.

White Wolf Game Studio's Storyteller System, which is used in World of Darkness role-playing games such as "" and live-action games under the Mind's Eye Theatre imprint, is the best-known and most popular role-playing game described as a "storytelling game".

An early design of a collaborative storytelling game not based in simulation was created by Chris Engle c. 1988 with his "Matrix Game". In this system, a referee decides the likeliness of the facts proposed by the players, and those facts happen or are rejected according with a dice roll. Players can propose counter-arguments that are resolved in a dice rolling contest. A conflict round can follow to resolve any inconsistencies or further detail new plot points. Matrix Games are now presented in a board game format.

In 1999, game designer Ian Millington developed an early work called "Ergo" which established the basis for collaborative role-playing. It was designed with the rules of the Fudge universal role-playing system in mind but added modifications necessary to get rid of the need for a gamemaster, distributing the responsibility for the game and story equally among all players and undoing the equivalence between player and character.

Modern rule systems (such as the coin system in Universalis) rely less on randomness and more in collaboration between players. This includes rules based on economic systems that force players to negotiate the details of the story, and solve conflicts based on the importance that they give to a given plot element and the resources they're willing to spend to make it into the story.

Collaborative fiction is a form of storytelling which uses collaborative writing as the primary medium, where a group of authors share creative control of a story. Collaborative fiction can occur for commercial gain, as part of education, or recreationally – many collaboratively written works have been the subject of a large degree of academic research.



</doc>
<doc id="28922" url="https://en.wikipedia.org/wiki?curid=28922" title="Scorpion">
Scorpion

Scorpions are predatory arachnids of the order Scorpiones. They have eight legs and are easily recognized by the pair of grasping pedipalps and the narrow, segmented tail, often carried in a characteristic forward curve over the back, ending with a venomous stinger. Scorpions range in size from 9 mm / 0.3 in. ("Typhlochactas mitchelli") to 23 cm / 9 in. ("Heterometrus swammerdami").

The evolutionary history of scorpions goes back to the Silurian period 435 million years ago. They have adapted to a wide range of environmental conditions, and they can now be found on all continents except Antarctica. Scorpions number about 1,750 described species, with 13 extant (living) families recognised to date. The taxonomy has undergone changes and is likely to change further, as genetic studies are bringing forth new information.

All scorpions have a venomous sting, but the vast majority of the species do not represent a serious threat to humans, and in most cases, healthy adults do not need any medical treatment after being stung. Only about 25 species are known to have venom capable of killing a human. In some parts of the world with highly venomous species, human fatalities regularly occur, primarily in areas with limited access to medical treatment.

The word "scorpion" is thought to have originated in Middle English between 1175 and 1225 AD from Old French ', or from Italian ', both derived from the Latin ', which is the romanization of the Greek word  – '.

Scorpions are found on all major land masses except Antarctica and New Zealand. Scorpions did not occur naturally in Great Britain, Ireland, Japan, South Korea, and some of the islands in Oceania, but now have been accidentally introduced in some of these places by human trade and commerce. The greatest diversity of scorpions in the Northern Hemisphere is to be found in regions between the latitudes 23 and 38°N. Above these latitudes, the diversity decreases with the northernmost natural occurrence of scorpions being the northern scorpion "Paruroctonus boreus" at Medicine Hat, Alberta, Canada 50°N. Five colonies of scorpions ("Euscorpius flavicaudis") have established themselves in Sheerness on the Isle of Sheppey in the United Kingdom. This small population has been resident since the 1860s, having probably arrived with imported fruit from Africa. This scorpion species is small and completely harmless to humans. At just over 51°N, this marks the northernmost limit where scorpions live in the wild.

Today, scorpions are found in virtually every terrestrial habitat including: high-elevation mountains, caves, and intertidal zones, with the exception of boreal ecosystems such as: the tundra, high-altitude taiga, and the permanently snow-clad tops of some mountains. As regards to microhabitats, scorpions may be ground-dwelling, tree-living, rock-loving or sand-loving. Some species, such as "Vaejovis janssi", are versatile and are found in every type of habitat in Baja California, while others occupy specialized niches such as "Euscorpius carpathicus", which is endemic to the littoral zone of rivers in Romania.

Thirteen families and about 1,750 described species and subspecies of scorpions are known. In addition, 111 described taxa of scorpions are extinct.

This classification is based on that of Soleglad and Fet (2003), which replaced the older, unpublished classification of Stockwell. Additional taxonomic changes are from papers by Soleglad et al. (2005).

This classification covers extant taxa to the rank of family:


Scorpion remains have been found in many fossil records, including marine Silurian and estuarine Devonian deposits, coal deposits from the Carboniferous Period and in amber. The oldest known scorpion, "Parioscorpio" lived around 435 million years ago in the Silurian period. Though once believed to have lived on the bottom of shallow tropical seas, early scorpions are now believed to have been terrestrial and to have washed into marine settings together with plant matter. These first scorpions were believed to have had gills instead of the present forms' book lungs, though this has subsequently been refuted. The oldest Gondwanan scorpions ("Gondwanascorpio") comprise the earliest known terrestrial animals from Gondwana. Currently, 111 fossil species of scorpion are known. Unusually for arachnids, there are more species of Palaeozoic scorpion than Mesozoic or Cenozoic ones.

Ancestral scorpions had compound eyes, but as they adapted to a nocturnal lifestyle, they became simplified.

The eurypterids, commonly called "sea scorpions", were aquatic creatures that lived during the Palaeozoic era that share several physical traits with scorpions and may be closely related to them. Various species of Eurypterida could grow to be anywhere from to in length. However, they exhibit anatomical differences marking them off as a group distinct from their Carboniferous and Recent relatives. Despite this, they are commonly referred to as "sea scorpions". Their legs are thought to have been short, thick, tapering and to have ended in a single strong claw. It appears that they were well-adapted for maintaining a secure hold upon rocks or seaweed against the wash of waves, like the legs of a shore crab. Cladistic analyses have supported the idea that the eurypterids are a distinct group from the scorpions.

The body of a scorpion is divided into two parts (tagmata): the head (cephalothorax) and the abdomen (opisthosoma), which is subdivided into a broad anterior (mesosoma), or preabdomen, and a narrow taillike posterior (metasoma), or postabdomen.

The cephalothorax, also called the "prosoma", comprises the carapace, eyes, chelicerae (mouth parts), pedipalps (the pedipalps of scorpions have chelae, commonly called claws or pincers) and four pairs of walking legs. The scorpion's exoskeleton is thick and durable, providing good protection from predators. Scorpions have two eyes on the top of the cephalothorax, and usually two to five pairs of eyes along the front corners of the cephalothorax. While unable to form sharp images, their central eyes are amongst the most light sensitive in the animal kingdom, especially in dim light, and makes it possible for nocturnal species to use starlight to navigate at night. Some species also have light receptors in their tail. The position of the eyes on the cephalothorax depends in part on the hardness or softness of the soil upon which they spend their lives.

The pedipalp is a segmented, chelate (clawed) appendage used for prey immobilization, defense and sensory purposes. The segments of the pedipalp (from closest to the body outwards) are coxa, trochanter, femur (humerus), patella, tibia (including the fixed claw and the manus) and tarsus (moveable claw). A scorpion has darkened or granular raised linear ridges, called "keels" or "carinae" on the pedipalp segments and on other parts of the body, which are useful taxonomically.

The mesosoma is the broad part of the opisthosoma. Sometimes it is loosely called the "abdomen". It consists of the anterior seven somites (segments) of the opisthosoma, each covered dorsally by a sclerotosed plate, its tergite. Ventrally somites 3 to 7 are armoured with matching plates called sternites.

Ventrally somites 1 and 2 are more complex; the first abdominal sternite is modified into a pair of genital opercula covering the gonopore. Sternite 2 forms the basal plate bearing the pectines. Morphologically the pectines are a pair of limbs that function as sensory organs.

The next four somites, 3 to 6, all bear pairs of spiracles. They serve as openings for the scorpion's respiratory organs, known as book lungs. The spiracle openings may be slits, circular, elliptical or oval according to the species of scorpion.

The 7th and last somite do not bear appendages or any other significant external structures.

The metasoma is commonly known as the scorpion's "tail", though this is in some ways misleading because unlike most so-called tails it is not an appendage or limb. It is in fact part of the opisthosoma. It comprises five segments, of which the fifth segment bears the telson. In many species, it superficially seems as though the metasoma has four segments only, because their first (anterior) metasomal segment gives the impression of being the posterior segment of the mesosoma. The fifth segment of the metasoma is the caudal segment of the opisthosoma, and accordingly bears the anus. The scorpion's telson is the part commonly called the stinger; it is attached to the end of the fifth segment just dorsad from the anus, but as the distal end of the tail at rest normally is carried upside down with the sting pointing forward, the anus usually is above the base of the telson and facing upwards.

The telson includes the vesicle, containing a symmetrical pair of venom glands. Externally it bears the curved sting, the hypodermic aculeus or venom-injecting barb. It is equipped with various sensory hairs, as the sting cannot be guided visually. Each of the venom glands has its own duct to convey its secretion internally along the aculeus from the bulb of the gland to immediately subterminal of the point of the aculeus, where each of the paired ducts has its own venom pore.

On rare occasions, scorpions are born with two metasomata. Two-tailed scorpions are no more than examples of adventitious ontogenic abnormality. Whether there ever is a genetic component to the condition is uncertain, but such evidence as is available from offspring is negative so far as no two-tailed scorpions have been observed among the rarely-observed progeny of multiple-tailed scorpion specimens.

Scorpions are also known to glow a vibrant blue-green when exposed to certain wavelengths of ultraviolet light such as that produced by a black light, due to the presence of fluorescent chemicals in the cuticle. One fluorescent component is now known to be beta-carboline. A hand-held UV lamp has long been a standard tool for nocturnal field surveys of these animals. Fluorescence occurs as a result of sclerotisation and increases in intensity with each successive instar. This fluorescence may have an active role in scorpion light detection.

Scorpions prefer areas where the temperatures range from , but may survive temperatures ranging from well below freezing to desert heat. Scorpions of the genus "Scorpiops" living in high Asian mountains, bothriurid scorpions from Patagonia and small "Euscorpius" scorpions from Central Europe can all survive winter temperatures of about . In Repetek (Turkmenistan), seven species of scorpion (of which "Pectinibuthus birulai" is endemic) live in temperatures varying from .

They are nocturnal and fossorial, finding shelter during the day in the relative cool of underground holes or undersides of rocks, and emerging at night to hunt and feed. Scorpions exhibit photophobic behavior, primarily to evade detection by predators such as birds, lizards, rodents such as the grasshopper mouse, opossums, and larger mammals including mongooses and the honey badger.

Scorpions are opportunistic predators of small arthropods, although the larger kinds have been known to kill small lizards and snakes. The large pincers are studded with highly sensitive tactile hair, and the moment an insect touches these, they use their chelae (pincers) to catch the prey. Depending on the toxicity of their venom and size of their claws, they will then either crush the prey or inject it with neurotoxic venom. This will kill or paralyze the prey so the scorpion can eat it. Scorpions have an unusual style of eating using chelicerae, small clawlike structures that protrude from the mouth that are unique to the Chelicerata among arthropods. The chelicerae, which are very sharp, are used to pull small amounts of food off the prey item for digestion into a "pre-oral cavity" below the chelicerae and carapace. Scorpions can ingest food only in a liquid form; they have external digestion. The digestive juices from the gut are egested onto the food and the digested food sucked in liquid form. Any solid indigestible matter (fur, exoskeleton, etc.) is trapped by setae in the pre-oral cavity and ejected by the scorpion.

Scorpions can consume huge amounts of food at one sitting. They have a very efficient food storage organ and a very low metabolic rate combined with a relatively inactive lifestyle. This enables scorpions to survive long periods when deprived of food. Some are able to survive 6 to 12 months of starvation. Scorpions excrete very little. Their waste consists mostly of insoluble nitrogenous compounds, such as xanthine, guanine and uric acid.

Most scorpions reproduce sexually, and most species have male and female individuals; however, some species, such as "Hottentotta hottentotta", "Hottentotta caboverdensis", "Liocheles australasiae", "Tityus columbianus", "Tityus metuendus", "Tityus serrulatus", "Tityus stigmurus", "Tityus trivittatus" and "Tityus urugayensis", reproduce through parthenogenesis, a process in which unfertilized eggs develop into living embryos. Parthenogenic reproduction starts following the scorpion's final molt to maturity and continues thereafter.
Sexual reproduction is accomplished by the transfer of a spermatophore from the male to the female. Scorpions possess a complex courtship and mating ritual to effect this transfer. Mating starts with the male and female locating and identifying each other using a mixture of pheromones and vibrational communication. Once they have satisfied the other that they are of opposite sex and of the correct species, mating can commence.

The courtship starts with the male grasping the female's pedipalps with his own. The pair then perform a "dance" called the ""promenade à deux"". In this "dance," the male leads the female around searching for a suitable place to deposit his spermatophore. The courtship ritual can involve several other behaviors such as juddering and a cheliceral kiss, in which the male's chelicerae – pincers – grasp the female's in a smaller, more intimate version of the male's grasping the female's pedipalps, and in some cases injecting a small amount of his venom into her pedipalp or on the edge of her cephalothorax, probably as a means of pacifying the female.

When the male has identified a suitable location, he deposits the spermatophore and then guides the female over it. This allows the spermatophore to enter her genital opercula, which triggers release of the sperm, thus fertilizing the female. The mating process can take from 1 to 25+ hours, and depends on the ability of the male to find a suitable place to deposit his spermatophore. If mating continues too long, the female may lose interest, ending the process.

Once the mating is complete, the male will generally retreat quickly, for unknown reasons; sexual cannibalism is infrequent with scorpions.

Unlike the majority of species in the class Arachnida, which are oviparous, scorpions seem to be universally viviparous. The young are born one by one, expel the embryonic membrane, if any, and the brood is carried about on its mother's back until the young have undergone at least one molt. Before the first molt, scorplings cannot survive naturally without the mother, since they depend on her for protection and to regulate their moisture levels. Especially in species that display more advanced sociability (e.g. "Pandinus" spp.), the young/mother association can continue for an extended period of time. The size of the litter depends on the species and environmental factors, and can range from 2 to more than 100 scorplings. The average litter however, consists of around eight scorplings.

The young generally resemble their parents. Growth is accomplished by periodic shedding of the exoskeleton (ecdysis). A scorpion's developmental progress is measured in instars (how many molts it has undergone). Scorpions typically require between five and seven molts to reach maturity. Molting commences with a split in the old exoskeleton just below the edge of the carapace (at the front of the prosoma). The scorpion then emerges from this split. The pedipalps and legs are first removed from the old exoskeleton, followed eventually by the metasoma. When it emerges, the scorpion's new exoskeleton is soft, making the scorpion highly vulnerable to attack. The scorpion must constantly stretch while the new exoskeleton hardens to ensure that it can move when the hardening is complete. The process of hardening is called sclerotisation. The new exoskeleton does not fluoresce. As sclerotisation occurs, the fluorescence gradually returns.

Although scorpions are usually not found in large numbers in densely populated urban areas, they do regularly occur in and near human habitation in all tropical parts of the world. The lack of predators, readily available shelter, and abundance of insect prey such as crickets, cockroaches, silverfishes, and earwigs have caused scorpions to thrive in industrial and residential areas. In Brazil, the number of people stung by scorpions increased from 12,000 in 2000 to 140,000 in 2018.

All known scorpion species possess venom and use it primarily to kill or paralyze their prey so that it can be eaten. The venom consists of a collection of peptides.

In general, the venom is fast-acting, allowing for effective prey capture; however, as a general rule, scorpions kill their prey with brute force if they can, as opposed to using venom, which is also used as a defense against predators. The venom is a mixture of compounds (neurotoxins, enzyme inhibitors, etc.), each not only causing a different effect, but possibly also targeting a specific animal. Each compound is made and stored in a pair of glandular sacs and is released in a quantity regulated by the scorpion itself. Of the more than one thousand known species of scorpions, only 25 have venom that is deadly to humans; most of those belong to the family Buthidae (including "Leiurus quinquestriatus", "Hottentotta" spp., "Centruroides" spp., and "Androctonus" spp.).

According to the United States National Institute for Occupational Safety and Health, these steps should be taken to prevent scorpion stings:

First aid for scorpion stings is generally symptomatic. It includes strong analgesia, either systemic (opioids or paracetamol) or locally applied (such as a cold compress). Cases of very high blood pressure are treated with anxiety-relieving medications and medications which lower the blood pressure by widening the diameter of blood vessels. Scorpion envenomation with high morbidity and mortality is usually due to either excessive autonomic activity and cardiovascular toxic effects or neuromuscular toxic effects. Antivenom is the specific treatment for scorpion envenomation combined with supportive measures including vasodilators in patients with cardiovascular toxic effects and benzodiazepines when neuromuscular involvement occurs. Although rare, severe hypersensitivity reactions including anaphylaxis to scorpion antivenin (SAV) are possible.<ref name="10.4103/0972-5229.164807"></ref>

Short-chain scorpion toxins constitute the largest group of potassium (K) channel-blocking peptides. An important physiological role of the KCNA3 channel, also known as K1.3, is to help maintain large electrical gradients for the sustained transport of ions such as Ca that controls T lymphocyte (T cell) proliferation. Thus K1.3 blockers could be potential immunosuppressants for the treatment of autoimmune disorders (such as rheumatoid arthritis, inflammatory bowel disease, and multiple sclerosis).

The venom of "Uroplectes lineatus" is clinically important in dermatology.

Toxins being investigated include the following:

Scorpions for use in the pharmaceutical industry are collected from the wild in Pakistan. Farmers in the Thatta District are paid about US$100 for each 40-gram scorpion, and 60-gram specimens are reported to fetch at least US$50,000. The trade is reported to be illegal but thriving.

The venom is one of the most valuable liquids by volume on earth and it costs $39 million to produce a gallon of the toxin.

Fried scorpion is a traditional dish from Shandong, China.





</doc>
<doc id="28923" url="https://en.wikipedia.org/wiki?curid=28923" title="Shriners">
Shriners

Shriners International, also commonly known as The Shriners or formerly known as the Ancient Arabic Order of the Nobles of the Mystic Shrine (AAONMS, anagram for A MASON), is a society established in 1870 and is headquartered in Tampa, Florida.

Shriners International describes itself as a fraternity based on fun, fellowship, and the Masonic principles of brotherly love, relief, and truth. There are approximately 350,000 members from 196 temples (chapters) in the U.S., Canada, Brazil, Bolivia, Mexico, the Republic of Panama, the Philippines, Puerto Rico, Europe, and Australia. The organization is best known for the Shriners Hospitals for Children that it administers, and the red fezzes that members wear.

The organization was previously known as "Shriners North America". The name was changed in 2010 across North America, Central America, South America, Europe, and Southeast Asia.

In 1870 there were several thousand Freemasons in Manhattan, many of whom lunched at the Knickerbocker Cottage at a special table on the second floor. There, the idea of a new fraternity for Masons, stressing fun and fellowship, was discussed. Walter M. Fleming, and William J. Florence took the idea seriously enough to act upon it.

Florence, a world-renowned actor, while on tour in Marseille, was invited to a party given by an Arab diplomat. The entertainment was something in the nature of an elaborately staged musical comedy. At its conclusion, the guests became members of a secret society. Florence took copious notes and drawings at his initial viewing and on two other occasions, once in Algiers and once in Cairo. When he returned to New York in 1870, he showed his material to Fleming.

Fleming created the ritual, emblem and costumes. Florence and Fleming were initiated August 13, 1870, and they initiated 11 other men on June 16, 1871.

The group adopted a Middle Eastern theme and soon established Temples (though the term Temple has now generally been replaced by Shrine Auditorium or Shrine Center). The first Temple established was Mecca Temple (now known as Mecca Shriners), established at the New York City Masonic Hall on September 26, 1872. Fleming was the first Potentate.

In 1875, there were only 43 Shriners in the organization. In an effort to encourage membership, at the June 6, 1876 meeting of Mecca Temple, the Imperial Grand Council of the Ancient Order of the Nobles of the Mystic Shrine for North America was created. Fleming was elected the first Imperial Potentate. After some other reworking, by 1878 there were 425 members in 13 temples in eight states, and by 1888, there were 7,210 members in 48 temples in the United States and Canada. By the Imperial Session held in Washington, D.C. in 1900, there were 55,000 members and 82 Temples.

By 1938 there were about 340,000 members in the United States. That year "Life" published photographs of its rites for the first time. It described the Shriners as "among secret lodges the No. 1 in prestige, wealth and show", and stated that "[i]n the typical city, especially in the Middle West, the Shriners will include most of the prominent citizens."

Shriners often participate in local parades, sometimes as rather elaborate units: miniature vehicles in themes (all sports cars; all miniature 18-wheeler trucks; all fire engines, and so on), an "Oriental Band" dressed in cartoonish versions of Middle Eastern dress; pipe bands, drummers, motorcycle units, Drum and Bugle Corps, and even traditional brass bands.

Until 2000, before being eligible for membership in the Shrine, a Mason had to complete either the Scottish Rite or York Rite systems, but now any Master Mason can join.

In the past, Shriners have practiced hazing rituals as a part of initiating new members: in 1991, a would-be Shriner sued the Oleika Shrine Temple of Lexington, Kentucky over injuries suffered during the hazing, which included being blindfolded and having a jolt of electricity applied to his bare buttocks.

While there are plenty of activities for Shriners, there are two organizations tied to the Shrine that are for women only: The Ladies' Oriental Shrine and the Daughters of the Nile. They both support the Shriners Hospitals and promote sociability, and membership in either organization is open to any woman 18 years of age and older who is related to a Shriner or Master Mason by birth or marriage.

The Ladies Oriental Shrine of North America was founded in 1903 in Wheeling, West Virginia, and the Daughters of the Nile was founded in 1913 in Seattle, Washington. The latter organization has locals called "Temples". There were ten of these in 1922. Among the famous members of the Daughters of the Nile was First Lady Florence Harding, wife of Warren G. Harding.

Some of the earliest Shrine Centers often chose a Moorish Revival style for their Temples. Architecturally notable Shriners Temples include the Shrine Auditorium in Los Angeles, the former Mecca Temple, now called New York City Center and used primarily as a concert hall, Newark Symphony Hall, the Landmark Theater (formerly The Mosque) in Richmond, Virginia, the Tripoli Shrine Temple in Milwaukee, Wisconsin, the Polly Rosenbaum Building (formerly the El Zaribah Shrine Auditorium) in Phoenix, the Helena Civic Center (Montana) (formerly the Algeria Shrine Temple), Abou Ben Adhem Shrine Mosque in Springfield, Missouri and the Fox Theatre (Atlanta, Georgia) which was jointly built between the Atlanta Shriners and movie mogul William Fox.

The Shrine's charitable arm is the Shriners Hospitals for Children, a network of twenty-two healthcare facilities in the United States, Mexico, and Canada.

In June 1920, the Imperial Council Session voted to establish a "Shriners Hospital for Crippled Children." The purpose of this hospital was to treat orthopedic injuries and conditions, diseases, burns, spinal cord injuries, and birth defects, such as cleft lip and palate, in children. After much research and debate, the committee chosen to determine the site of the hospital decided there should be not just one hospital but a network of hospitals spread across North America. The first hospital was opened in 1922 in Shreveport, Louisiana, and by the end of the decade 13 more hospitals were in operation. Shriners Hospitals now provide orthopedic care, burn treatment, cleft lip and palate care, and spinal cord injury rehabilitation.

The rules for all of the Shriners Hospitals are simple and to the point: Any child under the age of 18 can be admitted to the hospital if, in the opinion of the doctors, the child can be treated. There is no requirement for religion, race, or relationship to a Shriner.

Until June 2012, all care at Shriners Hospitals was provided without charge to patients and their families. At that time, because the size of their endowment had decreased due to losses in the stock market, Shriners Hospitals started billing patients' insurance companies, but still offered free care to children without insurance and waives all out of pocket costs insurance does not cover. Shriners Hospitals for Children is a 501(c)(3) nonprofit organization, meaning that they rely on the generosity of donors to cover the cost of treatment for their patients.

In 2008, Shriners Hospitals had a total budget of $826 million. In 2007 they approved 39,454 new patient applications, and attended to the needs of 125,125 patients. Shriners Hospitals for Children can be found in these cities:


<nowiki>*</nowiki>This location is an outpatient, ambulatory care center.

Most Shrine Temples support several parade units. These units are responsible for promoting a positive Shriner image to the public by participating in local parades. The parade units often include miniature cars powered by lawn mower engines.
An example of a Shrine parade unit is the Heart Shrine Clubs' Original Fire Patrol of Effingham, Illinois. This unit operates miniature fire engines, memorializing a hospital fire that took place in the 1940s in Effingham. They participate in most parades in a 100-mile radius of Effingham. Shriners in Dallas, Texas participate annually in the Twilight Parade at the Texas State Fair.

Shriners in St. Louis have several parade motor units, including miniature cars styled after 1932 Ford coupes and 1970s-era Jeep CJ models, and a unit of miniature Indianapolis-styled race cars. Some of these are outfitted with high-performance, alcohol-fueled engines. The drivers' skills are demonstrated during parades with high-speed spinouts.

The Shriners are committed to community service and have been instrumental in countless public projects throughout their domain.

Shriners host the annual "East-West Shrine Game", a college football all-star game.

The Shriners originally hosted a golf tournament in association with singer/actor Justin Timberlake, titled the "Justin Timberlake Shriners Hospitals for Children Open", a PGA TOUR golf tournament held in Las Vegas, Nevada. The relationship between Timberlake and the Shriners ended in 2012, due to the lack of previously agreed participation on Timberlake's part. In July 2012, The PGA TOUR and Shriners Hospitals for Children announced a five-year title sponsorship extension, carrying the commitment to the Shriners Hospitals for Children Open through 2017. now titled "The Shriners Hospitals for Children Open", It is still held in Las Vegas, Nevada.

Once a year, the fraternity meets for the Imperial Council Session in a major North American city. It is not uncommon for these conventions to have 20,000 participants or more, which generates significant revenue for the local economy.

Many Shrine Centers also hold a yearly "Shrine Circus" as a fundraiser.




</doc>
<doc id="28925" url="https://en.wikipedia.org/wiki?curid=28925" title="Science fiction fandom">
Science fiction fandom

Science fiction fandom or SF fandom is a community or fandom of people interested in science fiction in contact with one another based upon that interest. SF fandom has a life of its own, but not much in the way of formal organization (although clubs such as the Futurians (1937–1945) are a recognized example of organized fandom).

Most often called simply "fandom" within the community, it can be viewed as a distinct subculture, with its own literature and jargon; marriages and other relationships among fans are common, as are multi-generational fan families.

Science fiction fandom started through the letter column of Hugo Gernsback's fiction magazines. Not only did fans write comments about the stories—they sent their addresses, and Gernsback published them. Soon, fans were writing letters directly to each other, and meeting in person when they lived close together, or when one of them could manage a trip. In New York City, David Lasser, Gernsback's managing editor, nurtured the birth of a small local club called the Scienceers, which held its first meeting in a Harlem apartment on December 11, 1929. Almost all the members were adolescent boys. Around this time a few other small local groups began to spring up in metropolitan areas around the United States, many of them connecting with fellow enthusiasts via the Science Correspondence Club. In May 1930 the first science-fiction fan magazine, "The Comet", was produced by the Chicago branch of the Science Correspondence Club under the editorship of Raymond A. Palmer (later a noted, and notorious, sf magazine editor) and Walter Dennis. In January 1932, the New York City circle, which by then included future comic-book editors Julius Schwartz and Mort Weisinger, brought out the first issue of their own publication, "The Time Traveller", with Forrest J Ackerman of the embryonic Los Angeles group as a contributing editor.

In 1934, Gernsback established a correspondence club for fans called the Science Fiction League, the first fannish organization. Local groups across the nation could join by filling out an application. A number of clubs came into being around this time. LASFS (the Los Angeles Science Fantasy Society) was founded at this time as a local branch of the SFL, while several competing local branches sprang up in New York City and immediately began feuding among themselves.

In 1935, PSFS (the Philadelphia Science Fiction Society, 1935–present) was formed. The next year, half a dozen fans from NYC came to Philadelphia to meet with the PSFS members, as the first Philadelphia Science Fiction Conference, which some claim as the world's first science fiction convention.

Soon after the fans started to communicate directly with each other came the creation of science fiction fanzines. These amateur publications might or might not discuss science fiction and were generally traded rather than sold. They ranged from the utilitarian or inept to professional-quality printing and editing. In recent years, Usenet newsgroups such as rec.arts.sf.fandom, websites and blogs have somewhat supplanted printed fanzines as an outlet for expression in fandom, though many popular fanzines continue to be published. Science-fiction fans have been among the first users of computers, email, personal computers and the Internet.

Many professional science fiction authors started their interest in science fiction as fans, and some still publish their own fanzines or contribute to those published by others.

A widely regarded (though by no means error-free) history of fandom in the 1930s can be found in Sam Moskowitz's "The Immortal Storm: A History of Science Fiction Fandom" Hyperion Press 1988 (original edition The Atlanta Science Fiction Organization Press, Atlanta, Georgia 1954). Moskowitz was himself involved in some of the incidents chronicled and has his own point of view, which has often been criticized.

Organized fandom in Sweden ("Sverifandom") emerged during the early-1950s. The first Swedish science fiction fanzine was started in the early 1950s. The oldest still existing club, Club Cosmos in Gothenburg, was formed in 1954, and the first Swedish science-fiction convention, LunCon, was held in Lund in 1956.

Today, there are a number of science fiction clubs in the country, including Skandinavisk Förening för Science Fiction (whose club fanzine, "Science Fiction Forum", was once edited by Stieg Larsson, a board member and one-time chairman thereof), Linköpings Science Fiction-Förening and Sigma Terra Corps. Between one and four science-fiction conventions are held each year in Sweden, among them Swecon, the annual national Swedish con. An annual prize is awarded to someone that has contributed to the national fandom by the Alvar Appeltofft Memorial Fund.

SF fandom in the UK has close ties with that in the USA. In the UK there are multiple conventions. The largest regular convention for Literary SF (Book focused) fandom is the British National convention or Eastercon. Strangely enough this is held over the Easter weekend. Committee membership and location changes year-to-year. The license to use the Eastercon name for a year is awarded by votes of the business meeting of the Eastercon two years previously. There are substantially larger events run by UK Media Fandom and commercial organisations also run Gate Shows (for-profit operations with paid staff.) The UK has also hosted the Worldcon several times, most recently in 2014. News of UK events appears in the fanzine Ansible produced by David Langford each month.

The beginning of an Italian science fiction fandom can be located between the late 1950s and early 1960s, when magazines such as "Oltre il Cielo" and "Futuro" started to publish readers’ letters and promote correspondences and the setting-up of clubs in various cities. Among the first fanzines, "Futuria Fantasia" was cyclostyled in Milan in 1963 by Luigi Cozzi (later to become a filmmaker), its title paid homage to Ray Bradbury's fanzine by the same name; "L’Aspidistra", edited by Riccardo Leveghi in Trento starting in 1965 featured contributions by Gianfranco de Turris, Gian Luigi Staffilano, and Sebastiano Fusco, future editors of professional magazines and book series; also Luigi Naviglio, editor in 1965 of the fanzine "Nuovi Orizzonti", was soon to become a writer for "I Romanzi del Cosmo". During subsequent years fanzines continued to function as training grounds for future editors and writers, and the general trend was towards improved quality and life expectancy (e.g. "The Time Machine" run for 50 issues starting in 1975, "Intercom" for 149 issues between 1979 and 1999, before its migration to the web as an e-zine until 2003, then as a website).

In 1963 the first Trieste Festival of Science Fiction Cinema took place, anticipating the first conventions as an opportunity for a nationwide social gathering. Informal meetings were organized in Milan, Turin and Carrara between 1965 and 1967. In 1972, the first European convention, Eurocon, was organized in Trieste, during which an Italia Award was also created. Eurocon was back in Italy in 1980 and 2009 (in 1989 a Eurocon was held in San Marino).

Since its foundation in 2013, the association "World SF Italia" coordinates the organization the annual national convention (Italcon) and awards (Premio Italia – with thirty- two categories across media – and Premio Vegetti – best Italian novel and essay).

Since the late 1930s, SF fans have organized conventions, non-profit gatherings where the fans (some of whom are also professionals in the field) meet to discuss SF and generally enjoy themselves. (A few fannish couples have held their weddings at conventions.) The 1st World Science Fiction Convention or Worldcon was held in conjunction with the 1939 New York World's Fair, and has been held annually since the end of World War II. Worldcon has been the premier convention in fandom for over half a century; it is at this convention that the Hugo Awards are bestowed, and attendance can approach 8,000 or more.

SF writer Cory Doctorow calls science fiction "perhaps the most social of all literary genres", and states, "Science fiction is driven by organized fandom, volunteers who put on hundreds of literary conventions in every corner of the globe, every weekend of the year."

SF conventions can vary from minimalist "relaxacons" with a hundred or so attendees to heavily programmed events with four to six or more simultaneous tracks of programming, such as WisCon and Worldcons.

Commercial shows dealing with SF-related fields are sometimes billed as 'science fiction conventions,' but are operated as for-profit ventures, with an orientation towards passive spectators, rather than involved fans, and a tendency to neglect or ignore written SF in favor of television, film, comics, video games, etc. One of the largest of these is the annual Dragon*Con in Atlanta, Georgia with an attendance of more than 20,000 since 2000.

In the United States, many science-fiction societies were launched as chapters of the Science Fiction League and, when it faded into history, several of the original League chapters remained viable and were subsequently incorporated as independent organizations. Most notable among the former League chapters which were spun off was the Philadelphia Science Fiction Society, which served as a model for subsequent SF societies formed independent of the League history.

Science-fiction societies, more commonly referred to as "clubs" except on the most formal of occasions, form a year-round base of activities for science-fiction fans. They are often associated with an SF convention or group of conventions, but maintain a separate existence as cultural institutions within specific geographic regions. Several have purchased property and maintain ongoing collections of SF literature available for research, as in the case of the Los Angeles Science Fantasy Society, the New England Science Fiction Association, and the Baltimore Science Fiction Society. Other SF Societies maintain a more informal existence, meeting at general public facilities or the homes of individual members, such as the Bay Area Science Fiction Association.

As a community devoted to discussion and exploration of new ideas, fandom has become an incubator for many groups that started out as special interests within fandom, some of which have partially separated into independent intentional communities not directly associated with science fiction. Among these groups are comic-book fandom, media fandom, the Society for Creative Anachronism, gaming, and furry fandom, sometimes referred to collectively as "fringe fandoms" with the implication that the original fandom centered on science-fiction texts (magazines and later books and fanzines) is the "true" or "core" fandom. Fandom also welcomes and shares interest with other groups including LGBT communities, libertarians, neo-pagans, and space activist groups like the L5 Society, among many others. Some groups exist almost entirely within fandom but are distinct and cohesive subcultures in their own rights, such as filkers, costumers, and convention runners (sometimes called "SMOFs").

Fandom encompasses subsets of fans that are principally interested in a single writer or subgenre, such as Tolkien fandom, and ("Trekkies"). Even short-lived television series may have dedicated followings, such as the fans of Joss Whedon's "Firefly" television series and movie "Serenity", known as Browncoats.

Participation in science fiction fandom often overlaps with other similar interests, such as fantasy role-playing games, comic books and anime, and in the broadest sense fans of these activities are felt to be part of the greater community of SF fandom.

There are active SF fandoms around the world. Fandom in non-Anglophone countries is based partially on local literature and media, with cons and other elements resembling those of English-speaking fandom, but with distinguishing local features. For example, Finland's national gathering Finncon is funded by the government, while all conventions and fan activities in Japan are heavily influenced by anime and manga.

Science fiction and fantasy fandom has its own slang or jargon, sometimes called "fanspeak" (the term has been in use since at least 1962).

Fanspeak is made up of acronyms, blended words, obscure in-jokes, and standard terms used in specific ways. Some terms used in fanspeak have spread to members of the Society for Creative Anachronism ("Scadians"), Renaissance Fair participants ("Rennies"), hacktivists, and internet gaming and chat fans, due to the social and contextual intersection between the communities. Examples of fanspeak used in these broader fannish communities include gafiate, a term meaning to drop out of SF related community activities, with the implication to Get A Life. The word is derived via the acronym for "get away from it all". A related term is fafiate, for "forced away from it all". The implication is that one would really rather still be involved in fandom, but circumstances make it impossible.

Two other acronyms commonly used in the community are FIAWOL (Fandom Is A Way Of Life) and its opposite FIJAGH (Fandom Is Just A Goddamned Hobby) to describe two ways of looking at the place of fandom in one's life.

Science-fiction fans often refer to themselves using the irregular plural "fen": man/men, fan/fen.

As science fiction fans became professional writers, they started slipping the names of their friends into stories. Wilson "Bob" Tucker slipped so many of his fellow fans and authors into his works that doing so is called tuckerization.

The subgenre of "recursive science fiction" has a fan-maintained bibliography at the New England Science Fiction Association's website; some of it is about science fiction fandom, some not.

In Robert Bloch's 1956 short story, "A Way Of Life", science-fiction fandom is the only institution to survive a nuclear holocaust and eventually becomes the basis for the reconstitution of civilization. The science-fiction novel "Gather in the Hall of the Planets", by K.M. O'Donnell (aka Barry Malzberg), 1971, takes place at a New York City science-fiction convention and features broad parodies of many SF fans and authors. A pair of SF novels by Gene DeWeese and Robert "Buck" Coulson, "Now You See It/Him/Them" and "Charles Fort Never Mentioned Wombats" are set at Worldcons; the latter includes an in-character "introduction" by Wilson Tucker (himself a character in the novel) which is a sly self-parody verging on a self-tuckerization.

The 1991 SF novel "Fallen Angels" by Larry Niven, Jerry Pournelle and Michael Flynn constitutes a tribute to SF fandom. The story includes a semi-illegal fictional Minneapolis Worldcon in a post-disaster world where science, and thus fandom, is disparaged. Many of the characters are barely tuckerized fans, mostly from the Greater Los Angeles area.

Mystery writer Sharyn McCrumb's "Bimbos of the Death Sun" and "Zombies of the Gene Pool" are murder mysteries set at a science-fiction convention and within the broader culture of fandom respectively. While containing mostly nasty caricatures of fans and fandom, some fans take them with good humor; others consider them vicious and cruel.

In 1994 and 1996, two anthologies of alternate history science fiction involving World Science Fiction Conventions, titled "Alternate Worldcons" and "Again, Alternate Worldcons", edited by Mike Resnick were published.

A.E. van Vogt's 1940 novel "Slan" was about a mutant variety of humans who are superior to regular humanity and are therefore hunted down and killed by the normal human population. While the story has nothing to do with fandom, many science-fiction fans felt very close to the protagonists, feeling their experience as bright people in a mundane world mirrored that of the mutants; hence, the rallying cry, "Fans Are Slans!"; and the tradition that a building inhabited primarily by fans can be called a slan shack.




</doc>
<doc id="28926" url="https://en.wikipedia.org/wiki?curid=28926" title="Spin">
Spin

Spin or spinning may refer to:
















</doc>
<doc id="28927" url="https://en.wikipedia.org/wiki?curid=28927" title="Stellar classification">
Stellar classification

In astronomy, stellar classification is the classification of stars based on their spectral characteristics. Electromagnetic radiation from the star is analyzed by splitting it with a prism or diffraction grating into a spectrum exhibiting the rainbow of colors interspersed with spectral lines. Each line indicates a particular chemical element or molecule, with the line strength indicating the abundance of that element. The strengths of the different spectral lines vary mainly due to the temperature of the photosphere, although in some cases there are true abundance differences. The "spectral class" of a star is a short code primarily summarizing the ionization state, giving an objective measure of the photosphere's temperature.

Most stars are currently classified under the Morgan-Keenan (MK) system using the letters "O", "B", "A", "F", "G", "K", and "M", a sequence from the hottest ("O" type) to the coolest ("M" type). Each letter class is then subdivided using a numeric digit with "0" being hottest and "9" being coolest (e.g. A8, A9, F0, and F1 form a sequence from hotter to cooler). The sequence has been expanded with classes for other stars and star-like objects that do not fit in the classical system, such as class "D" for white dwarfs and classes "S" and "C" for carbon stars.

In the MK system, a luminosity class is added to the spectral class using Roman numerals. This is based on the width of certain absorption lines in the star's spectrum, which vary with the density of the atmosphere and so distinguish giant stars from dwarfs. Luminosity class "0" or "Ia+" is used for "hypergiants", class "I" for "supergiants", class "II" for bright "giants", class "III" for regular "giants", class "IV" for "sub-giants", class "V" for "main-sequence stars", class "sd" (or "VI") for "sub-dwarfs", and class "D" (or "VII") for "white dwarfs". The full spectral class for the Sun is then G2V, indicating a main-sequence star with a surface temperature around 5,800 K.

The conventional color description takes into account only the peak of the stellar spectrum. In actuality, however, stars radiate in all parts of the spectrum. Because all spectral colors combined appear white, the actual apparent colors the human eye would observe are far lighter than the conventional color descriptions would suggest. This characteristic of 'lightness' indicates that the simplified assignment of colors within the spectrum can be misleading. Excluding color-contrast illusions in dim light, there are no green, indigo, or violet stars. Red dwarfs are a deep shade of orange, and brown dwarfs do not literally appear brown, but hypothetically would appear dim grey to a nearby observer.

The modern classification system is known as the "Morgan–Keenan" (MK) classification. Each star is assigned a spectral class from the older Harvard spectral classification and a luminosity class using Roman numerals as explained below, forming the star's spectral type.

Other modern stellar classification systems, such as the UBV system, are based on color indexes—the measured differences in three or more color magnitudes. Those numbers are given labels such as "U−V" or "B−V", which represent the colors passed by two standard filters (e.g. "U"ltraviolet, "B"lue and "V"isual).

The "Harvard system" is a one-dimensional classification scheme by astronomer Annie Jump Cannon, who re-ordered and simplified a prior alphabetical system. Stars are grouped according to their spectral characteristics by single letters of the alphabet, optionally with numeric subdivisions. Main-sequence stars vary in surface temperature from approximately 2,000 to 50,000 K, whereas more-evolved stars can have temperatures above 100,000 K. Physically, the classes indicate the temperature of the star's atmosphere and are normally listed from hottest to coldest.

The spectral classes O through M, as well as other more specialized classes discussed later, are subdivided by Arabic numerals (0–9), where 0 denotes the hottest stars of a given class. For example, A0 denotes the hottest stars in class A and A9 denotes the coolest ones. Fractional numbers are allowed; for example, the star Mu Normae is classified as O9.7. The Sun is classified as G2.

Conventional color descriptions are traditional in astronomy, and represent colors relative to the mean color of an A class star, which is considered to be white. The apparent color descriptions are what the observer would see if trying to describe the stars under a dark sky without aid to the eye, or with binoculars. However, most stars in the sky, except the brightest ones, appear white or bluish white to the unaided eye because they are too dim for color vision to work. Red supergiants are cooler and redder than dwarfs of the same spectral type, and stars with particular spectral features such as carbon stars may be far redder than any black body.

The fact that the Harvard classification of a star indicated its surface or photospheric temperature (or more precisely, its effective temperature) was not fully understood until after its development, though by the time the first Hertzsprung–Russell diagram was formulated (by 1914), this was generally suspected to be true. In the 1920s, the Indian physicist Meghnad Saha derived a theory of ionization by extending well-known ideas in physical chemistry pertaining to the dissociation of molecules to the ionization of atoms. First he applied it to the solar chromosphere, then to stellar spectra.

Harvard astronomer Cecilia Payne then demonstrated that the "O-B-A-F-G-K-M" spectral sequence is actually a sequence in temperature. Because the classification sequence predates our understanding that it is a temperature sequence, the placement of a spectrum into a given subtype, such as B3 or A7, depends upon (largely subjective) estimates of the strengths of absorption features in stellar spectra. As a result, these subtypes are not evenly divided into any sort of mathematically representable intervals.

The "Yerkes spectral classification", also called the "MKK" system from the authors' initials, is a system of stellar spectral classification introduced in 1943 by William Wilson Morgan, Philip C. Keenan, and Edith Kellman from Yerkes Observatory. This two-dimensional (temperature and luminosity) classification scheme is based on spectral lines sensitive to stellar temperature and surface gravity, which is related to luminosity (whilst the "Harvard classification" is based on just surface temperature). Later, in 1953, after some revisions of list of standard stars and classification criteria, the scheme was named the "Morgan–Keenan classification", or "MK", and this system remains in use.

Denser stars with higher surface gravity exhibit greater pressure broadening of spectral lines. The gravity, and hence the pressure, on the surface of a giant star is much lower than for a dwarf star because the radius of the giant is much greater than a dwarf of similar mass. Therefore, differences in the spectrum can be interpreted as "luminosity effects" and a luminosity class can be assigned purely from examination of the spectrum.

A number of different "luminosity classes" are distinguished, as listed in the table below.

Marginal cases are allowed; for example, a star may be either a supergiant or a bright giant, or may be in between the subgiant and main-sequence classifications. 
In these cases, two special symbols are used:

For example, a star classified as A3-4III/IV would be in between spectral types A3 and A4, while being either a giant star or a subgiant.

Sub-dwarf classes have also been used: VI for sub-dwarfs (stars slightly less luminous than the main sequence).

Nominal luminosity class VII (and sometimes higher numerals) is now rarely used for white dwarf or "hot sub-dwarf" classes, since the temperature-letters of the main sequence and giant stars no longer apply to white dwarfs.

Occasionally, letters "a" and "b" are applied to luminosity classes other than supergiants; for example, a giant star slightly more luminous than typical may be given a luminosity class of IIIb.

A sample of extreme V stars with strong absorption in He II λ4686 spectral lines have been given the "Vz" designation. An example star is HD 93129 B.

Additional nomenclature, in the form of lower-case letters, can follow the spectral type to indicate peculiar features of the spectrum.

For example, 59 Cygni is listed as spectral type B1.5Vnne, indicating a spectrum with the general classification B1.5V, as well as very broad absorption lines and certain emission lines.

The reason for the odd arrangement of letters in the Harvard classification is historical, having evolved from the earlier Secchi classes and been progressively modified as understanding improved.

During the 1860s and 1870s, pioneering stellar spectroscopist Angelo Secchi created the "Secchi classes" in order to classify observed spectra. By 1866, he had developed three classes of stellar spectra, shown in the table below.

In the late 1890s, this classification began to be superseded by the Harvard classification, which is discussed in the remainder of this article.

The Roman numerals used for Secchi classes should not be confused with the completely unrelated Roman numerals used for Yerkes luminosity classes and the proposed neutron star classes.

In the 1880s, the astronomer Edward C. Pickering began to make a survey of stellar spectra at the Harvard College Observatory, using the objective-prism method. A first result of this work was the "Draper Catalogue of Stellar Spectra", published in 1890. Williamina Fleming classified most of the spectra in this catalogue.

The catalogue used a scheme in which the previously used Secchi classes (I to V) were subdivided into more specific classes, given letters from A to P. Also, the letter Q was used for stars not fitting into any other class.

In 1897, another worker at Harvard, Antonia Maury, placed the Orion subtype of Secchi class I ahead of the remainder of Secchi class I, thus placing the modern type B ahead of the modern type A. She was the first to do so, although she did not use lettered spectral types, but rather a series of twenty-two types numbered from I to XXII.

In 1901, Annie Jump Cannon returned to the lettered types, but dropped all letters except O, B, A, F, G, K, M, and N used in that order, as well as P for planetary nebulae and Q for some peculiar spectra. She also used types such as B5A for stars halfway between types B and A, F2G for stars one-fifth of the way from F to G, and so on. Finally, by 1912, Cannon had changed the types B, A, B5A, F2G, etc. to B0, A0, B5, F2, etc. This is essentially the modern form of the Harvard classification system.

A common mnemonic for remembering the order of the spectral type letters, from hottest to coolest, is "Oh, Be A Fine Guy/Girl: Kiss Me!".

A luminosity classification known as the Mount Wilson system was used to distinguish between stars of different luminosities. This notation system is still sometimes seen on modern spectra.

The stellar classification system is taxonomic, based on type specimens, similar to classification of species in biology: The categories are defined by one or more standard stars for each category and sub-category, with an associated description of the distinguishing features.

Stars are often referred to as "early" or "late" types. "Early" is a synonym for "hotter", while "late" is a synonym for "cooler".

Depending on the context, "early" and "late" may be absolute or relative terms. "Early" as an absolute term would therefore refer to O or B, and possibly A stars. As a relative reference it relates to stars hotter than others, such as "early K" being perhaps K0, K1, and K3.

"Late" is used in the same way, with an unqualified use of the term indicating stars with spectral types such as K and M, but it can also be used for stars that are cool relative to other stars, as in using "late G" to refer to G7, G8, and G9.

In the relative sense, "early" means a lower Arabic numeral following the class letter, and "late" means a higher number.

This obscure terminology is a hold-over from an early 20th century model of stellar evolution, which supposed that stars were powered by gravitational contraction via the Kelvin–Helmholtz mechanism, which is now known to not apply to main sequence stars. If that were true, then stars would start their lives as very hot "early-type" stars and then gradually cool down into "late-type" stars. This mechanism provided ages of the Sun that were much smaller than what is observed in the geologic record, and was rendered obsolete by the discovery that stars are powered by nuclear fusion. The terms "early" and "late" were carried over, beyond the demise of the model they were based on.

O-type stars are very hot and extremely luminous, with most of their radiated output in the ultraviolet range. These are the rarest of all main-sequence stars. About 1 in 3,000,000 (0.00003%) of the main-sequence stars in the solar neighborhood are O-type stars. Some of the most massive stars lie within this spectral class. O-type stars frequently have complicated surroundings that make measurement of their spectra difficult.

O-type spectra formerly were defined by the ratio of the strength of the He II λ4541 relative to that of He I λ4471, where λ is the wavelength, measured in ångströms. Spectral type O7 was defined to be the point at which the two intensities are equal, with the He I line weakening towards earlier types. Type O3 was, by definition, the point at which said line disappears altogether, although it can be seen very faintly with modern technology. Due to this, the modern definition uses the ratio of the nitrogen line N IV λ4058 to N III λλ4634-40-42.

O-type stars have dominant lines of absorption and sometimes emission for He II lines, prominent ionized (Si IV, O III, N III, and C III) and neutral helium lines, strengthening from O5 to O9, and prominent hydrogen Balmer lines, although not as strong as in later types. Because they are so massive, O-type stars have very hot cores and burn through their hydrogen fuel very quickly, so they are the first stars to leave the main sequence.

When the MKK classification scheme was first described in 1943, the only subtypes of class O used were O5 to O9.5. The MKK scheme was extended to O9.7 in 1971 and O4 in 1978, and new classification schemes that add types O2, O3, and O3.5 have subsequently been introduced.

Spectral standards:

B-type stars are very luminous and blue. Their spectra have neutral helium lines, which are most prominent at the B2 subclass, and moderate hydrogen lines. As O- and B-type stars are so energetic, they only live for a relatively short time. Thus, due to the low probability of kinematic interaction during their lifetime, they are unable to stray far from the area in which they formed, apart from runaway stars.

The transition from class O to class B was originally defined to be the point at which the He II λ4541 disappears. However, with modern equipment, the line is still apparent in the early B-type stars. Today for main-sequence stars, the B-class is instead defined by the intensity of the He I violet spectrum, with the maximum intensity corresponding to class B2. For supergiants, lines of silicon are used instead; the Si IV λ4089 and Si III λ4552 lines are indicative of early B. At mid B, the intensity of the latter relative to that of Si II λλ4128-30 is the defining characteristic, while for late B, it is the intensity of Mg II λ4481 relative to that of He I λ4471.

These stars tend to be found in their originating OB associations, which are associated with giant molecular clouds. The Orion OB1 association occupies a large portion of a spiral arm of the Milky Way and contains many of the brighter stars of the constellation Orion. About 1 in 800 (0.125%) of the main-sequence stars in the solar neighborhood are B-type main-sequence stars.

Massive yet non-supergiant entities known as "Be stars" are main-sequence stars that notably have, or had at some time, one or more Balmer lines in emission, with the hydrogen-related electromagnetic radiation series projected out by the stars being of particular interest. Be stars are generally thought to feature unusually strong stellar winds, high surface temperatures, and significant attrition of stellar mass as the objects rotate at a curiously rapid rate. Objects known as "B(e)" or "B[e]" stars possess distinctive neutral or low ionisation emission lines that are considered to have 'forbidden mechanisms', undergoing processes not normally allowed under current understandings of quantum mechanics.

Spectral standards:

A-type stars are among the more common naked eye stars, and are white or bluish-white. They have strong hydrogen lines, at a maximum by A0, and also lines of ionized metals (Fe II, Mg II, Si II) at a maximum at A5. The presence of Ca II lines is notably strengthening by this point. About 1 in 160 (0.625%) of the main-sequence stars in the solar neighborhood are A-type stars.

Spectral standards:

F-type stars have strengthening spectral lines "H" and "K" of Ca II. Neutral metals (Fe I, Cr I) beginning to gain on ionized metal lines by late F. Their spectra are characterized by the weaker hydrogen lines and ionized metals. Their color is white. About 1 in 33 (3.03%) of the main-sequence stars in the solar neighborhood are F-type stars.

Spectral standards:

G-type stars, including the Sun, have prominent spectral lines "H" and "K" of Ca II, which are most pronounced at G2. They have even weaker hydrogen lines than F, but along with the ionized metals, they have neutral metals. There is a prominent spike in the G band of CH molecules. Class G main-sequence stars make up about 7.5%, nearly one in thirteen, of the main-sequence stars in the solar neighborhood.

Class G contains the "Yellow Evolutionary Void". Supergiant stars often swing between O or B (blue) and K or M (red). While they do this, they do not stay for long in the yellow supergiant G class, as this is an extremely unstable place for a supergiant to be.

Spectral standards:

K-type stars are orangish stars that are slightly cooler than the Sun. They make up about 12% of the main-sequence stars in the solar neighborhood. There are also giant K-type stars, which range from hypergiants like RW Cephei, to giants and supergiants, such as Arcturus, whereas orange dwarfs, like Alpha Centauri B, are main-sequence stars.

They have extremely weak hydrogen lines, if those are present at all, and mostly neutral metals (Mn I, Fe I, Si I). By late K, molecular bands of titanium oxide become present. There is a suggestion that K-spectrum stars may potentially increase the chances of life developing on orbiting planets that are within the habitable zone.

Spectral standards:

Class M stars are by far the most common. About 76% of the main-sequence stars in the solar neighborhood are class M stars. However, class M main-sequence stars (red dwarfs) have such low luminosities that none are bright enough to be seen with the unaided eye, unless under exceptional conditions. The brightest known M-class main-sequence star is M0V Lacaille 8760, with magnitude 6.6 (the limiting magnitude for typical naked-eye visibility under good conditions is typically quoted as 6.5), and it is extremely unlikely that any brighter examples will be found.

Although most class M stars are red dwarfs, most of the largest ever supergiant stars in the Milky Way are M stars, such as VV Cephei, Antares, and Betelgeuse, which are also class M. Furthermore, the larger, hotter brown dwarfs are late class M, usually in the range of M6.5 to M9.5.

The spectrum of a class M star contains lines from oxide molecules (in the visible spectrum, especially TiO) and all neutral metals, but absorption lines of hydrogen are usually absent. TiO bands can be strong in class M stars, usually dominating their visible spectrum by about M5. Vanadium(II) oxide bands become present by late M.

Spectral standards:

A number of new spectral types have been taken into use from newly discovered types of stars.

Spectra of some very hot and bluish stars exhibit marked emission lines from carbon or nitrogen, or sometimes oxygen.

Once included as type O stars, the Wolf-Rayet stars of class W or WR are notable for spectra lacking hydrogen lines. Instead their spectra are dominated by broad emission lines of highly ionized helium, nitrogen, carbon, and sometimes oxygen. They are thought to mostly be dying supergiants with their hydrogen layers blown away by stellar winds, thereby directly exposing their hot helium shells. Class W is further divided into subclasses according to the relative strength of nitrogen and carbon emission lines in their spectra (and outer layers).

WR spectra range is listed below:

Although the central stars of most planetary nebulae (CSPNe) show O type spectra, around 10% are hydrogen-deficient and show WR spectra. These are low-mass stars and to distinguish them from the massive Wolf-Rayet stars, their spectra are enclosed in square brackets: e.g. [WC]. Most of these show [WC] spectra, some [WO], and very rarely [WN].

The "slash" stars are O-type stars with WN-like lines in their spectra. The name "slash" comes from their printed spectral type having a slash in it (e.g. "Of/WNL").

There is a secondary group found with this spectra, a cooler, "intermediate" group designated "Ofpe/WN9". These stars have also been referred to as WN10 or WN11, but that has become less popular with the realisation of the evolutionary difference from other Wolf–Rayet stars. Recent discoveries of even rarer stars have extended the range of slash stars as far as O2-3.5If/WN5-7, which are even hotter than the original "slash" stars.

They are O stars with strong magnetic fields. Designation is Of?p.

The new spectral types L, T, and Y were created to classify infrared spectra of cool stars. This includes both red dwarfs and brown dwarfs that are very faint in the visible spectrum.

Brown dwarfs, whose energy comes from gravitational attraction alone, cool as they age and so progress to later spectral types. Brown dwarfs start their lives with M-type spectra and will cool through the L, T, and Y spectral classes, faster the less massive they are; the highest-mass brown dwarfs cannot have cooled to Y or even T dwarfs within the age of the universe. Because this leads to an unresolvable overlap between spectral types effective temperature and luminosity for some masses and ages of different L-T-Y types, no distinct temperature or luminosity values can be given.

Class L dwarfs get their designation because they are cooler than M stars and L is the remaining letter alphabetically closest to M. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs. They are a very dark red in color and brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra.

Due to low surface gravity in giant stars, TiO- and VO-bearing condensates never form. Thus, L-type stars larger than dwarfs can never form in an isolated environment. However, it may be possible for these L-type supergiants to form through stellar collisions, an example of which is V838 Monocerotis while in the height of its luminous red nova eruption.

Class T dwarfs are cool brown dwarfs with surface temperatures between approximately . Their emission peaks in the infrared. Methane is prominent in their spectra.

Classes T and L could be more common than all the other classes combined if recent research is accurate. Because brown dwarfs persist for so long—a few times the age of the universe—in the absence of catastrophic collisions these smaller bodies can only increase in number.

Study of the number of proplyds (protoplanetary disks, clumps of gas in nebulae from which stars and planetary systems are formed) indicates that the number of stars in the galaxy should be several orders of magnitude higher than what was previously conjectured. It is theorized that these proplyds are in a race with each other. The first one to form will become a protostar, which are very violent objects and will disrupt other proplyds in the vicinity, stripping them of their gas. The victim proplyds will then probably go on to become main-sequence stars or brown dwarfs of the L and T classes, which are quite invisible to us.

Brown dwarfs of spectral class Y are cooler than those of spectral class T and have qualitatively different spectra from them. A total of 17 objects have been placed in class Y as of August 2013. Although such dwarfs have been modelled and detected within forty light-years by the Wide-field Infrared Survey Explorer (WISE) there is no well-defined spectral sequence yet and no prototypes. Nevertheless, several objects have been proposed as spectral classes Y0, Y1, and Y2.

The spectra of these prospective Y objects display absorption around 1.55 micrometers. Delorme et al. have suggested that this feature is due to absorption from ammonia, and that this should be taken as the indicative feature for the T-Y transition. In fact, this ammonia-absorption feature is the main criterion that has been adopted to define this class. However, this feature is difficult to distinguish from absorption by water and methane, and other authors have stated that the assignment of class Y0 is premature.

The latest brown dwarf proposed for the Y spectral type, WISE 1828+2650, is a > Y2 dwarf with an effective temperature originally estimated around 300 K, the temperature of the human body. Parallax measurements have, however, since shown that its luminosity is inconsistent with it being colder than ~400 K. The coolest Y dwarf currently known is WISE 0855−0714 with an approximate temperature of 250 K.

The mass range for Y dwarfs is 9–25 Jupiter masses, but young objects might reach below one Jupiter mass, which means that Y class objects straddle the 13 Jupiter mass deuterium-fusion limit that marks the current IAU division between brown dwarfs and planets.

Carbon-stars are stars whose spectra indicate production of carbon—a byproduct of triple-alpha helium fusion. With increased carbon abundance, and some parallel s-process heavy element production, the spectra of these stars become increasingly deviant from the usual late spectral classes G, K, and M. Equivalent classes for carbon-rich stars are S and C.

The giants among those stars are presumed to produce this carbon themselves, but some stars in this class are double stars, whose odd atmosphere is suspected of having been transferred from a companion that is now a white dwarf, when the companion was a carbon-star.

Originally classified as R and N stars, these are also known as "carbon stars". These are red giants, near the end of their lives, in which there is an excess of carbon in the atmosphere. The old R and N classes ran parallel to the normal classification system from roughly mid G to late M. These have more recently been remapped into a unified carbon classifier C with N0 starting at roughly C6. Another subset of cool carbon stars are the C-J type stars, which are characterized by the strong presence of molecules of CN in addition to those of CN. A few main-sequence carbon stars are known, but the overwhelming majority of known carbon stars are giants or supergiants. There are several subclasses:

Class S stars form a continuum between class M stars and carbon stars. Those most similar to class M stars have strong ZrO absorption bands analogous to the TiO bands of class M stars, whereas those most similar to carbon stars have strong sodium D lines and weak C bands. Class S stars have excess amounts of zirconium and other elements produced by the s-process, and have more similar carbon and oxygen abundances than class M or carbon stars. Like carbon stars, nearly all known class S stars are asymptotic-giant-branch stars.

The spectral type is formed by the letter S and a number between zero and ten. This number corresponds to the temperature of the star and approximately follows the temperature scale used for class M giants. The most common types are S3 to S5. The non-standard designation S10 has only been used for the star Chi Cygni when at an extreme minimum.

The basic classification is usually followed by an abundance indication, following one of several schemes: S2,5; S2/5; S2 Zr4 Ti2; or S2*5. A number following a comma is a scale between 1 and 9 based on the ratio of ZrO and TiO. A number following a slash is a more recent but less common scheme designed to represent the ratio of carbon to oxygen on a scale of 1 to 10, where a 0 would be an MS star. Intensities of zirconium and titanium may be indicated explicitly. Also occasionally seen is a number following an asterisk, which represents the strength of the ZrO bands on a scale from 1 to 5.

In between the M and S classes, border cases are named MS stars. In a similar way, border cases between the S and C-N classes are named SC or CS. The sequence M → MS → S → SC → C-N is hypothesized to be a sequence of increased carbon abundance with age for carbon stars in the asymptotic giant branch.

The class D (for Degenerate) is the modern classification used for white dwarfs – low-mass stars that are no longer undergoing nuclear fusion and have shrunk to planetary size, slowly cooling down. Class D is further divided into spectral types DA, DB, DC, DO, DQ, DX, and DZ. The letters are not related to the letters used in the classification of other stars, but instead indicate the composition of the white dwarf's visible outer layer or atmosphere.

The white dwarf types are as follows:

The type is followed by a number giving the white dwarf's surface temperature. This number is a rounded form of 50400/"T", where "T" is the effective surface temperature, measured in kelvins. Originally, this number was rounded to one of the digits 1 through 9, but more recently fractional values have started to be used, as well as values below 1 and above 9.

Two or more of the type letters may be used to indicate a white dwarf that displays more than one of the spectral features above.

Extended white dwarf spectral types:

A different set of spectral peculiarity symbols are used for white dwarfs than for other types of stars:

Finally, the classes P and Q, left over from the Draper system by Cannon, are occasionally used for certain non-stellar objects. Type P objects are stars within planetary nebulae and type Q objects are novae.

Stellar remnants are objects associated with the death of stars. Included in the category are white dwarfs, and as can be seen from the radically different classification scheme for class D, non-stellar objects are difficult to fit into the MK system.

The Hertzsprung-Russell diagram, which the MK system is based on, is observational in nature so these remnants cannot easily be plotted on the diagram, or cannot be placed at all. Old neutron stars are relatively small and cold, and would fall on the far right side of the diagram. Planetary nebulae are dynamic and tend to quickly fade in brightness as the progenitor star transitions to the white dwarf branch. If shown, a planetary nebula would be plotted to the right of the diagram's upper right quadrant. A black hole emits no visible light of its own, and therefore would not appear on the diagram.

A classification system for neutron stars using Roman numerals has been proposed: type I for less massive neutron stars with low cooling rates, type II for more massive neutron stars with higher cooling rates, and a proposed type III for more massive neutron stars (possible exotic star candidates) with higher cooling rates. The more massive a neutron star is, the higher neutrino flux it carries. These neutrinos carry away so much heat energy that after only a few years the temperature of an isolated neutron star falls from the order of billions to only around a million Kelvin. This proposed neutron star classification system is not to be confused with the earlier Secchi spectral classes and the Yerkes luminosity classes.

Several spectral types, all previously used for non-standard stars in the mid-20th century, have been replaced during revisions of the stellar classification system. They may still be found in old editions of star catalogs: R and N have been subsumed into the new C class as C-R and C-N.

Humans may eventually be able to colonize any kind of stellar habitat, this section will address the probability of life arising around other stars.

Stability, luminosity, and lifespan are all factors in stellar habitability. We only know of one star that hosts life, and that is our own -- a G class star with an abundance of heavy elements and low variability in brightness. It is also unlike many stellar systems in that it only has one star in it (see Planetary habitability, under the binary systems section).

Working from these constraints and the problems of having an empirical sample set of only one, the range of stars that are predicted to be able to support life as we know it is limited by a few factors. Of the main-sequence star types, stars more massive than 1.5 times that of the Sun (spectral types O, B, and A) age too quickly for advanced life to develop (using Earth as a guideline). On the other extreme, dwarfs of less than half the mass of our Sun (spectral type M) are likely to tidally lock planets within their habitable zone, along with other problems (see Habitability of red dwarf systems). While there are many problems facing life on red dwarfs, due to their sheer numbers and longevity, many astronomers continue to model these systems.

For these reasons NASA's Kepler Mission is searching for habitable planets at nearby main sequence stars that are less massive than spectral type A but more massive than type M -- making the most probable stars to host life dwarf stars of types F, G, and K.




</doc>
<doc id="28928" url="https://en.wikipedia.org/wiki?curid=28928" title="Sinope">
Sinope

Sinope may refer to:




</doc>
<doc id="28929" url="https://en.wikipedia.org/wiki?curid=28929" title="Seven Sisters">
Seven Sisters

Seven Sisters may refer to:














</doc>
<doc id="28930" url="https://en.wikipedia.org/wiki?curid=28930" title="SN 1987A">
SN 1987A

SN 1987A was a type II supernova in the Large Magellanic Cloud, a dwarf galaxy satellite of the Milky Way. It occurred approximately from Earth and was the closest observed supernova since Kepler's Supernova. 1987A's light reached Earth on February 23, 1987, and as the earliest supernova discovered that year, was labeled "1987A". Its brightness peaked in May, with an apparent magnitude of about 3.

It was the first supernova that modern astronomers were able to study in great detail, and its observations have provided much insight into core-collapse supernovae.

SN 1987A provided the first opportunity to confirm by direct observation the radioactive source of the energy for visible light emissions, by detecting predicted gamma-ray line radiation from two of its abundant radioactive nuclei. This proved the radioactive nature of the long-duration post-explosion glow of supernovae.

For over thirty years, the expected collapsed neutron star could not be found, but in 2019 it was announced found using the ALMA telescope.

SN 1987A was discovered independently by Ian Shelton and Oscar Duhalde at the Las Campanas Observatory in Chile on February 24, 1987, and within the same 24 hours by Albert Jones in New Zealand.

Later investigations found photographs showing the supernova brightening rapidly early on February 23rd. On March 4–12, 1987, it was observed from space by "Astron", the largest ultraviolet space telescope of that time.

[[File:New Hubble Observations of Supernova 1987A Trace Shock Wave (4954621859).jpg|thumb|left|The remnant of SN 1987A]]
Four days after the event was recorded, the progenitor star was tentatively identified as Sanduleak −69 202 (Sk -69 202), a [[blue supergiant]].
After the supernova faded, that identification was definitively confirmed by Sk −69 202 having disappeared. This was an unexpected identification, because models of [[Stellar evolution#Massive stars|high mass stellar evolution]] at the time did not predict that blue supergiants are susceptible to a supernova event.

Some models of the progenitor attributed the color to its chemical composition rather than its evolutionary state, particularly the low levels of heavy elements, among other factors. There was some speculation that the star might have merged with a [[Binary star|companion star]] before the supernova. However, it is now widely understood that blue supergiants are natural progenitors of some supernovae, although there is still speculation that the evolution of such stars could require mass loss involving a binary companion.

[[Image:Composite image of Supernova 1987A.jpg|thumb|left|Remnant of SN 1987A seen in light overlays of different spectra. [[Atacama Large Millimeter Array|ALMA]] data ([[Radio astronomy|radio]], in red) shows newly formed dust in the center of the remnant. [[Hubble Space Telescope|Hubble]] ([[Visible-light astronomy|visible]], in green) and [[Chandra X-ray Observatory|Chandra]] ([[X-ray astronomy|X-ray]], in blue) data show the expanding [[shock wave]].]]
Approximately two to three hours before the visible light from SN 1987A reached Earth, a burst of [[neutrino]]s was observed at three [[neutrino detector|neutrino observatories]]. This was likely due to neutrino emission, which occurs simultaneously with core collapse, but before visible light was emitted. Visible light is transmitted only after the shock wave reaches the stellar surface. At 07:35 [[Universal Time|UT]], [[Kamiokande II]] detected 12 [[antineutrino]]s; [[Irvine-Michigan-Brookhaven (detector)|IMB]], 8 antineutrinos; and [[Baksan Neutrino Observatory|Baksan]], 5 antineutrinos; in a burst lasting less than 13 seconds. Approximately three hours earlier, the [[Mont Blanc]] [[Neutrino detector#Scintillators|liquid scintillator]] detected a five-neutrino burst, but this is generally not believed to be associated with SN 1987A.

The Kamiokande II detection, which at 12 neutrinos had the largest sample population, showed the neutrinos arriving in two distinct pulses. The first pulse started at 07:35:35 and comprised 9 neutrinos, all of which arrived over a period of 1.915 seconds. A second pulse of three neutrinos arrived between 9.219 and 12.439 seconds after the first neutrino was detected, for a pulse duration of 3.220 seconds.

Although only 25 neutrinos were detected during the event, it was a significant increase from the previously observed background level. This was the first time neutrinos known to be emitted from a supernova had been observed directly, which marked the beginning of [[neutrino astronomy]]. The observations were consistent with theoretical supernova models in which 99% of the energy of the collapse is radiated away in the form of neutrinos. The observations are also consistent with the models' estimates of a total neutrino count of 10 with a total energy of 10 joules, i.e. a mean value of some dozens of MeV per neutrino.

The neutrino measurements allowed upper bounds on neutrino mass and charge, as well as the number of flavors of neutrinos and other properties. For example, the data show that within 5% confidence, the rest mass of the electron neutrino is at most 16 eV/c, 1/30,000 the mass of an electron. The data suggest that the total number of neutrino flavors is at most 8 but other observations and experiments give tighter estimates. Many of these results have since been confirmed or tightened by other neutrino experiments such as more careful analysis of solar neutrinos and atmospheric neutrinos as well as experiments with artificial neutrino sources.

[[File:New image of SN 1987A.jpg|thumb|The bright ring around the central region of the exploded star is composed of ejected material.]]

SN 1987A appears to be a core-collapse supernova, which should result in a [[neutron star]] given the size of the original star. The neutrino data indicate that a compact object did form at the star's core. Since the supernova first became visible, astronomers have been searching for the collapsed core. The [[Hubble Space Telescope]] has taken images of the supernova regularly since August 1990 without a clear detection of a neutron star.

A number of possibilities for the "missing" neutron star are being considered. The first is that the neutron star is enshrouded in dense dust clouds so that it cannot be seen. Another is that a [[pulsar]] was formed, but with either an unusually large or small magnetic field. It is also possible that large amounts of material fell back on the neutron star, so that it further collapsed into a [[black hole]]. Neutron stars and black holes often give off light as material falls onto them. If there is a compact object in the supernova remnant, but no material to fall onto it, it would be very dim and could therefore avoid detection. Other scenarios have also been considered, such as whether the collapsed core became a [[quark star]]. In 2019, evidence was presented that a neutron star was inside one of the brightest dust clumps close to the expected position of the supernova remnant.

Much of the [[light curve]], or graph of luminosity as a function of time, after the explosion of a [[type II supernova]] such as SN 1987A is provided its energy by [[radioactive decay]]. Although the luminous emission consists of optical photons, it is the radioactive power absorbed that keeps the remnant hot enough to radiate light. Without radioactive heat it would quickly dim. The radioactive decay of [[isotopes of nickel|Ni]] through its daughters [[isotopes of cobalt|Co]] to [[isotopes of iron|Fe]] produces gamma-ray [[photon]]s that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of Ni to Co (half life of 6 days) while energy for the later light curve in particular fit very closely with the 77.3-day half-life of Co decaying to Fe. Later measurements by space gamma-ray telescopes of the small fraction of the Co and Co gamma rays that escaped the SN1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power source.

Because the Co in SN1987A has now completely decayed, it no longer supports the luminosity of the SN 1987A ejecta. That is currently powered by the radioactive decay of [[Isotopes of titanium|Ti]] with a half life of about 60 years. With this change, X-rays produced by the ring interactions of the ejecta began to contribute significantly to the total light curve. This was noticed by the Hubble Space Telescope as a steady increase in luminosity 10,000 days after the event in the blue and red spectral bands. X-ray lines Ti observed by the [[INTEGRAL]] space X-ray telescope showed that the total mass of radioactive Ti synthesized during the explosion was .

Observations of the radioactive power from their decays in the 1987A light curve have measured accurate total masses of the Ni, Ni, and Ti created in the explosion, which agree with the masses measured by gamma-ray line space telescopes and provides nucleosynthesis constraints on the computed supernova model.

[[Image:Sn87a.jpg|thumb|left|upright=1.3|The expanding ring-shaped [[Supernova remnant|remnant]] of SN 1987A and its interaction with its surroundings, seen in X-ray and visible light.]]
[[File:SN1987a debris evolution animation time scaled.gif|thumb|Sequence of [[Hubble Space Telescope|HST]] images from 1994 to 2009, showing the collision of the expanding [[supernova remnant|remnant]] with a ring of material ejected by the progenitor 20,000 years before the supernova]]

The three bright rings around SN 1987A that were visible after a few months in images by the Hubble Space Telescope are material from the [[stellar wind]] of the progenitor. These rings were ionized by the ultraviolet flash from the supernova explosion, and consequently began emitting in various emission lines. These rings did not "turn on" until several months after the supernova; the turn-on process can be very accurately studied through spectroscopy. The rings are large enough that their angular size can be measured accurately: the inner ring is 0.808 arcseconds in radius. The time light traveled to light up the inner ring gives its radius of 0.66 (ly) [[light years]]. Using this as the base of a right angle triangle and the angular size as seen from the Earth for the local angle, one can use basic trigonometry to calculate the distance to SN 1987A, which is about 168,000 light-years. The material from the explosion is catching up with the material expelled during both its red and blue supergiant phases and heating it, so we observe ring structures about the star.

Around 2001, the expanding (>7000 km/s) supernova ejecta collided with the inner ring. This caused its heating and the generation of x-rays—the x-ray flux from the ring increased by a factor of three between 2001 and 2009. A part of the x-ray radiation, which is absorbed by the dense ejecta close to the center, is responsible for a comparable increase in the optical flux from the supernova remnant in 2001–2009. This increase of the brightness of the remnant reversed the trend observed before 2001, when the optical flux was decreasing due to the decaying of [[Isotopes of titanium|Ti]] isotope.

A study reported in June 2015, using images from the Hubble Space Telescope and the [[Very Large Telescope]] taken between 1994 and 2014, shows that the emissions from the clumps of matter making up the rings are fading as the clumps are destroyed by the shock wave. It is predicted the ring will fade away between 2020 and 2030. These findings are also supported by the results of a three-dimensional hydrodynamic model which describes the interaction of the blast wave with the circumstellar nebula. 
The model also shows that X-ray emission from ejecta heated up by the shock will be dominant very soon, after the ring will fade away. As the shock wave passes the circumstellar ring it will trace the history of mass loss of the supernova's progenitor and provide useful information for discriminating among various models for the progenitor of SN 1987A.

In 2018, radio observations from the interaction between the circumstellar ring of dust and the shockwave has confirmed the shockwave has now left the circumstellar material. It also shows that the speed of the shockwave, which slowed down to 2,300 km/s while interacting with the dust in the ring, has now re-accelerated to 3,600 km/s.

[[File:Images of the Warm Dust in the SN 1987A debris.png|thumb|left|Images of the SN 1987A debris obtained with the instruments T-ReCS at the 8-m Gemini telescope and VISIR at one of the four VLT. Dates are indicated. An HST image is inserted at the bottom right (credits Patrice Bouchet, CEA-Saclay)]]

Soon after the SN 1987A outburst, three major groups embarked in a photometric monitoring of the supernova: [[SAAO]], [[CTIO]], and [[ESO]]. In particular, the ESO team reported an [[infrared excess]] which became apparent beginning less than one month after the explosion (March 11, 1987). Three possible interpretations for it were discussed in this work: the infrared echo hypothesis was discarded, and [[thermal emission]] from dust that could have condensed in the ejecta was favoured (in which case the estimated temperature at that epoch was ~ 1250 K, and the dust mass was approximately ). The possibility that the IR excess could be produced by optically thick [[free-free emission]] seemed unlikely because the luminosity in UV photons needed to keep the envelope ionized was much larger than what was available, but it was not ruled out in view of the eventuality of electron scattering, which had not been considered.

However, none of these three groups had sufficiently convincing proofs to claim for a dusty ejecta on the basis of an IR excess alone. 
[[File:Model of the dust distribution.png|thumb|right|Distribution of the dust inside the SN 1987A ejecta, as from the Lucy et al.'s model built at ESO
An independent Australian team advanced several argument in favour of an echo interpretation. This seemingly straightforward interpretation of the nature of the IR emission was challenged by the ESO group and definitively ruled out after presenting optical evidence for the presence of dust in the SN ejecta.
To discriminate between the two interpretations, they considered the implication of the presence of an echoing dust cloud on the optical light curve, and on the existence of diffuse optical emission around the SN. They concluded that the expected optical echo from the cloud should be resolvable, and could be very bright with an integrated visual brightness of [[apparent magnitude|magnitude]] 10.3 around day 650. However, further optical observations, as expressed in SN light curve, showed no [[inflection point|inflection]] in the light curve at the predicted level. Finally, the ESO team presented a convincing clumpy model for dust condensation in the ejecta.

Although it had been thought more than 50 years ago that dust could form in the ejecta of a core-collapse supernova, which in particular could explain the origin of the dust seen in young galaxies, that was the first time that such a condensation was observed. If SN 1987A is a typical representative of its class then the derived mass of the warm dust formed in the debris of core collapse supernovae is not sufficient to account for all the dust observed in the early universe. However, a much larger reservoir of ~0.25 solar mass of colder dust (at ~26 K) in the ejecta of SN 1987A was found with the Hershel infrared space telescope in 2011 and confirmed by ALMA later on (in 2014).

Following the confirmation of a large amount of cold dust in the ejecta, ALMA has continued observing SN 1987A. Synchrotron radiation due to shock interaction in the equatorial ring has been measured. Cold (20–100K) carbon monoxide (CO) and silicate molecules (SiO) were observed. The data show that CO and SiO distributions are clumpy, and that different nucleosynthesis products (C, O and Si) are located in different places of the ejecta, indicating the footprints of the stellar interior at the time of the explosion.



[[Category:Supernova remnants]]
[[Category:Supernovae]]
[[Category:Large Magellanic Cloud]]
[[Category:Astronomical objects discovered in 1987]]

</doc>
<doc id="28931" url="https://en.wikipedia.org/wiki?curid=28931" title="Standard Oil">
Standard Oil

Standard Oil Co. Inc. was an American oil producing, transporting, refining, marketing company, and monopoly. Established in 1870 by John D. Rockefeller and Henry Flagler as a corporation in Ohio, it was the largest oil refiner in the world of its time. Its history as one of the world's first and largest multinational corporations ended in 1911, when the U.S. Supreme Court ruled, in a landmark case, that Standard Oil was an illegal monopoly.

Standard Oil dominated the oil products market initially through horizontal integration in the refining sector, then, in later years vertical integration; the company was an innovator in the development of the business trust. The Standard Oil trust streamlined production and logistics, lowered costs, and undercut competitors. "Trust-busting" critics accused Standard Oil of using aggressive pricing to destroy competitors and form a monopoly that threatened other businesses.

Rockefeller ran the company as its chairman, until his retirement in 1897. He remained the major shareholder, and in 1911, with the dissolution of the Standard Oil trust into 34 smaller companies, Rockefeller became the richest person in modern history, as the initial income of these individual enterprises proved to be much bigger than that of a single larger company. Its successors such as ExxonMobil, Marathon Petroleum, Amoco, or Chevron are still among the companies with the largest revenues in the world. By 1882, his top aide was John Dustin Archbold. After 1896, Rockefeller disengaged from business to concentrate on his philanthropy, leaving Archbold in control. Other notable Standard Oil principals include Henry Flagler, developer of the Florida East Coast Railway and resort cities, and Henry H. Rogers, who built the Virginian Railway.

Standard Oil's pre-history began in 1863 as an Ohio partnership formed by industrialist John D. Rockefeller, his brother William Rockefeller, Henry Flagler, chemist Samuel Andrews, silent partner Stephen V. Harkness, and Oliver Burr Jennings, who had married the sister of William Rockefeller's wife. In 1870, Rockefeller abolished the partnership and incorporated Standard Oil in Ohio. Of the initial 10,000 shares, John D. Rockefeller received 2,667; Harkness received 1,334; William Rockefeller, Flagler, and Andrews received 1,333 each; Jennings received 1,000, and the firm of Rockefeller, Andrews & Flagler received 1,000. Rockefeller chose the "Standard Oil" name as a symbol of the reliable "standards" of quality and service that he envisioned for the nascent oil industry.

In the early years, John D. Rockefeller dominated the combine; he was the single most important figure in shaping the new oil industry. He quickly distributed power and the tasks of policy formation to a system of committees, but always remained the largest shareholder. Authority was centralized in the company's main office in Cleveland, but decisions in the office were made in a cooperative way.

The company grew by increasing sales and through acquisitions. After purchasing competing firms, Rockefeller shut down those he believed to be inefficient and kept the others. In a seminal deal, in 1868, the Lake Shore Railroad, a part of the New York Central, gave Rockefeller's firm a going rate of one cent a gallon or forty-two cents a barrel, an effective 71% discount from its listed rates in return for a promise to ship at least 60 carloads of oil daily and to handle load and unload on its own. Smaller companies decried such deals as unfair because they were not producing enough oil to qualify for discounts.

Standard's actions and secret transport deals helped its kerosene price to drop from 58 to 26 cents from 1865 to 1870. Rockefeller used the Erie Canal as a cheap alternative form of transportation—in the summer months when it was not frozen—to ship his refined oil from Cleveland to New York City. In the winter months his only options were the three trunk lines—the Erie Railroad and the New York Central Railroad to New York City, and the Pennsylvania Railroad to Philadelphia. Competitors disliked the company's business practices, but consumers liked the lower prices. Standard Oil, being formed well before the discovery of the Spindletop oil field (in Texas, far from Standard Oil's base in the Midwest) and a demand for oil other than for heat and light, was well placed to control the growth of the oil business. The company was perceived to own and control all aspects of the trade.

In 1872, Rockefeller joined the South Improvement Co. which would have allowed him to receive rebates for shipping and drawbacks on oil his competitors shipped. But when this deal became known, competitors convinced the Pennsylvania Legislature to revoke South Improvement's charter. No oil was ever shipped under this arrangement. Using highly effective tactics, later widely criticized, it absorbed or destroyed most of its competition in Cleveland in less than two months and later throughout the northeastern United States.

A. Barton Hepburn was directed by the New York State Legislature in 1879 to investigate the railroads' practice of giving rebates within the state. Merchants without ties to the oil industry had pressed for the hearings. Prior to the committee's investigation, few knew of the size of Standard Oil's control and influence on seemingly unaffiliated oil refineries and pipelines—Hawke (1980) cites that only a dozen or so within Standard Oil knew the extent of company operations. The committee counsel, Simon Sterne, questioned representatives from the Erie Railroad and the New York Central Railroad and discovered that at least half of their long-haul traffic granted rebates, and that much of this traffic came from Standard Oil. The committee then shifted focus to Standard Oil's operations. John Dustin Archbold, as president of Acme Oil Company, denied that Acme was associated with Standard Oil. He then admitted to being a director of Standard Oil. The committee's final report scolded the railroads for their rebate policies and cited Standard Oil as an example. This scolding was largely moot to Standard Oil's interests since long-distance oil pipelines were now their preferred method of transportation.

In response to state laws trying to limit the scale of companies, Rockefeller and his associates developed innovative ways of organizing, to effectively manage their fast growing enterprise. On January 2, 1882, they combined their disparate companies, spread across dozens of states, under a single group of trustees. By a secret agreement, the existing 37 stockholders conveyed their shares "in trust" to nine trustees: John and William Rockefeller, Oliver H. Payne, Charles Pratt, Henry Flagler, John D. Archbold, William G. Warden, Jabez Bostwick, and Benjamin Brewster. This organization proved so successful that other giant enterprises adopted this "trust" form.

In 1885, Standard Oil of Ohio moved its headquarters from Cleveland to its permanent headquarters at 26 Broadway in New York City. Concurrently, the trustees of Standard Oil of Ohio chartered the Standard Oil Co. of New Jersey (SOCNJ) to take advantages of New Jersey's more lenient corporate stock ownership laws.

In 1890, Congress overwhelmingly passed the Sherman Antitrust Act (Senate 51-1; House 242-0), a source of American anti-monopoly laws. The law forbade every contract, scheme, deal, or conspiracy to restrain trade, though the phrase "restraint of trade" remained subjective. The Standard Oil group quickly attracted attention from antitrust authorities leading to a lawsuit filed by Ohio Attorney General David K. Watson.

From 1882 to 1906, Standard paid out $548,436,000 in dividends at 65.4% payout ratio. The total net earnings from 1882 to 1906 amounted to $838,783,800, exceeding the dividends by $290,347,800, which was used for plant expansions.

In 1896, John Rockefeller retired from the Standard Oil Co. of New Jersey, the holding company of the group, but remained president and a major shareholder. Vice-president John Dustin Archbold took a large part in the running of the firm. In the year 1904, Standard Oil controlled 91% of oil refinement and 85% of final sales in the United States. At this point in time, state and federal laws sought to counter this development with antitrust laws. In 1911, the U.S. Justice Department sued the group under the federal antitrust law and ordered its breakup into 34 companies.

Standard Oil's market position was initially established through an emphasis on efficiency and responsibility. While most companies dumped gasoline in rivers (this was before the automobile was popular), Standard used it to fuel its machines. While other companies' refineries piled mountains of heavy waste, Rockefeller found ways to sell it. For example, Standard created the first synthetic competitor for beeswax and bought the company that invented and produced Vaseline, the Chesebrough Manufacturing Co., which was a Standard company only from 1908 until 1911.

One of the original "Muckrakers" was Ida M. Tarbell, an American author and journalist. Her father was an oil producer whose business had failed because of Rockefeller's business dealings. After extensive interviews with a sympathetic senior executive of Standard Oil, Henry H. Rogers, Tarbell's investigations of Standard Oil fueled growing public attacks on Standard Oil and on monopolies in general. Her work was published in 19 parts in "McClure's" magazine from November 1902 to October 1904, then in 1904 as the book "The History of the Standard Oil Co".

The Standard Oil Trust was controlled by a small group of families. Rockefeller stated in 1910: "I think it is true that the Pratt family, the Payne–Whitney family (which were one, as all the stock came from Colonel Payne), the Harkness-Flagler family (which came into the company together) and the Rockefeller family controlled a majority of the stock during all the history of the company up to the present time."

These families reinvested most of the dividends in other industries, especially railroads. They also invested heavily in the gas and the electric lighting business (including the giant Consolidated Gas Co. of New York City). They made large purchases of stock in U.S. Steel, Amalgamated Copper, and even Corn Products Refining Co.

Weetman Pearson, a British petroleum entrepreneur in Mexico, began negotiating with Standard Oil in 1912–13 to sell his "El Aguila" oil company, since Pearson was no longer bound to promises to the Porfirio Díaz regime (1876–1911) to not to sell to U.S. interests. However, the deal fell through and the firm was sold to Royal Dutch Shell.

Standard Oil's production increased so rapidly it soon exceeded U.S. demand and the company began viewing export markets. In the 1890s, Standard Oil began marketing kerosene to China's large population of close to 400 million as lamp fuel. For its Chinese trademark and brand Standard Oil adopted the name "Mei Foo" (), (which translates to Mobil). Mei Foo also became the name of the tin lamp that Standard Oil produced and gave away or sold cheaply to Chinese farmers, encouraging them to switch from vegetable oil to kerosene. Response was positive, sales boomed and China became Standard Oil's largest market in Asia. Prior to Pearl Harbor, Stanvac was the largest single U.S. investment in Southeast Asia.

The North China Department of Socony (Standard Oil Company of New York) operated a subsidiary called Socony River and Coastal Fleet, North Coast Division, which became the North China Division of Stanvac (Standard Vacuum Oil Company) after that company was formed in 1933. To distribute its products, Standard Oil constructed storage tanks, canneries (bulk oil from large ocean tankers was re-packaged into tins), warehouses and offices in key Chinese cities. For inland distribution the company had motor tank trucks and railway tank cars, and for river navigation it had a fleet of low-draft steamers and other vessels.

Stanvac's North China Division, based in Shanghai, owned hundreds of river going vessels, including motor barges, steamers, launches, tugboats and tankers. Up to 13 tankers operated on the Yangtze River, the largest of which were "Mei Ping" (), "Mei Hsia" (), and "Mei An" (). All three were destroyed in the 1937 USS "Panay" incident. "Mei An" was launched in 1901 and was the first vessel in the fleet. Other vessels included "Mei Chuen", "Mei Foo", "Mei Hung", "Mei Kiang", "Mei Lu", "Mei Tan", "Mei Su", "Mei Xia", "Mei Ying", and "Mei Yun". "Mei Hsia", a tanker, was specially designed for river duty and was built by New Engineering and Shipbuilding Works of Shanghai, who also built the 500-ton launch "Mei Foo" in 1912. "Mei Hsia" ("Beautiful Gorges") was launched in 1926 and carried 350 tons of bulk oil in three holds, plus a forward cargo hold, and space between decks for carrying general cargo or packed oil. She had a length of , a beam of , depth of , and had a bullet-proof wheelhouse. "Mei Ping" ("Beautiful Tranquility"), launched in 1927, was designed offshore, but assembled and finished in Shanghai. Its oil-fuel burners came from the U.S. and water-tube boilers came from England.

Standard Oil Company and Socony-Vacuum Oil Company became partners in providing markets for the oil reserves in the Middle East. In 1906, SOCONY (later Mobil) opened its first fuel terminals in Alexandria. It explored in Palestine before the World War broke out, but ran into conflict with the British government.

By 1890, Standard Oil controlled 88 percent of the refined oil flows in the United States. The state of Ohio successfully sued Standard, compelling the dissolution of the trust in 1892. But Standard simply separated Standard Oil of Ohio and kept control of it. Eventually, the state of New Jersey changed its incorporation laws to allow a company to hold shares in other companies in any state. So, in 1899, the Standard Oil Trust, based at 26 Broadway in New York, was legally reborn as a holding company, the "Standard Oil Co. of New Jersey" (SOCNJ), which held stock in 41 other companies, which controlled other companies, which in turn controlled yet other companies. According to Daniel Yergin in his Pulitzer Prize-winning "" (1990), this conglomerate was seen by the public as all-pervasive, controlled by a select group of directors, and completely unaccountable.

In 1904, Standard controlled 91 percent of production and 85 percent of final sales. Most of its output was kerosene, of which 55 percent was exported around the world. After 1900 it did not try to force competitors out of business by underpricing them. The federal Commissioner of Corporations studied Standard's operations from the period of 1904 to 1906 and concluded that "beyond question ... the dominant position of the Standard Oil Co. in the refining industry was due to unfair practices—to abuse of the control of pipe-lines, to railroad discriminations, and to unfair methods of competition in the sale of the refined petroleum products". Because of competition from other firms, their market share had gradually eroded to 70 percent by 1906 which was the year when the antitrust case was filed against Standard, and down to 64 percent by 1911 when Standard was ordered broken up and at least 147 refining companies were competing with Standard including Gulf, Texaco, and Shell. It did not try to monopolize the exploration and pumping of oil (its share in 1911 was 11 percent).

In 1909, the U.S. Justice Department sued Standard under federal antitrust law, the Sherman Antitrust Act of 1890, for sustaining a monopoly and restraining interstate commerce by:

Rebates, preferences, and other discriminatory practices in favor of the combination by railroad companies; restraint and monopolization by control of pipe lines, and unfair practices against competing pipe lines; contracts with competitors in restraint of trade; unfair methods of competition, such as local price cutting at the points where necessary to suppress competition; [and] espionage of the business of competitors, the operation of bogus independent companies, and payment of rebates on oil, with the like intent.

The lawsuit argued that Standard's monopolistic practices had taken place over the preceding four years:

The general result of the investigation has been to disclose the existence of numerous and flagrant discriminations by the railroads in behalf of the Standard Oil Co. and its affiliated corporations. With comparatively few exceptions, mainly of other large concerns in California, the Standard has been the sole beneficiary of such discriminations. In almost every section of the country that company has been found to enjoy some unfair advantages over its competitors, and some of these discriminations affect enormous areas.

The government identified four illegal patterns: (1) secret and semi-secret railroad rates; (2) discriminations in the open arrangement of rates; (3) discriminations in classification and rules of shipment; (4) discriminations in the treatment of private tank cars. The government alleged:

Almost everywhere the rates from the shipping points used exclusively, or almost exclusively, by the Standard are relatively lower than the rates from the shipping points of its competitors. Rates have been made low to let the Standard into markets, or they have been made high to keep its competitors out of markets. Trifling differences in distances are made an excuse for large differences in rates favorable to the Standard Oil Co., while large differences in distances are ignored where they are against the Standard. Sometimes connecting roads prorate on oil—that is, make through rates which are lower than the combination of local rates; sometimes they refuse to prorate; but in either case the result of their policy is to favor the Standard Oil Co. Different methods are used in different places and under different conditions, but the net result is that from Maine to California the general arrangement of open rates on petroleum oil is such as to give the Standard an unreasonable advantage over its competitors.

The government said that Standard raised prices to its monopolistic customers but lowered them to hurt competitors, often disguising its illegal actions by using bogus supposedly independent companies it controlled.

The evidence is, in fact, absolutely conclusive that the Standard Oil Co. charges altogether excessive prices where it meets no competition, and particularly where there is little likelihood of competitors entering the field, and that, on the other hand, where competition is active, it frequently cuts prices to a point which leaves even the Standard little or no profit, and which more often leaves no profit to the competitor, whose costs are ordinarily somewhat higher.

On May 15, 1911, the US Supreme Court upheld the lower court judgment and declared the Standard Oil group to be an "unreasonable" monopoly under the Sherman Antitrust Act, Section II. It ordered Standard to break up into 34 independent companies with different boards of directors, the biggest two of the companies were Standard Oil of New Jersey (which became Exxon) and Standard Oil of New York (which became Mobil).

Standard's president, John D. Rockefeller, had long since retired from any management role. But, as he owned a quarter of the shares of the resultant companies, and those share values mostly doubled, he emerged from the dissolution as the richest man in the world. The dissolution had actually propelled Rockefeller's personal wealth.

By 1911, with public outcry at a climax, the Supreme Court of the United States ruled, in "Standard Oil Co. of New Jersey v. United States", that Standard Oil of New Jersey must be dissolved under the Sherman Antitrust Act and split into 34 companies. Two of these companies were Standard Oil of New Jersey (Jersey Standard or Esso), which eventually became Exxon, and Standard Oil of New York (Socony), which eventually became Mobil; those two companies later merged into ExxonMobil.

Over the next few decades, both companies grew significantly. Jersey Standard, led by Walter C. Teagle, became the largest oil producer in the world. It acquired a 50 percent share in Humble Oil & Refining Co., a Texas oil producer. Socony purchased a 45 percent interest in Magnolia Petroleum Co., a major refiner, marketer and pipeline transporter. In 1931, Socony merged with Vacuum Oil Co., an industry pioneer dating back to 1866, and a growing Standard Oil spin-off in its own right.

In the Asia-Pacific region, Jersey Standard had oil production and refineries in Indonesia but no marketing network. Socony-Vacuum had Asian marketing outlets supplied remotely from California. In 1933, Jersey Standard and Socony-Vacuum merged their interests in the region into a 50–50 joint venture. Standard-Vacuum Oil Co., or "Stanvac", operated in 50 countries, from East Africa to New Zealand, before it was dissolved in 1962.

The original Standard Oil Company corporate entity continues in existence and was the operating entity for Sohio; it is now a subsidiary of BP. BP continued to sell gasoline under the Sohio brand until 1991. Other Standard oil entities include "Standard Oil of Indiana" which became Amoco after other mergers and a name change in the 1980s, and "Standard Oil of California" which became the Chevron Corp.

The U.S. Supreme Court ruled in 1911 that antitrust law required Standard Oil to be broken into smaller, independent companies. Among the "baby Standards" that still exist are ExxonMobil and Chevron. Some have speculated that if not for that court ruling, Standard Oil could have possibly been worth more than $1 trillion in the 2000s.
Whether the breakup of Standard Oil was beneficial is a matter of some controversy.
Some economists believe that Standard Oil was not a monopoly, and also argue that the intense free market competition resulted in cheaper oil prices and more diverse petroleum products. Critics claimed that success in meeting consumer needs was driving other companies out of the market who were not as successful. An example of this thinking was given in 1890 when Rep. William Mason, arguing in favor of the Sherman Antitrust Act, said: "trusts have made products cheaper, have reduced prices; but if the price of oil, for instance, were reduced to one cent a barrel, it would not right the wrong done to people of this country by the "trusts" which have destroyed legitimate competition and driven honest men from legitimate business enterprise".

The Sherman Antitrust Act prohibits the restraint of trade. Defenders of Standard Oil insist that the company did not restrain trade; they were simply superior competitors. The federal courts ruled otherwise.

Some economic historians have observed that Standard Oil was in the process of losing its monopoly at the time of its breakup in 1911. Although Standard had 90 percent of American refining capacity in 1880, by 1911 that had shrunk to between 60 and 65 percent because of the expansion in capacity by competitors. Numerous regional competitors (such as Pure Oil in the East, Texaco and Gulf Oil in the Gulf Coast, Cities Service and Sun in the Midcontinent, Union in California, and Shell overseas) had organized themselves into competitive vertically integrated oil companies, the industry structure pioneered years earlier by Standard itself. In addition, demand for petroleum products was increasing more rapidly than the ability of Standard to expand. The result was that although in 1911 Standard still controlled most production in the older regions of the Appalachian Basin (78 percent share, down from 92 percent in 1880), Lima-Indiana (90 percent, down from 95 percent in 1906), and the Illinois Basin (83 percent, down from 100 percent in 1906), its share was much lower in the rapidly expanding new regions that would dominate U.S. oil production in the 20th century. In 1911 Standard controlled only 44 percent of production in the Midcontinent, 29 percent in California, and 10 percent on the Gulf Coast.

Some analysts argue that the breakup was beneficial to consumers in the long run, and no one has ever proposed that Standard Oil be reassembled in pre-1911 form. ExxonMobil, however, does represent a substantial part of the original company.

Since the breakup of Standard Oil, several companies, such as General Motors and Microsoft, have come under antitrust investigation for being inherently too large for market competition; however, most of them remained together. The only company since the breakup of Standard Oil that was divided into parts like Standard Oil was AT&T, which after decades as a regulated natural monopoly, was forced to divest itself of the Bell System in 1984.

The successor companies from Standard Oil's breakup form the core of today's US oil industry. (Several of these companies were considered among the Seven Sisters who dominated the industry worldwide for much of the 20th century.) They include:

Other Standard Oil spin-offs:

Other companies divested in the 1911 breakup:


Note: Standard Oil of Colorado was not a successor company; the name was used to capitalize on the Standard Oil brand in the 1930s. Standard Oil of Connecticut is a fuel oil marketer not related to the Rockefeller companies.

Of the 34 "Baby Standards", 11 were given rights to the Standard Oil name, based on the state they were in. Conoco and Atlantic elected to use their respective names instead of the Standard name, and their rights would be claimed by other companies.

By the 1980s, most companies were using their individual brand names instead of the Standard name, with Amoco being the last one to have widespread use of the "Standard" name, as it gave Midwestern owners the option of using the Amoco name or Standard.

Three supermajor companies now own the rights to the Standard name in the United States: ExxonMobil, Chevron Corp., and BP. BP acquired its rights through acquiring Standard Oil of Ohio and Amoco, and has a small handful of stations in the Midwestern United States using the Standard name. Likewise, BP continues to sell marine fuel under the Sohio brand at various marinas throughout Ohio. ExxonMobil keeps the Esso trademark alive at stations that sell diesel fuel by selling "Esso Diesel" displayed on the pumps. ExxonMobil has full international rights to the Standard name, and continues to use the Esso name overseas and in Canada. To protect its trademark Chevron has one station in each state it owns the rights to branded as Standard. Some of its Standard-branded stations have a mix of some signs that say Standard and some signs that say Chevron. Over time, Chevron has changed which station in a given state is the Standard station.





</doc>
<doc id="28935" url="https://en.wikipedia.org/wiki?curid=28935" title="Seismology">
Seismology

Seismology (; from Ancient Greek σεισμός ("seismós") meaning "earthquake" and -λογία ("-logía") meaning "study of") is the scientific study of earthquakes and the propagation of elastic waves through the Earth or through other planet-like bodies. The field also includes studies of earthquake environmental effects such as tsunamis as well as diverse seismic sources such as volcanic, tectonic, oceanic, atmospheric, and artificial processes such as explosions. A related field that uses geology to infer information regarding past earthquakes is paleoseismology. A recording of earth motion as a function of time is called a seismogram. A seismologist is a scientist who does research in seismology.

Scholarly interest in earthquakes can be traced back to antiquity. Early speculations on the natural causes of earthquakes were included in the writings of Thales of Miletus (c. 585 BCE), Anaximenes of Miletus (c. 550 BCE), Aristotle (c. 340 BCE) and Zhang Heng (132 CE).

In 132 CE, Zhang Heng of China's Han dynasty designed the first known seismoscope.

In the 17th century, Athanasius Kircher argued that earthquakes were caused by the movement of fire within a system of channels inside the Earth. Martin Lister (1638 to 1712) and Nicolas Lemery (1645 to 1715) proposed that earthquakes were caused by chemical explosions within the earth.

The Lisbon earthquake of 1755, coinciding with the general flowering of science in Europe, set in motion intensified scientific attempts to understand the behaviour and causation of earthquakes. The earliest responses include work by John Bevis (1757) and John Michell (1761). Michell determined that earthquakes originate within the Earth and were waves of movement caused by "shifting masses of rock miles below the surface".

From 1857, Robert Mallet laid the foundation of instrumental seismology and carried out seismological experiments using explosives. He is also responsible for coining the word "seismology".

In 1897, Emil Wiechert's theoretical calculations led him to conclude that the Earth's interior consists of a mantle of silicates, surrounding a core of iron.

In 1906 Richard Dixon Oldham identified the separate arrival of P-waves, S-waves and surface waves on seismograms and found the first clear evidence that the Earth has a central core.

In 1910, after studying the April 1906 San Francisco earthquake, Harry Fielding Reid put forward the "elastic rebound theory" which remains the foundation for modern tectonic studies. The development of this theory depended on the considerable progress of earlier independent streams of work on the behaviour of elastic materials and in mathematics.

In 1926, Harold Jeffreys was the first to claim, based on his study of earthquake waves, that below the mantle, the core of the Earth is liquid.

In 1937, Inge Lehmann determined that within the earth's liquid outer core there is a solid "inner" core.

By the 1960s, earth science had developed to the point where a comprehensive theory of the causation of seismic events had come together in the now well-established theory of plate tectonics.

Seismic waves are elastic waves that propagate in solid or fluid materials. They can be divided into "body waves" that travel through the interior of the materials; "surface waves" that travel along surfaces or interfaces between materials; and "normal modes", a form of standing wave.

There are two types of body waves, pressure waves or primary waves (P-waves) and shear or secondary waves (S-waves). P-waves are longitudinal waves that involve compression and expansion in the direction that the wave is moving and are always the first waves to appear on a seismogram as they are the fastest moving waves through solids. S-waves are transverse waves that move perpendicular to the direction of propagation. S-waves are slower than P-waves. Therefore, they appear later than P-waves on a seismogram. Fluids cannot support perpendicular motion, so S-waves only travel in solids.

Surface waves are the result of P- and S-waves interacting with the surface of the Earth. These waves are dispersive, meaning that different frequencies have different velocities. The two main surface wave types are Rayleigh waves, which have both compressional and shear motions, and Love waves, which are purely shear. Rayleigh waves result from the interaction of P-waves and vertically polarized S-waves with the surface and can exist in any solid medium. Love waves are formed by horizontally polarized S-waves interacting with the surface, and can only exist if there is a change in the elastic properties with depth in a solid medium, which is always the case in seismological applications. Surface waves travel more slowly than P-waves and S-waves because they are the result of these waves traveling along indirect paths to interact with Earth's surface. Because they travel along the surface of the Earth, their energy decays less rapidly than body waves (1/distance vs. 1/distance), and thus the shaking caused by surface waves is generally stronger than that of body waves. The primary surface waves are often the largest signals on earthquake seismograms. Surface waves are strongly excited when their source is close to the surface, as in a shallow earthquake or a near surface explosion, and are much weaker for deep earthquake sources.

Both body and surface waves are traveling waves; however, large earthquakes can also make the entire Earth "ring" like a resonant bell. This ringing is a mixture of normal modes with discrete frequencies and periods of an hour or shorter. Motion caused by a large earthquake can be observed for up to a month after the event. The first observations of normal modes were made in the 1960s as the advent of higher fidelity instruments coincided with two of the largest earthquakes of the 20th century – the 1960 Valdivia earthquake and the 1964 Alaska earthquake. Since then, the normal modes of the Earth have given us some of the strongest constraints on the deep structure of the Earth.

One of the first attempts at the scientific study of earthquakes followed the 1755 Lisbon earthquake. Other notable earthquakes that spurred major advancements in the science of seismology include the 1857 Basilicata earthquake, the 1906 San Francisco earthquake, the 1964 Alaska earthquake, the 2004 Sumatra-Andaman earthquake, and the 2011 Great East Japan earthquake.

Seismic waves produced by explosions or vibrating controlled sources are one of the primary methods of underground exploration in geophysics (in addition to many different electromagnetic methods such as induced polarization and magnetotellurics). Controlled-source seismology has been used to map salt domes, anticlines and other geologic traps in petroleum-bearing rocks, faults, rock types, and long-buried giant meteor craters. For example, the Chicxulub Crater, which was caused by an impact that has been implicated in the extinction of the dinosaurs, was localized to Central America by analyzing ejecta in the Cretaceous–Paleogene boundary, and then physically proven to exist using seismic maps from oil exploration.

Seismometers are sensors that detect and record the motion of the Earth arising from elastic waves. Seismometers may be deployed at the Earth's surface, in shallow vaults, in boreholes, or underwater. A complete instrument package that records seismic signals is called a seismograph. Networks of seismographs continuously record ground motions around the world to facilitate the monitoring and analysis of global earthquakes and other sources of seismic activity. Rapid location of earthquakes makes tsunami warnings possible because seismic waves travel considerably faster than tsunami waves. Seismometers also record signals from non-earthquake sources ranging from explosions (nuclear and chemical), to local noise from wind or anthropogenic activities, to incessant signals generated at the ocean floor and coasts induced by ocean waves (the global microseism), to cryospheric events associated with large icebergs and glaciers. Above-ocean meteor strikes with energies as high as 4.2 × 10 J (equivalent to that released by an explosion of ten kilotons of TNT) have been recorded by seismographs, as have a number of industrial accidents and terrorist bombs and events (a field of study referred to as forensic seismology). A major long-term motivation for the global seismographic monitoring has been for the detection and study of nuclear testing.

Because seismic waves commonly propagate efficiently as they interact with the internal structure of the Earth, they provide high-resolution noninvasive methods for studying the planet's interior. One of the earliest important discoveries (suggested by Richard Dixon Oldham in 1906 and definitively shown by Harold Jeffreys in 1926) was that the outer core of the earth is liquid. Since S-waves do not pass through liquids, the liquid core causes a "shadow" on the side of the planet opposite the earthquake where no direct S-waves are observed. In addition, P-waves travel much slower through the outer core than the mantle.

Processing readings from many seismometers using seismic tomography, seismologists have mapped the mantle of the earth to a resolution of several hundred kilometers. This has enabled scientists to identify convection cells and other large-scale features such as the large low-shear-velocity provinces near the core–mantle boundary.

Forecasting a probable timing, location, magnitude and other important features of a forthcoming seismic event is called earthquake prediction. Various attempts have been made by seismologists and others to create effective systems for precise earthquake predictions, including the VAN method. Most seismologists do not believe that a system to provide timely warnings for individual earthquakes has yet been developed, and many believe that such a system would be unlikely to give useful warning of impending seismic events. However, more general forecasts routinely predict seismic hazard. Such forecasts estimate the probability of an earthquake of a particular size affecting a particular location within a particular time-span, and they are routinely used in earthquake engineering.

Public controversy over earthquake prediction erupted after Italian authorities indicted six seismologists and one government official for manslaughter in connection with a magnitude 6.3 earthquake in L'Aquila, Italy on April 5, 2009. The indictment has been widely perceived as an indictment for failing to predict the earthquake and has drawn condemnation from the American Association for the Advancement of Science and the American Geophysical Union. The indictment claims that, at a special meeting in L'Aquila the week before the earthquake occurred, scientists and officials were more interested in pacifying the population than providing adequate information about earthquake risk and preparedness.

Engineering seismology is the study and application of seismology for engineering purposes. It generally applied to the branch of seismology that deals with the assessment of the seismic hazard of a site or region for the purposes of earthquake engineering. It is, therefore, a link between earth science and civil engineering. There are two principal components of engineering seismology. Firstly, studying earthquake history (e.g. historical and instrumental catalogs of seismicity) and tectonics to assess the earthquakes that could occur in a region and their characteristics and frequency of occurrence. Secondly, studying strong ground motions generated by earthquakes to assess the expected shaking from future earthquakes with similar characteristics. These strong ground motions could either be observations from accelerometers or seismometers or those simulated by computers using various techniques, which are then often used to develop ground motion prediction equations (or ground-motion models).

Seismological instruments can generate large amounts of data. Systems for processing such data include:






</doc>
<doc id="28936" url="https://en.wikipedia.org/wiki?curid=28936" title="Cyanoacrylate">
Cyanoacrylate

Cyanoacrylates are a family of strong fast-acting adhesives with industrial, medical, and household uses. They are various esters of cyanoacrylic acid. The acryl groups in the resin rapidly polymerize in the presence of water to form long, strong chains. They have some minor toxicity.
Specific cyanoacrylates include methyl 2-cyanoacrylate (MCA), ethyl 2-cyanoacrylate (ECA, commonly sold under trade names such as "Super Glue" and "Krazy Glue", or Toagosei), "n"-butyl cyanoacrylate (n-BCA), octyl cyanoacrylate and 2-octyl cyanoacrylate (used in medical, veterinary and first aid applications). Octyl cyanoacrylate was developed to address toxicity concerns and to reduce skin irritation and allergic response. Cyanoacrylate adhesives are sometimes known generically as instant glues, power glues or superglues. The abbreviation "CA" is commonly used for industrial grade cyanoacrylate.

The original patent for cyanoacrylate was filed in 1942 by the B.F. Goodrich Company as an outgrowth of a search for materials suitable for clear plastic gun sights for the war effort. In 1942, a team of scientists headed by Harry Coover Jr. stumbled upon a formulation that stuck to everything with which it came in contact. The team quickly rejected the substance for the wartime application, but in 1951, while working as researchers for Eastman Kodak, Coover and a colleague, Fred Joyner, rediscovered cyanoacrylates. The two realized the true commercial potential, and a form of the adhesive was first sold in 1958 under the title "Eastman #910" (later "Eastman 910").

During the 1960s, Eastman Kodak sold cyanoacrylate to Loctite, which in turn repackaged and distributed it under a different brand name "Loctite Quick Set 404". In 1971, Loctite developed its own manufacturing technology and introduced its own line of cyanoacrylate, called "Super Bonder". Loctite quickly gained market share, and by the late 1970s it was believed to have exceeded Eastman Kodak's share in the North American industrial cyanoacrylate market. National Starch and Chemical Company purchased Eastman Kodak’s cyanoacrylate business and combined it with several acquisitions made throughout the 1970s forming Permabond. Other manufacturers of cyanoacrylate include LePage (a Canadian company acquired by Henkel in 1996), the Permabond Division of National Starch and Chemical, which was a subsidiary of Unilever. Together, Loctite, Eastman and Permabond accounted for approximately 75% of the industrial cyanoacrylate market. Permabond continued to manufacture the original 910 formula.

In its liquid form, cyanoacrylate consists of monomers of cyanoacrylate ester molecules. Methyl 2-cyanoacrylate (CH=C(C≡N)COOCH) has a molecular weight of 111.1 g/mol, a flashpoint of , and a density of 1.1 g/mL. Ethyl 2-cyanoacrylate ((CH=C(C≡N)COOCHCH)) has a molecular weight of 125 g/mol and a flashpoint of more than . To facilitate easy handling, a cyanoacrylate adhesive is frequently formulated with an ingredient such as fumed silica to make it more viscous or gel-like. More recently, formulations are available with additives to increase shear strength, creating a more impact resistant bond. Such additives may include rubber, as in Loctite's "Ultra Gel", or others which are not specified.

In general, the acryl groups rapidly undergo chain-growth polymerisation in the presence of water (specifically hydroxide ions), forming long, strong chains, joining the bonded surfaces together. Because the presence of moisture causes the glue to set, exposure to normal levels of humidity in the air causes a thin skin to start to form within seconds, which very greatly slows the reaction; hence, cyanoacrylates are applied as thin coats to ensure that the reaction proceeds rapidly for bonding.

Cyanoacrylate adhesives have a short shelf life—about one year from manufacture if unopened, and one month once opened. The reaction with moisture can cause a container of glue which has been opened and resealed to become unusable more quickly than if never opened. To minimise this reduction in shelf life, cyanoacrylate, once opened, should be stored in an airtight container with a package of silica gel desiccant. Another technique is to insert a hypodermic needle into the opening of a tube. After using the glue, residual glue soon clogs the needle, keeping moisture out. The clog is removed by heating the needle (e.g. with a lighter) before use. The polymerisation is also temperature-dependent: storage below freezing point of water, , stops the reaction, so keeping it in the freezer is also effective.
Cyanoacrylates are mainly used as adhesives. They require some care and knowledge for effective use: they do not bond some materials; they do not fill spaces, unlike epoxies, and a very thin layer bonds more effectively than a thicker one that does not cure properly; they bond many substances, including human skin and tissues. They have an exothermic reaction to natural fibres: cotton, wool, leather, see reaction with cotton below.

Cyanoacrylate glue has a low shearing strength, which has led to its use as a temporary adhesive in cases where the piece needs to be sheared off later. Common examples include mounting a workpiece to a sacrificial glue block on a lathe, and tightening pins and bolts. It is also used in conjunction with another, slower, but more resilient adhesive as a way of rapidly forming a joint, which then holds the pieces in the appropriate configuration until the second adhesive has set.

Cyanoacrylate-based glue has a weak bond with smooth surfaces and as such easily gives to friction; a good example of this is the fact that cyanoacrylates may be removed from human skin by means of abrasives (e.g. sugar or sandpaper).

Cyanoacrylates are used to assemble prototype electronics (used in wire wrap), flying model aircraft, and as retention dressings for nuts and bolts. Their effectiveness in bonding metal and general versatility have made them popular among modeling and miniatures hobbyists.

Cyanoacrylate glue's ability to resist water has made it popular with marine aquarium hobbyists for fragmenting corals. The cut branches of hard corals, such as Acropora, can be glued to a piece of live rock (harvested reef coral) or Milliput (epoxy putty) to allow the new fragment to grow out. It is safe to use directly in the tank, unlike silicone which must be cured to be safe. However, as a class of adhesives, traditional cyanoacrylates are classified as having "weak" resistance to both moisture and heat although the inclusion of phthalic anhydride reportedly counteracts both of these characteristics. Cyanoacrylate glue is also used frequently in aquascaping both freshwater and marine aquariums for the purpose of securing the rhizomes of live plants to pieces of wood or stone.

Most standard cyanoacrylate adhesives do not bond well with smooth glass, although they can be used as a quick, temporary bond prior to application of an epoxy or cyanoacrylate specifically formulated for use on glass. A mechanical adhesive bond may be formed around glass fibre mat or tissue to reinforce joints or to fabricate small parts.

When added to baking soda (sodium bicarbonate), cyanoacrylate glue forms a hard, lightweight adhesive filler (baking soda is first used to fill a gap then the adhesive is dropped onto the baking soda). This works well with porous materials that do not work well with the adhesive alone. This method is sometimes used by aircraft modelers to assemble or repair polystyrene foam parts. It is also used to repair small nicks in the leading edge of wood propeller blades on light aircraft, although this technique is limited to use on aircraft registered in the "experimental" category (composite propellers can be repaired in a similar way using two-part epoxies). This technique can also be used to fill in the slots in the nut of a guitar so that new ones can be cut. The reaction between cyanoacrylate and baking soda is very exothermic (heat-producing) and also produces noxious vapors.

One brand of cyanoacrylate, "SupaFix", uses calcium oxide as a filler giving an even harder (mortar-like in texture) result that can be used to join hard materials and even repair cracked castings.

Cyanoacrylate is used as a forensic tool to capture latent fingerprints on non-porous surfaces like glass, plastic, etc. Cyanoacrylate is warmed to produce fumes that react with the invisible fingerprint residues and atmospheric moisture to form a white polymer (polycyanoacrylate) on the fingerprint ridges. The ridges can then be recorded. The developed fingerprints are, on most surfaces (except on white plastic or similar), visible to the naked eye. Invisible or poorly visible prints can be further enhanced by applying a luminescent or non-luminescent stain.

Thin cyanoacrylate glue has application in woodworking. It can be used as a fast-drying, glossy finish. The use of oil, such as boiled linseed oil, may be used to control the rate at which the cyanoacrylate cures. Cyanoacrylate glue is also used in combination with sawdust (from a saw or sanding) to fill voids and cracks. These repair methods are used on piano soundboards, wood instruments, and wood furniture. Cyanoacrylate glue is also used in the finishing of pen blanks (wooden blanks for turning pens) that have been turned on a lathe by applying multiple thin layers to build up a hard, clear finish that can then be sanded and polished to a glossy finish.

Cyanoacrylate glue was in veterinary use for mending bone, hide, and tortoise shell by the early 1970s or before. Harry Coover said in 1966 that a cyanoacrylate spray was used in the Vietnam War to reduce bleeding in wounded soldiers until they could be taken to a hospital. "n"-Butyl cyanoacrylate has been used medically since the 1970s. In the US, due to its potential to irritate the skin, the U.S. Food and Drug Administration (FDA) did not approve its use as a medical adhesive until 1998 with Dermabond (2-octyl cyanoacrylate). Studies confirm that cyanoacrylate can be safer and more functional for wound closure than traditional suturing (stitches). The adhesive is superior in time required to close wounds, incidence of infection (suture canals through the skin's epidermal, dermal, and subcutaneous fat layers introduce additional routes of contamination), and finally cosmetic appearance.

Some rock climbers use cyanoacrylate to repair damage to the skin on their fingertips. Similarly, stringed-instrument players can form protective finger caps (in addition to calluses) with cyanoacrylates. While the glue is not very toxic and wears off quickly with shed skin, applying large quantities of glue and its fumes directly to the skin can cause chemical burns.

While standard "superglue" is 100% ethyl 2-cyanoacrylate, many custom formulations (e.g."," 91% ECA, 9% poly(methyl methacrylate), <0.5% hydroquinone, and a small amount of organic sulfonic acid, and variations on the compound "n"-butyl cyanoacrylate for medical applications) have come to be used for specific applications. There are three cyanoacrylate compounds currently available as topical skin adhesives. 2-Octyl cyanoacrylate is marketed as Dermabond, SurgiSeal and more recently LiquiBand Exceed. "n"-Butyl cyanoacrylate is marketed as Histoacryl, Indermil, GluStitch, GluSeal, PeriAcryl, and LiquiBand. The compound ethyl 2-cyanoacrylate is available as Epiglu.

Cyanoacrylate is used in archery to glue fletching to arrow shafts. Some special fletching glues are primarily cyanoacrylate repackaged in special fletching glue kits. Such tubes often have a long, thin metal nozzle for improved precision in applying the glue to the base of the fletching and to ensure secure bonding to the arrow shaft.

Cyanoacrylate is used in the cosmetology and beauty industry as an eyelash extension glue, or a "nail glue" for some artificial nail enhancements such as nail tips and nail wraps, and is sometimes mistaken for eye drops causing accidental injury (chemical eye injury).

Cyanoacrylate adhesives may adhere to body parts, and injuries may occur when parts of the skin are torn off. Without force, however, the glue will spontaneously separate from the skin in time (up to four days). Separation can be accelerated by applying vegetable oil near, on, and around the glue. In the case of glued eyelids, a doctor should be consulted.

The fumes from cyanoacrylate are a vaporized form of the cyanoacrylate monomer that irritate the sensitive mucous membranes of the respiratory tract (i.e., eyes, nose, throat, and lungs). They are immediately polymerized by the moisture in the membranes and become inert. These risks can be minimized by using cyanoacrylate in well-ventilated areas. About 5% of the population can become sensitized to cyanoacrylate fumes after repeated exposure, resulting in flu-like symptoms. Cyanoacrylate may also be a skin irritant, causing an allergic skin reaction. The American Conference of Governmental Industrial Hygienists (ACGIH) assign a threshold limit value exposure limit of 200 parts per billion. On rare occasions, inhalation may trigger asthma. There is no singular measurement of toxicity for all cyanoacrylate adhesives because of the large number of adhesives that contain various cyanoacrylate formulations.

The United Kingdom's Health and Safety Executive and the United States National Toxicology Program have concluded that the use of ethyl cyanoacrylate is safe and that additional study is unnecessary. The compound 2-octyl cyanoacrylate degrades much more slowly due to its longer organic backbone (series of covalently bonded carbon molecules) and the adhesive does not reach the threshold of tissue toxicity. Due to the toxicity issues of ethyl cyanoacrylate, the use of 2-octyl cyanoacrylate for sutures is preferred.

Applying cyanoacrylate to some natural materials such as cotton (jeans, cotton swabs, cotton balls, and certain yarns or fabrics), or leather or wool results in a powerful, rapid, exothermic reaction. This reaction also occurs with fiberglass and carbon fiber. The heat released may cause serious burns or release irritating white smoke. Material Safety Data Sheets for cyanoacrylate instruct users not to wear cotton (jeans) or wool clothing, especially cotton gloves, when applying or handling cyanoacrylates.

Acetone, commonly found as a fraction of nail polish remover (or at hardware stores in pure form), is a widely available solvent capable of softening cured cyanoacrylate. Other solvents include nitromethane, dimethyl sulfoxide, and methylene chloride. gamma-Butyrolactone may also be used to remove cured cyanoacrylate. Commercial debonders are also available.

Cyanoacrylate adhesives have a short shelf life. Date-stamped containers help to ensure that the adhesive is still viable. One manufacturer supplies the following information and advice: When kept unopened in a cool, dry location such as a refrigerator at a temperature of about 55 °F (13 °C), the shelf life of cyanoacrylate will be extended from about one year from manufacture to at least 15 months. If the adhesive is to be used within six months, it is not necessary to refrigerate it. Cyanoacrylates are moisture-sensitive, and moving from a cool to a hot location will create condensation; after removing from the refrigerator, it is best to let the adhesive reach room temperature before opening. After opening, it should be used within 30 days. Open containers should not be refrigerated. Another manufacturer says that the maximum shelf life of 12 months is obtained for some of their cyanoacrylates if the original containers are stored at . User forums and some manufacturers say that an almost unlimited shelf life is attainable by storing unopened at , the typical temperature of a domestic freezer, and allowing the contents to reach room temperature before use. Rechilling an opened container may cause moisture from the air to condense in the container; however, reports from hobbyists suggest that storing cyanoacrylate in a freezer can preserve opened cyanoacrylate indefinitely.

As cyanoacrylates age, they polymerize, become thicker, and cure more slowly. They can be thinned with a cyanoacrylate of the same chemical composition with lower viscosity. Storing cyanoacrylates below will nearly stop the polymerization process and prevent aging.





</doc>
<doc id="28938" url="https://en.wikipedia.org/wiki?curid=28938" title="Shell script">
Shell script

A shell script is a computer program designed to be run by the Unix shell, a command-line interpreter. The various dialects of shell scripts are considered to be scripting languages. Typical operations performed by shell scripts include file manipulation, program execution, and printing text. A script which sets up the environment, runs the program, and does any necessary cleanup, logging, etc. is called a wrapper.

The term is also used more generally to mean the automated mode of running an operating system shell; in specific operating systems they are called other things such as batch files (MSDos-Win95 stream, OS/2), command procedures (VMS), and shell scripts (Windows NT stream and third-party derivatives like 4NT—article is at cmd.exe), and mainframe operating systems are associated with a number of terms.

The typical Unix/Linux/POSIX-compliant installation includes the KornShell (codice_1) in several possible versions such as ksh88, Korn Shell '93 and others. The oldest shell still in common use is the Bourne shell (codice_2); Unix systems invariably also include the C shell (codice_3), Bash (codice_4), a Remote Shell (codice_5), a Secure Shell (codice_6) for SSL telnet connections, and a shell which is a main component of the Tcl/Tk installation usually called codice_7; wish is a GUI-based Tcl/Tk shell. The C and Tcl shells have syntax quite similar to that of said programming languages, and the Korn shells and Bash are developments of the Bourne shell, which is based on the ALGOL language with elements of a number of others added as well. On the other hand, the various shells plus tools like awk, sed, grep, and BASIC, Lisp, C and so forth contributed to the Perl programming language.

Other shells available on a machine or available for download and/or purchase include Almquist shell (codice_8), PowerShell (codice_9), Z shell (codice_10, a particularly common enhanced KornShell), the Tenex C Shell (codice_11), a Perl-like shell (codice_12). Related programs such as shells based on Python, Ruby, C, Java, Perl, Pascal, Rexx &c in various forms are also widely available. Another somewhat common shell is osh, whose manual page states it "is an enhanced, backward-compatible port of the standard command interpreter from Sixth Edition UNIX."

Windows-Unix interoperability software such as the MKS Toolkit, Cygwin, UWIN, Interix and others make the above shells and Unix programming available on Windows systems, providing functionality all the way down to signals and other inter-process communication, system calls and APIs. The Hamilton C shell is a Windows shell that is very similar to the Unix C Shell. Microsoft distributes Windows Services for UNIX for use with its NT-based operating systems in particular, which have a POSIX environmental subsystem.

Typically "comments begin with an octothorpe, or hash (#)... and continue until the end of the line. The shell ignores them", with one exception, as noted below.

If the first two bytes of the file consist of the # character followed by an exclamation mark (!) "this is a special type of comment called a 'shebang' or hash-bang. It tells the system which interpreter to use to execute the file." In Unix-like operating systems the characters following #! are interpreted as a path to an executable program that will interpret the script.

A shell script can provide a convenient variation of a system command where special environment settings, command options, or post-processing apply automatically, but in a way that allows the new script to still act as a fully normal Unix command.

One example would be to create a version of ls, the command to list files, giving it a shorter command name of l, which would be normally saved in a user's bin directory as /home/"username"/bin/l, and a default set of command options pre-supplied.

Here, the first line (shebang) indicates which interpreter should execute the rest of the script, and the second line makes a listing with options for file format indicators, columns, all files (none omitted), and a size in blocks. The LC_COLLATE=C sets the default collation order to not fold upper and lower case together, not intermix dotfiles with normal filenames as a side effect of ignoring punctuation in the names (dotfiles are usually only shown if an option like -a is used), and the "$@" causes any parameters given to l to pass through as parameters to ls, so that all of the normal options and other syntax known to ls can still be used.

The user could then simply use l for the most commonly used short listing.

Another example of a shell script that could be used as a shortcut would be to print a list of all the files and directories within a given directory.

In this case, the shell script would start with its normal starting line of #!/bin/sh. Following this, the script executes the command clear which clears the terminal of all text before going to the next line. The following line provides the main function of the script. The ls -al command lists the files and directories that are in the directory from which the script is being run. The ls command attributes could be changed to reflect the needs of the user.

Note: If an implementation does not have the clear command, try using the clr command instead.

Shell scripts allow several commands that would be entered manually at a command-line interface to be executed automatically, and without having to wait for a user to trigger each stage of the sequence. For example, in a directory with three C source code files, rather than manually running the four commands required to build the final program from them, one could instead create a C shell script, here named build and kept in the directory with them, which would compile them automatically:

The script would allow a user to save the file being edited, pause the editor, and then just run ./build to create the updated program, test it, and then return to the editor. Since the 1980s or so, however, scripts of this type have been replaced with utilities like make which are specialized for building programs.

Simple batch jobs are not unusual for isolated tasks, but using shell loops, tests, and variables provides much more flexibility to users. A Bash (Unix shell) script to convert JPEG images to PNG images, where the image names are provided on the command-line—possibly via wildcards—instead of each being listed within the script, can be created with this file, typically saved in a file like /home/"username"/bin/jpg2png

The jpg2png command can then be run on an entire directory full of JPEG images with just /home/"username"/bin/jpg2png *.jpg

A key feature of shell scripts is that the invocation of their interpreters is handled as a core operating system feature. So rather than a user's shell only being able to execute scripts in that shell's language, or a script only having its interpreter directive handled correctly if it was run from a shell (both of which were limitations in the early Bourne shell's handling of scripts), shell scripts are set up and executed by the OS itself. A modern shell script is not just on the same footing as system commands, but rather many system commands are actually shell scripts (or more generally, scripts, since some of them are not interpreted by a shell, but instead by Perl, Python, or some other language). This extends to returning exit codes like other system utilities to indicate success or failure, and allows them to be called as components of larger programs regardless of how those larger tools are implemented.

Like standard system commands, shell scripts classically omit any kind of filename extension unless intended to be read into a running shell through a special mechanism for this purpose (such as sh’s "codice_13", or csh’s source).

Many modern shells also supply various features usually found only in more sophisticated general-purpose programming languages, such as control-flow constructs, variables, comments, arrays, subroutines and so on. With these sorts of features available, it is possible to write reasonably sophisticated applications as shell scripts. However, they are still limited by the fact that most shell languages have little or no support for data typing systems, classes, threading, complex math, and other common full language features, and are also generally much slower than compiled code or interpreted languages written with speed as a performance goal.

The standard Unix tools sed and awk provide extra capabilities for shell programming; Perl can also be embedded in shell scripts as can other scripting languages like Tcl. Perl and Tcl come with graphics toolkits as well.

Many powerful scripting languages have been introduced for tasks that are too large or complex to be comfortably handled with ordinary shell scripts, but for which the advantages of a script are desirable and the development overhead of a full-blown, compiled programming language would be disadvantageous. The specifics of what separates scripting languages from high-level programming languages is a frequent source of debate, but, generally speaking, a scripting language is one which requires an interpreter.
Shell scripts often serve as an initial stage in software development, and are often subject to conversion later to a different underlying implementation, most commonly being converted to Perl, Python, or C. The interpreter directive allows the implementation detail to be fully hidden inside the script, rather than being exposed as a filename extension, and provides for seamless reimplementation in different languages with no impact on end users.

While files with the ".sh" file extension are usually a shell script of some kind, most shell scripts do not have any filename extension.

Perhaps the biggest advantage of writing a shell script is that the commands and syntax are exactly the same as those directly entered at the command-line. The programmer does not have to switch to a totally different syntax, as they would if the script were written in a different language, or if a compiled language were used.

Often, writing a shell script is much quicker than writing the equivalent code in other programming languages. The many advantages include easy program or file selection, quick start, and interactive debugging. A shell script can be used to provide a sequencing and decision-making linkage around existing programs, and for moderately sized scripts the absence of a compilation step is an advantage. Interpretive running makes it easy to write debugging code into a script and re-run it to detect and fix bugs. Non-expert users can use scripting to tailor the behavior of programs, and shell scripting provides some limited scope for multiprocessing.

On the other hand, shell scripting is prone to costly errors. Inadvertent typing errors such as rm -rf * / (instead of the intended rm -rf */) are folklore in the Unix community; a single extra space converts the command from one that deletes everything in the sub-directories to one which deletes everything—and also tries to delete everything in the root directory. Similar problems can transform cp and mv into dangerous weapons, and misuse of the > redirect can delete the contents of a file. This is made more problematic by the fact that many UNIX commands differ in name by only one letter: cp, cd, dd, df, etc.

Another significant disadvantage is the slow execution speed and the need to launch a new process for almost every shell command executed. When a script's job can be accomplished by setting up a pipeline in which efficient filter commands perform most of the work, the slowdown is mitigated, but a complex script is typically several orders of magnitude slower than a conventional compiled program that performs an equivalent task.

There are also compatibility problems between different platforms. Larry Wall, creator of Perl, famously wrote that "It is easier to port a shell than a shell script."

Similarly, more complex scripts can run into the limitations of the shell scripting language itself; the limits make it difficult to write quality code, and extensions by various shells to ameliorate problems with the original shell language can make problems worse.

Many disadvantages of using some script languages are caused by design flaws within the language syntax or implementation, and are not necessarily imposed by the use of a text-based command-line; there are a number of shells which use other shell programming languages or even full-fledged languages like Scsh (which uses Scheme).

Interoperability software such as Cygwin, the MKS Toolkit, Interix (which is available in the Microsoft Windows Services for UNIX), Hamilton C shell, UWIN (AT&T Unix for Windows) and others allow Unix shell programs to be run on machines running Windows NT and its successors, with some loss of functionality on the MS-DOS-Windows 95 branch, as well as earlier MKS Toolkit versions for OS/2. At least three DCL implementations for Windows type operating systems—in addition to XLNT, a multiple-use scripting language package which is used with the command shell, Windows Script Host and CGI programming—are available for these systems as well. Mac OS X and subsequent are Unix-like as well.

In addition to the aforementioned tools, some POSIX and OS/2 functionality can be used with the corresponding environmental subsystems of the Windows NT operating system series up to Windows 2000 as well. A third, 16-bit subsystem often called the MS-DOS subsystem uses the Command.com provided with these operating systems to run the aforementioned MS-DOS batch files.

The console alternatives 4DOS, 4OS2, FreeDOS, Peter Norton's NDOS and 4NT / Take Command which add functionality to the Windows NT-style cmd.exe, MS-DOS/Windows 95 batch files (run by Command.com), OS/2's cmd.exe, and 4NT respectively are similar to the shells that they enhance and are more integrated with the Windows Script Host, which comes with three pre-installed engines, VBScript, JScript, and VBA and to which numerous third-party engines can be added, with Rexx, Perl, Python, Ruby, and Tcl having pre-defined functions in 4NT and related programs. PC DOS is quite similar to MS-DOS, whilst DR DOS is more different. Earlier versions of Windows NT are able to run contemporary versions of 4OS2 by the OS/2 subsystem.

Scripting languages are, by definition, able to be extended; for example, a MS-DOS/Windows 95/98 and Windows NT type systems allows for shell/batch programs to call tools like KixTart, QBasic, various BASIC, Rexx, Perl, and Python implementations, the Windows Script Host and its installed engines. On Unix and other POSIX-compliant systems, awk and sed are used to extend the string and numeric processing ability of shell scripts. Tcl, Perl, Rexx, and Python have graphics toolkits and can be used to code functions and procedures for shell scripts which pose a speed bottleneck (C, Fortran, assembly language &c are much faster still) and to add functionality not available in the shell language such as sockets and other connectivity functions, heavy-duty text processing, working with numbers if the calling script does not have those abilities, self-writing and self-modifying code, techniques like recursion, direct memory access, various types of sorting and more, which are difficult or impossible in the main script, and so on. Visual Basic for Applications and VBScript can be used to control and communicate with such things as spreadsheets, databases, scriptable programs of all types, telecommunications software, development tools, graphics tools and other software which can be accessed through the Component Object Model.




</doc>
<doc id="28940" url="https://en.wikipedia.org/wiki?curid=28940" title="Subtitle (disambiguation)">
Subtitle (disambiguation)

Subtitles are text derived from film or television show dialogue that is usually displayed at the bottom of the screen.

Subtitle or Subtitles may also refer to:




</doc>
<doc id="28942" url="https://en.wikipedia.org/wiki?curid=28942" title="Solder">
Solder

Solder (, or in North America ) is a fusible metal alloy used to create a permanent bond between metal workpieces. The word solder comes from the Middle English word , via Old French and , from the Latin , meaning "to make solid". In fact, solder must first be melted in order to adhere to and connect the pieces together after cooling, which requires that an alloy suitable for use as solder have a lower melting point than the pieces being joined. The solder should also be resistant to oxidative and corrosive effects that would degrade the joint over time. Solder used in making electrical connections also needs to have favorable electrical characteristics.

Soft solder typically has a melting point range of , and is commonly used in electronics, plumbing, and sheet metal work. Alloys that melt between are the most commonly used. Soldering performed using alloys with a melting point above is called "hard soldering", "silver soldering", or brazing.

In specific proportions, some alloys can become eutectic — that is, the alloy's melting point is lower than that of either component. Non-eutectic alloys have markedly different "solidus" and "liquidus" temperatures, and within that range they exist as a paste of solid particles in a melt of the lower-melting phase. In electrical work, if the joint is disturbed in the pasty state before it has solidified totally, a poor electrical connection may result; use of eutectic solder reduces this problem. The pasty state of a non-eutectic solder can be exploited in plumbing, as it allows molding of the solder during cooling, e.g. for ensuring watertight joint of pipes, resulting in a so-called "wiped joint".

For electrical and electronics work, solder wire is available in a range of thicknesses for hand-soldering (manual soldering is performed using a soldering iron or soldering gun), and with cores containing flux. It is also available as a paste, as a preformed foil shaped to match the workpiece, more suitable for mechanized mass-production, or in small "tabs" that can be wrapped around the joint and melted with a flame, for field repairs where an iron isn't usable or available. Alloys of lead and tin were commonly used in the past and are still available; they are particularly convenient for hand-soldering. Lead-free solders have been increasing in use due to regulatory requirements plus the health and environmental benefits of avoiding lead-based electronic components. They are almost exclusively used today in consumer electronics.

Plumbers often use bars of solder, much thicker than the wire used for electrical applications. Jewelers often use solder in thin sheets, which they cut into snippets.

The European Union Waste Electrical and Electronic Equipment Directive (WEEE) and Restriction of Hazardous Substances Directive (RoHS) were adopted in early 2003 and came into effect on July 1, 2006, restricting the inclusion of lead in most consumer electronics sold in the EU, and having a broad effect on consumer electronics sold worldwide. In the US, manufacturers may receive tax benefits by reducing the use of lead-based solder. Lead-free solders in commercial use may contain tin, copper, silver, bismuth, indium, zinc, antimony, and traces of other metals. Most lead-free replacements for conventional 60/40 and 63/37 Sn-Pb solder have melting points from 50 to 200 °C higher, though there are also solders with much lower melting points. Lead-free solder typically requires around 2% flux by mass for adequate wetting ability.

When lead-free solder is used in wave soldering, a slightly modified solder pot may be desirable (e.g. titanium liners or impellers) to reduce maintenance cost due to increased tin-scavenging of high-tin solder.

Lead-free solder may be less desirable for critical applications, such as aerospace and medical projects, because its properties are less thoroughly known.

Tin-silver-copper (Sn-Ag-Cu, or "SAC") solders are used by two-thirds of Japanese manufacturers for reflow and wave soldering, and by about 75% of companies for hand soldering. The widespread use of this popular lead-free solder alloy family is based on the reduced melting point of the Sn-Ag-Cu ternary eutectic behavior (217 °C), which is below the 22/78 Sn-Ag (wt.%) eutectic of 221 °C and the 59/41 Sn-Cu eutectic of 227 °C. The ternary eutectic behavior of Sn-Ag-Cu and its application for electronics assembly was discovered (and patented) by a team of researchers from Ames Laboratory, Iowa State University, and from Sandia National Laboratories-Albuquerque.

Much recent research has focused on the addition of a fourth element to Sn-Ag-Cu solder, in order to provide compatibility for the reduced cooling rate of solder sphere reflow for assembly of ball grid arrays. Examples of these four-element compositions are 18/64/14/4 tin-silver-copper-zinc (Sn-Ag-Cu-Zn) (melting range 217–220 °C) and 18/64/16/2 tin-silver-copper-manganese (Sn-Ag-Cu-Mn) (melting range of 211–215 °C).

Tin-based solders readily dissolve gold, forming brittle intermetallics; for Sn-Pb alloys the critical concentration of gold to embrittle the joint is about 4%. Indium-rich solders (usually indium-lead) are more suitable for soldering thicker gold layer as the dissolution rate of gold in indium is much slower. Tin-rich solders also readily dissolve silver; for soldering silver metallization or surfaces, alloys with addition of silvers are suitable; tin-free alloys are also a choice, though their wettability is poorer. If the soldering time is long enough to form the intermetallics, the tin surface of a joint soldered to gold is very dull.

Tin-lead (Sn-Pb) solders, also called soft solders, are commercially available with tin concentrations between 5% and 70% by weight. The greater the tin concentration, the greater the solder’s tensile and shear strengths. Historically, lead has been widely believed to mitigate the formation of tin whiskers, though the precise mechanism for this is unknown. Today, many techniques are used to mitigate the problem, including changes to the annealing process (heating and cooling), addition of elements like copper and nickel, and the inclusion of conformal coatings. Alloys commonly used for electrical soldering are 60/40 Sn-Pb, which melts at , and 63/37 Sn-Pb used principally in electrical/electronic work. This mixture is a eutectic alloy of these metals, which:

In the United States, lead is prohibited in solder and flux in plumbing applications for drinking water use, per the Safe Drinking Water Act (SDWA). Historically, a higher proportion of lead was used, commonly 50/50. This had the advantage of making the alloy solidify more slowly. With the pipes being physically fitted together before soldering, the solder could be wiped over the joint to ensure water tightness. Although lead water pipes were displaced by copper when the significance of lead poisoning began to be fully appreciated, lead solder was still used until the 1980s because it was thought that the amount of lead that could leach into water from the solder was negligible from a properly soldered joint. The electrochemical couple of copper and lead promotes corrosion of the lead and tin. Tin, however, is protected by insoluble oxide. Since even small amounts of lead have been found detrimental to health, lead in plumbing solder was replaced by silver (food-grade applications) or antimony, with copper often added, and the proportion of tin was increased (see Lead-free solder.)

The addition of tin—more expensive than lead—improves wetting properties of the alloy; lead itself has poor wetting characteristics. High-tin tin-lead alloys have limited use as the workability range can be provided by a cheaper high-lead alloy.

Lead-tin solders readily dissolve gold plating and form brittle intermetallics.
60/40 Sn-Pb solder oxidizes on the surface, forming a complex 4-layer structure: tin(IV) oxide on the surface, below it a layer of tin(II) oxide with finely dispersed lead, followed by a layer of tin(II) oxide with finely dispersed tin and lead, and the solder alloy itself underneath.

Lead, and to some degree tin, as used in solder contains small but significant amounts of radioisotope impurities. Radioisotopes undergoing alpha decay are a concern due to their tendency to cause soft errors. Polonium-210 is especially problematic; lead-210 beta decays to bismuth-210 which then beta decays to polonium-210, an intense emitter of alpha particles. Uranium-238 and thorium-232 are other significant contaminants of alloys of lead.

Flux is a reducing agent designed to help reduce (return oxidized metals to their metallic state) metal oxides at the points of contact to improve the electrical connection and mechanical strength. The two principal types of flux are acid flux (sometimes called "active flux"), containing strong acids, used for metal mending and plumbing, and rosin flux (sometimes called "passive flux"), used in electronics. Rosin flux comes in a variety of "activities", corresponding roughly to the speed and effectiveness of the organic acid components of the rosin in dissolving metallic surface oxides, and consequently the corrosiveness of the flux residue.

Due to concerns over atmospheric pollution and hazardous waste disposal, the electronics industry has been gradually shifting from rosin flux to water-soluble flux, which can be removed with deionized water and detergent, instead of hydrocarbon solvents.

In contrast to using traditional bars or coiled wires of all-metal solder and manually applying flux to the parts being joined, much hand soldering since the mid-20th century has used flux-core solder. This is manufactured as a coiled wire of solder, with one or more continuous bodies of inorganic acid or rosin flux embedded lengthwise inside it. As the solder melts onto the joint, it frees the flux and releases that on it as well.

Hard solders are used for brazing, and melt at higher temperatures. Alloys of copper with either zinc or silver are the most common.

In silversmithing or jewelry making, special hard solders are used that will pass assay. They contain a high proportion of the metal being soldered and lead is not used in these alloys. These solders vary in hardness, designated as "enameling", "hard", "medium" and "easy". Enameling solder has a high melting point, close to that of the material itself, to prevent the joint desoldering during firing in the enameling process. The remaining solder types are used in decreasing order of hardness during the process of making an item, to prevent a previously soldered seam or joint desoldering while additional sites are soldered. Easy solder is also often used for repair work for the same reason. Flux is also used to prevent joints from desoldering.

Silver solder is also used in manufacturing to join metal parts that cannot be welded. The alloys used for these purposes contain a high proportion of silver (up to 40%), and may also contain cadmium.

The solidifying behavior depends on the alloy composition. Pure metals solidify at a certain temperature, forming crystals of one phase. Eutectic alloys also solidify at a single temperature, all components precipitating simultaneously in so-called coupled growth. Non-eutectic compositions on cooling start to first precipitate the non-eutectic phase; dendrites when it is a metal, large crystals when it is an intermetallic compound. Such a mixture of solid particles in a molten eutectic is referred to as a mushy state. Even a relatively small proportion of solids in the liquid can dramatically lower its fluidity.

The temperature of total solidification is the solidus of the alloy, the temperature at which all components are molten is the liquidus.

The mushy state is desired where a degree of plasticity is beneficial for creating the joint, allowing filling larger gaps or being wiped over the joint (e.g. when soldering pipes). In hand soldering of electronics it may be detrimental as the joint may appear solidified while it is not yet. Premature handling of such joint then disrupts its internal structure and leads to compromised mechanical integrity.

Different elements serve different roles in the solder alloy:

Impurities usually enter the solder reservoir by dissolving the metals present in the assemblies being soldered. Dissolving of process equipment is not common as the materials are usually chosen to be insoluble in solder.


Many different intermetallic compounds are formed during solidifying of solders and during their reactions with the soldered surfaces.

The intermetallics form distinct phases, usually as inclusions in a ductile solid solution matrix, but also can form the matrix itself with metal inclusions or form crystalline matter with different intermetallics. Intermetallics are often hard and brittle. Finely distributed intermetallics in a ductile matrix yield a hard alloy while coarse structure gives a softer alloy. A range of intermetallics often forms between the metal and the solder, with increasing proportion of the metal; e.g. forming a structure of Cu-CuSn-CuSn-Sn.

Layers of intermetallics can form between the solder and the soldered material. These layers may cause mechanical reliability weakening and brittleness, increased electrical resistance, or electromigration and formation of voids. The gold-tin intermetallics layer is responsible for poor mechanical reliability of tin-soldered gold-plated surfaces where the gold plating did not completely dissolve in the solder.

Gold and palladium readily dissolve in solders. Copper and nickel tend to form intermetallic layers during normal soldering profiles. Indium forms intermetallics as well.

Indium-gold intermetallics are brittle and occupy about 4 times more volume than the original gold. Bonding wires are especially susceptible to indium attack. Such intermetallic growth, together with thermal cycling, can lead to failure of the bonding wires.

Copper plated with nickel and gold is often used. The thin gold layer facilitates good solderability of nickel as it protects the nickel from oxidation; the layer has to be thin enough to rapidly and completely dissolve so bare nickel is exposed to the solder.

Lead-tin solder layers on copper leads can form copper-tin intermetallic layers; the solder alloy is then locally depleted of tin and form a lead-rich layer. The Sn-Cu intermetallics then can get exposed to oxidation, resulting in impaired solderability.

Two processes play a role in a solder joint formation: interaction between the substrate and molten solder, and solid-state growth of intermetallic compounds. The base metal dissolves in the molten solder in an amount depending on its solubility in the solder. The active constituent of the solder reacts with the base metal with a rate dependent on the solubility of the active constituents in the base metal. The solid-state reactions are more complex – the formation of intermetallics can be inhibited by changing the composition of the base metal or the solder alloy, or by using a suitable barrier layer to inhibit diffusion of the metals.

Glass solders are used to join glasses to other glasses, ceramics, metals, semiconductors, mica, and other materials, in a process called glass frit bonding. The glass solder has to flow and wet the soldered surfaces well below the temperature where deformation or degradation of either of the joined materials or nearby structures (e.g., metallization layers on chips or ceramic substrates) occurs. The usual temperature of achieving flowing and wetting is between .

Two types of glass solders are used: vitreous, and devitrifying. Vitreous solders retain their amorphous structure during remelting, can be reworked repeatedly, and are relatively transparent. Devitrifying solders undergo partial crystallization during solidifying, forming a glass-ceramic, a composite of glassy and crystalline phases. Devitrifying solders usually create a stronger mechanical bond, but are more temperature-sensitive and the seal is more likely to be leaky; due to their polycrystalline structure they tend to be translucent or opaque. Devitrifying solders are frequently "thermosetting", as their melting temperature after recrystallization becomes significantly higher; this allows soldering the parts together at lower temperature than the subsequent bake-out without remelting the joint afterwards. Devitrifying solders frequently contain up to 25% zinc oxide. In production of cathode ray tubes, devitrifying solders based on PbO-BO-ZnO are used.

Very low temperature melting glasses, fluid at , were developed for sealing applications for electronics. They can consist of binary or ternary mixtures of thallium, arsenic and sulfur. Zinc-silicoborate glasses can also be used for passivation of electronics; their coefficient of thermal expansion must match silicon (or the other semiconductors used) and they must not contain alkaline metals as those would migrate to the semiconductor and cause failures.

The bonding between the glass or ceramics and the glass solder can be either covalent, or, more often, van der Waals. The seal can be leak-tight; glass soldering is frequently used in vacuum technology. Glass solders can be also used as sealants; a vitreous enamel coating on iron lowered its permeability to hydrogen 10 times. Glass solders are frequently used for glass-to-metal seals and glass-ceramic-to-metal seals.

Glass solders are available as frit powder with grain size below 60 micrometers. They can be mixed with water or alcohol to form a paste for easy application, or with dissolved nitrocellulose or other suitable binder for adhering to the surfaces until being melted. The eventual binder has to be burned off before melting proceeds, requiring careful firing regime. The solder glass can be also applied from molten state to the area of the future joint during manufacture of the part. Due to their low viscosity in molten state, lead glasses with high PbO content (often 70–85%) are frequently used. The most common compositions are based on lead borates (leaded borate glass or borosilicate glass). Smaller amount of zinc oxide or aluminium oxide can be added for increasing chemical stability. Phosphate glasses can be also employed. Zinc oxide, bismuth trioxide, and copper(II) oxide can be added for influencing the thermal expansion; unlike the alkali oxides, these lower the softening point without increasing of thermal expansion.

Glass solders are frequently used in electronic packaging. CERDIP packagings are an example. Outgassing of water from the glass solder during encapsulation was a cause of high failure rates of early CERDIP integrated circuits. Removal of glass-soldered ceramic covers, e.g., for gaining access to the chip for failure analysis or reverse engineering, is best done by shearing; if this is too risky, the cover is polished away instead.

As the seals can be performed at much lower temperature than with direct joining of glass parts and without use of flame (using a temperature-controlled kiln or oven), glass solders are useful in applications like subminiature vacuum tubes or for joining mica windows to vacuum tubes and instruments (e.g., Geiger tube). Thermal expansion coefficient has to be matched to the materials being joined and often is chosen in between the coefficients of expansion of the materials. In case of having to compromise, subjecting the joint to compression stresses is more desirable than to tensile stresses. The expansion matching is not critical in applications where thin layers are used on small areas, e.g., fireable inks, or where the joint will be subjected to a permanent compression (e.g., by an external steel shell) offsetting the thermally introduced tensile stresses.

Glass solder can be used as an intermediate layer when joining materials (glasses, ceramics) with significantly different coefficient of thermal expansion; such materials cannot be directly joined by diffusion welding. Evacuated glazing windows are made of glass panels soldered together.

A glass solder is used, e.g., for joining together parts of cathode ray tubes and plasma display panels. Newer compositions lowered the usage temperature from by reducing the lead(II) oxide content down from 70%, increasing the zinc oxide content, adding titanium dioxide and bismuth(III) oxide and some other components. The high thermal expansion of such glass can be reduced by a suitable ceramic filler. Lead-free solder glasses with soldering temperature of were also developed.

Phosphate glasses with low melting temperature were developed. One of such compositions is phosphorus pentoxide, lead(II) oxide, and zinc oxide, with addition of lithium and some other oxides.

Conductive glass solders can be also prepared.

A preform is a pre-made shape of solder specially designed for the application where it is to be used. Many methods are used to manufacture the solder preform, stamping being the most common. The solder preform may include the solder flux needed for the soldering process. This can be an internal flux, inside the solder preform, or external, with the solder preform coated.




</doc>
<doc id="28943" url="https://en.wikipedia.org/wiki?curid=28943" title="Shōgun">
Shōgun

The was the title of the military dictators of Japan during most of the period spanning from 1185 to 1868. Nominally appointed by the Emperor, "shōguns" were usually the "de facto" rulers of the country, though during part of the Kamakura period "shōguns" were themselves figureheads. The office of "shōgun" was in practice hereditary, though over the course of the history of Japan several different clans held the position. "Shōgun" is the short form of , a high military title from the early Heian period in the 8th and 9th centuries; when Minamoto no Yoritomo gained political ascendency over Japan in 1185, the title was revived to regularize his position, making him the first "shōgun" in the usually understood sense.

The "shōgun"s officials were collectively referred to as the "bakufu," or "tent government"; they were the ones who carried out the actual duties of administration, while the Imperial court retained only nominal authority. The tent symbolized the "shōgun"s role as the military's field commander, but also denoted that such an office was meant to be temporary. Nevertheless, the institution, known in English as the shogunate ( ), persisted for nearly 700 years, ending when Tokugawa Yoshinobu relinquished the office to Emperor Meiji in 1867 as part of the Meiji Restoration.

The term , composed of the kanji 将, which means «commander» and 軍 which means «army», is the abbreviation of the historical title , which was used to refer to the general who commanded the army sent to fight the tribes of northern Japan. After the twelfth century, the term was used to designate the leader of the samurai. The title is also translated as "Commander-in-Chief of the Expeditionary Force Against the Barbarians". 

The term Seii Taishōgun is written with these kanji "征夷大将軍". 征 means «conquer or subjugate (seisuru, 征する)». 夷 means «barbarian or savage (ebisu, えびす)». The Ebisu were peoples formerly of northern Japan (Northern Honshu, Hokkaido, the Kuril Islands and Sakhalin) with distinct language and culture (i.e. the Ainu and Emishi)​. These people were conquered by the Japanese and integrated into the Japanese state since the 8th century. 大 means «big or great (dai, だい)». 将 means «commander, general, leader (shō, しょう)» 軍 means «army, armed forces (gun, ぐん)». So a more literal translation of Seii Taishōgun is "Great General of Barbarian Subjugation". Great General is similar to a Commander-in-chief which is a supreme commander of the armed forces. However, the shogun was more powerful than a supreme commander, more similar to a military dictator with complete or substantial control over Japan.

The administration of a shōgun is called In Japanese and literally means "government from the maku ()." During the battles, the head of the samurai army used to be sitting in a scissor chair inside a semi-open tent called maku that exhibited its respective mon or blazon. The application of the term bakufu to the shōgun government shows an extremely strong and representative symbolism.

Historically, similar terms to Seii Taishōgun were used with varying degrees of responsibility, although none of them had equal or more importance than Seii Taishōgun. Some of them were:

There is no consensus among the various authors since some sources consider Tajihi no Agatamori the first, others say Ōtomo no Otomaro, other sources assure that the first was Sakanoue no Tamuramaro, while others avoid the problem by just mentioning from the first Kamakura shogun Minamoto no Yoritomo.

Originally, the title of "Sei-i Taishōgun" ("Commander-in-Chief of the Expeditionary Force Against the Barbarians") was given to military commanders during the early Heian period for the duration of military campaigns against the Emishi, who resisted the governance of the Kyoto-based imperial court. Ōtomo no Otomaro was the first "Sei-i Taishōgun". The most famous of these "shōguns" was Sakanoue no Tamuramaro.

In the later Heian period, one more "shōgun" was appointed. Minamoto no Yoshinaka was named "sei-i taishōgun" during the Genpei War, only to be killed shortly thereafter by Minamoto no Yoshitsune.

Sakanoue no Tamuramaro (758-811) was a Japanese general who fought against the tribes of northern Japan (settled in the territory that today integrates the provinces of Mutsu and Dewa). Tamarumaro was the first general to bend these tribes, integrating its territory to that of the Japanese State. For his military feats he was named Seii Taishōgun and probably because he was the first to win the victory against the northern tribes he is generally recognized as the first shōgun in history. (Note: according to historical sources Ōtomo no Otomaro also had the title of Seii Taishōgun).

In the early 11th century, "daimyō" protected by samurai came to dominate internal Japanese politics. Two of the most powerful families – the Taira and Minamoto – fought for control over the declining imperial court. The Taira family seized control from 1160 to 1185, but was defeated by the Minamoto in the Battle of Dan-no-ura. Minamoto no Yoritomo seized power from the central government and aristocracy and established a feudal system based in Kamakura in which the private military, the samurai, gained some political powers while the Emperor and the aristocracy remained the "de jure" rulers. In 1192, Yoritomo was awarded the title of "Sei-i Taishōgun" by Emperor Go-Toba and the political system he developed with a succession of "shōguns" as the head became known as a shogunate. Yoritomo's wife's family, the Hōjō, seized power from the Kamakura "shōguns". When Yoritomo's sons and heirs were assassinated, the "shōgun" himself became a hereditary figurehead. Real power rested with the Hōjō regents. The Kamakura shogunate lasted for almost 150 years, from 1192 to 1333.

The end of the Kamakura shogunate came when Kamakura fell in 1333, and the Hōjō Regency was destroyed. Two imperial families – the senior Northern Court and the junior Southern Court – had a claim to the throne. The problem was solved with the intercession of the Kamakura shogunate, who had the two lines alternate. This lasted until 1331, when Emperor Go-Daigo (of the Southern Court) tried to overthrow the shogunate to stop the alternation. As a result, Daigo was exiled. Around 1334–1336, Ashikaga Takauji helped Daigo regain his throne.

The fight against the shogunate left the Emperor with too many people claiming a limited supply of land. Takauji turned against the Emperor when the discontent about the distribution of land grew great enough. In 1336 Daigo was banished again, in favor of a new Emperor.

During the Kenmu Restoration, after the fall of the Kamakura shogunate in 1333, another short-lived "shōgun" arose. Prince Moriyoshi (Morinaga), son of Go-Daigo, was awarded the title of "Sei-i Taishōgun". However, Prince Moriyoshi was later put under house arrest and, in 1335, killed by Ashikaga Tadayoshi.

In 1338, Ashikaga Takauji, like Minamoto no Yoritomo, a descendant of the Minamoto princes, was awarded the title of "sei-i taishōgun" and established the Ashikaga shogunate, which nominally lasted until 1573. The Ashikaga had their headquarters in the Muromachi district of Kyoto, and the time during which they ruled is also known as the Muromachi period.

While the title of "Shōgun" went into abeyance due to technical reasons, Oda Nobunaga and his successor, Toyotomi Hideyoshi, who later obtained the position of Imperial Regent, gained far greater power than any of their predecessors had. Hideyoshi is considered by many historians to be among Japan's greatest rulers.

Tokugawa Ieyasu seized power and established a government at Edo (now known as Tokyo) in 1600. He received the title "sei-i taishōgun" in 1603, after he forged a family tree to show he was of Minamoto descent. The Tokugawa shogunate lasted until 1867, when Tokugawa Yoshinobu resigned as "shōgun" and abdicated his authority to Emperor Meiji. Ieyasu set a precedent in 1605 when he retired as "shōgun" in favour of his son Tokugawa Hidetada, though he maintained power from behind the scenes as (, cloistered "shōgun").

During the Edo period, effective power rested with the Tokugawa "shōgun", not the Emperor in Kyoto, even though the former ostensibly owed his position to the latter. The "shōgun" controlled foreign policy, the military, and feudal patronage. The role of the Emperor was ceremonial, similar to the position of the Japanese monarchy after the Second World War.

The term originally meant the dwelling and household of a "shōgun", but in time, became a metonym for the system of government of a feudal military dictatorship, exercised in the name of the "shōgun" or by the "shōgun" himself. Therefore, various "bakufu" held absolute power over the country (territory ruled at that time) without pause from 1192 to 1867, glossing over actual power, clan and title transfers.

The shogunate system was originally established under the Kamakura shogunate by Minamoto no Yoritomo. Although theoretically, the state (and therefore the Emperor) held ownership of all land in Japan. The system had some feudal elements, with lesser territorial lords pledging their allegiance to greater ones. Samurai were rewarded for their loyalty with agricultural surplus, usually rice, or labor services from peasants. In contrast to European feudal knights, samurai were not landowners. The hierarchy that held this system of government together was reinforced by close ties of loyalty between the "daimyōs", samurai and their subordinates.

Each shogunate was dynamic, not static. Power was constantly shifting and authority was often ambiguous. The study of the ebbs and flows in this complex history continues to occupy the attention of scholars. Each shogunate encountered competition. Sources of competition included the Emperor and the court aristocracy, the remnants of the imperial governmental systems, the "daimyōs", the "shōen" system, the great temples and shrines, the "sōhei", the "shugo" and "jitō", the "jizamurai" and early modern "daimyō". Each shogunate reflected the necessity of new ways of balancing the changing requirements of central and regional authorities.

Since Minamoto no Yoritomo turned the figure of the shōgun into a permanent and hereditary position and until the Meiji Restoration there were two ruling classes in Japan: 1. the emperor or , who acted as «chief priest» of the official religion of the country, Shinto, and 2. the shōgun, head of the army who also enjoyed civil, military, diplomatic and judicial authority. Although in theory the shōgun was an emperor's servant, it became the true power behind the throne.

No shōgun tried to usurp the throne, even when they had at their disposal the military power of the territory. There were two reasons primarily:


Unable to usurp the throne, the Shoguns sought throughout history to keep the emperor away from the country's political activity, relegating them from the sphere of influence. One of the few powers that the imperial house could retain was that of being able to "control time" through the designation of the Japanese Nengō or Eras and the issuance of calendars.

This is a highlight of two historical attempts of the emperor to recover the power they enjoyed before the establishment of the shogunate. In 1219 the Emperor Go-Toba accused the Hōjō as outlaws. Imperial troops mobilized, leading to the Jōkyū War (1219-1221), which would culminate in the third Battle of Uji (1221). During this, the imperial troops were defeated and the emperor Go-Toba was exiled. With the defeat of Go-Toba, the samurai government over the country was confirmed. At the beginning of the fourteenth century the Emperor Go-Daigo decided to rebel, but the Hōjō, who were then regents, sent an army from Kamakura. The emperor fled before the troops arrived and took the imperial insignia. The shōgun named his own emperor, giving rise to the era .

During the 1850s and 1860s, the shogunate was severely pressured both abroad and by foreign powers. It was then that various groups angry with the shogunate for the concessions made to the various European countries found in the figure of the emperor an ally through which they could expel the Tokugawa shogunate from power. The motto of this movement was and they finally succeeded in 1868, when imperial power was restored after centuries of being in the shadow of the country's political life.

Upon Japan's surrender after World War II, American Army General Douglas MacArthur became Japan's "de facto" ruler during the years of occupation. So great was his influence in Japan that he has been dubbed the .

Today, the head of the Japanese government is the Prime Minister; the usage of the term ""shōgun"" has nevertheless continued in colloquialisms. A retired Prime Minister who still wields considerable power and influence behind the scenes is called a , a sort of modern incarnation of the cloistered rule. Examples of "shadow "shōguns"" are former Prime Minister Kakuei Tanaka and the politician Ichirō Ozawa.





</doc>
<doc id="28944" url="https://en.wikipedia.org/wiki?curid=28944" title="Short-term memory">
Short-term memory

Short-term memory (or "primary" or "active memory") is the capacity for holding, but not manipulating, a small amount of information in mind in an active, readily available state for a short period of time. For example, short-term memory can be used to remember a phone number that has just been recited. The duration of short-term memory (when rehearsal or active maintenance is prevented) is believed to be in the order of seconds. The most commonly cited capacity is "The Magical Number Seven, Plus or Minus Two" (which is frequently referred to as "Miller's Law"), despite the facts that Miller himself stated that the figure was intended as "little more than a joke" (Miller, 1989, page 401) and that Cowan (2001) provided evidence that a more realistic figure is 4±1 units. In contrast, long-term memory can hold the information indefinitely.
Short-term memory should be distinguished from working memory, which refers to structures and processes used for temporarily storing and manipulating information (see details below).

The idea of the division of memory into short-term and long-term dates back to the 19th century. A classical model of memory developed in the 1960s assumed that all memories pass from a short-term to a long-term store after a small period of time. This model is referred to as the "modal model" and has been most famously detailed by Shiffrin. The exact mechanisms by which this transfer takes place, whether all or only some memories are retained permanently, and indeed the existence of a genuine distinction between the two stores, remain controversial topics among experts.

One form of evidence, cited in favor of the separate existence of a short-term store comes from anterograde amnesia, the inability to learn new facts and episodes. Patients with this form of amnesia, have intact ability to retain small amounts of information over short time scales (up to 30 seconds) but are dramatically impaired in their ability to form longer-term memories (a famous example is patient HM). This is interpreted as showing that the short-term store is spared from amnesia and other brain diseases.

Other evidence comes from experimental studies showing that some manipulations (e.g., a distractor task, such as repeatedly subtracting a single-digit number from a larger number following learning; cf Brown-Peterson procedure) impair memory for the 3 to 5 most recently learned words of a list (it is presumed, still held in short-term memory), while leaving recall for words from earlier in the list (it is presumed, stored in long-term memory) unaffected; other manipulations (e.g., semantic similarity of the words) affect only memory for earlier list words, but do not affect memory for the last few words in a list. These results show that different factors affect short-term recall (disruption of rehearsal) and long-term recall (semantic similarity). Together, these findings show that long-term memory and short-term memory can vary independently of each other.

Not all researchers agree that short-term and long-term memory are separate systems. Some theorists propose that memory is unitary over all time scales, from milliseconds to years. Support for the unitary memory hypothesis comes from the fact that it has been difficult to demarcate a clear boundary between short-term and long-term memory. For instance, Tarnow shows that the recall probability vs. latency curve is a straight line from 6 to 600 seconds (ten minutes), with the probability of failure to recall only saturating after 600 seconds. If there were really two different memory stores operating in this time frame, one could expect a discontinuity in this curve. Other research has shown that the detailed pattern of recall errors looks remarkably similar for recall of a list immediately after learning (it is presumed, from short-term memory) and recall after 24 hours (necessarily from long-term memory).

Further evidence against the existence of a short-term memory store comes from experiments involving continual distractor tasks. In 1974, Robert Bjork and William B. Whitten presented subjects with word pairs to be remembered; however, before and after each word pair, subjects had to do a simple multiplication task for 12 seconds. After the final word-pair, subjects had to do the multiplication distractor task for 20 seconds. In their results, Bjork and Whitten found that the recency effect (the increased probability of recall of the last items studied) and the primacy effect (the increased probability of recall of the first few items) still remained. These results would seem inconsistent with the idea of short-term memory as the distractor items would have taken the place of some of the word-pairs in the buffer, thereby weakening the associated strength of the items in long-term memory. Bjork and Whitten hypothesized that these results could be attributed to the memory processes at work for long-term memory retrieval versus short-term memory retrieval.
Ovid J.L. Tzeng (1973) also found an instance where the recency effect in free recall did not seem to result from the function of a short-term memory store. Subjects were presented with four study-test periods of 10 word lists, with a continual distractor task (20-second period of counting-backward). At the end of each list, participants had to free recall as many words from the list as possible. After free-recall of the fourth list, participants were asked to free recall items from all four lists. Both the initial free recall and the final free recall showed a recency effect. These results went against the predictions of a short-term memory model, where no recency effect would be expected in either initial or final free recall.
Koppenaal and Glanzer (1990) attempted to explain these phenomena as a result of the subjects' adaptation to the distractor task, which therefore allowed them to preserve at least some of the functions of the short-term memory store. As evidence, they provided the results of their experiment, in which the long-term recency effect disappeared when the distractor after the last item differed from the distractors that preceded and followed all the other items (e.g., arithmetic distractor task and word reading distractor task).
Thapar and Greene challenged this theory. In one of their experiments, participants were given a different distractor task after every item to be studied. According to Koppenaal's and Glanzer's theory, there should be no recency effect as subjects would not have had time to adapt to the distractor; yet such a recency effect remained in place in the experiment.

One proposed explanation of the existence of the recency effect in a continual distractor condition, and the disappearance of it in an end-only distractor task is the influence of contextual and distinctive processes. According to this model, recency is a result of the final items' processing context being similar to the processing context of the other items and the distinctive position of the final items versus items in the middle of the list. In the end distractor task, the processing context of the final items is no longer similar to the processing context of the other list items. At the same time, retrieval cues for these items are no longer as effective as without the distractor. Therefore, the recency effect recedes or vanishes. However, when distractor tasks are placed before and after each item, the recency effect returns, because all the list items once again have similar processing context.

Various researchers have proposed that stimuli are coded in short-term memory using transmitter depletion. According to this hypothesis, a stimulus activates a spatial pattern of activity across neurons in a brain region. As these neurons fire, the available neurotransmitters in their store are depleted and this pattern of depletion is iconic, representing stimulus information and functions as a memory trace. The memory trace decays over time as a consequence of neurotransmitter reuptake mechanisms that restore neurotransmitters to the levels that existed prior to stimulus presentation.

The relationship between short-term memory and working memory is described differently by various theories, but it is generally acknowledged that the two concepts are distinct. Working memory is a theoretical framework that refers to structures and processes used for temporarily storing and manipulating information. As such, working memory might also be referred to as "working attention". Working memory and attention together play a major role in the processes of thinking. Short-term memory in general refers, in a theory-neutral manner, to the short-term storage of information, and it does not entail the manipulation or organization of material held in memory. Thus, while there are short-term memory components to working memory models, the concept of short-term memory is distinct from these more hypothetical concepts.

Within Baddeley's influential 1986 model of working memory there are two short-term storage mechanisms: the phonological loop and the visuospatial sketchpad. Most of the research referred to here involves the phonological loop, because most of the work done on short-term memory has used verbal material. Since the 1990s, however, there has been a surge in research on visual short-term memory, and also increasing work on spatial short-term memory.

The limited duration of short-term memory (~18 seconds without a form of memory rehearsal) quickly suggests that its contents spontaneously decay over time. The decay assumption is part of many theories of short-term memory, the most notable one being Baddeley's model of working memory. The decay assumption is usually paired with the idea of rapid covert rehearsal: In order to overcome the limitation of short-term memory, and retain information for longer, information must be periodically repeated or rehearsed—either by articulating it out loud or by mentally simulating such articulation. In this way, the information is likely to re-enter the short-term store and be retained for a further period.

Several researchers; however, dispute that spontaneous decay plays any significant role in forgetting over the short-term, and the evidence is far from conclusive.

Authors doubting that decay causes forgetting from short-term memory often offer as an alternative some form of interference: When several elements (such as digits, words, or pictures, or logos in general) are held in short-term memory simultaneously, their representations compete with each other for recall, or degrade each other. Thereby, new content gradually pushes out older content, unless the older content is actively protected against interference by rehearsal or by directing attention to it. 

Whatever the cause or causes of forgetting over the short-term may be, there is consensus that it severely limits the amount of new information that we can retain over brief periods of time. This limit is referred to as the finite capacity of short-term memory. The capacity of short-term memory is often called memory span, in reference to a common procedure of measuring it. In a memory span test, the experimenter presents lists of items (e.g. digits or words) of increasing length. An individual's span is determined as the longest list length that he or she can recall correctly in the given order on at least half of all trials.

In an early and highly influential article, The Magical Number Seven, Plus or Minus Two, psychologist George Miller suggested that human short-term memory has a forward memory span of approximately seven items plus or minus two and that that was well known at the time (it seems to go back to the 19th-century researcher Wundt). More recent research has shown that this "magical number seven" is roughly accurate for college students recalling lists of digits, but memory span varies widely with populations tested and with material used. For example, the ability to recall words in order depends on a number of characteristics of these words: fewer words can be recalled when the words have longer spoken duration; this is known as the "word-length effect", or when their speech sounds are similar to each other; this is called the "phonological similarity effect". More words can be recalled when the words are highly familiar or occur frequently in the language. Recall performance is also better when all of the words in a list are taken from a single semantic category (such as games) than when the words are taken from different categories. A more up-to-date estimate of short-term memory capacity is about four pieces or "chunks" of information. However other prominent theories of short-term memory capacity argue against measuring capacity in terms of a fixed number of elements.

Rehearsal is the process where information is kept in short-term memory by mentally repeating it. When the information is repeated each time, that information is reentered into the short-term memory, thus keeping that information for another 10 to 20 seconds (the average storage time for short-term memory).

Chunking is a process by which one can expand his/her ability to remember things in the short term. Chunking is also a process by which a person organizes material into meaningful groups. Although the average person may retain only about four different units in short-term memory, chunking can greatly increase a person's recall capacity. For example, in recalling a phone number, the person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456), and, last, a four-digit chunk (7890). This method of remembering phone numbers is far more effective than attempting to remember a string of 10 digits.

Practice and the usage of existing information in long-term memory can lead to additional improvements in one's ability to use chunking. In one testing session, an American cross-country runner was able to recall a string of 79 digits after hearing them only once by chunking them into different running times (e.g., the first four numbers were 1518, a three-mile time).

It is very difficult to demonstrate the exact capacity of short-term memory (STM) because it will vary depending on the nature of the material to be recalled. There is currently no way of defining the basic unit of information to be stored in the STM store. It is also possible that STM is not the store described by Atkinson and Shiffrin. In that case, the task of defining the task of STM becomes even more difficult.

However, capacity of STM can be affected by the following:
Influence of long-term memory, Reading aloud, Pronunciation time and Individual differences.

Diseases that cause neurodegeneration, such as Alzheimer's disease, can also be a factor in a person's short-term and eventually long-term memory. Damage to certain sections of the brain due to this disease causes a shrinkage in the cerebral cortex which disables the ability to think and recall memories.

Memory loss is a natural process in aging. One study investigated whether or not there were deficits in short-term memory in older adults. This was a previous study which compiled normative French data for three short-term memory tasks (Verbal, visual and spatial). They found impairments present in participants between the ages of 55 and 85 years of age.

Memory distortion in Alzheimer's disease is a very common disorder found in older adults. Performance of patients with mild to moderate Alzheimer's disease was compared with the performance of age matched healthy adults. Researchers concluded the study with findings that showed reduced short-term memory recall for Alzheimer's patients. Episodic memory and semantic abilities deteriorate early in Alzheimer's disease. Since the cognitive system includes interconnected and reciprocally influenced neuronal networks, one study hypothesized that stimulation of lexical-semantic abilities may benefit semantically structured episodic memory. They found that with Lexical-Semantic stimulation treatment may improve episodic memory in Alzheimer's Disease patients. It could also be regarded as a clinical option to counteract the cognitive decline typical of the disease.

Aphasias are also seen in many elder adults. Aphasias are responsible for many sentence comprehension deficits. Many language-impaired patients make several complaints about short-term memory deficits, with several family members confirming that patients have trouble recalling previously known names and events. The opinion is supported by many studies showing that many aphasics also have trouble with visual-memory required tasks.

Core symptoms of schizophrenia patients have been linked to cognitive deficits. One neglected factor that contributes to those deficits is the comprehension of time. In this study, results confirm that cognitive dysfunctions are a major deficit in patients with schizophrenia. The study provided evidence that patients with schizophrenia process temporal information inefficiently.

Advanced age is associated with decrements in episodic memory. The associative deficit is in which age differences in recognition memory reflect difficulty in binding components of a memory episode and bound units. A previous study used mixed and blocked test designs to examine deficits in short-term memory of older adults and found there was an associative deficit for older adults. This study along with many other previous studies, continue to build evidence of deficits found in older adults short-term memory.

Even when neurological diseases and disorders are not present, there is a progressive and gradual loss of some intellectual functions that become evident in later years. There are several tests used to examine the psychophysical characteristics of the elderly and of them, a well suitable test would be the functional reach (FR) test, and the mini–mental state examination (MMSE). The FR test is an index of the aptitude to maintain balance in an upright position and the MMSE test is a global index of cognitive abilities. These tests were both used by Costarella et al. to evaluate the psychophysical characteristics of older adults. They found a loss of physical performance (FR, related to height) as well as a loss of cognitive abilities (MMSE).

Posttraumatic stress disorder (PTSD) is associated with altered processing of emotional material with a strong attentional bias toward trauma-related information and interferes with cognitive processing. Aside from trauma processing specificities, a wide range of cognitive impairments have been related to PTSD state with predominant attention and verbal memory deficits.

There have been few studies done on the relationship between short-term memory and intelligence in PTSD. However, examined whether people with PTSD had equivalent levels of short-term, non-verbal memory on the Benton Visual Retention Test (BVRT), and whether they had equivalent levels of intelligence on the Raven Standard Progressive Matrices (RSPM). They found that people with PTSD had worse short-term, non-verbal memory on the BVRT, despite having comparable levels of intelligence on the RSPM, concluding impairments in memory influence intelligence assessments in the subjects.

There are many tests to measure digit span and short term visual memory, some paper- and some computer-based, including the following:





</doc>
<doc id="28945" url="https://en.wikipedia.org/wiki?curid=28945" title="State supreme court">
State supreme court

In the United States, a state supreme court (known by other names in some states) is the ultimate judicial tribunal in the court system of a particular state ("i.e.", that state's court of last resort). On matters of state law, the decisions of a state supreme court are considered final and binding on state and even United States federal courts.

Generally, the state supreme court, like most appellate tribunals, is exclusively for hearing appeals of legal issues. It does not make any finding of facts, and thus holds no trials. In the case where the trial court made an egregious error in its finding of facts, the state supreme court will remand to the trial court for a new trial. This responsibility of correcting the errors of inferior courts is the origin of a number of the different names for supreme courts in various state court systems.

The court consists of a panel of judges selected by methods outlined in the state constitution. State supreme courts are completely distinct from any United States federal courts located within the geographical boundaries of a state's territory, or the federal United States Supreme Court (although appeals, on some issues, from judgments of a state's highest court can be sought in the U.S. Supreme Court).

Under American federalism, a state supreme court's ruling on a matter of purely state law is final and binding and must be accepted in both state and federal courts. Where applicable, state supreme courts also apply federal law. 

Federal appellate review of state supreme court rulings on federal matters may be sought by way of a petition for writ of "certiorari" to the Supreme Court of the United States. As the U.S. Supreme Court recognized in "Erie Railroad Co. v. Tompkins" (1938), no part of the federal Constitution actually grants federal courts or the federal Congress the power to directly dictate the content of state law (as distinguished from creating altogether separate federal law that in a particular situation may override state law). Clause 1 of Section 2 of Article Three of the United States Constitution describes the scope of federal judicial power, but only extended it to "the Laws of the United States" and not the laws of the several or individual states. It is this silence on that latter issue that gave rise to the American distinction between state and federal common law not found in other English-speaking common law federations like Australia and Canada.

One of the informal traditions of the American legal system, derived from the common law, is that all litigants are guaranteed at least one appeal after a final judgment on the merits. However, appeal is merely a "privilege" provided by statute in 47 states and in federal judicial proceedings; the U.S. Supreme Court has repeatedly ruled that there is no federal constitutional "right" to an appeal.

Since a few states lack intermediate appellate courts, the state supreme court may operate under "mandatory review", in which it "must" hear all appeals from the trial courts. This was the case, for example, in Nevada (prior to 2014). Such judicial systems are usually very congested.

Most state supreme courts have implemented "discretionary review," like their federal counterpart. Under such a system, intermediate appellate courts are entrusted with deciding the vast majority of appeals. Intermediate appellate courts generally focus on the mundane task of what appellate specialists call "error correction," which means their primary task is to decide whether the record reflects that the trial court correctly applied existing law. 

For certain limited categories of cases, the state supreme court still operates under mandatory review, usually with regard to cases involving the interpretation of the state constitution or capital punishment. But for the vast majority, the state supreme court possesses the discretion to grant "certiorari" (known as "review" in states that discourage the use of Latin). These cases usually pertain to issues which different appellate courts within its jurisdiction have decided differently, or highly controversial cases involving a completely new legal issue never seen in that state. In other words, once the state supreme court is able to offload the tedious burden of error correction to intermediate courts, it can then focus on the long-term task (i.e., a policymaking role) of developing a coherent body of case law for the people of its state. 

Iowa, Oklahoma, and Nevada have a unique procedure for appeals. In those states, "all" appeals are filed with the appropriate Supreme Court (Iowa has a single Supreme Court, while Oklahoma has separate civil and criminal Supreme Courts) which then keeps all cases of first impression for itself to decide. It forwards the remaining caseswhich deal with points of law it has already addressedto the intermediate Court of Appeals. Under this so-called "push-down" or "deflection" model of appellate procedure, the state supreme court can immediately establish final statewide precedents on important issues of first impression as soon as they arise, rather than waiting several months or years for the intermediate appellate court to make a first attempt at resolving the issue (and leaving the law uncertain in the interim). 

Notably, the Supreme Court of Virginia operates under discretionary review for nearly all cases, but the intermediate Court of Appeals of Virginia hears appeals as a matter of right only in family and administrative cases. The result is that there is "no" first appeal of right for the vast majority of civil and criminal cases in that state. Appellants are still free to petition for review, of course, but such petitions are subject to severe length constraints (6,125 words or 35 pages in Virginia) and necessarily are more narrowly targeted than a long opening appellate brief to an intermediate appellate court (in contrast, an opening brief to a California intermediate appellate court can run up to 14,000 words). In turn, the vast majority of decisions of Virginia circuit courts in civil and criminal cases are thereby insulated from appellate review on the merits. 

New Hampshire and West Virginia formerly also provided only discretionary review for nearly all cases, even though they had no intermediate appellate court. Both states gradually recognized that even if this arrangement did not offend the federal Constitution, it was unduly harsh for hapless appellants, and transitioned to mandatory review, respectively, in 2004 and 2010.

As noted above, the U.S. Supreme Court may hear appeals from state supreme courts "only" if there is a question of law under the United States Constitution (which includes issues arising from federal treaties, statutes, or regulations), and those appeals are heard at the Court's sole discretion (that is, only if the Court grants a petition for writ of certiorari). 

In theory, state supreme courts are bound by the precedent established by the U.S. Supreme Court as to all issues of federal law, but in practice, the Supreme Court reviews very few decisions from state courts. For example, in 2007 the Court reviewed 244 cases appealed from federal courts and only 22 from state courts. Despite the relatively small number of decisions reviewed, Professors Sara Benesh and Wendy Martinek found that state supreme courts follow precedent more closely than federal courts in the area of search and seizure and appear to follow precedent in confessions as well.

Traditionally, state supreme courts are headquartered in the capital cities of their respective states, though they may occasionally hold oral arguments elsewhere. The seven main exceptions are:

As for the court's actual facilities, a state supreme court may be housed in the state capitol, in a nearby state office building shared with other courts or state executive branch agencies, or in a small courthouse reserved for its exclusive use. State supreme courts normally require a courtroom for oral argument, private chambers for all justices, a conference room, offices for law clerks and other support staff, a law library, and a lobby with a window where the court clerk can accept filings and release new decisions in the form of "slip opinions" (that is, in looseleaf format held together only by a staple).

Because state supreme courts generally hear only appeals, some courts have names which directly indicate their functionin the states of New York and Maryland, and in the District of Columbia, the highest court is called the "Court of Appeals". In New York, the "Supreme Court" is the trial court of general unlimited jurisdiction and the intermediate appellate court is called the "Supreme Court—Appellate Division". Maryland's jury trial courts are called "Circuit Courts" (non-jury trials are usually conducted by the "District Courts," whose decisions may be appealed to the Circuit Courts), and the intermediate appellate court is called the "Court of Special Appeals". West Virginia mixes the two; its highest court is called the "Supreme Court of Appeals".

Other states' supreme courts have used the term "Appeals": New Jersey's supreme courts under the 1844 constitution and Delaware's supreme court were both the "Court of Errors and Appeals"; The term "Errors" refers to the now-obsolete writ of error, which was used by state supreme courts to correct certain types of egregious errors committed by lower courts.

Massachusetts and New Hampshire originally named their highest courts the "Superior Court of Judicature." Currently, Massachusetts uses the names "Supreme Judicial Court" (to distinguish itself from the state legislature, which is called the Massachusetts General Court), while New Hampshire uses the name "Supreme Court". Additionally the highest court in Maine is named the "Supreme Judicial Court". This similar terminology is probably a holdover from the time when Maine was part of Massachusetts. In Connecticut, Delaware, New Jersey, and New York, the highest courts formerly used variations of the term "Court of Errors," which indicated that the court's primary purpose was to correct the errors of lower courts.

Oklahoma and Texas have two separate supreme courts: one for criminal appeals and one for civil cases. In both states, the first is formally called the Court of Criminal Appeals, and the second is called the Supreme Court.

Texas and Oklahoma have dual supreme courts. In Texas, both have nine justices. In Oklahoma the Supreme Court has nine justices and the Court of Criminal Appeals has five (assimilated to nine in the above table).

Thirty-two states have mandatory retirement ages for justices, nearly all from 70 to 75 years old. It varies whether justices must retire on their birthday, at the end of that year, or the end of that term in which they reach the retirement age. Rhode Island is the only state with neither terms for reelection or retention or a retirement age.

Judges are either appointed, selected through a merit process (with an election thereafter in some cases), or elected. The elections may be through partisan or nonpartisan elections. A nonpartisan election does not mean that the judges run and are selected with no regard to political beliefs. In many cases nonpartisan election merely means the prospective judges' parties are not printed on the ballot. Political contributions to these campaigns may be allowed, including from trade associations and political action committees. The plurality of states (14) follow the Missouri Plan, in which the governor makes an appointment from selection of candidates put forward by a judicial nominating commission. Gubernatorial appointments are typically subject to regular retention elections. 

Of the seven states that elect justices in partisan elections, Democrats have a majority in four and Republicans in three. Although Michigan and Ohio have nonpartisan general elections, candidates are nominated by parties; Republicans hold a majority in both states. 

In other states with contested elections, ideological leanings may be ascertained from statements, endorsements, and donations. Judges appointed by governors typically share the partisanship or ideology of that governor, but this relationship may be constrained by the makeup of a nominating commission or confirmation body.

States
†Partisanship reflects the parties of the appointing governors

Territories and federal district



</doc>
<doc id="28946" url="https://en.wikipedia.org/wiki?curid=28946" title="Stability">
Stability

Stability may refer to:










</doc>
<doc id="28951" url="https://en.wikipedia.org/wiki?curid=28951" title="Spyware">
Spyware

Spyware is a software that aims to gather information about a person or organization, without their knowledge, and send such information to another entity without the consumer's consent. Furthermore, spyware asserts control over a device without the consumer's knowledge, sending confidential information to another entity with the consumer's consent, through cookies.

Spyware is mostly classified into four types: adware, system monitors, tracking cookies, and trojans; examples of other notorious types include digital rights management capabilities that "phone home", keyloggers, rootkits, and web beacons.

Spyware is mostly used for the stealing information and storing Internet users' movements on the Web and serving up pop-up ads to Internet users. Whenever spyware is used for malicious purposes, its presence is typically hidden from the user and can be difficult to detect. Some spyware, such as keyloggers, may be installed by the owner of a shared, corporate, or public computer intentionally in order to monitor users.

While the term "spyware" suggests software that monitors a user's computing, the functions of spyware can extend beyond simple monitoring. Spyware can collect almost any type of data, including personal information like internet surfing habits, user logins, and bank or credit account information. Spyware can also interfere with a user's control of a computer by installing additional software or redirecting web browsers. Some spyware can change computer settings, which can result in slow Internet connection speeds, un-authorized changes in browser settings, or changes to software settings.

Sometimes, spyware is included along with genuine software, and may come from a malicious website or may have been added to the intentional functionality of genuine software (see the paragraph about Facebook, below). In response to the emergence of spyware, a small industry has sprung up dealing in anti-spyware software. Running anti-spyware software has become a widely recognized element of computer security practices, especially for computers running Microsoft Windows. A number of jurisdictions have passed anti-spyware laws, which usually target any software that is surreptitiously installed to control a user's computer.

In German-speaking countries, spyware used or made by the government is called "govware" by computer experts (in common parlance: "Regierungstrojaner", literally "Government Trojan"). Govware is typically a trojan horse software used to intercept communications from the target computer. Some countries, like Switzerland and Germany, have a legal framework governing the use of such software. In the US, the term "policeware" has been used for similar purposes.

Use of the term "spyware" has eventually declined as the practice of tracking users has been pushed ever further into the mainstream by major websites and data mining companies; these generally break no known laws and compel users to be tracked, not by fraudulent practices "per se", but by the default settings created for users and the language of terms-of-service agreements. In one documented example, on CBS/CNet News reported, on March 7, 2011, on a "Wall Street Journal" analysis revealing the practice of Facebook and other websites of tracking users' browsing activity, linked to their identity, far beyond users' visits and activity within the Facebook site itself. The report stated: "Here's how it works. You go to Facebook, you log in, you spend some time there, and then ... you move on without logging out. Let's say the next site you go to is "New York Times". Those buttons, without you clicking on them, have just reported back to Facebook and Twitter that you went there and also your identity within those accounts. Let's say you moved on to something like a site about depression. This one also has a tweet button, a Google widget, and those, too, can report back who you are and that you went there." The" WSJ" analysis was researched by Brian Kennish, founder of Disconnect, Inc.

Spyware does not necessarily spread in the same way as a virus or worm because infected systems generally do not attempt to transmit or copy the software to other computers. Instead, spyware installs itself on a system by deceiving the user or by exploiting software vulnerabilities.

Most spyware is installed without knowledge, or by using deceptive tactics. Spyware may try to deceive users by bundling itself with desirable software. Other common tactics are using a Trojan horse, spy gadgets that look like normal devices but turn out to be something else, such as a USB Keylogger. These devices actually are connected to the device as memory units but are capable of recording each stroke made on the keyboard. Some spyware authors infect a system through security holes in the Web browser or in other software. When the user navigates to a Web page controlled by the spyware author, the page contains code which attacks the browser and forces the download and installation of spyware.

The installation of spyware frequently involves Internet Explorer. Its popularity and history of security issues have made it a frequent target. Its deep integration with the Windows environment make it susceptible to attack into the Windows operating system. Internet Explorer also serves as a point of attachment for spyware in the form of Browser Helper Objects, which modify the browser's behaviour.

A spyware rarely operates alone on a computer; an affected machine usually has multiple infections. Users frequently notice unwanted behavior and degradation of system performance. A spyware infestation can create significant unwanted CPU activity, disk usage, and network traffic. Stability issues, such as applications freezing, failure to boot, and system-wide crashes are also common. Spyware, which interferes with networking software commonly causes difficulty connecting to the Internet.

In some infections, the spyware is not even evident. Users assume in those situations that the performance issues relate to faulty hardware, Windows installation problems, or another malware infection. Some owners of badly infected systems resort to contacting technical support experts, or even buying a new computer because the existing system "has become too slow". Badly infected systems may require a clean reinstallation of all their software in order to return to full functionality.

Moreover, some types of spyware disable software firewalls and antivirus software, and/or reduce browser security settings, which opens the system to further opportunistic infections. Some spyware disables or even removes competing spyware programs, on the grounds that more spyware-related annoyances increase the likelihood that users will take action to remove the programs.

Keyloggers are sometimes part of malware packages downloaded onto computers without the owners' knowledge. Some keylogger software is freely available on the internet, while others are commercial or private applications. Most keyloggers allow not only keyboard keystrokes to be captured, they also are often capable of collecting screen captures from the computer.

A typical Windows user has administrative privileges, mostly for convenience. Because of this, any program the user runs has unrestricted access to the system. As with other operating systems, Windows users are able to follow the principle of least privilege and use non-administrator accounts. Alternatively, they can reduce the privileges of specific vulnerable Internet-facing processes, such as Internet Explorer.

Since Windows Vista is, by default, a computer administrator that runs everything under limited user privileges, when a program requires administrative privileges, a User Account Control pop-up will prompt the user to allow or deny the action. This improves on the design used by previous versions of Windows.

As the spyware threat has evolved, a number of techniques have emerged to counteract it. These include programs designed to remove or block spyware, as well as various user practices which reduce the chance of getting spyware on a system.

Nonetheless, spyware remains a costly problem. When a large number of pieces of spyware have infected a Windows computer, the only remedy may involve backing up user data, and fully reinstalling the operating system. For instance, some spyware cannot be completely removed by Symantec, Microsoft, PC Tools.

Many programmers and some commercial firms have released products dedicated to remove or block spyware. Programs such as PC Tools' Spyware Doctor, Lavasoft's "Ad-Aware SE" and Patrick Kolla's "Spybot - Search & Destroy" rapidly gained popularity as tools to remove, and in some cases intercept, spyware programs. On December 16, 2004, Microsoft acquired the "GIANT AntiSpyware" software, rebranding it as "Windows AntiSpyware beta" and releasing it as a free download for Genuine Windows XP and Windows 2003 users. (In 2006 it was renamed Windows Defender).

Major anti-virus firms such as Symantec, PC Tools, McAfee and Sophos have also added anti-spyware features to their existing anti-virus products. Early on, anti-virus firms expressed reluctance to add anti-spyware functions, citing lawsuits brought by spyware authors against the authors of web sites and programs which described their products as "spyware". However, recent versions of these major firms home and business anti-virus products do include anti-spyware functions, albeit treated differently from viruses. Symantec Anti-Virus, for instance, categorizes spyware programs as "extended threats" and now offers real-time protection against these threats.

Anti-spyware programs can combat spyware in two ways:
Such programs inspect the contents of the Windows registry, operating system files, and installed programs, and remove files and entries which match a list of known spyware. Real-time protection from spyware works identically to real-time anti-virus protection: the software scans disk files at download time, and blocks the activity of components known to represent spyware.
In some cases, it may also intercept attempts to install start-up items or to modify browser settings. Earlier versions of anti-spyware programs focused chiefly on detection and removal. Javacool Software's SpywareBlaster, one of the first to offer real-time protection, blocked the installation of ActiveX-based spyware.

Like most anti-virus software, many anti-spyware/adware tools require a frequently updated database of threats. As new spyware programs are released, anti-spyware developers discover and evaluate them, adding to the list of known spyware, which allows the software to detect and remove new spyware. As a result, anti-spyware software is of limited usefulness without regular updates. Updates may be installed automatically or manually.

A popular generic spyware removal tool used by those that requires a certain degree of expertise is HijackThis, which scans certain areas of the Windows OS where spyware often resides and presents a list with items to delete manually. As most of the items are legitimate windows files/registry entries it is advised for those who are less knowledgeable on this subject to post a HijackThis log on the numerous antispyware sites and let the experts decide what to delete.

If a spyware program is not blocked and manages to get itself installed, it may resist attempts to terminate or uninstall it. Some programs work in pairs: when an anti-spyware scanner (or the user) terminates one running process, the other one respawns the killed program. Likewise, some spyware will detect attempts to remove registry keys and immediately add them again. Usually, booting the infected computer in safe mode allows an anti-spyware program a better chance of removing persistent spyware. Killing the process tree may also work.

To detect spyware, computer users have found several practices useful in addition to installing anti-spyware programs. Many users have installed a web browser other than Internet Explorer, such as Mozilla Firefox or Google Chrome. Though no browser is completely safe, Internet Explorer was once at a greater risk for spyware infection due to its large user base as well as vulnerabilities such as ActiveX but these three major browsers are now close to equivalent when it comes to security.

Some ISPs—particularly colleges and universities—have taken a different approach to blocking spyware: they use their network firewalls and web proxies to block access to Web sites known to install spyware. On March 31, 2005, Cornell University's Information Technology department released a report detailing the behavior of one particular piece of proxy-based spyware, "Marketscore", and the steps the university took to intercept it. Many other educational institutions have taken similar steps.

Individual users can also install firewalls from a variety of companies. These monitor the flow of information going to and from a networked computer and provide protection against spyware and malware. Some users install a large hosts file which prevents the user's computer from connecting to known spyware-related web addresses. Spyware may get installed via certain shareware programs offered for download. Downloading programs only from reputable sources can provide some protection from this source of attack.

Individual users can use cellphone / computer with physical (electric) switch, or isolated electronic switch that disconnects microphone, camera without bypass and keep it in disconnected position where not in use, that limits information that spyware can collect. (Policy recommended by NIST Guidelines for Managing the Security of Mobile Devices, 2013).

A few spyware vendors, notably 180 Solutions, have written what the "New York Times" has dubbed "stealware", and what spyware researcher Ben Edelman terms "affiliate fraud", a form of click fraud. Stealware diverts the payment of affiliate marketing revenues from the legitimate affiliate to the spyware vendor.

Spyware which attacks affiliate networks places the spyware operator's affiliate tag on the user's activity – replacing any other tag, if there is one. The spyware operator is the only party that gains from this. The user has their choices thwarted, a legitimate affiliate loses revenue, networks' reputations are injured, and vendors are harmed by having to pay out affiliate revenues to an "affiliate" who is not party to a contract. Affiliate fraud is a violation of the terms of service of most affiliate marketing networks. As a result, spyware operators such as 180 Solutions have been terminated from affiliate networks including LinkShare and ShareSale. Mobile devices can also be vulnerable to chargeware, which manipulates users into illegitimate mobile charges.

In one case, spyware has been closely associated with identity theft. In August 2005, researchers from security software firm Sunbelt Software suspected the creators of the common CoolWebSearch spyware had used it to transmit "chat sessions, user names, passwords, bank information, etc."; however it turned out that "it actually (was) its own sophisticated criminal little trojan that's independent of CWS." This case is currently under investigation by the FBI.

The Federal Trade Commission estimates that 27.3 million Americans have been victims of identity theft, and that financial losses from identity theft totaled nearly $48 billion for businesses and financial institutions and at least $5 billion in out-of-pocket expenses for individuals.

Some copy-protection technologies have borrowed from spyware. In 2005, Sony BMG Music Entertainment was found to be using rootkits in its XCP digital rights management technology Like spyware, not only was it difficult to detect and uninstall, it was so poorly written that most efforts to remove it could have rendered computers unable to function.
Texas Attorney General Greg Abbott filed suit, and three separate class-action suits were filed. Sony BMG later provided a workaround on its website to help users remove it.

Beginning on April 25, 2006, Microsoft's Windows Genuine Advantage Notifications application was installed on most Windows PCs as a "critical security update". While the main purpose of this deliberately uninstallable application is to ensure the copy of Windows on the machine was lawfully purchased and installed, it also installs software that has been accused of "phoning home" on a daily basis, like spyware. It can be removed with the RemoveWGA tool.

Stalkerware is spyware that has been used to monitor electronic activities of partners in intimate relationships. At least one software package, Loverspy, was specifically marketed for this purpose. Depending on local laws regarding communal/marital property, observing a partner's online activity without their consent may be illegal; the author of Loverspy and several users of the product were indicted in California in 2005 on charges of wiretapping and various computer crimes.

Anti-spyware programs often report Web advertisers' HTTP cookies, the small text files that track browsing activity, as spyware. While they are not always inherently malicious, many users object to third parties using space on their personal computers for their business purposes, and many anti-spyware programs offer to remove them.

These common spyware programs illustrate the diversity of behaviors found in these attacks. Note that as with computer viruses, researchers give names to spyware programs which may not be used by their creators. Programs may be grouped into "families" based not on shared program code, but on common behaviors, or by "following the money" of apparent financial or business connections. For instance, a number of the spyware programs distributed by Claria are collectively known as "Gator". Likewise, programs that are frequently installed together may be described as parts of the same spyware package, even if they function separately.

The first recorded use of the term spyware occurred on October 16, 1995 in a Usenet post that poked fun at Microsoft's business model. "Spyware" at first denoted "software" meant for espionage purposes. However, in early 2000 the founder of Zone Labs, Gregor Freund, used the term in a press release for the ZoneAlarm Personal Firewall. Later in 2000, a parent using ZoneAlarm was alerted to the fact that "Reader Rabbit," educational software marketed to children by the Mattel toy company, was surreptitiously sending data back to Mattel. Since then, "spyware" has taken on its present sense.

According to a 2005 study by AOL and the National Cyber-Security Alliance, 61 percent of surveyed users' computers were infected with form of spyware. 92 percent of surveyed users with spyware reported that they did not know of its presence, and 91 percent reported that they had not given permission for the installation of the spyware.
, spyware has become one of the preeminent security threats to computer systems running Microsoft Windows operating systems. Computers on which Internet Explorer (IE) is the primary browser are particularly vulnerable to such attacks, not only because IE is the most widely used, but because its tight integration with Windows allows spyware access to crucial parts of the operating system.

Before Internet Explorer 6 SP2 was released as part of Windows XP Service Pack 2, the browser would automatically display an installation window for any ActiveX component that a website wanted to install. The combination of user ignorance about these changes, and the assumption by Internet Explorer that all ActiveX components are benign, helped to spread spyware significantly. Many spyware components would also make use of exploits in JavaScript, Internet Explorer and Windows to install without user knowledge or permission.

The Windows Registry contains multiple sections where modification of key values allows software to be executed automatically when the operating system boots. Spyware can exploit this design to circumvent attempts at removal. The spyware typically will link itself from each location in the registry that allows execution. Once running, the spyware will periodically check if any of these links are removed. If so, they will be automatically restored. This ensures that the spyware will execute when the operating system is booted, even if some (or most) of the registry links are removed.



Malicious programmers have released a large number of rogue (fake) anti-spyware programs, and widely distributed Web banner ads can warn users that their computers have been infected with spyware, directing them to purchase programs which do not actually remove spyware—or else, may add more spyware of their own.

The proliferation of fake or spoofed antivirus products that bill themselves as antispyware can be troublesome. Users may receive popups prompting them to install them to protect their computer, when it will in fact add spyware. This software is called rogue software. It is recommended that users do not install any freeware claiming to be anti-spyware unless it is verified to be legitimate. Some known offenders include:

Fake antivirus products constitute 15 percent of all malware.

On January 26, 2006, Microsoft and the Washington state attorney general filed suit against Secure Computer for its Spyware Cleaner product.

Unauthorized access to a computer is illegal under computer crime laws, such as the U.S. Computer Fraud and Abuse Act, the U.K.'s Computer Misuse Act, and similar laws in other countries. Since owners of computers infected with spyware generally claim that they never authorized the installation, a "prima facie" reading would suggest that the promulgation of spyware would count as a criminal act. Law enforcement has often pursued the authors of other malware, particularly viruses. However, few spyware developers have been prosecuted, and many operate openly as strictly legitimate businesses, though some have faced lawsuits.

Spyware producers argue that, contrary to the users' claims, users do in fact give consent to installations. Spyware that comes bundled with shareware applications may be described in the legalese text of an end-user license agreement (EULA). Many users habitually ignore these purported contracts, but spyware companies such as Claria say these demonstrate that users have consented.

Despite the ubiquity of EULAs agreements, under which a single click can be taken as consent to the entire text, relatively little caselaw has resulted from their use. It has been established in most common law jurisdictions that this type of agreement can be a binding contract "in certain circumstances." This does not, however, mean that every such agreement is a contract, or that every term in one is enforceable.

Some jurisdictions, including the U.S. states of Iowa and Washington, have passed laws criminalizing some forms of spyware. Such laws make it illegal for anyone other than the owner or operator of a computer to install software that alters Web-browser settings, monitors keystrokes, or disables computer-security software.

In the United States, lawmakers introduced a bill in 2005 entitled the Internet Spyware Prevention Act, which would imprison creators of spyware.

The US Federal Trade Commission has sued Internet marketing organizations under the "unfairness doctrine" to make them stop infecting consumers' PCs with spyware. In one case, that against Seismic Entertainment Productions, the FTC accused the defendants of developing a program that seized control of PCs nationwide, infected them with spyware and other malicious software, bombarded them with a barrage of pop-up advertising for Seismic's clients, exposed the PCs to security risks, and caused them to malfunction. Seismic then offered to sell the victims an "antispyware" program to fix the computers, and stop the popups and other problems that Seismic had caused. On November 21, 2006, a settlement was entered in federal court under which a $1.75 million judgment was imposed in one case and $1.86 million in another, but the defendants were insolvent

In a second case, brought against CyberSpy Software LLC, the FTC charged that CyberSpy marketed and sold "RemoteSpy" keylogger spyware to clients who would then secretly monitor unsuspecting consumers' computers. According to the FTC, Cyberspy touted RemoteSpy as a "100% undetectable" way to "Spy on Anyone. From Anywhere." The FTC has obtained a temporary order prohibiting the defendants from selling the software and disconnecting from the Internet any of their servers that collect, store, or provide access to information that this software has gathered. The case is still in its preliminary stages. A complaint filed by the Electronic Privacy Information Center (EPIC) brought the RemoteSpy software to the FTC's attention.

An administrative fine, the first of its kind in Europe, has been issued by the Independent Authority of Posts and Telecommunications (OPTA) from the Netherlands. It applied fines in total value of Euro 1,000,000 for infecting 22 million computers. The spyware concerned is called DollarRevenue. The law articles that have been violated are art. 4.1 of the Decision on universal service providers and on the interests of end users; the fines have been issued based on art. 15.4 taken together with art. 15.10 of the Dutch telecommunications law.

Former New York State Attorney General and former Governor of New York Eliot Spitzer has pursued spyware companies for fraudulent installation of software. In a suit brought in 2005 by Spitzer, the California firm Intermix Media, Inc. ended up settling, by agreeing to pay US$7.5 million and to stop distributing spyware.

The hijacking of Web advertisements has also led to litigation. In June 2002, a number of large Web publishers sued Claria for replacing advertisements, but settled out of court.

Courts have not yet had to decide whether advertisers can be held liable for spyware that displays their ads. In many cases, the companies whose advertisements appear in spyware pop-ups do not directly do business with the spyware firm. Rather, they have contracted with an advertising agency, which in turn contracts with an online subcontractor who gets paid by the number of "impressions" or appearances of the advertisement. Some major firms such as Dell Computer and Mercedes-Benz have sacked advertising agencies that have run their ads in spyware.

Litigation has gone both ways. Since "spyware" has become a common pejorative, some makers have filed libel and defamation actions when their products have been so described. In 2003, Gator (now known as Claria) filed suit against the website PC Pitstop for describing its program as "spyware". PC Pitstop settled, agreeing not to use the word "spyware", but continues to describe harm caused by the Gator/Claria software. As a result, other anti-spyware and anti-virus companies have also used other terms such as "potentially unwanted programs" or greyware to denote these products.

In the 2010 WebcamGate case, plaintiffs charged two suburban Philadelphia high schools secretly spied on students by surreptitiously and remotely activating webcams embedded in school-issued laptops the students were using at home, and therefore infringed on their privacy rights. The school loaded each student's computer with LANrev's remote activation tracking software. This included the now-discontinued "TheftTrack". While TheftTrack was not enabled by default on the software, the program allowed the school district to elect to activate it, and to choose which of the TheftTrack surveillance options the school wanted to enable.

TheftTrack allowed school district employees to secretly remotely activate the webcam embedded in the student's laptop, above the laptop's screen. That allowed school officials to secretly take photos through the webcam, of whatever was in front of it and in its line of sight, and send the photos to the school's server. The LANrev software disabled the webcams for all other uses ("e.g.", students were unable to use Photo Booth or video chat), so most students mistakenly believed their webcams did not work at all. In addition to webcam surveillance, TheftTrack allowed school officials to take screenshots, and send them to the school's server. In addition, LANrev allowed school officials to take snapshots of instant messages, web browsing, music playlists, and written compositions. The schools admitted to secretly snapping over 66,000 webshots and screenshots, including webcam shots of students in their bedrooms.





</doc>
<doc id="28952" url="https://en.wikipedia.org/wiki?curid=28952" title="William Jones (philologist)">
William Jones (philologist)

Sir William Jones FRS FRSE (28 September 1746 – 27 April 1794) was an Anglo-Welsh philologist, a puisne judge on the Supreme Court of Judicature at Fort William in Bengal, and a scholar of ancient India, particularly known for his proposition of the existence of a relationship among European and Indo-Aryan languages, which he coined as Indo-European.

Jones is also credited for establishing the Asiatic Society of Bengal in the year 1784.

William Jones was born in London at Beaufort Buildings, Westminster; his father William Jones (1675–1749) was a mathematician from Anglesey in Wales, noted for introducing the use of the symbol π. The young William Jones was a linguistic prodigy, who in addition to his native languages English and Welsh, learned Greek, Latin, Persian, Arabic, Hebrew and the basics of Chinese writing at an early age. By the end of his life he knew eight languages with critical thoroughness, was fluent in a further eight, with a dictionary at hand, and had a fair competence in another twelve.

Jones' father died when he was aged three, and his mother Mary Nix Jones raised him. He was sent to Harrow School in September 1753 and then went on to University College, Oxford. He graduated there in 1768 and became M.A. in 1773. Financially constrained, he took a position tutoring the seven-year-old Lord Althorp, son of Earl Spencer. For the next six years he worked as a tutor and translator. During this time he published "Histoire de Nader Chah" (1770), a French translation of a work originally written in Persian by Mirza Mehdi Khan Astarabadi. This was done at the request of King Christian VII of Denmark: he had visited Jones, who by the age of 24 had already acquired a reputation as an orientalist. This would be the first of numerous works on Persia, Turkey, and the Middle East in general.

In 1770, Jones joined the Middle Temple and studied law for three years, a preliminary to his life-work in India. He was elected a Fellow of the Royal Society on 30 April 1772. After a spell as a circuit judge in Wales, and a fruitless attempt to resolve the conflict that eventually led to the American Revolution in concert with Benjamin Franklin in Paris, he was appointed puisne judge to the Supreme Court of Judicature at Fort William in Calcutta, Bengal on 4 March 1783, and on 20 March he was knighted. In April 1783 he married Anna Maria Shipley, the eldest daughter of Dr. Jonathan Shipley, Bishop of Llandaff and Bishop of St Asaph. Anna Maria used her artistic skills to help Jones document life in India. On 25 September 1783 he arrived in Calcutta.

Jones was a radical political thinker, a friend of American independence. His work, "The principles of government; in a dialogue between a scholar and a peasant" (1783), was the subject of a trial for seditious libel after it was reprinted by his brother-in-law William Shipley.

In the Subcontinent he was entranced by Indian culture, an as-yet untouched field in European scholarship, and on 15 January 1784 he founded the Asiatic Society in Calcutta and started a journal called "Asiatick Researches". He studied the Vedas with Rāmalocana, a pandit teaching at the Nadiya Hindu university, becoming a proficient Sanskritist. Jones kept up a ten-year correspondence on the topic of "jyotisa" or Hindu astronomy with fellow orientalist Samuel Davis. He learnt the ancient concept of Hindu Laws from Pandit Jagannath Tarka Panchanan.

Over the next ten years he would produce a flood of works on India, launching the modern study of the subcontinent in virtually every social science. He also wrote on the local laws, music, literature, botany, and geography, and made the first English translations of several important works of Indian literature.

Sir William Jones sometimes also went by the nom de plume Youns Uksfardi (یونس اوکسفردی, "Jones of Oxford"). This pen name can be seen on the inner front cover of his "Persian Grammar" published in 1771 (and in subsequent editions).

He died in Calcutta on 27 April 1794 at the age of 47 and is buried in South Park Street Cemetery.

Jones is known today for making and propagating the observation about relationships between the Indo-European languages. In his "Third Anniversary Discourse " to the Asiatic Society (1786) he suggested that Sanskrit, Greek and Latin languages had a common root, and that indeed they may all be further related, in turn, to Gothic and the Celtic languages, as well as to Persian.
Although his name is closely associated with this observation, he was not the first to make it. In the 16th century, European visitors to India became aware of similarities between Indian and European languages and as early as 1653 Van Boxhorn had published a proposal for a proto-language ("Scythian") for Germanic, Romance, Greek, Baltic, Slavic, Celtic and Iranian. Finally, in a memoir sent to the French Academy of Sciences in 1767 Gaston-Laurent Coeurdoux, a French Jesuit who spent all his life in India, had specifically demonstrated the existing analogy between Sanskrit and European languages. In 1786 Jones postulated a proto-language uniting Sanskrit, Iranian, Greek, Latin, Germanic and Celtic, but in many ways his work was less accurate than his predecessors', as he erroneously included Egyptian, Japanese and Chinese in the Indo-European languages, while omitting Hindustani and Slavic
Nevertheless, Jones' third annual discourse before the Asiatic Society on the history and culture of the Hindus (delivered on 2 February 1786 and published in 1788) with the famed "philologer" passage is often cited as the beginning of comparative linguistics and Indo-European studies.

This common source came to be known as Proto-Indo-European.

Jones was the first to propose a racial division of India involving an Aryan invasion but at that time there was insufficient evidence to support it. It was an idea later taken up by British administrators such as Herbert Hope Risley but remains disputed today.

Jones also propounded theories that might appear peculiar today but were less so in his time. For example, he believed that Egyptian priests had migrated and settled down in India in prehistoric times. He also posited that the Chinese were originally Hindus belonging to the Kshatriya caste.

Jones, in his 1772 ‘Essay on the Arts called Imitative’, was one of the first to propound an expressive theory of poetry, valorising expression over description or imitation: “If the arguments, used in this essay, have any weight, it will appear, that the finest parts of poetry, musick, and painting, are expressive of the passions...the inferior parts of them are descriptive of natural objects”. He thereby anticipated Wordsworth in grounding poetry on the basis of a Romantic subjectivity.

Jones was a contributor to Hyde's Notebooks during his term on the bench of the Supreme Court of Judicature. The notebooks are a valuable primary source of information for life in late 18th century Bengal and are the only remaining source for the proceedings of the Supreme Court.

In Europe a discussion as to the authenticity of the work of first translation of Avesta arose. It was the first evidence of an indo-european language as old as sanskrit that had been translated into a european language. It was suggested that the so-called Zend-Avesta was not the genuine work of Zoroaster, but was a forgery. Foremost among the detractors, it is to be regretted, was the distinguished Orientalist, Sir William Jones. He claimed, in a letter published in French (1771), that Anquetil had been duped, that the Parsis of Surat had palmed off upon him a conglomeration of worthless fabrications and absurdities. In England, Sir William Jones was supported by Richardson and Sir John Chardin; in Germany, by Meiners. Anquetil du Perron was labelled an impostor who had invented his own script to support his claim. It is not curious that Jones didn't include Iranian in his naming the cluster of indoeuropean languages, especially since he hadn't any idea about the relationship between avestan and sanskrit as the two main branches of this language family. 

In 1763, at the age of 17, Jones wrote the poem "Caissa", based on a 658-line poem called "Scacchia, Ludus" published in 1527 by Marco Girolamo Vida, giving a mythical origin of chess that has become well known in the chess world. This poem he wrote in English.

In the poem the nymph Caissa initially repels the advances of Mars, the god of war. Spurned, Mars seeks the aid of the god of sport, who creates the game of chess as a gift for Mars to win Caissa's favour. Mars wins her over with the game.

Caissa has since been characterised as the "goddess" of chess, her name being used in several contexts in modern chess playing.

Arthur Schopenhauer referred to one of Sir William Jones's publications in §1 of "The World as Will and Representation" (1819). Schopenhauer was trying to support the doctrine that "everything that exists for knowledge, and hence the whole of this world, is only object in relation to the subject, perception of the perceiver, in a word, representation." He quoted Jones's original English:
... how early this basic truth was recognized by the sages of India, since it appears as the fundamental tenet of the Vedânta philosophy ascribed to Vyasa, is proved by Sir William Jones in the last of his essays: "On the Philosophy of the Asiatics" ("Asiatic Researches", vol. IV, p. 164): "The fundamental tenet of the Vedânta school consisted not in denying the existence of matter, that is solidity, impenetrability, and extended figure (to deny which would be lunacy), but in correcting the popular notion of it, and in contending that it has no essence independent of mental perception; that existence and perceptibility are convertible terms."

Schopenhauer used Jones's authority to relate the basic principle of his philosophy to what was, according to Jones, the most important underlying proposition of Vedânta. He made more passing reference to Sir William Jones's writings elsewhere in his works.

In 1822 the Dutch orientalist Hendrik Arent Hamaker accepted a professorship at the University of Leiden, and gave as his inaugural lecture in Latin "De vita et meritis Guilielmi Jonesii (Leiden, 1823).

Edgar Allan Poe's short story "Berenice" starts with a motto, the first half of a poem, by Ibn Zaiat: "Dicebant mihi sodales si sepulchrum amicae visitarem, curas meas aliquantulum fore levatas." It was taken from the works of William Jones, and here is the missing part (from Complete Works, Vol. 2, London, 1799): 
"Dixi autem, an ideo aliud praeter hoc pectus habet sepulchrum?"

My companions said to me, if I would visit the grave of my friend, I might somewhat alleviate my worries. I answered "could she be buried elsewhere than in my heart?"

Listing in most cases only editions and reprints that came out during Jones's own lifetime, books by, or prominently including work by, William Jones, are:






</doc>
<doc id="28953" url="https://en.wikipedia.org/wiki?curid=28953" title="Stephen, King of England">
Stephen, King of England

Stephen (1092/6 – 25 October 1154), often referred to as Stephen of Blois, was King of England from 22 December 1135 to his death. He was Count of Boulogne from 1125 until 1147 and Duke of Normandy from 1135 until 1144. His reign was marked by the Anarchy, a civil war with his cousin and rival, the Empress Matilda, whose son, Henry II, succeeded Stephen as the first of the Angevin kings of England.

Stephen was born in the County of Blois in central France; his father, Count Stephen-Henry, died while Stephen was still young, and he was brought up by his mother, Adela, daughter of William the Conqueror. Placed into the court of his uncle, Henry I of England, Stephen rose in prominence and was granted extensive lands. He married Matilda of Boulogne, inheriting additional estates in Kent and Boulogne that made the couple one of the wealthiest in England. Stephen narrowly escaped drowning with Henry I's son, William Adelin, in the sinking of the "White Ship" in 1120; William's death left the succession of the English throne open to challenge. When Henry died in 1135, Stephen quickly crossed the English Channel and with the help of his brother Henry, Bishop of Winchester and Abbot of Glastonbury, took the throne, arguing that the preservation of order across the kingdom took priority over his earlier oaths to support the claim of Henry I's daughter, the Empress Matilda.

The early years of Stephen's reign were largely successful, despite a series of attacks on his possessions in England and Normandy by David I of Scotland, Welsh rebels, and the Empress Matilda's husband Geoffrey Plantagenet, Count of Anjou. In 1138, the Empress's half-brother Robert of Gloucester rebelled against Stephen, threatening civil war. Together with his close advisor, Waleran de Beaumont, Stephen took firm steps to defend his rule, including arresting a powerful family of bishops. When the Empress and Robert invaded in 1139, Stephen was unable to crush the revolt rapidly, and it took hold in the south-west of England. Captured at the battle of Lincoln in 1141, he was abandoned by many of his followers and lost control of Normandy. He was freed only after his wife and William of Ypres, one of his military commanders, captured Robert at the Rout of Winchester, but the war dragged on for many years with neither side able to win an advantage.

Stephen became increasingly concerned with ensuring that his son Eustace would inherit his throne. The King tried to convince the Church to agree to crown Eustace to reinforce his claim; Pope Eugene III refused, and Stephen found himself in a sequence of increasingly bitter arguments with his senior clergy. In 1153 the Empress's son, Henry FitzEmpress, invaded England and built an alliance of powerful regional barons to support his claim for the throne. The two armies met at Wallingford, but neither side's barons were keen to fight another pitched battle. Stephen began to examine a negotiated peace, a process hastened by the sudden death of Eustace. Later in the year Stephen and Henry agreed to the Treaty of Winchester, in which Stephen recognised Henry as his heir in exchange for peace, passing over William, Stephen's second son. Stephen died the following year. Modern historians have extensively debated the extent to which his personality, external events, or the weaknesses in the Norman state contributed to this prolonged period of civil war.

Stephen was born in Blois, France, in either 1092 or 1096. His father was Stephen-Henry, Count of Blois and Chartres, an important French nobleman, and an active crusader, who played only a brief part in Stephen's early life. During the First Crusade Stephen-Henry had acquired a reputation for cowardice, and he returned to the Levant again in 1101 to rebuild his reputation; there he was killed at the battle of Ramlah. Stephen's mother, Adela, was the daughter of William the Conqueror and Matilda of Flanders, famous amongst her contemporaries for her piety, wealth and political talent. She had a strong matriarchal influence on Stephen during his early years.

France in the 12th century was a loose collection of counties and smaller polities, under the minimal control of the King of France. The King's power was linked to his control of the rich province of Île-de-France, just to the east of Stephen's home county of Blois. In the west lay the three counties of Maine, Anjou and Touraine, and to the north of Blois was the Duchy of Normandy, from which William the Conqueror had conquered England in 1066. William's children were still fighting over the collective Anglo-Norman inheritance. The rulers across this region spoke a similar language, albeit with regional dialects, followed the same religion, and were closely interrelated; they were also highly competitive and frequently in conflict with one another for valuable territory and the castles that controlled them.

Stephen had at least four brothers and one sister, along with two probable half-sisters. His eldest brother was William, who under normal circumstances would have ruled Blois and Chartres. William was probably intellectually disabled, and Adela instead had the counties pass to her second son, later also Count Theobald II of Champagne. Stephen's remaining older brother, Odo, died young, probably in his early teens. His younger brother, Henry of Blois, was probably born four years after him. The brothers formed a close-knit family group, and Adela encouraged Stephen to take up the role of a feudal knight, whilst steering Henry towards a career in the church, possibly so that their personal career interests would not overlap. Unusually, Stephen was raised in his mother's household rather than being sent to a close relative; he was taught Latin and riding, and was educated in recent history and Biblical stories by his tutor, William the Norman.

Stephen's early life was heavily influenced by his relationship with his uncle Henry I. Henry seized power in England following the death of his elder brother William Rufus. In 1106 he invaded and captured the Duchy of Normandy, controlled by his eldest brother, Robert Curthose, defeating Robert's army at the battle of Tinchebray. Henry then found himself in conflict with Louis VI of France, who took the opportunity to declare Robert's son William Clito the Duke of Normandy. Henry responded by forming a network of alliances with the western counties of France against Louis, resulting in a regional conflict that would last throughout Stephen's early life. Adela and Theobald allied themselves with Henry, and Stephen's mother decided to place him in Henry's court. Henry fought his next military campaign in Normandy, from 1111 onwards, where rebels led by Robert of Bellême were opposing his rule. Stephen was probably with Henry during the military campaign of 1112, when he was knighted by the King. He was present at court during the King's visit to the Abbey of Saint-Evroul in 1113. Stephen probably first visited England in either 1113 or 1115, almost certainly as part of Henry's court.

Henry became a powerful patron of Stephen, and probably chose to support him because Stephen was part of his extended family and a regional ally, yet not sufficiently wealthy or powerful in his own right to represent a threat to either the King or his heir, William Adelin. As a third surviving son, even of an influential regional family, Stephen still needed the support of a powerful patron to progress in life. With Henry's support, he rapidly began to accumulate lands and possessions. Following the battle of Tinchebray in 1106, Henry confiscated the County of Mortain from William, Count of Mortain, and the Honour of Eye, a large lordship previously owned by Robert Malet. In 1113, Stephen was granted both the title and the honour, although without the lands previously held by William in England. The gift of the Honour of Lancaster also followed after it was confiscated by Henry from Roger the Poitevin. Stephen was also given lands in Alençon in southern Normandy by Henry, but the local Normans rebelled, seeking assistance from Fulk IV, Count of Anjou. Stephen and his older brother Theobald were comprehensively beaten in the subsequent campaign, which culminated in the battle of Alençon, and the territories were not recovered.

Finally, the King arranged for Stephen to marry Matilda in 1125, the daughter and only heiress of the Count of Boulogne, who owned both the important continental port of Boulogne and vast estates in the north-west and south-east of England. In 1127, William Clito, a potential claimant to the English throne, seemed likely to become the Count of Flanders; Stephen was sent by the King on a mission to prevent this, and in the aftermath of his successful election, William Clito attacked Stephen's lands in neighbouring Boulogne in retaliation. Eventually a truce was declared, and William Clito died the following year.

In 1120, the English political landscape changed dramatically. Three hundred passengers embarked on the "White Ship" to travel from Barfleur in Normandy to England, including the heir to the throne, William Adelin, and many other senior nobles. Stephen had intended to sail on the same ship but changed his mind at the last moment and got off to await another vessel, either out of concern for overcrowding on board the ship, or because he was suffering from diarrhoea. The ship foundered en route, and all but two of the passengers died, including William Adelin.

With Adelin dead, the inheritance to the English throne was thrown into doubt. Rules of succession in western Europe at the time were uncertain; in some parts of France, male primogeniture, in which the eldest son would inherit a title, was becoming more popular. It was also traditional for the King of France to crown his successor whilst he himself was still alive, making the intended line of succession relatively clear, but this was not the case in England. In other parts of Europe, including Normandy and England, the tradition was for lands to be divided up, with the eldest son taking patrimonial lands—usually considered to be the most valuable—and younger sons being given smaller, or more recently acquired, partitions or estates. The problem was further complicated by the sequence of unstable Anglo-Norman successions over the previous sixty years—William the Conqueror had gained England by force, William Rufus and Robert Curthose had fought a war between them to establish their inheritance, and Henry had only acquired control of Normandy by force. There had been no peaceful, uncontested successions.

With William Adelin dead, Henry had only one other legitimate child, Matilda, but as a woman she was at a substantial political disadvantage. Despite the King taking a second wife, Adeliza of Louvain, it became increasingly unlikely that he would have another legitimate son, and he instead looked to Matilda as his intended heir. Matilda claimed the title of Holy Roman Empress through her marriage to Emperor Henry V, but her husband died in 1125, and she was remarried in 1128 to Geoffrey Plantagenet, Count of Anjou, whose lands bordered the Duchy of Normandy. Geoffrey was unpopular with the Anglo-Norman elite: as an Angevin ruler, he was a traditional enemy of the Normans. At the same time, tensions continued to grow as a result of Henry's domestic policies, in particular the high level of revenue he was raising to pay for his various wars. Conflict was curtailed, however, by the power of the King's personality and reputation.

Henry attempted to build up a base of political support for Matilda in both England and Normandy, demanding that his court take oaths first in 1127, and then again in 1128 and 1131, to recognise Matilda as his immediate successor and recognise her descendants as the rightful rulers after her. Stephen was amongst those who took this oath in 1127. Nonetheless, relations between Henry, Matilda, and Geoffrey became increasingly strained towards the end of the King's life. Matilda and Geoffrey suspected that they lacked genuine support in England, and proposed to Henry in 1135 that the King should hand over the royal castles in Normandy to Matilda whilst he was still alive and insist on the Norman nobility swearing immediate allegiance to her, thereby giving the couple a much more powerful position after Henry's death. Henry angrily declined to do so, probably out of a concern that Geoffrey would try to seize power in Normandy somewhat earlier than intended. A fresh rebellion broke out in southern Normandy, and Geoffrey and Matilda intervened militarily on behalf of the rebels. In the middle of this confrontation, Henry unexpectedly fell ill and died near Lyons-la-Forêt.

Stephen was a well established figure in Anglo-Norman society by 1135. He was extremely wealthy, well-mannered and liked by his peers; he was also considered a man capable of firm action. Chroniclers recorded that despite his wealth and power he was a modest and easy-going leader, happy to sit with his men and servants, casually laughing and eating with them. He was very pious, both in terms of his observance of religious rituals and his personal generosity to the church. Stephen also had a personal Augustinian confessor appointed to him by the Archbishop of Canterbury, who implemented a penitential regime for him, and Stephen encouraged the new order of Cistercians to form abbeys on his estates, winning him additional allies within the church.

Rumours about his father's cowardice during the First Crusade, however, continued to circulate, and a desire to avoid the same reputation may have influenced some of Stephen's rasher military actions. His wife, Matilda, played a major role in running their vast English estates, which contributed to the couple being the second-richest lay household in the country after the King and Queen. The landless Flemish nobleman William of Ypres had joined Stephen's household in 1133.

Stephen's younger brother, Henry of Blois, had also risen to power under Henry I. Henry of Blois had become a Cluniac monk and followed Stephen to England, where the King made him Abbot of Glastonbury, the richest abbey in England. The King then appointed him Bishop of Winchester, one of the richest bishoprics, allowing him to retain Glastonbury as well. The combined revenues of the two positions made Henry of Winchester the second-richest man in England after the King. Henry of Winchester was keen to reverse what he perceived as encroachment by the Norman kings on the rights of the church. The Norman kings had traditionally exercised a great deal of power and autonomy over the church within their territories. From the 1040s onwards, however, successive popes had put forward a reforming message that emphasised the importance of the church being "governed more coherently and more hierarchically from the centre" and established "its own sphere of authority and jurisdiction, separate from and independent of that of the lay ruler", in the words of historian Richard Huscroft.
When news began to spread of Henry I's death, many of the potential claimants to the throne were not well placed to respond. Geoffrey and Matilda were in Anjou, rather awkwardly supporting the rebels in their campaign against the royal army, which included a number of Matilda's supporters such as Robert of Gloucester. Many of these barons had taken an oath to stay in Normandy until the late King was properly buried, which prevented them from returning to England. Stephen's elder brother Theobald was further south still, in Blois. Stephen, however, was in Boulogne, and when news reached him of Henry's death he left for England, accompanied by his military household. Robert of Gloucester had garrisoned the ports of Dover and Canterbury and some accounts suggest that they refused Stephen access when he first arrived. Nonetheless Stephen probably reached his own estate on the edge of London by 8 December and over the next week he began to seize power in England.

The crowds in London traditionally claimed a right to elect the King, and they proclaimed Stephen the new monarch, believing that he would grant the city new rights and privileges in return. Henry of Blois delivered the support of the church to Stephen: Stephen was able to advance to Winchester, where Roger, Bishop of Salisbury and Lord Chancellor, instructed the royal treasury to be handed over to Stephen. On 15 December, Henry delivered an agreement under which Stephen would grant extensive freedoms and liberties to the church, in exchange for the Archbishop of Canterbury and the Papal Legate supporting his succession to the throne. There was the slight problem of the religious oath that Stephen had taken to support the Empress Matilda, but Henry convincingly argued that the late King had been wrong to insist that his court take the oath.

Furthermore, the late King had only insisted on that oath to protect the stability of the kingdom, and in light of the chaos that might now ensue, Stephen would be justified in ignoring it. Henry was also able to persuade Hugh Bigod, the late King's royal steward, to swear that the King had changed his mind about the succession on his deathbed, nominating Stephen instead. Stephen's coronation was held a week later at Westminster Abbey on 22 December.

Meanwhile, the Norman nobility gathered at Le Neubourg to discuss declaring Theobald king, probably following the news that Stephen was gathering support in England. The Normans argued that the count, as the more senior grandson of William the Conqueror, had the most valid claim over the kingdom and the duchy, and was certainly preferable to Matilda.

Theobald met with the Norman barons and Robert of Gloucester at Lisieux on 21 December. Their discussions were interrupted by the sudden news from England that Stephen's coronation was to occur the next day. Theobald then agreed to the Normans' proposal that he be made king, only to find that his former support immediately ebbed away: the barons were not prepared to support the division of England and Normandy by opposing Stephen, who subsequently financially compensated Theobald, who in return remained in Blois and supported his brother's succession.

Stephen's new Anglo-Norman kingdom had been shaped by the Norman conquest of England in 1066, followed by the Norman expansion into south Wales over the coming years. Both the kingdom and duchy were dominated by a small number of major barons who owned lands on both sides of the English Channel, with the lesser barons beneath them usually having more localised holdings. The extent to which lands and positions should be passed down through hereditary right or by the gift of the King was still uncertain, and tensions concerning this issue had grown during the reign of Henry I. Certainly lands in Normandy, passed by hereditary right, were usually considered more important to major barons than those in England, where their possession was less certain. Henry had increased the authority and capabilities of the central royal administration, often bringing in "new men" to fulfil key positions rather than using the established nobility. In the process he had been able to maximise revenues and contain expenditures, resulting in a healthy surplus and a famously large treasury, but also increasing political tensions.

Stephen had to intervene in the north of England immediately after his coronation. David I of Scotland invaded the north on the news of Henry's death, taking Carlisle, Newcastle and other key strongholds. Northern England was a disputed territory at this time, with the Scottish kings laying a traditional claim to Cumberland, and David also claiming Northumbria by virtue of his marriage to the daughter of Waltheof, Earl of Northumbria. Stephen rapidly marched north with an army and met David at Durham. An agreement was made under which David would return most of the territory he had taken, with the exception of Carlisle. In return, Stephen confirmed David's son Prince Henry's possessions in England, including the Earldom of Huntingdon.

Returning south, Stephen held his first royal court at Easter 1136. A wide range of nobles gathered at Westminster for the event, including many of the Anglo-Norman barons and most of the higher officials of the church. Stephen issued a new royal charter, confirming the promises he had made to the church, promising to reverse Henry I's policies on the royal forests and to reform any abuses of the royal legal system. He portrayed himself as the natural successor to Henry's policies, and reconfirmed the existing seven earldoms in the kingdom on their existing holders. The Easter court was a lavish event, and a large amount of money was spent on the event itself, clothes and gifts. Stephen gave out grants of land and favours to those present and endowed numerous church foundations with land and privileges. His accession to the throne still needed to be ratified by the Pope, however, and Henry of Blois appears to have been responsible for ensuring that testimonials of support were sent both from Stephen's brother Theobald and from the French king Louis VI, to whom Stephen represented a useful balance to Angevin power in the north of France. Pope Innocent II confirmed Stephen as king by letter later that year, and Stephen's advisers circulated copies widely around England to demonstrate his legitimacy.

Troubles continued across Stephen's kingdom. After the Welsh victory at the battle of Llwchwr in January 1136 and the successful ambush of Richard Fitz Gilbert de Clare in April, south Wales rose in rebellion, starting in east Glamorgan and rapidly spreading across the rest of south Wales during 1137. Owain Gwynedd and Gruffydd ap Rhys successfully captured considerable territories, including Carmarthen Castle. Stephen responded by sending Richard's brother Baldwin and the Marcher Lord Robert Fitz Harold of Ewyas into Wales to pacify the region. Neither mission was particularly successful, and by the end of 1137 the King appears to have abandoned attempts to put down the rebellion. Historian David Crouch suggests that Stephen effectively "bowed out of Wales" around this time to concentrate on his other problems. Meanwhile, he had put down two revolts in the south-west led by Baldwin de Redvers and Robert of Bampton; Baldwin was released after his capture and travelled to Normandy, where he became an increasingly vocal critic of the King.

The security of Normandy was also a concern. Geoffrey of Anjou invaded in early 1136 and, after a temporary truce, invaded later the same year, raiding and burning estates rather than trying to hold the territory. Events in England meant that Stephen was unable to travel to Normandy himself, so Waleran de Beaumont, appointed by Stephen as the lieutenant of Normandy, and Theobald led the efforts to defend the duchy. Stephen himself only returned to the duchy in 1137, where he met with Louis VI and Theobald to agree to an informal regional alliance, probably brokered by Henry, to counter the growing Angevin power in the region. As part of this deal, Louis recognised Stephen's son Eustace as Duke of Normandy in exchange for Eustace giving fealty to the French King. Stephen was less successful, however, in regaining the Argentan province along the Normandy and Anjou border, which Geoffrey had taken at the end of 1135. Stephen formed an army to retake it, but the frictions between his Flemish mercenary forces led by William of Ypres and the local Norman barons resulted in a battle between the two halves of his army. The Norman forces then deserted Stephen, forcing the King to give up his campaign. He agreed to another truce with Geoffrey, promising to pay him 2,000 marks a year in exchange for peace along the Norman borders.

In the years following his succession, Stephen's relationship with the church became gradually more complex. The royal charter of 1136 had promised to review the ownership of all the lands that had been taken by the crown from the church since 1087, but these estates were now typically owned by nobles. Henry of Blois's claims, in his role as Abbot of Glastonbury, to extensive lands in Devon resulted in considerable local unrest. In 1136, Archbishop of Canterbury William de Corbeil died. Stephen responded by seizing his personal wealth, which caused some discontent amongst the senior clergy. Henry wanted to succeed to the post, but Stephen instead supported Theobald of Bec, who was eventually appointed. The papacy named Henry papal legate, possibly as consolation for not receiving Canterbury.

Stephen's first few years as king can be interpreted in different ways. He stabilised the northern border with Scotland, contained Geoffrey's attacks on Normandy, was at peace with Louis VI, enjoyed good relations with the church and had the broad support of his barons. There were significant underlying problems, nonetheless. The north of England was now controlled by David and Prince Henry, Stephen had abandoned Wales, the fighting in Normandy had considerably destabilised the duchy, and an increasing number of barons felt that Stephen had given them neither the lands nor the titles they felt they deserved or were owed. Stephen was also rapidly running out of money: Henry's considerable treasury had been emptied by 1138 due to the costs of running Stephen's more lavish court and the need to raise and maintain his mercenary armies fighting in England and Normandy.

Stephen was attacked on several fronts during 1138. First, Robert, Earl of Gloucester, rebelled against the King, starting the descent into civil war in England. An illegitimate son of Henry I and the half-brother of the Empress Matilda, Robert was one of the most powerful Anglo-Norman barons, controlling estates in Normandy. He was known for his qualities as a statesman, his military experience, and leadership ability. Robert had tried to convince Theobald to take the throne in 1135; he did not attend Stephen's first court in 1136 and it took several summons to convince him to attend court at Oxford later that year. In 1138, Robert renounced his fealty to Stephen and declared his support for Matilda, triggering a major regional rebellion in Kent and across the south-west of England, although Robert himself remained in Normandy. In France, Geoffrey of Anjou took advantage of the situation by re-invading Normandy. David of Scotland also invaded the north of England once again, announcing that he was supporting the claim of his niece the Empress Matilda to the throne, pushing south into Yorkshire.

Anglo-Norman warfare during the reign of Stephen was characterised by attritional military campaigns, in which commanders tried to seize key enemy castles in order to allow them to take control of their adversaries' territory and ultimately win a slow, strategic victory. The armies of the period centred on bodies of mounted, armoured knights, supported by infantry and crossbowmen. These forces were either feudal levies, drawn up by local nobles for a limited period of service during a campaign, or, increasingly, mercenaries, who were expensive but more flexible and often more skilled. These armies, however, were ill-suited to besieging castles, whether the older motte-and-bailey designs or the newer, stone-built keeps. Existing siege engines were significantly less powerful than the later trebuchet designs, giving defenders a substantial advantage over attackers. As a result, slow sieges to starve defenders out, or mining operations to undermine walls, tended to be preferred by commanders over direct assaults. Occasionally pitched battles were fought between armies but these were considered highly risky endeavours and were usually avoided by prudent commanders. The cost of warfare had risen considerably in the first part of the 12th century, and adequate supplies of ready cash were increasingly proving important in the success of campaigns.

Stephen's personal qualities as a military leader focused on his skill in personal combat, his capabilities in siege warfare and a remarkable ability to move military forces quickly over relatively long distances. In response to the revolts and invasions, he rapidly undertook several military campaigns, focusing primarily on England rather than Normandy. His wife Matilda was sent to Kent with ships and resources from Boulogne, with the task of retaking the key port of Dover, under Robert's control. A small number of Stephen's household knights were sent north to help the fight against the Scots, where David's forces were defeated later that year at the battle of the Standard in August by the forces of Thurstan, the Archbishop of York. Despite this victory, however, David still occupied most of the north. Stephen himself went west in an attempt to regain control of Gloucestershire, first striking north into the Welsh Marches, taking Hereford and Shrewsbury, before heading south to Bath. The town of Bristol itself proved too strong for him, and Stephen contented himself with raiding and pillaging the surrounding area. The rebels appear to have expected Robert to intervene with support that year, but he remained in Normandy throughout, trying to persuade the Empress Matilda to invade England herself. Dover finally surrendered to the queen's forces later in the year.

Stephen's military campaign in England had progressed well, and historian David Crouch describes it as "a military achievement of the first rank". The King took the opportunity of his military advantage to forge a peace agreement with Scotland. Stephen's wife Matilda was sent to negotiate another agreement between Stephen and David, called the treaty of Durham; Northumbria and Cumbria would effectively be granted to David and his son Henry, in exchange for their fealty and future peace along the border. Unfortunately, the powerful Ranulf I, Earl of Chester, considered himself to hold the traditional rights to Carlisle and Cumberland and was extremely displeased to see them being given to the Scots. Nonetheless, Stephen could now focus his attention on the anticipated invasion of England by Robert and Matilda's forces.

Stephen prepared for the Angevin invasion by creating a number of additional earldoms. Only a handful of earldoms had existed under Henry I and these had been largely symbolic in nature. Stephen created many more, filling them with men he considered to be loyal, capable military commanders, and in the more vulnerable parts of the country assigning them new lands and additional executive powers. He appears to have had several objectives in mind, including both ensuring the loyalty of his key supporters by granting them these honours, and improving his defences in key parts of the kingdom. Stephen was heavily influenced by his principal advisor, Waleran de Beaumont, the twin brother of Robert of Leicester. The Beaumont twins and their younger brother and cousins received the majority of these new earldoms. From 1138 onwards, Stephen gave them the earldoms of Worcester, Leicester, Hereford, Warwick and Pembroke, which—especially when combined with the possessions of Stephen's new ally, Prince Henry, in Cumberland and Northumbria—created a wide block of territory to act as a buffer zone between the troubled south-west, Chester, and the rest of the kingdom. With their new lands, the power of the Beamounts grew to the point where David Crouch suggests that it became "dangerous to be anything other than a friend of Waleran" at Stephen's court.

Stephen took steps to remove a group of bishops he regarded as a threat to his rule. The royal administration under Henry I had been headed by Roger, the Bishop of Salisbury, supported by Roger's nephews, Alexander and Nigel, the Bishops of Lincoln and Ely respectively, and Roger's son, Lord Chancellor Roger le Poer. These bishops were powerful landowners as well as ecclesiastical rulers, and they had begun to build new castles and increase the size of their military forces, leading Stephen to suspect that they were about to defect to the Empress Matilda. Roger and his family were also enemies of Waleran, who disliked their control of the royal administration. In June 1139, Stephen held his court in Oxford, where a fight between Alan of Brittany and Roger's men broke out, an incident probably deliberately created by Stephen. Stephen responded by demanding that Roger and the other bishops surrender all of their castles in England. This threat was backed up by the arrest of the bishops, with the exception of Nigel who had taken refuge in Devizes Castle; the bishop only surrendered after Stephen besieged the castle and threatened to execute Roger le Poer. The remaining castles were then surrendered to the King.

Stephen's brother, Henry of Blois, was alarmed by this, both as a matter of principle, since Stephen had previously agreed in 1135 to respect the freedoms of the church, and more pragmatically because he himself had recently built six castles and had no desire to be treated in the same way. As the papal legate, he summoned the King to appear before an ecclesiastical council to answer for the arrests and seizure of property. Henry asserted the Church's right to investigate and judge all charges against members of the clergy. Stephen sent Aubrey de Vere II as his spokesman to the council, who argued that Roger of Salisbury had been arrested not as a bishop, but rather in his role as a baron who had been preparing to change his support to the Empress Matilda. The King was supported by Hugh of Amiens, Archbishop of Rouen, who challenged the bishops to show how canon law entitled them to build or hold castles. Aubrey threatened that Stephen would complain to the pope that he was being harassed by the English church, and the council let the matter rest following an unsuccessful appeal to Rome. The incident successfully removed any military threat from the bishops, but it may have damaged Stephen's relationship with the senior clergy, and in particular with his brother Henry.

The Angevin invasion finally arrived in 1139. Baldwin de Redvers crossed over from Normandy to Wareham in August in an initial attempt to capture a port to receive the Empress Matilda's invading army, but Stephen's forces forced him to retreat into the south-west. The following month, however, the Empress was invited by the Dowager Queen Adeliza to land at Arundel instead, and on 30 September Robert of Gloucester and the Empress arrived in England with 140 knights. The Empress stayed at Arundel Castle, whilst Robert marched north-west to Wallingford and Bristol, hoping to raise support for the rebellion and to link up with Miles of Gloucester, a capable military leader who took the opportunity to renounce his fealty to the King. Stephen promptly moved south, besieging Arundel and trapping Matilda inside the castle.

Stephen then agreed to a truce proposed by his brother, Henry; the full details of the truce are not known, but the results were that Stephen first released Matilda from the siege and then allowed her and her household of knights to be escorted to the south-west, where they were reunited with Robert of Gloucester. The reasoning behind Stephen's decision to release his rival remains unclear. Contemporary chroniclers suggested that Henry argued that it would be in Stephen's own best interests to release the Empress and concentrate instead on attacking Robert, and Stephen may have seen Robert, not the Empress, as his main opponent at this point in the conflict. He also faced a military dilemma at Arundel—the castle was considered almost impregnable, and he may have been worried that he was tying down his army in the south whilst Robert roamed freely in the west. Another theory is that Stephen released Matilda out of a sense of chivalry; he was certainly known for having a generous, courteous personality and women were not normally expected to be targeted in Anglo-Norman warfare.

Having released the Empress, Stephen focused on pacifying the south-west of England. Although there had been few new defections to the Empress, his enemies now controlled a compact block of territory stretching out from Gloucester and Bristol south-west into Devon and Cornwall, west into the Welsh Marches and east as far as Oxford and Wallingford, threatening London. Stephen started by attacking Wallingford Castle, held by the Empress's childhood friend Brien FitzCount, only to find it too well defended. He then left behind some forces to blockade the castle and continued west into Wiltshire to attack Trowbridge Castle, taking the castles of South Cerney and Malmesbury en route. Meanwhile, Miles of Gloucester marched east, attacking Stephen's rearguard forces at Wallingford and threatening an advance on London. Stephen was forced to give up his western campaign, returning east to stabilise the situation and protect his capital.
At the start of 1140, Nigel, Bishop of Ely, whose castles Stephen had confiscated the previous year, rebelled against Stephen as well. Nigel hoped to seize East Anglia and established his base of operations in the Isle of Ely, then surrounded by protective fenland. Stephen responded quickly, taking an army into the fens and using boats lashed together to form a causeway that allowed him to make a surprise attack on the isle. Nigel escaped to Gloucester, but his men and castle were captured, and order was temporarily restored in the east. Robert of Gloucester's men retook some of the territory that Stephen had taken in his 1139 campaign. In an effort to negotiate a truce, Henry of Blois held a peace conference at Bath, to which Stephen sent his wife. The conference collapsed over the insistence by Henry and the clergy that they should set the terms of any peace deal, which Stephen found unacceptable.

Ranulf of Chester remained upset over Stephen's gift of the north of England to Prince Henry. Ranulf devised a plan for dealing with the problem by ambushing Henry whilst the prince was travelling back from Stephen's court to Scotland after Christmas. Stephen responded to rumours of this plan by escorting Henry himself north, but this gesture proved the final straw for Ranulf. Ranulf had previously claimed that he had the rights to Lincoln Castle, held by Stephen, and under the guise of a social visit, Ranulf seized the fortification in a surprise attack. Stephen marched north to Lincoln and agreed to a truce with Ranulf, probably to keep him from joining the Empress's faction, under which Ranulf would be allowed to keep the castle. Stephen returned to London but received news that Ranulf, his brother and their family were relaxing in Lincoln Castle with a minimal guard force, a ripe target for a surprise attack of his own. Abandoning the deal he had just made, Stephen gathered his army again and sped north, but not quite fast enough—Ranulf escaped Lincoln and declared his support for the Empress. Stephen was forced to place the castle under siege.

While Stephen and his army besieged Lincoln Castle at the start of 1141, Robert of Gloucester and Ranulf of Chester advanced on the King's position with a somewhat larger force. When the news reached Stephen, he held a council to decide whether to give battle or to withdraw and gather additional soldiers: Stephen decided to fight, resulting in the Battle of Lincoln on 2 February 1141. The King commanded the centre of his army, with Alan of Brittany on his right and William of Aumale on his left. Robert and Ranulf's forces had superiority in cavalry and Stephen dismounted many of his own knights to form a solid infantry block; he joined them himself, fighting on foot in the battle. Stephen was not a gifted public speaker, and delegated the pre-battle speech to Baldwin of Clare, who delivered a rousing declaration. After an initial success in which William's forces destroyed the Angevins' Welsh infantry, the battle went badly for Stephen. Robert and Ranulf's cavalry encircled Stephen's centre, and the King found himself surrounded by the enemy army. Many of his supporters, including Waleran de Beaumont and William of Ypres, fled from the field at this point but Stephen fought on, defending himself first with his sword and then, when that broke, with a borrowed battle axe. Finally, he was overwhelmed by Robert's men and taken away from the field in custody.

Robert took Stephen back to Gloucester, where the King met with the Empress Matilda, and was then moved to Bristol Castle, traditionally used for holding high-status prisoners. He was initially left confined in relatively good conditions, but his security was later tightened and he was kept in chains. The Empress now began to take the necessary steps to have herself crowned queen in his place, which would require the agreement of the church and her coronation at Westminster. Stephen's brother Henry summoned a council at Winchester before Easter in his capacity as papal legate to consider the clergy's view. He had made a private deal with the Empress Matilda that he would deliver the support of the church, if she agreed to give him control over church business in England. Henry handed over the royal treasury, rather depleted except for Stephen's crown, to the Empress, and excommunicated many of Stephen's supporters who refused to switch sides. Archbishop Theobald of Canterbury was unwilling to declare Matilda queen so rapidly, however, and a delegation of clergy and nobles, headed by Theobald, travelled to see Stephen in Bristol and consult about their moral dilemma: should they abandon their oaths of fealty to the King? Stephen agreed that, given the situation, he was prepared to release his subjects from their oath of fealty to him, and the clergy gathered again in Winchester after Easter to declare the Empress "Lady of England and Normandy" as a precursor to her coronation. When Matilda advanced to London in an effort to stage her coronation in June, though, she faced an uprising by the local citizens in support of Stephen that forced her to flee to Oxford, uncrowned.

Once news of Stephen's capture reached him, Geoffrey of Anjou invaded Normandy again and, in the absence of Waleran of Beaumont, who was still fighting in England, Geoffrey took all the duchy south of the river Seine and east of the river Risle. No help was forthcoming from Stephen's brother Theobald this time either, who appears to have been preoccupied with his own problems with France—the new French king, Louis VII, had rejected his father's regional alliance, improving relations with Anjou and taking a more bellicose line with Theobald, which would result in war the following year. Geoffrey's success in Normandy and Stephen's weakness in England began to influence the loyalty of many Anglo-Norman barons, who feared losing their lands in England to Robert and the Empress, and their possessions in Normandy to Geoffrey. Many started to leave Stephen's faction. His friend and advisor Waleran was one of those who decided to defect in mid-1141, crossing into Normandy to secure his ancestral possessions by allying himself with the Angevins, and bringing Worcestershire into the Empress's camp. Waleran's twin brother, Robert of Leicester, effectively withdrew from fighting in the conflict at the same time. Other supporters of the Empress were restored in their former strongholds, such as Bishop Nigel of Ely, or received new earldoms in the west of England. The royal control over the minting of coins broke down, leading to coins being struck by local barons and bishops across the country.
Stephen's wife Matilda played a critical part in keeping the King's cause alive during his captivity. Queen Matilda gathered Stephen's remaining lieutenants around her and the royal family in the south-east, advancing into London when the population rejected the Empress. Stephen's long-standing commander William of Ypres remained with the Queen in London; William Martel, the royal steward, commanded operations from Sherborne in Dorset, and Faramus of Boulogne ran the royal household. The Queen appears to have generated genuine sympathy and support from Stephen's more loyal followers. Henry's alliance with the Empress proved short-lived, as they soon fell out over political patronage and ecclesiastical policy; the bishop met the Queen at Guildford and transferred his support to her.

The King's eventual release resulted from the Angevin defeat at the rout of Winchester. Robert of Gloucester and the Empress besieged Henry in the city of Winchester in July. Queen Matilda and William of Ypres then encircled the Angevin forces with their own army, reinforced with fresh troops from London. In the subsequent battle the Empress's forces were defeated and Robert of Gloucester himself was taken prisoner. Further negotiations attempted to deliver a general peace agreement but the Queen was unwilling to offer any compromise to the Empress, and Robert refused to accept any offer to encourage him to change sides to Stephen. Instead, in November the two sides simply exchanged Robert and the King, with Stephen releasing Robert on 1 November 1141. Stephen began re-establishing his authority. Henry held another church council, which this time reaffirmed Stephen's legitimacy to rule, and a fresh coronation of Stephen and Matilda occurred at Christmas 1141.

At the beginning of 1142 Stephen fell ill, and by Easter rumours had begun to circulate that he had died. Possibly this illness was the result of his imprisonment the previous year, but he finally recovered and travelled north to raise new forces and to successfully convince Ranulf of Chester to change sides once again. Stephen then spent the summer attacking some of the new Angevin castles built the previous year, including Cirencester, Bampton and Wareham. In September, he spotted an opportunity to seize the Empress Matilda herself in Oxford. Oxford was a secure town, protected by walls and the river Isis, but Stephen led a sudden attack across the river, leading the charge and swimming part of the way. Once on the other side, the King and his men stormed into the town, trapping the Empress in the castle. Oxford Castle, however, was a powerful fortress and, rather than storming it, Stephen had to settle down for a long siege, albeit secure in the knowledge that Matilda was now surrounded. Just before Christmas, the Empress left the castle unobserved, crossed the icy river on foot and made her escape to Wallingford. The garrison surrendered shortly afterwards, but Stephen had lost an opportunity to capture his principal opponent.

The war between the two sides in England reached a stalemate in the mid-1140s, while Geoffrey of Anjou consolidated his hold on power in Normandy. 1143 started precariously for Stephen when he was besieged by Robert of Gloucester at Wilton Castle, an assembly point for royal forces in Herefordshire. Stephen attempted to break out and escape, resulting in the battle of Wilton. Once again, the Angevin cavalry proved too strong, and for a moment it appeared that Stephen might be captured for a second time. On this occasion, however, William Martel, Stephen's steward, made a fierce rear guard effort, allowing Stephen to escape from the battlefield. Stephen valued William's loyalty sufficiently to agree to exchange Sherborne Castle for his safe release—this was one of the few instances where Stephen was prepared to give up a castle to ransom one of his men.

In late 1143, Stephen faced a new threat in the east, when Geoffrey de Mandeville, Earl of Essex, rose up in rebellion against him in East Anglia. The King had disliked the Earl for several years, and provoked the conflict by summoning Geoffrey to court, where the King arrested him. He threatened to execute Geoffrey unless the Earl handed over his various castles, including the Tower of London, Saffron Walden and Pleshey, all important fortifications because they were in, or close to, London. Geoffrey gave in, but once free he headed north-east into the Fens to the Isle of Ely, from where he began a military campaign against Cambridge, with the intention of progressing south towards London. With all of his other problems and with Hugh Bigod, 1st Earl of Norfolk, in open revolt in Norfolk, Stephen lacked the resources to track Geoffrey down in the Fens and made do with building a screen of castles between Ely and London, including Burwell Castle.

For a period, the situation continued to worsen. Ranulf of Chester revolted once again in the summer of 1144, splitting up Stephen's Honour of Lancaster between himself and Prince Henry. In the west, Robert of Gloucester and his followers continued to raid the surrounding royalist territories, and Wallingford Castle remained a secure Angevin stronghold, too close to London for comfort. Meanwhile, Geoffrey of Anjou finished securing his hold on southern Normandy and in January 1144 he advanced into Rouen, the capital of the duchy, concluding his campaign. Louis VII recognised him as Duke of Normandy shortly after. By this point in the war, Stephen was depending increasingly on his immediate royal household, such as William of Ypres and others, and lacked the support of the major barons who might have been able to provide him with significant additional forces; after the events of 1141, Stephen made little use of his network of earls.

After 1143 the war ground on, but progressing slightly better for Stephen. Miles of Gloucester, one of the most talented Angevin commanders, had died whilst hunting over the previous Christmas, relieving some of the pressure in the west. Geoffrey de Mandeville's rebellion continued until September 1144, when he died during an attack on Burwell. The war in the west progressed better in 1145, with the King recapturing Faringdon Castle in Oxfordshire. In the north, Stephen came to a fresh agreement with Ranulf of Chester, but then in 1146 repeated the ruse he had played on Geoffrey de Mandeville in 1143, first inviting Ranulf to court, before arresting him and threatening to execute him unless he handed over a number of castles, including Lincoln and Coventry. As with Geoffrey, the moment Ranulf was released he immediately rebelled, but the situation was a stalemate: Stephen had few forces in the north with which to prosecute a fresh campaign, whilst Ranulf lacked the castles to support an attack on Stephen. By this point, however, Stephen's practice of inviting barons to court and arresting them had brought him into some disrepute and increasing distrust.

England had suffered extensively from the war by 1147, leading later Victorian historians to call the period of conflict "the Anarchy". The contemporary "Anglo-Saxon Chronicle" recorded how "there was nothing but disturbance and wickedness and robbery". Certainly in many parts of the country, such as Wiltshire, Berkshire, the Thames Valley and East Anglia, the fighting and raiding had caused serious devastation. Numerous "adulterine", or unauthorised, castles had been built as bases for local lords—the chronicler Robert of Torigny complained that as many as 1,115 such castles had been built during the conflict, although this was probably an exaggeration as elsewhere he suggested an alternative figure of 126. The previously centralised royal coinage system was fragmented, with Stephen, the Empress and local lords all minting their own coins. The royal forest law had collapsed in large parts of the country. Some parts of the country, though, were barely touched by the conflict—for example, Stephen's lands in the south-east and the Angevin heartlands around Gloucester and Bristol were largely unaffected, and David I ruled his territories in the north of England effectively. Stephen's overall income from his estates, however, declined seriously during the conflict, particularly after 1141, and royal control over the minting of new coins remained limited outside of the south-east and East Anglia. With Stephen often based in the south-east, increasingly Westminster, rather than the older site of Winchester, was used as the centre of royal government.

The character of the conflict in England gradually began to shift; as historian Frank Barlow suggests, by the late 1140s "the civil war was over", barring the occasional outbreak of fighting. In 1147 Robert of Gloucester died peacefully, and the next year the Empress Matilda left south-west England for Normandy, both of which contributed to reducing the tempo of the war. The Second Crusade was announced, and many Angevin supporters, including Waleran of Beaumont, joined it, leaving the region for several years. Many of the barons were making individual peace agreements with each other to secure their lands and war gains. Geoffrey and Matilda's son, the future King Henry II of England, mounted a small mercenary invasion of England in 1147 but the expedition failed, not least because Henry lacked the funds to pay his men. Surprisingly, Stephen himself ended up paying their costs, allowing Henry to return home safely; his reasons for doing so are unclear. One potential explanation is his general courtesy to a member of his extended family; another is that he was starting to consider how to end the war peacefully, and saw this as a way of building a relationship with Henry.

The young Henry FitzEmpress returned to England again in 1149, this time planning to form a northern alliance with Ranulf of Chester. The Angevin plan involved Ranulf agreeing to give up his claim to Carlisle, held by the Scots, in return for being given the rights to the whole of the Honour of Lancaster; Ranulf would give homage to both David and Henry FitzEmpress, with Henry having seniority. Following this peace agreement, Henry and Ranulf agreed to attack York, probably with help from the Scots. Stephen marched rapidly north to York and the planned attack disintegrated, leaving Henry to return to Normandy, where he was declared duke by his father.

Although still young, Henry was increasingly gaining a reputation as an energetic and capable leader. His prestige and power increased further when he unexpectedly married the attractive Eleanor, Duchess of Aquitaine, the recently divorced wife of Louis VII, in 1152. The marriage made Henry the future ruler of a huge swathe of territory across France.

In the final years of the war, Stephen began to focus on the issue of his family and the succession. He wanted to confirm his eldest son, Eustace, as his successor, although chroniclers recorded that Eustace was infamous for levying heavy taxes and extorting money from those on his lands. Stephen's second son, William, was married to the extremely wealthy heiress Isabel de Warenne. In 1148, Stephen built the Cluniac Faversham Abbey as a resting place for his family. Both Stephen's wife, Queen Matilda, and his older brother Theobald died in 1152.

Stephen's relationship with the church deteriorated badly towards the end of his reign. The reforming movement within the church, which advocated greater autonomy from royal authority for the clergy, had continued to grow, while new voices such as the Cistercians had gained additional prestige within the monastic orders, eclipsing older orders such as the Cluniacs. Stephen's dispute with the church had its origins in 1140, when Archbishop Thurstan of York died. An argument then broke out between a group of reformers based in York and backed by Bernard of Clairvaux, the head of the Cistercian order, who preferred William of Rievaulx as the new archbishop, and Stephen and his brother Henry, who preferred various Blois family relatives. The row between Henry and Bernard grew increasingly personal, and Henry used his authority as legate to appoint his nephew William of York to the post in 1144 only to find that, when Pope Innocent II died in 1145, Bernard was able to get the appointment rejected by Rome. Bernard then convinced Pope Eugene III to overturn Henry's decision altogether in 1147, deposing William, and appointing Henry Murdac as archbishop instead.

Stephen was furious over what he saw as potentially precedent-setting papal interference in his royal authority, and initially refused to allow Murdac into England. When Theobald, the Archbishop of Canterbury, went to consult with the Pope on the matter against Stephen's wishes, the King refused to allow him back into England either, and seized his estates. Stephen also cut his links to the Cistercian order, and turned instead to the Cluniacs, of which Henry was a member.

Nonetheless, the pressure on Stephen to get Eustace confirmed as his legitimate heir continued to grow. The King gave Eustace the County of Boulogne in 1147, but it remained unclear whether Eustace would inherit England. Stephen's preferred option was to have Eustace crowned while he himself was still alive, as was the custom in France, but this was not the normal practice in England, and Celestine II, during his brief tenure as pope between 1143 and 1144, had banned any change to this practice. Since the only person who could crown Eustace was Archbishop Theobald, who refused to do so without agreement from the current pope, Eugene III, the matter reached an impasse. At the end of 1148, Stephen and Theobald came to a temporary compromise that allowed Theobald to return to England. Theobald was appointed a papal legate in 1151, adding to his authority. Stephen then made a fresh attempt to have Eustace crowned at Easter 1152, gathering his nobles to swear fealty to Eustace, and then insisting that Theobald and his bishops anoint him king. When Theobald refused yet again, Stephen and Eustace imprisoned both him and the bishops and refused to release them unless they agreed to crown Eustace. Theobald escaped again into temporary exile in Flanders, pursued to the coast by Stephen's knights, marking a low point in Stephen's relationship with the church.

Henry FitzEmpress returned to England again at the start of 1153 with a small army, supported in the north and east of England by Ranulf of Chester and Hugh Bigod. Stephen's castle at Malmesbury was besieged by Henry's forces, and the King responded by marching west with an army to relieve it. He unsuccessfully attempted to force Henry's smaller army to fight a decisive battle along the river Avon. In the face of the increasingly wintry weather, Stephen agreed to a temporary truce and returned to London, leaving Henry to travel north through the Midlands where the powerful Robert de Beaumont, Earl of Leicester, announced his support for the Angevin cause. Despite only modest military successes, Henry and his allies now controlled the south-west, the Midlands and much of the north of England.

Over the summer, Stephen intensified the long-running siege of Wallingford Castle in a final attempt to take this major Angevin stronghold. The fall of Wallingford appeared imminent and Henry marched south in an attempt to relieve the siege, arriving with a small army and placing Stephen's besieging forces under siege themselves. Upon news of this, Stephen gathered up a large force and marched from Oxford, and the two sides confronted each other across the River Thames at Wallingford in July. By this point in the war, the barons on both sides seem to have been eager to avoid an open battle. As a result, instead of a battle ensuing, members of the church brokered a truce, to the annoyance of both Stephen and Henry.

In the aftermath of Wallingford, Stephen and Henry spoke together privately about a potential end to the war; Stephen's son Eustace, however, was furious about the peaceful outcome at Wallingford. He left his father and returned home to Cambridge to gather more funds for a fresh campaign, where he fell ill and died the next month. Eustace's death removed an obvious claimant to the throne and was politically convenient for those seeking a permanent peace in England. It is possible, however, that Stephen had already begun to consider passing over Eustace's claim; historian Edmund King observes that Eustace's claim to the throne was not mentioned in the discussions at Wallingford, for example, and this may have added to his anger.

Fighting continued after Wallingford, but in a rather half-hearted fashion. Stephen lost the towns of Oxford and Stamford to Henry while the King was diverted fighting Hugh Bigod in the east of England, but Nottingham Castle survived an Angevin attempt to capture it. Meanwhile, Stephen's brother Henry of Blois and Archbishop Theobald of Canterbury were for once unified in an effort to broker a permanent peace between the two sides, putting pressure on Stephen to accept a deal. The armies of Stephen and Henry FitzEmpress met again at Winchester, where the two leaders would ratify the terms of a permanent peace in November. Stephen announced the Treaty of Winchester in Winchester Cathedral: he recognised Henry FitzEmpress as his adopted son and successor, in return for Henry doing homage to him; Stephen promised to listen to Henry's advice, but retained all his royal powers; Stephen's remaining son, William, would do homage to Henry and renounce his claim to the throne, in exchange for promises of the security of his lands; key royal castles would be held on Henry's behalf by guarantors, whilst Stephen would have access to Henry's castles; and the numerous foreign mercenaries would be demobilised and sent home. Stephen and Henry sealed the treaty with a kiss of peace in the cathedral.

Stephen's decision to recognise Henry as his heir was, at the time, not necessarily a final solution to the civil war. Despite the issuing of new currency and administrative reforms, Stephen might potentially have lived for many more years, whilst Henry's position on the continent was far from secure. Although Stephen's son William was young and unprepared to challenge Henry for the throne in 1153, the situation could well have shifted in subsequent years—there were widespread rumours during 1154 that William planned to assassinate Henry, for example. Historian Graham White describes the treaty of Winchester as a "precarious peace", in line with the judgement of most modern historians that the situation in late 1153 was still uncertain and unpredictable.

Certainly many problems remained to be resolved, including re-establishing royal authority over the provinces and resolving the complex issue of which barons should control the contested lands and estates after the long civil war. Stephen burst into activity in early 1154, travelling around the kingdom extensively. He began issuing royal writs for the south-west of England once again and travelled to York where he held a major court in an attempt to impress upon the northern barons that royal authority was being reasserted. After a busy summer in 1154, however, Stephen travelled to Dover to meet Thierry, Count of Flanders; some historians believe that the King was already ill and preparing to settle his family affairs. Stephen fell ill with a stomach disorder and died on 25 October at the local priory, being buried at Faversham Abbey with his wife Matilda and son Eustace.

After Stephen's death, Henry II succeeded to the throne of England. Henry vigorously re-established royal authority in the aftermath of the civil war, dismantling castles and increasing revenues, although several of these trends had begun under Stephen. The destruction of castles under Henry was not as dramatic as once thought, and although he restored royal revenues, the economy of England remained broadly unchanged under both rulers. Stephen's son William was confirmed as the Earl of Surrey by Henry, and prospered under the new regime, with the occasional point of tension with Henry. Stephen's daughter Marie I, Countess of Boulogne, also survived her father; she had been placed in a convent by Stephen, but after his death she left and married. Stephen's middle son, Baldwin, and second daughter, Matilda, had died before 1147 and were buried at Holy Trinity Priory, Aldgate. Stephen probably had three illegitimate sons, Gervase, Abbot of Westminster, Ralph and Americ, by his mistress Damette; Gervase became abbot in 1138, but after his father's death he was removed by Henry in 1157 and died shortly afterwards.

Much of the modern history of Stephen's reign is based on accounts of chroniclers who lived in, or close to, the middle of the 12th century, forming a relatively rich account of the period. All of the main chronicler accounts carry significant regional biases in how they portray the disparate events. Several of the key chronicles were written in the south-west of England, including the "Gesta Stephani", or "Acts of Stephen", and William of Malmesbury's "Historia Novella", or "New History". In Normandy, Orderic Vitalis wrote his "Ecclesiastical History", covering Stephen's reign until 1141, and Robert of Torigni wrote a later history of the rest of the period. Henry of Huntingdon, who lived in the east of England, produced the "Historia Anglorum" that provides a regional account of the reign. The "Anglo-Saxon Chronicle" was past its prime by the time of Stephen, but is remembered for its striking account of conditions during "the Anarchy". Most of the chronicles carry some bias for or against Stephen, Robert of Gloucester or other key figures in the conflict. Those writing for the church after the events of Stephen's later reign, such as John of Salisbury for example, paint the King as a tyrant due to his argument with the Archbishop of Canterbury; by contrast, clerics in Durham regarded Stephen as a saviour, due to his contribution to the defeat of the Scots at the battle of the Standard. Later chronicles written during the reign of Henry II were generally more negative: Walter Map, for example, described Stephen as "a fine knight, but in other respects almost a fool." A number of charters were issued during Stephen's reign, often giving details of current events or daily routine, and these have become widely used as sources by modern historians.

Historians in the "Whiggish" tradition that emerged during the Victorian era traced a progressive and universalist course of political and economic development in England over the medieval period. William Stubbs focused on these constitutional aspects of Stephen's reign in his 1874 volume the "Constitutional History of England", beginning an enduring interest in Stephen and his reign. Stubbs' analysis, focusing on the disorder of the period, influenced his student John Round to coin the term "the Anarchy" to describe the period, a label that, whilst sometimes critiqued, continues to be used today. The late-Victorian scholar Frederic William Maitland also introduced the possibility that Stephen's reign marked a turning point in English legal history—the so-called "tenurial crisis".

Stephen remains a popular subject for historical study: David Crouch suggests that after King John he is "arguably the most written-about medieval king of England". Modern historians vary in their assessments of Stephen as a king. Historian R. H. C. Davis's influential biography paints a picture of a weak king: a capable military leader in the field, full of activity and pleasant, but "beneath the surface ... mistrustful and sly", with poor strategic judgement that ultimately undermined his reign. Stephen's lack of sound policy judgement and his mishandling of international affairs, leading to the loss of Normandy and his consequent inability to win the civil war in England, is also highlighted by another of his biographers, David Crouch. Historian and biographer Edmund King, whilst painting a slightly more positive picture than Davis, also concludes that Stephen, while a stoic, pious and genial leader, was also rarely, if ever, his own man, usually relying upon stronger characters such as his brother or wife. Historian Keith Stringer provides a more positive portrayal of Stephen, arguing that his ultimate failure as king was the result of external pressures on the Norman state, rather than the result of personal failings.

Stephen and his reign have been occasionally used in historical fiction. Stephen and his supporters appear in Ellis Peters' historical detective series "Brother Cadfael", set between 1137 and 1145. Peters' depiction of Stephen's reign is an essentially local narrative, focused on the town of Shrewsbury and its environs. Peters paints Stephen as a tolerant man and a reasonable ruler, despite his execution of the Shrewsbury defenders after the taking of the city in 1138. In contrast, he is depicted unsympathetically in both Ken Follett's historical novel "The Pillars of the Earth" and the TV mini-series adapted from it.

Stephen of Blois married Matilda of Boulogne in 1125. They had five children:


King Stephen's illegitimate children by his mistress Damette included:



</doc>
<doc id="28954" url="https://en.wikipedia.org/wiki?curid=28954" title="Space Battleship Yamato">
Space Battleship Yamato

It is one of the most influential anime series in Japan due to its theme and story, marking a turn towards more complex serious works and influencing works such as "Mobile Suit Gundam", "Neon Genesis Evangelion" and "Super Dimension Fortress Macross" as well as video games such as "Space Invaders". Hideaki Anno has ranked "Yamato" as his favorite anime and credited it with sparking his interest in anime.

"Yamato" was the first anime series or movie to win the Seiun Award, a feat not repeated until the film "Nausicaä of the Valley of the Wind" (1984).

Conceived in 1973 by producer Yoshinobu Nishizaki, the project underwent heavy revisions. Originally intended to be an outer-space variation on "Lord of the Flies", the project at first was titled "Asteroid Ship Icarus" and had a multinational teenage crew journeying through space in a hollowed-out asteroid in search of the planet Iscandar. There was to be much discord among the crew; many of them acting purely out of self-interest and for personal gain. The enemy aliens were originally called Rajendora.

In the year 2199, an alien race known as the Gamilas (Gamilons in the English "Star Blazers" dub) unleash radioactive meteorite bombs on Earth, rendering the planet's surface uninhabitable. Humanity has retreated into deep underground cities, but the radioactivity is slowly affecting them as well, with humanity's extinction estimated in one year. Earth's space fleet is hopelessly outclassed by the Gamilas and all seems lost until a message capsule from a mysterious crashed spaceship is retrieved on Mars. The capsule yields blueprints for a faster-than-light engine and an offering of help from Queen Starsha of the planet Iscandar in the Large Magellanic Cloud. She says that her planet has a device, the Cosmo-Cleaner D (Cosmo DNA), which can cleanse Earth of its radiation damage.

The inhabitants of Earth secretly build a massive spaceship inside the ruins of the gigantic Japanese battleship "Yamato" which lies exposed at the former bottom of the ocean location where she was sunk in World War II. This becomes the "Space Battleship Yamato" for which the story is titled. In the English "Star Blazers" dub, the ship is noted as being the historical "Yamato", but is then renamed the "Argo" (after the ship of Jason and the Argonauts).

Using Starsha's blueprints, they equip the new ship with a space warp drive, called the "wave motion engine", and a new, incredibly powerful weapon at the bow called the "Wave Motion Gun". The is capable of converting the vacuum of space into tachyon energy, as well as functioning like a normal rocket engine, and providing essentially infinite power to the ship, it enables the "Yamato" to "ride" the wave of tachyons and travel faster than light. The , also called the Dimensional Wave Motion Explosive Compression Emitter, is the "trump card" of the Yamato that functions by connecting the Wave Motion Engine to the enormous firing gate at the ship's bow, enabling the tachyon energy power of the engine to be fired in a stream directly forwards. Enormously powerful, it can vaporize a fleet of enemy ships—or a small continent (as seen in the first season, fifth episode)—with one shot; however, it takes a brief but critical period to charge before firing.

A crew of 114 departs for Iscandar in the "Yamato" to retrieve the radiation-removing device and return to Earth within the one-year deadline. Along the way, they discover the motives of their blue-skinned adversaries: the planet Gamilas, sister planet to Iscandar, is dying; and its leader, Lord Desslar (Desslok in the "Star Blazers" dub), is trying to irradiate Earth enough for his people to move there, at the expense of the "barbarians" he considers humanity to be.

The first season contained 26 episodes, following the "Yamato"s voyage out of the Milky Way Galaxy and back again. A continuing story, it features the declining health of "Yamato"s Captain Okita (Avatar in the "Star Blazers" dub), and the transformation of the brash young orphan Susumu Kodai (Derek Wildstar) into a mature officer, as well as his budding romance with female crewmember Yuki Mori (Nova Forrester). The foreign edits tend to play up the individual characters, while the Japanese original is often more focused on the ship itself. In a speech at the 1995 Anime Expo, series episode director Noboru Ishiguro said low ratings and high production expenses forced producer Yoshinobu Nishizaki to trim down the episode count from the original 39 episodes to only 26. The 13 episodes would have introduced Captain Harlock as a new series character.

The series was condensed into a 130-minute-long movie by combining elements from a few key episodes of the first season. Additional animation was created for the movie (such as the scenes on Iscandar) or recycled from the series' test footage (such as the opening sequence). The movie, which was released in Japan on August 6, 1977, was edited down further and dubbed into English in 1978; entitled "Space Cruiser Yamato" or simply "Space Cruiser", it was only given a limited theatrical release in Europe and Latin America, where it was called ' ("Star Patrol", in Brazilian Portuguese) or ' ("Starship Intrepid", in Spanish), though it was later released on video in most countries.


The success of the "Yamato" movie in Japan eclipsed that of the local release of "Star Wars", leading to the production of a second movie that would end the story. Also going by the name " Yamato", "Farewell to Space Battleship Yamato", set in the year 2201, shows the "Yamato" crew going up against the White Comet Empire, a mobile city fortress called Gatlantis, from the Andromeda Galaxy. A titanic space battle results in the crew going out on a suicide mission to save humanity. The film has been considered as a non-canonical, alternate timeline.

Viewer dissatisfaction with the ending of "Farewell to Space Battleship Yamato" prompted the production of a second "Yamato" television season which retconned the film and presented a slightly different plot against Zōdah (Prince Zordar in the "Star Blazers" dub) and his Comet Empire, and ended without killing off the "Yamato" or its primary characters. Like "Farewell", the story is set in the year 2201, and expands the film story to 26 episodes. This second season featured additional plots such as a love story between Teresa (Trelaina) and "Yamato" crew member Daisuke Shima (Mark Venture), and an onboard antagonism between Kodai and Saito (Knox), leader of a group of space marines.

Footage from "Farewell to Space Battleship Yamato" was reused in the second season, particularly in the opening titles. The sequence of the "Yamato" launching from water was also reused in two of the subsequent movies.

The television movie "Yamato: The New Voyage" (aka "Yamato: The New Journey"), came next, featuring a new enemy, the Black Nebula Empire. The story opens in late 2201. In the film, later modified into a theatrical movie, Desslar sees his homeworld, Gamilas, destroyed by the grey-skinned aliens, and its twin planet Iscandar next in line for invasion. He finds an eventual ally in the "Yamato", then on a training mission under deputy captain Kodai.

The theatrical movie "Be Forever Yamato", set in the year 2202, sees the Black Nebula Empire launch a powerful weapon at Earth, a hyperon bomb which will annihilate humanity if they resist a full-scale invasion. The "Yamato", under new captain, Yamanami, travels to the aliens' home galaxy only to discover what appears to be a future Earth—defeated and ruled by the enemy. Appearing in this film is Sasha, the daughter of Queen Starsha of Iscandar and Mamoru Kodai (Susumu's older brother).

Following these movies, a third season of the television series was produced, broadcast on Japanese television in 1980. Its date was not mentioned in the broadcast, but design documents, as well as anime industry publications, cited the year 2205. In the story, the Sun is hit by a stray proton missile from a nearby battle between forces of the Galman Empire and Bolar Federation. This missile greatly accelerates nuclear fusion in the Sun, and humanity must either evacuate to a new home or find a means of preventing a supernova. During the course of the story, it is learned that the people of the Galman Empire are actually the forebears of Desslar and the Gamilas race. Desslar and the remnants of his space fleet have found and liberated Galman from the Bolar Federation. Originally conceived as a 52-episode story, funding cuts meant the season had to be truncated to 25 episodes, with a corresponding loss of overall story development. This third season was adapted into English several years after the original "Star Blazers" run and, to the dissatisfaction of fans, used different voice actors than did the earlier seasons.

Premiering in Japanese theaters on March 19, 1983, "Final Yamato" reunites the crew one more time to combat the threat of the Denguilu, a militaristic alien civilization that intends to use the water planet, Aquarius, to flood Earth and resettle there (having lost their home planet to a galactic collision). Captain Okita, who was found to be in cryogenic sleep since the first season, returns to command the "Yamato" and sacrifices himself to stop the Denguili's plan. Susumu and Yuki also get married.

The story is set in the year 2203, contradicting earlier assumptions that its predecessor, "Yamato III", took place in 2205. Having a running time of 163 minutes, "Final Yamato" holds the record of being the longest animated film ever made, a record which has yet to be surpassed as of 2019.

In the mid-1990s, Nishizaki attempted to create a sequel to "Yamato", set hundreds of years after the original. "Yamato 2520" was to chronicle the adventures of the eighteenth starship to bear the name, and its battle against the Seiren Federation. Much of the continuity established in the original series (including the destruction of Earth's moon) is ignored in this sequel.

In place of Leiji Matsumoto, American artist Syd Mead ("∀ Gundam", "Blade Runner", "Tron" and "") provided the conceptual art.

Due to the bankruptcy of Nishizaki's company Office Academy (former Academy Productions), and legal disputes with Matsumoto over the ownership of the "Yamato" copyrights, the series was never finished and only three episodes were produced.

 is a graphic novel comic created by the animator Leiji Matsumoto. For a time it was streaming online. However this has since stopped.

In March 2002, a Tokyo court ruled that Yoshinobu Nishizaki legally owned the "Yamato" copyrights. Nishizaki and Matsumoto eventually settled, and Nishizaki pushed ahead with developing a new Yamato television series. Project proposals for a 26-episode television series were drawn up in early 2004, but no further work was done with Tohoku Shinsha not backing the project. American series expert Tim Eldred was able to secure a complete package of art, mecha designs, and story outline at an auction over Japanese store Mandarake in April 2014.

Set 20 years after Final Yamato, the series would have shown Susumu Kodai leading a salvage operation for the remains of the Yamato. The ship is rebuilt as the Earth Defense Force builds a second Space Battleship Yamato to combat the Balbard Empire, an alien race that has erected a massive honeycombed cage called Ru Sak Gar, over Earth in a bid to stop the human race's spacefaring efforts. A feature film to be released after the series ended would have featured the original space battleship fighting the Balbards' attempt to launch a black hole at Earth. Kodai, Yuki, and Sanada are the only original series characters who would have returned in the series.

 is the second original animated video based on Space Battleship Yamato

The story begins in 3199, when a mighty enemy attacks the Milky Way from a neighbouring galaxy, and defeats the Milky Way Alliance, reducing them to just six fleets. After the Alliance headquarters is destroyed, and when the collapse of the central Milky Way Alliance is imminent, the Great Yamato "Zero" embarks on a mission to assist the Milky Way Alliance in one last great battle.

Although New Space Battleship Yamato was sent to the discard pile, Nishizaki began work on a new movie titled , set after the original series, while Matsumoto planned a new "Yamato series". However, additional legal conflicts stalled both projects until August 2008, when Nishizaki announced plans for the release of his film on December 12, 2009.

Set 17 years after the events of "Final Yamato", "Resurrection" brings together some members of the "Yamato" crew, who lead Earth's inhabitants to resettle in a far-flung star system after a black hole is discovered, which will destroy the solar system in three months.

Released on December 1, 2010, "Space Battleship Yamato" is the franchise's first live-action film. Directed by Takashi Yamazaki, the movie stars Takuya Kimura as Susumu Kodai and Meisa Kuroki as Yuki. It was revealed originally that the plot would be based on that of the 1974 series. However, an official trailer released during June 2010 on Japanese television has also shown elements from the series' second season (1978).

Debuting in Japanese cinemas on April 7, 2012, "2199" is a remake of the 1974 series. Yutaka Izubuchi serves as supervising director, with character designs by Nobuteru Yuki, and Junichiro Tamamori and Makoto Kobayashi in charge of mecha and conceptual designs. The series is a joint project of Xebec and AIC. Hideaki Anno designed the new series' opening sequence.

The sequel to the first remake heptalogy, and debuting in Japanese Cinemas on February 25, 2017, "2202" is a remake of the second series, with Nobuyoshi Habara as director and Harutoshi Fukui as writer. Most of the staff and original cast from the first remake were brought back to the project. It is animated by Xebec.

With the retelling of " Yamato" as the open-ended "Yamato II" television series (ending in late 2201), " Yamato" was redesignated as a discardable, alternate timeline. The follow-on film, "Yamato: New Journey", took place in late 2201; and its successor, "Be Forever Yamato", in early 2202. "Yamato III" was commonly believed to be set in 2205 (several printed publications used this date, although it was never stated in the show's broadcast). But the following film, "Final Yamato", was set in 2203. The opening narration of "Final" mentioned the Bolar/Galman conflict, implying that the date for "Yamato III" was to be regarded as some time between 2202 and 2203 (making for an unrealistic and compressed timeline).

It is not known if this change was due to the lackluster response to "Yamato III", the production staff's dissatisfaction with the truncated series (additionally, Nishizaki and Matsumoto had limited involvement with it), or a mere oversight.

In 2220, the ship is rebuilt following the events of "Final Yamato". The new captain of the ship is Susumu Kodai, who was the main character in the previous movies. This told in "Space Battleship Yamato: Resurrection" that it is set 17 years after "Final Yamato".

Space Battleship Yamato was a 1985 Japanese exclusive Laserdisc video game designed by Taito which was based on the television series of the same name.

The "Space Battleship Yamato" series generally involves themes of brave sacrifice, noble enemies, and respect for heroes lost in the line of duty. This can be seen as early as the second episode of the first season, which recounts the defeat of the original battleship "Yamato" while sailors and pilots from both sides salute her as she sinks (this scene was cut from the English dub, but later included on the "Star Blazers" DVD release). The movies spend much time showing the crew visiting monuments to previous missions and recalling the bravery of their fallen comrades. Desslar, the enemy defeated in the first season and left without a home or a people, recognizes that his foes are fighting for the same things he fought for and, eventually, becomes Earth's most important ally.

For many years, English-language releases of the anime bore the title "Space Cruiser Yamato". This romanization has appeared in Japanese publications because Nishizaki, a sailing enthusiast who owned a cruiser yacht, ordered that this translation be used out of love for his boat. However, in reference to naval nomenclature, it is technically inaccurate, as means "battleship" and not "cruiser" (which in Japanese would be ). Leiji Matsumoto's manga adaptation was titled "Cosmoship Yamato". Today, "Yamato" releases, including the Voyager Entertainment DVD, are marketed either as "Star Blazers" or "Space Battleship Yamato".

"Star Blazers" (1979) is a heavily edited dubbed version for the United States market produced by Westchester Film Corporation. Voyager Entertainment released DVD volumes and comic adaptations of the anime years later.



</doc>
<doc id="28957" url="https://en.wikipedia.org/wiki?curid=28957" title="Southern blot">
Southern blot

A Southern blot is a method used in molecular biology for detection of a specific DNA sequence in DNA samples. Southern blotting combines transfer of electrophoresis-separated DNA fragments to a filter membrane and subsequent fragment detection by probe hybridization.

The method is named after the British biologist Edwin Southern, who first published it in 1975. Other blotting methods (i.e., western blot, northern blot, eastern blot, southwestern blot) that employ similar principles, but using RNA or protein, have later been named in reference to Edwin Southern's name. As the label is eponymous, Southern is capitalised, as is conventional of proper nouns. The names for other blotting methods may follow this convention, by analogy.


Hybridization of the probe to a specific DNA fragment on the filter membrane indicates that this fragment contains DNA sequence that is complementary to the probe.
The transfer step of the DNA from the electrophoresis gel to a membrane permits easy binding of the labeled hybridization probe to the size-fractionated DNA. It also allows for the fixation of the target-probe hybrids, required for analysis by autoradiography or other detection methods.
Southern blots performed with restriction enzyme-digested genomic DNA may be used to determine the number of sequences (e.g., gene copies) in a genome. A probe that hybridizes only to a single DNA segment that has not been cut by the restriction enzyme will produce a single band on a Southern blot, whereas multiple bands will likely be observed when the probe hybridizes to several highly similar sequences (e.g., those that may be the result of sequence duplication). Modification of the hybridization conditions (for example, increasing the hybridization temperature or decreasing salt concentration) may be used to increase specificity and decrease hybridization of the probe to sequences that are less than 100% similar.

Southern blotting transfer may be used for homology-based cloning on the basis of amino acid sequence of the protein product of the target gene. Oligonucleotides are designed so that they are similar to the target sequence. The oligonucleotides are chemically synthesized, radiolabeled, and used to screen a DNA library, or other collections of cloned DNA fragments. Sequences that hybridize with the hybridization probe are further analysed, for example, to obtain the full length sequence of the targeted gene.

Southern blotting can also be used to identify methylated sites in particular genes. Particularly useful are the restriction nucleases "MspI" and "HpaII", both of which recognize and cleave within the same sequence. However, "HpaII" requires that a C within that site be methylated, whereas "MspI" cleaves only DNA unmethylated at that site. Therefore, any methylated sites within a sequence analyzed with a particular probe will be cleaved by the former, but not the latter, enzyme.




</doc>
<doc id="28961" url="https://en.wikipedia.org/wiki?curid=28961" title="Standard-gauge railway">
Standard-gauge railway

A standard-gauge railway is a railway with a track gauge of . The standard gauge is also called Stephenson gauge after George Stephenson, International gauge, UIC gauge, uniform gauge, normal gauge and European gauge in Europe. It is the most widely used railway track gauge across the world, with approximately 55% of the lines in the world using it. All high-speed rail lines use standard gauge except those in Russia, Finland, Portugal and Uzbekistan. The distance between the inside edges of the rails is defined to be 1435 mm except in the United States and on some heritage British lines, where it is still defined in U.S. customary units as exactly "four feet eight and one half inches" (0.1 mm larger than the metric standard).

As railways developed and expanded, one of the key issues was the track gauge (the distance, or width, between the inner sides of the rails) to be used. Different railways used different gauges, and where rails of different gauge met – a "gauge break" – loads had to be unloaded from one set of rail cars and re-loaded onto another, a time-consuming and expensive process. The result was the adoption throughout a large part of the world of a "standard gauge" of , allowing interconnectivity and interoperability.

A popular legend that has been around since at least 1937 traces the origin of the gauge even further back than the coalfields of northern England, pointing to the evidence of rutted roads marked by chariot wheels dating from the Roman Empire. It is curious that the Roman pace or "passus" was 4.855 ft or 1435 mm; a thousand such was one Roman mile. Snopes categorised this legend as "false", but commented that "it is perhaps more fairly labelled as 'True, but for trivial and unremarkable reasons. The historical tendency to place the wheels of horse-drawn vehicles approximately apart probably derives from the width needed to fit a carthorse in between the shafts. In addition, while road-travelling vehicles are typically measured from the outermost portions of the wheel rims (and there is some evidence that the first railways were measured in this way as well), it became apparent that for vehicles travelling on rails it was better to have the wheel flanges located "inside" the rails, and thus the distance measured on the inside of the wheels (and, by extension, the inside faces of the rail heads) was the important one.

There was never a standard gauge for horse railways, but there were rough groupings: in the north of England none was less than . Wylam colliery's system, built before 1763, was , as was John Blenkinsop's Middleton Railway; the old plateway was relaid to so that Blenkinsop's engine could be used. Others were (in Beamish) or (in Bigges Main (in Wallsend), Kenton, and Coxlodge).

The English railway pioneer George Stephenson spent much of his early engineering career working for the coal mines of County Durham. He favoured for wagonways in Northumberland and Durham, and used it on his Killingworth line. The Hetton and Springwell wagonways also used this gauge.

Stephenson's Stockton and Darlington railway (S&DR) was built primarily to transport coal from mines near Shildon to the port at Stockton-on-Tees. The initial gauge of was set to accommodate the existing gauge of hundreds of horse-drawn chaldron wagons that were already in use on the wagonways in the mines. The railway used this gauge for 15 years before a change was made to the in gauge. The historic Mount Washington Cog Railway, the world's first mountain-climbing rack railway, is still in operation in the 21st century, and has used the earlier gauge since its inauguration in 1868.

George Stephenson used the gauge (including a belated extra of free movement to reduce binding on curves) for the Liverpool and Manchester Railway, authorised in 1826 and opened 30 September 1830. The success of this project led to Stephenson and his son Robert being employed to engineer several other larger railway projects. Thus the gauge became widespread and dominant in Britain. Robert was reported to have said that if he had had a second chance to choose a standard gauge, he would have chosen one wider than . "I would take a few inches more, but a very few".

During the "gauge war" with the Great Western Railway, standard gauge was called narrow gauge, in contrast to the Great Western's broad gauge. The modern use of the term "narrow gauge" for gauges less than standard did not arise for many years, until the first such locomotive-hauled passenger railway, the Ffestiniog Railway was built.

In 1845, in the United Kingdom of Great Britain and Ireland, a Royal Commission on Railway Gauges reported in favour of a standard gauge. The subsequent Gauge Act ruled that new passenger-carrying railways in Great Britain should be built to a standard gauge of , and those in Ireland to a new standard gauge of . In Great Britain, Stephenson's gauge was chosen on the grounds that existing lines of this gauge were eight times longer than those of the rival (later ) gauge adopted principally by the Great Western Railway. It allowed the broad-gauge companies in Great Britain to continue with their tracks and expand their networks within the "Limits of Deviation" and the exceptions defined in the Act. After an intervening period of mixed-gauge operation (tracks were laid with three rails), the Great Western Railway finally completed the conversion of its network to standard gauge in 1892. In North East England, some early lines in colliery (coal mining) areas were , while in Scotland some early lines were . All these lines had been widened to standard gauge by 1846. The British gauges converged starting from 1846 as the advantages of equipment interchange became increasingly apparent. By the 1890s, the entire network was converted to standard gauge.

The Royal Commission made no comment about small lines narrower than standard gauge (to be called "narrow gauge"), such as the Ffestiniog Railway. Thus it permitted a future multiplicity of narrow gauges in the UK. It also made no comments about future gauges in British colonies, which allowed various gauges to be adopted across the colonies.

Parts of the United States, mainly in the Northeast, adopted the same gauge, because some early trains were purchased from Britain. The American gauges converged, as the advantages of equipment interchange became increasingly apparent. Notably, all the broad gauge track in the South was converted to "almost standard" gauge over the course of two days beginning on 31 May 1886. "See" Track gauge in the United States.

In continental Europe, France and Belgium adopted a gauge (measured between the midpoints of each rail's profile) for their early railways. The gauge between the interior edges of the rails (the measurement adopted from 1844) differed slightly between countries, and even between networks within a country (for example, to in France).
The first tracks in Austria and in the Netherlands had other gauges ( in Austria for the Donau Moldau linen and in the Netherlands for the Hollandsche IJzeren Spoorweg-Maatschappij), but for interoperability reasons (the first rail service between Paris and Berlin began in 1849, first Chaix timetable) Germany adopted standard gauges, as did most other European countries.

The modern method of measuring rail gauge was agreed in the first Berne rail convention of 1886, according to the "Revue générale des chemins de fer, July 1928".






Several states in the United States had laws requiring road vehicles to have a consistent gauge to allow them to follow ruts in the road. Those gauges were similar to railway standard gauge.





</doc>
<doc id="28962" url="https://en.wikipedia.org/wiki?curid=28962" title="Sodium laureth sulfate">
Sodium laureth sulfate

Sodium laureth sulfate (SLES), an accepted contraction of sodium lauryl ether sulfate (SLES), is an anionic detergent and surfactant found in many personal care products (soaps, shampoos, toothpaste, etc.). SLES is an inexpensive and very effective foaming agent. SLES, sodium lauryl sulfate (SLS), ammonium lauryl sulfate (ALS), and sodium pareth sulfate are surfactants that are used in many cosmetic products for their cleaning and emulsifying properties. They behave similarly to soap. It is derived from palm kernel oil or coconut oil. 

Its chemical formula is . Sometimes the number represented by "n" is specified in the name, for example laureth-2 sulfate. The product is heterogeneous in the number of ethoxyl groups, where "n" is the mean. Laureth-3 sulfate is common in commercial products.

SLES is prepared by ethoxylation of dodecyl alcohol, which is produced industrially from palm kernel oil or coconut oil. The resulting ethoxylate is converted to a half ester of sulfuric acid, which is neutralized by conversion to the sodium salt. The related surfactant sodium lauryl sulfate (also known as sodium dodecyl sulfate or SDS) is produced similarly, but without the ethoxylation step. SLS and ammonium lauryl sulfate (ALS) are commonly used alternatives to SLES in consumer products.

Tests in the US indicate that it is safe for consumer use. The Australian government's Department of Health and Ageing and its National Industrial Chemicals Notification and Assessment Scheme (NICNAS) have determined SLES does not react with DNA.

Like many other detergents, SLES is an irritant. It has also been shown that SLES causes eye or skin irritation in experiments conducted on animals and humans. The related surfactant SLS is a known irritant.

Some products containing SLES contain traces (up to 300 ppm) of 1,4-dioxane, which is formed as a by-product during the ethoxylation step of its production. 1,4-Dioxane is classified by the International Agency for Research on Cancer as a Group 2B carcinogen: "possibly carcinogenic to humans". The United States Food and Drug Administration (FDA) recommends that these levels be monitored, and encourages manufacturers to remove 1,4-dioxane, though it is not required by federal law.



</doc>
<doc id="28965" url="https://en.wikipedia.org/wiki?curid=28965" title="Saraswati River (disambiguation)">
Saraswati River (disambiguation)

The Sarasvati River was one of the Rigvedic rivers that played an important role in the Vedic religion.

Saraswati River may also refer to:


</doc>
<doc id="28967" url="https://en.wikipedia.org/wiki?curid=28967" title="Simpson Desert">
Simpson Desert

The Simpson Desert is a large area of dry, red sandy plain and dunes in Northern Territory, South Australia and Queensland in central Australia. It is the fourth-largest Australian desert, with an area of .

The desert is underlain by the Great Artesian Basin, one of the largest inland drainage areas in the world. Water from the basin rises to the surface at numerous natural springs, including Dalhousie Springs, and at bores drilled along stock routes, or during petroleum exploration. As a result of exploitation by such bores, the flow of water to springs has been steadily decreasing in recent years. It is also part of the Lake Eyre basin.

The Simpson Desert is an erg that contains the world's longest parallel sand dunes. These north-south oriented dunes are static, held in position by vegetation. They vary in height from in the west to around on the eastern side. The largest dune, Nappanerica or Big Red, is in height.

The Wangkangurru people lived in the Simpson Desert, using hand-dug wells called mikiri, up until the Federation Drought.

The explorer Charles Sturt, who visited the region from 1844–1846, was the first European to see the desert. In 1880 Augustus Poeppel, a surveyor with the South Australian Survey Department determined the border between Queensland and South Australia to the west of Haddon Corner and in doing so marked the corner point where the States of Queensland and South Australia meet the Northern Territory. After he returned to Adelaide, it was discovered that the links in his surveyor's chain had stretched. Poeppel’s border post was too far west by 300 metres. In 1884, surveyor Larry Wells moved the post to its proper position on the eastern bank of Lake Poeppel. The tri-state border is now known as Poeppel Corner. In January 1886 surveyor David Lindsay ventured into the desert from the western edge, in the process discovering and documenting, with the help of a Wangkangurru Aboriginal man, nine native wells and travelling as far east as the Queensland/Northern Territory border.

In 1936 Ted Colson became the first non-indigenous person to cross the desert in its entirety, riding camels. The name Simpson Desert was coined by Cecil Madigan, after Alfred Allen Simpson, an Australian industrialist, philanthropist, geographer, and president of the South Australian branch of the Royal Geographical Society of Australasia. Mr Simpson was the owner of the Simpson washing machine company.

In 1984, Dennis Bartel was the first white man to successfully walk solo and unsupported west-to-east across the Simpson, 390 km in 24 days, relying on old Aboriginal wells for water. In 2006 Lucas Trihey was the first non-indigenous person to walk across the desert through the geographical centre away from vehicle tracks and unsupported. He carried all his equipment in a two-wheeled cart and crossed from East Bore on the western edge of the desert to Birdsville in the east. In 2008, Michael Giacometti completed the first, and only, east-to-west walk across the Simpson Desert. Starting at Bedourie in Queensland, he walked solo and unsupported, towing all his equipment, food and water in a two-wheeled cart to Old Andado homestead. Also in 2008, Belgian Louis-Philippe Loncke became the first non-indigenous person to complete a north-south crossing of the desert on foot and unsupported and through the geographical centre.

In 2016, explorer Sebastian Copeland and partner Mark George completed the longest unsupported latitudinal crossing (west-to-east across the dunes) of the Simpson They linked the Madigan Line, Colson Track and French Line for the first time, walking from Old Andado homestead to Birdsville, a distance of in 26 days.

In 1967, the Queensland Government established the Munga-Thirri National Park, formerly known as the Simpson Desert National Park

No maintained roads cross the desert. The Donohue Highway is an unpaved outback track passing from near Boulia towards the Northern Territory border in the north of the desert. There are tracks that were created during seismic surveys in the search for gas and oil during the 1960s and 1970s. These include the French Line, the Rig Road, and the QAA Line. Such tracks are still navigable by well-equipped four-wheel-drive vehicles which must carry extra fuel and water. Towns providing access to the South Australian edge of the Simpson Desert include Innamincka to the south and Oodnadatta to the southwest; and from the eastern (Queensland) side include Birdsville, Bedourie, Thargomindah and Windorah. Last fuel on the western side is at the Mount Dare hotel and store. Before 1980, a section of the Commonwealth Railways Central Australian line passed along the western side of the Simpson Desert.

The desert is popular with tourists, particularly in winter, and popular landmarks include the ruins and mound springs at Dalhousie Springs, Purnie Bore wetlands, Approdinna Attora Knoll and Poeppel Corner (where Queensland, South Australia and Northern Territory meet). Because of the excessive heat and inadequately experienced drivers attempting to access the desert in the past, the Department of Environment and Natural Resources has decided since 2008-2009 to close the Simpson Desert during the summer — to save unprepared "adventurers" from themselves.

The area has an extremely hot, dry desert climate. Rainfall is minimal, averaging only about 150 mm per year and falling mainly in summer. Temperatures in summer can approach 50 °C and large sand storms are common. Winters are generally cool, however, heatwaves even in the middle of July are not unheard of.

Some of the heaviest rain in decades occurred during 2009-2010, and has seen the Simpson Desert burst into life and colour. In early March 2010, Birdsville recorded more rain in 24 hours than is usual in a whole year. Rain inundated Queensland’s north-west and Gulf regions. In total, 17 million megalitres of water entered the State’s western river systems leading to Lake Eyre. In 2010, researchers uncovered the courses of ancient river systems under the desert.

The Simpson Desert is also a large part of the World Wildlife Fund ecoregion of the same name which consists of the Channel Country and the Simpson Strzelecki Dunefields bioregions of the Interim Biogeographic Regionalisation for Australia (IBRA).

The flora of the Simpson Desert ecoregion is limited to drought-resistant shrubs and grasses especially "Zygochloa paradoxa" grass that holds the dunes together and the spinifex and other tough grasses of sides slopes and sandy desert floor between the dunes. The Channel Country section of the ecoregion lies to the northeast of the desert proper around the towns of Bedourie and Windorah in Queensland, and consists of low hills covered with Mitchell grass cut through with rivers lined with coolabah trees. The ecoregion also includes areas of rocky upland and seasonally wet clay and salt pans, particularly Lake Eyre, the centre of one of the largest inland drainage systems in the world, including the Georgina and Diamantina Rivers.

Wildlife adapted to this hot, dry environment and seasonal flooding includes the water-holding frog ("Litoria platycephala") and a number of reptiles that inhabit the desert grasses. Endemic mammals of the desert include the kowari ("Dasycercus byrnei") while birds include the grey grasswren ("Amytornis barbatus") and Eyrean grasswren ("Amytornis goyderi"). Lake Eyre and the other seasonal wetlands are important habitats for fish and birds, especially as a breeding ground for waterbirds while the rivers are home to birds, bats and frogs. The seasonal wetlands of the ecoregion include Lake Eyre and the Coongie Lakes as well as the swamps that emerge when Cooper Creek, Strzelecki Creek and the Diamantina River are in flood. The birds that use these wetlands include the freckled duck ("Stictonetta naevosa"), musk duck ("Biziura lobata"), silver gull ("Larus novaehollandiae"), Australian pelican ("Pelecanus conspicillatus"), great egret ("Ardea alba"), glossy ibis ("Plegadis falcinellus"), and banded stilt ("Cladorhynchus leucocephalus"). Finally the mound springs of the Great Artesian Basin are important habitat for a number of plants, fish, snails and other invertebrates.

Native vegetation is largely intact as the desert is uninhabitable. Therefore, habitats are not threatened by agriculture, but are damaged by introduced species, particularly rabbits and feral camels. The only human activity in the desert proper has been the construction of the gas pipelines, while the country on its fringes has been used for cattle grazing and contains towns such as Innamincka. Mound springs and other waterholes are vulnerable to overuse and damage. Protected areas of the ecoregion include the Simpson Desert, Goneaway, Lochern, Bladensburg, Witjira and Kati Thanda-Lake Eyre National Parks as well as the Munga-Thirri—Simpson Desert Conservation Park, Innamincka Regional Reserve and Munga-Thirri–Simpson Desert Regional Reserve. Ethabuka Reserve is a nature reserve in the north of the desert owned and managed by Bush Heritage Australia.

The extensive dunefields of the Simpson Desert display a range of colours from brilliant white to dark red and include pinks and oranges.

The sand ridges have a trend of SSE-NNW and continue parallel for kilometres. This pattern is seen throughout the deserts of Australia. Some of the ridges continue unbroken for up to 200 km. The height and the spacing between the ridges are directly related. Where there are 5-6 ridges in a kilometre, the height is around 15 metres but when there is one or two ridges per kilometre the height jumps to 35–38 metres. In cross section, the lee side is the eastern slope with an incline of 34-38 degrees, while the stoss side is the western slope with an incline of only 10-20 degrees. In cross section, the cross beds are planar with foresets alternating between east and west. The foresets have incline angles of 10-30 degrees.

The sand is predominately made up of quartz grains. The grains are rounded and sub angular. They range in size from 0.05 mm to 1.2 mm with 0.5 mm being the average size for the crests and 0.3 mm being the average size on the dune flanks. The active crests have sand sediment but on the inter-dunes, the sediment is not as well sorted. The sediment varies in color from pink to brick red, but by the rivers and playas the sediment color is light grey. The progression of the color from grey to red is due to the release of iron oxide from the sediment when weathered.



</doc>
<doc id="28968" url="https://en.wikipedia.org/wiki?curid=28968" title="Skycar">
Skycar

Skycar may refer to:



</doc>
<doc id="28969" url="https://en.wikipedia.org/wiki?curid=28969" title="Silesian Voivodeship">
Silesian Voivodeship

Silesian Voivodeship, or Silesia Province ( ; ; ) is a voivodeship, or province, in southern Poland, centered on the historic region known as Upper Silesia ("), with Katowice serving as its capital. 

Despite the Silesian Voivodeship's name, most of the historic Silesia region lies outside the present Silesian Voivodeship — divided among Lubusz, Lower Silesian, and Opole Voivodeships — while the eastern half of Silesian Voivodeship (and, notably, Częstochowa in the north) was historically part of Lesser Poland.

The Voivodeship was created on 1 January 1999 out of the former Katowice, Częstochowa and Bielsko-Biała Voivodeships, pursuant to the Polish local government reforms adopted in 1998. 

It is the most densely populated voivodeship in Poland and within the area of 12,300 squared kilometres, there are almost 5 million inhabitants. It is also the largest urbanised area in Central and Eastern Europe. In relation to economy, over 13% of Poland’s Gross Domestic Product (GDP) is generated here, making the Silesian Voivodeship one of the wealthiest provinces in the country.

For the first time Silesian Voivodeship was appointed in Second Polish Republic. It had much wider range of power autonomy, than other contemporary Polish voivodeships and it covered all historical lands of Upper Silesia, which ended up in the Interwar period Poland (among them: Katowice (Kattowitz), Rybnik (Rybnik), Pszczyna (Pleß), Wodzisław (Loslau), Żory (Sohrau), Mikołów (Nikolai), Tychy (Tichau), Królewska Huta (Königshütte), Tarnowskie Góry (Tarnowitz), Miasteczko Śląskie (Georgenberg), Woźniki (Woischnik), Lubliniec (Lublinitz), Cieszyn (Teschen), Skoczów (Skotschau), Bielsko (Bielitz)). This Voivodeship did not include – as opposed to the present one – lands and cities of old pre-Partition Polish–Lithuanian Commonwealth. Among the last ones the Southern part was included in Kraków Voivodeship Żywiec (Saybusch), Wilamowice (Wilmesau), Biała Krakowska (Biala) and Jaworzno), and the North Western part Będzin (Bendzin), Dąbrowa Górnicza (Dombrowa), Sosnowiec (Sosnowitz), Częstochowa (Tschenstochau), Myszków, Szczekociny (Schtschekotzin), Zawiercie, Sławków) belonged to Kielce Voivodeship.

After aggression of Nazi Germany (Invasion of Poland), on 8 October 1939, Hitler published a decree "About division and administration of Eastern Territories". A Silesian Province (") was created, with a seat in Breslau (Wrocław). It consisted of four districts: Kattowitz, Oppeln, Breslau and Liegnitz.

The following counties were included in Kattowitz District: Kattowitz, Königshütte, Tarnowitz, Beuthen Hindenburg, Gleiwitz, Freistadt, Teschen, Biala, Bielitz, Saybusch, Pleß, Sosnowitz, Bendzin and parts of the following counties: Kranau, Olkusch, Riebnich and Wadowitz. However, according to Hitler’s decree from 12 October 1939 about establishing General Government ("Generalgouvernement"), Tschenstochau (Częstochowa) belonged to GG.

In 1941 the Silesian Province ("") underwent new administrative division and as a result Upper Silesian Province was created ("Provinz Oberschlesien"):

After the War during 1945 - 1950 there existed a Silesian Voivodeship, commonly known as Śląsko-Dąbrowskie Voivodeship, which included a major part of today's Silesian Voivodeship. In 1950 Śląsko-Dąbrowskie Voivodeship was divided into Opole and Katowice Voivodeships. The latter one had borders similar to the borders of modern Silesian Voivodeship.

The present Silesian Voivodeship was formed in 1999 from the following voivodeships of the previous administrative division:

The Silesian Voivodeship borders both the Moravian-Silesian Region (Czech Republic), Žilina Region (Slovakia) to the south. It is also bordered by four other Polish voivodeships: those of Opole (to the west), Łódź (to the north), Świętokrzyskie (to the north-east), and Lesser Poland (to the east).

The region includes the Silesian Upland (') in the centre and north-west, and the Krakowsko-Częstochowska Upland (') in the north-east. The southern border is formed by the Beskidy Mountains (Beskid Śląski and Beskid Żywiecki).

The current administrative unit of Silesian Voivodeship is just a fraction of the historical Silesia which is within the borders of today's Poland (there are also fragments of Silesia in the Czech Republic and Germany). Other parts of today's Polish Silesia are administered as the Opole, the Lower Silesian Voivodeships and the Lubusz Voivodeship. On the other hand, a large part of the current administrative unit of the Silesian Voivodeship is not part of historical Silesia (e.g., Częstochowa, Zawiercie, Myszków, Jaworzno, Sosnowiec, Żywiec, Dąbrowa Górnicza, Będzin and east part of Bielsko-Biała, which are historically parts of Lesser Poland).

Silesian Voivodeship has the highest population density in the country (379 people per square kilometre, compared to the national average of 124). The region's considerable industrialisation gives it the lowest unemployment rate nationally (6.2%). The Silesian region is the most industrialized and the most urbanized region in Poland: 78% of its population live in towns and cities.

Both northern and southern part of the voivodeship is surrounded by a green belt. Bielsko-Biała is enveloped by the Beskidy Mountains which are popular with winter sports fans. It offers over 150 ski lifts and 200 kilometres of ski routes. More and more slopes are illuminated and equipped with artificial snow generators. Szczyrk, Brenna, Wisła and Ustroń are the most popular winter mountain resorts. Rock climbing sites can be found in Jura Krakowsko-Czestochowska. The ruins of castles forming the Eagle Nests Trail are a famous attraction of the region. Often visited is the Black Madonna's Jasna Góra Sanctuary in Częstochowa - the annual destination of over 4 million pilgrims from all over the world. In south-western part of the voivodeship are parks, palaces and old monastery (Rudy Raciborskie, Wodzisław Śląski). Along Odra river are interesting natural reserve and at summer places for swimming.

With its more than two centuries of industrialisation history, region has a number of technical heritage memorials. These include narrow and standard gauge railways, coal and silver mines, shafts and its equipment from 19th and 20th century.

Due to its industrial and urban nature, the voivodeship has many cities and large towns. Of Poland's 40 most-populous cities, 12 are in Silesian Voivodeship. 19 of the cities in the voivodeship have the legal status of "city-county" (see powiat). In all it has 71 cities and towns (with legal city rights), listed below in descending order of population (according to official figures for 2006):
The Silesian voivodship is predominantly an industrial region. Most of the
mining is derived from one of the world's largest bituminous coalfields of the Upper Silesian Industrial District (') and the Rybnik Coal District (') with its major cities Rybnik, Jastrzębie Zdrój, Żory and Wodzisław Śląski. Lead and zinc can be found near Bytom, Zawiercie and Tarnowskie Góry; iron ore and raw materials for building - near Częstochowa. The most important regional industries are: mining, iron, lead and zinc metallurgy, power industry, engineering, automobile, chemical, building materials and textile. In the past, the Silesian economy was determined by coal mining. Now, considering the investment volume, car manufacturing is becoming more and more important. The most profitable company in the region is Fiat Auto-Poland S.A. in Bielsko-Biała with a revenue of PLN 6.2 billion in 1997. Recently a new car factory has been opened by GM Opel in Gliwice. There are two Special Economic Zones in the area: Katowice and Częstochowa. The voivodship's economy consists of about 323,000, mostly small and medium-sized, enterprises employing over 3 million people. The biggest Polish steel-works "Huta Katowice" is situated in Dąbrowa Górnicza.

Silesian Voivodship is also one of the richest regions in Poland. Average monthly salary is about 3,800 zlotys (over €1,200).

The unemployment rate stood at 3.9% in 2017 and was lower than the national average.

Katowice International Airport (in Tarnowskie Góry County) is used for domestic and international flights, Other Nearby Airports are John Paul II International Airport Kraków-Balice and Warsaw Frédéric Chopin Airport. The Silesian agglomeration railway network has the largest concentration in the country. The voivodship capital enjoys good railway and road connections with Gdańsk (motorway A1) and Ostrava (motorway A1), Kraków (motorway A4), Wrocław (motorway A4), Łódź (motorway A1) and Warsaw. It is also the crossing point for many international routes like E40 connecting Calais, Brussels, Cologne, Dresden, Wrocław, Kraków and Kiev and E75 from Scandinavia to the Balkans. A relatively short distance to Vienna facilitates cross-border co-operation and may positively influence the process of European integration.
Linia Hutnicza Szerokotorowa (known by its acronym "LHS", English: "Broad gauge metallurgy line") in Sławków is the longest broad gauge railway line in Poland. The line runs on a single track for almost 400 km from the Polish-Ukrainian border, crossing it just east of Hrubieszów. It is the westernmost broad gauge railway line in Europe that is connected to the broad gauge rail system of the countries of the former Soviet Union.

There are eleven public universities in the voivodship. The biggest university is the University of Silesia in Katowice, with 43,000 students. The region's capital boasts the Medical University, The Karol Adamiecki University of Economics in Katowice, the University of Music in Katowice, the Physical Education Academy and the Academy of Fine Arts. Częstochowa is the seat of the Częstochowa University of Technology and Pedagogic University. The Silesian University of Technology in Gliwice is nationally renowned. Bielsko-Biała is home of the Technical-Humanistic Academy. In addition, 17 new private schools have been established in the region.

There are over 300,000 people currently studying in the Voivodeship.

The Silesian voivodeship's government is headed by the province's ' (governor) who is appointed by the Polish Prime Minister. The ' is then assisted in performing his duties by the voivodeship's marshal, who is the appointed speaker for the voivodeship's executive and is elected by the ' (provincial assembly). The current ' of Silesia is Jarosław Wieczorek, whilst the present marshal is Wojciech Saługa.

The Sejmik of Silesia consists of 48 members.

Silesian Voivodeship is divided into 36 counties (powiats). These include 19 city counties (far more than any other voivodeship) and 17 land counties. The counties are further divided into 167 gminas.

The counties are listed in the following table (ordering within categories is by decreasing population).

Protected areas in Silesian Voivodeship include eight areas designated as Landscape Parks:




</doc>
<doc id="28970" url="https://en.wikipedia.org/wiki?curid=28970" title="SECD machine">
SECD machine

The SECD machine is a highly influential ("See: #Landin's contribution") virtual machine and abstract machine intended as a target for functional programming language compilers. The letters stand for Stack, Environment, Control, Dump, the internal registers of the machine. The registers Stack, Control, and Dump point to (some realisations of) stacks, and Environment points to (some realisation of) an associative array. 

The machine was the first to be specifically designed to evaluate lambda calculus expressions. It was originally described by Peter J. Landin in "The Mechanical Evaluation of Expressions" in 1964. The description published by Landin was fairly abstract, and left many implementation choices open (like an operational semantics). Hence the SECD machine is often presented in a more detailed form, such as Peter Henderson's Lispkit Lisp compiler, which has been distributed since 1980. Since then it has been used as the target for several other experimental compilers.

In 1989 researchers at the University of Calgary worked on a hardware implementation of the machine.

D. A. Turner (2012) points out that "The Revised Report on Algol 60" (Naur 1963) specifies a procedure call by a copying rule which avoids variable capture with a systematic change of identifiers. This method works in the Algol 60 implementation, but in a functional programming language where functions are first-class citizens, a free variable on a call stack might be dereferenced in error.

Turner notes that Landin solved this with his SECD machine, in which a function is represented by a closure in the heap instead.

When evaluation of an expression begins, the expression is loaded as the only element of control codice_1. The environment codice_2, stack codice_3 and dump codice_4 begin empty.

During evaluation of codice_1 it is converted to reverse Polish notation (RPN) with codice_6 (for apply) being the only operator. For example, the expression codice_7 (a single list element) is changed to the list codice_8.

Evaluation of codice_1 proceeds similarly to other RPN expressions. If the first item in codice_1 is a value, it is pushed onto the stack codice_3. More exactly, if the item is an identifier, the value pushed onto the stack will be the binding for that identifier in the current environment codice_2. If the item is an abstraction, a closure is constructed to preserve the bindings of its free variables (which are in codice_2), and it is this closure which is pushed onto the stack.

If the item is codice_6, two values are popped off the stack and the application done (first applied to second). If the result of the application is a value, it is pushed onto the stack.

If the application is of an abstraction to a value, however, it will result in a lambda calculus expression which may itself be an application (rather than a value), and so cannot be pushed onto the stack. In this case, the current contents of codice_3, codice_2, and codice_1 are pushed onto the dump codice_4 (which is a stack of these triples), codice_3 is reinitialised to empty, and codice_1 is reinitialised to the application result with codice_2 containing the environment for the free variables of this expression, augmented with the binding that resulted from the application. Evaluation then proceeds as above.

Completed evaluation is indicated by codice_1 being empty, in which case the result will be on the stack codice_3. The last saved evaluation state on codice_4 is then popped, and the result of the completed evaluation is pushed onto the stack contents restored from codice_4. Evaluation of the restored state then continues as above.

If codice_1 and codice_4 are both empty, overall evaluation has completed with the result on the stack codice_3.

The SECD machine is stack-based. Functions take their arguments from the stack. The arguments to built-in instructions are encoded immediately after them in the instruction stream.

Like all internal data-structures, the stack is a list, with the codice_3 register pointing at the list's "head" or beginning. Due to the list structure, the stack need not be a continuous block of memory, so stack space is available as long as there is a single free memory cell. Even when all cells have been used, garbage collection may yield additional free memory. Obviously, specific implementations of the SECD structure can implement the stack as a canonical stack structure, so improving the overall efficiency of the virtual machine, provided that a strict bound be put on the dimension of the stack.

The codice_1 register points at the head of the code or instruction list that will be evaluated. Once the instruction there has been executed, the codice_1 is pointed at the next instruction in the list—it is similar to an "instruction pointer" (or program counter) in conventional machines, except that subsequent instructions are always specified during execution and are not by default contained in subsequent memory locations, as it is the case with the conventional machines.

The current variable environment is managed by the codice_2 register, which points at a list of lists. Each individual list represents one environment level: the parameters of the current function are in the head of the list, variables that are free in the current function, but bound by a surrounding function, are in other elements of codice_2.

The dump, at whose head the codice_4 register points, is used as temporary storage for values of the other registers, for example during function calls. It can be likened to the return stack of other machines.

The memory organization of the SECD machine is similar to the model used by most functional language interpreters: a number of memory cells, each of which can hold either an "atom" (a simple value, for example "13"), or represent an empty or non-empty list. In the latter case, the cell holds two pointers to other cells, one representing the first element, the other representing the list except for the first element. The two pointers are traditionally named "car" and "cdr" respectively—but the more modern terms "head" and "tail" are often used instead. The different types of values that a cell can hold are distinguished by a "tag". Often different types of atoms (integers, strings, etc.) are distinguished as well.

So, a list holding the numbers "1", "2", and "3", usually written as codice_35, might be represented as follows:

The memory cells 3 to 5 do not belong to our list, the cells of which can be distributed randomly over the memory. Cell 2 is the head of the list, it points to cell 1 which holds the first element's value, and the list containing only "2" and "3" (beginning at cell 6). Cell 6 points at a cell holding 2 and at cell 7, which represents the list containing only "3". It does so by pointing at cell 8 containing the value "3", and pointing at an empty list ("nil") as cdr. In the SECD machine, cell 0 always implicitly represents the empty list, so no special tag value is needed to signal an empty list (everything needing that can simply point to cell 0).

The principle that the cdr in a list cell must point at another list is just a convention. If both car and cdr point at atoms, that will yield a pair, usually written like codice_36


A number of additional instructions for basic functions like car, cdr, list construction, integer addition, I/O, etc. exist. They all take any necessary parameters from the stack.




</doc>
<doc id="28971" url="https://en.wikipedia.org/wiki?curid=28971" title="Stratego">
Stratego

Stratego is a strategy board game for two players on a board of 10×10 squares. Each player controls 40 pieces representing individual officer and soldier ranks in an army. The pieces have Napoleonic insignia. The objective of the game is to find and capture the opponent's "Flag", or to capture so many enemy pieces that the opponent cannot make any further moves. "Stratego" has simple enough rules for young children to play but a depth of strategy that is also appealing to adults. The game is a slightly modified copy of an early 20th century French game named "L'Attaque". It has been in production in Europe since World War II and the United States since 1961. There are now two- and four-handed versions, versions with 10, 30 or 40 pieces per player, and boards with smaller sizes (number of spaces). There are also variant pieces and different .

The International Stratego Federation, the game's governing body, sponsors an annual Stratego World Championship.

"Stratego" is from the French or Greek "strategos" (var. "strategus") for leader of an ancient (especially Greek) army; first general.

The name "Stratego" was first registered in 1942 in the Netherlands. The United States trademark was filed in 1958 and registered in 1960 to Jacques Johan Mogendorff and is presently owned by Jumbo Games as successors to Hausemann and Hotte, headquartered in the Netherlands. It has been licensed to manufacturers like Milton Bradley, Hasbro and others, as well as retailers like Barnes & Noble, Target stores, etc.

The game box contents are a set of 40 gold-embossed red playing pieces, a set of 40 silver-embossed blue playing pieces, a glossy folding rectangular cardboard playing board imprinted with a 10×10 grid of spaces, and instructions printed in English on the underside of the box top. The early sets featured painted wood pieces, later sets colored plastic. The pieces are small and roughly rectangular, tall and wide, and unweighted. More modern versions first introduced in Europe have cylindrical castle-shaped pieces. Some versions have a cardboard privacy screen to assist setup. A few versions have wooden boxes or boards.

Typically, color is chosen by lot: one player uses red pieces, and the other uses blue pieces. Before the start of the game, players arrange their 40 pieces in a 4×10 configuration at either end of the board. The ranks are printed on one side only and placed so that the players cannot identify the opponent's pieces. Players may not place pieces in the lakes or the 12 squares in the center of the board. Such pre-play distinguishes the fundamental strategy of particular players, and influences the outcome of the game.

Players alternate moving; red moves first. Each player moves one piece per turn. A player must move a piece in his turn; there is no "pass" (like that in the game of Go).

Two zones in the middle of the board, each 2×2, cannot be entered by either player's pieces at any time. They are shown as lakes on the battlefield and serve as choke points to make frontal assaults less direct.

The game can be won by capturing the opponent's "Flag", or all of his moveable pieces. It is possible to have ranked pieces that are not moveable because they are trapped behind "bomb"s.

The average game has 381 moves. The number of legal positions is 10. The number of possible games is 10. "Stratego" has many more moves and substantially greater complexity than other familiar games like chess and backgammon; however, unlike those games where a single bad move at any point may result in loss of the game, most moves in "Stratego" are inconsequential.

All movable pieces, with the exception of the "Scout", may move only one step to any adjacent space vertically or horizontally (but not diagonally). A piece may not move onto a space occupied by a like-color piece. "Bomb" and "Flag" pieces are not moveable. The "Scout" may move any number of spaces in a straight line (such as the rook in chess). In the older versions of "Stratego" the "Scout" could not move and strike in the same turn; in newer versions this was allowed. Even before that, sanctioned play usually amended the original "Scout" movement to allow moving and striking in the same turn because it facilitates gameplay. No piece can move back and forth between the same two spaces for more than three consecutive turns (two square rule), nor can a piece endlessly chase a piece it has no hope of capturing (more square rule).

When the player wants to attack, they move their piece onto a square occupied by an opposing piece. Both players then reveal their piece's rank; the weaker piece (see exceptions below) is removed from the board. If the engaging pieces are of equal rank, both are removed. A piece may not move onto a square already occupied unless it attacks. Two pieces have special attack powers. One special piece is the "Bomb" which only "Miners" can defuse. It immediately eliminates any other piece striking it, without itself being destroyed. Each player also has one "Spy", which succeeds only if it attacks the "Marshal" or the "Flag". If the "Spy" attacks any other piece, or is attacked by any piece (including the "Marshal"), the "Spy" is defeated. The original rules contained a provision that following a strike, the winning piece immediately occupies the space vacated by the losing piece. This makes sense when the winning piece belongs to the player on move, but no sense when the winning piece belongs to the player not on move. The latter part of the rule has been quietly ignored in most play.

Competitive play does not include recording the game, unlike chess. The game is fast-paced, no standard notation exists, and players keep their initial setups secret, so recording games is impractical. However, digital interfaces like web-based gaming interfaces, may have a facility for recording, replaying and downloading the game. Those interfaces use an algebraic-style notation that numbers the rows ('ranks') 1 to 10 from bottom to top and the columns ('files') A to J from left to right. Alternately, a few interfaces designate the files as A to K, omitting 'I'. Moves are recorded as source square followed by destination square separated by a "-" (move) or "x" (strike). Revealed pieces on strikes precede the square designation, and may be by either rank name or rank number for brevity, for example "major B2xcaptain B3". The bottom half of the board is by default considered to be the 'red' side, and the top half the 'blue' side.

No published compilation of recorded exemplary games (akin to master games in chess) exists.

Unlike chess, "Stratego" is a game of incomplete information. In addition to calculated sequences of moves, this gives rise to aspects of battle psychology like concealment, bluffing, lying in wait and guessing. No exposition of the strategy of "Stratego", either set up or game play, has been published.

There are seven immobile pieces – six "Bombs" and one "Flag" – and 33 ranked mobile pieces per player. From highest rank to lowest the pieces are:

Some versions (primarily those released since 2000) make 10 (the "Marshal") the highest rank with the "Spy" ranked 1, while others (versions prior to 2000, as well as the Nostalgia version released in 2002) have the "Marshal" piece ranked at 1 and the "Spy" designated S. The European version depicts pieces with the lower number to be higher ranked, whereas the American version depicts pieces with the higher number to be higher ranked. All moving pieces can capture the flag. 

Variant versions of the game have a few different pieces with different rules of movement, like the "Cannon", "Archer" (possibly a different name for the "Cannon"), "Spotter", "Infiltrator", "Corporal" and "Cavalry Captain". In one version, mobile pieces are allowed to "carry" the "Flag". In some variants like "Stratego Waterloo" and "Fire and Ice Stratego", all or most of the pieces have substantially different moves; these are essentially different games.

In nearly its present form "Stratego" appeared in France from La Samaritaine in 1910, and then in Britain before World War I, as a game called "L'attaque". Historian and game collector Thierry Depaulis writes:

It was in fact designed by a lady, Mademoiselle Hermance Edan, who filed a patent for a ""jeu de bataille avec pièces mobiles sur damier"" (a battle game with mobile pieces on a gameboard) on 1908-11-26. The patent was released by the French Patent Office in 1909 (patent #396.795). Hermance Edan had given no name to her game but a French manufacturer named Au Jeu Retrouvé was selling the game as "L'Attaque" as early as 1910.

Depaulis further notes that the 1910 version was played with 36 pieces per player on a 9×10 board and the armies were divided into red and blue colors. The rules of "L'attaque" were basically the same as for the game we know as "Stratego". It featured standing cardboard rectangular pieces, color printed with soldiers who wore contemporary (to 1900) uniforms, not Napoleonic uniforms. In papers of her estate, Ms. Edan states that she developed the game in the 1880s.

"L'attaque" was later produced in England by game maker H.P. Gibson and Sons, who bought the rights to the game in 1925, until the 1970s, at least, retaining the French name at least to begin with.



"Stratego" was created by some time before 1942. The name was registered as a trademark in 1942 by the Dutch company Van Perlstein & Roeper Bosch N.V. (which also produced the first edition of "Monopoly"). After WW2, Mogendorff licensed Stratego to Smeets and Schippers, a Dutch company, in 1946. Hausemann and Hotte acquired a license in 1958 for European distribution, and in 1959 for global distribution. After Mogendorff's death in 1961, Hausemann and Hotte purchased the trademark from his heirs, and sublicensed it to Milton Bradley (which was acquired by Hasbro in 1984) in 1961 for United States distribution. In 2009, Hausemann and Hotte was succeeded by Koninklijke Jumbo B.V. in the Netherlands.

The modern game of "Stratego", with its Napoleonic imagery, was originally manufactured in the Netherlands. Pieces were originally made of printed cardboard and inserted in metal clip stands. After World War II, painted wood pieces became standard. Starting in the early 1960s all versions switched to plastic pieces. The change from wood to plastic was made for economical reasons, as was the case with many products during that period, but with Stratego the change also served a structural function: Unlike the wooden pieces, the plastic pieces were designed with a small base. The wooden pieces had none, often resulting in pieces tipping over. This was disastrous for that player, since it often immediately revealed the piece's rank, as well as unleashing a literal domino effect by having a falling piece knock over other pieces. European versions introduced cylindrical castle-shaped pieces that proved to be popular. American editions later introduced new rectangular pieces with a more stable base and colorful stickers, not images directly imprinted on the plastic.

European versions of the game give the "Marshal" the highest number (10), while the initial American versions give the "Marshal" the lowest number (1) to show the highest value (i.e. it is the #1 or most powerful tile). More recent American versions of the game, which adopted the European system, caused considerable complaint among American players who grew up in the 1960s and 1970s. This may have been a factor in the release of a Nostalgic edition, in a wooden box, reproducing the Classic edition of the early 1970s.

"Electronic Stratego" was introduced by Milton Bradley in 1982. It has features that make many aspects of the game strikingly different from those of classic "Stratego". Each type of playing piece in "Electronic Stratego" has a unique series of bumps on its bottom that are read by the game's battery-operated touch-sensitive "board". When attacking another piece a player hits their Strike button, presses their piece and then the targeted piece: the game either rewards a successful attack or punishes a failed strike with an appropriate bit of music. In this way the players never know for certain the rank of the piece that wins the attack, only whether the attack wins, fails, or ties (similar to the role of the referee in the Chinese game of "Luzhanqi"). Instead of choosing to move a piece, a player can opt to "probe" an opposing piece by hitting the Probe button and pressing down on the enemy piece: the game then beeps out a rough approximation of the strength of that piece. There are no "Bomb" pieces: "Bombs" are set using pegs placed on a touch-sensitive "peg board" that is closed from view prior to the start of the game. Hence, it is possible for a player to have their piece occupying a square with a bomb on it. If an opposing piece lands on the seemingly empty square, the game plays the sound of an explosion and that piece is removed from play. As in classic "Stratego", only a "Miner" can remove a "Bomb" from play. A player who successfully captures the opposing "Flag" is rewarded with a triumphant bit of music from the "1812 Overture".

In the late 1990s, the Jumbo Company released several European variants, including a three- and four-player version, and a new "Cannon" piece (which jumps two squares to capture any piece, but loses to any attack against it). It also included some alternate rules such as "Barrage" (a quicker two-player game with fewer pieces) and "Reserves" (reinforcements in the three- and four-player games). The four-player version appeared in America in 1997.

Starting in the 2000s, Hasbro, under its Milton Bradley label, released a series of popular media-themed Stratego editions.

Besides themed variants with substantially different rules, current production includes three slightly different editions: sets with classic (1961) piece numbering (highest rank=1), sets with European piece numbering (highest rank=10), and sets that allow substitution of one or two variant pieces such as "Cannons", usually in place of scouts. Sets produced since 1970 or so have uniformly adopted the rule that scouts can move and strike in the same turn.

Accolade first introduced a Microsoft DOS-based "Stratego" AI in 1990, but it was not even so good as a rank beginner human player, lacking any apparent strategic conception and making many tactical blunders. Modern AIs exist and compete in various tournaments including the Computer Stratego World Championship, but are currently no better than an intermediate level human player.

A digital Stratego CD-ROM was introduced by Hasbro Interactive in 1998 for Windows 95/98. It included all the games of Ultimate Stratego as well as classic Stratego, and was designed to be used over an LAN, modem-to-modem, or over the internet.

In 2013, Jumbo, together with Keesing Games launched stratego.com, which is free to play online with other players. Since its launch, the site has come to have the largest stratego player base.

"Stratego" and its predecessor "L'Attaque" have spawned several derivative games, notably two 20th century Chinese games, "Game of the fighting animals" ("Dou Shou Qi") also known as Jungle or "Animal Chess", and Land Battle Chess (Lu Zhan Qi).

The game Jungle also has pieces (but of animals rather than soldiers) with different ranks and pieces with higher rank capture the pieces with lower rank. The board, with two lakes in the middle, is also remarkably similar to that in "Stratego". The major differences between the two games is that in Jungle, the pieces are not hidden from the opponent, and the initial setup is fixed. According to historian C.R. Bell, this game is 20th century, and cannot have been a predecessor of "L'Attaque" or "Stratego".

A modern, more elaborate, Chinese game known as Land Battle Chess (Lu Zhan Qi) or Army Chess (Lu Zhan Jun Qi) is a descendant of Jungle, and a cousin of Stratego: It is played on a 5×13 board with two un-occupiable spaces in the middle and each player has 25 playing pieces. The initial setup is not fixed, both players keep their pieces hidden from their opponent, and the objective is to capture the enemy's flag.[2] Lu Zhan Jun Qi's basic gameplay is similar, though differences include "missile" pieces and a xiangqi-style board layout with the addition of railroads and defensive "camps". A third player is also typically used as a neutral referee to decide battles between pieces without revealing their identities. An expanded version of the Land Battle Chess game also exists, adding naval and aircraft pieces and is known as Sea-Land-Air Battle Chess (Hai Lu Kong Zhan Qi).[3]

A capture the flag game called "Stratego" and loosely based on the board game is played at summer camps. In this game, two teams of thirty to sixty players are assigned ranks by distribution of coloured objects such as pinnies or glowsticks, the colours representing rank, not team. Players can tag and capture lower-ranked opponents, with the exception that the lowest rank captures the highest. Players who do not know their teammates may not be able to tell which team other players are on, creating incomplete information and opportunities for bluffing.

Unlike the vast literature for chess, checkers and backgammon, as of 2019, there is a single book, "Stratego: From Beginner To Winner", written by Richard Ratcliffe and published by Steel City Press.

The game remains in production, with new versions continuing to appear every few years. These are a few of the notable ones. In addition, the first U.S. edition (1961) Milton Bradley set, and a special edition 1963 set called "Stratego Fine", had wooden pieces. The 1961 wood pieces had a design that looked like vines scaling a castle wall on the back. But note that later 1961 production featured plastic pieces (not true first editions). All other regular edition sets had plastic pieces. A few special editions as noted below had wooden or metal pieces.

These have 10×10 boards, 40 pieces per side with classic pieces and rules of movement.

Official Modern Version: Also known as Stratego Original. Redesigned pieces and game art. The pieces now use stickers attached to new "castle-like" plastic pieces. The stickers must be applied by the player after purchase, though the box does not mention any assembly being required. Rank numbering is reversed in European style (higher numbers equals higher rank). Comes with an optional alternate piece, the "Infiltrator".

Stratego 50th Anniversary 1997 by Spin Master comes in both a book style box and a cookie tin like metal box, with original artwork, pieces and gameplay. Optional "Cannons" (2 per player) playing pieces.

Nostalgia Game Series Edition: Released 2002. Traditional stamped plastic pieces, although the metallic paint is dull and less reflective than some older versions, and the pieces are not engraved as some previous editions were. Wooden box, traditional board and piece numbering.

Library Edition: Hasbro's Library Series puts what appears to be the classic Stratego of the Nostalgia Edition into a compact, book-like design. The wooden box approximates the size of a book and is made to fit in a bookcase in one's library. In this version, the scout may not move and strike in the same turn.

Michael Graves Design Stratego by Milton Bradley introduced in 2002 and sold exclusively through Target Stores. It features a finished wood box, wooden pedestal board, and closed black and white roughly wedge-shaped plastic pieces. Limited production, no longer available.

Stratego Onyx: Introduced in 2009, Stratego Onyx was sold exclusively by Barnes & Noble. It includes foil stamped wooden game pieces and a raised gameboard with a decorative wooden frame. One-time production, no longer available.

Franklin Mint Civil War Collector's Edition: In the mid-1990s, Franklin Mint created a luxury version of Stratego with a Civil War theme and gold- and silver-plated pieces. Due to a last-minute licensing problem, the set was never officially released and offered for sale. The only remaining copies are those sent to the company's retail stores for display.

These have substantially different configurations and rules.

Ultimate Stratego: No longer in production, this version can still be found at some online stores and specialty gaming stores. This version is a variant of traditional "Stratego" and can accommodate up to 4 players simultaneously. The "Ultimate Stratego" board game contained four different Stratego versions: "Ultimate Lightning", "Alliance Campaign", "Alliance Lightning" and "Ultimate Campaign".

Science Fiction Version: Jumbo B.V. / Spin Master version of "Stratego", common in North American department stores. The game has a futuristic science fiction theme. Played on a smaller 8×10 board, with 30 pieces per player. Features unique "Spotter" playing pieces.

Stratego Waterloo: For the bicentenary of the Battle of Waterloo in June 2015, the Dutch publishing group Jumbo published "Stratego Waterloo". Instead of using ranks, the different historical units that had actually fought at the battle were added as "Pawns" (Old Guard, 95th Rifles...) – each with their own strengths and weaknesses. The "Pawns" are divided into light infantry, line infantry, light cavalry, heavy cavalry, artillery, commanders and commanders-in-chief (Wellington and Napoleon). Instead of capturing the "Flag", the players must get two of their pawns on the lines of communication of their opponent.

Stratego Conquest: 1996, two- to four-handed game played on world map; alternate pieces cannons and cavalry

Stratego Fortress: A 3D version of "Stratego" featuring a 3-level fortress and mystical themed pieces and maneuvers

Fire and Ice Stratego: The Hasbro version called Fire and Ice Stratego has different pieces and rules of movement. The game features a smaller 8×10 board and each player has 30 magical and mythological themed pieces with special powers.

Hertog Jan, a Dutch brand of beer, released "Stratego Tournament", a promotional version of "Stratego" with variant rules. It includes substantially fewer pieces, including only one Bomb and no Miners. Since each side has only about 18 pieces, the pieces are far more mobile. The scout in this version is allowed to move three squares in any combination of directions (including L-shapes) and there is a new piece called the "Archer", which is defeated by anything, but can defeat any piece other than the "Bomb" by shooting it from a two-square distance, in direct orthogonal, or straight, directions only. If one player is unable to move any more of his or her pieces, the game results in a tie because neither player's "Flag" was captured.

These variants are produced by the company with pop culture themed pieces.

Produced by Avalon Hill:

Produced by USAopoly:
"Stratego" is a very competitive game and this competition has increased over the years. There are now many "Stratego" competitions held throughout the world.
The game is particularly popular in the Netherlands, Germany, Greece, and Belgium, where regular world and national championships are organized. The international "Stratego" scene has, more recently, been dominated by players from the Netherlands. Stratego World Championships have been held since 1997 and continue to be held yearly around August; the latest was 2019.

Competitive "Stratego" competitions are now held in all four versions of the game:





World Championships

Other tournaments






</doc>
<doc id="28972" url="https://en.wikipedia.org/wiki?curid=28972" title="Sindh">
Sindh

Sindh (; ; , ) is one of the four provinces of Pakistan. Located in the southeast of the country, it is the historical home of the Sindhi people. Sindh is the third largest province of Pakistan by area, and second largest province by population after Punjab. Sindh is bordered by Balochistan province to the west, and Punjab province to the north. Sindh also borders the Indian states of Gujarat and Rajasthan to the east, and Arabian Sea to the south. Sindh's landscape consists mostly of alluvial plains flanking the Indus River, the Thar desert in the eastern portion of the province closest to the border with India, and the Kirthar Mountains in the western part of Sindh.

Sindh has Pakistan's second largest economy, while its provincial capital Karachi is Pakistan's largest city and financial hub, and hosts the headquarters of several multinational banks. Sindh is home to a large portion of Pakistan's industrial sector and contains two of Pakistan's commercial seaports, Port Bin Qasim and the Karachi Port. The remainder of Sindh has an agriculture based economy, and produces fruit, food consumer items, and vegetables for the consumption of other parts of the country.

Sindh is known for its distinct culture which is strongly influenced by Sufism, an important marker of Sindhi identity for both Hindus (Sindh has Pakistan's highest percentage of Hindu residents) and Muslims in the province. Several important Sufi shrines are located throughout the province which attract millions of annual devotees.

Sindh's capital, Karachi, is Pakistan's most ethnically diverse city, with Muhajirs, or descendants of those who migrated to Pakistan from India after 1947 and throughout the 1950s and 1960s, making up the majority of the population. Sindh is home to two UNESCO World Heritage Sites – the Historical Monuments at Makli, and the Archaeological Ruins at Mohenjodaro.

The word "Sindh" is derived from the Sanskrit term "Sindhu" (literally meaning "river"), which is a reference to Indus River. The official spelling "Sind" (from the Perso-Arabic pronunciation ) was discontinued in 1988 by an amendment passed in Sindh Assembly.

The Greeks who conquered Sindh in 325 BC under the command of Alexander the Great rendered it as "Indós", hence the modern "Indus". The ancient Iranians referred to everything east of the river Indus as "hind".

Sindh's first known village settlements date as far back as 7000 BC. Permanent settlements at Mehrgarh, currently in Balochistan, to the west expanded into Sindh. This culture blossomed over several millennia and gave rise to the Indus Valley Civilization around 3000 BC. The Indus Valley Civilization rivalled the contemporary civilizations of Ancient Egypt and Mesopotamia in size and scope, numbering nearly half a million inhabitants at its height with well-planned grid cities and sewer systems.

The primitive village communities in Balochistan were still struggling against a difficult highland environment, a highly cultured people were trying to assert themselves at Kot Diji. This was one of the most developed urban civilizations of the ancient world. It flourished between the 25th and 15th centuries BC in the Indus valley sites of Mohenjo Daro and Harappa. The people had a high standard of art and craftsmanship and a well-developed system of quasi-pictographic writing which remains un-deciphered. The ruins of the well planned towns, the brick buildings of the common people, roads, public baths and the covered drainage system suggest a highly organized community.

According to some accounts, there is no evidence of large palaces or burial grounds for the elite. The grand and presumably holy site might have been the great bath, which is built upon an artificially created elevation. This civilization collapsed around 1700 BC for reasons uncertain; the cause is hotly debated and may have been a massive earthquake, which dried up the Ghaggar River. Skeletons discovered in the ruins of Moan Jo Daro ("mount of dead") were thought to indicate that the city was suddenly attacked and the population was wiped out, but further examinations showed that the marks on the skeletons were due to erosion and not of violence.

The ancient city of Roruka, identified with modern Aror/Rohri, was capital of the Sauvira Kingdom, and finds mentioned early Buddhist literature as a major trading center. Sindh finds mention in the Hindu epic "Mahabharata" as being part of Bharatvarsha. Sindh was conquered by the Persian Achaemenid Empire in the 6th century BC. In the late 4th century BC, Sindh was conquered by a mixed army led by Macedonian Greeks under Alexander the Great. The region remained under control of Greek satraps for only a few decades. After Alexander's death, there was a brief period of Seleucid rule, before Sindh was traded to the Mauryan Empire led by Chandragupta in 305 BC. During the rule of the Mauryan Emperor Ashoka, the Buddhist religion spread to Sindh.

Mauryan rule ended in 185 BC with the overthrow of the last king by the Shunga Dynasty. In the disorder that followed, Greek rule returned when Demetrius I of Bactria led a Greco-Bactrian invasion of India and annexed most of the northwestern lands, including Sindh. Demetrius was later defeated and killed by a usurper, but his descendants continued to rule Sindh and other lands as the Indo-Greek Kingdom. Under the reign of Menander I, many Indo-Greeks followed his example and converted to Buddhism.

In the late 2nd century BC, Scythian tribes shattered the Greco-Bactrian empire and invaded the Indo-Greek lands. Unable to take the Punjab region, they invaded South Asia through Sindh, where they became known as Indo-Scythians (later Western Satraps). By the 1st century AD, the Kushan Empire annexed Sindh. Kushans under Kanishka were great patrons of Buddhism and sponsored many building projects for local beliefs. Ahirs were also found in large numbers in Sindh. Abiria country of Abhira tribe was in southern Sindh.

The Kushan Empire was defeated in the mid 3rd century AD by the Sassanid Empire of Persia, who installed vassals known as the Kushanshahs in these far eastern territories. These rulers were defeated by the Kidarites in the late 4th century.

It then came under the Gupta Empire after dealing with the Kidarites. By the late 5th century, attacks by Hephthalite tribes known as the Indo-Hephthalites or "Hunas" (Huns) broke through the Gupta Empire's northwestern borders and overran much of northwestern India. Concurrently, Ror dynasty ruled parts of the region for several centuries.

Afterwards, Sindh came under the rule of Emperor Harshavardhan, then the Rai Dynasty around 478. The Rais were overthrown by Chachar of Alor around 632. The Brahman dynasty ruled a vast territory that stretched from Multan in the north to the Rann of Kutch, Alor was their capital.

The connection between the Sindh and Islam was established by the initial Muslim missions during the Rashidun Caliphate. Al-Hakim ibn Jabalah al-Abdi, who attacked Makran in the year AD 649, was an early partisan of Ali ibn Abu Talib. During the caliphate of Ali, many Jats of Sindh had come under the influence of Shi'ism and some even participated in the Battle of Camel and died fighting for Ali. Under the Umayyads (661 – 750 AD), many Shias sought asylum in the region of Sindh, to live in relative peace in the remote area. Ziyad Hindi is one of those refugees.

Muhammad Ali Jinnah claimed that the Pakistan movement started when the first Muslim put his foot on the soil of Sindh, the Gateway of Islam in India.

In 712, Muhammad bin Qasim conquered the Sindh and Indus Valley, bringing South Asian societies into contact with Islam. Dahir was an unpopular Hindu king that ruled over a Buddhist majority and that Chach of Alor and his kin were regarded as usurpers of the earlier Buddhist Rai Dynasty, a view questioned by those who note the diffuse and blurred nature of Hindu and Buddhist practices in the region, especially that of the royalty to be patrons of both and those who believe that Chach may have been a Buddhist. The forces of Muhammad bin Qasim defeated Raja Dahir in alliance with the Hindu Jats and other regional governors.

In 711 AD, Muhammad bin Qasim led an Umayyad force of 20,000 cavalry and 5 catapults. Muhammad bin Qasim defeated the Raja Dahir and captured the cities of Alor, Multan and Debal. Sindh became the easternmost State of the Umayyad Caliphate and was referred to as "Sind" on Arab maps, with lands further east known as "Hind". Muhammad bin Qasim built the city of Mansura as his capital; the city then produced famous historical figures such as Abu Mashar Sindhi, Abu Ata al-Sindhi, Abu Raja Sindhi and Sind ibn Ali. At the port city of Debal, most of the Bawarij embraced Islam and became known as Sindhi Sailors, who were renowned for their navigation, geography and languages. After Bin Qasim left, the Umayyads ruled Sindh through the Habbari dynasty.

By the year 750, Debal (modern Karachi) was second only to Basra; Sindhi sailors from the port city of Debal voyaged to Basra, Bushehr, Musqat, Aden, Kilwa, Zanzibar, Sofala, Malabar, Sri Lanka and Java (where Sindhi merchants were known as the Santri). During the power struggle between the Umayyads and the Abbasids. The Habbari Dynasty became semi independent and was eliminated and Mansura was invaded by Sultan Mahmud Ghaznavi. Sindh then became an easternmost State of the Abbasid Caliphate ruled by the Soomro Dynasty until the Siege of Baghdad (1258). Mansura was the first capital of the Soomra Dynasty and the last of the Habbari dynasty. Muslim geographers, historians and travelers such as al-Masudi, Ibn Hawqal, Istakhri, Ahmed ibn Sahl al-Balkhi, al-Tabari, Baladhuri, Nizami, al-Biruni, Saadi Shirazi, Ibn Battutah and Katip Çelebi wrote about or visited the region, sometimes using the name "Sindh" for the entire area from the Arabian Sea to the Hindu Kush.

When Sindh was under the Arab Umayyad Caliphate, the Arab Habbari dynasty was in control. The Umayyads appointed Aziz al Habbari as the governor of Sindh. Habbaris ruled Sindh until Sultan Mahmud Ghaznavi defeated the Habbaris in 1024. Sultan Mahmud Ghaznavi viewed the Abbasid Caliphate to be the caliphs thus he removed the remaining influence of the Umayyad Caliphate in the region and Sindh fell to Abbasid control following the defeat of the Habbaris. The Abbasid Caliphate then appointed Al Khafif from Samarra; 'Soomro' means 'of Samarra' in Sindhi. The new governor of Sindh was to create a better, stronger and stable government. Once he became the governor, he allotted several key positions to his family and friends; thus Al-Khafif or Sardar Khafif Soomro formed the Rajput Soomro Dynasty in Sindh; and became its first ruler. Until the Siege of Baghdad (1258) the Soomro dynasty was the Abbasid Caliphate's functionary in Sindh, but after that it became independent.

When the Soomro dynasty lost ties with the Abbasid Caliphate after the Siege of Baghdad (1258,) the Soomra ruler Dodo-I established their rule from the shores of the Arabian Sea to the Punjab in the north and in the east to Rajasthan and in the west to Pakistani Balochistan. The Soomros were one of the first indigenous Muslim dynasties in Sindh of Parmar Rajput origin. They were the first Muslims to translate the Quran into the Sindhi language. The Sammas created a chivalrous culture in Sindh, which eventually facilitated their rule centred at Mansura. It was later abandoned due to changes in the course of the Puran River; they ruled for the next 95 years until 1351. During this period, Kutch was ruled by the Samma Dynasty, who enjoyed good relations with the Soomras in Sindh. Since the Soomro Dynasty lost its support from the Abbasid Caliphate, the Sultans of Delhi wanted a piece of Sindh. The Soomros successfully defended their kingdom for about 36 years, but their dynasties soon fell to the might of the Sultanate of Delhi's massive armies such as the Tughluks and the Khaljis.

In 1339 Jam Unar founded a Sindhi Muslim Rajput Samma Dynasty and challenged the Sultans of Delhi. He used the title of the "Sultan of Sindh". The Samma tribe reached its peak during the reign of Jam Nizamuddin II (also known by the nickname Jám Nindó). During his reign from 1461 to 1509, Nindó greatly expanded the new capital of Thatta and its Makli hills, which replaced Debal. He patronized Sindhi art, architecture and culture. The Samma had left behind a popular legacy especially in architecture, music and art. Important court figures included the poet Kazi Kadal, Sardar Darya Khan, Moltus Khan, Makhdoom Bilawal and the theologian Kazi Kaadan. However, Thatta was a port city; unlike garrison towns, it could not mobilize large armies against the Arghun and Tarkhan Mongol invaders, who killed many regional Sindhi Mirs and Amirs loyal to the Samma. Some parts of Sindh still remained under the Sultans of Delhi and the ruthless Arghuns and the Tarkhans sacked Thatta during the rule of Jam Ferozudin.

According to Dr. Akhtar Baloch, Professor at University of Karachi, and Nadeem Wagan, General Manager at HANDS, the Balochi migrated from Balochistan during the Little Ice Age and settled in Sindh and Punjab. The Little Ice Age is conventionally defined as a period extending from the sixteenth to the nineteenth centuries, or alternatively, from about 1300 to about 1850. According to Professor Baloch, the climate of Balochistan was very cold during this epoch and the region was uninhabitable during the winters so the Baloch people emigrated in waves to Sindh and Punjab.

In the year 1524, the few remaining Sindhi Amirs welcomed the Mughal Empire and Babur dispatched his forces to rally the Arghuns and the Tarkhans, branches of a Turkic dynasty. In the coming centuries, Sindh became a region loyal to the Mughals, a network of forts manned by cavalry and musketeers further extended Mughal power in Sindh. In 1540 a mutiny by Sher Shah Suri forced the Mughal Emperor Humayun to withdraw to Sindh, where he joined the Sindhi Emir Hussein Umrani. In 1541 Humayun married Hamida Banu Begum, who gave birth to the infant Akbar at Umarkot in the year 1542.

During the reign of Akbar the Great, Sindh produced scholars and others such as Mir Ahmed Nasrallah Thattvi, Tahir Muhammad Thattvi and Mir Ali Sir Thattvi and the Mughal chronicler Abu'l-Fazl ibn Mubarak and his brother the poet Faizi was a descendant of a Sindhi Shaikh family from Rel, Siwistan in Sindh. Abu'l-Fazl ibn Mubarak was the author of "Akbarnama" (an official biographical account of Akbar) and the "Ain-i-Akbari" (a detailed document recording the administration of the Mughal Empire).

Shah Jahan carved a subah (imperial province), covering Sindh, called Thatta after its capital, out of Multan, further bordering on the Ajmer and Gujarat subahs as well as the rival Persian Safavid empire.

During the Mughal period, Sindhi literature began to flourish and historical figures such as Shah Abdul Latif Bhittai, Sulatn-al-Aoliya Muhammad Zaman and Sachal Sarmast became prominent throughout the land. In 1603 Shah Jahan visited the State of Sindh; at Thatta, he was generously welcomed by the locals after the death of his father Jahangir. Shah Jahan ordered the construction of the Shahjahan Mosque, which was completed during the early years of his rule under the supervision of Mirza Ghazi Beg. During his reign, in 1659 in the Mughal Empire, Muhammad Salih Tahtawi of Thatta created a seamless celestial globe with Arabic and Persian inscriptions using a wax casting method.

Sindh was home to several wealthy merchant-rulers such as Mir Bejar of Sindh, whose great wealth had attracted the close ties with the Sultan bin Ahmad of Oman.

In the year 1701, the Kalhora Nawabs were authorized in a firman by the Mughal Emperor Aurangzeb to administer subah Sindh.

From 1752 to 1762, Marathas collected Chauth or tributes from Sindh. Maratha power was decimated in the entire region after the Third Battle of Panipat in 1761. In 1762, Mian Ghulam Shah Kalhoro brought stability in Sindh, he reorganized and independently defeated the Marathas and their prominent vassal the "Rao of Kuch" in the Thar Desert and returned victoriously.

After the Sikhs annexed Multan, the Kalhora Dynasty supported counterattacks against the Sikhs and defined their borders.

In 1783 a firman which designated Mir Fateh Ali Khan Talpur as the new "Nawab of Sindh", and mediated peace particularly after the Battle of Halani and the defeat of the ruling Kalhora by the Talpur Baloch tribes.

According to Nadeem Wagan, General Manager at HANDS, the Talpur tribe migrated from Dera Ghazi Khan in Punjab to Sindh on the invitation of Kalhora to help them organize unruly Baloch tribes living in Sindh. Talpurs, who learned the Sindhi language, settled in northern Sindh. Very soon they united all the Baloch tribes of Sindh and formed a confederacy against the Kalhora Dynasty. The Talpur Baloch soon gained power, overthrowing the Kalhora after the Battle of Halani to conquer and rule Sindh and other parts of present-day Pakistan, from 1783 to 1843.
British East India Company forces led by General Charles James Napier overthrew the Talpur Baloch in 1843.

In 1802, when Mir Ghulam Ali Khan Talpur succeeded as the Talpur Nawab, internal tensions broke out in the state. As a result, the following year the Maratha Empire declared war on Sindh and Berar Subah, during which Arthur Wellesley took a leading role causing much early suspicion between the Emirs of Sindh and the British Empire. The British East India Company made its first contacts in the Sindhi port city of Thatta, which according to a report was:
"a city as large as London containing 50,000 houses which were made of stone and mortar with large verandahs some three or four stories high ... the city has 3,000 looms ... the textiles of Sindh were the flower of the whole produce of the East, the international commerce of Sindh gave it a place among that of Nations, Thatta has 400 schools and 4,000 Dhows at its docks, the city is guarded by well armed Sepoys".

British and Bengal Presidency forces under General Charles James Napier arrived in Sindh in the mid-19th century and conquered Sindh in February 1843. The Baloch coalition led by Talpur under Mir Nasir Khan Talpur was defeated at the Battle of Miani during which 5,000 Talpur Baloch were killed. Shortly afterwards, Hoshu Sheedi commanded another army at the Battle of Dubbo, where 5,000 Baloch were killed.

The first Agha Khan (was escaping persecution from Persia and looking for a foothold in the British Raj) he helped the British in their conquest of Sindh. As a result, he was granted a lifetime pension.

A British journal by Thomas Postans mentions the captive Sindhi Amirs: "The Amirs as being the prisoners of 'Her Majesty'... they are maintained in strict seclusion; they are described as Broken-Hearted and Miserable men, maintaining much of the dignity of fallen greatness, and without any querulous or angry complaining at this unlivable source of sorrow, refusing to be comforted".
Within weeks, Charles Napier and his forces occupied Sindh.

After 1853 the British divided Sindh into districts and later made it part of British India's Bombay Presidency.

In the year 1868, the Bombay Presidency assigned "Narayan Jagannath Vaidya" to replace the Abjad used in Sindhi, with the "Khudabadi script". The script was decreed a standard script by the Bombay Presidency thus inciting anarchy in the Muslim majority region. A powerful unrest followed, after which Twelve Martial Laws were imposed by the British authorities.

The Bombay Presidency caused the rise of rebels such as Sibghatullah Shah Rashidi pioneered the Sindhi Muslim Hur Movement against the British Raj. He was hanged on 20March 1943 in Hyderabad, Sindh. His burial place is not known.

During the British period, railways, printing presses and bridges were introduced in the province. Writers like Mirza Kalich Beg compiled and traced the literary history of Sindh.

Although Sindh had a culture of religious syncretism, communal harmony and tolerance due to Sindh's strong Sufi culture in which both Sindhi Muslims and Sindhi Hindus partook, the mostly Muslim peasantry was oppressed by the Hindu moneylending class and also by the landed Muslim elite. Sindhi Muslims eventually demanded the separation of Sindh from the Bombay Presidency, a move opposed by Sindhi Hindus.

By 1936 Sindh was separated from the Bombay Presidency. Elections in 1937 resulted in local Sindhi Muslim parties winning the bulk of seats. By the mid-1940s the Muslim League gained a foothold in the province and after winning over the support of local Sufi "pirs", it didn't take long for the overwhelming majority of Sindhi Muslims to campaign for the creation of Pakistan.

At the time of Partition, there were 1,400,000 Hindu Sindhis, dominating the province's upper middle class. There was very little communal violence in Sindh, in comparison to Punjab. Communal violence in Ajmer, India, in December 1947 led to Muslim refugees crossing over the Thar Desert to Sindh in Pakistan. This sparked riots in Hyderabad and later in Karachi, although less than 500 Hindu were killed in Sindh between 1947–48 as Sindhi Muslims largely resisted calls to turn against their Hindu neighbours. Hundreds of thousands of Sindhi Hindus fled to India. The arrival of Sindhi Hindu refugees in the Indian town of Godhra sparked the March 1948 anti-Muslim riots there which led to an emigration of Ghanchi Muslims from Godhra to Pakistan. Indian Muslims from the United Provinces, Central Provinces and Bombay continued migrating to and settling in Sindh's urban centers throughout the 1950s and 1960s.

Sindh has the 2nd highest Human Development Index out of all of Pakistan's provinces at 0.628. The 2017 Census of Pakistan indicated a population of 47.9 million.

The major ethnic group of the province is the Sindhis, but there is also a significant presence of other groups. Sindhis of Baloch origin make up about 30% of the total Sindhi population (although they speak Sindhi Saraiki as their native tongue), while Urdu-speaking Muhajirs make up over 19% of the total population of the province, while Punjabi are 10% and Pashtuns represent 7%. In August 1947, before the partition of India, the total population of Sindh was 3,887,070 out of which 2,832,000 were Muslims and 1,015,000 were Hindus

Islam in Sindh has a strong Sufi ethos with numerous Muslim saints and mystics, such as the Sufi poet Shah Abdul Latif Bhittai, having lived in Sindh historically. One popular legend which highlights the strong Sufi presence in Sindh is that 125,000 Sufi saints and mystics are buried on Makli Hill near Thatta. The development of Sufism in Sindh was similar to the development of Sufism in other parts of the Muslim world. In the 16th century two Sufi tareeqat (orders) – Qadria and Naqshbandia – were introduced in Sindh. Sufism continues to play an important role in the daily lives of Sindhis.

Sindh also has Pakistan's highest percentage of Hindu residents, with 7.5% of Sindh's population overall, and 11.56% of Sindh's rural population, classifying itself as Hindu, and a majority of residents in Tharparkar District identifying themselves as Hindu. The communal harmony between Sindhi Muslims and Hindus is an example of Sindh's pluralistic and tolerant Sufi culture.

According to the 2017 census, the most widely spoken language in the province is Sindhi, the first language of % of the population. It is followed by Urdu (%), Pashto (%), Punjabi (%), Saraiki (%) and Balochi (2%).

Other languages with substantial numbers of speakers include Kutchi and Gujarati. Other minority languages include Aer, Bagri, Bhaya, Brahui, Dhatki, Ghera, Goaria, Gurgula, Jadgali, Jandavra, Jogi, Kabutra, Kachi Koli, Parkari Koli, Wadiyari Koli, Loarki, Marwari, Sansi, and Vaghri.

According to the 1998 census, 7.3% of people Karachi's residents are Sindhi-speaking. However, since the last few decades, every year thousands of Sindhi speaking from the rural areas are moving and settling to the Karachi due to which population of the Sindhis is increasing drastically. Karachi is 40% populated by Muhajirs who speak Urdu. Other immigrant communities in Karachi are Pashtuns from Khyber Pakhtunkhwa, Punjabis from Punjab and other linguistic groups from various regions of Pakistan.

Sindh is in the western corner of South Asia, bordering the Iranian plateau in the west. Geographically it is the third largest province of Pakistan, stretching about from north to south and (extreme) or (average) from east to west, with an area of of Pakistani territory. Sindh is bounded by the Thar Desert to the east, the Kirthar Mountains to the west and the Arabian Sea in the south. In the centre is a fertile plain along the Indus River.

The province is mostly arid with scant vegetation except for the irrigated Indus Valley. The dwarf palm, "Acacia Rupestris" (kher), and "Tecomella undulata" (lohirro) trees are typical of the western hill region. In the Indus valley, the "Acacia nilotica" (babul) (babbur) is the most dominant and occurs in thick forests along the Indus banks. The "Azadirachta indica" (neem) (nim), "Zizyphys vulgaris" (bir) (ber), "Tamarix orientalis" (jujuba lai) and "Capparis aphylla" (kirir) are among the more common trees.

Mango, date palms and the more recently introduced banana, guava, orange and chiku are the typical fruit-bearing trees. The coastal strip and the creeks abound in semi-aquatic and aquatic plants and the inshore Indus delta islands have forests of "Avicennia tomentosa" (timmer) and "Ceriops candolleana" (chaunir) trees. Water lilies grow in abundance in the numerous lake and ponds, particularly in the lower Sindh region.

Among the wild animals, the Sindh ibex (sareh), blackbuck, wild sheep (Urial or gadh) and wild bear are found in the western rocky range. The leopard is now rare and the Asiatic cheetah extinct. The Pirrang (large tiger cat or fishing cat) of the eastern desert region is also disappearing. Deer occur in the lower rocky plains and in the eastern region, as do the striped hyena (charakh), jackal, fox, porcupine, common gray mongoose and hedgehog. The Sindhi phekari, red lynx or Caracal cat, is found in some areas. Phartho (hog deer) and wild bear occur, particularly in the central inundation belt. There are bats, lizards and reptiles, including the cobra, lundi (viper) and the mysterious Sindh krait of the Thar region, which is supposed to suck the victim's breath in his sleep.
Some unusual sightings of Asian cheetah occurred in 2003 near the Balochistan border in Kirthar Mountains. The rare houbara bustard find Sindh's warm climate suitable to rest and mate. Unfortunately, it is hunted by locals and foreigners.

Crocodiles are rare and inhabit only the backwaters of the Indus, eastern Nara channel and Karachi backwater. Besides a large variety of marine fish, the plumbeous dolphin, the beaked dolphin, rorqual or blue whale and skates frequent the seas along the Sindh coast. The Pallo (Sable fish), a marine fish, ascends the Indus annually from February to April to spawn. The Indus river dolphin is among the most endangered species in Pakistan and is found in the part of the Indus river in northern Sindh. Hog deer and wild bear occur, particularly in the central inundation belt.

Although Sindh has a semi arid climate, through its coastal and riverine forests, its huge fresh water lakes and mountains and deserts, Sindh supports a large amount of varied wildlife. Due to the semi-arid climate of Sindh the left out forests support an average population of jackals and snakes. The national parks established by the Government of Pakistan in collaboration with many organizations such as World Wide Fund for Nature and Sindh Wildlife Department support a huge variety of animals and birds. The Kirthar National Park in the Kirthar range spreads over more than 3000 km of desert, stunted tree forests and a lake. The KNP supports Sindh ibex, wild sheep (urial) and black bear along with the rare leopard. There are also occasional sightings of The Sindhi phekari, ped lynx or Caracal cat. There is a project to introduce tigers and Asian elephants too in KNP near the huge Hub Dam Lake. Between July and November when the monsoon winds blow onshore from the ocean, giant olive ridley turtles lay their eggs along the seaward side. The turtles are protected species. After the mothers lay and leave them buried under the sands the SWD and WWF officials take the eggs and protect them until they are hatched to keep them from predators.

Sindh lies in a tropical to subtropical region; it is hot in the summer and mild to warm in winter. Temperatures frequently rise above between May and August, and the minimum average temperature of occurs during December and January in the northern and higher elevated regions. The annual rainfall averages about seven inches, falling mainly during July and August. The southwest monsoon wind begins in mid-February and continues until the end of September, whereas the cool northerly wind blows during the winter months from October to January.

Sindh lies between the two monsoons—the southwest monsoon from the Indian Ocean and the northeast or retreating monsoon, deflected towards it by the Himalayan mountains—and escapes the influence of both. The region's scarcity of rainfall is compensated by the inundation of the Indus twice a year, caused by the spring and summer melting of Himalayan snow and by rainfall in the monsoon season.

Sindh is divided into three climatic regions: Siro (the upper region, centred on Jacobabad), Wicholo (the middle region, centred on Hyderabad), and Lar (the lower region, centred on Karachi). The thermal equator passes through upper Sindh, where the air is generally very dry. Central Sindh's temperatures are generally lower than those of upper Sindh but higher than those of lower Sindh. Dry hot days and cool nights are typical during the summer. Central Sindh's maximum temperature typically reaches . Lower Sindh has a damper and humid maritime climate affected by the southwestern winds in summer and northeastern winds in winter, with lower rainfall than Central Sindh. Lower Sindh's maximum temperature reaches about . In the Kirthar range at and higher at Gorakh Hill and other peaks in Dadu District, temperatures near freezing have been recorded and brief snowfall is received in the winters.

The Provincial Assembly of Sindh is a unicameral and consists of 168 seats, of which 5% are reserved for non-Muslims and 17% for women. The provincial capital of Sindh is Karachi. The provincial government is led by Chief Minister who is directly elected by the popular and landslide votes; the Governor serves as a ceremonial representative nominated and appointed by the President of Pakistan. The administrative boss of the province who is in charge of the bureaucracy is the Chief Secretary Sindh, who is appointed by the Prime Minister of Pakistan. Most of the influential Sindhi tribes in the province are involved in Pakistan's politics.

In addition, Sindh's politics leans towards the left-wing and its political culture serves as a dominant place for the left-wing spectrum in the country. The province's trend towards the Pakistan Peoples Party and away from the Pakistan Muslim League (N) can be seen in nationwide general elections, in which, Sindh is a stronghold of the Pakistan Peoples Party (PPP). The PML(N) has a limited support due to its centre-right agenda.

In metropolitan cities such as Karachi and Hyderabad, the MQM (another left-wing party with the support of "Muhajirs") has a considerable vote bank and support. Minor leftist parties such as People's Movement also found support in rural areas of the province.

In 2008, after the public elections, the new government decided to restore the structure of Divisions of all provinces. In Sindh after the lapse of the Local Governments Bodies term in 2010 the Divisional Commissioners system was to be restored.
In July 2011, following excessive violence in the city of Karachi and after the political split between the ruling PPP and the majority party in Sindh, the MQM and after the resignation of the MQM Governor of Sindh, PPP and the Government of Sindh decided to restore the commissionerate system in the province. As a consequence, the five divisions of Sindh were restored – namely Karachi, Hyderabad, Sukkur, Mirpurkhas and Larkana with their respective districts. Subsequently, two new divisions have been added in Sindh, Banbore and Nawab Shah/Shaheed Benazirabad division.

Karachi district has been de-merged into its five original constituent districts: Karachi East, Karachi West, Karachi Central, Karachi South and Malir. Recently Korangi has been upgraded to the status of the sixth district of Karachi. These six districts form the Karachi Division now.

Sindh has the second largest economy in Pakistan. A 2016 study commissioned by Pakistan Ministry of Planning found that urban Sindh and northern Punjab province are the most prosperous regions in Pakistan. Its GDP per capita was $1,400 in 2010 which is 50 percent more than the rest of the nation or 35 percent more than the national average. Historically, Sindh's contribution to Pakistan's GDP has been between 30% to 32.7%. Its share in the service sector has ranged from 21% to 27.8% and in the agriculture sector from 21.4% to 27.7%. Performance wise, its best sector is the manufacturing sector, where its share has ranged from 36.7% to 46.5%.

Endowed with coastal access, Sindh is a major centre of economic activity in Pakistan and has a highly diversified economy ranging from heavy industry and finance centred in Karachi to a substantial agricultural base along the Indus. Manufacturing includes machine products, cement, plastics, and other goods.

Agriculture is very important in Sindh with cotton, rice, wheat, sugar cane, dates, bananas, and mangoes as the most important crops. The largest and finer quality of rice is produced in Larkano district.

The following is a chart of the education market of Sindh estimated by the government in 1998:
Major public and private educational institutes in Sindh include:

The rich culture, art and architectural landscape of Sindh have fascinated historians. The culture, folktales, art and music of Sindh form a mosaic of human history.

Sindh has a rich heritage of traditional handicraft that has evolved over the centuries. Perhaps the most professed exposition of Sindhi culture is in the handicrafts of Hala, a town some 30 kilometres from Hyderabad. Hala's artisans manufacture high-quality and impressively priced wooden handicrafts, textiles, paintings, handmade paper products, and blue pottery. Lacquered wood works known as Jandi, painting on wood, tiles, and pottery known as Kashi, hand weaved textiles including "khadi", "susi", and "ajraks" are synonymous with Sindhi culture preserved in Hala's handicraft.

The work of Sindhi artisans was sold in ancient markets of Damascus, Baghdad, Basra, Istanbul, Cairo and Samarkand. Referring to the lacquer work on wood locally known as Jandi, T. Posten (an English traveller who visited Sindh in the early 19th century) asserted that the articles of Hala could be compared with exquisite specimens of China. Technological improvements such as the spinning wheel (charkha) and treadle (pai-chah) in the weaver's loom were gradually introduced and the processes of designing, dyeing and printing by block were refined. The refined, lightweight, colourful, washable fabrics from Hala became a luxury for people used to the woollens and linens of the age.

Non-governmental organisations (NGOs) such as the World Wildlife Fund, Pakistan, play an important role to promote the culture of Sindh. They provide training to women artisans in the interior of Sindh so they get a source of income. They promote their products under the name of "Crafts Forever". Many women in rural Sindh are skilled in the production of caps. Sindhi caps are manufactured commercially on a small scale at New Saeedabad and Hala New. These are in demand with visitors from Karachi and other places; however, these manufacturing units have a limited production capacity. Sindhi people began celebrating Sindhi Topi Day on December 6, 2009, to preserve the historical culture of Sindh by wearing Ajrak and Sindhi topi.

Tourist sites include the ruins of Mohenjo-daro near the city of Larkana, Runi Kot, Kot Deji, the Jain temples of Nangar Parker and the historic temple of Sadhu Bela, Sukkur. Islamic architecture is quite prominent in the province; its numerous mausoleums include the ancient Shahbaz Qalander mausoleum.





</doc>
<doc id="28975" url="https://en.wikipedia.org/wiki?curid=28975" title="Super Bowl III">
Super Bowl III

Super Bowl III was the third AFL–NFL Championship Game in professional American football, and the first to officially bear the trademark name "Super Bowl". Played on January 12, 1969, at the Orange Bowl in Miami, Florida, the game is regarded as one of the greatest upsets in both American football history and in the recorded history of sports. The 18-point underdog American Football League (AFL) champion New York Jets defeated the National Football League (NFL) champion Baltimore Colts by a score of 16–7.

This was the first Super Bowl victory for the AFL. Before the game, most sports writers and fans believed that AFL teams were less talented than NFL clubs, and expected the Colts to defeat the Jets by a wide margin. Baltimore posted a 13–1 record in the regular season and shut out the Cleveland Browns 34–0 in the NFL Championship Game. The Jets were 11–3 in the regular season, and defeated the Oakland Raiders 27–23 in the AFL Championship Game.

Jets quarterback Joe Namath famously made an appearance three days before the Super Bowl at the Miami Touchdown Club and personally guaranteed his team's victory. His team backed up his words by controlling most of the game, building a 16–0 lead by the fourth quarter off of a touchdown run by Matt Snell and three field goals by Jim Turner. Colts quarterback Earl Morrall threw three interceptions before being replaced by Johnny Unitas, who then led Baltimore to its only touchdown, during the last few minutes of the game. With the victory, the Jets were the only winning team to score only one touchdown (either offensive, defensive, or special teams) until the New England Patriots in Super Bowl LIII. Namath, who completed 17 out of 28 passes for 206 yards, was named as the Super Bowl's most valuable player, making him the first player in Super Bowl history to be declared MVP without personally scoring or throwing for a touchdown.

The game was awarded to Miami on May 14, 1968, at the owners meetings held in Atlanta. 

The National Football League (NFL) had dominated professional football from its origins after World War I. Rival leagues had crumbled or merged with it, and when the American Football League (AFL) began to play in 1960, it was the fourth to hold that similar name to challenge the older NFL. Unlike its earlier namesakes, however, this AFL was able to command sufficient financial resources to survive; one factor in this was becoming the first league to sign a television contract—previously, individual franchises had signed agreements with networks to televise games. The junior league proved successful enough, in fact, to make attractive offers to players. After the 1964 season, in fact, there had been a well-publicized bidding war which culminated with the signing, by the AFL's New York Jets (formerly New York Titans), of Alabama quarterback Joe Namath for an unprecedented contract. Fearing that bidding wars over players would become the norm, greatly increasing labor costs, NFL owners, ostensibly led by league Commissioner Pete Rozelle, obtained a merger agreement with the AFL in June 1966, which provided for a common draft, interleague play in the pre-season, a world championship game to follow each season, and the integration of the two leagues into one in a way to be agreed at a future date. As the two leagues had an unequal number of teams (under the new merger agreement, the NFL expanded to sixteen in , and the AFL to ten in 1968), realignment was advocated by some owners, but was opposed. Eventually, three NFL teams (Cleveland Browns, Pittsburgh Steelers, and the Baltimore Colts) agreed to move over to join the ten AFL franchises in the American Football Conference.

Despite the ongoing merger, it was a commonly held view that the NFL was a far superior league. This was seemingly confirmed by the results of the first two interleague championship games, in January 1967 and 1968, in which the NFL champion Green Bay Packers, coached by the legendary Vince Lombardi, easily defeated the AFL's Kansas City Chiefs and Oakland Raiders. Although publicized as the inter-league championship games, it was not until later that the moniker for this championship contest between the now two conferences (National and American) began having the nickname of "Super Bowl" applied to it by the media and later began being counted by using Roman numerals, the creation of the term being credited to the founder of the AFL, Lamar Hunt.

The Baltimore Colts had won the 1958 and 1959 NFL championships under Coach Weeb Ewbank. In the following years, however, the Colts failed to make the playoffs, and the Colts dismissed Ewbank after a 7–7 record in 1962. He was soon hired by New York's new AFL franchise, which had just changed its name from the Titans to the Jets. In Ewbank's place, Baltimore hired an untested young head coach, Don Shula, who would also go on to become one of the game's greatest coaches. The Colts did well under Shula, despite losing to the Cleveland Browns in the 1964 NFL Championship Game and, in 1965, losing in overtime to the Green Bay Packers in a tie-breaker game to decide the NFL Western Conference title. The Colts finished a distant second in the West to the Packers in 1966, and in 1967, with the NFL realigned into four divisions of four teams each, went undefeated with two ties through their first 13 games, but lost the game and the Coastal Division championship to the Los Angeles Rams on the final Sunday of the season—under newly instituted tiebreakers procedures, L.A. won the division championship as it had better net points in the two games the teams played (the Rams win and an earlier tie). The Colts finished 11–1–2, tied for the best record in the league, but were excluded from the playoffs. In 1968, Shula and the Colts were considered a favorite to win the NFL championship again, which carried with it an automatic berth what was now becoming popularly known as the "Super Bowl" against the champion of the younger AFL. The NFL champion, in both cases the Green Bay Packers, had easily won the first two Super Bowls (1967 and 1968) over the AFL winner, establishing for a while then the superiority of the older NFL circuit.

Baltimore's quest for a championship seemed doomed from the start when long-time starting quarterback Johnny Unitas suffered a pre-season injury to his throwing arm and was replaced by Earl Morrall, a veteran who had started inconsistently over the course of his 12 seasons with four teams. But Morrall would go on to have the best year of his career, leading the league in passer rating (93.2) during the regular season. His performance was so impressive that Colts coach Don Shula decided to keep Morrall in the starting lineup after Unitas was healthy enough to play. The Colts had won ten games in a row, including four shutouts, and finished the season with an NFL-best 13–1 record. In those ten games, they had allowed only seven touchdowns. Then, the Colts avenged their sole regular-season loss against the Cleveland Browns by crushing them 34–0 in the NFL Championship Game.

The Colts offense ranked second in the NFL in points scored (402). Wide receivers Jimmy Orr (29 receptions, 743 yards, 6 touchdowns) and Willie Richardson (37 receptions, 698 yards, 8 touchdowns) provided Baltimore with two deep threats, with Orr averaging 25.6 yards per catch, and Richardson averaging 18.9. Tight end John Mackey also recorded 45 receptions for 644 yards and 5 touchdowns. Pro Bowl running back Tom Matte was the team's top rusher with 662 yards and 9 touchdowns. He also caught 25 passes for 275 yards and another touchdown. Running backs Terry Cole and Jerry Hill combined for 778 rushing yards and 236 receiving yards.

The Colts defense led the NFL in fewest points allowed (144, tying the then all-time league record), and ranked third in total rushing yards allowed (1,339). Bubba Smith, a 6'7" 295-pound defensive end considered the NFL's best pass rusher, anchored the line. Linebacker Mike Curtis was considered one of the top linebackers in the NFL. Baltimore's secondary consisted of defensive backs Bobby Boyd (8 interceptions), Rick Volk (6 interceptions), Lenny Lyles (5 interceptions), and Jerry Logan (3 interceptions). The Colts were the only NFL team to routinely play a zone defense. That gave them an advantage in the NFL because the other NFL teams were inexperienced against a zone defense. (This would not give them an advantage over the upstart New York Jets, however, because zone defenses were common in the AFL and the Jets knew how to attack them.)

The New York Jets, led by head coach Weeb Ewbank (who was the head coach of the Colts when they won the famous 1958 NFL Championship game and later the '59 title also), finished the season with an 11–3 regular season record (one of the losses was to the Oakland Raiders in the infamous "Heidi Game") and had to rally to defeat those same Raiders, 27–23, in a thrilling AFL Championship Game.

Jets quarterback Joe Namath threw for 3,147 yards during the regular season, but completed just 49.2 percent of his passes, and threw more interceptions (17) than touchdowns (15). Still, he led the offense effectively enough for them to finish the regular season with more total points scored (419) than Baltimore, and finished fourth in completion percentage, fifth in touchdown passes, and third in passing yards as one of only three quarterbacks to pass for over 3,000 yards in the AFL that season. More importantly, Namath usually found ways to win. For example, late in the fourth quarter of the AFL championship game, Namath threw an interception that allowed the Raiders to take the lead. But he then made up for his mistake by completing 3 consecutive passes on the ensuing drive, advancing the ball 68 yards in just 55 seconds to score a touchdown to regain the lead for New York. Future Hall of Fame wide receiver Don Maynard caught the game-winning pass in the end zone but strained his hamstring on the play.

The Jets had a number of offensive weapons that Namath used. Maynard had the best season of his career, catching 57 passes for 1,297 yards (an average of 22.8 yards per catch) and 10 touchdowns. Wide receiver George Sauer Jr. recorded 66 receptions for 1,141 yards and 3 touchdowns. The Jets rushing attack was also effective. Fullback Matt Snell, a power runner, was the top rusher on the team with 747 yards and 6 touchdowns, while elusive halfback Emerson Boozer contributed 441 yards and 5 touchdowns. Meanwhile, kicker Jim Turner made 34 field goals and 43 extra points for a combined total of 145 points.

The Jets defense led the AFL in total rushing yards allowed (1,195). Gerry Philbin, Paul Rochester, John Elliott, and Verlon Biggs anchored the defensive line. The Jets linebacking core was led by middle linebacker Al Atkinson. The secondary was led by defensive backs Johnny Sample (a former Colt who played on their 1958 NFL Championship team) who recorded 7 interceptions, and Jim Hudson, who recorded 5.

Several of the Jets' players had been cut by NFL teams. Maynard had been cut by the New York Giants after they lost the 1958 NFL Championship Game to the Colts. "I kept a little bitterness in me," he says. Sample had been cut by the Colts. "I was almost in a frenzy by the time the game arrived," he says. "I held a private grudge against the Colts. I was really ready for that game. All of us were." Offensive tackle Winston Hill had been cut five years earlier by the Colts as a rookie in training camp. "Ordell Braase kept making me look bad in practice," he says. Hill would be blocking Braase in Super Bowl III.

At an all-night party to celebrate the Jets victory over the Raiders at Namath's nightclub, Bachelors III, Namath poured champagne over Johnny Carson as the talk show host commented, "First time I ever knew you to waste the stuff."

The Colts advanced to the Super Bowl with two dominating wins. First, they jumped to a 21–0 fourth quarter lead against the Minnesota Vikings and easily held off their meager comeback attempt in the final period for a 24–14 win.

Then they faced the Cleveland Browns, who had defeated them in week 5 of the regular season. But in this game, they proved to be no challenge as Baltimore held them to just 173 total yards and only allowed them to cross midfield twice in the entire game. Matte scored three of the Colts four rushing touchdowns as the team won easily, 34-0.

Meanwhile, New York in the AFL championship game faced a red hot Oakland Raiders team who had just defeated the Kansas City Chiefs 41–6 one week earlier, with quarterback Daryle Lamonica throwing five touchdown passes. The championship game was close and hard fought the whole way through, with both teams trading scores at a relatively even pace. The momentum seemed to swing in the Raiders' favor when George Atkinson picked off a pass from Namath and returned it 32 yards to the Jets 5-yard line, setting up a touchdown that gave Oakland their first lead of the game at 23–20 with 8:18 left in regulation. But Namath quickly led the team back, completing a 10-yard pass to Sauer and a 52-yard pass to Maynard on the Raiders' six-yard line. On the next play, his six-yard touchdown pass to Maynard gave them a 27–23 lead they would never relinquish. Oakland's final three possessions of the game would result in a turnover on downs, a lost fumble, and time expiring in the game.

After the Jets' AFL championship victory, Namath stated to "The New York Times" sportswriter Dave Anderson, "There are five quarterbacks in the AFL who are better than Morrall." The five were himself, his 38-year-old backup Babe Parilli, Lamonica, John Hadl of the San Diego Chargers, and Bob Griese of the Miami Dolphins. Namath added, "You put Babe Parilli with Baltimore instead of Morrall and Baltimore might be better. Babe throws better than Morrall."

Despite the Jets' accomplishments, AFL teams were generally not regarded as having the same caliber of talent as NFL teams. However, three days before the game, Namath appeared at the Miami Touchdown Club and boldly predicted to the audience, "We're gonna win the game. I guarantee it". Coach Ewbank later joked that he "could have shot" Namath for the statement. Namath made his famous "guarantee" in response to a rowdy Colts supporter at the club, who boasted the Colts would easily defeat the Jets. Namath said he never intended to make such a public prediction, and would not have done so if he had not been confronted by the fan. Sportswriter Dave Anderson did not think that the remark was notable because, he recalled, Namath had said similar things during the week ("I know we're gonna win" for example), but an article by Luther Evans of the "Miami Herald" made the statement famous. Namath's comments and subsequent performance in the game itself are one of the more famous instances in NFL lore.
The Colts, linebacker Curtis recalled, "sort of laughed at" Namath's guarantee. The team did not adjust the defense it had used during the season against the Jets because "that should be good enough," Curtis said. The AFL champions shared the confident feelings of their quarterback. According to Matt Snell, all of the Jets, not just Namath, were insulted and angry that they were 18-point underdogs. Most of the Jets considered the Raiders, whom they barely beat (27–23) in the AFL title game, a better team than the Colts. Indeed, watching films of the Colts and in preparation for the game, Jets coaching staff and offensive players noted that their offense was particularly suited against the Colts defense. The Colts defensive schemes relied on frequent blitzing, which covered up weak points in pass coverage. The Jets had an automatic contingency for such blitzes by short passing to uncovered tight ends or backs. After a film session the Wednesday prior to the game, Jets tight end Pete Lammons was heard to drawl, "Damn, y'all, we gotta stop watching these films. We gonna get overconfident".

The game was broadcast in the United States by NBC Sports – at the time, still a "Service of NBC News" – with Curt Gowdy handling the play-by-play duties and joined by color commentators Al DeRogatis and Kyle Rote in the broadcast booth. Also helping with NBC's coverage were Jim Simpson (reporting from the sidelines) and Pat Summerall, on loan from CBS (helping conduct player interviews for the pregame show, along with Rote). In an interview later done with NFL Films, Gowdy called it the most memorable game he ever called because of its historical significance.

While the Orange Bowl was sold out for the game, the live telecast was not shown in Miami due to both leagues' unconditional blackout rules at the time.

This game is thought to be the earliest surviving Super Bowl game preserved on videotape in its entirety, save for a portion of the Colts' fourth quarter scoring drive. The original NBC broadcast was aired as part of the NFL Network "Super Bowl Classics" series.

"Mr. Football" was the title of the pregame show, which featured marching bands playing "Mr. Touchdown U.S.A." as people in walking footballs representing all NFL and AFL teams except the Jets and Colts were paraded, after which performers representing a Jets player and a Colts player appeared on top of a large, multi-layered, smoke topped cake. Astronauts of the Apollo 8 mission (Frank Borman, Jim Lovell, and William Anders), the first manned flight around the Moon, which had returned to Earth just 18 days prior to the game, then led the Pledge of Allegiance. Lloyd Geisler, first trumpeter of the Washington National Symphony Orchestra, performed the national anthem. The Florida A&M University band was featured during the "America Thanks" halftime show.

New York entered the game with their primary deep threat, wide receiver Don Maynard, playing with a pulled hamstring. But his 112-yard, two touchdown performance against the Oakland Raiders in the AFL championship game made the Colts defense pay special attention to him, not realizing he was injured. Using Maynard as a decoy—he had no receptions in the game—Joe Namath was able to take advantage of single coverage on wide receiver George Sauer Jr.. (After studying the Colts' zone defense, Ewbank had told his receivers, "Find the dead spots in the zone, hook up, and Joe will hit you.") The Jets had a conservative game plan, emphasizing the run as well as short high-percentage passes to minimize interceptions. Meanwhile, with the help of many fortunate plays, the Jets defense kept the Colts offense from scoring for most of the game. Also, Baltimore had a distinctly older group of players with 10+ years experience (Braase, Shinnick, Lyles, Boyd) on their defense's right side versus New York's younger, bigger left offensive side (Hill, Talamini, Schmitt, Sauer)--and back Snell when running left behind left tackle Hill, who thoroughly defeated defensive end Braase.

Namath recalled that he did not become "dead serious" until, on the sideline before the game, he saw Unitas. The Jets, led by captains Namath and Johnny Sample, and Colts, led by captains Preston Pearson, Unitas, and Lyles, met at midfield where referee Tom Bell announced that the Jets had won the coin toss and had elected to receive the football. The coin toss had been conducted an hour prior to kickoff but this was done for the benefit of the spectators. Colts kicker Lou Michaels kicked the ball off to Earl Christy who returned the ball 25 yards to the Jets' 23-yard line. Namath handed the ball off to Snell on first down who carried it 3 yards. On second down, Snell carried the ball for 9 yards, earning the Jets their first first down of the game. Colts' free safety Rick Volk sustained a concussion when he tackled Snell and was subsequently lost for the game. On the ensuing play, Emerson Boozer lost four yards when he was tackled behind the line of scrimmage by Don Shinnick. Namath threw his first pass to Snell that gained 9 yards on 2nd and 14, but a 2-yard loss by Snell on the following play forced the Jets to punt the ball. The Jets noticed, however, from watching film the predictability of the Colts' defense based on how players lined up. Instead of calling plays in the huddle, Namath usually gave formations to his team and operated from the line of scrimmage. Center John Schmitt recalled that the Colts were "in shock" and "it drove them crazy ... no matter what [the Colts] did, [Snell] would run it the other way".
The Colts began their first offensive series on their own 27-yard line. Quarterback Earl Morrall completed a 19-yard pass to tight end John Mackey and then running back Tom Matte ran for 10 yards to place the ball on the Jets' 44-yard line. Jerry Hill's runs of 7 and 5 yards picked up another Colts first down, then Morrall's pass to tight end Tom Mitchell gained 15 yards on third and thirteen and saw the ball placed at the Jets' 19-yard line. In scoring position, Morrall attempted to score quickly against a reeling Jets defense. Receiver Willie Richardson dropped Morrall's pass on first down followed by an incompletion on second down after Mitchell was overthrown. On third down, none of his receivers were open and Morrall was tackled at the line of scrimmage by Al Atkinson. Michaels was brought out to attempt a 27-yard field goal, but it was wide left. "You could almost feel the steam go out of them", said Snell.

The Jets did not only rely on Snell; Namath said "if they're going to blitz, then we're going to throw". Shula said that Namath "beat our blitz" with his fast release, which let him quickly dump the football off to a receiver. On the Jets' second possession, Namath threw deep to Maynard, who, despite his pulled hamstring, was open by a step. The ball was overthrown, but this one play helped change the outcome of the game. Fearing the speedy Maynard, the Colts decided to rotate their zone defense to help cover Maynard, leaving Sauer covered one-on-one by Lenny Lyles, helping Sauer catch 8 passes for 133 yards, including a crucial third quarter 39-yard reception that kept a scoring drive alive. The Jets kept rushing Snell to their strong left, rushing off tackle with Boozer blocking the linebacker, and gained first down after first down as the Colts defense gave ground. The Colts defense was more concerned about Maynard, the passing game, and the deep threat of a Namath to Maynard touchdown. Although the Colts were unaware of Maynard's injury, the Jets were aware that Lyles had been weakened by tonsillitis all week, causing them great glee when they saw the one-on-one matchup with Sauer.

With less than two minutes left in the period, Colts punter David Lee booted a 51-yard kick that pinned the Jets back at their own 4-yard line. Three plays later, Sauer caught a 3-yard pass from Namath, but fumbled while being tackled by Lyles, and Baltimore linebacker Ron Porter recovered it at New York's 12-yard line.

However, on third down (the second play of the second quarter), Morrall's pass was tipped by Jets linebacker Al Atkinson, bounced crazily, high into the air off tight end Tom Mitchell, and was intercepted by Jets cornerback Randy Beverly in the end zone for a touchback. "That was the game in a nutshell," says Matte. Starting from their own 20-yard line, Snell rushed on the next 4 plays, advancing the ball 26 yards. The Jets would have success all day running off left tackle behind the blocking of Winston Hill, who, according to Snell, was overpowering 36-year-old defensive end Ordell Braase, the man who had tormented the rookie Hill in Colts' training camp. Said Snell, "Braase pretty much faded out." Namath later completed 3 consecutive passes, moving the ball to the Colts 23-yard line. Boozer gained just 2 yards on the next play, but Snell followed it up with a 12-yard reception at the 9-yard line and a 5-yard run to the 4-yard line, and capped the drive with a 4-yard touchdown run, once again off left tackle. The score gave the Jets a 7–0 lead, and marked the first time in history that an AFL team led in the Super Bowl.

On Baltimore's ensuing drive, a 30-yard completion from Morrall to running back Tom Matte helped the Colts advance to the New York 42-yard line, but they once again failed to score as Jets cornerback Johnny Sample broke up Morrall's third down pass and Michaels missed his second field goal attempt, this time from 46 yards. Two plays after the Jets took over following the missed field goal, Namath's 36-yard completion to Sauer enabled New York to eventually reach the Baltimore 32-yard line. But Namath then threw two incompletions, and was sacked on third down by Colts linebacker Dennis Gaubatz for a 2-yard loss. New York kicker Jim Turner tried to salvage the drive with a 41-yard field goal attempt, but he missed.

On their next possession, Baltimore went from their own 20-yard line to New York's 15-yard line in three plays, aided by Matte's 58-yard run. However, with 2 minutes left in the half, Morrall was intercepted again, by Sample at the Jets' 2-yard line, deflating the Colts considerably. The Jets then were forced to punt on their ensuing drive, and the Colts advanced the ball to New York's 41-yard line. What followed is one of the most famous plays in Super Bowl history. Baltimore tried a flea flicker play, which had a huge impact on the momentum of the game. Matte ran off right tackle after taking a handoff, then pitched the ball back to Morrall. The play completely fooled the NBC Camera Crew, and the Jets defense, leaving receiver Jimmy Orr wide open near the end zone. However, Morrall failed to spot him and instead threw a pass intended for running back Jerry Hill that was intercepted by Jets safety Jim Hudson as time expired, maintaining the Jets' 7–0 lead at halftime. Earlier in the season, against the Atlanta Falcons, on the same play, Morrall had completed the same pass for a touchdown to Orr, the play's intended target. "I was the primary receiver," Orr said later. "Earl said he just didn't see me. I was open from here to Tampa." "I'm just a lineman, but I looked up and saw Jimmy open," added center Bill Curry. "I don't know what happened." Some speculated that Morrall couldn't see Orr because the Florida A&M marching band (in blue uniforms similar to the Colts) was gathering behind the end zone for the halftime show.

The third quarter belonged to the Jets, who controlled the ball for all but three minutes of the period. Baltimore ran only seven offensive plays all quarter, gaining only 11 yards. Matte lost a fumble on the first play from scrimmage in the second half, yet another demoralizing event, which was recovered by linebacker Ralph Baker on the Colts 33-yard line, leading to Turner's 32-yard field goal to increase the Jets' lead, 10–0. Then after forcing the Colts to punt again, Namath completed 4 passes for 40 yards to set up Turner's 30-yard field goal to increase the lead, 13–0. On that drive, Namath temporarily went out of the game after injuring his right thumb, and was replaced by backup quarterback Babe Parilli for a few plays. Namath returned by the end of the third quarter, but the Jets would not run a pass play for the entire fourth quarter.

Matt Snell said, "By this time, the Colts were pressing. You saw the frustration and worry on all their faces." After Turner's second field goal, with 4 minutes left in the third quarter, Colts head coach Don Shula took Morrall out of the game and put in the sore-armed Johnny Unitas to see if he could provide a spark to Baltimore's offense. Unitas could not get the Colts offense moving on their next drive and they were forced to punt again after 3 plays.

Aided by a 39-yard pass from Namath to Sauer, the Jets drove all the way to the Colts 2-yard line. Baltimore's defense would not quit, and kept them out of the end zone. Turner kicked his third field goal early in the final period to make the score 16–0.

The Colts' inability to score made Namath so confident by the fourth quarter, that he told Ewbank that he preferred to run out the clock instead of playing aggressively. Namath did not throw any passes in the quarter. On Baltimore's next possession, they managed to drive all the way to the Jets' 25-yard line. However, Beverly ended the drive by intercepting a pass from Unitas in the end zone, the Jets' fourth interception of the game. New York then drove to the Colts 35-yard line with seven consecutive running plays, but ended up with no points after Turner missed a 42-yard field goal attempt.

Unitas started out the next drive with three incomplete passes, but completed a key 17-yard pass to Orr on fourth down. Ten plays later, aided by three Jets penalties, Baltimore finally scored a touchdown on a 1-yard run by Hill to cut their deficit to 16–7, but with only 3:19 left in the game. The Colts then recovered an onside kick and drove to the Jets 19-yard line with 3 consecutive completions by Unitas, but his next 3 passes fell incomplete. Instead of kicking a field goal and attempting another onside kick (which would have been necessary in the end), they opted to throw on 4th down, and the pass fell incomplete, turning the ball over on downs. That ended any chance of a Baltimore comeback, as the Jets ran the ball for six plays before being forced to punt.
When the Colts got the ball back, only 8 seconds remained in the game. The Colts then attempted two more passes before the game ended. Matt Snell said, "Leaving the field, I saw the Colts were exhausted and in a state of shock. I don't remember any Colt coming over to congratulate me". As he ran off the field, Namath, in a spontaneous show of defiance held up his index finger, signaling "number one"; "the only time I ever did that in my life", he said.

Namath finished the game having completed 17 of his 28 passes. He is the only quarterback to win Super Bowl MVP without throwing a touchdown pass. Snell rushed for 121 yards on 30 carries with a touchdown, and caught 4 passes for 40 yards. Sauer caught eight passes for 133 yards. Beverly became the first player in Super Bowl history to record two interceptions. Morrall had a terrible game—just 6 of 17 completions for 71 yards, with 3 interceptions. Through 51 games, he had the third worst passer rating in Super Bowl history, with a 9.3, one of only 3 ratings below 10. Despite not being put into the game until late in the third quarter, Unitas finished with more pass completions (11) and passing yards (110) than Morrall, but he also threw one interception. Matte was the Colts' top rusher with 116 yards on just 11 carries, an average of 10.5 yards per run, and caught 2 passes for 30 yards. The Colts were minus-4 in turnovers throwing four interceptions, all of which were deep in Jet territory.

When Sal Marchiano asked Namath in the locker room if he was the "king of the hill", Namath replied "No, no, we're king of the hill. We got the team, brother". Morrall later said, "I thought we would win handily. We'd only lost twice in our last 30 games. I'm still not sure what happened that day at the Orange Bowl, however; it's still hard to account for." Snell wrote, "The most distinct image I have from that whole game is of Ordell Braase and some other guys—not so much Mike Curtis--having a bewildered look".

Sources: NFL.com Super Bowl III, Super Bowl III Play Finder NYJ, Super Bowl III Play Finder Bal

Completions/Attempts
Carries
Long gain
Receptions
Times targeted

The following records were set or tied in Super Bowl III, according to the official NFL.com boxscore and the Pro-Football-Reference.com game summary. Some records have to meet NFL minimum number of attempts to be recognized. The minimums are shown (in parenthesis).


Turnovers are defined as the number of times losing the ball on interceptions and fumbles.

Source:


Unlike the first two Super Bowls, officials wore their standard uniform. The AFL switched to the NFL uniform for 1968 in anticipation of the 1970 merger.

Jack Reader became the first official to work two Super Bowls. He was the only official to work two prior to the merger. He was promoted to referee in 1969.

The following season, 1969, would be the last one before the AFL-NFL merger. The AFL's Kansas City Chiefs would go on to defeat the NFL's Minnesota Vikings in Super Bowl IV. That victory by the AFL squared the Super Bowl series with the NFL at two games apiece before the two leagues merged into one.

As part of the merger, the Colts were one of three NFL teams that moved to the newly formed American Football Conference (AFC) with the Jets and the other AFL teams (the other two were the Cleveland Browns and Pittsburgh Steelers, who were first division rivals in the AFC Central and later the AFC North, with a three-year gap that resulted from the Browns' controversial relocation to Baltimore interrupting it). The former Super Bowl III combatants became divisional rivals in the AFC East until the 2002 realignment shifted the Colts, who had moved to Indianapolis in 1984, to the new AFC South. The teams would however not meet in the playoffs until the 2002 season. And being in the same conference, they can no longer meet in a Super Bowl rematch unless the NFL radically changes its conference alignment or its playoff structure.

The Jets have never gone back to the Super Bowl since the merger, only reaching as far as the AFC Championship Game in the 1982, 1998, 2009 and 2010 seasons. On the other hand, the Colts won Super Bowl V (1970), then after relocating to Indianapolis they won Super Bowl XLI (2006) and lost Super Bowl XLIV (2009).

However, teams representing Baltimore and New York have contested one Super Bowl since the merger: Super Bowl XXXV between the Jets' crosstown rival (the Giants) and Baltimore's replacement team (the Ravens), with the latter contest being won by Baltimore.

This was the first of three occasions in which a team from New York defeated one from Baltimore in postseason play during 1969, with the Knicks eliminating the Bullets in the NBA playoffs, and the Mets upsetting the heavily-favored Orioles in the World Series, being the other two.

This was the last postseason victory for the Jets until they beat the Cincinnati Bengals in the 1982–83 playoffs.




</doc>
<doc id="28976" url="https://en.wikipedia.org/wiki?curid=28976" title="Super Bowl XX">
Super Bowl XX

Super Bowl XX was an American football game between the National Football Conference (NFC) champion Chicago Bears and the American Football Conference (AFC) champion New England Patriots to decide the National Football League (NFL) champion for the 1985 season. The Bears defeated the Patriots by the score of 46–10, capturing their first NFL championship since 1963, three years prior to the birth of the Super Bowl. Super Bowl XX was played on January 26, 1986 at the Louisiana Superdome in New Orleans.

This was the fourth Super Bowl and, to date, the last time in which both teams made their Super Bowl debuts. The Bears entered the game after becoming the second team in NFL history to win 15 regular season games. With their then-revolutionary 46 defense, Chicago led the league in several defensive categories, outscored their opponents with a staggering margin of 456–198, and recorded two postseason shutouts. The Patriots were considered a Cinderella team during the 1985 season, and posted an 11–5 regular season record, but entered the playoffs as a wild card because of tiebreakers. But defying the odds, New England posted three road playoff wins to advance to Super Bowl XX.

In their victory over the Patriots, the Bears set or tied Super Bowl records for sacks (seven), fewest rushing yards allowed (seven), and margin of victory (36 points). At the time, New England broke the record for the quickest lead in Super Bowl history, with Tony Franklin's 36-yard field goal 1:19 into the first quarter after a Chicago fumble. But the Patriots were eventually held to negative yardage (−19) throughout the entire first half, and finished with just 123 total yards from scrimmage, the second lowest total yards in Super Bowl history, behind the Minnesota Vikings (119 total yards) in Super Bowl IX. Bears defensive end Richard Dent, who had 1.5 quarterback sacks, forced two fumbles, and blocked a pass, was named the game's Most Valuable Player (MVP).

The telecast of the game on NBC was watched by an estimated 92.57 million viewers. To commemorate the 20th Super Bowl, all previous Super Bowl MVPs were honored during the pregame ceremonies.

NFL owners awarded the hosting of Super Bowl XX to New Orleans, Louisiana on December 14, 1982, at an owners meeting held in Dallas. This was the sixth time that New Orleans hosted the Super Bowl. Tulane Stadium was the site of Super Bowls IV, VI, and IX; while the Louisiana Superdome previously hosted XII and XV.

As of 2019, Super Bowl XX remains the last Super Bowl to feature two teams both making their first appearance in the game. It was the fourth overall following Super Bowl I, Super Bowl III, and Super Bowl XVI. Absent further expansion of the NFL, any future Super Bowl that would have such a combination would have to have the Detroit Lions playing either the Cleveland Browns, Houston Texans, or Jacksonville Jaguars in the game. All 16 NFC teams have played in an NFL championship game (Detroit last made an NFL championship game in the pre-merger era); only the three AFC franchises that began play since 1995 (the technicalities of the Browns franchise relocating means this version began in 1999) have yet to reach a league championship game.

The nation's recognition of the Bears' accomplishment was overshadowed by STS 51-L two days later, an event which caused the cancellation of the Bears' post-Super Bowl White House visit. Jim McMahon drew controversy after Super Bowl XXXI by wearing a Bears jersey to the Green Bay Packers' visit following their championship, owing to his first official visit never having happened at the time. Twenty-five years after the championship, surviving members of the team would be invited to the White House in 2011 by President Barack Obama, a Chicago native and Bears fan.

Under head coach Mike Ditka, who won the 1985 NFL Coach of the Year Award, the Bears went 15–1 in the regular season, becoming the second NFL team to win 15 regular season games, while outscoring their opponents with a staggering margin of 456–198.

The Bears' defense, the "46 defense", allowed the fewest points (198), fewest total yards (4,135), and fewest rushing of any team during the regular season (1,319). They also led the league in interceptions (34) and ranked third in sacks (64).

Pro Bowl quarterback Jim McMahon provided the team with a solid passing attack, throwing for 2,392 yards and 15 touchdowns, while also rushing for 252 yards and three touchdowns. Running back Walter Payton, who was then the NFL's all-time leading rusher with 14,860 yards, rushed for 1,551 yards. He also caught 49 passes for 500 yards, and scored 11 touchdowns. Linebacker Mike Singletary won the NFL Defensive Player of the Year Award by recording three sacks, three fumble recoveries, and one interception.

But one of the most distinguishable players on defense was a large rookie lineman named William "The Refrigerator" Perry. Perry came into training camp before the season weighing over 380 pounds. But after Bears defensive coordinator Buddy Ryan told the press that the team "wasted" their first round draft pick on him, Perry lost some weight and ended up being an effective defensive tackle, finishing the season with 5 sacks. He got even more attention when Ditka started putting him in the game at the fullback position during offensive plays near the opponent's goal line. During the regular season, Perry rushed for 2 touchdowns, caught a pass for another touchdown, and was frequently a lead blocker for Payton during goal line plays.

The Bears "46 defense" also had the following impact players: On the defensive line, Pro Bowler and future Hall of Famer Richard Dent led the NFL in sacks with 17, while Pro Bowler and future Hall of Famer Dan Hampton recorded 6.5 sacks, and nose tackle Steve McMichael compiled 8. In addition to Singletary, linebacker Otis Wilson had 10.5 sacks and 3 interceptions, while Wilber Marshall recorded 4 interceptions. In the secondary, defensive back Leslie Frazier had 6 interceptions, Mike Richardson recorded 4 interceptions, Dave Duerson had 5 interceptions, and Gary Fencik recorded 5 interceptions and 118 tackles.

Chicago's main offensive weapon was Payton and the running game. A big reason for Payton's success was fullback Matt Suhey as the primary lead blocker. Suhey was also a good ball carrier, rushing for 471 yards and catching 33 passes for 295 yards. The team's rushing was also aided by Pro Bowlers Jim Covert and Jay Hilgenberg and the rest of the Bears' offensive line including Mark Bortz, Keith Van Horne, and Tom Thayer.

In their passing game, the Bears' primary deep threat was wide receiver Willie Gault, who caught 33 passes for 704 yards, an average of 21.3 yards per catch, and returned 22 kickoffs for 557 yards and a touchdown. Tight end Emery Moorehead was another key contributor, catching 35 passes for 481 yards. Wide receiver Dennis McKinnon was another passing weapon, recording 31 receptions, 555 yards, and 7 touchdowns. On special teams, Kevin Butler set a rookie scoring record with 144 points, making 31 of 37 field goals (83%) and 51 of 51 extra points.

Meanwhile, the players brought their characterizations to the national stage with "The Super Bowl Shuffle", a rap song the Bears recorded during the season. Even though it was in essence a novelty song, it actually peaked at #41 on the Billboard charts and received a Grammy nomination for best R&B song by a group.

The Patriots were a Cinderella team during the 1985 season because many sports writers and fans thought they were lucky to make the playoffs at all. New England began the season losing three of their first five games, but won six consecutive games to finish with an 11–5 record. However, the 11–5 mark only earned them third place in the AFC East behind the Miami Dolphins and the New York Jets.

Quarterback Tony Eason, in his third year in the NFL, was inconsistent during the regular season, completing 168 out of 299 passes for 2,156 yards and 11 touchdowns, but also 17 interceptions. His backup, Steve Grogan, was considered one of the best reserve quarterbacks in the league. Grogan was the starter in six of the Patriots' games, and finished the regular season with 85 out of 156 completions for 1,311 yards, 7 touchdowns, and 5 interceptions.

Wide receiver Stanley Morgan provided the team with a good deep threat, catching 39 passes for 760 yards and 5 touchdowns. On the other side of the field, multi-talented wide receiver Irving Fryar was equally effective, catching 39 passes for 670 yards, while also rushing for 27 yards, gaining another 559 yards returning punts and kickoffs, and scoring 10 touchdowns. But like the Bears, the Patriots' main strength on offense was their rushing attack. Halfback Craig James rushed for 1,227 yards, caught 27 passes for 370 yards, and scored 7 touchdowns. Fullback Tony Collins rushed for 657 yards, recorded a team-leading 52 receptions for 549 yards, and scored 5 touchdowns. The Patriots also had an outstanding offensive line, led by Pro Bowl tackle Brian Holloway and future Hall of Fame guard John Hannah.

New England's defense ranked 5th in the league in fewest yards allowed (5,048). Pro Bowl linebacker Andre Tippett led the AFC with 16.5 sacks and recovered 3 fumbles. Pro Bowl linebacker Steve Nelson was also a big defensive weapon, excelling at pass coverage and run stopping. Also, the Patriots' secondary only gave up 14 touchdown passes during the season, second fewest in the league. Pro Bowl defensive back Raymond Clayborn recorded 6 interceptions for 80 return yards and 1 touchdown, while Pro Bowler Fred Marion had 7 interceptions for 189 return yards.

In the playoffs, the Patriots qualified as the AFC's second wild card.

But the Patriots, under head coach Raymond Berry, defied the odds, beating the New York Jets 26–14, Los Angeles Raiders 27–20, and the Dolphins 31–14 – all on the road – to make it to the Super Bowl. The win against Miami had been especially surprising, not only because Miami was the only team to beat Chicago in the season, but also because New England had not won in the Orange Bowl (Miami's then-home field) since 1966, the Dolphins' first season (then in the AFL). The Patriots had lost to Miami there 18 consecutive times, including a 30–27 loss in their 15th game of the season. But New England dominated the Dolphins in the AFC Championship Game, recording two interceptions from quarterback Dan Marino and recovering 4 fumbles. New England remains the only team to finish third in their division and qualify for the Super Bowl in the same season.

Meanwhile, the Bears became the first and only team in NFL history to shut out both of their opponents in the playoffs, beating the New York Giants 21–0 and the Los Angeles Rams 24–0.

Much of the Super Bowl pregame hype centered on Bears quarterback Jim McMahon. First, he was fined by the NFL during the playoffs for a violation of the league's dress code, wearing a head band from Adidas. He then started to wear a head band where he hand-wrote "Rozelle", after then-league commissioner Pete Rozelle.

McMahon suffered a strained glute as the result of a hit taken in the NFC Championship Game and flew his acupuncturist into New Orleans to get treatment. During practice four days before the Super Bowl, he wore a headband reading "Acupuncture". During a Bears practice before the Super Bowl, McMahon mooned a helicopter that was hovering over the practice.

Another anecdote involving McMahon during the Super Bowl anticipation involved WDSU sports anchor Buddy Diliberto reporting a quote attributed to McMahon, where he had allegedly referred to the women of New Orleans as "sluts" on a local morning sports talk show. This caused wide controversy among the women of New Orleans and McMahon began receiving calls from irate fans in his hotel. A groggy McMahon, who had not been able to sleep well because of all the calls he had gotten, was confronted by Mike Ditka later that morning and denied making the statement, saying he would not have even been awake to make the comment when he was said to have done so. He was supported in his claim by WLS reporter Les Grobstein, who was present when the alleged statements were made. WDSU would later retract the statement, have an on-air apology read by the station's general manager during the noon newscast on January 23, and suspended Diliberto.

The suburban village of Frankfort, Illinois has hosted an annual "Super Bowl Sunday" golf outing for over 30 years starting on the morning of Super Bowl XX. The tradition began when two neighbors Alan Heath and Bill Holley broke onto the local golf course to ease their nerves before the game. The first outing lasted 14 holes in the snow with temperatures hovering around 9 degrees.

In the aftermath of the Bears' Super Bowl victory the two made a pact that they would return to the golf course every year until the Bears repeated as Super Bowl Champions. With the dominance of the 1985 Bears squad the pair didn't think they would be starting a local tradition still going strong 30 years later.

The outing has taken place every year in all types of weather and will continue every year until the Bears repeat as Champions. Depending on the weather, the attendance hovers between 2 & 20 people each year.

The NBC telecast of the game, with play-by-play announcer Dick Enberg and color commentators Merlin Olsen and Bob Griese (who was not in the booth with Enberg and Olsen), garnered the third highest Nielsen rating of any Super Bowl to date at 48.3, but it ended up being the first Super Bowl to garner over 90 million viewers, the highest ever at that time. While Dick Enberg, Merlin Olsen and Bob Griese called the game, Bob Costas and his "NFL '85" castmates, Ahmad Rashad and Pete Axthelm anchored the pregame, halftime and postgame coverage. Other contributors included Charlie Jones (recapping Super Bowl I), Larry King (interviewing Mike Ditka and Raymond Berry), and Bill Macatee (profiling Patriots owner Billy Sullivan and his family). Also, the pregame coverage included what became known as "the silent minute"; a 60-second countdown over a black screen (a concept devised by then-NBC Sports executive Michael Weisman); a skit featuring comedian Rodney Dangerfield and an interview by "NBC Nightly News" anchor Tom Brokaw of United States President Ronald Reagan at the White House (this would not become a regular Super Bowl pregame feature until Super Bowl XLIII, when "Today show" host Matt Lauer interviewed U.S. President Barack Obama).

"The Last Precinct" debuted on NBC after the game.

Super Bowl XX was simulcast in Canada on CTV and also broadcast on Channel 4 in the United Kingdom, and Canal 5 (Mexico) on Mexico, with play-by-play announcers Toño de Valdés, Enrique Burak and color commentator Pepe Segarra.

Super Bowl XX is featured on "NFL's Greatest Games" under the title "Super Bears" with narration by Don LaFontaine.

The national radio broadcast was aired by NBC Radio, which outbid CBS Radio for the nationwide NFL contract in March 1985. Don Criqui was the play-by-play announcer, with Bob Trumpy as the color analyst. WGN-AM carried the game in the Chicago area (and thanks to WGN's 50,000-watt clear-channel signal, to much of the continental United States), with Wayne Larrivee on play-by-play, and Jim Hart and Dick Butkus providing commentary. WEEI carried the game in the Boston area, with John Carlson and Jon Morris on the call.

This was the first year that the NFL itself implemented the pregame entertainment. The pregame entertainment show began after the players left the field and ended with kick-off. Lesslee Fitzmorris created and directed the show. To celebrate the 20th Super Bowl game, the Most Valuable Players of the previous Super Bowls were featured during the pregame festivities. The number one song of the year coupled with video plays from each Super Bowl accompanied the presentation of each player. Performers formed the score of each championship game. The show concluded with the question of who would be the next Super Bowl Champions. This would start a tradition occurring every ten years (in Super Bowls XXX, XL and 50) in which past Super Bowl MVPs would be honored before the game.

After trumpeter Wynton Marsalis performed the national anthem, Bart Starr, MVP of Super Bowl I and Super Bowl II, tossed the coin.

The performance event group Up with People performed during the halftime show titled "Beat of the Future". Up with People dancers portrayed various scenes into the future. This was the last Super Bowl to feature Up with People as a halftime show, though they later performed in the Super Bowl XXV pregame show. The halftime show was dedicated to the memory of Dr. Martin Luther King Jr. (the first observance of Martin Luther King Jr. Day had been held the previous Monday).

The Patriots took the then-quickest lead in Super Bowl history after linebacker Larry McGrew recovered a fumble from Walter Payton at the Chicago 19-yard line on the second play of the game (the Bears themselves would break this record in Super Bowl XLI when Devin Hester ran back the opening kickoff for a touchdown). Bears quarterback Jim McMahon took responsibility for this fumble after the game, saying he had called the wrong play. This set up Tony Franklin's 36-yard field goal 1:19 into the first quarter after three incomplete passes by Tony Eason (during the first of those three, starting tight end Lin Dawson went down with torn ligaments in his knee). "I looked up at the message board", said Chicago linebacker Mike Singletary, "and it said that 15 of the 19 teams that scored first won the game. I thought, yeah, but none of those 15 had ever played the Bears." Chicago struck back with a 7-play, 59-yard drive, featuring a 43-yard pass completion from McMahon to wide receiver Willie Gault, to set up a field goal from Kevin Butler, tying the score at 3–3.

After both teams traded punts, Richard Dent and linebacker Wilber Marshall shared a sack on Eason, forcing a fumble that lineman Dan Hampton recovered on the Patriots 13-yard line. Chicago then drove to the 3-yard line, but had to settle for another field goal from Butler after rookie defensive lineman William "The Refrigerator" Perry was tackled (and technically sacked) for a 1-yard loss while trying to throw his first NFL pass on a halfback option play. On the Patriots' ensuing drive, Dent forced running back Craig James to fumble, which was recovered by Singletary at the 13-yard line. Two plays later, Bears fullback Matt Suhey scored on an 11-yard touchdown run to increase the lead to 13–3.

New England took the ensuing kickoff and ran one play before the first quarter ended, which resulted in positive yardage for the first time in the game (a 3-yard run by James).

After an incomplete pass and a 4-yard loss, the Patriots had to send in punter Rich Camarillo again, and receiver Keith Ortego returned the ball 12 yards to the 41-yard line. The Bears subsequently drove 59 yards in 10 plays, featuring a 24-yard reception by Suhey, to score on McMahon's 2-yard touchdown run to increase their lead, 20–3. After the ensuing kickoff, New England lost 13 yards in 3 plays and had to punt again, but got the ball back with great field position when defensive back Raymond Clayborn recovered a fumble from Suhey at their own 46-yard line. On the punt, Ortego forgot what the play call was for the punt return, and the ensuing chaos resulted in him being penalized for running after a fair catch and teammate Leslie Frazier suffering a knee injury, which ended his career.

Patriots head coach Raymond Berry then replaced Eason with Steve Grogan, who had spent the previous week hoping he would have the opportunity to step onto the NFL's biggest stage. "I probably won't get a chance", he had told reporters a few days before the game. "I just hope I can figure out some way to get on the field. I could come in on the punt-block team and stand behind the line and wave my arms, or something." But on his first drive, Grogan could only lead them to the 37-yard line, and they decided to punt rather than risk a 55-yard field goal attempt. The Bears then marched 72 yards in 11 plays, moving the ball inside the Patriots' 10-yard line. New England kept them out of the end zone, but Butler kicked his third field goal on the last play of the half to give Chicago a 23–3 halftime lead.
The end of the first half was controversial. With 21 seconds left, McMahon scrambled to the Patriots' 3-yard line and was stopped inbounds. With the clock ticking down, players from both teams were fighting, and the Bears were forced to snap the ball before the officials formally put it back into play, allowing McMahon to throw the ball out of bounds and stop the clock with three seconds left. The Bears were penalized five yards for delay of game, but according to NFL rules, 10 seconds should have also been run off the clock during such a deliberate clock-stopping attempt in the final two minutes of a half. In addition, a flag should have been thrown for fighting (also according to NFL rules). This would have likely resulted in offsetting penalties, which would still allow for a field goal attempt. Meanwhile, the non-call on the illegal snap was promptly acknowledged by the officials and reported by NBC sportscasters during halftime, but the resulting three points were not taken away from the Bears (because of this instance, the NFL instructed officials to strictly enforce the 10-second run-off rule at the start of the 1986 season).

The Bears had dominated New England in the first half, holding them to 21 offensive plays (only four of which resulted in positive yardage), −19 total offensive yards, two pass completions, one first down, and 3 points. While Eason was in the game, the totals were six possessions, one play of positive yardage out of 15 plays, no first downs, 3 points, 3 punts, 2 turnovers, no pass completions, and -36 yards of total offense. Meanwhile, Chicago gained 236 yards and scored 23 points themselves.

After the Patriots received the second-half kickoff, they managed to get one first down, but then had to punt after Grogan was sacked twice. Camarillo, who punted four times in the first half, managed to pin the Bears back at their own 4-yard line with a then-Super Bowl record 62-yard punt. But the Patriots' defense still had no ability to stop Chicago's offense. On their very first play, McMahon faked a handoff to Payton, then threw a 60-yard completion to Gault. Eight plays later, McMahon finished the Super Bowl-record 96-yard drive with a 1-yard touchdown run to increase the Bears' lead to 30–3. On New England's second drive of the quarter, Chicago cornerback Reggie Phillips (who replaced Frazier) intercepted a pass from Grogan and returned it 28 yards for a touchdown to increase the lead to 37–3.

On the second play of their ensuing possession, the Patriots turned the ball over again, when receiver Cedric Jones lost a fumble after catching a 19-yard pass from Grogan, and Wilber Marshall returned the fumble 13 yards to New England's 37-yard line. A few plays later, McMahon's 27-yard completion to receiver Dennis Gentry moved the ball to the 1-yard line, setting up perhaps the most memorable moment of the game. William "The Refrigerator" Perry was brought on to score on offense, as he had done twice in the regular season. His touchdown (while running over Patriots linebacker Larry McGrew in the process) made the score 44–3. The Bears' 21 points in the third quarter is still a record for the most points scored in that period, and their 41-point lead remains the record for widest margin after three quarters in a Super Bowl.

Perry's surprise touchdown cost Las Vegas sports books hundreds of thousands of dollars in losses from prop bets.

The Patriots finally scored a touchdown early in the fourth quarter, advancing the ball 76 yards in 12 plays and scoring on an 8-yard fourth-down pass from Grogan to receiver Irving Fryar. But the Bears' defense dominated New England for the rest of the game, forcing another fumble, another interception, and defensive lineman Henry Waechter's sack on Grogan in the end zone for a safety to make the final score 46–10.

One oddity in the Bears' victory was that Walter Payton had a relatively poor performance running the ball and never scored a touchdown in Super Bowl XX, his only Super Bowl appearance during his Hall of Fame career. Many people including Mike Ditka have claimed that the reason for this was due to the fact that the Patriots' defensive scheme was centered on stopping Payton. Although Payton was ultimately the Bears' leading rusher during the game, the Patriots' defense held him to only 61 yards on 22 carries, with his longest run being only 7 yards. He was given several opportunities to score near the goal line, but New England stopped him every time before he reached the end zone (such as his 2-yard loss from the New England 3-yard line a few plays before Butler's second field goal, and his 2-yard run from the 4-yard line right before McMahon's first rushing touchdown). Thus, Chicago head coach Mike Ditka opted to go for other plays to counter the Patriots' defense. Ditka has since stated that his biggest regret of his career was not creating a scoring opportunity for Payton during the game.

McMahon, who completed 12 out of 20 passes for 256 yards, became the first quarterback in a Super Bowl to score 2 rushing touchdowns. Bears receiver Willie Gault finished the game with 129 receiving yards on just 4 receptions, an average of 32.3 yards per catch. He also returned 4 kickoffs for 49 yards. Suhey had 11 carries for 52 yards and a touchdown, and caught a pass for 24 yards. Singletary tied a Super Bowl record with 2 fumble recoveries.

Eason became the first Super Bowl starting quarterback to fail to complete a pass, going 0 for 6 attempts. Grogan completed 17 out of 30 passes for 177 yards and 1 touchdown, with 2 interceptions. Although fullback Tony Collins was the Patriots' leading rusher, he was limited to just 4 yards on 3 carries, and caught 2 passes for 19 yards. New England receiver Stephen Starring returned 7 kickoffs for 153 yards and caught 2 passes for 39 yards. The Patriots, as a team, only recorded 123 total offensive yards, the second-lowest total in Super Bowl history.

Sources: NFL.com Super Bowl XX, USA Today Super Bowl XX Play by Play, Super Bowl XX Play Finder Chi, Super Bowl XX Play Finder NE

Completions/attempts
Carries
Long gain
Receptions
Times targeted

The following records were set in Super Bowl XX, according to the official NFL.com boxscore and the Pro-Football-Reference.com game summary.

Source:




</doc>
<doc id="28977" url="https://en.wikipedia.org/wiki?curid=28977" title="Salute">
Salute

A salute is a gesture or other action used to display respect. Salutes are primarily associated with armed forces, but other organizations and civilians also use salutes.

In military traditions of various times and places, there have been numerous methods of performing salutes, using hand gestures, cannon or rifle shots, hoisting of flags, removal of headgear, or other means of showing respect or deference. In the Commonwealth of Nations, only commissioned officers are saluted, and the salute is to the commission they carry from their respective commanders-in-chief representing the Monarch, not the officers themselves.

Hand salutes are normally carried out by bringing the right hand to the head in some way, the precise manner varying between different countries. The British Army's salute is almost identical to the French salute, with the palm facing outward. The customary salute in the Polish Armed Forces is the two-fingers salute, a variation of the British military salute with only two fingers extended. In the Russian military, the right hand, palm down, is brought to the right temple, almost, but not quite, touching; the head has to be covered. In the Hellenic Army salute, the palm is facing down and the fingers point to the coat of arms.

In the United States Navy, United States Marine Corps, United States Coast Guard, United States Public Health Service Commissioned Corps, Colombian Army and Ecuadorian Army, as well as in all branches of the French Armed Forces, Spanish Armed Forces, British Armed Forces (with the exception of the Blues and Royals), Canadian Forces, Danish Armed Forces, Hellenic Armed Forces, Italian Armed Forces, Norwegian Armed Forces, Polish Armed Forces, Irish Defence Forces, Australian Defence Force, South African National Defence Force, Swedish Defence Forces, Turkish Armed Forces, Portuguese Armed Forces and Russian and all former Soviet republic forces, hand salutes are only given when a cover (protection for the head, usually a hat) is worn.

If there is a reason not to salute with the right hand, due for example to performing an activity that should not be interrupted, or injury, an equivalent left-hand salute is sometimes performed. A right-handed boatswain's mate piping an officer aboard may salute with their left hand.

When the presence of enemy snipers is suspected, military salutes are generally forbidden, since the enemy may use them to recognize officers as valuable targets.

According to some modern military manuals, the modern Western salute originated in France when knights greeted each other to show friendly intentions by raising their visors to show their faces, using a salute. Others also note that the raising of one's visor was a way to identify oneself saying "This is who I am, and I am not afraid." Medieval visors were, to this end, equipped with a protruding spike that allowed the visor to be raised using a saluting motion.

The US Army Quartermaster School provides another explanation of the origin of the hand salute: that it was a long-established military courtesy for subordinates to remove their headgear in the presence of superiors. As late as the American Revolution, a British Army soldier saluted by removing his hat. With the advent of increasingly cumbersome headgear in the 18th and 19th centuries, however, the act of removing one's hat was gradually converted into the simpler gesture of grasping or touching the visor and issuing a courteous salutation.

As early as 1745, a British order book stated that: "The men are ordered not to pull off their hats when they pass an officer, or to speak to them, but only to clap up their hands to their hats and bow as they pass." Over time, it became conventionalized into something resembling the modern hand salute. In the Austrian Army the practice of making a hand salute replaced that of removing the headdress in 1790, although officers wearing cocked hats continued to remove them when greeting superiors until 1868.

The naval salute, with the palm downwards is said to have evolved because the palms of naval ratings, particularly deckhands, were often dirty through working with lines and was deemed insulting to present a dirty palm to an officer; thus the palm was turned downwards. During the Napoleonic Wars, British crews saluted officers by touching a clenched fist to the brow as though grasping a hat-brim between fingers and thumb.

When carrying a sword, still done on ceremonial occasions, European military forces and their cultural descendants use a two-step gesture. The sword is first raised, in the right hand, to the level of and close to the front of the neck. The blade is inclined forward and up 30 degrees from the vertical; the true edge is to the left. Then the sword is slashed downward to a position with the point close to the ground in front of the right foot. The blade is inclined down and forward with the true edge to the left. This gesture originated in the Crusades. The hilt of a sword formed a cross with the blade, so if a crucifix was not available, a Crusader could kiss the hilt of his sword when praying, before entering battle, for oaths and vows, and so on. The lowering of the point to the ground is a traditional act of submission.

In fencing, the fencers salute each other before putting their masks on to begin a bout. There are several methods of doing this, but the most common is to bring the sword in front of the face so that the blade is pointing up in front of the nose. The fencers also salute the referee and the audience.

When armed with a rifle, two methods are available when saluting. The usual method is called "present arms"; the rifle is brought to the vertical, muzzle up, in front of center of the chest with the trigger away from the body. The hands hold the stock close to the positions they would have if the rifle were being fired, though the trigger is not touched. Less formal salutes include the "order arms salute" and the "shoulder arms salutes." These are most often given by a sentry to a low-ranking superior who does not rate the full "present arms" salute. In the "order arms salute," the rifle rests on its butt by the sentry's right foot, held near the muzzle by the sentry's right hand, and does not move. The sentry brings his flattened left hand across his body and touches the rifle near its muzzle. When the rifle is being carried on the shoulder, a similar gesture is used in which the flattened free hand is brought across the body to touch the rifle near the rear of the receiver.

A different type of salute with a rifle is a ritual firing performed during military funerals, known as a three-volley salute. In this ceremonial act, an odd number of rifleman fire three blank cartridges in unison into the air over the casket. This originates from an old European tradition wherein a battle was halted to remove the dead and wounded, then three shots were fired to signal readiness to re-engage.

The custom of firing cannon salutes originated in the Royal Navy. When a cannon was fired, it partially disarmed the ship until reloaded, so needlessly firing a cannon showed respect and trust. As a matter of courtesy a warship would fire her guns harmlessly out to sea, to show that she had no hostile intent. At first, ships were required to fire seven guns, and forts, with their more numerous guns and a larger supply of gunpowder, to fire 21 times. Later, as the quality of gunpowder improved, the British increased the number of shots required from ships to match the forts.

The system of odd-numbered rounds originated from Samuel Pepys, Secretary to the Navy in the Restoration, as a way of economising on the use of powder, the rule until that time having been that all guns had to be fired. Odd numbers were chosen, as even numbers indicated a death.

As naval customs evolved, the 21-gun salute came to be reserved for heads of state, with fewer rounds used to salute lower-ranking officials. Today, In the US Armed Forces, heads of government and cabinet ministers (e.g., the Vice President, U.S. cabinet members, and service secretaries), and military officers with five-star rank receive 19 rounds; four-stars receive 17 rounds; three-stars receive 15; two-stars receive 13; and a one-star general or admiral receives 11. These same standards are currently adhered to by ground-based saluting batteries.

Multiples of 21-gun salutes may be fired for particularly important celebrations. In monarchies this is often done at births of members of the royal family of the country and other official celebrations associated with the royal family.

A specialty platoon of the 3rd US Infantry Regiment (The Old Guard), the Presidential Salute Battery is based at Fort Myer, Virginia. The Guns Platoon (as it is known for short) has the task of rendering military honors in the National Capital Region, including armed forces full-honors funerals; state funerals; presidential inaugurations; full-honors wreath ceremonies at the Tomb of the Unknowns in Arlington National Cemetery; state arrivals at the White House and Pentagon, and retirement ceremonies for general-grade officers in the Military District of Washington, which are normally conducted at Fort Myer.

The Presidential Salute Battery also participates in A Capitol Fourth, the Washington Independence Day celebration; the guns accompany the National Symphony Orchestra in performing the "1812 Overture".

The platoon maintains its battery of ten ceremonially-modified World War II-vintage M-5 anti-tank guns at the Old Guard regimental motor pool.

A ceremonial or celebratory form of aerial salute is the flypast (known as a "flyover" in the United States), which often follows major parades such as the annual Trooping the Colour in the United Kingdom or the French "défilé du 14 juillet". It is seen in other countries as well, notably Singapore and Canada. In Singapore, the Republic of Singapore Air Force usually conducts aerial salutes during the annual National Day Parade and major state events, such as during the funeral of Lee Kuan Yew.

Gun salute by aircraft, primarily displayed during funerals, began with simple flypasts during World War I and have evolved into the missing man formation, where either a formation of aircraft is conspicuously missing an element, or where a single plane abruptly leaves a formation

A casual salute by an aircraft, somewhat akin to waving to a friend, is the custom of "waggling" the wings by partially rolling the aircraft first to one side, and then the other.

In both countries, the right-hand salute is generally identical to, and drawn from the traditions of, the British armed forces. The salute of the Australian or New Zealand Army is best described as the right arm taking the path of the longest way up and then the shortest way down. Similar in many ways, the salute of the Royal Australian Air Force and Royal New Zealand Air Force takes the longest way up and the shortest way down. The Royal Australian Navy and Royal New Zealand Navy, however, take the shortest way up, palm down, and the shortest way down. The action of the arm rotating up is slower than the action of the conclusion of the salute which is the arm being quickly "snapped" down to the saluter's side. Junior members are required to salute first and the senior member is obliged to return the compliment. Protocol dictates that the Monarch, members of the Royal Family, the Governor-General and State Governors are to be saluted at all times by all ranks. Except where a Drill Manual (or parade) protocol dictates otherwise, the duration of the salute is timed at three beats of the quick-time march (approximately 1.5 seconds), timed from the moment the senior member first returns it. In situations where cover (or "headdress", as it is called in the Australian Army) is not being worn, the salute is given verbally; the junior party (or at least the senior member thereof) will first come to attention, then offer the salute "Good morning/afternoon Your Majesty/Your Royal Highness/Prime Minister/Your Grace/Sir/Ma'am", etc., as the case may be. It is this, rather than the act of standing to attention, which indicates that a salute is being offered. If either party consists of two or more members, all will come to attention, but only the most senior member of the party will offer (or return) the physical or verbal salute. The party which is wearing headdress must always offer, or respond with, a full salute. However, within the Forward Edge of the Battle Area (FEBA) no salutes of any kind are given, under any circumstances; it is always sensible to assume that there are snipers in the area who may see or overhear. In this case, parties personally known to each other are addressed familiarly by their first or given names, regardless of rank; senior officers are addressed as one might address a stranger, courteously, but without any naming or mark of respect.

Since 1917, the British Army's salute has been given with the right hand palm facing forwards with the fingers almost touching the cap or beret. Before 1917, for Other Ranks (i.e. not officers) the salute was given with whichever hand was furthest from the person being saluted, whether that was the right or the left. Officers always saluted with the right hand (as the left, in theory, would always be required to hold the scabbard of their sword) The salute is given to acknowledge the Queen's commission. A salute may not be given unless a soldier is wearing his regimental headdress, for example a beret, caubeen, Tam o' Shanter, Glengarry, field service cap or peaked cap. This does not apply to members of The Blues and Royals (RHG/1stD) The Household Cavalry who, after The Battle of Warburg were allowed to salute without headdress. If a soldier or officer is not wearing headdress then he or she must come to attention instead of giving/returning the salute. The subordinate salutes first and maintains the salute until the superior has responded in kind.

There is a widespread though erroneous belief that it is statutory for "all ranks to salute a bearer of the Victoria Cross". There is no official requirement that appears in the official Warrant of the VC, nor in Queen's Regulations and Orders, but tradition dictates that this occurs and as such the Chiefs of Staff will salute a Private awarded either a VC or George Cross.

The custom of saluting commissioned officers relates wholly to the commission given by Her Majesty the Queen to that officer, not the person. Therefore, when a subordinate airman salutes an officer, he is indirectly acknowledging Her Majesty as Head of State. A salute returned by the officer is on behalf of the Queen.

The RAF salute is similar to the British Army, the hand is brought upwards in a circular motion out from the body, it is stopped 1 inch (20 mm) to the rear of the right of the right eye, the elbow and wrist are kept inline with the shoulder. The hand is then brought straight down back to the position of attention, this movement is completed to the timing "UP TWO-THREE/DOWN"

The Naval salute differs in that the palm of the hand faces down towards the shoulder. This dates back to the days of sailing ships, when tar and pitch were used to seal a ship's timbers from seawater. To protect their hands, officers wore white gloves and it was considered most undignified to present a dirty palm in the salute, so the hand was turned through 90 degrees. A common story is that Queen Victoria, having been saluted by an individual with a dirty palm, decreed that in future sailors of the fleet would salute palm down, with the palm facing the ground.

The Royal Marines follow the British Army and salute with the right hand palm facing forward.

In the British Empire (originally in the maritime and hinterland sphere of influence of the East India Company, HEIC, later transformed into crown territories), mainly in British India, the numbers of guns fired as a "gun salute" to the ruler of a so-called princely state became a politically highly significant indicator of his status, not governed by objective rules, but awarded (and in various cases increased) by the British paramount power, roughly reflecting his state's socio-economic, political and/or military weight, but also as a prestigious reward for loyalty to the Raj, in classes (always odd numbers) from three to twenty-one (seven lacking), for the "vassal" indigenous rulers (normally hereditary with a throne, sometimes raised as a personal distinction for an individual ruling prince). Two sovereign monarchies officially outside the Empire were granted a higher honor: thirty-one guns for the royal houses of Afghanistan (under British and Russian influence), and Siam (which was then ruled by the Rattanakosin Kingdom).

In addition, the right to style himself "Highness" (Majesty, which since its Roman origin expresses the sovereign authority of the state, was denied to all "vassals"), a title of great importance in international relations, was formally restricted to rulers of relatively high salute ranks (originally only those with eleven guns or more, later also those with nine guns).

Much as the British salute, described above, the Canadian military salutes to demonstrate a mark of respect and courtesy for the commissioned ranks. When in uniform and not wearing headdress one does not salute. Instead, compliments shall be paid by standing at attention. If on the march, arms shall be swung and the head turned to the left or right as required.

On Remembrance Day, 2009, The Prince of Wales attended the national ceremony in Ottawa with Governor General Michaëlle Jean—both wearing Canadian military dress. CBC live television coverage of the event noted that, when Prince Charles saluted, he performed the Canadian form of the salute with a cupped hand (the British "naval salute"—appropriate, as he did his military service as an officer in the Royal Navy), adopted by all elements of the Canadian Forces after unification in 1968, rather than the British (Army) form with the palm facing forward.

In the Danish military, there are two types of military salutes. The first type is employed by the Royal Danish Navy, Royal Danish Air Force, and Guard Hussar Regiment Mounted Squadron, and is the same as the one used by the U.S. The second is employed by the Royal Danish Army, and goes as follows: Raise the right arm forward, as to have upper arm 90 degrees from the body. Move the right hand to the temple, and have it parallel to the ground.

The French military salutes to demonstrate a mark of respect, fraternity and courtesy for all soldiers ; subordinates salute superiors and every salute is given back. Salutes are not performed if a member is not wearing a headdress or if he is holding a weapon. The French salute, as the original template, is performed with a flat hand, palm facing forwards; the upper arm is horizontal and the tips of the fingers come near the corner of the eyes. The hand unlike th British salute remains at a 45 degree angle in line with the lower arm. The five fingers are lined together It perfectly mirrors the gesture made when knights greeted each other to show friendly intentions by raising their visors to show their faces. A crisp tension may be given when the salute is taken or broken. However some "creative" salutes are in use in certain mounted (cavalry) units. The fingers can be spread out with only the right thumb brushing the temple, or the hand can be cocked vertically along the cheek, with the little finger detached or not. These unusual regimental salutes are mannerisms which are lost during official ceremonies. A salute is never given with a bare head or holding a weapon. A civilian, even if he has a hat, never salutes but a nod to a patrolling soldier is appreciated.

In the German Bundeswehr, the salute is performed with a flat hand, with the thumb resting on the index finger. The hand is slightly tilted to the front so that the thumb can not be seen. The upper arm is horizontal and the fingers point to the temple but do not touch it or the headgear. Every soldier saluting another uniformed soldier is entitled to be saluted in return. Soldiers below the rank of Feldwebel are not permitted to speak while saluting. Since the creation of the Bundeswehr, soldiers are required to salute with and without headgear. Originally, in the Reichswehr it was not permitted to perform the salute when the soldier is not wearing uniform headgear. In the Wehrmacht, the traditional military salute was required when wearing headgear, but the Nazi salute was performed when not wearing headgear. The Wehrmacht eventually fully adopted the Nazi salute following the 20 July Plot. East German National People's Army followed the Reichswehr protocol.

In India, the three forces have different salutes with the Indian Army and the Indian Navy following the British tradition. In the Indian army, the salute is performed by keeping the open palm forward, with fingers and thumb together and middle finger almost touching the hatband or right eyebrow. This is often accompanied by the regimental salutation, e.g.:"Sat Sri Akal" in the Sikh Regiment. The Navy salute has the palm facing towards the ground at a 90-degree angle. The Indian Air Force salute involves the right arm being sharply raised from the front by the shortest possible way, with the plane of the palm at 45-degree angle to the forehead.

In Indonesia, executing a salute has its regulations. For members who are part of a uniformed institution and wearing a uniform will implement a gesture of salute according to the regulations of the institution the member is part of. In this case, personnel of the TNI and Indonesian National Police are to implement a hand salute by forming the right hand up making an angle of 90 degrees and is bent 45 degrees, fingers are pressed together and placed near the temple of the right eye, palm facing down. For personnel wearing a headdress is to place the tip of the right index finger touching the front right tip of the headdress. 
Other uniformed organizations/institutions which are not part of the military/police will implement a hand salute as done by members of the military/police.

The command for this gesture in Indonesian is "Hormat, Gerak!". Military and police personnel armed with a rifle during a ceremony will implement a Present arms while personnel unarmed will execute the hand salute.

This is done during the raising and/or lowering of the national flag, rendition or singing of the national anthem, and when saluting to a person or object worth saluting.

In the Israel Defense Forces, saluting is normally reserved for special ceremonies. Unlike in the US Army, saluting is not a constant part of day to day barracks life.

In Pakistan, the salute is generally identical to that of British armed forces. Salute is given with the right hand palm facing forward and fingers slightly touching the right side of the forehead, but not on the forehead. The salute must be performed by the lower rank officials to the higher rank officials under all conditions except when the higher rank official is not in uniform or if the lower rank official is the driver and the vehicle is in motion. The salute is sometimes also performed by left hand if the right hand of the person is completely occupied.

Military personnel of the People's Liberation Army salute palm-down, similar to the Royal Navy or US Military salutes.

In Polish military forces, military men use two fingers to salute, and when they wear headdress (including helmet) because soldiers are supposed to salute to the Coat of Arms on the military headdress, out of respect to the national symbols (This is called the Two-finger salute). There are some exceptions in Polish regulations when salute is not demonstrated, for instance after proclaiming alert in military unit area. As above, salute is marking respect for higher rank or command. .

Salutes are similar to those of the Royal Navy. The official instruction for stationary salute states: "The right hand is quickly raised straight up to the headgear. The fingers straight but not stiff next to each other, the little finger edge facing forward. One or two finger tips lightly resting against the right part of the head gear (visor), so that the hand does not obstruct the eye. The wrist straight, the elbow angled forward and slightly lower than the shoulder." Salutes to persons are normally not made when further away than 30 m. Hand salutes are performed only when carrying head gear, if bare headed (normally only indoors) a swift turning of the head towards the person that is being saluted is made instead. The same applies if the right hand is carrying any item that cannot easily be transferred to the left hand. During inspections and when on guard duty, the salute is made by coming to attention. Drivers of moving vehicles never salute. In formations, only the commander salutes.

Swiss soldiers are required to salute any higher-ranking military personnel whenever they encounter them. When the soldier announces to a higher-ranking person he has to state the superior's rank, his rank and his name. When a military formation encounters a superior, it has to state the name of the formation. The salute is given like that of the British navy with the palm pointing towards the shoulder, the tips of the fingers pointing towards the temple.

Within the Turkish military hand salutes are only given when a cover (protection for the head, usually a hat) is worn.

If head is not covered or when the personnel is carrying a rifle on the shoulder the "head salute" is performed by nodding the head forward slightly while maintaining erect posture.

The salute (hand or head) must be performed first by the lower ranking personnel to the higher ranking personnel, and higher official is expected to return the salute, under all conditions except:

Despite of the rank, casket of a martyr personnel while in transport or on stand has to be saluted by all ranks of personnel.


Within United States' military, the salute is a courteous exchange of greetings, with the junior member always saluting first. When returning or rendering an individual salute, the head and eyes are turned toward the Colors or person saluted. Military personnel in uniform are required to salute when they meet and recognize persons entitled to a salute, except when it is inappropriate or impractical (in public conveyances such as planes and buses, in public places such as inside theaters, or when driving a vehicle).

It is believed that the U.S. military's salute was influenced by British military, although differs slightly, in that the palm of the hand faces down towards the shoulder. This difference may date back to the days of sailing ships, when tar and pitch were used to seal the timber from seawater. During such times, was considered undignified to present a dirty palm in the salute, so the hand was turned through 90 degrees.

Specifically, a proper salute goes as follows: Raise the right hand sharply, fingers and thumb extended and joined, palm facing down, and place the tip of the right forefinger on the rim of the visor, slightly to the right of the eye. The outer edge of the hand is barely canted downward so that neither the back of the hand nor the palm is clearly visible from the front. The hand and wrist are straight, the elbow inclined slightly forward, and the upper arm is horizontal.

The United States Army and United States Air Force give salutes both covered and uncovered, but saluting indoors is forbidden except when formally reporting to a superior officer or during an indoor ceremony. When outdoors, a cover is to be worn at all times when wearing Battle Dress Uniforms/Army Combat Uniforms, but is not required when wearing physical training (PT) gear.


State Defense Forces (SDF; also known as state military, state guards, state militias, or state military reserves) in the United States are military units that operate under the sole authority of a state government. State defense forces are authorized by state and federal law and are under the command of the governor of each state.

State defense forces soldiers are subject to the Uniform Code of Military Justice. They are also subject to their state military laws and regulations and render the same customs and courtesies as active duty, Reserve and National Guard personnel.

The Zogist salute is a military salute that was instituted by Zog I of Albania. It is a gesture whereby the right hand is placed over the heart, with the palm facing downwards. It was first widely used by Zog's personal police force and was later adopted by the Royal Albanian Army.

In Mexico, a salute similar to the Zogist one is rendered by Mexican civilians during the playing of the Mexican national anthem.

Most police forces salute similar to the Canadian Armed Forces standard, with the exception of the Royal Canadian Mounted Police and the Royal Newfoundland Constabulary, which follow the British Army standard of saluting with the full palm facing forward, touching the brim of the hat, if worn.

Similar salutes are used by guards of honour for non-police services (e.g. Toronto Fire Services, Toronto Transit Commission) during funerals or ceremonial events.

All uniform branches of the Hong Kong (Police, Police Auxiliary, Police Pipeband, Fire (including Ambulance service members), Immigration, Customs, Correctional Services, Government Flying Service, Civil Aid Service) salute according to British Army traditions. Personnel stationed with the People's Liberation Army in Hong Kong salute using the Chinese military standards and similar to those used by the Royal Navy.

Non-government organizations like Hong Kong Air Cadet Corps, Hong Kong Adventure Corps, the Boys' Brigade, Hong Kong, Hong Kong Sea Cadet Corps and St. John Ambulance all follow the same military salutes due to their ties with the British Armed Forces.

In the United States, civilian military auxiliaries such as the Civil Air Patrol are required to salute all commissioned and warrant officers of higher rank and return the salute of those with lower ranks of the U.S. Uniformed Services (Army, Navy, Air Force, Marine Corps, Coast Guard, U.S. Public Health Service, National Oceanic and Atmospheric Administration Commissioned Corps) senior in rank to them, as well as all friendly foreign officers, though military members are not required to reciprocate (they may salute voluntarily if they choose). CAP officers are required to salute one another though this is not uniformly observed throughout the CAP. Cadets are required to salute all senior members and military/uniformed services personnel.

The U.S. Coast Guard Auxiliary requires its members to salute all commissioned and warrant officers of higher rank and return the salute of those with lower ranks; since Auxiliarists hold "office" rather than "grade" (indicated by modified military insignia), all Auxiliarists are required to perform this courtesy. Saluting between Auxiliarists is not usually the custom, but is not out of protocol to do so. When operating in direct support of the USCG, or when on military installations in general, Auxiliarists usually wear "member" insignia unless specified otherwise by the officer/NCO in charge.

In most countries, civilians have their own form of salutes.

The same salute of the United States was instituted in Albania as the "Zog salute" by King Zog I.

In Indonesia, executing a salute is also regulated for civilians according to the Constitution of Indonesia. The salute gesture for Civilians in civilian clothing are to stand upright in their respective positions with perfect posture, straightening their arms down, clenching palms, and thumbs facing forward against the thighs with a straight ahead gaze. Members of a uniformed organization/institution which are not part of the military/police such as Fire fighters, traffic wardens, municipal policemen, immigration officers, customs officers, Search and Rescue personnel, scouts, school students, etc. in uniform will implement a hand salute as done by members of the military/police.

This is done during the raising and/or lowering of the national flag, rendition or singing of the national anthem, and when saluting to a person or object worth saluting.

In Iran a salute similar to the United States is given. In ancient times a salute would be given by raising a flat hand in front of the chest with the thumb facing the saluters face.

In Latin America, except in Mexico, a salute similar to the United States flag salute is used, with the hand over the heart.

In the Philippines, civilians salute to the national flag during flag raising and upon hearing the Philippine National Anthem by standing at attention and doing the same hand-to-heart salute as their American, Italian, Nigerian, and South African counterparts. People wearing hats or caps must bare their heads and hold the headwear over their heart; this rule however exempts those who wear headgear or headwear for religious purposes/reasons. Members of the Armed Forces of the Philippines, the Philippine National Police, Philippine Coast Guard, security guards, Boy Scouts of the Philippines, Girl Scouts of the Philippines, including citizens military training, and sometimes airline pilots and civilian ship crews, meanwhile do the traditional military salutes if they are in uniform on duty; off-duty personnel do the hand-to-heart salutes. During the Martial Law years from 1972–1981 up to the 1986 EDSA Revolution, the "raised clenched fist" salute was done during the singing and playing of the National Anthem by some groups.

People whose faith or religious beliefs prohibit them from singing the anthem or reciting the patriotic pledge such as Jehovah's Witnesses are exempted from doing the salutes but are still required to show full respect when the anthem is being sung or played on record by standing at attention and not engaging in disruptive activities.

Boy Scouts and Girl Scouts meanwhile have their own form of salutes.

Thailand also has the same rule like Indonesia wherein all persons present regardless of nationality are expected to stand at attention and respectfully during the flag raising and lowering and upon hearing the Thai National Anthem every 8:00 a.m. and 6:00 p.m. or hearing the Sansoen Phra Barami. The Lèse majesté in Thailand says that it is a serious criminal offense to dishonor the flag of Thailand or National/Royal Anthem.

The Roman salute is a gesture in which the arm is held out forward straight, with palm down and fingers extended straight and touching. Sometimes the arm is raised upward at an angle, sometimes it is held out parallel to the ground. A well known symbol of Fascism, it is commonly perceived to be based on a classical Roman custom. but no known Roman work of art displays this salute, and no known Roman text describes it.

Beginning with Jacques-Louis David's painting "The Oath of the Horatii" (1784), an association of the gesture with Roman republican and imperial culture emerged through 18th-century French art. The association with ancient Roman traditions was further developed in France during the Napoleonic era and again in popular culture through late 19th- and early 20th-century plays and films. These include the epic "Cabiria" (1914), whose screenplay was attributed to Italian nationalist Gabriele d'Annunzio. In a case of life imitating art, d'Annunzio appropriated the salute as a neo-imperial ritual when he led the occupation of Fiume in 1919. It was soon adopted by the Italian Fascist party, whose use of the salute inspired the Nazi party salute. However, the armed forces ("Wehrmacht") of the Third Reich used a German form of the military salute until, in the wake of the July 20 plot on Hitler's life in 1944, the Nazi salute or "Hitlergruss" was imposed on them.

The Bellamy salute was a similar gesture and was the civilian salute of the United States from 1892 to 1942.

In Germany showing the Roman salute is today prohibited by law. Those rendering similar salutes, for example raising the left instead of the right hand, or raising only three fingers, are liable to prosecution. The punishment derives from § 86a of the German Criminal Code and can be up to three years imprisonment or a fine (in minor cases).

According to SOPs (standard operating procedures) of most airlines, the ground crew that handles departure of an aircraft from a gate (such handling normally includes: disconnecting of required for engine start pneumatic generators or aircraft power and ventilation utilities, aircraft push-back, icing inspection, etc.) is required to salute the captain before the aircraft is released for taxi. Captain normally returns the salute. Since a large percentage of airline pilots are ex-military pilots, this practice was transferred to the airline industry from the military. Exactly the same saluting practice is appropriate to most military aircraft operations, including Air Force, Navy and Army.

In Islam raising the index finger signifies the Tawhīd (تَوْحِيد), which denotes the indivisible oneness of God. It is used to express the unity of God ("there is no god but God"). The gesture has recently become widespread among supporters of Islamism, particularly members of ISIS, though its use does not necessarily signal extremism.

In Arabic, the index or fore finger is called musabbiḥa (مُسَبِّحة), mostly used with the definite article: al-musabbiḥa (الْمُسَبِّحة). Sometimes also as-sabbāḥa (السَّبّاحة) is used. The Arabic verb سَبَّحَ (sabbaḥa), which has the same root as the Arabic word for index finger, means to praise or glorify God by saying: "Subḥāna Allāh" (سُبْحانَ الله).

The raised clenched fist, symbolizing unity in struggle, was popularized in the 19th century by the socialist, communist and anarchist movements, and is still used today.

In the United States, the raised fist was associated with the Black Power movement, symbolized in the 1968 Olympics Black Power salute; a clenched-fist salute is also proper in many African nations, including South Africa. However, the two salutes are somewhat different: in the Black Power salute, the arm is held straight, while in the salute of leftist movements the arm is bent slightly at the elbow.

Many different gestures are used throughout the world as simple greetings. In Western cultures the handshake is very common, though it has numerous subtle variations in the strength of grip, the vigour of the shake, the dominant position of one hand over the other, and whether or not the left hand is used.

Historically, when men normally wore hats out of doors, male greetings to people they knew, and sometimes those they did not, involved touching, raising slightly ("tipping"), or removing their hat in a variety of gestures, see hat tip. This basic gesture remained normal in very many situations from the Middle Ages until men typically ceased wearing hats in the mid-20th century. Hat-raising began with an element of recognition of superiority, where only the socially inferior party might perform it, but gradually lost this element; King Louis XIV of France made a point of at least touching his hat to all women he encountered. However the gesture was never used by women, for whom their head-covering included considerations of modesty. When a man was not wearing a hat he might touch his hair to the side of the front of his head to replicate a hat tipping gesture. This was typically performed by lower-class men to social superiors, such as peasants to the land-owner, and is known as "tugging the forelock", which still sometimes occurs as a metaphor for submissive behaviour.

In Europe, the formal style of upper-class greeting used by a man to a woman in the Early Modern Period was to hold the woman's presented hand (usually the right) with his right hand and kiss it while bowing, see hand-kissing and kissing hands. This style has not been widespread for a century or more. In cases of a low degree of intimacy, the hand is held but not kissed. The ultra-formal style, with the man's right knee on the floor, is now only used in marriage proposals, as a romantic gesture.

The Arabic term "salaam" (literally "peace", from the spoken greeting that accompanies the gesture), refers to the practice of placing the right palm on the heart, before and after a handshake.

A Chinese greeting, Bao Quan Li (抱拳礼 or "fist wrapping rite"), features the right fist placed in the palm of the left hand and both shaken back and forth two or three times; it may be accompanied by a head nod or bow. The gesture may be used on meeting and parting, and when offering thanks or apologies.

In India, it is common to see the Namaste greeting (or "Sat Sri Akal" for Sikhs) where the palms of the hands are pressed together and held near the heart with the head gently bowed.

Adab, meaning respect and politeness, is a hand gesture used as a Muslim greeting of south Asian Muslims, especially of Urdu-speaking communities of Uttar Pradesh, Hyderabadi Muslims, Bengali Muslims and Muhajir people of Pakistan. The gesture involves raising the right hand towards the face with palm inwards such that it is in front of the eyes and the finger tips are almost touching the forehead, as the upper torso is bent forward. It is typical for the person to say ""adab arz hai"", or just ""adab"". It is often answered with the same or the word ""Tasleem"" is said as an answer or sometimes it is answered with a facial gesture of acceptance.

In Indonesia, a nation with a huge variety of cultures and religions, many greetings are expressed, from the formalized greeting of the highly stratified and hierarchical Javanese to the more egalitarian and practical greetings of outer islands. Javanese, Batak and other ethnicities currently or formerly involved in the armed forces will salute a Government-employed superior, and follow with a deep bow from the waist or short nod of the head and a passing, loose handshake. Hand position is highly important; the superior's hand must be higher than the inferior's.
Muslim men will clasp both hands, palms together at the chest and utter the correct Islamic "slametan" (greeting) phrase, which may be followed by cheek-to-cheek contact, a quick hug or loose handshake. Pious Muslim women rotate their hands from a vertical to perpendicular prayer-like position in order to barely touch the finger tips of the male greeter and may opt out of the cheek-to-cheek contact.
If the male is an "Abdi Dalem" royal servant, courtier or particularly "peko-peko" (taken directly from Japanese to mean obsequious) or even a highly formal individual, he will retreat backwards with head downcast, the left arm crossed against the chest and the right arm hanging down, never showing his side or back to his superior. His head must always be lower than that of his superior.
Younger Muslim males and females will clasp their elder's or superior's outstretched hand to the forehead as a sign of respect and obeisance.
If a manual worker or a person with obviously dirty hands salutes or greets an elder or superior, he will show deference to his superior and avoid contact by bowing, touching the right forehead in a very quick salute or a distant "slamet" gesture.

The traditional Javanese "Sungkem" involves clasping the palms of both hands together, aligning the thumbs with the nose, turning the head downwards and bowing deeply, bending from the knees. In a royal presence, the one performing "sungkem" would kneel at the base of the throne.

A gesture called a "wai" is used in Thailand, where the hands are placed together palm to palm, approximately at nose level, while bowing. The "wai" is similar in form to the gesture referred to by the Japanese term "gassho" by Buddhists. In Thailand, the men and women would usually press two palms together and bow a little while saying "Sawadee ka" (female speaker) or "Sawadee krap" (male speaker).

Some cultures use hugs and kisses (regardless of the sex of the greeters), but those gestures show an existing degree of intimacy and are not used between total strangers. All of these gestures are being supplemented or completely displaced by the handshake in areas with large amounts of business contact with the West.

These bows indicate respect and acknowledgment of social rank, but do not necessarily imply obeisance.

An "obeisance" is a gesture not only of respect but also of submission. Such gestures are rarer in cultures that do not have strong class structures; citizens of the Western World, for example, often react with hostility to the idea of bowing to an authority figure. The distinction between a formally polite greeting and an obeisance is often hard to make; for example, "proskynesis" (from the words πρός "pros" (towards) and κυνέω "kyneo" (to kiss)) is described by the Greek researcher Herodotus of Halicarnassus, who lived in the 5th century BC in his "Histories" 1.134:

After his conquest of Persia, Alexander the Great introduced Persian etiquette into his own court, including the practice of proskynesis. Visitors, depending on their ranks, would have to prostrate themselves, bow to, kneel in front of, or kiss the king. His Greek countrymen objected to this practice, as they considered these rituals only suitable to the gods.

In countries with recognized social classes, bowing to nobility and royalty is customary. Standing bows of obeisance all involve bending forward from the waist with the eyes downcast, though variations in the placement of the arms and feet are seen. In western European cultures, women do not bow, they "curtsey" (a contraction of "courtesy" that became its own word), a movement in which one foot is moved back and the entire body lowered to a crouch while the head is bowed.

The European formal greeting used from men to women can be transformed into an obeisance gesture by holding the suzerain's hand with both hands. This kind of respect is due to kings, princes, sovereigns (in their kingdoms), archbishops (in their metropolitan province) or the Pope (everywhere). In ultra-formal ceremonies (a coronation, oath of allegiance or episcopal inauguration) the right knee shall touch the ground.

In South Asia traditions, obeisance also involves prostrating oneself before a king.

Many religious believers kneel in prayer, and some (Roman Catholics, and Anglicans) "genuflect", bending one knee to touch the ground, at various points during religious services; the Orthodox Christian equivalent is a deep bow from the waist, and as an especially solemn obeisance the Orthodox make prostrations, bending down on both knees and touching the forehead to the floor. Roman Catholics also employ prostrations on Good Friday and at ordinations. During Islamic prayer, a kneeling bow called "sajdah" is used, with forehead, nose, hands, knees, and toes all touching the ground. Jews bow from the waist many times during prayer. Four times during the Yom Kippur service, and once on each day of Rosh Hashanah, many Jews will kneel and then prostrate. With the Salvation Army, when becoming a soldier, at a christening or other official event, underneath the flag, a salute is often used. This involves holding the hand, palm forwards, with all the fingers held in a clenched fist position. The index finger is left raised pointing towards God, and the hand is often held at chest height, in a similar position to that of Girl Guides.

Hand salutes similar to those used in the military are rendered by the Drum Major of a marching band or drum corps just prior to beginning their performance (after the show announcer asks if the group is ready), following completion of the performance and at other appropriate times. In all cases the salute is rendered to the audience.

The classic "corps style" salute is often known as the "punch" type, where the saluting party will first punch their right arm straight forward from their body, arm parallel to the ground, hand in a fist, followed by the more traditional salute position with the right hand, left arm akimbo. Dropping the salute typically entails snapping the saluting hand to the side and clenching the fist, then dropping both arms to the sides.

In the US, a Drum Major carrying a large baton or mace will often salute by bringing the right hand, holding the mace with the head upward, to the left shoulder.

There are occasional, more flamboyant variations, such as the windmill action of the saluting arm given by the Madison Scouts drum major, or the running of the saluting hand around the brim of the hat worn by the Cavaliers drum major.

In the United Kingdom and the Commonwealth, civilians are not expected to salute. In the United Kingdom, certain civilians, such as officers of HM Revenue and Customs, salute the quarterdeck of Royal Navy vessels on boarding.

In the past most gentlemen in Britain wore hats, and it was customary to tip the hat to a lady in salutation, but this custom has long died out.

In the United States, civilians may salute the national flag by placing their right hand over their heart or by standing at attention during the playing of the national anthem or while reciting the U.S. Pledge of Allegiance, or when the flag is passing by, as in a parade. Men and boys remove their hats and other headgear during the salute; religious headdress (and military headdress worn by veterans in uniform, who are otherwise civilians) are exempt. The nature of the headgear determines whether it is held in the left or right hand, tucked under the left arm, etc. However, if it is held in the right hand, the headgear is not held over the heart but the hand is placed in the same position it would be if it were not holding anything.

The Defense Authorization Act of 2009, signed by President Bush, contained a provision that gave veterans and active-duty service members not in uniform the right to salute during the playing of the national anthem. Previous legislation authorized saluting when not in uniform during the raising, lowering and passing of the flag. However, because a salute is a form of communication protected by the Free Speech clause of the First Amendment, legislative authorization is not required for any civilian—veteran or non-veteran—to salute the U.S. flag. Civilians in some other countries, like Italy, South Africa, Afghanistan, Bosnia and Herzegovina, South Korea, Croatia, Poland, Kazakhstan, and Nigeria also render the same civilian salute as their U.S. counterparts when hearing their respective national anthems.

Many artefacts of popular culture have created military salutes for fictional purposes, more often than not with a cynical or sarcastic purpose.

In his 1953 comic book album "Le Dictateur et le Champignon", which is part of the "Spirou et Fantasio" series, Belgian artist Franquin creates a silly salute, used in a fictional Latin American country named Palombia. When saluting, subordinates of General Zantas must raise their hands over their heads, with the palm facing forward, then point to the top of their heads with their thumbs. Franquin repeats this idea in his 1957 comic book album "Z comme Zorglub", another episode of the "Spirou et Fantasio" series. Here, almighty science wizard Zorglub's conscripted soldiers salute their leader by pointing to their heads with their index fingers to cynically underline how much of a genius they consider him to be.

In the Marvel Comics universe members of the organisation Hydra salute in a similar way to a fascist salute but instead raise both hands with fists clenched. This is also accompanied by chanting "Hail Hydra".

In the 1987 parodic science fiction film "Spaceballs", directed by Mel Brooks, all subordinates of Supreme leader President Skroob salute him by first bending their forearms over their opposed hands as though they are about to give him the arm of honor salute, but at the last moment, use their raised hands to wave him good bye, rather than showing him the middle finger.

In the manga "Attack on Titan" the members of the armed forces (and sometimes civilians in a show of respect towards military) salute by bending their arms and placing their clenched fist over their hearts. The gesture, known as "offering hearts" is meant to demonstrate that the soldiers are willing to give their bodies and lives to protect humanity and to ensure its survival.

In the BBC TV science fiction comedy "Red Dwarf", Arnold J. Rimmer continually performs an elaborate special salute that he has invented for the Space Corps, in spite of the fact that he is not a member of the Corps. It consists of extending the hand out in front of the body, palm down and rotating it about the wrist five times (to represent the five rings of the Space Corps) followed by bringing the hand close to the head with the palm facing out.




</doc>
<doc id="28979" url="https://en.wikipedia.org/wiki?curid=28979" title="Hyoscine">
Hyoscine

Hyoscine, also known as scopolamine, is a medication used to treat motion sickness and postoperative nausea and vomiting. It is also sometimes used before surgery to decrease saliva. When used by injection, effects begin after about 20 minutes and last for up to 8 hours. It may also be used by mouth and as a skin patch.
Common side effects include sleepiness, blurred vision, dilated pupils, and dry mouth. It is not recommended in people with angle-closure glaucoma or bowel obstruction. It is unclear if use during pregnancy is safe; however, it appears to be safe during breastfeeding. Hyoscine is in the antimuscarinic family of medications and works by blocking some of the effects of acetylcholine within the nervous system.
Hyoscine was first written about in 1881 and started to be used for anesthesia around 1900. It is on the World Health Organization's List of Essential Medicines, the safest and most effective medicines needed in a health system. Hyoscine is produced by plants of the nightshade family which historically have been used as psychoactive drugs. The name "scopolamine" is derived from one type of nightshade known as "Scopolia" while the name "hyoscine" is derived from another type known as "Hyoscyamus niger".

Hyoscine has a number of uses in medicine, where it is used to treat the following:
It is sometimes used as a premedication, (especially to reduce respiratory tract secretions) in surgery, mostly commonly by injection.

Hyoscine crosses the placenta and is a United States pregnancy category C and Australian Category B1 medication, meaning a risk to the fetus cannot be ruled out. Sufficient studies in women and animals are not available to rule out harm, but existing studies have not shown increased risk. Drugs should be given only if the potential benefits justify the potential risk to the fetus. It may cause respiratory depression and/or neonatal hemorrhage when used during pregnancy. Transdermal hyoscine has been used as an adjunct to epidural anesthesia for Caesarean delivery without adverse CNS effects on the newborn. Except when used prior to Caesarean section, it should only be used during pregnancy if the benefit to the mother outweighs the potential risk to the fetus.

Hyoscine enters breast milk by secretion. Although no human studies exist to document the safety of hyoscine while nursing, the manufacturer recommends that caution be taken if hyoscine is administered to a breastfeeding woman.

The likelihood of experiencing adverse effects from hyoscine is increased in the elderly relative to younger people. This phenomenon is especially true for older people who are also on several other medications. It is recommended that hyoscine use should be avoided in this age group because of these potent anticholinergic adverse effects which have also been linked to an increased risk for dementia.

Adverse effect incidence:

Uncommon (0.1–1% incidence) adverse effects include:

Rare (<0.1% incidence) adverse effects include:

Unknown frequency adverse effects include:

Physostigmine is a cholinergic drug that readily crosses the blood-brain barrier, and has been used as an antidote to treat the central nervous system depression symptoms of a hyoscine overdose. Other than this supportive treatment, gastric lavage and induced emesis (vomiting) are usually recommended as treatments for oral overdoses. The symptoms of overdose include:

Due to interactions with metabolism of other drugs, hyoscine can cause significant unwanted side effects when taken with other medications. Specific attention should be paid to other medications in the same pharmacologic class as hyoscine, also known as anticholinergics. The following medications could potentially interact with the metabolism of hyoscine: analgesics/pain medications, ethanol, zolpidem, thiazide diuretics, buprenorphine, anticholinergic drugs such as tiotropium, etc.

Hyoscine can be taken by mouth, subcutaneously, ophthalmically and intravenously, as well as via a transdermal patch. The transdermal patch ("e.g.," Transderm Scōp) for prevention of nausea and motion sickness employs hyoscine base, and is effective for up to three days. The oral, ophthalmic, and intravenous forms have shorter half-lives and are usually found in the form hyoscine hydrobromide (for example in Scopace, soluble tablets or Donnatal).

NASA is currently developing a nasal administration method. With a precise dosage, the NASA spray formulation has been shown to work faster and more reliably than the oral form.

Although it is usually referred to as a nonspecific antimuscarinic, indirect evidence indicates m1-receptor subtype specificity.

Hyoscine is among the secondary metabolites of plants from Solanaceae (nightshade) family of plants, such as henbane, jimson weed ("Datura"), angel's trumpets ("Brugmansia"), and corkwood ("Duboisia").

The biosynthesis of hyoscine begins with the decarboxylation of L-ornithine to putrescine by ornithine decarboxylase. Putrescine is methylated to N-methylputrescine by putrescine N-methyltransferase.

A putrescine oxidase that specifically recognizes methylated putrescine catalyzes the deamination of this compound to 4-methylaminobutanal, which then undergoes a spontaneous ring formation to N-methyl-pyrrolium cation. In the next step, the pyrrolium cation condenses with acetoacetic acid yielding hygrine. No enzymatic activity could be demonstrated to catalyze this reaction. Hygrine further rearranges to tropinone.

Subsequently, tropinone reductase I converts tropinone to tropine which condenses with phenylalanine-derived phenyllactate to littorine. A cytochrome P450 classified as Cyp80F1 oxidizes and rearranges littorine to hyoscyamine aldehyde. In the final step, hyoscyamine undergoes epoxidation catalyzed by 6beta-hydroxyhyoscyamine epoxidase yielding hyoscine.
One of the earlier alkaloids isolated from plant sources, hyoscine has been in use in its purified forms (such as various salts, including hydrochloride, hydrobromide, hydroiodide and sulfate), since its isolation by the German scientist Albert Ladenburg in 1880, and as various preparations from its plant-based form since antiquity and perhaps prehistoric times. Following the description of the structure and activity of hyoscine by Ladenburg, the search for synthetic analogues of and methods for total synthesis of hyoscine and/or atropine in the 1930s and 1940s resulted in the discovery of diphenhydramine, an early antihistamine and the prototype of its chemical subclass of these drugs, and pethidine, the first fully synthetic opioid analgesic, known as Dolantin and Demerol amongst many other trade names.

In 1899, a Dr. Schneiderlin recommended the use of hyoscine and morphine for surgical anaesthesia and it started to be used sporadically for that purpose. The use of this combination in obstetric anesthesiology, was first proposed by Richard von Steinbuchel in 1902 then was picked up and further developed by Carl Gauss in Freiburg, Germany starting in 1903. The method came to be known as "Dämmerschlaf" ("twilight sleep") or the "Freiburg method". It spread rather slowly, and different clinics experimented with different dosages and ingredients; in 1915 The Canadian Medical Association Journal reported that "the method [was] really still in a state of development". It remained widely used in the US until the 1960s, when growing chemophobia and a desire for more natural childbirth led to its abandonment.

"Hyoscine hydrobromide" is the international nonproprietary name, and "scopolamine hydrobromide" is the United States Adopted Name. Other names include "levo-duboisine", "devil's breath" and "burundanga".

While it has been occasionally used recreationally for its hallucinogenic properties, the experiences are often unpleasant, mentally and physically. It is also physically dangerous, so repeated use is rare. In June 2008, more than 20 people were hospitalized with psychosis in Norway after ingesting counterfeit Rohypnol tablets containing hyoscine. In January 2018, 9 individuals were hospitalized in Perth, Western Australia, after reportedly ingesting hyoscine.

Historically, the various plants which produce hyoscine have been used psychoactively for spiritual reasons. When entheogenic preparations of these plants were utilized, hyoscine was considered to be the main psychoactive compound and was largely responsible for the hallucinogenic effects, particularly when the preparation was made into a topical ointment (most notably flying ointment). Hyoscine is reported to be the only active alkaloid within these plants that can effectively be absorbed through the skin to cause effects. Different recipes for these ointments were explored in European witchcraft at least as far back as the Early Modern period and included multiple ingredients to help with the transdermal absorption of hyoscine (such as animal fat) as well as other possible ingredients to counteract its noxious and dysphoric effects.

The effects of hyoscine were studied for use as a truth serum in interrogations in the early 20th century, but because of the side effects, investigations were dropped. In 2009, the Czechoslovak state security secret police were proven to have used hyoscine at least three times to obtain confessions from alleged antistate dissidents.

Claims that hyoscine is commonly used in crime have been described as "exaggerated" or even implausible. Powdered hyoscine, in a form referred to as 'Devil's breath' does not 'brainwash' or control people into being defrauded by their attackers but these alleged effects are most likely urban legends. Nevertheless, the drug is known to produce loss of memory following exposure and sleepiness, similar to the effect of benzodiazepines or alcohol poisoning. 

A travel advisory published by the United States Department of State in 2012 stated: "One common and particularly dangerous method that criminals use in order to rob a victim is through the use of drugs. The most common [in Colombia] has been hyoscine. Unofficial estimates put the number of annual hyoscine incidents in Colombia at approximately 50,000. Hyoscine can render a victim unconscious for 24 hours or more. In large doses, it can cause respiratory failure and death. It is most often administered in liquid or powder form in foods and beverages. The majority of these incidents occur in night clubs and bars, and usually men, perceived to be wealthy, are targeted by young, attractive women. It is recommended that, to avoid becoming a victim of hyoscine, a person should never accept food or beverages offered by strangers or new acquaintances, nor leave food or beverages unattended in their presence. Victims of hyoscine or other drugs should seek immediate medical attention."

Beside robberies it is also allegedly involved in express kidnappings and sexual assault. The "Hospital Clínic" in Barcelona introduced a protocol in 2008 to help medical workers identify cases, while Madrid hospitals adopted a similar working document in February 2015. "Hospital Clínic" has found little scientific evidence to support this use and relies on the victims' stories to reach any conclusion. Although poisoning by hyoscine appears quite often in the media as an aid for raping, kidnapping, killing or robbery, the effects of this drug and the way it is applied by criminals (transdermal injection, on playing cards and papers etc.) are often exaggerated, especially skin exposure, as the dose that can be absorbed by the skin is too low to have any effect. Hyoscine transdermal patches must be used for hours to days.

The name "burundanga" derives from being an extract of the "Brugmansia" plant.

Between 1998 and 2004, 13% of emergency room admissions for "poisoning with criminal intentions" in a clinic of Bogotá, Colombia, have been attributed to hyoscine, and 44% to benzodiazepines. Most commonly, the person has been poisoned by a robber who gave the victim a scopolamine-laced beverage, in the hope that the victim would become unconscious or unable to effectively resist the robbery.

Hyoscine is used as a research tool to study memory encoding. Initially, in human trials, relatively low doses of the muscarinic receptor antagonist, scopolamine, were found to induce temporary cognitive defects. Since then, scopolamine has become a standard drug for experimentally inducing cognitive defects in animals. Results in primates suggest that acetylcholine is involved in the encoding of new information into long term memory.

Hyoscine produces detrimental effects on short-term memory, memory acquisition, learning, visual recognition memory, visuospatial praxis, visuospatial memory, visuoperceptual function, verbal recall and psychomotor speed. However, scopolamine does not seem to impair recognition and memory retrieval. Acetylcholine projections in hippocampal neurons, which are vital in mediating long term potentiation, are inhibited by scopolamine. Hyoscine also inhibits cholinergic mediated glutamate release in hippocampal neurons which assist in depolarization, potentiation of action potential, and synaptic suppression. Hyoscine's effects on acetylcholine and glutamate release in the hippocampus favors retrieval dominant cognitive functioning. Hyoscine has been used to model the defects in cholinergic function for models of Alzheimer's, dementia, fragile X syndrome, and Down syndrome.

Hyoscine has also been investigated as a rapid-onset antidepressant, with a number of small studies finding positive results.




</doc>
<doc id="28981" url="https://en.wikipedia.org/wiki?curid=28981" title="Society for Creative Anachronism">
Society for Creative Anachronism

The Society for Creative Anachronism (SCA) is an international living history group with the aim of studying and recreating mainly Medieval European cultures and their histories before the 17th century. A quip often used within the SCA describes it as a group devoted to the Middle Ages "as they ought to have been", choosing to "selectively recreate the culture, choosing elements of the culture that interest and attract us". Founded in 1966, the non-profit educational corporation has over 30,000 paid members with about 60,000 total participants in the society (including members and non-member participants).

The SCA's roots can be traced to a backyard party of a UC Berkeley medieval studies graduate, the author Diana Paxson, in Berkeley, California, on May Day in 1966. The party began with a "Grand Tournament" in which the participants wore helmets, fencing masks, and usually some semblance of a costume, and sparred with each other using weapons such as plywood swords, padded maces, and fencing foils. It ended with a parade down Telegraph Avenue with everyone singing "Greensleeves". It was styled as a "protest against the 20th century". The SCA still measures dates within the society from the date of that party, calling the system "Anno Societatis" (Latin for "in the Year of the Society"). For example, 2009 May 1 to 2010 April 30 was A.S. XLIV (44). The name Berkeley Society for Creative Anachronism was coined by science fiction author Marion Zimmer Bradley, an early participant, when the nascent group needed an official name in order to reserve a park for a tournament. "Berkeley" was dropped as the group expanded.

Three more co-founders are mentioned by Douglas Martin in the New York Times Obituaries of August 3, 2001 (p.A23, "Poul Anderson, Science Fiction Novelist, Dies at 74"): "[Anderson and Karen Kruse] moved to San Francisco and were married...They and their daughter, Astrid...founded the Society for Creative Anachronism, which...has spread nationwide."
In 1968, Bradley moved to Staten Island, New York and founded the Kingdom of the East, holding a tournament that summer to determine the first Eastern King of the SCA. That September, a tournament was held at the 26th World Science Fiction Convention, which was in Berkeley that year. The SCA had produced a book for the convention called "A Handbook for the Current Middle Ages", which was a how-to book for people wanting to start their own SCA chapters. Convention goers purchased the book and the idea spread. Soon, other local chapters began to form. In October 1968, the SCA was incorporated as a 501(c)(3) non-profit corporation in California. By the end of 1969, the SCA's three original kingdoms had been established: West Kingdom, East, and Middle. All SCA kingdoms trace their roots to these original three. The number of SCA kingdoms has continued to grow by the expansion and division of existing kingdoms; for example, the kingdoms now called the Outlands, Artemisia, Ansteorra, Gleann Abhann, Meridies, and Trimaris all are made up of lands originally belonging to the fourth kingdom, Atenveldt, which began as a branch of the West Kingdom.

In 2012, SCA agreed to pay $1.3 million to settle a lawsuit brought on behalf of 11 victims of child sexual abuse. The abuse was committed in Pennsylvania at the private residence of Ben Schragger, who pleaded guilty to criminal charges in 2004. Schragger was a member of SCA at the time of the abuse. His membership was suspended on his arrest and permanently revoked after his plea. The lawsuit contended that the SCA had not conducted a background check on Schragger, though at the time the organization did not perform background checks in general and there is no legal requirement to do so.

The SCA engages in a broad range of activities, including SCA armoured combat, SCA fencing, archery, equestrian activities, feasting, medieval dance and recreating medieval arts and sciences, including a broad range of crafts as well as medieval music and theatre. Other activities include the study and practice of heraldry and scribal arts (calligraphy and illumination). Members are afforded opportunities to register a medieval personal name and coat of arms (often colloquially called a "device" in SCA parlance). SCA scribes produce illuminated scrolls to be given by SCA royalty as awards for various achievements.

Most local groups in the SCA hold weekly fighter practices, and many also hold regular archery practices, dance practices, A&S (Arts & Science) nights and other regular gatherings. Some kingdoms and regions also have occasional war practices, where fighters practice formations and group tactics in preparation for large scale "war" events.

The research and approach by members of the SCA toward the recreation of history has led to new discoveries about medieval life.

Some local groups participate in nearby Renaissance fairs, though the main focus of activity is organized through the SCA's own events. Each kingdom in the SCA runs its own schedule of events which are announced in the kingdom newsletter (and usually posted on the kingdom web site), but some of the largest SCA-sanctioned events, called "wars", attract members from many kingdoms. Pennsic War, fought annually between the East Kingdom and Middle Kingdom, is the biggest event in the SCA. The Estrella War has been held for over thirty years, mainly between two large regional SCA groups: the Kingdom of Atenveldt and the Kingdom of the Outlands. Most Estrella wars are held near Phoenix, Arizona in late February and last around 7–9 days. Several thousand people attend each year, some from as far as Sweden, Germany, France, Italy, Greece, and Australia. Other annual SCA wars include Gulf Wars in Gleann Abhann (formerly Meridies), Great Western War in Caid, War of the Lillies in Calontir and others. Other annual or semi-annual Kingdom-level events held analogously by most or all SCA kingdoms include Crown Tournament, Coronation, Kingdom Arts and Sciences competition and Queen's Prize. Additionally, most baronies in the SCA have their own traditional annual events such as Baronial Arts and Sciences competition, a championship tournament, and often a Yule or Twelfth Night feast. Various SCA groups also sometimes host collegia or symposia, where members gather for a raft of classes on various medieval arts and sciences and other SCA-related topics.

The minimum standard for attendance at an SCA event is "an attempt at pre-17th century clothing", and there is a general goal of maintaining a historical atmosphere. However, SCA members will use modern elements when necessary for personal comfort, medical needs, or to promote safety (e.g. wearing prescription eye-wear, using rattan for swords or shear thickening substances for padding). Unlike some other living history groups, most SCA gatherings do not reenact a specific time or place in history, leaving members free to dress as any culture within the SCA's time period.

The SCA produces two quarterly publications, "The Compleat Anachronist" and "Tournaments Illuminated", and each kingdom publishes a monthly newsletter.

"The Compleat Anachronist" is a quarterly monographic series, each issue focused on a specific topic related to the period of circa 600–1600 in European history. Issues are written by SCA members and have covered a wide range of topics.

"Tournaments Illuminated" is a quarterly magazine, each issue covering a range of topics and including several features such as news, a humor column, book reviews, war reports and various articles on SCA-related topics of interest.

The SCA is incorporated as a 501(c)(3) non-profit corporation in California, with its current headquarters in the city of Milpitas. It is headed by a board of directors, each of whom is nominated by the membership of the SCA, selected by sitting directors, and elected to serve for 3.5 years. Each director serves as an ombudsman for various kingdoms and society officers. The BoD, as it is called, is responsible for handling the corporate affairs of the SCA and is also in charge of certain disciplinary actions, such as revoking the membership status of participants who have broken Corpora regulations or modern law while participating in SCA activities.
Because the SCA now has groups all over the world, it has also been incorporated in other countries, e.g. SCAA in Australia, SCANZ in New Zealand, SKA Nordmark in Sweden, SKA in Finland, and the UK CIC which covers both the UK and Ireland. These affiliated bodies work with the US board of directors with regards to societal issues, but make all decisions affected by local law independently of the US parent body. Although they agree to work in unity with the US SCA board of directors, they are autonomous and are not bound by any ruling of the US body.

The SCA is divided into administrative regions which it calls "kingdoms". Smaller branches within those kingdoms include "Principalities", "Regions", "Baronies", and "Provinces", and local chapters are known as "Cantons", "Ridings", "Shires", "Colleges", "Strongholds", and "Ports". Kingdoms, Principalities, and Baronies have ceremonial rulers who preside over activities and issue awards to individuals and groups. Colleges, Strongholds, and Ports are local chapters (like a shire) that are associated with an institution, such as a school, military base, or even a military ship at sea.

All SCA branches are organized in descending order as follows:


Groups are active all over the United States, Canada, Europe, Australia, South Africa, and New Zealand, with scattered groups elsewhere, including China, Panama and Thailand. At one time there was even a group on the aircraft carrier USS "Nimitz", known as the "Shire of Curragh Mor" (anglicized Irish for "Big Boat"), and the shire's arms played on the "Nimitz's" ship's badge. There is also an active chapter in South Korea, the Stronghold of Warrior's Gate, with a mix of active duty military personnel from the several services and military-connected civilians. There are also non-territorial, usually called "households", which are not part of the Society's formal organization, the largest of which is the Mongol Empire-themed Great Dark Horde.

The twenty SCA Kingdoms and the geographic areas they cover are (in order of founding)


The Society as a whole, each kingdom, and each local group within a kingdom, all have a standard group of officers with titles loosely based on medieval equivalents.

Members of the SCA study and take part in a variety of activities, including combat and chivalry, archery, heraldry, equestrian activities, costuming, cooking, metalwork, woodworking, leathercrafting, music, dance, calligraphy, fiber arts, and others as practiced during the member's time period.

To aid historical recreation, participants in the SCA create historically plausible characters known individually as "personae". To new members, a persona can simply be a costume and a name used for weekend events, while other members may study and create an elaborate personal history. The goal of a well-crafted persona is a historically accurate person who might have lived in a particular historical time and place. The SCA has onomastic students who assist members in creating an appropriate persona name. The SCA rules state that: "We allow elements and patterns for personal names from beyond Europe, but we require them to be from cultures that were known to medieval and Renaissance Europeans or whose members might reasonably have traveled to Europe". So, while less common, there are members with Saracen, Chinese, Japanese or Native American personas.

In addition, claiming to be a specific historical individual, especially a very familiar one (e.g. Genghis Khan, Julius Caesar, Henry Plantagenet, Queen Elizabeth I), is not permitted. Likewise, one is not allowed to claim the "persona" of a fellow SCA member, alive or dead. Nor is one allowed to take on the persona of a sufficiently familiar fictional character (e.g. Robin of Locksley/Robin Hood).

A major dimension to the SCA is its internal system of Heraldry. Any member of the society may apply to register a name and device for their persona, which are checked by the heralds for uniqueness and period authenticity, before being blazoned and recorded in the society's Armorial. The system has evolved since the formation of the society; and now has three Sovereigns of Arms, with "Principal Heralds" for each Kingdom, who oversee deputy officers for matters such as heraldic education and processing registrations, and local officers (generally one for each local chapter) who assist the local participants. In addition to design of arms, heralds in the Society also provide services such as voice heraldry (similar to a master of ceremonies) at tournaments and official functions, and organizing tournament brackets or "lists."

The SCA has ceremonial rulers chosen by winning tournaments (Kings/Queens, Princes/Princesses) in SCA armoured combat. Barons and Baronesses are appointed by Royalty, although some baronies hold elections or competitions to choose their preferred Baron and/or Baroness. One of the primary functions of state for reigning monarchs is to recognize participant achievement through awards. Most awards denote excellence in a specific pursuit such as local service, arts and sciences, and combat. Some awards change the precedence and title of the recipient, giving him or her the privilege of being known as "Lord"/"Lady", "Baron", "Duchess", "Master", and so forth. High level awards are often given with the consultation of the other people who have received the award, such as peerages and consulting orders. The Crown has some authority over other matters relating to leadership, but the extent of this varies from kingdom to kingdom.

Each SCA kingdom is "ruled" by a king and queen chosen by winning a Crown Tournament in armored combat. Corpora require this to be held as a "properly constituted armored combat" tournament. The winner of the Crown Tournament and his/her Consort are styled "Crown Prince and Princess" and serve an advisory period (three to six months, depending upon the scheduling of the Crown Tournament) under the current King and Queen prior to acceding to the throne and ruling in their turn.

This selection method is not based on how actual medieval monarchs were chosen, as there is no record of one being selected by Tournament combat in this manner. There are, however, literary and historical bases for the custom, most famously the tournament in Sir Walter Scott's "Ivanhoe". In the Middle Ages, there were a number of different "mock king" games, some of which involved some form of combat, such as King of the Mountain or the King of Archers. In the 17th century the Cotswold Games were developed, the winner of which was declared to be "king". Also, the medieval sagas contain accounts of uniting petty kingdoms under a single king through "actual" combat.

The SCA's first event did not choose a "king". Fighters vied for the right to declare their ladies (only men fought at the first event) "fairest", later called the "Queen of Love and Beauty".

The highest ranking titles in the SCA belong to the royalty, followed by the former royalty. Former kings and queens become counts and countesses (dukes and duchesses if they have reigned more than once), and former princes and princesses of Principalities become viscounts and viscountesses. This system is not historically based, but was developed out of practical necessity early in the Society's history.

Directly beneath this "landed" nobility (current and former royalty) rank the highest awards, the Peerages. The SCA has four orders of peerage: the Order of the Chivalry, awarded for skill at arms in Armored Combat; the Order of the Laurel, awarded for skill in the arts and sciences; the Order of the Pelican, awarded for outstanding service to the Society; and the Order of the Masters of Defense, awarded for skill at arms in Rapier Combat. In Several of the Kingdoms the Order of the Rose, made up of former Consorts, is considered a peerage equal to the other four.

Peerages are bestowed by the Crown (the Sovereign and Consort) of a Kingdom. In most cases, this is done with the consent of the members of a given peerage, often at their suggestion. The Society's Bylaws state that "the Crown may elevate subjects to the Peerage by granting membership in one of the Orders conferring a Patent of Arms, after consultation with the members of the Order within the Kingdom, and in accordance with the laws and customs of the kingdom. Restriction: to advance a candidate to the Order of the Chivalry, a Knight of the Society (usually the King) must bestow the accolade".

In May 1999, "The Onion" ran a front-page article headlined "Society For Creative Anachronism Seizes Control Of Russia" featuring photos of actual SCA participants from the Barony of Jaravellir (Madison, Wisconsin).

Members of the SCA are given pivotal roles in S.M. Stirling's Emberverse series, where their skills in pre-industrial technology and warfare become invaluable in helping humanity adapt when all modern technology (including firearms) ceases working.

In his conclusion to the "Space Odyssey" series, ""; Arthur C. Clarke portrays the SCA as still being active in the year 3001.

The novel "Murder at the War" ("Knightfall" in paperback edition) by Mary Monica Pulver is a murder mystery set entirely at the SCA's largest annual event, Pennsic War.

In David Weber's science fiction novel "Honor Among Enemies", main character Honor Harrington mentions that her uncle is a member of the SCA and that he taught her to shoot from the hip (the time the SCA covers having been moved up to the 19th century in the future era in which the novel is set, to include cowboy and Civil War reenactors).

In Christopher Stasheff's "Warlock" series the inhabitants of the planet Gramarye are revealed to be descended from SCA participants. A prequel, "Escape Velocity", describes how the SCAdians first came to Gramarye, and how lands were assigned to the royal peers.

In "Ariel" (1983), a post-apocalyptic fantasy by Steven R. Boyett, technology suddenly stops working and sorcery and sword fight take over. Several characters who are former SCA members attribute their survival to their SCA experience.

The fantasy novel "The Folk of the Air" by Peter S. Beagle was written after the author attended a few early SCA events "circa" 1968; but he has repeatedly stated that he then studiously avoided any contact with the actual SCA itself for almost two decades, so that his description of a fictitious "League for Archaic Pleasures" would not be "contaminated" by contact with the actual real-life organization.

In "Number of the Beast", Robert A. Heinlein portrayed an SCA tournament where live weapons were used and the battles actually fought to the 'death'. The defeated combatants were either transported to an alternate reality where medical technology was advanced enough that they could be revived from any wound or transported to the alternate reality that was Valhalla. The contestants' desires were placed in sealed envelopes prior to the tournament, which were destroyed if the competitor won and obeyed if a competitor lost.

In John Ringo's The Council Wars science-fiction series, characters with SCA or SCA-like experience help their society recover from a catastrophic loss of technology.




</doc>
<doc id="28982" url="https://en.wikipedia.org/wiki?curid=28982" title="Snowball Earth">
Snowball Earth

The Snowball Earth hypothesis proposes that during one or more of Earth's icehouse climates, Earth's surface became entirely or nearly entirely frozen, sometime earlier than 650 Mya (million years ago). Proponents of the hypothesis argue that it best explains sedimentary deposits generally regarded as of glacial origin at tropical palaeolatitudes and other enigmatic features in the geological record. Opponents of the hypothesis contest the implications of the geological evidence for global glaciation and the geophysical feasibility of an ice- or slush-covered ocean and emphasize the difficulty of escaping an all-frozen condition. A number of unanswered questions remain, including whether the Earth was a full snowball, or a "slushball" with a thin equatorial band of open (or seasonally open) water.

The snowball-Earth episodes are proposed to have occurred before the sudden radiation of multicellular bioforms, known as the Cambrian explosion. The most recent snowball episode may have triggered the evolution of multicellularity. Another, much earlier and longer snowball episode, the Huronian glaciation, which would have occurred 2400 to 2100 Mya, may have been triggered by the first appearance of oxygen in the atmosphere, the "Great Oxygenation Event".

Long before the idea of a global glaciation was established, a series of discoveries began to accumulate evidence for ancient Precambrian glaciations. The first of these discoveries was published in 1871 by J. Thomson who found ancient glacier-reworked material (tillite) in Islay, Scotland. Similar findings followed in Australia (1884) and India (1887). A fourth and very illustrative finding that came to be known as "Reusch's Moraine" was reported by Hans Reusch in northern Norway in 1891. Many other findings followed, but their understanding was hampered by the rejection of continental drift.

Sir Douglas Mawson (1882–1958), an Australian geologist and Antarctic explorer, spent much of his career studying the Neoproterozoic stratigraphy of South Australia, where he identified thick and extensive glacial sediments and late in his career speculated about the possibility of global glaciation.

Mawson's ideas of global glaciation, however, were based on the mistaken assumption that the geographic position of Australia, and those of other continents where low-latitude glacial deposits are found, have remained constant through time. With the advancement of the continental drift hypothesis, and eventually plate tectonic theory, came an easier explanation for the glaciogenic sediments—they were deposited at a time when the continents were at higher latitudes.

In 1964, the idea of global-scale glaciation reemerged when W. Brian Harland published a paper in which he presented palaeomagnetic data showing that glacial tillites in Svalbard and Greenland were deposited at tropical latitudes. From this palaeomagnetic data, and the sedimentological evidence that the glacial sediments interrupt successions of rocks commonly associated with tropical to temperate latitudes, he argued for an ice age that was so extreme that it resulted in the deposition of marine glacial rocks in the tropics.

In the 1960s, Mikhail Budyko, a Russian climatologist, developed a simple energy-balance climate model to investigate the effect of ice cover on global climate. Using this model, Budyko found that if ice sheets advanced far enough out of the polar regions, a feedback loop ensued where the increased reflectiveness (albedo) of the ice led to further cooling and the formation of more ice, until the entire Earth was covered in ice and stabilized in a new ice-covered equilibrium.

While Budyko's model showed that this ice-albedo stability could happen, he concluded that it had in fact never happened, because his model offered no way to escape from such a feedback loop. In 1971, Aron Faegre, an American physicist, showed that a similar energy-balance model predicted three stable global climates, one of which was snowball earth.

This model introduced Edward Norton Lorenz's concept of intransitivity indicating that there could be a major jump from one climate to another, including to snowball earth.

The term "snowball Earth" was coined by Joseph Kirschvink in a short paper published in 1992 within a lengthy volume concerning the biology of the Proterozoic eon. The major contributions from this work were: (1) the recognition that the presence of banded iron formations is consistent with such a global glacial episode, and (2) the introduction of a mechanism by which to escape from a completely ice-covered Earth—specifically, the accumulation of CO from volcanic outgassing leading to an ultra-greenhouse effect.

Franklyn Van Houten's discovery of a consistent geological pattern in which lake levels rose and fell is now known as the "Van Houten cycle". His studies of phosphorus deposits and banded iron formations in sedimentary rocks made him an early adherent of the "snowball Earth" hypothesis postulating that the planet's surface froze more than 650 million years ago.

Interest in the notion of a snowball Earth increased dramatically after Paul F. Hoffman and his co-workers applied Kirschvink's ideas to a succession of Neoproterozoic sedimentary rocks in Namibia and elaborated upon the hypothesis in the journal "Science" in 1998 by incorporating such observations as the occurrence of cap carbonates.

In 2010, Francis MacDonald reported evidence that Rodinia was at equatorial latitude during the Cryogenian period with glacial ice at or below sea level, and that the associated Sturtian glaciation was global.

The snowball Earth hypothesis was originally devised to explain geological evidence for the apparent presence of glaciers at tropical latitudes. According to modelling, an ice–albedo feedback would result in glacial ice rapidly advancing to the equator once the glaciers spread to within 25° to 30° of the equator. Therefore, the presence of glacial deposits within the tropics suggests global ice cover.

Critical to an assessment of the validity of the theory, therefore, is an understanding of the reliability and significance of the evidence that led to the belief that ice ever reached the tropics. This evidence must prove two things:

During a period of global glaciation, it must also be demonstrated that glaciers were active at different global locations at the same time, and that no other deposits of the same age are in existence.

This last point is very difficult to prove. Before the Ediacaran, the biostratigraphic markers usually used to correlate rocks are absent; therefore there is no way to prove that rocks in different places across the globe were deposited at precisely the same time. The best that can be done is to estimate the age of the rocks using radiometric methods, which are rarely accurate to better than a million years or so.

The first two points are often the source of contention on a case-to-case basis. Many glacial features can also be created by non-glacial means, and estimating the approximate latitudes of landmasses even as recently as can be riddled with difficulties.

The snowball Earth hypothesis was first posited to explain what were then considered to be glacial deposits near the equator. Since tectonic plates move slowly over time, ascertaining their position at a given point in Earth's long history is not easy. In addition to considerations of how the recognizable landmasses could have fit together, the latitude at which a rock was deposited can be constrained by palaeomagnetism.

When sedimentary rocks form, magnetic minerals within them tend to align themselves with the Earth's magnetic field. Through the precise measurement of this palaeomagnetism, it is possible to estimate the latitude (but not the longitude) where the rock matrix was formed. Palaeomagnetic measurements have indicated that some sediments of glacial origin in the Neoproterozoic rock record were deposited within 10 degrees of the equator, although the accuracy of this reconstruction is in question. This palaeomagnetic location of apparently glacial sediments (such as dropstones) has been taken to suggest that glaciers extended from land to sea level in tropical latitudes at the time the sediments were deposited. It is not clear whether this implies a global glaciation, or the existence of localized, possibly land-locked, glacial regimes. Others have even suggested that most data do not constrain any glacial deposits to within 25° of the equator.

Skeptics suggest that the palaeomagnetic data could be corrupted if Earth's ancient magnetic field was substantially different from today's. Depending on the rate of cooling of Earth's core, it is possible that during the Proterozoic, the magnetic field did not approximate a simple dipolar distribution, with north and south magnetic poles roughly aligning with the planet's axis as they do today. Instead, a hotter core may have circulated more vigorously and given rise to 4, 8 or more poles. Palaeomagnetic data would then have to be re-interpreted, as the sedimentary minerals could have aligned pointing to a 'West Pole' rather than the North Pole. Alternatively, Earth's dipolar field could have been oriented such that the poles were close to the equator. This hypothesis has been posited to explain the extraordinarily rapid motion of the magnetic poles implied by the Ediacaran palaeomagnetic record; the alleged motion of the north pole would occur around the same time as the Gaskiers glaciation.

Another weakness of reliance on palaeomagnetic data is the difficulty in determining whether the magnetic signal recorded is original, or whether it has been reset by later activity. For example, a mountain-building orogeny releases hot water as a by-product of metamorphic reactions; this water can circulate to rocks thousands of kilometers away and reset their magnetic signature. This makes the authenticity of rocks older than a few million years difficult to determine without painstaking mineralogical observations. Moreover, further evidence is accumulating that large-scale remagnetization events have taken place which may necessitate revision of the estimated positions of the palaeomagnetic poles.

There is currently only one deposit, the Elatina deposit of Australia, that was indubitably deposited at low latitudes; its depositional date is well-constrained, and the signal is demonstrably original.

Sedimentary rocks that are deposited by glaciers have distinctive features that enable their identification. Long before the advent of the "snowball Earth" hypothesis many Neoproterozoic sediments had been interpreted as having a glacial origin, including some apparently at tropical latitudes at the time of their deposition. However, it is worth remembering that many sedimentary features traditionally associated with glaciers can also be formed by other means. Thus the glacial origin of many of the key occurrences for snowball Earth has been contested.
As of 2007, there was only one "very reliable"—still challenged—datum point identifying tropical tillites, which makes statements of equatorial ice cover somewhat presumptuous. However, evidence of sea-level glaciation in the tropics during the Sturtian is accumulating.
Evidence of possible glacial origin of sediment includes:

It appears that some deposits formed during the snowball period could only have formed in the presence of an active hydrological cycle. Bands of glacial deposits up to 5,500 meters thick, separated by small (meters) bands of non-glacial sediments, demonstrate that glaciers melted and re-formed repeatedly for tens of millions of years; solid oceans would not permit this scale of deposition. It is considered possible that ice streams such as seen in Antarctica today could have caused these sequences.
Further, sedimentary features that could only form in open water (for example: wave-formed ripples, far-traveled ice-rafted debris and indicators of photosynthetic activity) can be found throughout sediments dating from the snowball-Earth periods. While these may represent "oases" of meltwater on a completely frozen Earth, computer modelling suggests that large areas of the ocean must have remained ice-free; arguing that a "hard" snowball is not plausible in terms of energy balance and general circulation models.

There are two stable isotopes of carbon in sea water: carbon-12 (C) and the rare carbon-13 (C), which makes up about 1.109 percent of carbon atoms.

Biochemical processes, of which photosynthesis is one, tend to preferentially incorporate the lighter C isotope. Thus ocean-dwelling photosynthesizers, both protists and algae, tend to be very slightly depleted in C, relative to the abundance found in the primary volcanic sources of Earth's carbon. Therefore, an ocean with photosynthetic life will have a lower C/C ratio within organic remains, and a higher ratio in corresponding ocean water. The organic component of the lithified sediments will remain very slightly, but measurably, depleted in C.

During the proposed episode of snowball Earth, there are rapid and extreme negative excursions in the ratio of C to C. Close analysis of the timing of C 'spikes' in deposits across the globe allows the recognition of four, possibly five, glacial events in the late Neoproterozoic.

Banded iron formations (BIF) are sedimentary rocks of layered iron oxide and iron-poor chert. In the presence of oxygen, iron naturally rusts and becomes insoluble in water. The banded iron formations are commonly very old and their deposition is often related to the oxidation of the Earth's atmosphere during the Palaeoproterozoic era, when dissolved iron in the ocean came in contact with photosynthetically produced oxygen and precipitated out as iron oxide.

The bands were produced at the tipping point between an anoxic and an oxygenated ocean. Since today's atmosphere is oxygen-rich (nearly 21% by volume) and in contact with the oceans, it is not possible to accumulate enough iron oxide to deposit a banded formation. The only extensive iron formations that were deposited after the Palaeoproterozoic (after 1.8 billion years ago) are associated with Cryogenian glacial deposits.

For such iron-rich rocks to be deposited there would have to be anoxia in the ocean, so that much dissolved iron (as ferrous oxide) could accumulate before it met an oxidant that would precipitate it as ferric oxide. For the ocean to become anoxic it must have limited gas exchange with the oxygenated atmosphere. Proponents of the hypothesis argue that the reappearance of BIF in the sedimentary record is a result of limited oxygen levels in an ocean sealed by sea-ice, while opponents suggest that the rarity of the BIF deposits may indicate that they formed in inland seas.

Being isolated from the oceans, such lakes could have been stagnant and anoxic at depth, much like today's Black Sea; a sufficient input of iron could provide the necessary conditions for BIF formation. A further difficulty in suggesting that BIFs marked the end of the glaciation is that they are found interbedded with glacial sediments. BIFs are also strikingly absent during the Marinoan glaciation.

Around the top of Neoproterozoic glacial deposits there is commonly a sharp transition into a chemically precipitated sedimentary limestone or dolomite metres to tens of metres thick. These cap carbonates sometimes occur in sedimentary successions that have no other carbonate rocks, suggesting that their deposition is result of a profound aberration in ocean chemistry.
These cap carbonates have unusual chemical composition, as well as strange sedimentary structures that are often interpreted as large ripples.
The formation of such sedimentary rocks could be caused by a large influx of positively charged ions, as would be produced by rapid weathering during the extreme greenhouse following a snowball Earth event. The isotopic signature of the cap carbonates is near −5 ‰, consistent with the value of the mantle—such a low value is usually/could be taken to signify an absence of life, since photosynthesis usually acts to raise the value; alternatively the release of methane deposits could have lowered it from a higher value, and counterbalance the effects of photosynthesis.

The precise mechanism involved in the formation of cap carbonates is not clear, but the most cited explanation suggests that at the melting of a snowball Earth, water would dissolve the abundant from the atmosphere to form carbonic acid, which would fall as acid rain. This would weather exposed silicate and carbonate rock (including readily attacked glacial debris), releasing large amounts of calcium, which when washed into the ocean would form distinctively textured layers of carbonate sedimentary rock. Such an abiotic "cap carbonate" sediment can be found on top of the glacial till that gave rise to the snowball Earth hypothesis.

However, there are some problems with the designation of a glacial origin to cap carbonates. Firstly, the high carbon dioxide concentration in the atmosphere would cause the oceans to become acidic, and dissolve any carbonates contained within—starkly at odds with the deposition of cap carbonates. Further, the thickness of some cap carbonates is far above what could reasonably be produced in the relatively quick deglaciations. The cause is further weakened by the lack of cap carbonates above many sequences of clear glacial origin at a similar time and the occurrence of similar carbonates within the sequences of proposed glacial origin. An alternative mechanism, which may have produced the Doushantuo cap carbonate at least, is the rapid, widespread release of methane. This accounts for incredibly low—as low as −48 ‰— values—as well as unusual sedimentary features which appear to have been formed by the flow of gas through the sediments.

Isotopes of the element boron suggest that the pH of the oceans dropped dramatically before and after the Marinoan glaciation.
This may indicate a buildup of carbon dioxide in the atmosphere, some of which would dissolve into the oceans to form carbonic acid. Although the boron variations may be evidence of extreme climate change, they need not imply a global glaciation.

Earth's surface is very depleted in the element iridium, which primarily resides in the Earth's core. The only significant source of the element at the surface is cosmic particles that reach Earth. During a snowball Earth, iridium would accumulate on the ice sheets, and when the ice melted the resulting layer of sediment would be rich in iridium. An iridium anomaly has been discovered at the base of the cap carbonate formations, and has been used to suggest that the glacial episode lasted for at least 3 million years, but this does not necessarily imply a "global" extent to the glaciation; indeed, a similar anomaly could be explained by the impact of a large meteorite.

Using the ratio of mobile cations to those that remain in soils during chemical weathering (the chemical index of alteration), it has been shown that chemical weathering varied in a cyclic fashion within a glacial succession, increasing during interglacial periods and decreasing during cold and arid glacial periods. This pattern, if a true reflection of events, suggests that the "snowball Earths" bore a stronger resemblance to Pleistocene ice age cycles than to a completely frozen Earth.

In addition, glacial sediments of the Port Askaig Tillite Formation in Scotland clearly show interbedded cycles of glacial and shallow marine sediments. The significance of these deposits is highly reliant upon their dating. Glacial sediments are difficult to date, and the closest dated bed to the Portaskaig group is 8 km stratigraphically above the beds of interest. Its dating to 600 Ma means the beds can be tentatively correlated to the Sturtian glaciation, but they may represent the advance or retreat of a snowball Earth.

The initiation of a snowball Earth event would involve some initial cooling mechanism, which would result in an increase in Earth's coverage of snow and ice. The increase in Earth's coverage of snow and ice would in turn increase Earth's albedo, which would result in positive feedback for cooling. If enough snow and ice accumulates, run-away cooling would result. This positive feedback is facilitated by an equatorial continental distribution, which would allow ice to accumulate in the regions closer to the equator, where solar radiation is most direct.

Many possible triggering mechanisms could account for the beginning of a snowball Earth, such as the eruption of a supervolcano, a reduction in the atmospheric concentration of greenhouse gases such as methane and/or carbon dioxide, changes in Solar energy output, or perturbations of Earth's orbit. Regardless of the trigger, initial cooling results in an increase in the area of Earth's surface covered by ice and snow, and the additional ice and snow reflects more Solar energy back to space, further cooling Earth and further increasing the area of Earth's surface covered by ice and snow. This positive feedback loop could eventually produce a frozen equator as cold as modern Antarctica.

Global warming associated with large accumulations of carbon dioxide in the atmosphere over millions of years, emitted primarily by volcanic activity, is the proposed trigger for melting a snowball Earth. Due to positive feedback for melting, the eventual melting of the snow and ice covering most of Earth's surface would require as little as a millennium.

A tropical distribution of the continents is, perhaps counter-intuitively, necessary to allow the initiation of a snowball Earth.
Firstly, tropical continents are more reflective than open ocean, and so absorb less of the Sun's heat: most absorption of Solar energy on Earth today occurs in tropical oceans.

Further, tropical continents are subject to more rainfall, which leads to increased river discharge—and erosion.
When exposed to air, silicate rocks undergo weathering reactions which remove carbon dioxide from the atmosphere. These reactions proceed in the general form: Rock-forming mineral + CO + HO → cations + bicarbonate + SiO. An example of such a reaction is the weathering of wollastonite:

The released calcium cations react with the dissolved bicarbonate in the ocean to form calcium carbonate as a chemically precipitated sedimentary rock. This transfers carbon dioxide, a greenhouse gas, from the air into the geosphere, and, in steady-state on geologic time scales, offsets the carbon dioxide emitted from volcanoes into the atmosphere.

As of 2003, a precise continental distribution during the Neoproterozoic was difficult to establish because there were too few suitable sediments for analysis. Some reconstructions point towards polar continents—which have been a feature of all other major glaciations, providing a point upon which ice can nucleate. Changes in ocean circulation patterns may then have provided the trigger of snowball Earth.

Additional factors that may have contributed to the onset of the Neoproterozoic snowball include the introduction of atmospheric free oxygen, which may have reached sufficient quantities to react with methane in the atmosphere, oxidizing it to carbon dioxide, a much weaker greenhouse gas, and a younger—thus fainter—Sun, which would have emitted 6 percent less radiation in the Neoproterozoic.

Normally, as Earth gets colder due to natural climatic fluctuations and changes in incoming solar radiation, the cooling slows these weathering reactions. As a result, less carbon dioxide is removed from the atmosphere and Earth warms as this greenhouse gas accumulates—this 'negative feedback' process limits the magnitude of cooling. During the Cryogenian period, however, Earth's continents were all at tropical latitudes, which made this moderating process less effective, as high weathering rates continued on land even as Earth cooled. This let ice advance beyond the polar regions. Once ice advanced to within 30° of the equator, a positive feedback could ensue such that the increased reflectiveness (albedo) of the ice led to further cooling and the formation of more ice, until the whole Earth is ice-covered.

Polar continents, due to low rates of evaporation, are too dry to allow substantial carbon deposition—restricting the amount of atmospheric carbon dioxide that can be removed from the carbon cycle. A gradual rise of the proportion of the isotope carbon-13 relative to carbon-12 in sediments pre-dating "global" glaciation indicates that draw-down before snowball Earths was a slow and continuous process.

The start of snowball Earths are always marked by a sharp downturn in the δC value of sediments, a hallmark that may be attributed to a crash in biological productivity as a result of the cold temperatures and ice-covered oceans.

In January 2016, Gernon et al. proposed a "shallow-ridge hypothesis" involving the breakup of the supercontinent Rodinia, linking the eruption and rapid alteration of hyaloclastites along shallow ridges to massive increases in alkalinity in an ocean with thick ice cover. Gernon et al. demonstrated that the increase in alkalinity over the course of glaciation is sufficient to explain the thickness of cap carbonates formed in the aftermath of Snowball Earth events.

Global temperature fell so low that the equator was as cold as modern-day Antarctica. This low temperature was maintained by the high albedo of the ice sheets, which reflected most incoming solar energy into space. A lack of heat-retaining clouds, caused by water vapor freezing out of the atmosphere, amplified this effect.

The carbon dioxide levels necessary to thaw Earth have been estimated as being 350 times what they are today, about 13% of the atmosphere. Since the Earth was almost completely covered with ice, carbon dioxide could not be withdrawn from the atmosphere by release of alkaline metal ions weathering out of siliceous rocks. Over 4 to 30 million years, enough and methane, mainly emitted by volcanoes but also produced by microbes converting organic carbon trapped under the ice into the gas, would accumulate to finally cause enough greenhouse effect to make surface ice melt in the tropics until a band of permanently ice-free land and water developed; this would be darker than the ice, and thus absorb more energy from the Sun—initiating a "positive feedback".

Destabilization of substantial deposits of methane hydrates locked up in low-latitude permafrost may also have acted as a trigger and/or strong positive feedback for deglaciation and warming.

On the continents, the melting of glaciers would release massive amounts of glacial deposit, which would erode and weather. The resulting sediments supplied to the ocean would be high in nutrients such as phosphorus, which combined with the abundance of would trigger a cyanobacteria population explosion, which would cause a relatively rapid reoxygenation of the atmosphere, which may have contributed to the rise of the Ediacaran biota and the subsequent Cambrian explosion—a higher oxygen concentration allowing large multicellular lifeforms to develop. Although the positive feedback loop would melt the ice in geological short order, perhaps less than 1,000 years, replenishment of atmospheric oxygen and depletion of the levels would take further millennia.

It is possible that carbon dioxide levels fell enough for Earth to freeze again; this cycle may have repeated until the continents had drifted to more polar latitudes.

More recent evidence suggests that with colder oceanic temperatures, the resulting higher ability of the oceans to dissolve gases led to the carbon content of sea water being more quickly oxidized to carbon dioxide. This leads directly to an increase of atmospheric carbon dioxide, enhanced greenhouse warming of Earth's surface, and the prevention of a total snowball state.

During millions of years, cryoconite would have accumulated on and inside the ice. Psychrophilic microorganisms, volcanic ash and dust from ice-free locations would settle on ice covering several million square kilometers. Once the ice started to melt, these layers would become visible and color the icy surfaces dark, helping to accelerate the process.

Ultraviolet light from the Sun would also produce hydrogen peroxide (HO) when it hits water molecules. Normally hydrogen peroxide is broken down by sunlight, but some would have been trapped inside the ice. When the glaciers started to melt, it would have been released in both the ocean and the atmosphere, where it was split into water and oxygen molecules, leading to an increase in atmospheric oxygen.

While the presence of glaciers is not disputed, the idea that the entire planet was covered in ice is more contentious, leading some scientists to posit a "slushball Earth", in which a band of ice-free, or ice-thin, waters remains around the equator, allowing for a continued hydrologic cycle.

This hypothesis appeals to scientists who observe certain features of the sedimentary record that can only be formed under open water, or rapidly moving ice (which would require somewhere ice-free to move to). Recent research observed geochemical cyclicity in clastic rocks, showing that the "snowball" periods were punctuated by warm spells, similar to ice age cycles in recent Earth history. Attempts to construct computer models of a snowball Earth have also struggled to accommodate global ice cover without fundamental changes in the laws and constants which govern the planet.

A less extreme snowball Earth hypothesis involves continually evolving continental configurations and changes in ocean circulation. Synthesised evidence has produced models indicating a "slushball Earth", where the stratigraphic record does not permit postulating complete global glaciations. Kirschivink's original hypothesis had recognised that warm tropical puddles would be expected to exist in a snowball earth.

The snowball Earth hypothesis does not explain the alternation of glacial and interglacial events, nor the oscillation of glacial sheet margins.

The argument against the hypothesis is evidence of fluctuation in ice cover and melting during "snowball Earth" deposits. Evidence for such melting comes from evidence of glacial dropstones, geochemical evidence of climate cyclicity, and interbedded glacial and shallow marine sediments. A longer record from Oman, constrained to 13°N, covers the period from 712 to 545 million years ago—a time span containing the Sturtian and Marinoan glaciations—and shows both glacial and ice-free deposition.

There have been difficulties in recreating a snowball Earth with global climate models. Simple GCMs with mixed-layer oceans can be made to freeze to the equator; a more sophisticated model with a full dynamic ocean (though only a primitive sea ice model) failed to form sea ice to the equator. In addition, the levels of necessary to melt a global ice cover have been calculated to be 130,000 ppm, which is considered by to be unreasonably large.

Strontium isotopic data have been found to be at odds with proposed snowball Earth models of silicate weathering shutdown during glaciation and rapid rates immediately post-glaciation. Therefore, methane release from permafrost during marine transgression was proposed to be the source of the large measured carbon excursion in the time immediately after glaciation.

Nick Eyles suggest that the Neoproterozoic Snowball Earth was in fact no different from any other glaciation in Earth's history, and that efforts to find a single cause are likely to end in failure. The "Zipper rift" hypothesis proposes two pulses of continental "unzipping"—first, the breakup of the supercontinent Rodinia, forming the proto-Pacific Ocean; then the splitting of the continent Baltica from Laurentia, forming the proto-Atlantic—coincided with the glaciated periods.
The associated tectonic uplift would form high plateaus, just as the East African Rift is responsible for high topography; this high ground could then host glaciers.

Banded iron formations have been taken as unavoidable evidence for global ice cover, since they require dissolved iron ions and anoxic waters to form; however, the limited extent of the Neoproterozoic banded iron deposits means that they may not have formed in frozen oceans, but instead in inland seas. Such seas can experience a wide range of chemistries; high rates of evaporation could concentrate iron ions, and a periodic lack of circulation could allow anoxic bottom water to form.

Continental rifting, with associated subsidence, tends to produce such landlocked water bodies. This rifting, and associated subsidence, would produce the space for the fast deposition of sediments, negating the need for an immense and rapid melting to raise the global sea levels.

A competing hypothesis to explain the presence of ice on the equatorial continents was that Earth's axial tilt was quite high, in the vicinity of 60°, which would place Earth's land in high "latitudes", although supporting evidence is scarce. A less extreme possibility would be that it was merely Earth's magnetic pole that wandered to this inclination, as the magnetic readings which suggested ice-filled continents depend on the magnetic and rotational poles being relatively similar. In either of these two situations, the freeze would be limited to relatively small areas, as is the case today; severe changes to Earth's climate are not necessary.

The evidence for low-latitude glacial deposits during the supposed snowball Earth episodes has been reinterpreted via the concept of inertial interchange true polar wander (IITPW).
This hypothesis, created to explain palaeomagnetic data, suggests that Earth's axis of rotation shifted one or more times during the general time-frame attributed to snowball Earth. This could feasibly produce the same distribution of glacial deposits without requiring any of them to have been deposited at equatorial latitude. While the physics behind the proposition is sound, the removal of one flawed data point from the original study rendered the application of the concept in these circumstances unwarranted.

Several alternative explanations for the evidence have been proposed.

A tremendous glaciation would curtail photosynthetic life on Earth, thus depleting atmospheric oxygen, and thereby allowing non-oxidized iron-rich rocks to form.

Detractors argue that this kind of glaciation would have made life extinct entirely. However, microfossils such as stromatolites and oncolites prove that, in shallow marine environments at least, life did not suffer any perturbation. Instead life developed a trophic complexity and survived the cold period unscathed. Proponents counter that it may have been possible for life to survive in these ways:

However, organisms and ecosystems, as far as it can be determined by the fossil record, do not appear to have undergone the significant change that would be expected by a mass extinction. With the advent of more precise dating, a phytoplankton extinction event which had been associated with snowball Earth was shown to precede glaciations by 16 million years. Even if life were to cling on in all the ecological refuges listed above, a whole-Earth glaciation would result in a biota with a noticeably different diversity and composition. This change in diversity and composition has not yet been observed—in fact, the organisms which should be most susceptible to climatic variation emerge unscathed from the snowball Earth.

A snowball Earth has profound implications in the history of life on Earth. While many refugia have been postulated, global ice cover would certainly have ravaged ecosystems dependent on sunlight. Geochemical evidence from rocks associated with low-latitude glacial deposits have been interpreted to show a crash in oceanic life during the glacials.

Because about half of the oceans' water was frozen solid as ice, the remaining water would be twice as salty as it is today, lowering its freezing point. When the ice sheet melted, it would cover the oceans with a layer of hot freshwater up to 2 kilometres thick. Only after the hot surface water mixed with the colder and deeper saltwater did the sea return to a warmer and less salty state.

The melting of the ice may have presented many new opportunities for diversification, and may indeed have driven the rapid evolution which took place at the end of the Cryogenian period.

The Neoproterozoic was a time of remarkable diversification of multicellular organisms, including animals. Organism size and complexity increased considerably after the end of the snowball glaciations. This development of multicellular organisms may have been the result of increased evolutionary pressures resulting from multiple icehouse-hothouse cycles; in this sense, snowball Earth episodes may have "pumped" evolution. Alternatively, fluctuating nutrient levels and rising oxygen may have played a part. Another major glacial episode may have ended just a few million years before the Cambrian explosion.

One hypothesis which has been gaining currency in recent years: that early snowball Earths did not so much "affect" the evolution of life on Earth as result from it. In fact the two hypotheses are not mutually exclusive. The idea is that Earth's life forms affect the global carbon cycle and so major evolutionary events alter the carbon cycle, redistributing carbon within various reservoirs within the biosphere system and in the process temporarily lowering the atmospheric (greenhouse) carbon reservoir until the revised biosphere system settled into a new state. The Snowball I episode (of the Huronian glaciation 2.4 to 2.1 billion years) and Snowball II (of the Precambrian's Cryogenian between 580–850 million years and which itself had a number of distinct episodes) are respectively thought to be caused by the evolution of oxygenic photosynthesis and then the rise of more advanced multicellular animal life and life's colonization of the land.

Global ice cover, if it existed, may—in concert with geothermal heating—have led to a lively, well mixed ocean with great vertical convective circulation.

There were three or four significant ice ages during the late Neoproterozoic. Of these, the Marinoan was the most significant, and the Sturtian glaciations were also truly widespread. Even the leading snowball proponent Hoffman agrees that the 350 thousand-year-long Gaskiers glaciation did not lead to global glaciation, although it was probably as intense as the late Ordovician glaciation. The status of the Kaigas "glaciation" or "cooling event" is currently unclear; some scientists do not recognise it as a glacial, others suspect that it may reflect poorly dated strata of Sturtian association, and others believe it may indeed be a third ice age. It was certainly less significant than the Sturtian or Marinoan glaciations, and probably not global in extent. Emerging evidence suggests that the Earth underwent a number of glaciations during the Neoproterozoic, which would stand strongly at odds with the snowball hypothesis.

The snowball Earth hypothesis has been invoked to explain glacial deposits in the Huronian Supergroup of Canada, though the palaeomagnetic evidence that suggests ice sheets at low latitudes is contested. The glacial sediments of the Makganyene formation of South Africa are slightly younger than the Huronian glacial deposits (~2.25 billion years old) and were deposited at tropical latitudes. It has been proposed that rise of free oxygen that occurred during the Great Oxygenation Event removed methane in the atmosphere through oxidation. As the Sun was notably weaker at the time, Earth's climate may have relied on methane, a powerful greenhouse gas, to maintain surface temperatures above freezing.

In the absence of this methane greenhouse, temperatures plunged and a snowball event could have occurred.

Before the theory of continental drift, glacial deposits in Carboniferous strata in tropical continental areas such as India and South America led to speculation that the Karoo Ice Age glaciation reached into the tropics. However, a continental reconstruction shows that ice was in fact constrained to the polar parts of the supercontinent Gondwana.





</doc>
<doc id="28984" url="https://en.wikipedia.org/wiki?curid=28984" title="S.S. Lazio">
S.S. Lazio

Società Sportiva Lazio (; "Lazio Sport Club"), commonly referred to as Lazio (), is an Italian professional sports club based in Rome, most known for its football activity. The society, founded in 1900, plays in the Serie A and have spent most of their history in the top tier of Italian football. Lazio have been Italian champions twice (1974, 2000), and have won the Coppa Italia seven times, the Supercoppa Italiana five times, and both the UEFA Cup Winners' Cup and UEFA Super Cup on one occasion.

The club had their first major success in 1958, winning the domestic cup. In 1974, they won their first Serie A title. The 1990s have been the most successful period in Lazio's history, seeing them win the UEFA Cup Winners' Cup and UEFA Super Cup in 1999, the Serie A title in 2000, and reaching their first UEFA Cup final in 1998. Due to a severe economic crisis in 2002 that forced president Sergio Cragnotti out of the club along with several star players being sold, Lazio's success in the league declined. In spite of the lower funds, the club has won four Coppa Italia titles since then; in 2004, 2009, 2013 and 2019. Current president Claudio Lotito took charge of the club in 2004 after two years of a vacuum after Cragnotti's departure.

Lazio's traditional kit colours are sky blue shirts and white shorts with white socks; the colours are reminiscent of Rome's ancient Hellenic legacy. Sky blue socks have also been interchangeably used as home colours. Their home is the 70,634 capacity Stadio Olimpico in Rome, which they share with A.S. Roma. Lazio have a long-standing rivalry with Roma, with whom they have contested the "Derby della Capitale" (in English "Derby of the capital city" or Rome derby) since 1929.

Despite initially not having any parent–subsidiary relation with the male and female professional team (that was incorporated as S.S. Lazio S.p.A.), the founding of Società Sportiva Lazio allowed for the club that participates in over 40 sports disciplines in total, more than any other sports association in the world.

"Società Podistica Lazio" was founded on 9 January 1900 in the Prati district of Rome. Until 1910, the club played at an amateur level until it officially joined the league competition in 1912 as soon as the Italian Football Federation began organising championships in the center and south of Italy, and reached the final of the national championship playoff three times, but never won, losing in 1913 to Pro Vercelli, in 1914 to Casale and in 1923 to Genoa 1893.

In 1927, Lazio was the only major Roman club which resisted the Fascist regime's attempts to merge all the city's teams into what would become A.S. Roma the same year.

The club played in the first organised Serie A in 1929 and, led by legendary Italian striker Silvio Piola, achieved a second-place finish in 1937 – its highest pre-war result.

The 1950s produced a mix of mid and upper table results with a Coppa Italia win in 1958. Lazio was relegated for the first time in 1961 to the Serie B, but returned in the top flight two years later. After a number of mid-table placements, another relegation followed in 1970–71. Back to Serie A in 1972–73, Lazio immediately emerged as surprise challengers for the "Scudetto" to Milan and Juventus in 1972–73, only losing out on the final day of the season, with a team comprising captain Giuseppe Wilson, as well as midfielders Luciano Re Cecconi and Mario Frustalupi, striker Giorgio Chinaglia, and head coach Tommaso Maestrelli. Lazio improved such successes the following season, ensuring its first title in 1973–74. However, tragic deaths of Re Cecconi and "Scudetto" trainer Maestrelli, as well as the departure of Chinaglia, would be a triple blow for Lazio. The emergence of Bruno Giordano during this period provided some relief as he finished League top scorer in 1979, when Lazio finished eighth.

Lazio were forcibly relegated to Serie B in 1980 due to a remarkable scandal concerning illegal bets on their own matches, along with Milan. They remained in Italy's second division for three seasons in what would mark the darkest period in Lazio's history. They would return in 1983 and manage a last-day escape from relegation the following season. The 1984–85 season would prove harrowing, with a pitiful 15 points and bottom place finish.

In 1986, Lazio was hit with a nine-point deduction (a true deathblow back in the day of the two-point win) for a betting scandal involving player Claudio Vinazzani. An epic struggle against relegation followed the same season in Serie B, with the club led by trainer Eugenio Fascetti only avoiding relegation to the Serie C after play-off wins over Taranto and Campobasso. This would prove a turning point in the club's history, with Lazio returning to Serie A in 1988 and, under the careful financial management of Gianmarco Calleri, the consolidation of the club's position as a solid top-flight club.

The arrival of Sergio Cragnotti in 1992 changed the club's history due to his long-term investments in new players to make the team a "Scudetto" competitor. A notable early transfer during his tenure was the capture of English midfielder Paul Gascoigne from Tottenham Hotspur for £5.5 million. Gascoigne's transfer to Lazio is credited with the increase of interest in Serie A in the United Kingdom during the 1990s. Cragnotti repeatedly broke transfer records in pursuit of players who were considered major stars – Juan Sebastián Verón for £18 million, Christian Vieri for £19 million and breaking the world transfer record, albeit only for a matter of weeks, to sign Hernán Crespo from Parma for £35 million.

Lazio were Serie A runners-up in 1995, third in 1996 and fourth in 1997, then losing the championship just by one point to Milan on the last championship's match in 1999 before, with the likes of Siniša Mihajlović, Alessandro Nesta, Marcelo Salas and Pavel Nedvěd in the side, winning its second "Scudetto" in 2000, as well as the Coppa Italia double with Sven-Göran Eriksson (1997–2001) as manager.

Lazio had two more Coppa Italia triumphs in 1998 and 2004, as well as the last ever UEFA Cup Winners' Cup in 1999. They also reached the UEFA Cup, but lost 0–3 against Internazionale.

In addition, Lazio won the Supercoppa Italiana twice and defeated Manchester United in 1999 to win the UEFA Super Cup.

In 2000, Lazio became also the first Italian football club to be quoted on the Italian Piazza Affari stock market.

With money running out, however, Lazio's results slowly worsened in the years. In 2002, a financial scandal involving Cragnotti and his food products multinational Cirio forced him to leave the club, and Lazio was controlled until 2004 by caretaker financial managers and a bank pool. This forced the club to sell their star players and even fan favourite captain Alessandro Nesta. In 2004, entrepreneur Claudio Lotito acquired the majority of the club.

In 2006, the club qualified to the 2006–07 UEFA Cup under coach Delio Rossi. The club, however, was excluded from European competitions due to their involvement in a match-fixing scandal.

In the 2006–07 season, despite a later-reduced points deduction, Lazio achieved a third-place finish, thus gaining qualification to the UEFA Champions League third qualifying round, where they defeated Dinamo București to reach the group phase, and ended fourth place in the group composed of Real Madrid, Werder Bremen and Olympiacos. Things in the league did not go much better, with the team spending most of the season in the bottom half of the table, sparking the protests of the fans, and eventually ending the Serie A season in 12th place. In the 2008–09 season, Lazio won their fifth Coppa Italia, beating Sampdoria in the final.

Lazio started the 2009–10 season playing the Supercoppa Italiana against Inter in Beijing and winning the match 2–1, with goals from Matuzalém and Tommaso Rocchi.

Lazio won the 2012–13 Coppa Italia 1–0 over rivals Roma with the lone goal coming from Senad Lulić. Lazio won the 2018–19 Coppa Italia 2–0 over Atalanta, winning their seventh title overall.

On 22 December 2019, Lazio won their fifth Supercoppa Italiana title, following a 3–1 victory over Juventus.

Lazio's colours of white and sky blue were inspired by the national emblem of Greece, due to the fact that Lazio is a mixed sports club this was chosen in recognition of the fact that the Ancient Olympic Games and along with it the sporting tradition in Europe is linked to Greece.

Originally, Lazio wore a shirt which was divided into white and sky blue quarters, with black shorts and socks. After a while of wearing a plain white shirt very early on, Lazio reverted to the colours which they wear today. Some seasons Lazio have used a sky blue and white shirt with stripes, but usually it is sky blue with a white trim, with the white shorts and socks. The club's colours have led to their Italian nickname of "Biancocelesti".

Lazio's traditional club badge and symbol is the eagle, which was chosen by founding member Luigi Bigiarelli. It is an acknowledgment to the emblem of Zeus (the god of sky and thunder in Greek mythology) commonly known as the Aquila; Lazio's use of the symbol has led to two of their nicknames; "le Aquile" ("the Eagles") and "Aquilotti" ("Eaglets"). The current club badge features a golden eagle above a white shield with a blue border; inside the shield is the club's name and a smaller tripartite shield with the colours of the club.

Stadio Olimpico, located on the Foro Italico, is the major stadium of Rome. It is the home of the Italy national football team as well as of both local teams Lazio and Roma. It was opened in 1937 and after its latest renovation in 2008, the stadium has a capacity of 70,634 seats. It was the site of the 1960 Summer Olympics, but has also served as the location of the 1987 World Athletics Championships, the 1980 European Championship final, the 1990 World Cup and the Champions League Final in 1996 and 2009.

Also on the Foro Italico lies the Stadio dei Marmi, or "marble stadium", which was built in 1932 and designed by Enrico Del Debbio. It has tiers topped by 60 white marble statues that were gifts from Italian cities in commemoration of 60 athletes.

During the 1989–90 season, Lazio and Roma played their games at the Stadio Flaminio of Rome, located in the district Flaminio, because of the renovation works carried out at the Stadio Olimpico.
Lazio is the sixth-most supported football club in Italy and the second in Rome, with around 2% of Italian football fans supporting the club (according to "La Repubblica's" research of August 2008). Historically, the largest section of Lazio supporters in the city of Rome has come from the far northern section, creating an arch-like shape across Rome with affluent areas such as Parioli, Prati, Flaminio, Cassia and Monte Mario.

Founded in 1987, "Irriducibili Lazio" were the club's biggest ultras group for over 20 years. They typically create traditional Italian ultra displays during the "Derby della Capitale" (Rome Derby), the match between Lazio and their main rivals, Roma. It is amongst the most heated and emotional footballing rivalries in the world, such as where Lazio fan Vincenzo Paparelli was killed at one of the derby games during the 1979–80 season after being hit in the eye by an emergency rocket thrown by a Roma fan. A minority of Lazio's ultras used to use swastikas and fascist symbols on their banners, and they have displayed racist behaviour in several occasions during the derbies. Most notably, at a derby of the season 1998–99, laziali unfurled a 50-metre banner around the Curva Nord that read, "Auschwitz is your town, the ovens are your houses". Black players of Roma have often been receivers of racist and offensive behaviour. Lazio also have a strong rivalry with Napoli and Livorno, as well as with Pescara and Atalanta. The club also maintains strong competitive rivalries with Fiorentina, Juventus, and Milan.

Conversely, the ultras have friendly relationships with Internazionale, Triestina, and Hellas Verona. Internationally, Lazio's fans maintain a long-standing strong friendship with the supporters of the Bulgarian club Levski Sofia and as such, Lazio were invited to participate in the centenary football match honouring the birthday of the Bulgarian club.

12 – Since the 2003–04 season, Curva Nord of Stadio Olimpico, as a sign of recognition towards the Curva Nord, is considered the 12th man in the field.

The following managers have all won at least one trophy when in charge of Lazio:







Giuseppe Favalli holds Lazio's official appearance record, having made 401 over the course of 16 years from 1992 until 2004. The record for total appearances by a goalkeeper is held by Luca Marchegiani, with 339 appearances, while the record for most league appearances is held by Aldo Puccinelli with 339.

The all-time leading goalscorer for Lazio is Silvio Piola, with 149 goals scored. Piola, who played also with Pro Vercelli, Torino, Juventus and Novara, is also the highest goalscorer in Serie A history, with 274 goals. Simone Inzaghi is the all-time top goalscorer in the European Competitions, with 20 goals. He is also one of the five players who scored four goals in a single UEFA Champions League match.

Officially, Lazio's highest home attendance is approximately 80,000 for a Serie A match against Foggia on 12 May 1974, the match that awarded to Lazio their first "Scudetto". This is also the record for the Stadio Olimpico, including matches held by Roma and the Italy national football team.

In 1998, during Sergio Cragnotti's period in charge as the chairman, Società Sportiva Lazio S.p.A. became a listed company: Lazio were the first Italian club to do so. However, Cragnotti resigned as chairman in 2001, after a "huge hole in the budget" of the club.

Claudio Lotito, the current chairman of Lazio, purchased the club from Cragnotti in 2004, but owned just 26.969% of shares as the largest shareholders at that time. It was followed by banking group Capitalia (and its subsidiaries Mediocredito Centrale, Banca di Roma and Banco di Sicilia) as the second largest shareholders for 17.717%. Capitalia also hold 49% stake of Italpetroli (via Capitalia's subsidiary Banca di Roma), the parent company of city rival Roma (via Italpetroli's subsidiary "Roma 2000"). Lotito later purchased the minority stake from Capitalia.

, Claudio Lotito owns just over two-thirds of the shares of Lazio. Lazio is one of only three Italian clubs listed on the Borsa Italiana, the others being Juventus and Roma. In the past, Lazio was the only one with a single primary share holder (Lotito). However, following several capital increases by Roma and Juventus, they also are significantly owned by a shareholder. According to The Football Money League, published by consultants Deloitte, in the 2004–05 season, Lazio was the 20th highest earning football club in the world with an estimated revenue of €83 million; the 2005 ranking of the club was 15th. However, in 2016 ranking (the rank used data in 2014–15 season), Lazio was not in the top 20.

Lazio was one of the few clubs that self-sustain from the financial support of a shareholder, and also consistently make an aggregate profit after every season. Unlike Inter Milan, Roma and Milan, who were sanctioned by UEFA due to breaches of Financial Fair Play, Lazio passed the regulations held by the administrative body with the high achievements. Lotito also received a prize that joint awarded by and DGS Sport&Cultura, due to Lazio's financial health.

In 2017, the club renewed their sponsorship deal with shirt manufacturer Macron. It is worth €16 million a season, plus variables of about €9 million stemming from league and European competition finishes.





</doc>
<doc id="28990" url="https://en.wikipedia.org/wiki?curid=28990" title="Ninian">
Ninian

Ninian is a Christian saint first mentioned in the 8th century as being an early missionary among the Pictish peoples of what is now Scotland. For this reason he is known as the Apostle to the Southern Picts, and there are numerous dedications to him in those parts of Scotland with a Pictish heritage, throughout the Scottish Lowlands, and in parts of Northern England with a Northumbrian heritage. In Scotland, Ninian is also known as Ringan, and as Trynnian in Northern England.

Ninian's major shrine was at Whithorn in Galloway, where he is associated with the Candida Casa (Latin for 'White House'). Nothing is known about his teachings, and there is no unchallenged authority for information about his life.

A link between the Ninian of tradition and a person who actually appears in the historical record is not yet confirmed, though Finnian of Moville has gained traction as a leading candidate. This article discusses the particulars and origins of what has come to be known as the "traditional" stories of Saint Ninian.

The Southern Picts, for whom Ninian is held to be the apostle, are the Picts south of the mountains known as the Mounth, which cross Scotland north of the Firths of Clyde and Forth. That they had once been Christian is known from a 5th-century mention of them by Saint Patrick in his "Letter to Coroticus", where he refers to them as 'apostate Picts'. Patrick could not have been referring to the Northern Picts who were converted by Saint Columba in the 6th century because they were not yet Christian, and thus could not be called 'apostate'. Northumbria had established a bishopric among the Southern Picts at Abercorn in 681, under Bishop Trumwine. This effort was abandoned shortly after the Picts defeated the Northumbrians at the Battle of Dun Nechtain in 685.

Christianity had flourished in Galloway in the 6th century. By the time of Bede's account in 731, the Northumbrians had enjoyed an unbroken relationship with Galloway for a century or longer, beginning with the Northumbrian predecessor state of Bernicia. The full nature of the relationship is uncertain. Also at this time, Northumbria was establishing bishoprics in its sphere of influence, to be subordinate to the Northumbrian Archbishop of York. One such bishopric was established at Whithorn in 731, and Bede's account serves to support the legitimacy of the new Northumbrian bishopric. The Bernician name "hwit ærn" is Old English for the Latin "candida casa", or 'white house' in modern English, and it has survived as the modern name of Whithorn.

There is as yet no unchallenged connection of the historical record to the person who was Bede's Ninian. However, the unlikelihood that the reputable historian Bede invented Ninian without some basis in the historical record, combined with an increased knowledge of Ireland's early saints and Whithorn's early Christian connections, has led to serious scholarly efforts to find Bede's basis. James Henthorn Todd, in his 1855 publication of the "Leabhar Imuinn" (The Book of Hymns of the Ancient Church of Ireland), suggested that it was Finnian of Moville, and that view has gained traction among modern scholars.

The earliest mention of Ninian of Whithorn is in a short passage of "The Ecclesiastical History of the English People" by the Northumbrian monk Bede in c. 731. The 9th-century poem "Miracula Nyniae Episcopi" records some of the miracles attributed to him. A "Life of Saint Ninian" ("Vita Sancti Niniani") was written around 1160 by Ailred of Rievaulx, and in 1639 James Ussher discusses Ninian in his "Brittanicarum Ecclesiarum Antiquitates". These are the sources of information about Ninian of Whithorn, and all provide seemingly innocuous personal details about his life. However, there is no unchallenged historical evidence to support any of their stories, and all sources had political and religious agendas that were served by their accounts of Saint Ninian (discussed below).

Tradition holds that Ninian was a Briton who had studied in Rome, that he established an episcopal see at the "Candida Casa" in Whithorn, that he named the see for Saint Martin of Tours, that he converted the southern Picts to Christianity, and that he is buried at Whithorn. Variations of the story add that he had actually met Saint Martin, that his father was a Christian king, and that he was buried in a stone sarcophagus near the altar of his church. Further variations assert that he left for Ireland, and died there in 432. Dates for his birth are derived from the traditional mention of Saint Martin, who died in 397.

Bede says that Ninian (whose name he only renders in the ablative case "Nynia") was a Briton who had been instructed in Rome; that he made his church of stone, which was unusual among the Britons; that his episcopal see was named after Saint Martin of Tours; that he preached to and converted the southern Picts; that his base was in a place called " Ad Candidam Casam", which was in the province of the Bernicians; and that he was buried there, along with many other saints.

Bede's information is minimal and he does not claim it as fact, asserting only that he is passing on "traditional" information. He provides the first historical reference to Saint Ninian, in a passing reference contained in the final part of a single paragraph.

Leaving aside the tales regarding miracles, in the "Vita Sancti Niniani" Aelred includes the following incidental information regarding Saint Ninian: that his father was a Christian king; that he was consecrated a bishop in Rome and that he met Saint Martin in Tours; that Saint Martin sent masons with him on his homeward journey, at his request; that these masons built a church of stone, situated on the shore, and that on learning of Saint Martin's death, Ninian dedicated the church to him; that a certain rich and powerful "King Tuduvallus" was converted by him; that he died after having converted the Picts and returned home, being buried in a stone sarcophagus near the altar of his church; and that he had once travelled with his brother, named "Plebia".

Aelred said that in addition to finding information about Ninian in Bede, he took much additional information for his "Life of S. Ninian" from a source written in a "barbarous language"; there is no further information about this text. Aelred wrote his "Life of S. Ninian" sometime after spending ten years at the Scottish court and thus had close connections both to the Scottish royal family and to Fergus of Galloway (who would resurrect the Bishopric of Galloway), all of whom would have been pleased to have a manuscript with such a glowing description of a Galwegian and Scottish saint. His work is what Thomas Heffernan refers to as a "sacred biography," probably intended for a politically ambitious audience.

Ussher wrote that Ninian left Candida Casa for "Cluayn-coner" in Ireland, and eventually died in Ireland; that his mother was a Spanish princess; that his father wished to regain him after having assented to his training for an ecclesiastical state; that a bell comes from heaven to call together his disciples; that a wooden church was raised by him, with beams delivered by stags; and that a harper with no experience at architecture was the builder of the church. He adds that a smith and his son, named respectively "Terna" and "Wyn", witnessed a miracle by Ninian and that the saint was granted lands to be called "Wytterna".

In addition, Skene attributes the "traditional" date of Ninian's death (16 September 432) ultimately to Ussher's "Life of Ninian," noting that the date is "without authority."

Ussher's contribution is often disparaged, as he both invented fictitious histories and misquoted legitimate manuscripts to suit his own purposes. Still, he had access to legitimate manuscripts, and he has contributed to some versions of the traditional stories.

Others who wrote of Saint Ninian used the accounts of Bede, Aelred, or Ussher, or used derivatives of them in combination with information from various manuscripts. This includes John Capgrave (1393–1464), John of Tinmouth (fl. c. 1366), John Colgan (d. c. 1657), and many others, up to the present day.

The anonymously written 8th century hagiographic "Miracula Nynie Episcopi" ("Miracles of Bishop Ninian") is discounted as a non-historical account, and copies are not widely extant.
Dedications to Saint Ninian are expressions of respect for the good works that are attributed to him, and the authenticity of the stories about him are not relevant to that point. Almost all of the dedications have their origins in the medieval era, after Aelred wrote his account.

The dedications are found throughout the lands of the ancient Picts of Scotland, throughout Scotland south of the Firths of Clyde and Forth, in Orkney and Shetland, and in parts of northern England. Ss Ninian and Triduana's Church, Edinburgh is a Roman Catholic church dedicated to Ninian.

Dedications on the Isle of Man date from the time of medieval Scottish dominance, and are not natively inspired.

There are dedications to St. Ninian in East Donegal and Belfast; and a spot formerly on the shore of Belfast Lough was traditionally known as St. Ninian's point, where the missionary reputedly landed after a voyage from Scotland. These connections reflect a strong Ulster-Scots heritage in both areas of Ulster.

There are also dedications elsewhere in the world where there is a Scottish heritage, such as Nova Scotia. St. Ninian's Cathedral is located in Antigonish, Nova Scotia.

There is a noticeable lack of dedications in the Scottish Highlands and Isles.

In Scotland the date 16 September is celebrated as St. Ninian's Day.

In the modern era, Ss Ninian and Triduana's Church, Edinburgh is a Roman Catholic church constructed in 1932 that is dedicated to St. Ninian.

St Martin and St Ninian is a Catholic church in Whithorn, Wigtownshire constructed in 1959–60 in the Diocese of Galloway. The architect was Harry Stuart Goodhart-Rendel, (1887–1859). 




</doc>
<doc id="28994" url="https://en.wikipedia.org/wiki?curid=28994" title="Standard Generalized Markup Language">
Standard Generalized Markup Language

The Standard Generalized Markup Language (SGML; ISO 8879:1986) is a standard for defining generalized markup languages for documents. ISO 8879 Annex A.1 states that generalized markup is "based on two postulates":

HTML was theoretically an example of an SGML-based language until HTML 5, which browsers cannot parse as SGML for compatibility reasons.

DocBook SGML and LinuxDoc are examples which were used almost exclusively with actual SGML tools.

SGML is an ISO standard: "ISO 8879:1986 Information processing – Text and office systems – Standard Generalized Markup Language (SGML)", of which there are three versions:


SGML is part of a trio of enabling ISO standards for electronic documents developed by ISO/IEC JTC1/SC34 (ISO/IEC Joint Technical Committee 1, Subcommittee 34 – Document description and processing languages) :


SGML is supported by various technical reports, in particular


SGML descended from IBM's Generalized Markup Language (GML), which Charles Goldfarb, Edward Mosher, and Raymond Lorie developed in the 1960s. Goldfarb, editor of the international standard, coined the “GML” term using their surname initials. Goldfarb also wrote the definitive work on SGML syntax in "The SGML Handbook". The syntax of SGML is closer to the COCOA format. As a document markup language, SGML was originally designed to enable the sharing of machine-readable large-project documents in government, law, and industry. Many such documents must remain readable for several decades—a long time in the information technology field. SGML also was extensively applied by the military, and the aerospace, technical reference, and industrial publishing industries. The advent of the XML profile has made SGML suitable for widespread application for small-scale, general-purpose use.

SGML (ENR+WWW) defines two kinds of validity. According to the revised Terms and Definitions of ISO 8879 (from the public draft):
A conforming SGML document must be either a type-valid SGML document, a tag-valid SGML document, or both. Note: A user may wish to enforce additional constraints on a document, such as whether a document instance is integrally-stored or free of entity references.
A type-valid SGML document is defined by the standard as
An SGML document in which, for each document instance, there is an associated document type declaration (DTD) to whose DTD that instance conforms.
A tag-valid SGML document is defined by the standard as
An SGML document, all of whose document instances are fully tagged. There need not be a document type declaration associated with any of the instances. Note: If there is a document type declaration, the instance can be parsed with or without reference to it.
"Tag-validity" was introduced in SGML (ENR+WWW) to support XML which allows documents with no DOCTYPE declaration but which can be parsed without a grammar, or documents which have a DOCTYPE declaration that makes no XML Infoset contributions to the document. The standard calls this "fully tagged". "Integrally stored" reflects the XML requirement that elements end in the same entity in which they started. "Reference-free" reflects the HTML requirement that entity references are for special characters and do not contain markup. SGML validity commentary, especially commentary that was made before 1997 or that is unaware of SGML (ENR+WWW), covers "type-validity" only.

The SGML emphasis on validity supports the requirement for generalized markup that "markup should be rigorous." (ISO 8879 A.1)

An SGML document may have three parts:

An SGML document may be composed from many entities (discrete pieces of text). In SGML, the entities and element types used in the document may be specified with a DTD, the different character sets, features, delimiter sets, and keywords are specified in the SGML Declaration to create the "concrete syntax" of the document.

Although full SGML allows implicit markup and some other kinds of tags, the XML specification (s4.3.1) states:

For introductory information on a basic, modern SGML syntax, see XML. The following material concentrates on features not in XML and is not a comprehensive summary of SGML syntax.

SGML generalizes and supports a wide range of markup languages as found in the mid 1980s. These ranged from terse Wiki-like syntaxes to RTF-like bracketed languages to HTML-like matching-tag languages. SGML did this by a relatively simple default "reference concrete syntax" augmented with a large number of optional features that could be enabled in the SGML Declaration. Not every SGML parser can necessarily process every SGML document. Because each processor's "System Declaration" can be compared to the document's "SGML Declaration" it is always possible to know whether a document is supported by a particular processor.

Many SGML features relate to markup minimization. Other features relate to concurrent (parallel) markup (CONCUR), to linking processing attributes (LINK), and to embedding SGML documents within SGML documents (SUBDOC).

The notion of customizable features was not appropriate for Web use, so one goal of XML was to minimize optional features. However, XML's well-formedness rules cannot support Wiki-like languages, leaving them unstandardized and difficult to integrate with non-text information systems.

The usual (default) SGML "concrete syntax" resembles this example, which is the default HTML concrete syntax:
<QUOTE TYPE="example">
</QUOTE>
SGML provides an "abstract syntax" that can be implemented in many different types of "concrete syntax". Although the markup norm is using angle brackets as start- and end- tag delimiters in an SGML document (per the standard-defined "reference concrete syntax"), it is possible to use other characters—provided a suitable "concrete syntax" is defined in the document's SGML declaration. For example, an SGML interpreter might be programmed to parse GML, wherein the tags are delimited with a left colon and a right full stop, thus, an ":e" prefix denotes an end tag: codice_1. According to the reference syntax, letter-case (upper- or lower-) is not distinguished in tag names, thus the three tags: (i) codice_2, (ii) codice_3, and (iii) codice_4 are equivalent. ("NOTE:" A concrete syntax might "change" this rule via the NAMECASE NAMING declarations).

SGML has features for reducing the number of characters required to mark up a document, which must be enabled in the SGML Declaration. SGML processors need not support every available feature, thus allowing applications to tolerate many types of inadvertent markup omissions; however, SGML systems usually are intolerant of invalid structures. XML is intolerant of syntax omissions, and does not require a DTD for checking well-formedness.

Both start tags and end tags may be omitted from a document instance, provided:

For example, if OMITTAG YES is specified in the SGML Declaration (enabling the OMITTAG feature), and the DTD includes the following declarations:

<!ELEMENT chapter - - (title, section+)>
<!ELEMENT title o o (#PCDATA)>
<!ELEMENT section - - (title, subsection+)>
then this excerpt:
<chapter>Introduction to SGML
<section>The SGML Declaration
<subsection>

which omits two tags and two tags, would represent valid markup.

Note also that omitting tags is optional – the same excerpt could be tagged like this:
<chapter><title>Introduction to SGML</title>
<section><title>The SGML Declaration</title>
<subsection>

and would still represent valid markup.
Note: The OMITTAG feature is unrelated to the tagging of elements whose declared content is codice_6 as defined in the DTD:

<!ELEMENT image - o EMPTY>
Elements defined like this have no end tag, and specifying one in the document instance would result in invalid markup. This is syntactically different than XML empty elements in this regard.

Tags can be replaced with delimiter strings, for a terser markup, via the SHORTREF feature. This markup style is now associated with wiki markup, e.g. wherein two equals-signs (==), at the start of a line, are the “heading start-tag”, and two equals signs (==) after that are the “heading end-tag”.

SGML markup languages whose concrete syntax enables the SHORTTAG VALUE feature, do not require attribute values containing only alphanumeric characters to be enclosed within quotation marks—either double codice_7 (LIT) or single codice_8 (LITA)—so that the previous markup example could be written:
<QUOTE TYPE=example>
</QUOTE>
One feature of SGML markup languages is the "presumptuous empty tagging", such that the empty end tag codice_9 in codice_10 "inherits" its value from the nearest previous full start tag, which, in this example, is codice_11 (in other words, it closes the most recently opened item). The expression is thus equivalent to codice_12.

Another feature is the "NET" (Null End Tag) construction: codice_13, which is structurally equivalent to codice_12.

Additionally, the SHORTTAG NETENABL IMMEDNET feature allows shortening tags surrounding an empty text value, but forbids shortening full tags:
<QUOTE></QUOTE>
can be written as
<QUOTE// <!-- not a typo! -->
wherein the first slash ( / ) stands for the NET-enabling “start-tag close” (NESTC), and the second slash stands for the NET. NOTE: XML defines NESTC with a /, and NET with an > (angled bracket)—hence the corresponding construct in XML appears as <QUOTE/>.

The third feature is 'text on the same line', allowing a markup item to be ended with a line-end; especially useful for headings and such, requiring using either SHORTREF or DATATAG minimization. For example, if the DTD includes the following declarations:
<!ELEMENT lines (line*)>
<!ELEMENT line O - (#PCDATA)>
<!ENTITY line-tagc "</line>">
<!SHORTREF one-line "&#RE;&#RS;" line-tagc>
<!USEMAP one-line line>
(and "&#RE;&#RS;" is a short-reference delimiter in the concrete syntax), then:
<lines>
first line
second line
</lines>
is equivalent to:
<lines>
<line>first line</line>
<line>second line</line>
</lines>
SGML has many features that defied convenient description with the popular formal automata theory and the contemporary parser technology of the 1980s and the 1990s. The standard warns in Annex H:

A report on an early implementation of a parser for basic SGML, the Amsterdam SGML Parser, notes and specifies various differences.

There appears to be no definitive classification of full SGML against a known class of formal grammar. Plausible classes may include tree-adjoining grammars and adaptive grammars.

XML is described as being generally parsable like a two-level grammar for non-validated XML and a Conway-style pipeline of coroutines (lexer, parser, validator) for valid XML. The SGML productions in the ISO standard are reported to be LL(3) or LL(4). XML-class subsets are reported to be expressible using a W-grammar. According to one paper, and probably considered at an "information set" or parse tree level rather than a character or delimiter level:

The SGML standard does not define SGML with formal data structures, such as parse trees; however, an SGML document is constructed of a rooted directed acyclic graph (RDAG) of physical storage units known as “entities”, which is parsed into a RDAG of structural units known as “elements”. The physical graph is loosely characterized as an "entity tree", but entities might appear multiple times. Moreover, the structure graph is also loosely characterized as an "element tree", but the ID/IDREF markup allows arbitrary arcs.

The results of parsing can also be understood as a data tree in different notations; where the document is the root node, and entities in other notations (text, graphics) are child nodes. SGML provides apparatus for linking to and annotating external non-SGML entities.

The SGML standard describes it in terms of "maps" and "recognition modes" (s9.6.1). Each entity, and each element, can have an associated "notation" or "declared content type", which determines the kinds of references and tags which will be recognized in that entity and element. Also, each element can have an associated "delimiter map" (and "short reference map"), which determines which characters are treated as delimiters in context. The SGML standard characterizes parsing as a state machine switching between recognition modes. During parsing, there is a stack of maps that configure the scanner, while the tokenizer relates to the recognition modes.

Parsing involves traversing the dynamically-retrieved entity graph, finding/implying tags and the element structure, and validating those tags against the grammar. An unusual aspect of SGML is that the grammar (DTD) is used both passively — to "recognize" lexical structures, and actively — to "generate" missing structures and tags that the DTD has declared optional. End- and start- tags can be omitted, because they can be inferred. Loosely, a series of tags can be omitted only if there is a single, possible path in the grammar to imply them. It was this active use of grammars that made concrete SGML parsing difficult to formally characterize.

SGML uses the term "validation" for both recognition and generation. XML does not use the grammar (DTD) to change delimiter maps or to inform the parse modes, and does not allow tag omission; consequently, XML validation of elements is not active in the sense that SGML validation is active. SGML "without" a DTD (e.g. simple XML), is a grammar or a language; SGML "with" a DTD is a metalanguage. SGML with an SGML declaration is, perhaps, a meta-metalanguage, since it is a metalanguage whose declaration mechanism "is" a metalanguage.

SGML has an abstract syntax implemented by many possible concrete syntaxes; however, this is not the same usage as in an abstract syntax tree and as in a concrete syntax tree. In the SGML usage, a concrete syntax is a set of specific delimiters, while the abstract syntax is the set of names for the delimiters. The XML Infoset corresponds more to the programming language notion of abstract syntax introduced by John McCarthy.

The W3C XML (Extensible Markup Language) is a profile (subset) of SGML designed to ease the implementation of the parser compared to a full SGML parser, primarily for use on the World Wide Web. In addition to disabling many SGML options present in the reference syntax (such as omitting tags and nested subdocuments) XML adds a number of additional restrictions on the kinds of SGML syntax. For example, despite enabling SGML shortened tag forms, XML does not allow unclosed start or end tags. It also relied on many of the additions made by the WebSGML Annex. XML currently is more widely used than full SGML. XML has lightweight internationalization based on Unicode. Applications of XML include XHTML, XQuery, XSLT, XForms, XPointer, JSP, SVG, RSS, Atom, XML-RPC, RDF/XML, and SOAP.

While HTML was developed partially independently and in parallel with SGML, its creator, Tim Berners-Lee, intended it to be an application of SGML. The design of HTML (Hyper Text Markup Language) was therefore inspired by SGML tagging, but, since no clear expansion and parsing guidelines were established, most actual HTML documents are not valid SGML documents. Later, HTML was reformulated (version 2.0) to be more of an SGML application; however, the HTML markup language has many legacy- and exception-handling features that differ from SGML's requirements. HTML 4 is an SGML application that fully conforms to ISO 8879 – SGML.

The charter for the 2006 revival of the World Wide Web Consortium HTML Working Group says, "the Group will not assume that an SGML parser is used for 'classic HTML'". Although HTML syntax closely resembles SGML syntax with the default "reference concrete syntax", HTML5 abandons any attempt to define HTML as an SGML application, explicitly defining its own parsing rules, which more closely match existing implementations and documents. It does, however, define an alternative XHTML serialization, which conforms to XML and therefore to SGML as well.

The second edition of the "Oxford English Dictionary" (OED) is entirely marked up with an SGML-based markup language using the LEXX (text editor)

The third edition is marked up as XML.

Other document markup languages are partly related to SGML and XML, but—because they cannot be parsed or validated or other-wise processed using standard SGML and XML tools—they are not considered either SGML or XML languages; the Z Format markup language for typesetting and documentation is an example.

Several modern programming languages support tags as primitive token types, or now support Unicode and regular expression pattern-matching. An example is the Scala programming language.

Document markup languages defined using SGML are called "applications" by the standard; many pre-XML SGML applications were proprietary property of the organizations which developed them, and thus unavailable in the World Wide Web. The following list is of pre-XML SGML applications.


Significant open-source implementations of SGML have included:

SP and Jade, the associated DSSSL processors, are maintained by the OpenJade project, and are common parts of Linux distributions. A general archive of SGML software and materials resides at SUNET. The original HTML parser class, in Sun System's implementation of Java, is a limited-features SGML parser, using SGML terminology and concepts.




</doc>
<doc id="29000" url="https://en.wikipedia.org/wiki?curid=29000" title="Speciation">
Speciation

Speciation is the evolutionary process by which populations evolve to become distinct species. The biologist Orator F. Cook coined the term in 1906 for cladogenesis, the splitting of lineages, as opposed to anagenesis, phyletic evolution within lineages. Charles Darwin was the first to describe the role of natural selection in speciation in his 1859 book "On the Origin of Species". He also identified sexual selection as a likely mechanism, but found it problematic.

There are four geographic modes of speciation in nature, based on the extent to which speciating populations are isolated from one another: allopatric, peripatric, parapatric, and sympatric. Speciation may also be induced artificially, through animal husbandry, agriculture, or laboratory experiments. Whether genetic drift is a minor or major contributor to speciation is the subject matter of much ongoing discussion.

Rapid sympatric speciation can take place through polyploidy, such as by doubling of chromosome number; the result is progeny which are immediately reproductively isolated from the parent population. New species can also be created through hybridisation followed, if the hybrid is favoured by natural selection, by reproductive isolation.

In addressing the question of the origin of species, there are two key issues: (1) what are the evolutionary mechanisms of speciation, and (2) what accounts for the separateness and individuality of species in the biota? Since Charles Darwin's time, efforts to understand the nature of species have primarily focused on the first aspect, and it is now widely agreed that the critical factor behind the origin of new species is reproductive isolation. Next we focus on the second aspect of the origin of species.

In "On the Origin of Species" (1859), Darwin interpreted biological evolution in terms of natural selection, but was perplexed by the clustering of organisms into species. Chapter 6 of Darwin's book is entitled "Difficulties of the Theory." In discussing these "difficulties" he noted "Firstly, why, if species have descended from other species by insensibly fine gradations, do we not everywhere see innumerable transitional forms? Why is not all nature in confusion instead of the species being, as we see them, well defined?" This dilemma can be referred to as the absence or rarity of transitional varieties in habitat space.

Another dilemma, related to the first one, is the absence or rarity of transitional varieties in time. Darwin pointed out that by the theory of natural selection "innumerable transitional forms must have existed," and wondered "why do we not find them embedded in countless numbers in the crust of the earth." That clearly defined species actually do exist in nature in both space and time implies that some fundamental feature of natural selection operates to generate and maintain species.

It has been argued that the resolution of Darwin's first dilemma lies in the fact that out-crossing sexual reproduction has an intrinsic cost of rarity. The cost of rarity arises as follows. If, on a resource gradient, a large number of separate species evolve, each exquisitely adapted to a very narrow band on that gradient, each species will, of necessity, consist of very few members. Finding a mate under these circumstances may present difficulties when many of the individuals in the neighborhood belong to other species. Under these circumstances, if any species’ population size happens, by chance, to increase (at the expense of one or other of its neighboring species, if the environment is saturated), this will immediately make it easier for its members to find sexual partners. The members of the neighboring species, whose population sizes have decreased, experience greater difficulty in finding mates, and therefore form pairs less frequently than the larger species. This has a snowball effect, with large species growing at the expense of the smaller, rarer species, eventually driving them to extinction. Eventually, only a few species remain, each distinctly different from the other. The cost of rarity not only involves the costs of failure to find a mate, but also indirect costs such as the cost of communication in seeking out a partner at low population densities.

Rarity brings with it other costs. Rare and unusual features are very seldom advantageous. In most instances, they indicate a (non-silent) mutation, which is almost certain to be deleterious. It therefore behooves sexual creatures to avoid mates sporting rare or unusual features (koinophilia). Sexual populations therefore rapidly shed rare or peripheral phenotypic features, thus canalizing the entire external appearance, as illustrated in the accompanying illustration of the African pygmy kingfisher, "Ispidina picta". This uniformity of all the adult members of a sexual species has stimulated the proliferation of field guides on birds, mammals, reptiles, insects, and many other taxa, in which a species can be described with a single illustration (or two, in the case of sexual dimorphism). Once a population has become as homogeneous in appearance as is typical of most species (and is illustrated in the photograph of the African pygmy kingfisher), its members will avoid mating with members of other populations that look different from themselves. Thus, the avoidance of mates displaying rare and unusual phenotypic features inevitably leads to reproductive isolation, one of the hallmarks of speciation.

In the contrasting case of organisms that reproduce asexually, there is no cost of rarity; consequently, there are only benefits to fine-scale adaptation. Thus, asexual organisms very frequently show the continuous variation in form (often in many different directions) that Darwin expected evolution to produce, making their classification into "species" (more correctly, morphospecies) very difficult.

All forms of natural speciation have taken place over the course of evolution; however, debate persists as to the relative importance of each mechanism in driving biodiversity.

One example of natural speciation is the diversity of the three-spined stickleback, a marine fish that, after the last glacial period, has undergone speciation into new freshwater colonies in isolated lakes and streams. Over an estimated 10,000 generations, the sticklebacks show structural differences that are greater than those seen between different genera of fish including variations in fins, changes in the number or size of their bony plates, variable jaw structure, and color differences.

During allopatric (from the ancient Greek "allos", "other" + "patrā", "fatherland") speciation, a population splits into two geographically isolated populations (for example, by habitat fragmentation due to geographical change such as mountain formation). The isolated populations then undergo genotypic or phenotypic divergence as: (a) they become subjected to dissimilar selective pressures; (b) they independently undergo genetic drift; (c) different mutations arise in the two populations. When the populations come back into contact, they have evolved such that they are reproductively isolated and are no longer capable of exchanging genes. Island genetics is the term associated with the tendency of small, isolated genetic pools to produce unusual traits. Examples include insular dwarfism and the radical changes among certain famous island chains, for example on Komodo. The Galápagos Islands are particularly famous for their influence on Charles Darwin. During his five weeks there he heard that Galápagos tortoises could be identified by island, and noticed that finches differed from one island to another, but it was only nine months later that he reflected that such facts could show that species were changeable. When he returned to England, his speculation on evolution deepened after experts informed him that these were separate species, not just varieties, and famously that other differing Galápagos birds were all species of finches. Though the finches were less important for Darwin, more recent research has shown the birds now known as Darwin's finches to be a classic case of adaptive evolutionary radiation.

In peripatric speciation, a subform of allopatric speciation, new species are formed in isolated, smaller peripheral populations that are prevented from exchanging genes with the main population. It is related to the concept of a founder effect, since small populations often undergo bottlenecks. Genetic drift is often proposed to play a significant role in peripatric speciation.

Case studies include Mayr's investigation of bird fauna; the Australian bird "Petroica multicolor"; and reproductive isolation in populations of "Drosophila" subject to population bottlenecking.

In parapatric speciation, there is only partial separation of the zones of two diverging populations afforded by geography; individuals of each species may come in contact or cross habitats from time to time, but reduced fitness of the heterozygote leads to selection for behaviours or mechanisms that prevent their interbreeding. Parapatric speciation is modelled on continuous variation within a "single," connected habitat acting as a source of natural selection rather than the effects of isolation of habitats produced in peripatric and allopatric speciation.

Parapatric speciation may be associated with differential landscape-dependent selection. Even if there is a gene flow between two populations, strong differential selection may impede assimilation and different species may eventually develop. Habitat differences may be more important in the development of reproductive isolation than the isolation time. Caucasian rock lizards "Darevskia rudis", "D. valentini" and "D. portschinskii" all hybridize with each other in their hybrid zone; however, hybridization is stronger between "D. portschinskii" and "D. rudis", which separated earlier but live in similar habitats than between "D. valentini" and two other species, which separated later but live in climatically different habitats.

Ecologists refer to parapatric and peripatric speciation in terms of ecological niches. A niche must be available in order for a new species to be successful. Ring species such as "Larus" gulls have been claimed to illustrate speciation in progress, though the situation may be more complex. The grass "Anthoxanthum odoratum" may be starting parapatric speciation in areas of mine contamination.

Sympatric speciation is the formation of two or more descendant species from a single ancestral species all occupying the same geographic location.

Often-cited examples of sympatric speciation are found in insects that become dependent on different host plants in the same area.

The best known example of sympatric speciation is that of the cichlids of East Africa inhabiting the Rift Valley lakes, particularly Lake Victoria, Lake Malawi and Lake Tanganyika. There are over 800 described species, and according to estimates, there could be well over 1,600 species in the region. Their evolution is cited as an example of both natural and sexual selection. A 2008 study suggests that sympatric speciation has occurred in Tennessee cave salamanders. Sympatric speciation driven by ecological factors may also account for the extraordinary diversity of crustaceans living in the depths of Siberia's Lake Baikal.

Budding speciation has been proposed as a particular form of sympatric speciation, whereby small groups of individuals become progressively more isolated from the ancestral stock by breeding preferentially with one another. This type of speciation would be driven by the conjunction of various advantages of inbreeding such as the expression of advantageous recessive phenotypes, reducing the recombination load, and reducing the cost of sex 

The hawthorn fly ("Rhagoletis pomonella"), also known as the apple maggot fly, appears to be undergoing sympatric speciation. Different populations of hawthorn fly feed on different fruits. A distinct population emerged in North America in the 19th century some time after apples, a non-native species, were introduced. This apple-feeding population normally feeds only on apples and not on the historically preferred fruit of hawthorns. The current hawthorn feeding population does not normally feed on apples. Some evidence, such as that six out of thirteen allozyme loci are different, that hawthorn flies mature later in the season and take longer to mature than apple flies; and that there is little evidence of interbreeding (researchers have documented a 4-6% hybridization rate) suggests that sympatric speciation is occurring.

Reinforcement, sometimes referred to as the Wallace effect, is the process by which natural selection increases reproductive isolation. It may occur after two populations of the same species are separated and then come back into contact. If their reproductive isolation was complete, then they will have already developed into two separate incompatible species. If their reproductive isolation is incomplete, then further mating between the populations will produce hybrids, which may or may not be fertile. If the hybrids are infertile, or fertile but less fit than their ancestors, then there will be further reproductive isolation and speciation has essentially occurred (e.g., as in horses and donkeys).

The reasoning behind this is that if the parents of the hybrid offspring each have naturally selected traits for their own certain environments, the hybrid offspring will bear traits from both, therefore would not fit either ecological niche as well as either parent. The low fitness of the hybrids would cause selection to favor assortative mating, which would control hybridization. This is sometimes called the Wallace effect after the evolutionary biologist Alfred Russel Wallace who suggested in the late 19th century that it might be an important factor in speciation. 
Conversely, if the hybrid offspring are more fit than their ancestors, then the populations will merge back into the same species within the area they are in contact.

Reinforcement favoring reproductive isolation is required for both parapatric and sympatric speciation. Without reinforcement, the geographic area of contact between different forms of the same species, called their "hybrid zone," will not develop into a boundary between the different species. Hybrid zones are regions where diverged populations meet and interbreed. Hybrid offspring are very common in these regions, which are usually created by diverged species coming into secondary contact. Without reinforcement, the two species would have uncontrollable inbreeding. Reinforcement may be induced in artificial selection experiments as described below.

Ecological selection is "the interaction of individuals with their environment during resource acquisition". Natural selection is inherently involved in the process of speciation, whereby, "under ecological speciation, populations in different environments, or populations exploiting different resources, experience contrasting natural selection pressures on the traits that directly or indirectly bring about the evolution of reproductive isolation". Evidence for the role ecology plays in the process of speciation exists. Studies of stickleback populations support ecologically-linked speciation arising as a by-product, alongside numerous studies of parallel speciation, where isolation evolves between independent populations of species adapting to contrasting environments than between independent populations adapting to similar environments. Ecological speciation occurs with much of the evidence, "...accumulated from top-down studies of adaptation and reproductive isolation".

It is widely appreciated that sexual selection could drive speciation in many clades, independently of natural selection. However the term “speciation”, in this context, tends to be used in two different, but not mutually exclusive senses. The first and most commonly used sense refers to the “birth” of new species. That is, the splitting of an existing species into two separate species, or the budding off of a new species from a parent species, both driven by a biological "fashion fad" (a preference for a feature, or features, in one or both sexes, that do not necessarily have any adaptive qualities). In the second sense, "speciation" refers to the wide-spread tendency of sexual creatures to be grouped into clearly defined species, rather than forming a continuum of phenotypes both in time and space - which would be the more obvious or logical consequence of natural selection. This was indeed recognized by Darwin as problematic, and included in his "On the Origin of Species" (1859), under the heading "Difficulties with the Theory". There are several suggestions as to how mate choice might play a significant role in resolving .

New species have been created by animal husbandry, but the dates and methods of the initiation of such species are not clear. Often, the domestic counterpart of the wild ancestor can still interbreed and produce fertile offspring as in the case of domestic cattle, that can be considered the same species as several varieties of wild ox, gaur, yak, etc., or domestic sheep that can interbreed with the mouflon.

The best-documented creations of new species in the laboratory were performed in the late 1980s. William R. Rice and George W. Salt bred "Drosophila melanogaster" fruit flies using a maze with three different choices of habitat such as light/dark and wet/dry. Each generation was placed into the maze, and the groups of flies that came out of two of the eight exits were set apart to breed with each other in their respective groups. After thirty-five generations, the two groups and their offspring were isolated reproductively because of their strong habitat preferences: they mated only within the areas they preferred, and so did not mate with flies that preferred the other areas. The history of such attempts is described by Rice and Elen E. Hostert (1993).
Diane Dodd used a laboratory experiment to show how reproductive isolation can develop in "Drosophila pseudoobscura" fruit flies after several generations by placing them in different media, starch- and maltose-based media.

Dodd's experiment has been easy for many others to replicate, including with other kinds of fruit flies and foods. Research in 2005 has shown that this rapid evolution of reproductive isolation may in fact be a relic of infection by "Wolbachia" bacteria.

Alternatively, these observations are consistent with the notion that sexual creatures are inherently reluctant to mate with individuals whose appearance or behavior is different from the norm. The risk that such deviations are due to heritable maladaptations is very high. Thus, if a sexual creature, unable to predict natural selection's future direction, is conditioned to produce the fittest offspring possible, it will avoid mates with unusual habits or features. Sexual creatures will then inevitably tend to group themselves into reproductively isolated species.

Few speciation genes have been found. They usually involve the reinforcement process of late stages of speciation. In 2008, a speciation gene causing reproductive isolation was reported. It causes hybrid sterility between related subspecies. The order of speciation of three groups from a common ancestor may be unclear or unknown; a collection of three such species is referred to as a "trichotomy."

Polyploidy is a mechanism that has caused many rapid speciation events in sympatry because offspring of, for example, tetraploid x diploid matings often result in triploid sterile progeny. However, not all polyploids are reproductively isolated from their parental plants, and gene flow may still occur for example through triploid hybrid x diploid matings that produce tetraploids, or matings between meiotically unreduced gametes from diploids and gametes from tetraploids (see also hybrid speciation).

It has been suggested that many of the existing plant and most animal species have undergone an event of polyploidization in their evolutionary history. Reproduction of successful polyploid species is sometimes asexual, by parthenogenesis or apomixis, as for unknown reasons many asexual organisms are polyploid. Rare instances of polyploid mammals are known, but most often result in prenatal death.

Hybridization between two different species sometimes leads to a distinct phenotype. This phenotype can also be fitter than the parental lineage and as such natural selection may then favor these individuals. Eventually, if reproductive isolation is achieved, it may lead to a separate species. However, reproductive isolation between hybrids and their parents is particularly difficult to achieve and thus hybrid speciation is considered an extremely rare event. The Mariana mallard is thought to have arisen from hybrid speciation.

Hybridization is an important means of speciation in plants, since polyploidy (having more than two copies of each chromosome) is tolerated in plants more readily than in animals. Polyploidy is important in hybrids as it allows reproduction, with the two different sets of chromosomes each being able to pair with an identical partner during meiosis. Polyploids also have more genetic diversity, which allows them to avoid inbreeding depression in small populations.

Hybridization without change in chromosome number is called homoploid hybrid speciation. It is considered very rare but has been shown in "Heliconius" butterflies and sunflowers. Polyploid speciation, which involves changes in chromosome number, is a more common phenomenon, especially in plant species.

Theodosius Dobzhansky, who studied fruit flies in the early days of genetic research in 1930s, speculated that parts of chromosomes that switch from one location to another might cause a species to split into two different species. He mapped out how it might be possible for sections of chromosomes to relocate themselves in a genome. Those mobile sections can cause sterility in inter-species hybrids, which can act as a speciation pressure. In theory, his idea was sound, but scientists long debated whether it actually happened in nature. Eventually a competing theory involving the gradual accumulation of mutations was shown to occur in nature so often that geneticists largely dismissed the moving gene hypothesis. However, 2006 research shows that jumping of a gene from one chromosome to another can contribute to the birth of new species. This validates the reproductive isolation mechanism, a key component of speciation.

There is debate as to the rate at which speciation events occur over geologic time. While some evolutionary biologists claim that speciation events have remained relatively constant and gradual over time (known as "Phyletic gradualism" - see diagram), some palaeontologists such as Niles Eldredge and Stephen Jay Gould have argued that species usually remain unchanged over long stretches of time, and that speciation occurs only over relatively brief intervals, a view known as "punctuated equilibrium". (See diagram, and .)

Evolution can be extremely rapid, as shown in the creation of domesticated animals and plants in a very short geological space of time, spanning only a few tens of thousands of years. Maize ("Zea mays"), for instance, was created in Mexico in only a few thousand years, starting about 7,000 to 12,000 years ago. This raises the question of why the long term rate of evolution is far slower than is theoretically possible.

Evolution is imposed on species or groups. It is not planned or striven for in some Lamarckist way. The mutations on which the process depends are random events, and, except for the "silent mutations" which do not affect the functionality or appearance of the carrier, are thus usually disadvantageous, and their chance of proving to be useful in the future is vanishingly small. Therefore, while a species or group might benefit from being able to adapt to a new environment by accumulating a wide range of genetic variation, this is to the detriment of the "individuals" who have to carry these mutations until a small, unpredictable minority of them ultimately contributes to such an adaptation. Thus, the "capability" to evolve would require group selection, a concept discredited by (for example) George C. Williams, John Maynard Smith and Richard Dawkins as selectively disadvantageous to the individual.

The resolution to Darwin's second dilemma might thus come about as follows:

If sexual individuals are disadvantaged by passing mutations on to their offspring, they will avoid mutant mates with strange or unusual characteristics. Mutations that affect the external appearance of their carriers will then rarely be passed on to the next and subsequent generations. They would therefore seldom be tested by natural selection. Evolution is, therefore, effectively halted or slowed down considerably. The only mutations that can accumulate in a population, on this punctuated equilibrium view, are ones that have no noticeable effect on the outward appearance and functionality of their bearers (i.e., they are "silent" or "neutral mutations," which can be, and are, used to trace the relatedness and age of populations and species.)
This argument implies that evolution can only occur if mutant mates cannot be avoided, as a result of a severe scarcity of potential mates. This is most likely to occur in small, isolated communities. These occur most commonly on small islands, in remote valleys, lakes, river systems, or caves, or during the aftermath of a mass extinction. Under these circumstances, not only is the choice of mates severely restricted but population bottlenecks, founder effects, genetic drift and inbreeding cause rapid, random changes in the isolated population's genetic composition. Furthermore, hybridization with a related species trapped in the same isolate might introduce additional genetic changes. If an isolated population such as this survives its genetic upheavals, and subsequently expands into an unoccupied niche, or into a niche in which it has an advantage over its competitors, a new species, or subspecies, will have come in being. In geological terms, this will be an abrupt event. A resumption of avoiding mutant mates will thereafter result, once again, in evolutionary stagnation.

In apparent confirmation of this punctuated equilibrium view of evolution, the fossil record of an evolutionary progression typically consists of species that suddenly appear, and ultimately disappear, hundreds of thousands or millions of years later, without any change in external appearance. Graphically, these fossil species are represented by lines parallel with the time axis, whose lengths depict how long each of them existed. The fact that the lines remain parallel with the time axis illustrates the unchanging appearance of each of the fossil species depicted on the graph. During each species' existence new species appear at random intervals, each also lasting many hundreds of thousands of years before disappearing without a change in appearance. The exact relatedness of these concurrent species is generally impossible to determine. This is illustrated in the diagram depicting the distribution of hominin species through time since the hominins separated from the line that led to the evolution of our closest living primate relatives, the chimpanzees.

For similar evolutionary time lines see, for instance, the paleontological list of African dinosaurs, Asian dinosaurs, the Lampriformes and Amiiformes.



</doc>
<doc id="29004" url="https://en.wikipedia.org/wiki?curid=29004" title="SQL">
SQL

SQL ( "S-Q-L", "sequel"; Structured Query Language) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e. data incorporating relations among entities and variables.

SQL offers two main advantages over older read–write APIs such as ISAM or VSAM. Firstly, it introduced the concept of accessing many records with one single command. Secondly, it eliminates the need to specify "how" to reach a record, e.g. with or without an index.

Originally based upon relational algebra and tuple relational calculus, SQL consists of many types of statements, which may be informally classed as sublanguages, commonly: a data query language (DQL), a data definition language (DDL), a data control language (DCL), and a data manipulation language (DML). The scope of SQL includes data query, data manipulation (insert, update and delete), data definition (schema creation and modification), and data access control. Although SQL is essentially a declarative language (4GL), it includes also procedural elements.

SQL was one of the first commercial languages to utilize Edgar F. Codd’s relational model. The model was described in his influential 1970 paper, "A Relational Model of Data for Large Shared Data Banks". Despite not entirely adhering to the relational model as described by Codd, it became the most widely used database language.

SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987. Since then, the standard has been revised to include a larger set of features. Despite the existence of such standards, most SQL code is not completely portable among different database systems without adjustments.

SQL was initially developed at IBM by Donald D. Chamberlin and Raymond F. Boyce after learning about the relational model from Ted Codd in the early 1970s. This version, initially called "SEQUEL" ("Structured English Query Language"), was designed to manipulate and retrieve data stored in IBM's original quasi-relational database management system, System R, which a group at IBM San Jose Research Laboratory had developed during the 1970s.

Chamberlin and Boyce's first attempt of a relational database language was Square, but it was difficult to use due to subscript notation. After moving to the San Jose Research Laboratory in 1973, they began work on SEQUEL. The acronym SEQUEL was later changed to SQL because "SEQUEL" was a trademark of the UK-based Hawker Siddeley Dynamics Engineering Limited company.

After testing SQL at customer test sites to determine the usefulness and practicality of the system, IBM began developing commercial products based on their System R prototype including System/38, SQL/DS, and DB2, which were commercially available in 1979, 1981, and 1983, respectively.

In the late 1970s, Relational Software, Inc. (now Oracle Corporation) saw the potential of the concepts described by Codd, Chamberlin, and Boyce, and developed their own SQL-based RDMS with aspirations of selling it to the U.S. Navy, Central Intelligence Agency, and other U.S. government agencies. In June 1979, Relational Software, Inc. introduced the first commercially available implementation of SQL, Oracle V2 (Version2) for VAX computers.

By 1986, ANSI and ISO standard groups officially adopted the standard "Database Language SQL" language definition. New versions of the standard were published in 1989, 1992, 1996, 1999, 2003, 2006, 2008, 2011 and, most recently, 2016.

SQL deviates in several ways from its theoretical foundation, the relational model and its tuple calculus. In that model, a table is a set of tuples, while in SQL, tables and query results are lists of rows: the same row may occur multiple times, and the order of rows can be employed in queries (e.g. in the LIMIT clause).

Critics argue that SQL should be replaced with a language that returns strictly to the original foundation: for example, see "The Third Manifesto". However, no known proof exists that such uniqueness cannot be added to SQL itself, or at least a variation of SQL. In other words, it's quite possible that SQL can be "fixed" or at least improved in this regard such that the industry may not have to switch to a completely different query language to obtain uniqueness. Debate on this remains open.

</math>

The SQL language is subdivided into several language elements, including:


SQL is designed for a specific purpose: to query data contained in a relational database. SQL is a set-based, declarative programming language, not an imperative programming language like C or BASIC. However, extensions to Standard SQL add procedural programming language functionality, such as control-of-flow constructs. These include:

In addition to the standard SQL/PSM extensions and proprietary SQL extensions, procedural and object-oriented programmability is available on many SQL platforms via DBMS integration with other languages. The SQL standard defines SQL/JRT extensions (SQL Routines and Types for the Java Programming Language) to support Java code in SQL databases. Microsoft SQL Server 2005 uses the SQLCLR (SQL Server Common Language Runtime) to host managed .NET assemblies in the database, while prior versions of SQL Server were restricted to unmanaged extended stored procedures primarily written in C. PostgreSQL lets users write functions in a wide variety of languages—including Perl, Python, Tcl, JavaScript (PL/V8) and C.

SQL implementations are incompatible between vendors and do not necessarily completely follow standards. In particular date and time syntax, string concatenation, codice_1s, and comparison case sensitivity vary from vendor to vendor. Particular exceptions are PostgreSQL and Mimer SQL which strive for standards compliance, though PostgreSQL does not adhere to the standard in how folding of unquoted names is done. The folding of unquoted names to lower case in PostgreSQL is incompatible with the SQL standard, which says that unquoted names should be folded to upper case. Thus, codice_2 should be equivalent to codice_3 not codice_4 according to the standard.

Popular implementations of SQL commonly omit support for basic features of Standard SQL, such as the codice_5 or codice_6 data types. The most obvious such examples, and incidentally the most popular commercial and proprietary SQL DBMSs, are Oracle (whose codice_5 behaves as codice_8, and lacks a codice_6 type) and MS SQL Server (before the 2008 version). As a result, SQL code can rarely be ported between database systems without modifications.

There are several reasons for this lack of portability between database systems:


SQL was adopted as a standard by the American National Standards Institute (ANSI) in 1986 as SQL-86 and the International Organization for Standardization (ISO) in 1987. It is maintained by "ISO/IEC JTC 1, Information technology, Subcommittee SC 32, Data management and interchange". The standard is commonly denoted by the pattern: "ISO/IEC 9075-n:yyyy Part n: title", or, as a shortcut, "ISO/IEC 9075".

"ISO/IEC 9075" is complemented by "ISO/IEC 13249: SQL Multimedia and Application Packages" (SQL/MM), which defines SQL based interfaces and packages to widely spread applications like video, audio and spatial data.

Until 1996, the National Institute of Standards and Technology (NIST) data management standards program certified SQL DBMS compliance with the SQL standard. Vendors now self-certify the compliance of their products.

The original standard declared that the official pronunciation for "SQL" was an initialism: ("ess cue el"). Regardless, many English-speaking database professionals (including Donald Chamberlin himself) use the acronym-like pronunciation of ("sequel"), mirroring the language's pre-release development name of "SEQUEL". The SQL standard has gone through a number of revisions:

Interested parties may purchase SQL standards documents from ISO, IEC or ANSI. A draft of SQL:2008 is freely available as a zip archive.

The SQL standard is divided into ten parts. There are gaps in the numbering due to the withdrawal of outdated parts.


ISO/IEC 9075 is complemented by ISO/IEC 13249 "SQL Multimedia and Application Packages". This closely related but separate standard is developed by the same committee. It defines interfaces and packages based on SQL. The aim is a unified access to typical database applications like text, pictures, data mining or spatial data.


ISO/IEC 9075 is also accompanied by a series of Technical Reports, published as ISO/IEC TR 19075 in 8 parts. These Technical Reports explain the justification for and usage of some features of SQL, giving examples where appropriate. The Technical Reports are non-normative; if there is any discrepancy from 9075, the text in 9075 holds. Currently available 19075 Technical Reports are:


A distinction should be made between alternatives to SQL as a language, and alternatives to the relational model itself. Below are proposed relational alternatives to the SQL language. See navigational database and NoSQL for alternatives to the relational model.

Distributed Relational Database Architecture (DRDA) was designed by a work group within IBM in the period 1988 to 1994. DRDA enables network connected relational databases to cooperate to fulfill SQL requests.

An interactive user or program can issue SQL statements to a local RDB and receive tables of data and status indicators in reply from remote RDBs. SQL statements can also be compiled and stored in remote RDBs as packages and then invoked by package name. This is important for the efficient operation of application programs that issue complex, high-frequency queries. It is especially important when the tables to be accessed are located in remote systems.

The messages, protocols, and structural components of DRDA are defined by the Distributed Data Management Architecture.

Chamberlin's 2012 paper discusses four historical criticisms of SQL:

Early specifications did not support major features, such as primary keys. Result sets could not be named, and sub-queries had not been defined. These were added in 1992.

SQL's controversial "NULL" and three-value logic. Predicates evaluated over nulls return the logical value of "unknown" rather than true or false. Features such as outer-join depend on nulls. Null is not equivalent to space. NULL represents no data in the row.

Another popular criticism is that it allows duplicate rows, making integration with languages such as Python, whose data types might make it difficult to accurately represent the data, difficult in terms of parsing and by the absence of modularity.

This can be avoided declaring a unique constraint with one or more fields that identifies uniquely a row in the table. That constraint could also become the primary key of the table.

In a similar sense to Object-relational impedance mismatch, there is a mismatch between the declarative SQL language and the procedural languages that SQL is typically embedded in.


The ISO/IEC Information Technology Task Force publishes publicly available standards including SQL. Technical Corrigenda (corrections) and Technical Reports (discussion documents) are published there.

SQL -- Part 1: Framework (SQL/Framework)

Formal SQL standards are available from ISO and ANSI for a fee. For informative use, as opposed to strict standards compliance, late drafts often suffice.




</doc>
<doc id="29005" url="https://en.wikipedia.org/wiki?curid=29005" title="Strait of Hormuz">
Strait of Hormuz

The Strait of Hormuz ( "Tangeh-ye Hormoz" "Maḍīq Hurmuz" ) is a strait between the Persian Gulf and the Gulf of Oman. It provides the only sea passage from the Persian Gulf to the open ocean and is one of the world's most strategically important choke points. On the north coast lies Iran, and on the south coast the United Arab Emirates and Musandam, an exclave of Oman. The strait is about long, with a width varying from about to .

A third of the world’s liquefied natural gas and almost 25% of total global oil consumption passes through the strait, making it a highly important strategic location for international trade.

The opening to the Persian Gulf was described, but not given a name, in the "Periplus of the Erythraean Sea", a 1st-century mariner's guide:

In the 10th17th centuries AD, the Kingdom of Ormus, which seems to have given the strait its name, was located here. Scholars, historians and linguists derive the name "Ormuz" from the local Persian word "Hur-mogh" meaning date palm. In the local dialects of Hurmoz and Minab this strait is still called Hurmogh and has the aforementioned meaning.
The resemblance of this word with the name of the Zoroastrian god "Hormoz" (a variant of "Ahura Mazda") has resulted in the popular belief that these words are related.

Jodocus Hondius labels the Strait "Basora fretum" ("Strait of Basra") on his .

To reduce the risk of collision, ships moving through the Strait follow a Traffic Separation Scheme (TSS): inbound ships use one lane, outbound ships another, each lane being two miles wide. The lanes are separated by a two-mile-wide "median".

To traverse the Strait, ships pass through the territorial waters of Iran and Oman under the transit passage provisions of the United Nations Convention on the Law of the Sea.
Although not all countries have ratified the convention, most countries, including the U.S., accept these customary navigation rules as codified in the Convention.

In April 1959 Iran altered the legal status of the strait by expanding its territorial sea to and declaring that it would recognize only transit by innocent passage through the newly expanded area. In July 1972, Oman also expanded its territorial sea to by decree. Thus, by mid-1972, the Strait of Hormuz was completely "closed" by the combined territorial waters of Iran and Oman. During the 1970s, neither Iran or Oman attempted to impede the passage of warships through the strait, but in the 1980s, both countries asserted claims that were different from customary (old) law. Upon ratifying UNCLOS in August 1989, Oman submitted declarations confirming its 1981 royal decree that only innocent passage is permitted through its territorial sea. The declarations further asserted that prior permission was required before foreign warships could pass through Omani territorial waters. Upon signing the convention in December 1982, Iran entered a declaration stating “that only states parties to the Law of the Sea Convention shall be entitled to benefit from the contractual rights created therein”, including “the right of transit passage through straits used for international navigation”. In May 1993, Iran enacted a comprehensive law on maritime areas, several provisions of which conflict with UNCLOS provisions, including a requirement that warships, submarines, and nuclear-powered ships obtain permission before exercising innocent passage through Iran’s territorial waters.The United States does not recognize any of the claims by Oman and Iran and has contested each of them.

Oman has a radar site Link Quality Indicator (LQI) to monitor the TSS in the Strait of Hormuz. This site is on a small island on the peak of Musandam Governorate.

A 2007 report from the Center for Strategic and International Studies also stated that 17 million barrels passed out of the Persian Gulf daily, but that oil flows through the Strait accounted for roughly 40% of all world-traded oil.

According to the U.S. Energy Information Administration, in 2011, an average of 14 tankers per day passed out of the Persian Gulf through the Strait carrying of crude oil. This was said to represent 35% of the world's seaborne oil shipments and 20% of oil traded worldwide. The report stated that more than 85% of these crude oil exports went to Asian markets, with Japan, India, South Korea and China the largest destinations. In 2018 alone, 21 million barrels a day were passing through the strait - this means $1.17 billion worth of oil a day, at September 2019 prices.

The Tanker War phase of the Iran–Iraq War started when Iraq attacked the oil terminal and oil tankers at Iran's Kharg Island in early 1984. Saddam Hussein's aim in attacking Iranian shipping was, among other things, to provoke the Iranians to retaliate with extreme measures, such as closing the Strait of Hormuz to all maritime traffic, thereby bringing American intervention. Iran limited the retaliatory attacks to Iraqi shipping, leaving the strait open.

On 18 April 1988, the U.S. Navy waged a one-day battle against Iranian forces in and around the strait. The battle, dubbed Operation Praying Mantis by the U.S., was launched in retaliation for the USS "Samuel B. Roberts" striking a mine laid in the channel by Iran on 14 April. U.S. forces sank one frigate, one gunboat, and up to six armed speedboats, as well as seriously damaging a second frigate.

On 3 July 1988, 290 people were killed when an Iran Air Airbus A300 was shot down over the strait by the United States Navy guided missile cruiser USS "Vincennes" (CG-49) when it was wrongly identified as a jet fighter.

On 8 January 2007, the nuclear submarine USS "Newport News", traveling submerged, struck , a 300,000-ton Japanese-flagged very large crude tanker, south of the strait. There were no injuries, and no oil leaked from the tanker.

A series of naval stand-offs between Iranian speedboats and U.S. warships in the Strait of Hormuz occurred in December 2007 and January 2008. U.S. officials accused Iran of harassing and provoking their naval vessels, but Iranian officials denied the allegations. On 14 January 2008, U.S. Navy officials appeared to contradict the Pentagon version of the 16 January event, in which the Pentagon had reported that U.S. vessels had almost fired on approaching Iranian boats. The Navy's regional commander, Vice Admiral Kevin Cosgriff, said the Iranians had "neither anti-ship missiles nor torpedoes" and he "wouldn't characterize the posture of the US 5th Fleet as afraid of these small boats".

On 29 June 2008, the commander of Iran's Revolutionary Guard, Mohammad Ali Jafari, said that if either Israel or the United States attacked Iran, it would seal off the Strait of Hormuz to wreak havoc in the oil markets. This followed more ambiguous threats from Iran's oil minister and other government officials that an attack on Iran would result in turmoil in the world's oil supply.

Vice Admiral Kevin Cosgriff, commander of the U.S. 5th Fleet stationed in Bahrain across the Persian Gulf from Iran, warned that such Iranian action would be considered an act of war, and the U.S. would not allow Iran to hold hostage nearly a third of the world's oil supply.

On 8 July 2008, Ali Shirazi, a mid-level clerical aide to Iran's Supreme Leader Ayatollah Ali Khamenei, was quoted by the student news agency ISNA as telling the Revolutionary Guards, "The Zionist regime is pressuring White House officials to attack Iran. If they commit such a stupidity, Tel Aviv and U.S. shipping in the Persian Gulf will be Iran's first targets and they will be burned."

In the last week of July 2008, in the Operation Brimstone, dozens of U.S. and foreign naval ships came to undergo joint exercises for possible military activity in the shallow waters off the coast of Iran.

As of 11 August 2008, more than 40 U.S. and allied ships reportedly were en route to the Strait of Hormuz. One U.S. carrier battle group from Japan would complement the two which are already in the Persian Gulf, for a total of five battle groups, not including the submarines.

On 20 March 2009, United States Navy collided with the in the strait. The collision, which slightly injured 15 sailors aboard "Hartford", ruptured a fuel tank aboard "New Orleans", spilling of marine diesel fuel.

On 27 December 2011, Iranian Vice President Mohammad-Reza Rahimi threatened to cut off oil supply from the Strait of Hormuz should economic sanctions limit, or cut off, Iranian oil exports. A U.S. Fifth Fleet spokeswoman said in response that the Fleet was "always ready to counter malevolent actions", whilst Admiral Habibollah Sayyari of the Iranian navy claimed that cutting off oil shipments would be "easy". Despite an initial 2% rise in oil prices, oil markets ultimately did not react significantly to the Iranian threat, with oil analyst Thorbjoern Bak Jensen of Global Risk Management concluding that "they cannot stop the flow for a longer period due to the amount of U.S. hardware in the area".
On 3 January 2012, Iran threatened to take action if the U.S. Navy moves an aircraft carrier back into the Persian Gulf. Iranian Army chief Ataollah Salehi said the United States had moved an aircraft carrier out of the Persian Gulf because of Iran's naval exercises, and Iran would take action if the ship returned. "Iran will not repeat its warning...the enemy's carrier has been moved to the Gulf of Oman because of our drill. I recommend and emphasize to the American carrier not to return to the Persian Gulf", he said.

The U.S. Navy spokesman Commander Bill Speaks quickly responded that deployment of U.S. military assets would continue as has been the custom stating: "The U.S. Navy operates under international maritime conventions to maintain a constant state of high vigilance in order to ensure the continued, safe flow of maritime traffic in waterways critical to global commerce."

While earlier statements from Iran had little effect on global oil markets, coupled with the new sanctions, these comments from Iran are driving crude futures higher, up over 4%. Pressure on prices reflect a combination of uncertainty driven further by China's recent response – reducing oil January 2012 purchases from Iran by 50% compared to those made in 2011.

The U.S. led sanctions may be "beginning to bite" as Iranian currency has recently lost some 12% of its value. Further pressure on Iranian currency was added by French Foreign Minister Alain Juppé who was quoted as calling for more "strict sanctions" and urged EU countries to follow the US in freezing Iranian central bank assets and imposing an embargo on oil exports.

On 7 January 2012, the British government announced that it would be sending the Type 45 destroyer to the Persian Gulf. "Daring", which is the lead ship of her class is one of the "most advanced warships" in the world, and will undertake its first mission in the Persian Gulf. The British Government however have said that this move has been long-planned, as "Daring" will replace another Armilla patrol frigate.
On 9 January 2012, Iranian Defense Minister Ahmad Vahidi denied that Iran had ever claimed that it would close the Strait of Hormuz, saying that "the Islamic Republic of Iran is the most important provider of security in the strait... if one threatens the security of the Persian Gulf, then all are threatened."

The Iranian Foreign Ministry confirmed on 16 January 2012 that it has received a letter from the United States concerning the Strait of Hormuz, "via three different channels." Authorities were considering whether to reply, although the contents of the letter were not divulged. The United States had previously announced its intention to warn Iran that closing the Strait of Hormuz is a "red line" that would provoke an American response. Gen. Martin E. Dempsey, the chairman of the Joint Chiefs of Staff, said this past weekend that the United States would "take action and re-open the strait,” which could be accomplished only by military means, including minesweepers, warship escorts and potentially airstrikes. Defense Secretary Leon E. Panetta told troops in Texas that the United States would not tolerate Iran's closing of the strait. Nevertheless, Iran continued to discuss the impact of shutting the Strait on world oil markets, saying that any disruption of supply would cause a shock to markets that "no country" could manage.

By 23 January, a flotilla had been established by countries opposing Iran's threats to close the Hormuz Strait. These ships operated in the Persian Gulf and Arabian Sea off the coast of Iran. The flotilla included three American aircraft carriers (the , the and ) and three destroyers (, , ), seven British warships, including the destroyer and a number of Type 23 frigates (, , and ), and a French warship, the frigate "La Motte-Picquet" .

On 24 January, tensions rose further after the European Union imposed sanctions on Iranian oil. A senior member of Iran's parliament said that the Islamic Republic would close the entry point to the Persian Gulf if new sanctions block its oil exports. "If any disruption happens regarding the sale of Iranian oil, the Strait of Hormuz will definitely be closed," Mohammad Kossari, deputy head of parliament's foreign affairs and national security committee, told the semi-official Fars News Agency.

On April 28, 2015, IRGCN patrol boats contacted the Marshall Islands-flagged container ship "Maersk Tigris", which was westbound through the strait, and directed the ship to proceed further into Iranian territorial waters, according to a spokesman for the U.S. Defense Department. When the ship's master declined, one of the Iranian craft fired shots across the bridge of "Maersk Tigris". The captain complied and proceeded into Iranian waters near Larak Island. The US Navy sent aircraft and a destroyer, USS "Farragut", to monitor the situation.

Maersk says they have agreed to pay an Iranian company $163,000 over a dispute about 10 container boxes transported to Dubai in 2005. The court ruling allegedly ordered a fine of $3.6 million.

In July 2018, Iran again made threats to close the strait. Citing looming American sanctions after the U.S withdrew from the JCPOA deal earlier in the year. Iran’s Revolutionary Guards reported they were ready to carry out the action if required.

In August 2018, Iran test-fired a ballistic missile for the first time in 2018. According to the officials, the anti-ship Fateh-110 Mod 3 flew over 100 miles on a flight path over the Strait of Hormuz to a test range in the Iranian desert. “It was shore-to-shore,” said one U.S. official describing the launch, who like the others requested anonymity to discuss sensitive information.

On the morning of June 13, 2019, the oil tankers "Front Altair" and "Kokuka Courageous" were both rocked by explosions shortly before dawn, the crew of the latter reported seeing a flying object strike the ship; the crew were rescued by the destroyer while the crew of the "Front Altair" were rescued by Iranian ships. That afternoon, U.S. secretary of state Mike Pompeo issued a statement accusing Iran of the attacks. Iran subsequently denied the accusations, calling the incident a false-flag attack.

In July 2019, a Stena Bulk Tanker, "Stena Impero", sailing under a British flag, was boarded and captured by Iranian forces. The spokesman for Iran’s Guardian Council, Abbas Ali Kadkhodaei, was quoted as describing the seizure as a "reciprocal action." This was presumed to be in reference to the seizure of an Iranian Tanker, "Grace 1", bound for Syria in Gibraltar a few days prior.

Millennium Challenge 2002 was a major war game exercise conducted by the United States armed forces in 2002. According to a 2012 article in The Christian Science Monitor, it simulated an attempt by Iran to close the strait. The assumptions and results were controversial.

A 2008 article in "International Security" contended that Iran could seal off or impede traffic in the Strait for a month, and an attempt by the U.S. to reopen it would be likely to escalate the conflict. In a later issue, however, the journal published a response which questioned some key assumptions and suggested a much shorter timeline for re-opening.

In December 2011, Iran's navy began a ten-day exercise in international waters along the strait. The Iranian Navy Commander, Rear Admiral Habibollah Sayyari, stated that the strait would not be closed during the exercise; Iranian forces could easily accomplish that but such a decision must be made at a political level.

Captain John Kirby, a Pentagon spokesman, was quoted in a December 2011 Reuters article: "Efforts to increase tension in that part of the world are unhelpful and counter-productive. For our part, we are comfortable that we have in the region sufficient capabilities to honor our commitments to our friends and partners, as well as the international community." In the same article, Suzanne Maloney, an Iran expert at the Brookings Institution, said, "The expectation is that the U.S. military could address any Iranian threat relatively quickly."

General Martin Dempsey, Chairman of the Joint Chiefs of Staff, said in January 2012 that Iran "has invested in capabilities that could, in fact, for a period of time block the Strait of Hormuz." He also stated, "We've invested in capabilities to ensure that if that happens, we can defeat that."

In May 2012, a learned article concluded that both the UNCLOS and the 1958 Convention on the High Seas would be violated if Iran followed through on its threat to block passage through the Straits of vessels such as oil tankers, and that the act of passage bears no relation in law to the imposition of economic sanctions. The coastal state is limited in its powers to prevent passage: 1) if threat or actual use of force against its sovereignty, its territorial integrity, or its political independence; or 2) the vessel in any other way violates the principles of international law such as embodied in the Charter of the United Nations.

In June 2012, Saudi Arabia reopened the Iraq Pipeline through Saudi Arabia (IPSA), which was confiscated from Iraq in 2001 and travels from Iraq across Saudi Arabia to a Red Sea port. It will have a capacity of 1.65 million barrels per day.

In July 2012, the UAE began using the new Habshan–Fujairah oil pipeline from the Habshan fields in Abu Dhabi to the Fujairah oil terminal on the Gulf of Oman, effectively bypassing the Strait of Hormuz. It has a maximum capacity of around 2 million barrels per day, over three-quarters of the UAE's 2012 production rate. The UAE is also increasing Fujairah's storage and off-loading capacities. The UAE is building the world's largest crude oil storage facility in Fujairah with a capacity of holding 14 million barrels to enhance Fujairah's growth as a global oil and trading hub. The Habshan – Fujairah route secures the UAE's energy security and has the advantage of being a ground oil pipeline transportation which is considered the cheapest form of oil transportation and also reduces insurance costs as oil tankers would no longer enter the Persian Gulf.

In a July 2012 "Foreign Policy" article, Gal Luft compared Iran and the Strait of Hormuz to the Ottoman Empire and the Dardanelles, a choke point for shipments of Russian grain a century ago. He indicated that tensions involving the Strait of Hormuz are leading those currently dependent on shipments from the Persian Gulf to find alternative shipping capabilities. He stated that Saudi Arabia was considering building new pipelines to Oman and Yemen, and that Iraq might revive the disused Iraq–Syria pipeline to ship crude to the Mediterranean. Luft stated that reducing Hormuz traffic "presents the West with a new opportunity to augment its current Iran containment strategy."






</doc>
<doc id="29006" url="https://en.wikipedia.org/wiki?curid=29006" title="Space telescope">
Space telescope

A space telescope or space observatory is an instrument located in outer space to observe distant planets, galaxies and other astronomical objects. Space telescopes avoid the filtering of ultraviolet frequencies, X-rays and gamma rays; the distortion (scintillation) of electromagnetic radiation; as well as light pollution which ground-based observatories encounter.

Suggested by Lyman Spitzer in 1946, the first operational space telescopes were the American Orbiting Astronomical Observatory, OAO-2 launched in 1968, and the Soviet Orion 1 ultraviolet telescope aboard space station Salyut 1 in 1971.

Space telescopes are distinct from satellites that point toward Earth for satellite imagery for espionage, weather analysis and other types of information gathering. Space observatories are divided into two types: Astronomical survey satellites to map the entire sky, and satellites which focus on selected astronomical objects or parts of the sky and beyond.

Wilhelm Beer and Johann Heinrich Mädler in 1837 discussed the advantages of an observatory on the Moon. In 1946, American theoretical astrophysicist Lyman Spitzer proposed a telescope in space, 11 years before the Soviet Union launched the first satellite, "Sputnik 1". Spitzer's proposal called for a large telescope that would not be hindered by Earth's atmosphere. After lobbying in the 1960s and 70s for such a system to be built, Spitzer's vision ultimately materialized into the Hubble Space Telescope, which was launched on April 24, 1990 by the Space Shuttle "Discovery" (STS-31).

Performing astronomy from ground-based observatories on Earth is limited by the filtering and distortion of electromagnetic radiation (scintillation or twinkling) due to the atmosphere. Some terrestrial telescopes can reduce atmospheric effects with adaptive optics. A telescope orbiting Earth outside the atmosphere is subject neither to twinkling nor to light pollution from artificial light sources on Earth. As a result, the angular resolution of space telescopes is often much smaller than a ground-based telescope with a similar aperture.
Space-based astronomy is even more important for frequency ranges which are outside the optical window and the radio window, the only two wavelength ranges of the electromagnetic spectrum that are not severely attenuated by the atmosphere. For example, X-ray astronomy is nearly impossible when done from Earth, and has reached its current importance in astronomy only due to orbiting X-ray telescopes such as the Chandra observatory and the XMM-Newton observatory. Infrared and ultraviolet are also largely blocked.

Space telescopes are much more expensive to build than ground-based telescopes. Due to their location, space telescopes are also extremely difficult to maintain. The Hubble Space Telescope was serviced by the Space Shuttle, but most space telescopes cannot be serviced at all.

Satellites have been launched and operated by NASA, ISRO, ESA, Japanese Space Agency and the Soviet space program later succeeded by Roscosmos of Russia. As of 2018, many space observatories have already completed their missions, while others continue operating on extended time. However, the future availability of space telescopes and observatories depends on timely and sufficient funding. While future space observatories are planned by NASA, JAXA and the China National Space Administration, scientists fear that there would be gaps in coverage that would not be covered immediately by future projects and this would affect research in fundamental science.




</doc>
<doc id="29007" url="https://en.wikipedia.org/wiki?curid=29007" title="Saint David">
Saint David

Saint David (; ; ) was a Welsh bishop of Mynyw (now St Davids) during the 6th century. He is the patron saint of Wales. David was a native of Wales, and a relatively large amount of information is known about his life. His birth date, however, is uncertain: suggestions range from 462 to 512. He is traditionally believed to be the son of Saint Non and the grandson of Ceredig ap Cunedda, king of Ceredigion. The Welsh annals placed his death 569 years after the birth of Christ, but Phillimore's dating revised this to 601.

Many of the traditional tales about David are found in the "Buchedd Dewi" ("Life of David"), a hagiography written by Rhygyfarch in the late 11th century. Rhygyfarch claimed it was based on documents found in the cathedral archives. Modern historians are sceptical of some of its claims: one of Rhygyfarch's aims was to establish some independence for the Welsh church, which had refused the Roman rite until the 8th century and now sought a metropolitan status equal to that of Canterbury. (This may apply to the supposed pilgrimage to Jerusalem where he is said to have been anointed as an archbishop by the patriarch).

The tradition that he was born at Henfynyw (Vetus-Menevia) in Ceredigion is not improbable. He became renowned as a teacher and preacher, founding monastic settlements and churches in Wales, Dumnonia, and Brittany. St David's Cathedral stands on the site of the monastery he founded in the Glyn Rhosyn valley of Pembrokeshire. Around 550, he attended the Synod of Brefi, where his eloquence in opposing Pelagianism caused his fellow monks to elect him primate of the region. As such he presided over the synod of Caerleon (the "Synod of Victory") around 569.

His best-known miracle is said to have taken place when he was preaching in the middle of a large crowd at the Synod of Brefi: the village of Llanddewi Brefi stands on the spot where the ground on which he stood is reputed to have risen up to form a small hill. A white dove, which became his emblem, was seen settling on his shoulder. John Davies notes that one can scarcely "conceive of any miracle more superfluous" in that part of Wales than the creation of a new hill. David is said to have denounced Pelagianism during this incident and he was declared archbishop by popular acclaim according to Rhygyfarch, bringing about the retirement of Dubricius. St David's metropolitan status as an archbishopric was later supported by Bernard, Bishop of St David's, Geoffrey of Monmouth and Gerald of Wales.

The Monastic Rule of David prescribed that monks had to pull the plough themselves without draught animals, and must drink only water and eat only bread with salt and herbs. The monks spent their evenings in prayer, reading and writing. No personal possessions were allowed: even to say "my book" was considered an offence. He lived a simple life and practised asceticism, teaching his followers to refrain from eating meat and drinking beer. His symbol, also the symbol of Wales, is the leek (this inspires a reference in Shakespeare's Henry V, Act V scene 1) :

"Fluellen: "If your Majesty is remembered of it, the Welshmen did good service in a garden where leeks did grow, wearing leeks in their Monmouth caps, which your Majesty knows, to this hour is an honourable badge of the service, and I do believe, your Majesty takes no scorn to wear the leek upon Saint Tavy's day". King Henry: "I wear it for a memorable honour; for I am Welsh, you know, good countryman"."

Rhigyfarch counted Glastonbury Abbey among the churches David founded. Around forty years later William of Malmesbury, believing the Abbey older, said that David visited Glastonbury only to rededicate the Abbey and to donate a travelling altar including a great sapphire. He had had a vision of Jesus who said that "the church had been dedicated long ago by Himself in honour of His Mother, and it was not seemly that it should be re-dedicated by human hands". So David instead commissioned an extension to be built to the abbey, east of the Old Church. (The dimensions of this extension given by William were verified archaeologically in 1921). One manuscript indicates that a sapphire altar was among the items Henry VIII of England confiscated from the abbey during the Dissolution of the Monasteries a thousand years later.

Though the exact date of his death is not certain, tradition holds that it was on 1 March, which is the date now marked as Saint David's Day. The two most common years given for his death are 601 and 589. The monastery is said to have been "filled with angels as Christ received his soul". His last words to his followers were in a sermon on the previous Sunday. The Welsh Life of St David gives these as, "Arglwydi, vrodyr, a chwioryd, Bydwch lawen a chedwch ych ffyd a'ch cret, a gwnewch y petheu bychein a glywyssawch ac a welsawch gennyf i. A mynheu a gerdaf y fford yd aeth an tadeu idi", which translates as, "Lords, brothers and sisters, Be joyful, and keep your faith and your creed, and do the little things that you have seen me do and heard about. And as for me, I will walk the path that our fathers have trod before us." "Do ye the little things in life" ("Gwnewch y pethau bychain mewn bywyd") is today a very well known phrase in Welsh. The same passage states that he died on a Tuesday, from which attempts have been made to calculate the year of his death.

David was buried at St David's Cathedral at St Davids, Pembrokeshire, where his shrine was a popular place of pilgrimage throughout the Middle Ages. During the 10th and 11th centuries the Cathedral was regularly raided by Vikings, who removed the shrine from the church and stripped off the precious metal adornments. In 1275 a new shrine was constructed, the ruined base of which remains to this day (see photo), which was originally surmounted by an ornamental wooden canopy with murals of David, Patrick and Denis. The relics of David and Justinian of Ramsey Island were kept in a portable casket on the stone base of the shrine. It was at this shrine that Edward I came to pray in 1284. During the reformation Bishop Barlow (1536–48), a staunch Protestant, stripped the shrine of its jewels and confiscated the relics of David and Justinian.

David was officially recognised at the Holy See by Pope Callixtus II in 1120, thanks to the work of Bernard, Bishop of St David's. Music for his Liturgy of the Hours has been edited by O. T. Edwards in "Matins, Lauds and Vespers for St David's Day: the Medieval Office of the Welsh Patron Saint in National Library of Wales MS 20541 E" (Cambridge, 1990). David was also canonized by the Eastern Orthodox Church at an unknown date.

Over 50 churches in South Wales were dedicated to him in pre-Reformation days.

In the 2004 edition of the Roman Martyrology, David is listed under 1 March with the Latin name "Dávus". He is recognised as bishop of Menevia in Wales who governed his monastery following the example of the Eastern Fathers. Through his leadership, many monks went forth to evangelise Wales, Ireland, Cornwall and Armorica (Brittany and surrounding provinces).

The restored Shrine of Saint David was unveiled and rededicated by the Right Reverend Wyn Evans, Bishop of St David's, at a Choral Eucharist on Saint David's Day, 2012.

A broadside ballad published around 1630 claimed that the Welsh wore a leek in their hats to commemorate a battle fought on St David's Day. So as to recognise friend from foe, the Welsh had pulled up leeks from a garden and put them in their hats, before going on to win the battle.

He is usually represented standing on a little hill, with a dove on his shoulder.

David's popularity in Wales is shown by the "Armes Prydein" of around 930, a popular poem which prophesied that in the future, when all might seem lost, the "Cymry" (Welsh people) would unite behind the standard of David to defeat the English; "A lluman glân Dewi a ddyrchafant" ("And they will raise the pure banner of Dewi").

David is said to have played a role in spreading Christianity on the continent, inspiring numerous place names in Brittany including Saint-Divy, Saint-Yvi and Landivy.

David's life and teachings have inspired a choral work by Welsh composer Karl Jenkins, "Dewi Sant". It is a seven-movement work best known for the classical crossover series Adiemus, which intersperses movements reflecting the themes of David's last sermon with those drawing from three Psalms. An oratorio by another Welsh composer Arwel Hughes, also entitled "Dewi Sant", was composed in 1950.

Saint David is also thought to be associated with corpse candles, lights that would warn of the imminent death of a member of the community. The story goes that David prayed for his people to have some warning of their death, so that they could prepare themselves. In a vision, David's wish was granted and told that from then on, people who lived in the land of Dewi Sant (Saint David) "would be forewarned by the dim light of mysterious tapers when and where the death might be expected". The colour and size of the tapers indicated whether the person to die would be a woman, man, or child.






</doc>
<doc id="29010" url="https://en.wikipedia.org/wiki?curid=29010" title="Saint George">
Saint George

Saint George (, "Geṓrgios"; ; d. 23 April 303), also George of Lydda, was a soldier of Cappadocian Greek origins, member of the Praetorian Guard for Roman emperor Diocletian, who was sentenced to death for refusing to recant his Christian faith. He became one of the most venerated saints and megalo-martyrs in Christianity, and he has been especially venerated as a military saint since the Crusades.

In hagiography, as one of the Fourteen Holy Helpers and one of the most prominent military saints, he is immortalised in the legend of Saint George and the Dragon. His memorial, Saint George's Day, is traditionally celebrated on 23 April. (See under "Feast days" below for the use of the Julian calendar by the Eastern Orthodox Church.)

England, Ethiopia, Georgia, Catalonia, and several other nation states, cities, universities, professions and organisations all claim Saint George as their patron.

Very little is known about St George's life, but it is thought he was a Roman officer of Greek descent from Cappadocia who was martyred in one of the pre-Constantinian persecutions. Beyond this, early sources give conflicting information.

There are two main versions of the legend, a Greek and a Latin version, which can both be traced to the 5th or 6th century. The saint's veneration dates to the 5th century with some certainty, and possibly still to the 4th. The addition of the dragon legend dates to the 11th century.

The earliest text which preserves fragments of George's narrative is in a Greek hagiography which is identified by Hippolyte Delehaye of the scholarly Bollandists to be a palimpsest of the 5th century. 
An earlier work by Eusebius, "Church history", written in the 4th century, contributed to the legend but did not name George or provide significant detail.
The work of the Bollandists Daniel Papebroch, Jean Bolland, and Godfrey Henschen in the 17th century was one of the first pieces of scholarly research to establish the saint's historicity via their publications in "Bibliotheca Hagiographica Graeca". Pope Gelasius I stated that George was among those saints "whose names are justly reverenced among men, but whose actions are known only to God."

The most complete version of the fifth century Greek text survives in a translation into Syriac from about 600. From text fragments preserved in the British Library a translation into English was published in 1925.

In the Greek tradition, George was born to Greek Christian parents, in Cappadocia.
His father died for the faith when George was fourteen, and his mother returned with George to her homeland of Syria Palaestina.
A few years later, George's mother died. George travelled to the capital Nicomedia and joined the Roman army. 
George was persecuted by one "Dadianus". 
In later versions of the Greek legend, this name is rationalized to Diocletian, and George's martyrdom is placed in the Diocletian persecution of AD 303. The setting in Nicomedia is also secondary, and inconsistent with the earliest cultus of the saint being located in Diospolis.

George was executed by decapitation before Nicomedia's city wall, on 23 April 303. A witness of his suffering convinced Empress Alexandra of Rome to become a Christian as well, so she joined George in martyrdom. His body was returned to Lydda for burial, where Christians soon came to honour him as a martyr.

The Latin "Acta Sancti Georgii" (6th century) follows the general course of the Greek legend, but Diocletian here becomes "Dacian, Emperor of the Persians". George lived and died in Melitene in Cappadocia.
His martyrdom was greatly extended to more than twenty separate tortures over the course of seven years.
Over the course of his martyrdom, 40,900 pagans were converted to Christianity, including the empress Alexandra. When George finally died, the wicked Dacian was carried away in a whirlwind of fire.
In later Latin versions, the persecutor is the Roman emperor Decius, or a Roman judge named Dacian serving under Diocletian.

There is little information on the early life of Saint George. Herbert Thurston in "The Catholic Encyclopedia" states that based upon an ancient cultus, narratives of the early pilgrims, and the early dedications of churches to Saint George, going back to the fourth century, "there seems, therefore, no ground for doubting the historical existence of St. George", although no faith can be placed in either the details of his history or his alleged exploits.

According to Donald Attwater, "No historical particulars of his life have survived, ... The widespread veneration for St George as a soldier saint from early times had its centre in Palestine at Diospolis, now Lydda. St George was apparently martyred there, at the end of the third or the beginning of the fourth century; that is all that can be reasonably surmised about him."

It has been established that Saint George the Martyr and the Arian Bishop George of Alexandria were not identical; furthermore that Bishop George was slain by Gentile Greeks for exacting onerous taxes, especially inheritance taxes. And that Saint George in all likelihood was martyred before the year 290. Although the Diocletianic Persecution of 303, associated with military saints because the persecution was aimed at Christians among the professional soldiers of the Roman army, is of undisputed historicity, the identity of Saint George as a historical individual had not been ascertained as of Edmund Spenser's day.

Edward Gibbon argued that George, or at least the legend from which the above is distilled, is based on George of Cappadocia, a notorious Arian bishop who was Athanasius of Alexandria's most bitter rival, and that it was he who in time became Saint George of England. J. B. Bury, who edited the 1906 edition of "The Decline and Fall", wrote "this theory of Gibbon's has nothing to be said for it." He adds that: "the connection of St. George with a dragon-slaying legend does not relegate him to the region of the myth".

The legend of Saint George and the Dragon was first recorded in the 11th century, in a Georgian source.
It reached Europe in the 12th century. In the "Golden Legend", by 13th-century Archbishop of Genoa Jacobus da Varagine, George's death was at the hands of Dacian, and about the year 287.
The tradition tells that a fierce dragon was causing panic at the city of Silene, Libya, at the time Saint George arrived there. In order to prevent the dragon from devastating people from the city, they gave two sheep each day to the dragon, but when the sheep were not enough they were forced to sacrifice humans instead of the two sheep. The human to be sacrificed was elected by the city's own people and that time the king's daughter was chosen to be sacrificed but no one was willing to take her place. Saint George saved the girl by slaying the dragon with a lance. The king was so grateful that he offered him treasures as a reward for saving his daughter's life, but Saint George refused it and instead he gave these to the poor. The people of the city were so amazed at what they had witnessed that they became Christians and were all baptized.

The "Golden Legend" offered a historicised narration of George's encounter with a dragon.
This account was very influential and it remains the most familiar version in English owing to William Caxton's 15th-century translation.

In the mediaeval romances, the lance with which Saint George slew the dragon was called Ascalon, after the Levantine city of Ashkelon, today in Israel. The name "Ascalon" was used by Winston Churchill for his personal aircraft during World War II, according to records at Bletchley Park. In Sweden, the princess rescued by Saint George is held to represent the kingdom of Sweden, while the dragon represents an invading army. Several sculptures of Saint George battling the dragon can be found in Stockholm, the earliest inside Storkyrkan ("The Great Church") in the Old Town. Iconography of the horseman with spear overcoming evil was widespread throughout the Christian period.

George (, "Jiriyas" or "Girgus") is included in some Muslim texts as a prophetic figure. The Islamic sources state that he lived among a group of believers who were in direct contact with the last apostles of Jesus. He is described as a rich merchant who opposed erection of Apollo's statue by Mosul's king Dadan. After confronting the king, George was tortured many times to no effect, was imprisoned and was aided by the angels. Eventually, he exposed that the idols were possessed by Satan, but was martyred when the city was destroyed by God in a rain of fire.

Muslim scholars had tried to find a historical connection of the saint due to his popularity. According to Muslim legend, he was martyred under the rule of Diocletian and was killed three times but resurrected every time. The legend is more developed in the Persian version of al-Tabari wherein he resurrects the dead, makes trees sprout and pillars bear flowers. After one of his deaths, the world is covered by darkness which is lifted only when he is resurrected. He is able to convert the queen but she is put to death. He then prays to God to allow him to die, which prayer is granted.

Al-Tha`labi states that he was from Palestine and lived in the times of some disciples of Jesus. He was killed many times by the king of Mosul, and resurrected each time. When the king tried to starve him, he touched a piece of dry wood brought by a woman and turned it green, with varieties of fruits and vegetables growing from it. After his fourth death, the city was burnt along with him. Ibn al-Athir's account of one of his deaths is parallel to the crucifixion of Jesus, stating, "When he died, God sent stormy winds and thunder and lightning and dark clouds, so that darkness fell between heaven and earth, and people were in great wonderment..." The account adds that the darkness was lifted after his resurrection.

A titular church built in Lydda during the reign of Constantine the Great (reigned 306–37) was consecrated to "a man of the highest distinction", according to the church history of Eusebius; the name of the "titulus" "patron" was not disclosed, but later he was asserted to have been George.

The veneration of George spread from Syria Palaestina through Lebanon to the rest of the Byzantine Empire—though the martyr is not mentioned in the Syriac "Breviarium"—and the region east of the Black Sea. 
By the 5th century, the veneration of Saint George had reached the Christian Western Roman Empire, as well: in 494, George was canonized as a saint by Pope Gelasius I, among those "whose names are justly reverenced among men, but whose acts are known only to [God]."

The early cult of the saint was localized in Diospolis (Lydda), in Palestine.
The first description of Lydda as a pilgrimage site where George's relics were venerated is "De Situ Terrae Sanctae" by
the archdeacon Theodosius, written between 518 and 530.
By the end of the 6th century, the center of his veneration appears to have shifted to Cappadocia.
The "Life" of Saint Theodore of Sykeon, written in the 7th century, mentions the veneration of the relics of the saint in Cappadocia.

By the time of the early Muslim conquests of the mostly Christian and Zoroastrian Middle East, a basilica in Lydda dedicated to Saint George existed. The church was destroyed by Muslims in 1010, but was later rebuilt and dedicated to Saint George by the Crusaders. In 1191 and during the conflict known as the Third Crusade (1189–92), the church was again destroyed by the forces of Saladin, Sultan of the Ayyubid dynasty (reigned 1171–93). A new church was erected in 1872 and is still standing.[In England, he was mentioned among the martyrs by the 8th-century monk Bede. The "Georgslied" is an adaptation of his legend in Old High German, composed in the late 9th century. The earliest dedication to the saint in England is a church at Fordington, Dorset that is mentioned in the will of Alfred the Great. Saint George did not rise to the position of "patron saint" of England, however, until the 14th century, and he was still obscured by Edward the Confessor, the traditional patron saint of England, until in 1552 during the reign of Edward VI all saints' banners other than George's were abolished in the English Reformation.
Belief in an apparition of St. George heartened the Franks at the Battle of Antioch in 1098, and a similar appearance occurred the following year at Jerusalem. The chivalric military Order of Sant Jordi d'Alfama was established by king Peter the Catholic from the Crown of Aragon in 1201, Republic of Genoa, Kingdom of Hungary (1326), and by Frederick III, Holy Roman Emperor. Edward III of England put his Order of the Garter under the banner of St. George, probably in 1348. The chronicler Jean Froissart observed the English invoking Saint George as a battle cry on several occasions during the Hundred Years' War. In his rise as a national saint, George was aided by the very fact that the saint had no legendary connection with England, and no specifically localized shrine, as that of Thomas Becket at Canterbury: "Consequently, numerous shrines were established during the late fifteenth century," Muriel C. McClendon has written, "and his did not become closely identified with a particular occupation or with the cure of a specific malady."

The establishment of George as a popular saint and protective giant in the West that had captured the medieval imagination was codified by the official elevation of his feast to a "festum duplex" at a church council in 1415, on the date that had become associated with his martyrdom, 23 April. There was wide latitude from community to community in celebration of the day across late medieval and early modern England, and no uniform "national" celebration elsewhere, a token of the popular and vernacular nature of George's "cultus" and its local horizons, supported by a local guild or confraternity under George's protection, or the dedication of a local church. When the English Reformation severely curtailed the saints' days in the calendar, Saint George's Day was among the holidays that continued to be observed.

In April 2019, the parish church of São Jorge, in São Jorge, Madeira Island, Portugal, solemnly received the relics of Saint George, patron saint of the parish, during the celebrations the 504th anniversary of its foundation. the relics were brought by the new Bishop of Funchal, D. Nuno Brás.

George is renowned throughout the Middle East, as both saint and prophet. His veneration by Christians and Muslims lies in his composite personality combining several Biblical, Quranic and other ancient mythical heroes.
William Dalrymple, who reviewed the literature in 1999, tells us that J. E. Hanauer in his 1907 book "Folklore of the Holy Land: Muslim, Christian and Jewish" "mentioned a shrine in the village of Beit Jala, beside Bethlehem, which at the time was frequented by Christians who regarded it as the birthplace of St. George and some Jews who regarded it as the burial place of the Prophet Elias. According to Hanauer, in his day the monastery was "a sort of madhouse. Deranged persons of all the three faiths are taken thither and chained in the court of the chapel, where they are kept for forty days on bread and water, the Eastern Orthodox priest at the head of the establishment now and then reading the Gospel over them, or administering a whipping as the case demands.' In the 1920s, according to Taufiq Canaan's "Mohammedan Saints and Sanctuaries in Palestine", nothing seemed to have changed, and all three communities were still visiting the shrine and praying together."

Dalrymple himself visited the place in 1995. "I asked around in the Christian Quarter in Jerusalem, and discovered that the place was very much alive. With all the greatest shrines in the Christian world to choose from, it seemed that when the local Arab Christians had a problem—an illness, or something more complicated—they preferred to seek the intercession of Saint George in his grubby little shrine at Beit Jala rather than praying at the Church of the Holy Sepulchre in Jerusalem or the Church of the Nativity in Bethlehem." He asked the priest at the shrine "Do you get many Muslims coming here?" The priest replied, "We get hundreds! Almost as many as the Christian pilgrims. Often, when I come in here, I find Muslims all over the floor, in the aisles, up and down."

The "Encyclopædia Britannica" quotes G. A. Smith in his "Historic Geography of the Holy Land" p. 164 saying "The Mahommedans who usually identify St. George with the prophet Elijah, at Lydda confound his legend with one about Christ himself. Their name for Antichrist is Dajjal, and they have a tradition that Jesus will slay Antichrist by the gate of Lydda. The notion sprang from an ancient bas-relief of George and the Dragon on the Lydda church. But Dajjal may be derived, by a very common confusion between "n" and "l", from Dagon, whose name two neighbouring villages bear to this day, while one of the gates of Lydda used to be called the Gate of Dagon."

Saint George is described as a prophetic figure in Islamic sources. George is venerated by some Christians and Muslims because of his composite personality combining several Biblical, Quranic and other ancient mythical heroes. In some sources he is identified with Elijah or Mar Elis, George or Mar Jirjus and in others as al-Khidr. The last epithet meaning the "green prophet", is common to both Christian and Muslim folk piety. Samuel Curtiss who visited an artificial cave dedicated to him where he is identified with Elijah, reports that childless Muslim women used to visit the shrine to pray for children. Per tradition, he was brought to his place of martyrdom in chains, thus priests of Church of St. George chain the sick especially the mentally ill to a chain for overnight or longer for healing. This is sought after by both Muslims and Christians.

According to Elizabeth Anne Finn's "Home in the Holy land" (1866):

The mosque of Nabi Jurjis which was restored by Timur in the 14th century, was located in Mosul and supposedly contained the tomb of George. It was however destroyed in July 2014 by the Islamic State of Iraq and the Levant, who also destroyed the Mosque of the Prophet Sheeth (Seth) and the Mosque of the Prophet Younis (Jonah). The militants claim such mosques have become places for apostasy instead of prayer.

George or "Hazrat" Jurjays was the patron saint of Mosul. Along with Theodosius, he was revered by both Christian and Muslim communities of Jazira and Anatolia. The wall paintings of Kırk Dam Altı Kilise at Belisırma dedicated to him are dated between 1282–1304. These paintings depict him as a mounted knight appearing between donors including a Georgian lady called Thamar and her husband, the Emir and Consul Basil, while the Seljuk Sultan Mesud II and Byzantine Emperor Androncius II are also named in the inscriptions.

In the General Roman Calendar, the feast of Saint George is on 23 April. In the Tridentine Calendar of 1568, it was given the rank of "Semidouble". In Pope Pius XII's 1955 calendar this rank was reduced to "Simple", and in Pope John XXIII's 1960 calendar to a "Commemoration". Since Pope Paul VI's 1969 revision, it appears as an optional "Memorial". In some countries, such as England, the rank is higher. In England, it is a Solemnity (Roman Catholic) or Feast (Church of England): if it falls between Palm Sunday and the Second Sunday of Easter inclusive, it is transferred to the Monday after the Second Sunday of Easter.

Saint George is very much honoured by the Eastern Orthodox Church, wherein he is referred to as a "Great Martyr", and in Oriental Orthodoxy overall. His major feast day is on 23 April (Julian calendar 23 April currently corresponds to Gregorian calendar 6 May). If, however, the feast occurs before Easter, it is celebrated on Easter Monday, instead. The Russian Orthodox Church also celebrates two additional feasts in honour of St. George. One is on 3 November, commemorating the consecration of a cathedral dedicated to him in Lydda during the reign Constantine the Great (305–37). When the church was consecrated, the relics of the Saint George were transferred there. The other feast is on 26 November for a church dedicated to him in Kiev, "circa" 1054.

In Bulgaria, Saint George's day () is celebrated on 6 May, when it is customary to slaughter and roast a lamb. Saint George's day is also a public holiday.

In Serbia and Bosnia and Herzegovina, the Serbian Orthodox Church refers to Saint George as "Sveti Djordje" ("Свети Ђорђе") or "Sveti Georgije" ("Свети Георгије"). Saint George's day ("Đurđevdan") is celebrated on May 6, and is a common slava (patron saint day) among ethnic Serbs.

In Egypt, the Coptic Orthodox Church of Alexandria refers to Saint George () as the "Prince of Martyrs" and celebrates his martyrdom on the 23rd of Paremhat of the Coptic calendar equivalent to 1 May. 
The Copts also celebrate the consecration of the first church dedicated to him on seventh of the month of Hatour of the Coptic calendar usually equivalent to 17 November.

In India, the Syro-Malabar Catholic Church, one of the oriental catholic churches (Eastern Catholic Churches) and Malankara Orthodox Church venerate Saint George. The main pilgrim centers of the saint in India are at Puthuppally, Kottayam District, Edathua in Alappuzha district, and Edappally in Ernakulam district of the southern state of Kerala. The saint is commemorated each year from 27 April to 14 May at Edathua On 27 April after the flag hoisting ceremony by the parish priest, the statue of the saint is taken from one of the altars and placed at the extension of the church to be venerated by the devotees till 14 May. The main feast day is 7 May, when the statue of the saint along with other saints is taken in procession around the church. Intercession to Saint George of Edathua is believed to be efficacious in repelling snakes and in curing mental ailments.The sacred relics of St. George were brought to Antioch from Mardeen in 900 and were taken to Kerala, India from Antioch in 1912 by Mar Dionysius of Vattasseril and kept in the Orthodox seminary at Kundara, Kerala. H.H Mathews II Catholicos had given the relics to St. George churches at Puthupally, Kottayam District and Chandanappally, Pathanamthitta district.

George is a highly celebrated saint in both the Western and Eastern Christian churches, and a large number of Patronages of Saint George exist throughout the world.

Saint George is the patron saint of England. His cross forms the national flag of England, and features within the Union Flag of the United Kingdom, and other national flags containing the Union Flag, such as those of Australia and New Zealand. By the 14th century, the saint had been declared both the patron saint and the protector of the royal family.

Saint George is the patron saint of Ethiopia. He is also the patron saint of the Ethiopian Orthodox Church, Saint George slaying the dragon is one of the most frequently used subjects of icons in the church. 

The country of Georgia, where devotions to the saint date back to the fourth century, is not technically named after the saint, but is a well-attested back-formation of the English name. However, a large number of towns and cities around the world are. Saint George is one of the patron saints of Georgia; the name Georgia ("Sakartvelo" in Georgian) is an anglicisation of "Gurj", ultimately derived from the Persian word "gurj"/"gurjān" ("wolf"). Chronicles describing the land as "Georgie" or Georgia in French and English, date from the early Middle Ages, as written by the travellers John Mandeville and Jacques de Vitry "because of their special reverence for Saint George", but these accounts have been seen as folk etymology and are rejected by the scholarly community. Exactly 365 Orthodox churches in Georgia are named after Saint George according to the number of days in a year. According to legend, Saint George was cut into 365 pieces after he fell in battle and every single piece was spread throughout the entire country.

Saint George is also one of the patron saints of the Mediterranean islands of Malta and Gozo. In a battle between the Maltese and the Moors, Saint George was alleged to have been seen with Saint Paul and Saint Agata, protecting the Maltese. Saint George is the protector of the island of Gozo and the patron of Gozo's largest city, Victoria. The St. George's Basilica in Victoria is dedicated to him.

Devotions to Saint George in Portugal date back to the 12th century. Nuno Álvares Pereira attributed the victory of the Portuguese in the battle of Aljubarrota in 1385 to Saint George. During the reign of John I of Portugal (1357–1433), Saint George became the patron saint of Portugal and the King ordered that the saint's image on the horse be carried in the "Corpus Christi" procession. The flag of Saint George (white with red cross) was also carried by the Portuguese troops and hoisted in the fortresses, during the 15th century. "Portugal and Saint George" became the battle cry of the Portuguese troops, being still today the battle cry of the Portuguese Army, with simply "Saint George" being the battle cry of the Portuguese Navy.

Saint George, is also the patron saint of the region of Aragon, in Spain, where his feast day is celebrated on the 23rd of April and is known as "Aragon Day", "or 'Día de Aragón"' in Spanish. He became the patron saint of the former Kingdom of Aragon and Crown of Aragon when King Pedro I of Aragon won the Battle of Alcoraz in 1096. Legend has it that victory eventually fell to the Christian armies when St. George appeared to them on the battlefield, helping them secure the reconquest of the city of Huesca which had been under the Muslim control of the Taifa of Zaragoza. The battle, which had begun two years earlier in 1094, was long and arduous, and had also taken the life of King Pedro's own father, King Sancho Ramirez. With the Aragonese spirits flagging, it is said that Saint George descending from heaven on his charger and bearing a dark red cross, appeared at the head of the Christian cavalry leading the knights into battle. Interpreting this as a sign of protection from God, the Christian militia returned emboldened to the battle field, more energized than ever, convinced theirs was the banner of the one true faith. Defeated, the moors rapidly abandoned the battlefield. After two years of being locked down under siege, Huesca was liberated and King Pedro made his triumphal entry into the city. To celebrate this victory, the cross of Saint George was adopted as the coat of arms of Huesca and Aragon, in honour of their saviour. After the taking of Huesca, King Pedro aided the military leader and nobleman, Rodrigo Díaz de Vivar, otherwise known as El Cid, with a coalition army from Aragon in the long reconquest of the Kingdom of Valencia.

Tales of King Pedro's success at Huesca and in leading his expedition of armies with El Cid against the Moors, under the auspices of Saint George on his standard, spread quickly throughout the realm and beyond the Crown of Aragon, and Christian armies throughout Europe quickly began adopting Saint George as their protector and patron, during all subsequent Crusades to the Holy Lands. By 1117, the military order of Templars adopted the Cross of Saint George as a simple unifying sign for international Christian militia embroidered on the left hand side of their tunics, placed above the heart.

The Cross of Saint George, also known in Aragon as The Cross of Alcoraz, continues to emblazon the flags of all of Aragon's provinces.

The association of Saint George with chivalry and noblemen in Aragon continued through the ages. Indeed, even the author Miguel de Cervantes, in his book on the Adventures of Don Quixote, also mentions the Jousting events that took place at the festival of Saint George in Zaragoza in Aragon where one could gain international renown in winning a joust against any of the knights of Aragon.

In Valencia, Catalonia, the Balearics, Malta, Sicily and Sardinia, the origins of the veneration of Saint George go back to their shared history as territories under the Crown of Aragon, thereby sharing the same legend.

One of the highest civil distinctions awarded in Catalonia is the Saint George's Cross ("Creu de Sant Jordi"). The Sant Jordi Awards have been awarded in Barcelona since 1957.

Saint George ("Sant Jordi" in Catalan) is also the patron saint of Catalonia. His cross appears in many buildings and local flags, including the flag of Barcelona, the Catalan capital. A Catalan variation to the traditional legend places Saint George's life story as having occurred in the town of Montblanc, near Tarragona.

It became fashionable in the 15th century, with the full development of classical heraldry, to provide attributed arms to saints and other historical characters from the pre-heraldic ages. The widespread attribution to Saint George of the red cross on a white field in western art - "Saint George's Cross" - probably first arose in Genoa, which had adopted this image for their flag and George as their patron saint in the 12th century. A "vexillum beati Georgii" is mentioned in the Genovese annals for the year 1198, referring to a red flag with a depiction of Saint George and the dragon. An illumination of this flag is shown in the annals for the year 1227. The Genoese flag with the red cross was used alongside this "George's flag", from at least 1218, and was known as the " insignia cruxata comunis Janue" ("cross ensign of the commune of Genoa"). The flag showing the saint himself was the city's principal war flag, but the flag showing the plain cross was used alongside it in the 1240s.

In 1348 Edward III of England chose Saint George as the patron saint of his Order of the Garter, and also took to using a red-on-white cross in the hoist of his Royal Standard.

The term "Saint George's cross" was at first associated with any plain Greek cross touching the edges of the field (not necessarily red on white). Thomas Fuller in 1647 spoke of "the plain or St George's cross" as "the mother of all the others" (that is, the other heraldic crosses).

Saint George is most commonly depicted in early icons, mosaics, and frescos wearing armour contemporary with the depiction, executed in gilding and silver colour, intended to identify him as a Roman soldier. Particularly after the Fall of Constantinople and Saint George's association with the crusades, he is often portrayed mounted upon a white horse. Thus, a 2003 Vatican stamp (issued on the anniversary of the Saint's death) depicts an armoured Saint George atop a white horse, killing the dragon.

Eastern Orthodox iconography also permits Saint George to ride a black horse, as in a Russian icon in the British museum collection.
In the south Lebanese village of Mieh Mieh, the Saint George Church for Melkite Catholics commissioned for its 75th jubilee in 2012 (under the guidance of Mgr Sassine Gregoire), the only icons in the world portraying the whole life of Saint George, as well as the scenes of his torture and martyrdom (drawn in eastern iconographic style).

Saint George may also be portrayed with Saint Demetrius, another early soldier saint. When the two saintly warriors are together and mounted upon horses, they may resemble earthly manifestations of the archangels Michael and Gabriel. Eastern traditions distinguish the two as Saint George rides a white horse and St. Demetrius a red horse (the red pigment may appear black if it has bituminized). 
Saint George can also be identified by his spearing a dragon, whereas Saint Demetrius may be spearing a human figure, representing Maximian.

During the early second millennium, Saint George became a model of chivalry in works of literature, including medieval romances. In the 13th century, Jacobus de Voragine, Archbishop of Genoa, compiled the "Legenda Sanctorum", ("Readings of the Saints") also known as "Legenda Aurea" (the "Golden Legend"). Its 177 chapters (182 in some editions) include the story of Saint George, among many others. After the invention of the printing press, the book became a bestseller.




April 23


</doc>
<doc id="29018" url="https://en.wikipedia.org/wiki?curid=29018" title="Sarah Lawrence (disambiguation)">
Sarah Lawrence (disambiguation)

Sarah Lawrence is a Liberal Arts college in Westchester County, New York.

Sarah Lawrence may also refer to:



</doc>
<doc id="29021" url="https://en.wikipedia.org/wiki?curid=29021" title="Secular humanism">
Secular humanism

Secular humanism is a philosophy or life stance that embraces human reason, secular ethics, and philosophical naturalism while specifically rejecting religious dogma, supernaturalism, and superstition as the basis of morality and decision making.

Secular humanism posits that human beings are capable of being ethical and moral without religion or belief in a deity. It does not, however, assume that humans are either inherently good or evil, nor does it present humans as being superior to nature. Rather, the humanist life stance emphasizes the unique responsibility facing humanity and the ethical consequences of human decisions. Fundamental to the concept of secular humanism is the strongly held viewpoint that ideology—be it religious or political—must be thoroughly examined by each individual and not simply accepted or rejected on faith. Along with this, an essential part of secular humanism is a continually adapting search for truth, primarily through science and philosophy. Many secular humanists derive their moral codes from a philosophy of utilitarianism, ethical naturalism, or evolutionary ethics, and some advocate a science of morality.

Humanists International is the world union of more than one hundred humanist, rationalist, irreligious, atheist, Bright, secular, Ethical Culture, and freethought organizations in more than 40 countries. The "Happy Human" is recognized as the official symbol of humanism internationally, used by secular humanist organizations in every part of the world. Those who call themselves humanists are estimated to number between four and five million people worldwide.

The meaning of the phrase "secular humanism" has evolved over time. The phrase has been used since at least the 1930s by Anglican priests, and in 1943, the then Archbishop of Canterbury, William Temple, was reported as warning that the "Christian tradition... was in danger of being undermined by a 'Secular Humanism' which hoped to retain Christian values without Christian faith." During the 1960s and 1970s the term was embraced by some humanists who considered themselves anti-religious, as well as those who, although not critical of religion in its various guises, preferred a non-religious approach. The release in 1980 of "A Secular Humanist Declaration" by the newly formed Council for Democratic and Secular Humanism (CODESH, later the Council for Secular Humanism, which with CSICOP in 1991 jointly formed the Center for Inquiry and in 2015 both ceased separate operations, becoming CFI programs) gave secular humanism an organisational identity within the United States; but no overall organisation involved currently uses a name featuring "secular humanism".

However, many adherents of the approach reject the use of the word "secular" as obfuscating and confusing, and consider that the term "secular humanism" has been "demonized by the religious right... All too often secular humanism is reduced to a sterile outlook consisting of little more than secularism slightly broadened by academic ethics. This kind of 'hyphenated humanism' easily becomes more about the adjective than its referent". Adherents of this view, including Humanists International and the American Humanist Association, consider that the unmodified but capitalized word Humanism should be used. The endorsement by the International Humanist and Ethical Union (IHEU) of the capitalization of the word "Humanism", and the dropping of any adjective such as "secular", is quite recent. The American Humanist Association began to adopt this view in 1973, and the IHEU formally endorsed the position in 1989. In 2002 the IHEU General Assembly unanimously adopted the Amsterdam Declaration, which represents the official defining statement of World Humanism for Humanists. This declaration makes exclusive use of capitalized "Humanist" and "Humanism", which is consistent with IHEU's general practice and recommendations for promoting a unified Humanist identity. To further promote Humanist identity, these words are also free of any adjectives, as recommended by prominent members of IHEU. Such usage is not universal among IHEU member organizations, though most of them do observe these conventions.

Historical use of the term humanism (reflected in some current academic usage), is related to the writings of pre-Socratic philosophers. These writings were lost to European societies until Renaissance scholars rediscovered them through Muslim sources and translated them from Arabic into European languages. Thus the term humanist can mean a humanities scholar, as well as refer to The Enlightenment/ Renaissance intellectuals, and those who have agreement with the pre-Socratics, as distinct from secular humanists.

In 1851 George Holyoake coined the term "secularism" to describe "a form of opinion which concerns itself only with questions, the issues of which can be tested by the experience of this life".

The modern secular movement coalesced around Holyoake, Charles Bradlaugh and their intellectual circle. The first secular society, the Leicester Secular Society, dates from 1851. Similar regional societies came together to form the National Secular Society in 1866.

Holyoake's secularism was strongly influenced by Auguste Comte, the founder of positivism and of modern sociology. Comte believed human history would progress in a "law of three stages" from a theological phase, to the "metaphysical", toward a fully rational "positivist" society. In later life, Comte had attempted to introduce a "religion of humanity" in light of growing anti-religious sentiment and social malaise in revolutionary France. This religion would necessarily fulfil the functional, cohesive role that supernatural religion once served.

Although Comte's religious movement was unsuccessful in France, the positivist philosophy of science itself played a major role in the proliferation of secular organizations in the 19th century in England. Richard Congreve visited Paris shortly after the French Revolution of 1848 where he met Auguste Comte and was heavily influenced by his positivist system. He founded the London Positivist Society in 1867, which attracted Frederic Harrison, Edward Spencer Beesly, Vernon Lushington, and James Cotter Morison amongst others.

In 1878, the Society established the Church of Humanity under Congreve's direction. There they introduced sacraments of the Religion of Humanity and published a co-operative translation of Comte's Positive Polity. When Congreve repudiated their Paris co-religionists in 1878, Beesly, Harrison, Bridges, and others formed their own positivist society, with Beesly as president, and opened a rival centre, Newton Hall, in a courtyard off Fleet Street.

The New York City version of the church was established by English immigrant Henry Edger. The American version of the "Church of Humanity". was largely modeled on the English church. Like the English version it wasn't atheistic and had sermons and sacramental rites. At times the services included readings from conventional religious works like the Book of Isaiah. It was not as significant as the church in England, but did include several educated people.

Another important precursor was the ethical movement of the 19th century. The South Place Ethical Society was founded in 1793 as the South Place Chapel on Finsbury Square, on the edge of the City of London, and in the early nineteenth century was known as "a radical gathering-place". At that point it was a Unitarian chapel, and that movement, like Quakers, supported female equality. Under the leadership of Reverend William Johnson Fox, it lent its pulpit to activists such as Anna Wheeler, one of the first women to campaign for feminism at public meetings in England, who spoke in 1829 on "rights of women". In later decades, the chapel changed its name to the South Place Ethical Society, now the Conway Hall Ethical Society. Today Conway Hall explicitly identifies itself as a humanist organisation, albeit one primarily focused on concerts, events, and the maintenance of its humanist library and archives. It bills itself as "The landmark of London’s independent intellectual, political and cultural life."

In America, the ethical movement was propounded by Felix Adler, who established the New York Society for Ethical Culture in 1877. By 1886, similar societies had sprouted up in Philadelphia, Chicago and St. Louis.

These societies all adopted the same statement of principles:

In effect, the movement responded to the religious crisis of the time by replacing theology with unadulterated morality. It aimed to "disentangle moral ideas from religious doctrines, metaphysical systems, and ethical theories, and to make them an independent force in personal life and social relations." Adler was also particularly critical of the religious emphasis on creed, believing it to be the source of sectarian bigotry. He therefore attempted to provide a universal fellowship devoid of ritual and ceremony, for those who would otherwise be divided by creeds. For the same reasons the movement at that time adopted a neutral position on religious beliefs, advocating neither atheism nor theism, agnosticism nor deism.

The first ethical society along these lines in Britain was founded in 1886. By 1896 the four London societies formed the Union of Ethical Societies, and between 1905 and 1910 there were over fifty societies in Great Britain, seventeen of which were affiliated with the Union. The Union of Ethical Societies would later incorporate as the Ethical Union, a registered charity, in 1928. Under the leadership of Harold Blackham, it renamed itself the British Humanist Association in 1967. It became Humanists UK in 2017.

In the 1930s, "humanism" was generally used in a religious sense by the Ethical movement in the United States, and not much favoured among the non-religious in Britain. Yet "it was from the Ethical movement that the non-religious philosophical sense of "Humanism" gradually emerged in Britain, and it was from the convergence of the Ethical and Rationalist movements that this sense of "Humanism" eventually prevailed throughout the Freethought movement".

As an organized movement, Humanism itself is quite recent – born at the University of Chicago in the 1920s, and made public in 1933 with the publication of the first Humanist Manifesto. The American Humanist Association was incorporated as an Illinois non-profit organization in 1943. The International Humanist and Ethical Union was founded in 1952, when a gathering of world Humanists met under the leadership of Sir Julian Huxley. The British Humanist Association took that name in 1967, but had developed from the Union of Ethical Societies which had been founded by Stanton Coit in 1896.

Humanists have put together various Humanist Manifestos, in attempts to unify the Humanist identity.

The original signers of the first Humanist Manifesto of 1933, declared themselves to be religious humanists. Because, in their view, traditional religions were failing to meet the needs of their day, the signers of 1933 declared it a necessity to establish a religion that was a dynamic force to meet the needs of the day. However, this "religion" did not profess a belief in any god. Since then two additional Manifestos were written to replace the first. In the Preface of Humanist Manifesto II, in 1973, the authors Paul Kurtz and Edwin H. Wilson assert that faith and knowledge are required for a hopeful vision for the future. Manifesto II references a section on Religion and states traditional religion renders a disservice to humanity. Manifesto II recognizes the following groups to be part of their naturalistic philosophy: "scientific", "ethical", "democratic", "religious", and "Marxist" humanism.

In 2002, the IHEU General Assembly unanimously adopted the Amsterdam Declaration 2002 which represents the official defining statement of World Humanism.

All member organisations of the International Humanist and Ethical Union are required by bylaw 5.1 to accept the "Minimum Statement on Humanism":
Humanism is a democratic and ethical life stance, which affirms that human beings have the right and responsibility to give meaning and shape to their own lives. It stands for the building of a more humane society through an ethic based on human and other natural values in the spirit of reason and free inquiry through human capabilities. It is not theistic, and it does not accept supernatural views of reality.

To promote and unify "Humanist" identity, prominent members of the IHEU have endorsed the following statements on Humanist identity:

According to the Council for Secular Humanism, within the United States, the term "secular humanism" describes a world view with the following elements and principles:


"A Secular Humanist Declaration" was issued in 1980 by the Council for Secular Humanism's predecessor, CODESH. It lays out ten ideals: Free inquiry as opposed to censorship and imposition of belief; separation of church and state; the ideal of freedom from religious control and from jingoistic government control; ethics based on critical intelligence rather than that deduced from religious belief; moral education; religious skepticism; reason; a belief in science and technology as the best way of understanding the world; evolution; and education as the essential method of building humane, free, and democratic societies.

A general outline of Humanism is also set out in the "Humanist Manifesto" prepared by the American Humanist Association.

In the 20th and 21st centuries, members of Humanist organizations have disagreed as to whether Humanism is a religion. They categorize themselves in one of three ways. Religious humanism, in the tradition of the earliest Humanist organizations in the UK and US, attempts to fulfill the traditional social role of religion. Secular humanism considers all forms of religion, including religious humanism, to be superseded. In order to sidestep disagreements between these two factions, recent Humanist proclamations define Humanism as a "life stance"; proponents of this view making up the third faction. All three types of Humanism (and all three of the American Humanist Association's manifestos) reject deference to supernatural beliefs; promoting the practical, methodological naturalism of science, but also going further and supporting the philosophical stance of metaphysical naturalism. The result is an approach to issues in a secular way. Humanism addresses ethics without reference to the supernatural as well, attesting that ethics is a human enterprise (see naturalistic ethics).

Secular humanism does not prescribe a specific theory of morality or code of ethics. As stated by the Council for Secular Humanism,
Secular humanism affirms that with the present state of scientific knowledge, dogmatic belief in an absolutist moral/ethical system (e.g. Kantian, Islamic, Christian) is unreasonable. However, it affirms that individuals engaging in rational moral/ethical deliberations can discover some universal "objective standards".
Many Humanists adopt principles of the Golden Rule. Some believe that universal moral standards are required for the proper functioning of society. However, they believe such necessary universality can and should be achieved by developing a richer notion of morality through reason, experience and scientific inquiry rather than through faith in a supernatural realm or source.
Humanism is compatible with atheism
and agnosticism,
but being atheist or agnostic does not automatically make one a humanist. Nevertheless, humanism is diametrically opposed to state atheism.
According to Paul Kurtz, considered by some to be the founder of the American secular humanist movement, one of the differences between Marxist–Leninist atheists and humanists is the latter's commitment to "human freedom and democracy" while stating that the militant atheism of the Soviet Union consistently violated basic human rights.
Kurtz also stated that the "defense of religious liberty is as precious to the humanist as are the rights of the believers". Greg M. Epstein states that, "modern, organized Humanism began, in the minds of its founders, as nothing more nor less than a religion without a God".

Many Humanists address ethics from the point of view of ethical naturalism, and some support an actual science of morality.

Secular humanist organizations are found in all parts of the world. Those who call themselves humanists are estimated to number between four and five million people worldwide in 31 countries, but there is uncertainty because of the lack of universal definition throughout censuses. Humanism is a non-theistic belief system and, as such, it could be a sub-category of "Religion" only if that term is defined to mean "Religion and (any) belief system". This is the case in the International Covenant on Civil and Political Rights on freedom of religion "and" beliefs. Many national censuses contentiously define Humanism as a further sub-category of the sub-category "No Religion", which typically includes atheist, rationalist and agnostic thought. In England, Wales 25% of people specify that they have 'No religion' up from 15% in 2001 and in Australia, around 30% of the population specifies "No Religion" in the national census. In the US, the decennial census does not inquire about religious affiliation or its lack; surveys report the figure at roughly 13%. In the 2001 Canadian census, 16.5% of the populace reported having no religious affiliation. In the 2011 Scottish census, 37% stated they had no religion up from 28% in 2001. One of the largest Humanist organizations in the world (relative to population) is Norway's "Human-Etisk Forbund", which had over 86,000 members out of a population of around 4.6 million in 2013 – approximately 2% of the population.
The International Humanist and Ethical Union (IHEU) is the worldwide umbrella organization for those adhering to the Humanist life stance. It represents the views of over three million Humanists organized in over 100 national organizations in 30 countries. Originally based in the Netherlands, the IHEU now operates from London. Some regional groups that adhere to variants of the Humanist life stance, such as the humanist subgroup of the Unitarian Universalist Association, do not belong to the IHEU. Although the European Humanist Federation is also separate from the IHEU, the two organisations work together and share an agreed protocol.

Starting in the mid-20th century, religious fundamentalists and the religious right began using the term "secular humanism" in hostile fashion. Francis A. Schaeffer, an American theologian based in Switzerland, seizing upon the exclusion of the divine from most humanist writings, argued that rampant secular humanism would lead to moral relativism and ethical bankruptcy in his book "How Should We Then Live: The Rise and Decline of Western Thought and Culture" (1976). Schaeffer portrayed secular humanism as pernicious and diabolical, and warned it would undermine the moral and spiritual tablet of America. His themes have been very widely repeated in Fundamentalist preaching in North America. Toumey (1993) found that secular humanism is typically portrayed as a vast evil conspiracy, deceitful and immoral, responsible for feminism, pornography, abortion, homosexuality, and New Age spirituality. In certain areas of the world, Humanism finds itself in conflict with religious fundamentalism, especially over the issue of the separation of church and state. Many Humanists see religions as superstitious, repressive and closed-minded, while religious fundamentalists may see Humanists as a threat to the values set out in their sacred texts.

In recent years, humanists such as Dwight Gilbert Jones and R. Joseph Hoffmann have decried the over-association of Humanism with affirmations of non-belief and atheism. Jones cites a lack of new ideas being presented or debated outside of secularism, while Hoffmann is unequivocal: "I regard the use of the term 'humanism' to mean secular humanism or atheism to be one of the greatest tragedies of twentieth century movementology, perpetrated by second-class minds and perpetuated by third-class polemicists and village atheists. The attempt to sever humanism from the religious and the spiritual was a flatfooted, largely American way of taking on the religious right. It lacked finesse, subtlety, and the European sense of history."

Some Humanists celebrate official religion-based public holidays, such as Christmas or Easter, but as secular holidays rather than religious ones. Many Humanists also celebrate the winter and summer solstice, the former of which (in the northern hemisphere) coincides closely with the religiously-oriented celebration of Christmas, and the equinoxes, of which the vernal equinox is associated with Christianity's Easter and indeed with all other springtime festivals of renewal, and the autumnal equinox which is related to such celebrations such as Halloween and All Souls' Day. The Society for Humanistic Judaism celebrates most Jewish holidays in a secular manner.

The IHEU endorses World Humanist Day (21 June), Darwin Day (12 February), Human Rights Day (10 December) and HumanLight (23 December) as official days of Humanist celebration, though none are yet a public holiday.

In many countries, humanist celebrants (officiants) perform celebrancy services for weddings, funerals, child namings, coming of age ceremonies, and other rituals.

The issue of whether and in what sense secular humanism might be considered a religion, and what the implications of this would be, has become the subject of legal maneuvering and political debate in the United States. The first reference to "secular humanism" in a US legal context was in 1961, although church-state separation lawyer Leo Pfeffer had referred to it in his 1958 book, "Creeds in Competition".

The Education for Economic Security Act of 1984 included a section, Section 20 U.S.C.A. 4059, which initially read: "Grants under this subchapter ['Magnet School Assistance'] may not be used for consultants, for transportation or for any activity which does not augment academic improvement." With no public notice, Senator Orrin Hatch tacked onto the proposed exclusionary subsection the words "or for any course of instruction the substance of which is Secular Humanism". Implementation of this provision ran into practical problems because neither the Senator's staff, nor the Senate's Committee on Labor and Human Resources, nor the Department of Justice could propose a definition of what would constitute a "course of instruction the substance of which is Secular Humanism". So, this determination was left up to local school boards. The provision provoked a storm of controversy which within a year led Senator Hatch to propose, and Congress to pass, an amendment to delete from the statute all reference to secular humanism. While this episode did not dissuade fundamentalists from continuing to object to what they regarded as the "teaching of Secular Humanism", it did point out the vagueness of the claim.

The phrase "secular humanism" became prominent after it was used in the United States Supreme Court case "Torcaso v. Watkins." In the 1961 decision, Justice Hugo Black commented in a footnote, "Among religions in this country which do not teach what would generally be considered a belief in the existence of God are Buddhism, Taoism, Ethical Culture, Secular Humanism, and others."

The footnote in "Torcaso v. Watkins" referenced "Fellowship of Humanity v. County of Alameda", a 1957 case in which an organization of humanists sought a tax exemption on the ground that they used their property "solely and exclusively for religious worship." Despite the group's non-theistic beliefs, the court determined that the activities of the "Fellowship of Humanity", which included weekly Sunday meetings, were analogous to the activities of theistic churches and thus entitled to an exemption. The "Fellowship of Humanity" case itself referred to "Humanism" but did not mention the term "secular humanism". Nonetheless, this case was cited by Justice Black to justify the inclusion of secular humanism in the list of religions in his note. Presumably Justice Black added the word "secular" to emphasize the non-theistic nature of the "Fellowship of Humanity" and distinguish their brand of humanism from that associated with, for example, Christian humanism.

Another case alluded to in the "Torcaso v. Watkins" footnote, and said by some to have established secular humanism as a religion under the law, is the 1957 tax case of "", 249 F.2d 127 (D.C. Cir. 1957). The "Washington Ethical Society" functions much like a church, but regards itself as a non-theistic religious institution, honoring the importance of ethical living without mandating a belief in a supernatural origin for ethics. The case involved denial of the Society's application for tax exemption as a religious organization. The U.S. Court of Appeals reversed the Tax Court's ruling, defined the Society as a religious organization, and granted its tax exemption. The Society terms its practice Ethical Culture. Though Ethical Culture is based on a humanist philosophy, it is regarded by some as a type of religious humanism. Hence, it would seem most accurate to say that this case affirmed that a religion need not be theistic to qualify as a religion under the law, rather than asserting that it established generic secular humanism as a religion.

In the cases of both the "Fellowship of Humanity" and the "Washington Ethical Society," the court decisions turned not so much on the particular beliefs of practitioners as on the function and form of the practice being similar to the function and form of the practices in other religious institutions.

The implication in Justice Black's footnote that secular humanism is a religion has been seized upon by religious opponents of the teaching of evolution, who have made the argument that teaching evolution amounts to teaching a religious idea. The claim that secular humanism could be considered a religion for legal purposes was examined by the United States Court of Appeals for the Ninth Circuit in "Peloza v. Capistrano School District", 37 F.3d 517 (9th Cir. 1994), "cert. denied", 515 U.S. 1173 (1995). In this case, a science teacher argued that, by requiring him to teach evolution, his school district was forcing him to teach the "religion" of secular humanism. The Court responded, "We reject this claim because neither the Supreme Court, nor this circuit, has ever held that evolutionism or Secular Humanism are 'religions' for Establishment Clause purposes." The Supreme Court refused to review the case.

The decision in a subsequent case, "Kalka v. Hawk et al.", offered this commentary:

Decisions about tax status have been based on whether an organization functions like a church. On the other hand, Establishment Clause cases turn on whether the ideas or symbols involved are inherently religious. An organization can function like a church while advocating beliefs that are not necessarily inherently religious. Author Marci Hamilton has pointed out: "Moreover, the debate is not between secularists and the religious. The debate is believers and non-believers on the one side debating believers and non-believers on the other side. You've got citizens who are [...] of faith who believe in the separation of church and state and you have a set of believers who do not believe in the separation of church and state."

In the 1987 case of "Smith v. Board of School Commissioners of Mobile County" a group of plaintiffs brought a case alleging that the school system was teaching the tenets of an anti-religious religion called "secular humanism" in violation of the Establishment Clause. The complainants asked that 44 different elementary through high school level textbooks (including books on home economics, social science and literature) be removed from the curriculum. Federal judge William Brevard Hand ruled for the plaintiffs agreeing that the books promoted secular humanism, which he ruled to be a religion. The Eleventh Circuit Court unanimously reversed him, with Judge Frank stating that Hand held a "misconception of the relationship between church and state mandated by the establishment clause," commenting also that the textbooks did not show "an attitude antagonistic to theistic belief. The message conveyed by these textbooks is one of neutrality: the textbooks neither endorse theistic religion as a system of belief, nor discredit it".

There are numerous Humanist Manifestos and Declarations, including the following:






</doc>
<doc id="29027" url="https://en.wikipedia.org/wiki?curid=29027" title="Game Gear">
Game Gear

The is an 8-bit fourth generation handheld game console released by Sega on October 6, 1990 in Japan, in April 1991 throughout North America and Europe, and during 1992 in Australia. The Game Gear primarily competed with Nintendo's Game Boy, the Atari Lynx, and NEC's TurboExpress. It shares much of its hardware with the Master System, and can play Master System games by the use of an adapter. Sega positioned the Game Gear, which had a full-color backlit screen with a landscape format, as a technologically superior handheld to the Game Boy.

Though the Game Gear was rushed to market, its unique game library and price point gave it an edge over the Atari Lynx and TurboExpress. However, due to its short battery life, lack of original games, and weak support from Sega, the Game Gear was unable to surpass the Game Boy, selling 10.62 million units by March 1996. The Game Gear was discontinued in 1997. It was re-released as a budget system by Majesco Entertainment in 2000, under license from Sega.

Reception of the Game Gear was mixed, with praise for its full-color backlit screen and processing power for its time, criticisms over its large size and short battery life, and questions over the quality of its game library.

Developed under the name "Project Mercury", the Game Gear was first released in Japan on October 6, 1990, in North America and Europe in 1991, and in Australia in 1992. Originally retailing at JP¥19,800 in Japan, US$149.99 in North America, and GB£99.99 in Europe, the Game Gear was developed to compete with the Game Boy, which Nintendo had released in 1989. The console had been designed as a portable version of the Master System, and featured more powerful systems than the Game Boy, including a full-color screen, in contrast to the monochromatic screen of its rival. According to former Sega console hardware research and development head Hideki Sato, Sega saw the Game Boy's black and white screen as "a challenge to make our own color handheld system."

In order to improve upon the design of their competition, Sega modeled the Game Gear with a similar shape to a Genesis controller, with the idea being that the curved surfaces and longer length would make the Game Gear more comfortable to hold than the Game Boy. The console's mass was carefully considered from the beginning of the development, aiming for a total mass between that of the Game Boy and the Atari Lynx, another full-color screen competing product. Despite the similarities the Game Gear shared with the Master System, the games of the latter were not directly playable on the Game Gear, and were only able to be played on the handheld by the use of an accessory called the Master Gear Converter. The original Game Gear pack-in game was "Columns", which was similar to the "Tetris" cartridge that Nintendo had included when it launched the Game Boy.
With a late start into the handheld gaming market, Sega rushed to get the Game Gear into stores quickly, having lagged behind Nintendo in sales without a handheld on the market. As one method of doing so, Sega based the hardware of the Game Gear on the Master System, albeit with a much larger color palette than its predecessor: the Game Gear supported 4096 colors, compared to the 64 colors supported by the Master System. Part of the intention of this move was to make Master System games easy to port to the Game Gear. Though the Game Gear was designed to be technologically superior to the Game Boy, its design came at a cost of battery life: whereas the Game Boy could run for more than 30 hours on four AA batteries, the Game Gear required six AA batteries and could only run for three to five hours. With its quick launch in Japan, the handheld sold 40,000 units in its first two days, 90,000 within a month, and the number of back orders for the system was over 600,000. According to Sega of America marketing director Robert Botch, "there is clearly a need for a quality portable system that provides features other systems have failed to deliver. This means easy-to-view, full-color graphics and exciting quality games that appeal to all ages."

Before the Game Gear's launch in 1990, Sega had success marketing its 16-bit home console, the Sega Genesis, by advertising it as a "more mature" option for gamers. In keeping with this approach, Sega positioned the Game Gear as a "grown-up" option compared to the Game Boy. While Sega's marketing in Japan did not take this perspective, instead opting for advertisements with Japanese women featuring the handheld, Sega's worldwide advertising prominently positioned the Game Gear as the "cooler" console than the Game Boy.

In North America, marketing for the Game Gear included side-by-side comparisons of Sega's new handheld with the Game Boy and likened Game Boy players to the obese and uneducated. One Sega advertisement featured the quote, "If you were color blind and had an IQ of less than 12, then you wouldn't mind which portable you had." Such advertising drew fire from Nintendo, who sought to have protests organized against Sega for insulting disabled persons. Sega responded with a statement from Sega of America president Tom Kalinske saying that Nintendo "should spend more time improving their products and marketing rather than working on behind-the-scenes coercive activities". Ultimately, this debate would have little impact on sales for the Game Gear.

Europe and Australia were the last regions to receive the Game Gear. Due to the delays in receiving the new handheld, some importers paid as much as £200 in order to have the new system. Upon the Game Gear's release in Europe, video game distributor Virgin Mastertronic unveiled the price of the Game Gear as £99.99, positioning it as being more expensive than the Game Boy, but less expensive than the Atari Lynx, which was also a full-color system. Marketing in the United Kingdom included the use of the slogan, "To be this good takes Sega", and also included advertisements with a biker with a Game Gear.

Support for the Game Gear by Sega was hurt by its primary focus on its home console systems. In addition to the success of the Genesis, Sega was also supporting two peripherals for its home system, the Sega CD, and the 32X, as well as developing its new 32-bit system, the Sega Saturn. Despite selling 10.62 million units by March 1996 (including 1.78 million in Japan), the Game Gear was never able to match the success of its main rival, the Game Boy, which sold over ten times that number. The system's late sales were further hurt by Nintendo's release of the Game Boy Pocket, a smaller version of the Game Boy which could run on two AAA batteries.

Plans for a 16-bit successor to the Game Gear were made to bring Sega's handheld gaming into the fifth generation of video games, but a new handheld system never materialized for Sega, leaving only the Genesis Nomad, a portable version of the Genesis, to take its place. Moreover, the Nomad was intended to supplement the Game Gear rather than replace it; in press coverage leading up to the Nomad's release, Sega representatives said the company was not dropping support for the Game Gear in favor of the Nomad, and that "We believe the two can co-exist". Though the Nomad had been released in 1995, Sega did not officially end support for the Game Gear until 1996 in Japan, and 1997 worldwide.

Though the system was no longer supported by Sega in 2000, third-party developer Majesco Entertainment released a version of the Game Gear at US$30, with games retailing at US$15. New games were released, such as a port of "Super Battletank". This version was also compatible with all previous Game Gear games, but was incompatible with the TV Tuner and some Master System converters. Over ten years later, on March 2, 2011, Nintendo announced that their 3DS Virtual Console service on the Nintendo eShop would feature games from Game Gear.

A handheld game console, the Game Gear was designed to be played while being held horizontally. The console contains an 8-bit 3.5MHz Zilog Z80 chip for a central processing unit, the same as the Master System. Its screen is 3.2 square inches in size and is able to display up to 32 colors at a time from a total palette of 4096 colors, at a display resolution of 160 × 144 pixels. The screen is backlit in order to allow gamers to play in low-lighting situations. Powered by 6 AA batteries, the Game Gear has an approximate battery life of 3 to 5 hours. In order to lengthen this duration and to save money for consumers, Sega also released two types of external rechargeable battery packs for the Game Gear. The system contains 8kB of RAM and an additional 16kB of video RAM. It produces sound using a Texas Instruments SN76489 PSG, which was also used in the Master System; however, unlike the Master System, stereo sound is able to be supplied through an output for headphones. Physically, the Game Gear measures 210mm across, 113mm high, and 38mm deep.

Several accessories were created for the Game Gear during its lifespan. A TV Tuner accessory with a whip antenna plugs into the system's cartridge slot, allowing the viewing of analog television stations over-the-air on the Game Gear's screen. Released at $105.88 ($186 in 2016), the add-on was expensive but unique for collectors and contributed to the system's popularity. Another accessory, the Super Wide Gear, magnifies the Game Gear screen to compensate for its relatively small size. Also released was the Car Gear adapter that plugs into cars or cigarette lighters to power the system while traveling, and the Gear to Gear Cable that establishes a data connection between two Game Gear systems using the same multiplayer game and let users play against each other.

Over the course of its lifespan, the Game Gear also received a number of variations. Later releases included several different colors for the console, including a blue "sports" variation released in North America bundled with "World Series Baseball '95" or "The Lion King". A white version was also released, sold in a bundle with a TV tuner. Other versions included a red Coca-Cola-themed unit, bundled with the game "Coca-Cola Kid", and the Kids Gear, a Japan-only variation targeted toward children.

Over 300 games were released for the Game Gear, although at the time of the console's launch, there were only six software games available. Prices for game cartridges initially ranged from $24.99 to $29.99 each. The casings were molded black plastic with a rounded front to aid in removal. Games for the system included "Sonic the Hedgehog", "The GG Shinobi", "Space Harrier", and "Land of Illusion Starring Mickey Mouse", which was considered the best game for the system by "GamesRadar+". Later games took advantage of the success of the Genesis, Sega's 16-bit video game console, with games released from franchises originally released on the Genesis. A large part of the Game Gear's library consists of Master System ports. Because of the landscape orientation of the Game Gear's screen and the similarities in hardware between the handheld console and the Master System, it was easy for developers to port Master System games to the Game Gear.

Due to Nintendo's licensing practices during the lifespan of the Game Gear, few third-party developers were available to create games for Sega's system. This was a contributing factor to the large number of Master System ports for the Game Gear. Likewise, because of this, the Game Gear library contained many games that were not available on other handhelds, pulling sales away from the Atari Lynx and NEC TurboExpress and helping to establish the Game Gear's position in the market. While the Game Gear's library consisted of over 300 games, however, the Game Boy's library contained over 1000 individual games. Several Game Gear games were released years later on the Nintendo 3DS's Virtual Console service on the Nintendo eShop. The emulator for the Virtual Console releases was handled by M2.

Game Gear surpassed the Atari Lynx and NEC TurboExpress, but lagged far behind the Game Boy in the handheld marketplace. Retrospective reception to the Game Gear is mixed. In 2008, "GamePro" listed the Game Gear as 10th on their list of the "10 Worst-Selling Handhelds of All Time" and criticized aspects of the implementation of its technology, but also stated that the Game Gear could be considered a success for having nearly 11 million units sold. According to "GamePro" reviewer Blake Snow, "Unlike the Game Boy, the Game Gear rocked the landscape holding position, making it less cramped for human beings with two hands to hold. And even though the Game Gear could be considered a success, its bulky frame, relative high price, constant consumption of AA batteries, and a lack of appealing games ultimately kept Sega from releasing a true successor." In speaking with "Famitsu DC" for their November 1998 issue, Sato stated that the Game Gear did take a significant piece of the handheld console market share, but that "Nintendo’s Game Boy was such a runaway success, and had gobbled up so much of the market, that our success was still seen as a failure, which I think is a shame."

"GamesRadar+" offered some praise for the system and its library, stating, "With its 8-bit processor and bright color screen, it was basically the Sega Master System in your hands. How many batteries did we suck dry playing Sonic, Madden and Road Rash on the bus or in the car, or in the dark when we were supposed to be sleeping? You couldn't do that on a Game Boy!" By contrast, "IGN" reviewer Levi Buchanan stated the Game Gear's biggest fault was its game library when compared to the Game Boy, stating, "the software was completely lacking compared to its chief rival, which was bathed in quality games. It didn't matter that the Game Gear was more powerful. The color screen did not reverse any fortunes. Content and innovation beat out technology, a formula that Nintendo is using right now with the continued ascendance of the DS and Wii." Buchanan later went on to praise some parts of the Game Gear's library, however, stating "Some of those Master System tweaks were very good games, and fun is resilient against time." "Retro Gamer" praised Sega's accomplishment in surviving against the competition of Nintendo in the handheld console market with the Game Gear, noting that "for all the handhelds that have gone up against the might of Nintendo and ultimately lost out, Sega's Game Gear managed to last the longest, only outdone in sales by the Sony PSP. For its fans, it will remain a piece of classic gaming hardware whose legacy lives on forever."



</doc>
<doc id="29028" url="https://en.wikipedia.org/wiki?curid=29028" title="32X">
32X

The 32X is an add-on for the Sega Genesis video game console. Codenamed "Project Mars", the 32X was designed to expand the power of the Genesis and serve as a transitional console into the 32-bit era until the release of the Sega Saturn. Independent of the Genesis, the 32X uses its own ROM cartridges and has its own library of games. The add-on was distributed under the name in Japan, Genesis 32X in North America, Mega Drive 32X in the PAL region, and Mega 32X in Brazil.

Unveiled by Sega at June 1994's Consumer Electronics Show, the 32X was presented as a low-cost option for consumers looking to play 32-bit games. Developed in response to the Atari Jaguar and concerns that the Saturn would not make it to market by the end of 1994, the product was conceived as an entirely new console. At the suggestion of Sega of America executive Joe Miller and his team, the console was converted into an add-on to the existing Genesis and made more powerful. The final design contained two 32-bit central processing units and a 3D graphics processor. To bring the new add-on to market by its scheduled release date of November 1994, development of the new system and its games was rushed. The console failed to attract third-party video game developers and consumers because of the announcement of the Sega Saturn's simultaneous release in Japan. Sega's efforts to rush the 32X to market cut into available time for game development, resulting in a weak library of forty titles that could not fully use the add-on's hardware, including Genesis ports. Sega produced 800,000 units of the 32X and managed to sell an estimated 665,000 by the end of 1994, selling the rest at steep discounts until it was discontinued in 1996 as Sega turned its focus to the Saturn.

The 32X is considered a commercial failure. Reception after the add-on's unveiling and launch was positive, highlighting the low price of the system and power expansion to the Genesis. Later reviews, both contemporary and retrospective, for the 32X have been mostly negative because of its shallow game library, poor market timing and the resulting market fragmentation for the Genesis.

The Sega Genesis, initially released in Japan as the Mega Drive in 1988, was Sega's entry into the 16-bit era of video game consoles. The console was then released as the Genesis in 1989 for the North American market, with releases in other regions following a year later.

Although the earlier release of the Sega CD add-on had been commercially disappointing, Sega began to develop a stop-gap solution that would bridge the gap between the Genesis and the Sega Saturn, serving as a less expensive entry into the 32-bit era. The decision to create a new system was made by Sega CEO Hayao Nakayama and broadly supported by Sega of America employees. According to former Sega of America producer Scot Bayless, Nakayama was worried that the Saturn would not be available until after 1994, and about the recent release of the 64-bit Atari Jaguar. As a result, the direction given was to have this second release to market by the end of the year.

During the Winter Consumer Electronics Show in January 1994, Sega of America research and development head Joe Miller took a phone call in his Las Vegas hotel suite from Nakayama, in which Nakayama stressed the importance of coming up with a quick response to the Jaguar. Included on this call were Bayless, Sega hardware team head Hideki Sato, and Sega of America vice president of technology Marty Franz. One potential idea for this came from a concept from Sega of Japan, later known as "Project Jupiter", an entirely new independent console. Project Jupiter was initially slated to be a new version of the Genesis, with an upgraded color palette and a lower cost than the upcoming Saturn, as well as with some limited 3D capabilities thanks to integration of ideas from the development of the Sega Virtua Processor chip. Miller suggested an alternative strategy, citing concerns with releasing a new console with no previous design specifications within six to nine months. According to former Sega of America producer Michael Latham, Miller said, "Oh, that's just a horrible idea. If all you're going to do is enhance the system, you should make it an add-on. If it's a new system with legitimate new software, great. But if the only thing it does is double the colors..." Miller, however, insists that the decision was made collectively to talk about alternative solutions. One idea was to leverage the existing Genesis as a way to keep from alienating Sega customers, who would otherwise be required to discard their Genesis systems entirely to play 32-bit games, and to control the cost of the new system. This would come in the form of an add-on. From these discussions, Project Jupiter was discontinued and the new add-on, codenamed "Project Mars", was advanced.

At the suggestion from Miller and his team, Sega designed the 32X as a peripheral for the existing Genesis, expanding its power with two 32-bit SuperH-2 processors. The SH-2 had been developed in 1993 as a joint venture between Sega and Japanese electronics company Hitachi. The original design for the 32X add-on, according to Bayless, was created on a cocktail napkin, but Miller insists that this was not the case. At the end of the Consumer Electronics show, with the basic design of the 32X in place, Sega of Japan invited Sega of America to assist in development of the new add-on.

Although the new unit was a stronger console than originally proposed, it was not compatible with Saturn games. This was justified by Sega's statement that both platforms would run at the same time, and that the 32X would be aimed at players who could not afford the more expensive Saturn. Bayless praised the potential of this system at this point, calling it "a coder's dream for the day" with its twin processors and 3D capabilities. Sega of America headed up the development of the 32X, with some assistance from Sato's team at Sega of Japan. Shortages of processors due to the same 32-bit chips being used in both the 32X and the Saturn hindered the development of the 32X, as did the language barrier between the teams in Japan and the United States.

Before the 32X could be launched, the release date of the Saturn was announced for November 1994 in Japan, coinciding with the 32X's target launch date in North America. Sega of America now was faced with trying to market the 32X with the Saturn's Japan release occurring simultaneously. Their answer was to call the 32X a "transitional device" between the Genesis and the Saturn, to which Bayless describes of the strategy, "[f]rankly, it just made us look greedy and dumb to consumers."

The unveiling of the 32X to the public came at the Summer Consumer Electronics Show in June 1994 in Chicago. Promoted as the "poor man's entry into 'next generation' games", 32X was marketed for its US$159 price point as a less-expensive alternative to the Saturn. However, Sega would not answer as to whether or not a Genesis console equipped with a Sega CD and a 32X would be able to run Saturn software. Founder of The 3DO Company, Trip Hawkins, was willing to point out that it would not, stating, "Everyone knows that 32X is a Band-Aid. It's not a 'next generation system.' It's fairly expensive. It's not particularly high-performance. It's hard to program for, and it's not compatible with the Saturn." In response to these comments, Sega executive Richard Brudvik-Lindner pointed out that the 32X would play Genesis titles, and had the same system architecture as the Saturn.

In August of that year, "GamePro" highlighted the advantages of the upcoming add-on in its 32-bit processors and significantly lower price, noting that "[n]o doubt gotta-get-it-now gamers will spend the big bucks to grab Saturn or PlayStation systems and games from Japan. For the rest of us, however, 32X may well be the system of choice in '94." In promotion for the new system, Sega promised 12 games available at launch and 50 games due for release in 1995 from third-party developers.

The 32X was released on November 21, 1994, in North America, in time for the holiday season that year. As announced, it retailed for $159.99, and had a reasonably successful launch in the marketplace. Demand among retailers was high, and Sega could not keep up with orders for the new system. Over 1,000,000 orders had been placed for 32X units, but Sega had only managed to ship 600,000 units by January 1995. Launching at about the same price as a Genesis console, the price of the 32X was less than half of what the Saturn's price would be at launch. Despite Sega's initial promises, only six titles were available at its North American launch, including "Doom", "Star Wars Arcade", "Virtua Racing Deluxe", and "Cosmic Carnage". Although "Virtua Racing" was considered a "strong" title, "Cosmic Carnage" "looked and played so poorly that reporters made jokes about it." Games were available at a retail price of $69.95. Advertising for the system included images of the 32X being connected to a Genesis console to create an "arcade system". Japan received the 32X on December 3, 1994. The system's PAL release came in January 1995, at a price of GB£169.99, and also experienced initial high demand.

Despite the lower price console's positioning as an inexpensive entry into 32-bit gaming, Sega had a difficult time convincing third-party developers to create games for the new system. Top developers were already aware of the coming arrival of the Sega Saturn, Nintendo 64, and PlayStation, and did not believe the 32X would be capable of competing with any of those systems. The quick development time of the 32X also made game development difficult, according to Franz. Not wanting to create games for an add-on that was "a technological dead-end", many developers decided not to make games for the system. Issues also plagued titles developed in-house due to the time crunch to release the 32X. According to Bayless, "games in the queue were effectively jammed into a box as fast as possible, which meant massive cutting of corners in every conceivable way. Even from the outset, designs of those games were deliberately conservative because of the time crunch. By the time they shipped they were even more conservative; they did nothing to show off what the hardware was capable of."

Journalists were similarly concerned about Sega's tactic of selling two similar consoles at different prices and attempting to support both, likening Sega's approach to that of General Motors and segmenting the market for its consoles. In order to convince the press that the 32X was a worthwhile console, Sega flew in journalists from all around the country to San Francisco for a party at a local nightclub. The event featured a speech from Tom Kalinske, live music with a local rapper praising the 32X, and 32X games on exhibition. However, the event turned out to be a bust, as journalists attempted to leave the party due to its loud music and unimpressive games on display, only to find that the buses that brought them to the nightclub had just left and would not return until the scheduled end of the party.

Though the system had a successful launch, demand soon disappeared. Over the first three months of 1995, several of the 32X's third party publishers, including Capcom and Konami, cancelled their 32X projects so that they could focus on producing games for the Saturn and PlayStation. The 32X failed to catch on with the public, and is considered a commercial failure. By 1995, the Genesis had still not proven successful in Japan, where it was known as Mega Drive, and the Saturn was beating the PlayStation, so Sega CEO Hayao Nakayama decided to force Sega of America to focus on the Saturn and cut support for Genesis products, executing a surprise early launch of the Saturn in the early summer of 1995. Sega was supporting five different consoles before this—Saturn, Genesis, Game Gear, Pico, and the Master System—as well as the Sega CD and Sega 32X add-ons. Sales estimates for the 32X stood at 665,000 units at the end of 1994. Despite assurances from Sega that many games would be developed for the system, in early 1996, Sega finally conceded that it had promised too much out of the add-on and decided to discontinue the 32X in order to focus on the Saturn. In September 1995, the retail price for the 32X dropped to $99, and later the remaining inventory was cleared out of stores at $19.95, with 800,000 units sold in total.

The Sega Neptune is an unproduced two-in-one Genesis and 32X console which Sega planned to release in fall 1995, with the retail price planned to be something less than US$200. It was featured as early as March 1995, with "Sega Magazine" saying the console "shows [Sega's] commitment to the hardware." Sega cancelled the Neptune in October 1995, citing fears that it would dilute their marketing for the Saturn while being priced too close to the Saturn to be a viable competitor. "Electronic Gaming Monthly" used the Sega Neptune as an April Fools' Day prank in its April 2001 issue. The issue included a small article in which the writers announced that Sega had found a warehouse full of old Sega Neptunes, and were selling them on a website for $199.

The 32X can be used only in conjunction with a Genesis system. It is inserted into the system like a standard game cartridge. The add-on requires its own separate power supply, a connection cable linking it to the Genesis, and an additional conversion cable for the original model of the Genesis. As well as playing its own library of cartridges, the 32X is backwards-compatible with Genesis games, and can also be used in conjunction with the Sega CD to play games that use both add-ons. The 32X also came with a spacer so it would fit properly with the second model of the Genesis; an optional spacer was offered for use with the Sega Genesis CDX system, but ultimately never shipped due to risks of electric shock when the 32X and CDX were connected. Installation of the 32X also requires the insertion of two included electromagnetic shield plates into the Genesis' cartridge slot.

Seated on top of a Genesis, the 32X measures . The 32X contains two Hitachi SH2 32-bit RISC processors with a clock speed of 23 MHz, which Sega claimed would allow the system to work 40 times faster than a stand-alone Genesis. Its graphics processing unit is capable of producing 32,768 colors and rendering 50,000 polygons per second, which provides a noticeable improvement over the polygon rendering of the Genesis. The 32X also includes 256 kilobytes of random-access memory (RAM), along with 256 kilobytes of video RAM. Sound is supplied through a pulse-width modulation sound source. Input/output is supplied to a television set via a provided A/V cable that supplies composite video and stereo audio, or through an RF modulator. Stereo audio can also be played through headphones via a headphone jack on the attached Genesis.

The 32X's game library consists of forty titles, including six that required both the Sega 32X and Sega CD. Among the titles for the 32X were ports of arcade games "After Burner", "Space Harrier", and "Star Wars Arcade", a sidescroller with a hummingbird as a main character in "Kolibri", and a 32X-exclusive game in the "Sonic the Hedgehog" series titled "Knuckles' Chaotix". Several of the games released for the 32X are enhanced ports of Genesis games, including "NFL Quarterback Club" and "World Series Baseball '95". In a retrospective review of the console, "Star Wars Arcade" was considered the best game for the 32X by "IGN" for its cooperative play, soundtrack, and faithful reproduction of the experiences of "Star Wars". In a separate review, "IGN"'s Buchanan praised the 32X title "Shadow Squadron" as superior to "Star Wars Arcade". "Retro Gamer" writer Damien McFerran, however, praised "Virtua Fighter" as "the jewel in the 32X's crown", and "GamesRadar+" named "Knuckles' Chaotix" as the best game for the system. "Next Generation" called "Virtua Fighter" "the colorful wreath on 32X's coffin", reflecting the consensus among contemporary critics that the game was at once arguably the 32X's best release and a clear harbinger of the platform's imminent discontinuation, since it was clearly inferior to the Saturn versions of "Virtua Fighter Remix" (which had already been released) and "Virtua Fighter 2" (which was due out in just a few months). In response to fan inquiries, Sega stated that the 32X architecture was not powerful enough to handle a port of "Virtua Fighter 2".

Although the console used 32-bit processing and was capable of better graphics and sound than the Genesis alone, most games for the 32X did not take advantage of its hardware. "Doom" for the 32X received near perfect reviews from gaming magazines upon launch, but was later criticized for being an inferior version of the game compared to releases for the PC and the Atari Jaguar, with the 32X version criticized for missing levels, poor graphic and audio quality, jerky movement, and running within a window on the screen. Though the system had enhanced audio capabilities, 32X games did not use this, which Franz believes was due to developers being unwilling to invest in designing games to work with the new audio enhancements. One source of these issues was the rush to release games on time for the 32X's launch; former Sega of America executive producer Michael Latham explained, in reference to 32X launch title "Cosmic Carnage", "We were rushed. We had to get games out for the 32X and it was going to be such a close cycle. When "Cosmic Carnage" showed up, we didn't even want to ship it. It took a lot of convincing, you know, to ship that title." Likewise with "Doom", id Software's John Carmack rushed to have the port ready for release at the 32X's launch and had to trim out a third of the game's levels in order to meet the deadline for the port to be published on time. Because of time limitations, game designs were intentionally conservative and did not show what the 32X hardware was able to do. In an interview at the end of 1995, Sega vice president of marketing Mike Ribero, while insisting that Sega was not abandoning the 32X, acknowledged that first party software support for the system had been lackluster: "I won't lie to you, we screwed up with 32X. We overpromised and underdelivered."

Initial reception to the 32X and its games upon the launch of the add-on was very positive. Four reviewers from "Electronic Gaming Monthly" scored the add-on 8, 7, 8, and 8 out of 10 in their 1995 Buyer's Guide, highlighting the add-on's enhancements to the Genesis but questioning how long the system would be supported. "GamePro" commented that the 32X's multiple input and power cords make it "as complicated as setting up your VCR" and noted some performance glitches with the prototype such as freezes and overheating, but expressed confidence that the production models would perform well and gave the add-on their overall approval. Reviews of its launch titles, such as "Doom", were likewise positive. By late 1995, feedback to the add-on had soured. In its 1996 Buyer's Guide, "Electronic Gaming Monthly"'s four reviewers scored the add-on 3, 3, 3, and 2 out of 10, criticizing the game library and Sega's abandonment of the system in favor of the Saturn. A review in "Next Generation" panned the 32X for its weak polygon processing, the tendency of developers to show off its capabilities with garishly colored games, and its apparent function as "simply a way of grabbing extra 1994 mind and market share while waiting for Saturn". The review gave it one out of five stars.

Retrospectively, the 32X is widely criticized as having been under-supported and a poor idea in the wake of the release of the Sega Saturn. "1UP.com"'s Jeremy Parish stated that the 32X "tainted just about everything it touched." "GamesRadar+" also panned the system, placing it as their ninth-worst console with reviewer Mikel Reparaz criticizing that "it was a stopgap system that would be thrown under the bus when the Sega Saturn came out six months later, and everyone seemed to know it except for die-hard Sega fans and the company itself." "<nowiki>Retro Gamer'</nowiki>"s Damien McFerran offered some praise for the power increase of the 32X to offer ports of "Space Harrier", "After Burner", and "Virtua Fighter" that were accurate to the original arcade versions, as well as the add-on's price point, stating, "If you didn't have deep enough pockets to afford a Saturn, then the 32X was a viable option; it's just a shame that it sold so poorly because the potential was there for true greatness." Levi Buchanan, writing for "IGN", saw some sense in the move for Sega to create the 32X but criticized its implementation. According to Buchanan, "I actually thought the 32X was a better idea than the SEGA CD... The 32X, while underpowered, at least advanced the ball. Maybe it only gained a few inches in no small part due to a weak library, but at least the idea was the right one."

In particular, the console's status as an add-on and poor timing after the announcement of the Saturn has been identified by reviewers as being responsible factors for fracturing the audience for Sega's video game consoles in terms of both developers and consumers. "Allgame"'s Scott Alan Marriott states that "[e]very add-on whittled away at the number of potential buyers and discouraged third-party companies from making the games necessary to boost sales." "GamePro" criticized the concept of the add-on, noting the expenses involved in purchasing the system. According to reviewer Blake Snow, "Just how many 16-bit attachments did one need? All in all, if you were one of the unlucky souls who completely bought into Sega's add-on frenzy, you would have spent a whopping $650 for something that weighed about as much as a small dog." Writing for "GamesRadar+", Reparaz noted that "developers—not wanting to waste time on a technological dead-end—abandoned the 32X in droves. Gamers quickly followed suit, turning what was once a promising idea into an embarrassing footnote in console history, as well as an object lesson in why console makers shouldn't split their user base with pricey add-ons." Reparaz went on to criticize Sega's decision to release the 32X, noting that "(u)ltimately, the 32X was the product of boneheaded short-sightedness: its existence put Sega into competition with itself once the Saturn rolled out." Writing for "IGN", Buchanan points out, "Notice that we haven't seen many add-ons like the 32X since 1994? I think the 32X killed the idea of an add-on like this—a power booster—permanently. And that's a good thing. Because add-ons, if not implemented properly, just splinter an audience."

Former executives at Sega have mixed opinions of the 32X. Bayless believes firmly that the 32X serves as a warning to the video game industry not to risk splintering the market for consoles by creating add-ons, and was critical of the Kinect and PlayStation Move for doing so. Franz places the 32X's commercial failure on its inability to function without an attached Genesis and lack of a CD drive, despite its compatibility with the Sega CD, stating, "The 32X was destined to die because it didn't have a CD drive and was an add-on. An add-on device is never as well thought out as a built-from-scratch device." Miller, on the other hand, remembers the 32X positively, stating, "I think the 32X actually was an interesting, viable platform. The timing was wrong, and certainly our ability to stick with it, given what we did with Saturn, was severely limited. There were a whole bunch of reasons why we couldn’t ultimately do what we had to do with that platform, without third party support and with the timing of Saturn, but I still think the project was a success for a bunch of other reasons. In hindsight, it was not a great idea for a whole bunch of other reasons."



</doc>
<doc id="29031" url="https://en.wikipedia.org/wiki?curid=29031" title="Severan dynasty">
Severan dynasty

The Severan dynasty was a Roman imperial dynasty, which ruled the Roman Empire between 193 and 235. The dynasty was founded by the general Septimius Severus, who rose to power as the victor of the Civil War of 193–197.

Although Septimius Severus successfully restored peace following the upheaval of the late 2nd century, the dynasty was disturbed by highly unstable family relationships, as well as constant political turmoil foreshadowing the imminent Crisis of the Third Century. 
It was one of the last lineages of the Principate founded by Augustus.

Lucius Septimius Severus was born to a family of Phoenicia equestrian rank in Leptis Magna, the Roman province of Africa proconsularis, in modern-day Libya. He rose through military service to consular rank under the later Antonines. He married Syrian noblewoman Julia Domna and had two children with her, Caracalla and Geta. He was subsequently proclaimed emperor in 193 by his legionaries in Noricum during the political unrest that followed the death of Commodus, he secured sole rule over the empire in 197 after defeating his last rival, Clodius Albinus, at the Battle of Lugdunum.

Severus fought a successful war against the Parthians and campaigned with success against barbarian incursions in Roman Britain, rebuilding Hadrian's Wall. In Rome, his relations with the Senate were poor, but he was popular with the commoners, as with his soldiers, whose salary he raised. Starting in 197, his Praetorian prefect Gaius Fulvius Plautianus was a negative influence, and he would be executed in 205. One of Plautianus's successors was the jurist Aemilius Papinianus. Severus continued official persecution of Christians and Jews, as they were the only two groups who would not assimilate their beliefs to the official syncretistic creed. (citation needed)

Severus died while campaigning in Britain. He was succeeded by his sons Caracalla and Geta, who reigned under the influence of their mother, Julia Domna.

The eldest son of Severus, he was born Lucius Septimius Bassianus in Lugdunum, Gaul. "Caracalla" was a nickname referring to the Gallic hooded tunic he habitually wore even when he slept. Upon his father's death, Caracalla was proclaimed co-emperor with his brother Geta. Conflict between the two culminated in the assassination of the latter less than a year after their father's death. Reigning alone, Caracalla was noted for lavish bribes to the legionaries and unprecedented cruelty, authorizing numerous assassinations of perceived enemies and rivals. He campaigned with indifferent success against the Alamanni. The Baths of Caracalla in Rome are the most enduring monument of his rule. He was assassinated while en route to a campaign against the Parthians by a Praetorian Guard.

Younger son of Severus, Geta was made co-emperor with his older brother Caracalla upon his father's death. Unlike the much more successful joint reign of Marcus Aurelius and his brother Lucius Verus in the previous century, relations were hostile between the two Severan brothers from the very start. Geta was assassinated in his mother's apartments by order of Caracalla, who thereafter ruled as sole Augustus.

Marcus Opelius Macrinus was born in 164 at Caesarea Mauretaniae (modern day Cherchell, Algeria). Although coming from a humble background that was "not" dynastically related to the Severan dynasty; he rose through the imperial household until, under the emperor Caracalla, he was made Prefect of the Praetorian Guard. On account of the cruelty and treachery of the emperor, Macrinus became involved in a conspiracy to kill him, and ordered the Praetorian Guard to do so. On April 8, 217, Caracalla was assassinated travelling to Carrhae. Three days later, Macrinus was declared Augustus.

His most significant early decision was to make peace with the Parthians, but many thought that the terms were degrading to the Romans. However, his downfall was his refusal to award the pay and privileges promised to the eastern troops by Caracalla. He also kept those forces wintered in Syria, where they became attracted to the young Elagabalus. After months of mild rebellion by the bulk of the army in Syria, Macrinus took his loyal troops to meet the army of Elagabalus near Antioch. Despite a good fight by the Praetorian Guard, his soldiers were defeated. Macrinus managed to escape to Chalcedon but his authority was lost: he was betrayed and executed after a short reign of just 14 months.

Marcus Opelius Diadumenianus (known as Diadumenian) was the son of Macrinus, born in 208. He was given the title Caesar in 217, when his father became Emperor. After his father's defeat outside Antioch, he tried to escape east to Parthia, but was captured and killed before he could achieve this.

Elagabalus was born Varius Avitus Bassianus in 204, and became known later as Marcus Aurelius Antonius. The name "Elagabalus" followed the Latin nomenclature for the Syrian sun god Elagabal, of whom he had become a priest at an early age. Elagabal was represented by a large, dark rock called a baetyl. Elagabalus's grandmother, Julia Maesa, Julia Domna's sister and sister-in-law of Emperor Septimius Severus, arranged for the restoration of the Severan dynasty, and persuaded soldiers from The Gallic Third Legion who were stationed near Emesa, using her enormous wealth, as well as the claim that Caracalla had slept with her daughter and that the boy was his bastard to swear fealty to Elagabalus. He was later invited alongside his mother and daughters to the military camp, clad in imperial purple, and crowned as emperor by the soldiers.

His reign in Rome has long been known for being outrageous, although the historical sources are few, and in many cases not to be fully trusted. He is said to have smothered guests at a banquet by flooding the room with rose petals, married his male lover (who was thereafter referred to as the "Empress's husband"), and married a vestal virgin. Dio suggests he was transgender, and that he offered large sums to the physician who could give him female genitalia.

The running of the Empire during this time was mainly left to his grandmother and mother (Julia Soamias). Seeing that her grandson's outrageous behavior could mean the loss of power, Julia Maesa persuaded Elagabalus to accept his young cousin Severus Alexander as Caesar (and thus the nominal Emperor-to-be). Alexander was popular with the troops, who increasingly objected to Elagabalus' behaviour. Jealous of this popularity, Elagabalus removed the title of Caesar from his cousin, enraging Alexander's protectors, the Praetorian Guard. Elagabalus and his mother were then assassinated in a Praetorian Guard camp mutiny.

Born Marcus Julius Gessius Bassianus Alexianus in around 208, Alexander was adopted as heir apparent by his slightly older and very unpopular cousin, the Emperor Elagabalus at the urging of the influential and powerful Julia Maesa— who was grandmother of both cousins and who had arranged for the emperor's acclamation by the Third Legion.

On March 6, 222, when Alexander was just fourteen, a rumor went around the city's troops that Alexander had been killed, which triggered his ascension as emperor. The eighteen-year-old Emperor Elagabalus and his mother were both taken from the palace, dragged through the streets, murdered and thrown in the river Tiber by the Praetorian Guard, who then proclaimed Alexander Severus as Augustus.

Ruling from the age of fourteen under the influence of his able mother, Julia Avita Mamaea, Alexander restored, to some extent, the moderation that characterized the rule of Septimius Severus. The rising strength of the Sasanian Empire (226–651) heralded perhaps the greatest external challenge that Rome faced in the 3rd century. His prosecution of the war against a German invasion of Gaul led to his overthrow by the troops he was leading there, whose regard the twenty-seven-year-old had lost during the affair.

His death was the epochal event beginning the troubled Crisis of the Third Century, where a succession of briefly-reigning military emperors, rebellious generals, and counter-claimants presided over governmental chaos, civil war, general instability and great economic disruption. He was succeeded by Maximinus Thrax, the first of a series of weak emperors, each ruling on average only 2 to 3 years, that ended fifty years later with the Emperor Diocletian splitting the Eastern and Western Roman Empires.

The women of the Severan dynasty, beginning with Septimius Severus's wife Julia Domna, were notably active in advancing the careers of their male relatives. Other notable women who exercised power behind the scenes in this period include Julia Maesa, sister of Julia Domna, and Maesa's two daughters Julia Soaemias, mother of Elagabalus, and Julia Avita Mamaea, mother of Alexander Severus. Also of interest is Publia Fulvia Plautilla, daughter of Gaius Fulvius Plautianus, the Prefect Commander of the Praetorian Guard, married to but despised by Caracalla who had her exiled and eventually executed.





</doc>
<doc id="29032" url="https://en.wikipedia.org/wiki?curid=29032" title="Sega CD">
Sega CD

The Sega CD, released as the in most regions outside North America and Brazil, is a CD-ROM accessory for the Sega Genesis designed and produced by Sega as part of the fourth generation of video game consoles. It was released on December 12, 1991 in Japan, October 15, 1992 in North America, and April 2, 1993 in Europe. The Sega CD plays CD-based games and adds hardware functionality such as a faster central processing unit and graphic enhancements. It can also play audio CDs and CD+G discs.

The main benefit of CD technology was greater storage, which allowed for games to be nearly 320 times larger than Genesis cartridges. This benefit manifested as full motion video (FMV) games such as the controversial "Night Trap", which became a focus of the 1993 congressional hearings on issues of video game violence and ratings. Sega of Japan partnered with JVC to design the Sega CD and refused to consult with Sega of America until the project was complete. Sega of America assembled parts from various "dummy" units to obtain a working prototype. It was redesigned several times by Sega and licensed third-party developers.

While the Sega CD became known for several well received games such as "Sonic CD" and "", its game library contained many Genesis ports and poorly received FMV games. 2.24 million Sega CD units were sold by March 1996, after which Sega discontinued the system to focus on the Sega Saturn. Retrospective reception is mixed, with praise for individual games and additional functions, but criticism for its lack of deep games, high price, and support from Sega.

Released in 1988, the Genesis (known as the Mega Drive in Europe and Japan) was Sega's entry into the fourth generation of video game consoles. In mid-1990, Sega CEO Hayao Nakayama hired Tom Kalinske as CEO of Sega of America. Kalinske developed a four-point plan for sales of the Genesis: cut the console's price, develop games for the American market with a new American team, continue aggressive advertising campaigns, and ship "Sonic the Hedgehog" with the Genesis as a pack-in game. The Japanese board of directors initially disapproved of the plan, but all four points were approved by Nakayama, who told Kalinske, "I hired you to make the decisions for Europe and the Americas, so go ahead and do it." Magazines praised "Sonic" as one of the greatest games yet made, and Sega's console finally took off as customers who had been waiting for the Super Nintendo Entertainment System (SNES) decided to purchase a Genesis instead.

By the early 1990s, compact discs (CDs) were making significant headway as a storage medium for music and video games. NEC had been the first to use CD technology in a video game console with their PC Engine CD-ROM² System add-on in October 1988 in Japan (launched in North America as the TurboGrafx-CD the following year), which sold 80,000 units in six months. That year, Nintendo announced a partnership with Sony to develop its own CD-ROM peripheral for the SNES. Commodore International released their CD-based CDTV multimedia system in early 1991, while the CD-i from Philips arrived towards the end of that year.

Shortly after the release of the Genesis, Sega's Consumer Products Research and Development Labs led by manager Tomio Takami were tasked with creating a CD-ROM add-on, which became the Sega CD. The Sega CD was originally intended to equal the capabilities of the TurboGrafx-CD, but with twice as much random-access memory (RAM), and sell for about JP¥20,000 (or US$150). In addition to relatively short loading times, Takami's team planned the device to feature hardware scaling and rotation similar to that of Sega's arcade games, which required a dedicated digital signal processor (DSP).

However, two changes made later in development contributed to the final unit's higher-than-expected price. Because the Genesis' Motorola 68000 CPU was too slow to handle the Sega CD's new graphical capabilities, an additional 68000 CPU was incorporated. In addition, upon hearing rumors that NEC planned a memory upgrade to the TurboGrafx-CD, which would bring its available RAM from 0.5 Mbit to between 2 and 4 Mbit, Sega decided to increase the Sega CD's available RAM from 1 Mbit to 6 Mbit. This proved to be one of the greatest technical challenges since the CD's access speed was initially too slow to run programs effectively. The cost of the device was now estimated at $370, but market research convinced Sega executives that consumers would be willing to pay more for a state-of-the-art machine. Sega partnered with JVC, which had been working with Warner New Media to develop a CD player under the CD+G standard.

Until mid-1991, Sega of America had been kept largely uninformed of the details of the project, without a functioning unit to test (although Sega of America was provided with preliminary technical documents earlier in the year). According to former Sega of America executive producer Michael Latham, "When you work at a multinational company, there are things that go well and there are things that don't. They didn't want to send us working Sega CD units. They wanted to send us dummies and not send us the working CD units until the last minute because they were concerned about what we would do with it and if it would leak out. It was very frustrating." Latham and Sega of America vice president of licensing Shinobu Toyoda put together a functioning Sega CD by acquiring a ROM for the system and installing it in a dummy unit.

Sega of America staff were also frustrated by the Sega CD's construction. Former Sega of America senior producer Scot Bayless said: "The Mega-CD was designed with a cheap, consumer-grade audio CD drive, not a CD-ROM. Quite late in the run-up to launch, the quality assurance teams started running into severe problems with many of the units—and when I say severe, I mean units literally bursting into flames. We worked around the clock, trying to catch the failure in-progress, and after about a week we finally realized what was happening," citing the need for games to use more time seeking data than the CD drive was designed to provide.

Sega announced the release of the Mega-CD in Japan for late 1991, and North America (as the Sega CD) in 1992. It was unveiled to the public for the first time at the 1991 Tokyo Toy Show, to positive reception from critics. It was released in Japan on December 12, 1991, initially retailing at JP¥49,800. Though the unit sold quickly, the small install base of the Mega Drive in Japan meant that sales declined rapidly. Within its first year in Japan, the Mega-CD only sold 100,000 units. Third-party development of games for the new system suffered because Sega took a long amount of time to release software development kits. Other factors affecting sales included the high launch price of the Mega-CD in Japan and only two games available at launch.

On October 15, 1992, the Sega CD was released in North America, with a retail price of US$299. Advertising included one of Sega's slogans, "Welcome to the Next Level". Though only 50,000 units were available at launch due to production problems, the Sega CD sold over 200,000 units by the end of 1992. As part of Sega's sales, Blockbuster LLC purchased Sega CD units for rental in their stores. The Mega-CD was launched in Europe in the spring of 1993, starting with the United Kingdom on April 2, 1993, at a price of GB£269.99. The European version was packaged with "Sol-Feace" and "Cobra Command" in a two-disc set, along with a compilation CD of five Mega Drive games. Only 70,000 units were initially available in the UK, but 60,000 units were sold by August 1993.

Emphasized by Sega of America, the benefits of the Sega CD's additional storage space allowed for a large amount of full motion video (FMV) games, with Digital Pictures becoming an important partner for Sega. After the initial competition between Sega and Nintendo to develop a CD-based add-on, Nintendo eventually canceled the development process of its own peripheral after having partnered with Sony and then with Philips to develop one.
Sega released a second model, the Sega CD 2 (Mega-CD 2), on April 23, 1993 in Japan at a price of JP¥29,800. It was released in North America several months later at the reduced price of US$229, bundled with one of the system's best-selling games, "Sewer Shark". Designed to bring down the manufacturing costs of the Sega CD, the newer model is smaller and does not use a motorized disc tray. A limited number of games were developed that used the Sega CD and the 32X add-on, released in November 1994.

On December 9, 1993, the United States Congress began to hold hearings on video game violence and the marketing of violent video games to children. One game at the center of this controversy was the Sega CD's "Night Trap", a full-motion video adventure game by Digital Pictures. "Night Trap" had been brought to the attention of United States Senator Joe Lieberman, who said: "It ends with this attack scene on this woman in lingerie, in her bathroom. I know that the creator of the game said it was all meant to be a satire of "Dracula"; but nonetheless, I thought it sent out the wrong message." Lieberman's research concluded that the average video game player was between seven and twelve years old and that video game publishers were marketing violence to children.

In the United Kingdom, former Sega of Europe development director Mike Brogan noted that ""Night Trap" got Sega an awful lot of publicity... Questions were even raised in the UK Parliament about its suitability. This came at a time when Sega was capitalizing on its image as an edgy company with attitude, and this only served to reinforce that image." Despite increased sales as a result of the hearings, Sega recalled "Night Trap" and rereleased it with revisions in 1994. Following these hearings, video game manufacturers came together in 1994 to establish a unified rating system, the Entertainment Software Rating Board.

Newer CD-based consoles such as the 3DO Interactive Multiplayer rendered the Sega CD technically obsolete, reducing public interest. In late 1993, less than a year after the Sega CD's launches in North America and Europe, the media reported that Sega was no longer accepting in-house development proposals for the Mega-CD in Japan. In early 1995, Sega shifted its focus to the Sega Saturn and discontinued advertising for Genesis hardware, including the Sega CD. Sega officially discontinued the Sega CD in the first quarter of 1996, saying that it needed to concentrate on fewer platforms and felt the Sega CD could not compete due to its high price and outdated single-speed drive. The last games scheduled to be released for the Sega CD, "Myst" and "Brain Dead 13", were cancelled. 2.24 million Sega CD units were sold worldwide, including 400,000 in Japan.

The Sega CD can only be used in conjunction with a Genesis system, attaching through an expansion slot on the side of the main console. It requires its own power supply. In addition to playing its own library of games in CD-ROM format, the Sega CD can also play compact discs and karaoke CD+G discs, and can be used in conjunction with the 32X to play 32-bit games that use both add-ons. The second model, also known as the Sega CD 2, includes a steel joining plate to be screwed into the bottom of the Genesis and extension spacer to work with the original Genesis model.

The main CPU of the Sega CD is a 12.5MHz 16-bit Motorola 68000 processor, which runs 5 MHz faster than the Genesis processor. It contains 1 Mbit of boot ROM, allocated for the CD game BIOS, CD player software, and compatibility with CD+G discs. 6 Mbit of RAM are allocated to data for programs, pictures, and sounds; 512 Kbit to PCM waveform memory; 128 Kbit to CD-ROM data cache memory; and an additional 64 Kbit are allocated as the backup memory. Additional backup memory in the form of a 1 Mbit Backup RAM Cartridge was also available as a separate purchase, released near the end of the system's life. Audio can be supplied through the Ricoh RF5C164, and two RCA pin jacks allow the Sega CD to output stereophonic sound separate from the Genesis. Combining stereo sound from a Genesis to either version of the Sega CD requires a cable between the Genesis's headphone jack and an input jack on the back of the CD unit. This is not required for the second model of the Genesis.

Though the Sega CD offers a faster processor, its main purpose is to expand the size of the games. Whereas ROM cartridges of the day typically contained 8 to 16 megabits of data, a CD-ROM disc can hold more than 640 megabytes of data, more than 320 times the storage of a Genesis cartridge. This allows the Sega CD to run games containing full motion video.

The Sega CD received several variations during its lifetime, of which Sega constructed three. The original model utilized a front-loading motorized disc tray and sat underneath the Genesis. Sega later released the second model of the Sega CD, which was redesigned to sit next to the Genesis console and featured a top-loading disc tray in place of the motorized tray of the original model. In addition to the add-on models, Sega also released the Genesis CDX (Multi-Mega in Europe). This console was a combination of the Genesis and Sega CD in one unit and initially retailed at US$399. Unique to this model was its additional functionality as a portable compact disc player.

Three additional system models were created by other electronics companies. Working with Sega, JVC released the Wondermega on April 1, 1992, in Japan, at an initial retail price of ¥82,800 (or US$620). The system was later redesigned by JVC and released as the X'Eye in North America in September 1994. Designed by JVC to be a Genesis and Sega CD combination with high-quality audio, the Wondermega's high price kept it out of the hands of average consumers. Likewise was the case with the Pioneer LaserActive, which was also an add-on that required an attachment developed by Sega, known as the Mega-LD pack, in order to play Genesis and Sega CD games. Though the LaserActive, developed by Pioneer Corporation, was lined up to compete with the 3DO Interactive Multiplayer, the combined system and Mega-LD pack retailed at nearly $1600, becoming a very expensive option for Sega CD players. Aiwa also released the CSD-GM1, a combination Genesis/Sega CD unit built into a boombox.

The Sega CD supports a library of over 200 games created by Sega and third-party publishers. Included in this library are six games which, while receiving individual Sega CD releases, also received separate versions that used both the Sega CD and 32X add-ons. Among the games were a number of FMV games, including "Sewer Shark" and "Fahrenheit". Well regarded games include "Sonic CD", "" and "", "Popful Mail", and "Snatcher", as well as the controversial "Night Trap". Although Sega created "Streets of Rage" for the Genesis to compete against the SNES port of the arcade hit "Final Fight", the Sega CD received an enhanced version of "Final Fight" game that has been praised for its greater faithfulness to the arcade original. "" was noted for its impressive use of the Sega CD hardware as well as its violent content. In particular, "Sonic CD" garnered acclaim for its graphics and time travel gameplay, which improved upon the traditional "Sonic" formula. The Sega CD also received enhanced ports of Genesis games including "Batman Returns" and "Ecco the Dolphin".

Given the large number of FMV games and Genesis ports, the Sega CD's game library has been criticized for its lack of depth. Full-motion video quality was substandard on the Sega CD due to poor video compression software and limited color palette, and the concept never caught on with the public. According to Digital Pictures founder Tom Zito, "Sega CD could only put up 32 colors at a time, so you had this horrible grainy look to the images," though the system was able to put up 64 colors at one time. Likewise, most Genesis ports for the Sega CD featured additional full motion video sequences, extra levels, and enhanced audio, but were otherwise identical to their Genesis release. The video quality in these sequences has also been criticized as comparable to an old VHS tape.

Near the time of its release, the Sega CD was awarded Best New Peripheral of 1992 by Electronic Gaming Monthly. Four separate reviews scored the add-on 8, 9, 8, and 8 out of 10; reviewers cited its upgrades to the Genesis as well as its high-quality and expanding library of games. Later reception in 1995 by Electronic Gaming Monthly showed a more mixed response to the peripheral, with four reviewers scoring it 5 out of 10, citing its game library issues and substandard video quality. "GamePro" also criticized the weak games library and substandard video quality, noting that many of the games were simple ports of cartridge games with minimal enhancements and commenting that "The Sega CD could have been an upgrade, but it's essentially a big memory device with CD sound." They gave it a "thumbs sideways" and recommended that Genesis fans buy an SNES before even considering a Sega CD. Likewise, in a special Game Machine Cross Review in May 1995, "Famicom Tsūshin" scored the Japanese Mega-CD 2 a 17 out of 40.

Retrospective reception of the Sega CD is mixed, praising certain games but criticizing its low value for money and limitations on the benefits it provides to the Genesis. "GamePro" listed the Sega CD as the 7th-worst selling video game console of all time, with reviewer Blake Snow noting that "The problem was threefold: the device was expensive at $299, it arrived late in the 16-bit life cycle, and it didn't do much (if anything) to enhance the gameplay experience." Snow went on to note, however, that the Sega CD did have in its library "the greatest "Sonic" game of all time" in "Sonic CD". IGN's Levi Buchanan criticized Sega's implementation of CD technology for the Genesis, noting, "What good is the extra storage space if there is nothing inventive to be done with it? No new gameplay concepts emerged from the SEGA CD—it just offered more of the same. In fact, with few exceptions like "Sonic CD", it often offered some of the 16-bit generation's worst games, like "Demolition Man"." Jeremy Parish of USgamer pointed out that "Sega was hardly the only company to muddy its waters with a CD add-on in the early '90s" and highlighted some "gems" for the system, but cautioned "the benefits offered by the Sega CD had to be balanced against the fact that the add-on more than doubled the price (and complexity) of the [Genesis]." Writing for "Retro Gamer", Damien McFerran cited various reasons for the Sega CD's limited sales, including the add-on's high price, lack of significant enhancement to the Genesis console, and lack of ability to function without a console attached. "Retro Gamer" writer Aaron Birch, however, defended the Sega CD and wrote that "the single biggest cause of the Mega-CD's failure was the console itself. When the system came out, CD-ROM technology was still in its infancy and companies had yet to get to grips with the possibilities it offered... quite simply, the Mega-CD was a console ahead of its time."

The poor support for the Sega CD has often been criticized as the first link in the devaluation of the Sega brand. Writing for IGN, Buchanan described an outside perspective on Sega's decision to release the Sega CD with its poor library and console support, stating, "[T]he SEGA CD instead looked like a strange, desperate move—something designed to nab some ink but without any real, thought-out strategy. Genesis owners that invested in the add-on were sorely disappointed, which undoubtedly helped sour the non-diehards on the brand." In reviewing for "GamePro", Snow commented that "[the] Sega CD marked the first of several Sega systems that saw very poor support; something that devalued the once-popular Sega brand in the eyes of consumers, and something that would ultimately lead to the company's demise as a hardware maker."

Former Sega of America senior producer Scot Bayless attributes the unsuccessful market to a lack of direction from Sega with the add-on. According to Bayless, "It was a fundamental paradigm shift with almost no thought given to consequences. I honestly don't think anyone at Sega asked the most important question: 'Why?' There's a rule I developed during my time as an engineer in the military aviation business: never fall in love with your tech. I think that's where the Mega-CD went off the rails. The whole company fell in love with the idea without ever really asking how it would affect the games you made." Sega of America producer Michael Latham offers a contrasting view of support for the add-on, however, stating "I loved the Sega CD. I always thought the platform was under-appreciated and that it was hurt by an over-concentration of trying to make Hollywood interactive film games versus using its storage and extended abilities to make just plain great video games." Former Sega of Europe president Nick Alexander commented on the Mega-CD, saying "The Mega CD was interesting but probably misconceived and was seen very much as the interim product it was. I am afraid I cannot recall the sales numbers, but it was not a success."



</doc>
<doc id="29033" url="https://en.wikipedia.org/wiki?curid=29033" title="Sega Pico">
Sega Pico

The Sega Pico, also known as is an educational video game console by Sega Toys. Marketed as "edutainment", the main focus of the Pico was educational video games for children between 3 and 7 years old. The Pico was released in June 1993 in Japan and November 1994 in North America and Europe, later reaching China. It was succeeded by the Advanced Pico Beena, which was released in Japan in 2005. Though the Pico was sold continuously in Japan through the release of the Beena, in North America and Europe the Pico was less successful and was discontinued in early 1998, later being re-released by Majesco Entertainment. Releases for the Pico were focused on education for children and included titles supported by licensed franchised animated characters, including Sega's own "Sonic the Hedgehog" series. Overall, Sega claims sales of 3.4 million Pico consoles and 11.2 million game cartridges, and over 350,000 Beena consoles and 800,000 cartridges.

Powered by the same hardware used in the Sega Genesis, the physical shape of the Pico was designed to appear similar to a laptop. Included in the Pico is a stylus called the "Magic Pen" and a pad to draw on. Controlling the games for the system is accomplished either by using the Magic Pen like a mouse or by pressing the directional buttons on the console. The Pico does not include its screen or RF output, and instead must be connected to a monitor through Composite video or a VCR to be played on an RF screen. Touching the pen to the pad would either allow drawing or animate a character on the screen.

Cartridges for the system were referred to as "Storyware", and take the form of picture books with a cartridge slot on the bottom. The Pico changes the television display and the set of tasks for the player to accomplish each time a page is turned. Sound, including voices and music, also accompanied every page. Games for the Pico focused on education, including on subjects such as music, counting, spelling, reading, matching, and coloring. Titles included licensed animated characters from various franchises, such as "Disney's The Lion King: Adventures at Pride Rock" and "A Year at Pooh Corner". Sega also released titles including their mascot, Sonic the Hedgehog, including "Sonic Gameworld" and "Tails and the Music Maker".

According to former Sega console hardware research and development head Hideki Sato, the development of the Sega Pico was possible due to the company's past work on the MyCard cartridges developed for the SG-1000, as well as on drawing tablets. The sensor technology used in the pad came from that developed for 1987 arcade game "World Derby", while its CPU and graphics chip came from the Genesis.

At the price of JP¥13,440, the Pico was released in Japan in June 1993. In North America, Sega unveiled the Pico at the 1994 American International Toy Fair, showcasing its drawing and display abilities before releasing it in November. The console was advertised at a price of approximately US$160 but was eventually released at a price of US$139. "Storyware" cartridges sold for US$39.99 to US$49.99. The Pico's slogan was: ""The computer that thinks it's a toy."" The Sega Pico won a few awards including the "National Parenting Seal of Approval", a "Platinum Seal Award" and a gold medal for "National Association of Parenting Publications Awards".

After a lack of success, Sega discontinued the Pico in North America in early 1998. Later, a remake of the Pico made by Majesco Entertainment was released in North America in August 1999 at a price of US$49.99, with Storyware selling at $19.99. The Pico would later be released in China in 2002, priced at CN¥690.

In 2000, Sega claimed that the Pico had sold 2.5 million units. As of April 2005, Sega claims that 3.4 million Pico consoles and 11.2 million software cartridges had been sold worldwide. The Pico was recognized in 1995 by being listed on Dr. Toy's 100 Best Products, as well as being listed in "Child" as one of the best computer games available. According to Joseph Szadkowski of "The Washington Times", "Pico has enough power to be a serious learning aid that teaches counting, spelling, matching, problem-solving, memory, logic, hand/eye coordination and important, basic computer skills." Former Sega of America vice president of product development Joe Miller claims that he named his dog after the system because of his passion for the console. By contrast, Steven L. Kent claims that Sega of Japan CEO Hayao Nakayama watched the Pico "utterly fail" in North America. According to Warren Buckleitner of "Children's Software Revenue", the Pico failed in North America due to a lack of credibility in the product.

The Advanced Pico Beena, also known simply as Beena or BeenaLite, is an educational console system targeted at young children sold by Sega Toys, released in 2005 in Japan. It is the successor to the Pico and marketed around the "learn while playing" concept. According to Sega Toys, the focus of the Advanced Pico Beena is on learning in a new social environment and is listed as their upper-end product. Topics listed as being educational focuses for the Beena include intellectual, moral, physical, dietary and safety education. The name of the console was chosen to sound like the first syllables of "Be Natural".

Compared to the Pico, Beena adds several functions. Beena can be played without a television, and supports multiplayer by a separately sold additional Magic Pen. The console also supports data saving. Playtime can be limited by settings in the system. Some games for the Beena also offer adaptive difficulty, which becomes more difficult to play based on the skill level of the player. The Beena Lite, a more affordable version of the console, was released on July 17, 2008. At the time of its release, Sega estimated that 350,000 Beena consoles had been sold, and 800,000 game cartridges. It is technically the last Sega console.




</doc>
<doc id="29034" url="https://en.wikipedia.org/wiki?curid=29034" title="Sega VR">
Sega VR

The Sega VR is a virtual reality headset developed by Sega in the early 1990s. Versions were planned for arcades and consoles (Sega Genesis and then Saturn), but only the arcade version was released, and the home console versions were cancelled.

The Sega VR's design was based on an IDEO virtual reality head-mounted display containing LCD screens in the visor and stereo headphones. Inertial sensors in the headset allow the system to track and react to the movements of the user's head.

Sega, flush with funds from the success of its Sega Genesis (released as the Mega Drive outside of North America), announced the device in 1991. It was later seen in early 1993, at the Winter Consumer Electronics Show (CES), where "Electronic Gaming Monthly" noted it was an adaptation of a similar headset that Sega were already using for arcades. The magazine stated that a Mega Drive/Genesis version was planned for release in late 1993 at and would be released with four launch games, including a port of arcade game "Virtua Racing". Sega later announced its release schedule for early 1994, according to "Electronic Games".

Because of development difficulties, the Sega VR headset remained only a prototype, and was never released to the general public. Then CEO Tom Kalinske stated that the system would not be released due to it inducing motion sickness and severe headaches in users. It was last seen at the 1993 Summer CES, where it was demonstrated by Alan Hunter. It vanished from release schedules in 1994. Four games were apparently developed for the system, each using 16 MB cartridges that were to be bundled with the headset.

The company claimed to have terminated the project because the virtual reality effect was too realistic. Users might move while wearing the headset and injure themselves. The limited processing power of the system makes this claim unlikely, although there were reports of testers developing headaches and motion sickness. Mark Pesce, who worked on the Sega VR project, says SRI International, a research institute, warned Sega of the "hazards of prolonged use".

Only four original games were known to be in development.


In addition, Sega also announced a port of Sega AM2's hit 1992 arcade game "Virtua Racing" as a launch game for the device.

Sega went on to other VR projects for use in arcades and a similar peripheral was reported but never seen for the Sega Saturn. The project encouraged a brief flurry of other companies to offer VR products.

In 1994, Sega VR technology was utilized for the Sega VR-1 motion simulator arcade attraction, which was available at SegaWorld arcades. It is able to track head movement and features 3D polygon graphics in stereoscopic 3D. A scaled-down version, "Dennoo Senki Net Merc", was demonstrated at Japan's 1995 AOU (Amusement Operators Union) show, using the Sega Model 1 arcade system board to produce the 3D graphics. However, the game's flat-shaded graphics were compared unfavorably to the Sega Model 2's textured-filtered graphics.


</doc>
<doc id="29035" url="https://en.wikipedia.org/wiki?curid=29035" title="Sega Saturn">
Sega Saturn

The is a 32-bit fifth-generation home video game console developed by Sega and released on November 22, 1994 in Japan, May 11, 1995 in North America, and July 8, 1995 in Europe. The successor to the successful Sega Genesis, the Saturn has a dual-CPU architecture and eight processors. Its games are in CD-ROM format, and its game library contains several arcade ports as well as original games.

Development of the Saturn began in 1992, the same year Sega's groundbreaking 3D Model 1 arcade hardware debuted. The system was designed around a new CPU from Japanese electronics company Hitachi. Sega added another video display processor in early 1994 to better compete with Sony's forthcoming PlayStation.

The Saturn was initially successful in Japan, but failed to sell in large numbers in the United States after its surprise May 1995 launch, four months before its scheduled release date. After the debut of the Nintendo 64 in late 1996, the Saturn rapidly lost market share in the U.S., where it was discontinued in 1998. Having sold 9.26 million units worldwide, the Saturn is considered a commercial failure. The failure of Sega's development teams to release a game in the "Sonic the Hedgehog" series, known in development as "Sonic X-treme", has been considered a factor in the console's poor performance.

Although the Saturn is remembered for several well-regarded games, including "Nights into Dreams", the "Panzer Dragoon" series, and the "Virtua Fighter" series, its reputation is mixed due to its complex hardware design and limited third-party support. Sega's management has been criticized for its decisions during the system's development and discontinuation.

Released in 1988, the Genesis (known as the Mega Drive in Europe, Japan and Australia) was Sega's entry into the fourth generation of video game consoles. In mid-1990, Sega CEO Hayao Nakayama hired Tom Kalinske as president and CEO of Sega of America. Kalinske developed a four-point plan for sales of the Genesis: lower the price of the console, create a U.S.-based team to develop games targeted at the American market, continue aggressive advertising campaigns, and sell "Sonic the Hedgehog" with the console. The Japanese board of directors initially disapproved of the plan, but all four points were approved by Nakayama, who told Kalinske, "I hired you to make the decisions for Europe and the Americas, so go ahead and do it." Magazines praised "Sonic" as one of the greatest games ever made, and Sega's console finally took off as customers who had been waiting for the Super Nintendo Entertainment System (SNES) decided to purchase a Genesis instead. However, the release of a CD-based add-on for the Genesis, the Sega CD (known as Mega-CD outside of North America), was commercially disappointing.

Sega also experienced success with arcade games. In 1992 and 1993, the new Sega Model 1 arcade system board showcased Sega AM2's "Virtua Racing" and "Virtua Fighter" (the first 3D fighting game), which played a crucial role in popularizing 3D polygonal graphics. In particular, "Virtua Fighter" garnered praise for its simple three-button control scheme, with strategy coming from the intuitively observed differences between characters that felt and acted differently rather than the more ornate combos of two-dimensional competitors. Despite its crude visuals—with characters composed of fewer than 1,200 polygons—"Virtua Fighter"<nowiki>'</nowiki>s fluid animation and relatively realistic depiction of distinct fighting styles gave its combatants a lifelike presence considered impossible to replicate with sprites. The Model 1 was an expensive system board, and bringing home releases of its games to the Genesis required more than its hardware could handle. Several alternatives helped to bring Sega's newest arcade games to the console, such as the Sega Virtua Processor chip used for "Virtua Racing", and eventually the Sega 32X add-on.

Development of the Saturn was supervised by Hideki Sato, Sega's director and deputy general manager of research and development. According to Sega project manager Hideki Okamura, the Saturn project started over two years before the system was showcased at the Tokyo Toy Show in June 1994. The name "Saturn" was the system's codename during development in Japan, but was chosen as the official product name. "Computer Gaming World" in March 1994 reported a rumor that "the Sega "Saturn" ... will release in Japan before the end of the year" for $250–300.

In 1993, Sega and Japanese electronics company Hitachi formed a joint venture to develop a new CPU for the Saturn, which resulted in the creation of the "SuperH RISC Engine" (or SH-2) later that year. The Saturn was designed around a dual-SH2 configuration. According to Kazuhiro Hamada, Sega's section chief for Saturn development during the system's conception, "the SH-2 was chosen for reasons of cost and efficiency. The chip has a calculation system similar to a DSP [digital signal processor], but we realized that a single CPU would not be enough to calculate a 3D world." Although the Saturn's design was largely finished before the end of 1993, reports in early 1994 of the technical capabilities of Sony's upcoming PlayStation console prompted Sega to include another video display processor (VDP) to improve the system's 2D performance and texture-mapping. CD-ROM-based and cartridge-only versions of the Saturn hardware were considered for simultaneous release during the system's development, but this idea was discarded due to concerns over the lower quality and higher price of cartridge-based games.

According to Kalinske, Sega of America "fought against the architecture of Saturn for quite some time". Seeking an alternative graphics chip for the Saturn, Kalinske attempted to broker a deal with Silicon Graphics, but Sega of Japan rejected the proposal. Silicon Graphics subsequently collaborated with Nintendo on the Nintendo 64. Kalinske, Sony Electronic Publishing's Olaf Olafsson, and Sony America's Micky Schulhof had discussed development of a joint "Sega/Sony hardware system", which never came to fruition due to Sega's desire to create hardware that could accommodate both 2D and 3D visuals and Sony's competing notion of focusing on 3D technology. Publicly, Kalinske defended the Saturn's design: "Our people feel that they need the multiprocessing to be able to bring to the home what we're doing next year in the arcades."

In 1993, Sega restructured its internal studios in preparation for the Saturn's launch. To ensure high-quality 3D games would be available early in the Saturn's life, and to create a more energetic working environment, developers from Sega's arcade division were asked to create console games. New teams, such as "Panzer Dragoon" developer Team Andromeda, were formed during this time.

In January 1994, Sega began to develop an add-on for the Genesis, the Sega 32X, which would serve as a less expensive entry into the 32-bit era. The decision to create the add-on was made by Nakayama and widely supported by Sega of America employees. According to former Sega of America producer Scot Bayless, Nakayama was worried that the Saturn would not be available until after 1994 and that the recently released Atari Jaguar would reduce Sega's hardware sales. As a result, Nakayama ordered his engineers to have the system ready for launch by the end of the year. The 32X would not be compatible with the Saturn, but Sega executive Richard Brudvik-Lindner pointed out that the 32X would play Genesis games, and had the same system architecture as the Saturn. This was justified by Sega's statement that both platforms would run at the same time, and that the 32X would be aimed at players who could not afford the more expensive Saturn. According to Sega of America research and development head Joe Miller, the 32X served a role in assisting development teams to familiarize themselves with the dual SH-2 architecture also used in the Saturn. Because both machines shared many of the same parts and were preparing to launch around the same time, tensions emerged between Sega of America and Sega of Japan when the Saturn was given priority.

Sega released the Saturn in Japan on November 22, 1994, at a price of ¥44,800. "Virtua Fighter", a faithful port of the popular arcade game, sold at a nearly one-to-one ratio with the Saturn console at launch and was crucial to the system's early success in Japan. Though Sega had wanted to launch with "Clockwork Knight" and "Panzer Dragoon," the only other first-party game available at launch was "Wan Chai Connection". Fueled by the popularity of "Virtua Fighter", Sega's initial shipment of 200,000 Saturn units sold out on the first day. Sega waited until the December 3 launch of the PlayStation to ship more units; when both were sold side-by-side, the Saturn proved more popular.

Meanwhile, Sega released the 32X on November 21, 1994 in North America, December 3, 1994 in Japan, and January 1995 in PAL territories, and was sold at less than half of the Saturn's launch price. After the holiday season, however, interest in the 32X rapidly declined. 500,000 Saturn units were sold in Japan by the end of 1994 (compared to 300,000 PlayStation units), and sales exceeded 1 million within the following six months. There were conflicting reports that the PlayStation enjoyed a higher sell-through rate, and the system gradually began to overtake the Saturn in sales during 1995. Sony attracted many third-party developers to the PlayStation with a liberal $10 licensing fee, excellent development tools, and the introduction of a 7- to 10-day order system that allowed publishers to meet demand more efficiently than the 10- to 12-week lead times for cartridges that had previously been standard in the Japanese video game industry.

In March 1995, Sega of America CEO Tom Kalinske announced that the Saturn would be released in the U.S. on "Saturnday" (Saturday) September 2, 1995.<ref name="Saturnday/1:1"></ref> However, Sega of Japan mandated an early launch to give the Saturn an advantage over the PlayStation. At the first Electronic Entertainment Expo (E3) in Los Angeles on May 11, 1995, Kalinske gave a keynote presentation in which he revealed the release price of US$399 (including a copy of "Virtua Fighter"), and described the features of the console. Kalinske also revealed that, due to "high consumer demand", Sega had already shipped 30,000 Saturns to Toys "R" Us, Babbage's, Electronics Boutique, and Software Etc. for immediate release. The announcement upset retailers who were not informed of the surprise release, including Best Buy and Walmart; KB Toys responded by dropping Sega from its lineup. Sony subsequently unveiled the retail price for the PlayStation: Olaf Olafsson, the head of Sony Computer Entertainment America (SCEA), summoned Steve Race to the stage, who said "$299", and then walked away to applause. The Saturn's release in Europe also came before the previously announced North American date, on July 8, 1995, at a price of £399.99. European retailers and press did not have time to promote the system or its games, harming sales. The PlayStation launched in Europe on September 29, 1995; by November, it had already outsold the Saturn by a factor of three in the United Kingdom, where Sony had allocated £20 million of marketing during the holiday season compared to Sega's £4 million.

The Saturn's U.S. launch was accompanied by a reported $50 million advertising campaign that included coverage in publications such as "Wired" and "Playboy". Early advertising for the system was targeted at a more mature, adult audience than the Sega Genesis ads. Because of the early launch, the Saturn had only six games (all published by Sega) available to start as most third-party games were slated to be released around the original launch date. "Virtua Fighter"<nowiki>'</nowiki>s relative lack of popularity in the West, combined with a release schedule of only two games between the surprise launch and September 1995, prevented Sega from capitalizing on the Saturn's early timing. Within two days of its September 9, 1995 launch in North America, the PlayStation (backed by a large marketing campaign) sold more units than the Saturn had in the five months following its surprise launch, with almost all of the initial shipment of 100,000 units being sold in advance, and the rest selling out across the U.S.

A high-quality port of the Namco arcade game "Ridge Racer" contributed to the PlayStation's early success, and garnered favorable media in comparison to the Saturn version of Sega's "Daytona USA", which was considered inferior to its arcade counterpart. Namco, a longtime arcade competitor with Sega, also unveiled the Namco System 11 arcade board, based on raw PlayStation hardware. Although the System 11 was technically inferior to Sega's Model 2 arcade board, its lower price made it attractive to smaller arcades. Following a 1994 acquisition of Sega developers, Namco released "Tekken" for the System 11 and PlayStation. Directed by former "Virtua Fighter" designer Seiichi Ishii, "Tekken" was intended to be fundamentally similar, with the addition of detailed textures and twice the frame rate. "Tekken" surpassed "Virtua Fighter" in popularity due to its superior graphics and nearly arcade-perfect console port, becoming the first million-selling PlayStation game.

On October 2, 1995, Sega announced a Saturn price reduction to $299. High-quality Saturn ports of the Sega Model 2 arcade hits "Sega Rally Championship", "Virtua Cop", and "Virtua Fighter 2" (running at 60 frames per second at a high resolution) were available by the end of the year, and were generally regarded as superior to competitors on the PlayStation. Notwithstanding a subsequent increase in Saturn sales during the 1995 holiday season, the games were not enough to reverse the PlayStation's decisive lead. By 1996, the PlayStation had a considerably larger library than the Saturn, although Sega hoped to generate interest with upcoming exclusives such as "Nights into Dreams". An informal survey of retailers showed that the Saturn and PlayStation sold in roughly equal numbers during the first quarter of 1996. Within its first year, the PlayStation secured over 20% of the entire U.S. video game market. On the first day of the May 1996 E3 show, Sony announced a PlayStation price reduction to $199, a reaction to the release of the Model 2 Saturn in Japan at a price roughly equivalent to $199. On the second day, Sega announced it would match this price, though Saturn hardware was more expensive to manufacture.

Despite the launch of the PlayStation and Saturn, sales of 16-bit games and consoles continued to account for 64% of the video game market in 1995. Sega underestimated the continued popularity of the Genesis, and did not have the inventory to meet demand. Sega was able to capture 43% of the dollar share of the U.S. video game market and sell more than 2 million Genesis units in 1995, but Kalinske estimated that "we could have sold another 300,000 Genesis systems in the November/December timeframe." Nakayama's decision to focus on the Saturn over the Genesis, based on the systems' relative performance in Japan, has been cited as the major contributing factor in this miscalculation.

Due to long-standing disagreements with Sega of Japan, Kalinske lost most of his interest in his work as CEO of Sega of America. By the spring of 1996, rumors were circulating that Kalinske planned to leave Sega, and a July 13 article in the press reported speculation that Sega of Japan was planning significant changes to Sega of America's management team. On July 16, 1996, Sega announced that Shoichiro Irimajiri had been appointed chairman and CEO of Sega of America, while Kalinske would be leaving Sega after September 30 of that year. A former Honda executive, Irimajiri had been actively involved with Sega of America since joining Sega in 1993. Sega also announced that David Rosen and Nakayama had resigned from their positions as chairman and co-chairman of Sega of America, though both men remained with the company. Bernie Stolar, a former executive at Sony Computer Entertainment of America, was named Sega of America's executive vice president in charge of product development and third-party relations. Stolar, who had arranged a six-month PlayStation exclusivity deal for "Mortal Kombat 3" and helped build close relations with Electronic Arts while at Sony, was perceived as a major asset by Sega officials. Finally, Sega of America made plans to expand its PC software business.

Stolar was not supportive of the Saturn due to his belief that the hardware was poorly designed, and publicly announced at E3 1997 that "The Saturn is not our future." While Stolar had "no interest in lying to people" about the Saturn's prospects, he continued to emphasize quality games for the system, and subsequently reflected that "we tried to wind it down as cleanly as we could for the consumer." At Sony, Stolar opposed the localization of certain Japanese PlayStation games that he felt would not represent the system well in North America, and advocated a similar policy for the Saturn during his time at Sega, although he later sought to distance himself from this perception. These changes were accompanied by a softer image that Sega was beginning to portray in its advertising, including removing the "Sega!" scream and holding press events for the education industry. Marketing for the Saturn in Japan also changed with the introduction of "Segata Sanshiro" (played by Hiroshi Fujioka) as a character in a series of TV advertisements starting in 1997; the character would eventually star in a Saturn video game.

Temporarily abandoning arcade development, Sega AM2 head Yu Suzuki began developing several Saturn-exclusive games, including a role-playing game in the "Virtua Fighter" series. Initially conceived as an obscure prototype "The Old Man and the Peach Tree" and intended to address the flaws of contemporary Japanese RPGs (such as poor non-player character artificial intelligence routines), "Virtua Fighter RPG" evolved into a planned 11-part, 45-hour "revenge epic in the tradition of Chinese cinema", which Suzuki hoped would become the Saturn's killer app. The game was eventually released as "Shenmue" for the Saturn's successor, the Dreamcast.

As Sonic Team was working on "Nights into Dreams", Sega tasked the U.S.-based Sega Technical Institute (STI) with developing what would have been the first fully 3D entry in its popular "Sonic the Hedgehog" series. The game, "Sonic X-treme", was moved to the Saturn after several prototypes for other hardware (including the 32X) were discarded. It featured a fisheye lens camera system that rotated levels with Sonic's movement. After Nakayama ordered the game be reworked around the engine created for its boss battles, the developers were forced to work between 16 and 20 hours a day to meet their December 1996 deadline. Weeks of development time proved fruitless after Stolar rescinded STI's access to Sonic Team's "Nights into Dreams" engine following an ultimatum by "Nights" programmer Yuji Naka. After programmer Ofer Alon quit and designers Chris Senn and Chris Coffin became ill, the project was cancelled in early 1997. Sonic Team started work on an original 3D "Sonic" game for the Saturn, but development was shifted to the Dreamcast and the game became "Sonic Adventure". STI was disbanded in 1996 as a result of changes in management at Sega of America.

Journalists and fans have speculated about the impact a completed "X-treme" might have had on the market. David Houghton of GamesRadar described the prospect of "a good 3D "Sonic" game" on the Saturn as "a 'What if...' situation on a par with the dinosaurs not becoming extinct." IGN's Travis Fahs called "X-treme" "the turning point not only for Sega's mascot and their 32-bit console, but for the entire company", but noted that the game served as "an empty vessel for Sega's ambitions and the hopes of their fans". Dave Zdyrko, who operated a prominent Saturn fan website during the system's lifespan, said: "I don't know if ["X-treme"] could've saved the Saturn, but ... "Sonic" helped make the Genesis and it made absolutely no sense why there wasn't a great new "Sonic" title ready at or near the launch of the [Saturn]". In a 2007 retrospective, producer Mike Wallis maintained that "X-treme" "definitely would have been competitive" with Nintendo's "Super Mario 64". "Next Generation" reported in late 1996 that "X-treme" would have harmed Sega's reputation if it did not compare well to contemporary competition. Naka said he had been relieved by the cancellation, feeling that the game was not promising.

From 1993 to early 1996, although Sega's revenue declined as part of an industry-wide slowdown, the company retained control of 38% of the U.S. video game market (compared to Nintendo's 30% and Sony's 24%). 800,000 PlayStation units were sold in the U.S. by the end of 1995, compared to 400,000 Saturn units. In part due to an aggressive price war, the PlayStation outsold the Saturn by two-to-one in 1996, while Sega's 16-bit sales declined markedly. By the end of 1996, the PlayStation had sold 2.9 million units in the U.S., more than twice the 1.2 million Saturn units sold. The Christmas 1996 "Three Free" pack, which bundled the Saturn with "Daytona USA", "Virtua Fighter 2", and "Virtua Cop," drove sales dramatically and ensured the Saturn remained a competitor into 1997.

However, the Saturn failed to take the lead. After the launch of the Nintendo 64 in 1996, sales of the Saturn and its games were sharply reduced, while the PlayStation outsold the Saturn by three-to-one in the U.S. in 1997. The 1997 release of "Final Fantasy VII" significantly increased the PlayStation's popularity in Japan. As of August 1997, Sony controlled 47% of the console market, Nintendo 40%, and Sega only 12%. Neither price cuts nor high-profile game releases proved helpful. Reflecting decreased demand for the system, worldwide Saturn shipments during March to September 1997 declined from 2.35 million to 600,000 versus the same period in 1996; shipments in North America declined from 800,000 to 50,000. Due to the Saturn's poor performance in North America, 60 of Sega of America's 200 employees were laid off in the fall of 1997.

As a result of Sega's deteriorating financial situation, Nakayama resigned as president in January 1998 in favor of Irimajiri. Stolar subsequently acceded to president of Sega of America. Following five years of generally declining profits, in the fiscal year ending March 31, 1998 Sega suffered its first parent and consolidated financial losses since its 1988 listing on the Tokyo Stock Exchange. Due to a 54.8% decline in consumer product sales (including a 75.4% decline overseas), the company reported a net loss of ¥43.3 billion (US$327.8 million) and a consolidated net loss of ¥35.6 billion (US$269.8 million).

Shortly before announcing its financial losses, Sega announced that it was discontinuing the Saturn in North America to prepare for the launch of its successor. Only 12 Saturn games were released in North America in 1998 ("Magic Knight Rayearth" was the final official release), compared to 119 in 1996. The Saturn would last longer in Japan and Europe. Rumors about the upcoming Dreamcast—spread mainly by Sega itself—were leaked to the public before the last Saturn games were released. The Dreamcast was released on November 27, 1998 in Japan and on September 9, 1999 in North America. The decision to abandon the Saturn effectively left the Western market without Sega games for over one year. Sega suffered an additional ¥42.881 billion consolidated net loss in the fiscal year ending March 1999, and announced plans to eliminate 1,000 jobs, nearly a quarter of its workforce.

Worldwide Saturn sales include at least the following amounts in each territory: 5.75 million in Japan (surpassing the Genesis' sales of 3.58 million there), 1.8 million in the United States, 1 million in Europe, and 530,000 elsewhere. With lifetime sales of 9.26 million units, the Saturn is considered a commercial failure, although its install base in Japan surpassed the Nintendo 64's 5.54 million. Lack of distribution has been cited as a significant factor contributing to the Saturn's failure, as the system's surprise launch damaged Sega's reputation with key retailers. Conversely, Nintendo's long delay in releasing a 3D console and damage caused to Sega's reputation by poorly supported add-ons for the Genesis are considered major factors allowing Sony to gain a foothold in the market.

Featuring a total of eight processors the Saturn's main central processing units are two Hitachi SH-2 microprocessors clocked at 28.6 MHz and capable of 56 MIPS. It contains a Motorola 68EC000 running at 11.3 MHz as a sound controller; a custom sound processor with an integrated Yamaha FH1 DSP running at 22.6 MHz capable of up to 32 sound channels with both FM synthesis and 16-bit PCM sampling at a maximum rate of 44.1 kHz; and two video display processors, the VDP1 (which handles sprites, textures and polygons) and the VDP2 (which handles backgrounds). Its double-speed CD-ROM drive is controlled by a dedicated Hitachi SH-1 processor to reduce load times. The System Control Unit (SCU), which controls all buses and functions as a co-processor of the main SH-2 CPU, has an internal DSP running at 14.3 MHz. The Saturn contains a cartridge slot for memory expansion, 16 Mbit of work random-access memory (RAM), 12 Mbit of video RAM, 4 Mbit of RAM for sound functions, 4 Mbit of CD buffer RAM and 256 Kbit (32 KB) of battery backup RAM. Its video output, provided by a stereo AV cable, displays at resolutions from 320×224 to 704×224 pixels, and can display up to 16.77 million colors simultaneously. The Saturn measures . It was sold packaged with an instruction manual, one control pad, a stereo AV cable, and its 100 V AC power supply, with a power consumption of approximately 15W.

The Saturn had technically impressive hardware at the time of its release, but its complexity made harnessing this power difficult for developers accustomed to conventional programming. The greatest disadvantage was that both CPUs shared the same bus and were unable to access system memory at the same time. Making full use of the 4 kB of cache memory in each CPU was critical to maintaining performance. For example, "Virtua Fighter" used one CPU for each character, while "Nights" used one CPU for 3D environments and the other for 2D objects. The Visual Display Processor 2 (VDP2), which can generate and manipulate backgrounds, has also been cited as one of the system's most important features.

The Saturn's design elicited mixed commentary among game developers and journalists. Developers quoted by "Next Generation" in December 1995 described the Saturn as "a real coder's machine" for "those who love to get their teeth into assembly and really hack the hardware", with "more flexibility" and "more calculating power than the PlayStation". The sound board was also widely praised. By contrast, Lobotomy Software programmer Ezra Dreisbach described the Saturn as significantly slower than the PlayStation, whereas Kenji Eno of WARP observed little difference. In particular, Dreisbach criticized the Saturn's use of quadrilaterals as its basic geometric primitive, in contrast to the triangles rendered by the PlayStation and the Nintendo 64. Ken Humphries of Time Warner Interactive remarked that compared to the PlayStation, the Saturn was worse at generating polygons but better at sprites. Third-party development was initially hindered by the lack of useful software libraries and development tools, requiring developers to write in assembly language. During early Saturn development, programming in assembly could offer a two-to-fivefold speed increase over higher-level languages such as C. 

The Saturn hardware is extremely difficult to emulate. Sega responded to complaints about the difficulty of programming for the Saturn by writing new graphics libraries which were claimed to make development easier. Sega of America also purchased a United Kingdom-based development firm, Cross Products, to produce the Saturn's development system. Despite these challenges, Treasure CEO Masato Maegawa stated that the Nintendo 64 was more difficult to develop for than the Saturn. Traveller's Tales' Jon Burton felt that while the PlayStation was easier "to get started on ... you quickly reach [its] limits", whereas the Saturn's "complicated" hardware had the ability to "improve the speed and look of a game when all used together correctly". A major criticism was the Saturn's use of 2D sprites to generate polygons and simulate 3D space. The PlayStation functioned similarly, but also featured a dedicated "Geometry Transfer Engine" that rendered additional polygons. As a result, several analysts described the Saturn as an "essentially" 2D system.

Several Saturn models were produced in Japan. An updated model in a recolored light gray (officially white) was released at ¥20,000 to reduce the system's cost and raise its appeal among women and younger children. Two models were released by third parties: Hitachi released the Hi-Saturn (a smaller model equipped with a car navigation function), while JVC released the V-Saturn. Saturn controllers came in various color schemes to match different models of the console. The system also supports several accessories. A wireless controller powered by AA batteries uses infrared signal to connect. Designed to work with "Nights", the Saturn 3D Pad includes both a control pad and an analog stick for directional input. Sega also released several versions of arcade sticks as peripherals, including the Virtua Stick, the Virtua Stick Pro, the Mission Analog Stick, and the Twin Stick. Sega also created a light gun peripheral, the Virtua Gun, for shooting games such as "Virtua Cop" and "The Guardian", and the Arcade Racer, a wheel for racing games. The Play Cable allows two Saturn consoles to be connected for multiplayer gaming across two screens, while a multitap allows up to six players to play on the same console. The Saturn was designed to support up to 12 players on a single console, by using two multitaps. RAM cartridges expand the memory. Other accessories include a keyboard, mouse, floppy disk drive, and movie card.

Like the Genesis, the Saturn had an internet-based gaming service. The Sega NetLink was a 28.8k modem that fit into the cartridge slot in the Saturn for direct dial multiplayer. In Japan, a pay-to-play service was used. It could also be used for web browsing, sending email, and online chat. Because the NetLink was released before the Saturn keyboard, Sega produced a series of CDs containing hundreds of website addresses so that Saturn owners could browse with the joypad. The NetLink functioned with "Daytona USA", "Duke Nukem 3D", "Saturn Bomberman", "Sega Rally", and "". In 1995, Sega announced a variant of the Saturn featuring a built-in NetLink modem under the code name "Sega Pluto", but it was never released.

Sega developed an arcade board based on the Saturn's hardware, the Sega ST-V (or Titan), intended as an affordable alternative to Sega's Model 2 arcade board and as a testing ground for upcoming Saturn software. The Titan was criticized for its comparatively weak performance by Sega AM2's Yu Suzuki and was overproduced by Sega's arcade division. Because Sega already had the "Die Hard" license, members of Sega AM1 working at the Sega Technical Institute developed "Die Hard Arcade" for the Titan to clear excess inventory. "Die Hard" became the most successful Sega arcade game produced in the United States at that point. Other games released for the Titan include "" and "Virtua Fighter Kids".

Much of the Saturn's library comes from Sega's arcade ports, including "Daytona USA", "The House of the Dead", "Last Bronx", "Sega Rally Championship", the "Virtua Cop" series, the "Virtua Fighter" series, and "Virtual-On". Saturn ports of 2D Capcom fighting games including "Darkstalkers 3", "Marvel Super Heroes vs. Street Fighter", and "Street Fighter Alpha 3" were noted for their faithfulness to their arcade counterparts. "Fighters Megamix", developed by Sega AM2 for the Saturn rather than arcades, combined characters from "Fighting Vipers" and "Virtua Fighter" to positive reviews. Highly rated Saturn exclusives include "Panzer Dragoon Saga", "Dragon Force", "Guardian Heroes", "Nights", "Panzer Dragoon II Zwei", and "Shining Force III". PlayStation games such as "", "Resident Evil", and "Wipeout 2097" received Saturn ports with mixed results. Lobotomy Software's "PowerSlave" featured some of the most impressive 3D graphics on the system, leading Sega to contract them to produce Saturn ports of "Duke Nukem 3D" and "Quake". While Electronic Arts' limited support for the Saturn and Sega's failure to develop a football game for the 1996 fall season gave Sony the lead in the sports genre, "Sega Sports" published Saturn sports games including the well-regarded "World Series Baseball" and "Sega Worldwide Soccer" series. With about 600 official releases, the Saturn's library is nearly twice as large as the Nintendo 64's.

Due to the cancellation of "Sonic X-treme", the Saturn lacks an exclusive "Sonic the Hedgehog" platformer; instead it received a graphically enhanced port of the Genesis game "Sonic 3D Blast", the compilation "Sonic Jam", and a racing game, "Sonic R". The platformer "Bug!" received attention for its eponymous main character being a potential mascot for the Saturn, but it failed to catch on as the "Sonic" series had. Considered one of the most important Saturn releases, Sonic Team developed "Nights into Dreams", a score attack game that attempted to simulate both the joy of flying and the fleeting sensation of dreams. The gameplay of "Nights" involves steering the imp-like androgynous protagonist, Nights, as it flies on a mostly 2D plane across surreal stages broken into four segments each. The levels repeat for as long as an in-game time limit allows, while flying over or looping around various objects in rapid succession earns additional points. Although it lacked the fully 3D environments of Nintendo's "Super Mario 64", "Nights"<nowiki>'</nowiki> emphasis on unfettered movement and graceful acrobatic techniques showcased the intuitive potential of analog control. Sonic Team's "Burning Rangers", a fully 3D action-adventure game involving a team of outer-space firefighters, garnered praise for its transparency effects and distinctive art direction, but was released in limited quantities late in the Saturn's lifespan and criticized for its short length.

Some of the games that made the Saturn popular in Japan, such as "Grandia" and the "Sakura Wars" series, never saw a Western release due to Sega of America's policy of not localizing RPGs and other Japanese games that might have damaged the system's reputation in North America. Despite appearing first on the Saturn, games such as "Dead or Alive", "Grandia", and "" only saw a Western release on the PlayStation. Working Designs localized several Japanese Saturn games before a public feud between Sega of America's Bernie Stolar and Working Designs president Victor Ireland resulted in the company switching their support to the PlayStation. "Panzer Dragoon Saga" was praised as perhaps the finest RPG for the system due to its cinematic presentation, evocative plot, and unique battle system—with a tactical emphasis on circling around opponents to identify weak points and the ability to "morph" the physical attributes of the protagonist's dragon companion during combat—but Sega released fewer than 20,000 retail copies of the game in North America in what IGN's Levi Buchanan characterized as one example of the Saturn's "ignominious send-off" in the region. Similarly, only the first of three installments of "Shining Force III" was released outside Japan. The Saturn's library also garnered criticism for its lack of sequels to high-profile Genesis-era Sega franchises, with Sega of Japan's cancellation of a planned third installment in Sega of America's popular "Eternal Champions" series cited as a significant source of controversy.

Later ports of Saturn games including "Guardian Heroes", "Nights", and "" continued to garner positive reviews. Partly due to rarity, Saturn games such as "Panzer Dragoon Saga" and "Radiant Silvergun" have been noted for their cult following. Due to the system's commercial failure and hardware limitations, planned Saturn releases such as "Resident Evil 2", "Shenmue", "Sonic Adventure", and "Virtua Fighter 3" were cancelled and moved to the Dreamcast.

At the time of the Saturn's release, "Famicom Tsūshin" awarded it 24 out of 40, higher than the PlayStation's 19 out of 40. In June 1995, Dennis Lynch of the "Chicago Tribune" and Albert Kim of "Entertainment Weekly" praised the Saturn as the most advanced console available; Lynch praised the double-speed CD-ROM drive and "intense surround-sound capabilities" and Kim cited "Panzer Dragoon" as a "lyrical and exhilarating epic" demonstrating the ability of new technology to "transform" the industry. In December 1995, "Next Generation" gave the Saturn three and a half stars out of five, highlighting Sega's marketing and arcade background as strengths but the system's complexity as a weakness. Four critics in "Electronic Gaming Monthly"<nowiki>'</nowiki>s December 1996 Buyer's Guide rated the Saturn 8, 6, 7, and 8 out of 10 and the PlayStation 9, 10, 9, and 9. By December 1998, "EGM"<nowiki>'</nowiki>s reviews were more mixed, with reviewers citing the lack of games as a major problem. According to "EGM" reviewer Crispin Boyer, "the Saturn is the only system that can thrill me one month and totally disappoint me the next".

Retrospective feedback of the Saturn is mixed, but generally praises its game library. According to Greg Sewart of 1UP.com, "the Saturn will go down in history as one of the most troubled, and greatest, systems of all time". In 2009, IGN named the Saturn the 18th best console of all time, praising its unique game library. According to the reviewers, "While the Saturn ended up losing the popularity contest to both Sony and Nintendo ... "Nights into Dreams", the "Virtua Fighter" and "Panzer Dragoon" series are all examples of exclusive titles that made the console a fan favorite." "Edge" noted "hardened loyalists continue to reminisce about the console that brought forth games like "Burning Rangers", "Guardian Heroes", "Dragon Force" and "Panzer Dragoon Saga"." In 2015, "The Guardian"<nowiki>'</nowiki>s Keith Stuart wrote that "the Saturn has perhaps the strongest line up of 2D shooters and fighting games in console history".

"Retro Gamer"<nowiki>'</nowiki>s Damien McFerran wrote: "Even today, despite the widespread availability of sequels and re-releases on other formats, the Sega Saturn is still a worthwhile investment for those who appreciate the unique gameplay styles of the companies that supported it." IGN's Adam Redsell wrote "[Sega's] devil-may-care attitude towards game development in the Saturn and Dreamcast eras is something that we simply do not see outside of the indie scene today." Necrosoft Games director Brandon Sheffield felt that "the Saturn was a landing point for games that were too 'adult' in content for other systems, as it was the only one that allowed an 18+ rating for content in Japan ... some games, like "Enemy Zero" used it to take body horror to new levels, an important step toward the expansion of games and who they served." Sewart praised the Saturn's first-party games as "Sega's shining moment as a game developer", with Sonic Team demonstrating its creative range and AM2 producing numerous technically impressive arcade ports. He also commented on the many Japan-exclusive Saturn releases, which he connected with a subsequent boom in the game import market. IGN's Travis Fahs was critical of the Saturn library's lack of "fresh ideas" and "precious few high-profile franchises", in contrast to what he described as Sega's more creative Dreamcast output.

Sega has been criticized for its management of the Saturn. McFerran felt its management staff had "fallen out of touch with both the demands of the market and the industry". Stolar has also been criticized; according to Fahs, "Stolar's decision to abandon the Saturn made him a villain to many Sega fans, but ... it was better to regroup than to enter the next fight battered and bruised. Dreamcast would be Stolar's redemption." Stolar defended his decision, saying, "I felt Saturn was hurting the company more than helping it. That was a battle that we weren't going to win." Sheffield said that the Saturn's quadrilaterals undermined third-party support, but because "nVidia invested in quads" at the same time, there had been "a remote possibility" they could have "become the standard instead of triangles ... if somehow, magically, the Saturn were the most popular console of that era." Speaking more positively, former Working Designs president Victor Ireland described the Saturn as "the start of the future of console gaming" because it "got the better developers thinking and designing with parallel-processing architecture in mind for the first time". In GamesRadar, Justin Towell wrote that the Saturn's 3D Pad "set the template for every successful controller that followed, with analog shoulder triggers and left thumbstick ... I don't see any three-pronged controllers around the office these days."

Douglass C. Perry of Gamasutra noted that, from its surprise launch to its ultimate failure, the Saturn "soured many gamers on Sega products". Sewart and IGN's Levi Buchanan cited the failure of the Saturn as the major reason for Sega's downfall as a hardware manufacturer, but USgamer's Jeremy Parish described it as "more a symptom ... than a cause" of the decline, which began with add-ons for the Genesis that fragmented the market and continued with Sega of America's and Sega of Japan's competing designs for the Dreamcast. Sheffield portrayed Sega's mistakes with the Saturn as emblematic of the broader decline of the Japanese gaming industry: "They thought they were invincible, and that structure and hierarchy were necessary for their survival, but more flexibility, and a greater participation with the West could have saved them." According to Stuart, Sega "didn't see ... the roots of a prevailing trend, away from arcade conversions and traditional role-playing adventures and toward a much wider console development community with fresh ideas about gameplay and structure." Pulp365 reviews editor Matt Paprocki concluded that "the Saturn is a relic, but an important one, which represents the harshness of progress and what it can leave in its wake".



</doc>
