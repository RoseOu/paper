<doc id="25400" url="https://en.wikipedia.org/wiki?curid=25400" title="Rational choice theory">
Rational choice theory

Rational choice theory, also known as choice theory or rational action theory, is a framework for understanding and often formally modeling social and economic behavior. The basic premise of rational choice theory is that aggregate social behavior results from the behavior of individual actors, each of whom is making their individual decisions. The theory also focuses on the determinants of the individual choices (methodological individualism). Rational choice theory then assumes that an individual has preferences among the available choice alternatives that allow them to state which option they prefer. These preferences are assumed to be complete (the person can always say which of two alternatives they consider preferable or that neither is preferred to the other) and transitive (if option A is preferred over option B and option B is preferred over option C, then A is preferred over C). The rational agent is assumed to take account of available information, probabilities of events, and potential costs and benefits in determining preferences, and to act consistently in choosing the self-determined best choice of action.In simpler terms, this theory dictates that every person, even when carrying out the most mundane of tasks, perform their own personal cost and benefit analysis in order to determine whether the action is worth perusing for the best possible outcome. And following this, a person will choose the optimum venture in every case. This could culminate in a student deciding on whether to attend a lecture or stay in bed, a shopper deciding to provide their own bag to avoid the five pence charge or even a voter deciding which candidate or party based on who will fulfil their needs the best on issues that have an impact themselves especially. 

Rationality is widely used as an assumption of the behavior of individuals in microeconomic models and analyses and appears in almost all economics textbook treatments of human decision-making. It is also used in political science, sociology, and philosophy. Gary Becker was an early proponent of applying rational actor models more widely. Becker won the 1992 Nobel Memorial Prize in Economic Sciences for his studies of discrimination, crime, and human capital.

A particular version of rationality is instrumental rationality, which involves seeking the most cost-effective means to achieve a specific goal without reflecting on the worthiness of that goal. 

Rational choice theorists do not claim that the theory describes the choice "process", but rather that it predicts the outcome and pattern of choices. 
An assumption often added to the rational choice paradigm is that individual preferences are self-interested, in which case the individual can be referred to as a homo economicus. Such an individual acts "as if" balancing costs against benefits to arrive at action that maximizes personal advantage. Proponents of such models, particularly those associated with the Chicago school of economics, do not claim that a model's assumptions are an accurate description of reality, only that they help formulate clear and falsifiable hypotheses. In this view, the only way to judge the success of a hypothesis is empirical tests. To use an example from Milton Friedman, if a theory that says that the behavior of the leaves of a tree is explained by their rationality passes the empirical test, it is seen as successful.

Without specifying the individual's goal or preferences it may not be possible to empirically test, or falsify, the rationality assumption. However, the predictions made by a specific version of the theory are testable. In recent years, the most prevalent version of rational choice theory, expected utility theory, has been challenged by the experimental results of behavioral economics. Economists are learning from other fields, such as psychology, and are enriching their theories of choice in order to get a more accurate view of human decision-making. For example, the behavioral economist and experimental psychologist Daniel Kahneman won the Nobel Memorial Prize in Economic Sciences in 2002 for his work in this field.

Rational choice theory has become increasingly employed in social sciences other than economics, such as sociology, evolutionary theory and political science in recent decades. It has had far-reaching impacts on the study of political science, especially in fields like the study of interest groups, elections, behaviour in legislatures, coalitions, and bureaucracy. In these fields, the use of the rational choice paradigm to explain broad social phenomena is the subject of active controversy.

The concept of rationality used in rational choice theory is different from the colloquial and most philosophical use of the word. Colloquially, "rational" behaviour typically means "sensible", "predictable", or "in a thoughtful, clear-headed manner." Rational choice theory uses a narrower definition of rationality. At its most basic level, behavior is rational if it is goal-oriented, reflective (evaluative), and consistent (across time and different choice situations). This contrasts with behavior that is random, impulsive, conditioned, or adopted by (unevaluative) imitation.

Early neoclassical economists writing about rational choice, including William Stanley Jevons, assumed that agents make consumption choices so as to maximize their happiness, or utility. Contemporary theory bases rational choice on a set of choice axioms that need to be satisfied, and typically does not specify where the goal (preferences, desires) comes from. It mandates just a consistent ranking of the alternatives. Individuals choose the best action according to their personal preferences and the constraints facing them. E.g., there is nothing irrational in preferring fish to meat the first time, but there is something irrational in preferring fish to meat in one instant and preferring meat to fish in another, without anything else having changed.

The premise of rational choice theory as a social science methodology is that the aggregate behavior in society reflects the sum of the choices made by individuals. Each individual, in turn, makes their choice based on their own preferences and the constraints (or choice set) they face.

At the individual level, rational choice theory stipulates that the agent chooses the action (or outcome) they most prefer. In the case where actions (or outcomes) can be evaluated in terms of costs and benefits, a rational individual chooses the action (or outcome) that provides the maximum net benefit, i.e., the maximum benefit minus cost.

The theory applies to more general settings than those identified by costs and benefit. In general, rational decision making entails choosing among all available alternatives the alternative that the individual most prefers. The "alternatives" can be a set of actions ("what to do?") or a set of objects ("what to choose/buy"). In the case of actions, what the individual really cares about are the outcomes that results from each possible action. Actions, in this case, are only an instrument for obtaining a particular outcome.

The available alternatives are often expressed as a set of objects, for example a set of "j" exhaustive and exclusive actions:
For example, if a person can choose to vote for either Roger or Sara or to abstain, their set of possible alternatives is:

The theory makes two technical assumptions about individuals' preferences over alternatives:

Together these two assumptions imply that given a set of exhaustive and exclusive actions to choose from, an individual can "rank" the elements of this set in terms of his preferences in an internally consistent way (the ranking constitutes a partial ordering), and the set has at least one maximal element.

The preference between two alternatives can be:

Research that took off in the 1980s sought to develop models which drop these assumptions and argue that such behaviour could still be rational, Anand (1993). This work, often conducted by economic theorists and analytical philosophers, suggests ultimately that the assumptions or axioms above are not completely general and might at best be regarded as approximations.


Alternative theories of human action include such components as Amos Tversky and Daniel Kahneman's prospect theory, which reflects the empirical finding that, contrary to standard preferences assumed under neoclassical economics, individuals attach extra value to items that they already own compared to similar items owned by others. Under standard preferences, the amount that an individual is willing to pay for an item (such as a drinking mug) is assumed to equal the amount he or she is willing to be paid in order to part with it. In experiments, the latter price is sometimes significantly higher than the former (but see Plott and Zeiler 2005, Plott and Zeiler 2007 and Klass and Zeiler, 2013). Tversky and Kahneman do not characterize loss aversion as irrational. Behavioral economics includes a large number of other amendments to its picture of human behavior that go against neoclassical assumptions.

Often preferences are described by their utility function or "payoff function". This is an ordinal number that an individual assigns over the available actions, such as:
The individual's preferences are then expressed as the relation between these ordinal assignments. For example, if an individual prefers the candidate Sara over Roger over abstaining, their preferences would have the relation:
A preference relation that as above satisfies completeness, transitivity, and, in addition, continuity, can be equivalently represented by a utility function.

Both the assumptions and the behavioral predictions of rational choice theory have sparked criticism from various camps. As mentioned above, some economists have developed models of bounded rationality, which hope to be more psychologically plausible without completely abandoning the idea that reason underlies decision-making processes. Other economists have developed more theories of human decision-making that allow for the roles of uncertainty, institutions, and determination of individual tastes by their socioeconomic environment (cf. Fernandez-Huerga, 2008).

Martin Hollis and Edward J. Nell's 1975 book offers both a philosophical critique of neo-classical economics and an innovation in the field of economic methodology. Further they outlined an alternative vision to neo-classicism based on a rationalist theory of knowledge. Within neo-classicism, the authors addressed consumer behaviour (in the form of indifference curves and simple versions of revealed preference theory) and marginalist producer behaviour in both product and factor markets. Both are based on rational optimizing behaviour. They consider imperfect as well as perfect markets since neo-classical thinking embraces many market varieties and disposes of a whole system for their classification. However, the authors believe that the issues arising from basic maximizing models have extensive implications for econometric methodology (Hollis and Nell, 1975, p. 2). In particular it is this class of models – rational behavior as maximizing behaviour – which provide support for specification and identification. And this, they argue, is where the flaw is to be found. Hollis and Nell (1975) argued that positivism (broadly conceived) has provided neo-classicism with important support, which they then show to be unfounded. They base their critique of neo-classicism not only on their critique of positivism but also on the alternative they propose, rationalism. Indeed, they argue that rationality is central to neo-classical economics – as rational choice – and that this conception of rationality is misused. Demands are made of it that it cannot fulfill.

In their 1994 work, "Pathologies of Rational Choice Theory", Donald P. Green and Ian Shapiro argue that the empirical outputs of rational choice theory have been limited. They contend that much of the applicable literature, at least in political science, was done with weak statistical methods and that when corrected many of the empirical outcomes no longer hold. When taken in this perspective, rational choice theory has provided very little to the overall understanding of political interaction - and is an amount certainly disproportionately weak relative to its appearance in the literature. Yet, they concede that cutting edge research, by scholars well-versed in the general scholarship of their fields (such as work on the U.S. Congress by Keith Krehbiel, Gary Cox, and Mat McCubbins) has generated valuable scientific progress.

Duncan K. Foley (2003, p. 1) has also provided an important criticism of the concept of "rationality" and its role in economics. He argued that“Rationality” has played a central role in shaping and establishing the hegemony of contemporary mainstream economics. As the specific claims of robust neoclassicism fade into the history of economic thought, an orientation toward situating explanations of economic phenomena in relation to rationality has increasingly become the touchstone by which mainstream economists identify themselves and recognize each other. This is not so much a question of adherence to any particular conception of rationality, but of taking rationality of individual behavior as the unquestioned starting point of economic analysis.

Foley (2003, p. 9) went on to argue thatThe concept of rationality, to use Hegelian language, represents the relations of modern capitalist society one-sidedly. The burden of rational-actor theory is the assertion that ‘naturally’ constituted individuals facing existential conflicts over scarce resources would rationally impose on themselves the institutional structures of modern capitalist society, or something approximating them. But this way of looking at matters systematically neglects the ways in which modern capitalist society and its social relations in fact constitute the ‘rational’, calculating individual. The well-known limitations of rational-actor theory, its static quality, its logical antinomies, its vulnerability to arguments of infinite regress, its failure to develop a progressive concrete research program, can all be traced to this starting-point.

Schram and Caterino (2006) contains a fundamental methodological criticism of rational choice theory for promoting the view that the natural science model is the only appropriate methodology in social science and that political science should follow this model, with its emphasis on quantification and mathematization. Schram and Caterino argue instead for methodological pluralism. The same argument is made by William E. Connolly, who in his work Neuropolitics shows that advances in neuroscience further illuminate some of the problematic practices of rational choice theory.

More recently Edward J. Nell and Karim Errouaki (2011, Ch. 1) argued that:The DNA of neoclassical economics is defective. Neither the induction problem nor the problems of methodological individualism can be solved within the framework of neoclassical assumptions. The neoclassical approach is to call on rational economic man to solve both. Economic relationships that reflect rational choice should be ‘projectible’. But that attributes a deductive power to ‘rational’ that it cannot have consistently with positivist (or even pragmatist) assumptions (which require deductions to be simply analytic). To make rational calculations projectible, the agents may be assumed to have idealized abilities, especially foresight; but then the induction problem is out of reach because the agents of the world do not resemble those of the model. The agents of the model can be abstract, but they cannot be endowed with powers actual agents could not have. This also undermines methodological individualism; if behaviour cannot be reliably predicted on the basis of the ‘rational choices of agents’, a social order cannot reliably follow from the choices of agents.

Furthermore, Pierre Bourdieu fiercely opposed rational choice theory as grounded in a misunderstanding of how social agents operate. Bourdieu argued that social agents do not continuously calculate according to explicit rational and economic criteria. According to Bourdieu, social agents operate according to an implicit practical logic—a practical sense—and bodily dispositions. Social agents act according to their "feel for the game" (the "feel" being, roughly, habitus, and the "game" being the field).

Other social scientists, inspired in part by Bourdieu's thinking have expressed concern about the inappropriate use of economic metaphors in other contexts, suggesting that this may have political implications. The argument they make is that by treating everything as a kind of "economy" they make a particular vision of the way an economy works seem more natural. Thus, they suggest, rational choice is as much ideological as it is scientific, which does not in and of itself negate its scientific utility.

An evolutionary psychology perspective is that many of the seeming contradictions and biases regarding rational choice can be explained as being rational in the context of maximizing biological fitness in the ancestral environment but not necessarily in the current one. Thus, when living at subsistence level where a reduction of resources may have meant death it may have been rational to place a greater value on losses than on gains. Proponents argue it may also explain differences between groups.

The rational choice approach allows preferences to be represented as real-valued utility functions. Economic decision making then becomes a problem of maximizing this utility function, subject to constraints (e.g. a budget). This has many advantages. It provides a compact theory that makes empirical predictions with a relatively sparse model - just a description of the agent's objectives and constraints. Furthermore, optimization theory is a well-developed field of mathematics. These two factors make rational choice models tractable compared to other approaches to choice. Most importantly, this approach is strikingly general. It has been used to analyze not only personal and household choices about
traditional economic matters like consumption and savings, but also choices about education, marriage, child-bearing, migration, crime and so on, as well as business decisions about output, investment, hiring, entry, exit, etc. with varying degrees of success.

Despite the empirical shortcomings of rational choice theory, the flexibility and tractability of rational choice models (and the lack of equally powerful alternatives) lead to them still being widely used.

The relationship between the rational choice theory and politics takes many forms, whether that be in voter behaviour, the actions of world leaders or even the way that important matters are dealt with. 

Voter behaviour shifts significantly thanks to rational theory, which is ingrained in human nature, the most significant of which occurs when there are times of economic trouble. This was assessed in detail by Anthony Downs who concluded that voters were acting on thoughts of higher income as a person ‘votes for whatever party he believes would provide him with the highest utility income from government action’. This is a significant simplification of how the theory influences people’s thoughts but makes up a core part of rational theory as a whole. In a more complex fashion, voters will react often radically in times of real economic strife, which can lead to an increase in extremism. The government will be made responsible by the voters and thus they see a need to make a change. Some of the most infamous extremist parties came to power on the back of economic recessions, the most significant being the far right Nazi Party in Germany, who used the hyperinflation at the time to gain power rapidly, as they promised a solution and a scapegoat for the blame. There is a trend to this, as a comprehensive study carried out by three political scientists concluded, as a ‘turn to the right’ occurs and it is clear that it is the work of the rational theory because within ten years the politics returns to a more common state. 

The actions of the leaders are becoming more aligned with the ideals of the rational choice theory due to the rise of nationalism across the world. The figurehead of rational thinking is Donald Trump, acting upon his much-used tagline ‘America First’, he claims to look out for his own country over anything else which is in essence the rational choice theory. Trump was met with uproar when he removed the USA from the Paris Climate treaty, which he believed were for their best interests. It is not just Trump who has recently shown the influence that the theory has on the most powerful people, Japan decided to pull out of the anti-whaling commission and ‘openly carry out commercial whaling in its territorial waters’ , much to the dismay of multiple environmental groups. However, for Japan, they valued their sport and tradition over any other country and acted upon their rational thinking. 

The fear for many is that rational thinking does not allow for an efficient resolution to some of the most troubling world problems, such as the climate crisis. In this way, nationalism will not allow countries to work together and thus the criticisms of the theory should be noted very carefully.




</doc>
<doc id="25401" url="https://en.wikipedia.org/wiki?curid=25401" title="Romance languages">
Romance languages

The Romance languages (nowadays rarely Romanic languages, Latin languages, or Neo-Latin languages) are the modern languages that evolved from Vulgar Latin between the third and eighth centuries. They are a subgroup of the Italic languages in the Indo-European language family.

Today, around 800 million people are native speakers worldwide, mainly in the Americas, Europe, and parts of Africa, as well as elsewhere. Additionally, the major Romance languages have many non-native speakers and are in widespread use as lingua francas. This is especially the case for French, which is in widespread use throughout Central and West Africa, Madagascar, Mauritius, and North Africa (excluding Egypt).

The five most widely spoken Romance languages by number of native speakers are Spanish (470 million), Portuguese (250 million), French (150 million), Italian (90 million), and Romanian (25 million).

Because of the difficulty of imposing boundaries on a continuum, various counts of the modern Romance languages are given; for example, Dalby lists 23 based on mutual intelligibility. The following, more extensive list, includes 35 current, living languages, and one extinct language, Dalmatian:


Romance languages are the continuation of Vulgar Latin, the popular and colloquial sociolect of Latin spoken by soldiers, settlers, and merchants of the Roman Empire, as distinguished from the classical form of the language spoken by the Roman upper classes, the form in which the language was generally written. Between 350 BC and 150 AD, the expansion of the Empire, together with its administrative and educational policies, made Latin the dominant native language in continental Western Europe. Latin also exerted a strong influence in southeastern Britain, the Roman province of Africa, western Germany, Pannonia and the whole Balkans.

During the Empire's decline, and after its fragmentation and the collapse of Western half in the fifth and sixth centuries, the spoken varieties of Latin became more isolated from each other, with the western dialects coming under heavy Germanic influence (the Goths and Franks in particular) and the eastern dialects coming under Slavic influence. The dialects diverged from classical Latin at an accelerated rate and eventually evolved into a continuum of recognizably different typologies. The colonial empires established by Portugal, Spain, and France from the fifteenth century onward spread their languages to the other continents to such an extent that about two-thirds of all Romance language speakers today live outside Europe.

Despite other influences (e.g. "substratum" from pre-Roman languages, especially Continental Celtic languages; and "superstratum" from later Germanic or Slavic invasions), the phonology, morphology, and lexicon of all Romance languages consist mainly of evolved forms of Vulgar Latin. However, some notable differences occur between today's Romance languages and their Roman ancestor. With only one or two exceptions, Romance languages have lost the declension system of Latin and, as a result, have SVO sentence structure and make extensive use of prepositions.

The term "Romance" comes from the Vulgar Latin adverb , "in Roman", derived from : for instance, in the expression , "to speak in Roman" (that is, the Latin vernacular), contrasted with , "to speak in Latin" (Medieval Latin, the conservative version of the language used in writing and formal contexts or as a lingua franca), and with , "to speak in Barbarian" (the non-Latin languages of the peoples living outside the Roman Empire). From this adverb the noun "romance" originated, which applied initially to anything written , or "in the Roman vernacular".

The word 'romance' with the modern sense of romance novel or love affair has the same origin. In the medieval literature of Western Europe, serious writing was usually in Latin, while popular tales, often focusing on heroic adventures and courtly love, were composed in the vernacular and came to be called "romances".

Lexical and grammatical similarities among the Romance languages, and between Latin and each of them, are apparent from the following examples having the same meaning in various Romance lects:

English: She always closes the window before she dines / before dining.

Romance-based creoles and pidgins

Some of the divergence comes from semantic change: where the same root words have developed different meanings. For example, the Portuguese word is descended from Latin "window" (and is thus cognate to French , Italian , Romanian and so on), but now means "skylight" and "slit". Cognates may exist but have become rare, such as in Spanish, or dropped out of use entirely. The Spanish and Portuguese terms meaning "to throw through a window" and meaning "replete with windows" also have the same root, but are later borrowings from Latin.

Likewise, Portuguese also has the word , a cognate of Italian and Spanish , but uses it in the sense of "to have a late supper" in most varieties, while the preferred word for "to dine" is (related to archaic Spanish "to eat") because of semantic changes in the 19th century. Galician has both (from medieval "fẽestra", the ancestor of standard Portuguese ) and the less frequently used and .

As an alternative to (originally the genitive form), Italian has the pronoun , a cognate of the other words for "she", but it is hardly ever used in speaking.

Spanish, Asturian, and Leonese and Mirandese and Sardinian come from Latin "wind" (cf. English "window", etymologically 'wind eye'), and Portuguese , Galician , Mirandese from Latin * "small opening", a derivative of "door".

Sardinian (alternative for /) comes from Old Italian and is similar to other Romance languages such as French (from Italian ), Portuguese , Romanian , Spanish , Catalan and Corsican (alternative for ).

Documentary evidence is limited about Vulgar Latin for the purposes of comprehensive research, and the literature is often hard to interpret or generalize. Many of its speakers were soldiers, slaves, displaced peoples, and forced resettlers, more likely to be natives of conquered lands than natives of Rome. In Western Europe, Latin gradually replaced Celtic and other Italic languages, which were related to it by a shared Indo-European origin. Commonalities in syntax and vocabulary facilitated the adoption of Latin.

Vulgar Latin is believed to have already had most of the features shared by all Romance languages, which distinguish them from Classical Latin, such as the almost complete loss of the Latin grammatical case system and its replacement by prepositions; the loss of the neuter grammatical gender and comparative inflections; replacement of some verb paradigms by innovations (e.g. the synthetic future gave way to an originally analytic strategy now typically formed by infinitive + evolved present indicative forms of 'have'); the use of articles; and the initial stages of the palatalization of the plosives /k/, /g/, and /t/.

To some scholars, this suggests the form of Vulgar Latin that evolved into the Romance languages was around during the time of the Roman Empire (from the end of the first century BC), and was spoken alongside the written Classical Latin which was reserved for official and formal occasions. Other scholars argue that the distinctions are more rightly viewed as indicative of sociolinguistic and register differences normally found within any language. Both were mutually intelligible as one and the same language, which was true until very approximately the second half of the 7th century. However, within two hundred years Latin became a dead language since "the Romanized people of Europe could no longer understand texts that were read aloud or recited to them," i.e. Latin had ceased to be a first language and became a foreign language that had to be learned, if the label Latin is constrained to refer to a state of the language frozen in past time and restricted to linguistic features for the most part typical of higher registers.

With the rise of the Roman Empire, Vulgar Latin spread first throughout Italy and then through southern, western, central, and southeast Europe, and northern Africa along parts of western Asia.

During the political decline of the Western Roman Empire in the fifth century, there were large-scale migrations into the empire, and the Latin-speaking world was fragmented into several independent states. Central Europe and the Balkans were occupied by Germanic and Slavic tribes, as well as by Huns. These incursions isolated the Vlachs from the rest of Romance-speaking Europe.

British and African Romance—the forms of Vulgar Latin used in Britain and the Roman province of Africa, where it had been spoken by much of the urban population—disappeared in the Middle Ages (as did Pannonian Romance in what is now Hungary, and Moselle Romance in Germany). But the Germanic tribes that had penetrated Roman Italy, Gaul, and Hispania eventually adopted Latin/Romance and the remnants of the culture of ancient Rome alongside existing inhabitants of those regions, and so Latin remained the dominant language there. In part due to regional dialects of the Latin language and local environments, several languages evolved from it.

Meanwhile, large-scale migrations into the Eastern Roman Empire started with the Goths and continued with Huns, Avars, Bulgars, Slavs, Pechenegs, Hungarians and Cumans. The invasions of Slavs were the most thoroughgoing, and they partially reduced the Romanic element in the Balkans. 
The invasion of the Turks and conquest of Constantinople in 1453 marked the end of the empire. The Slavs named the Romance-speaking population Vlachs, while the latter called themselves "Rumân" or "Român", from the Latin "Romanus" The Daco-Roman dialect became fully distinct from the three dialects spoken South of the Danube—Macedo-Romanian, Istro-Romanian, and Megleno-Romanian—during the ninth and tenth centuries, when the Romanians (sometimes called Vlachs or Wallachians) emerged as a people.

Over the course of the fourth to eighth centuries, local changes in phonology, morphology, syntax and lexicon accumulated to the point that the speech of any locale was noticeably different from another. In principle, differences between any two lects increased the more they were separated geographically, reducing easy mutual intelligibility between speakers of distant communities. Clear evidence of some levels of change is found in the "Reichenau Glosses", an eighth-century compilation of about 1,200 words from the fourth-century Vulgate of Jerome that had changed in phonological form or were no longer normally used, along with their eighth-century equivalents in proto-Franco-Provençal. The following are some examples with reflexes in several modern Romance languages for comparison:
In all of the above examples, the words appearing in the fourth century Vulgate are the same words as would have been used in Classical Latin of c. 50 BC. It is likely that some of these words had already disappeared from casual speech by the time of the "Glosses"; but if so, they may well have been still widely understood, as there is no recorded evidence that the common people of the time had difficulty understanding the language.

By the 8th century, the situation was very different. During the late 8th century, Charlemagne, holding that "Latin of his age was by classical standards intolerably corrupt", successfully imposed Classical Latin as an artificial written vernacular for Western Europe. Unfortunately, this meant that parishioners could no longer understand the sermons of their priests, forcing the Council of Tours in 813 to issue an edict that priests needed to translate their speeches into the "rustica romana lingua", an explicit acknowledgement of the reality of the Romance languages as separate languages from Latin.

By this time, and possibly as early as the 6th century according to Price (1984), the Romance lects had split apart enough to be able to speak of separate Gallo-Romance, Ibero-Romance, Italo-Romance and Eastern Romance languages. Some researchers have postulated that the major divergences in the spoken dialects began or accelerated considerably in the 5th century, as the formerly widespread and efficient communication networks of the Western Roman Empire rapidly broke down, leading to the total disappearance of the Western Roman Empire by the end of the century. The critical period between the 5th–10th centuries AD is poorly documented because little or no writing from the chaotic "Dark Ages" of the 5th–8th centuries has survived, and writing after that time was in consciously classicized Medieval Latin, with vernacular writing only beginning in earnest in the 11th or 12th centuries. An exception such as the Oaths of Strasbourg is evidence that by the ninth century effective communication with a non-learnèd audience was carried out in evolved Romance.

A language that was closely related to medieval Romanian was spoken during the Dark Ages by Vlachs in the Balkans, Herzegovina, Dalmatia (Morlachs), Ukraine (Hutsuls), Poland (Gorals), Slovakia, and Czech Moravia, but gradually these communities lost their maternal language.

Between the 10th and 13th centuries, some local vernaculars developed a written form and began to supplant Latin in many of its roles. In some countries, such as Portugal, this transition was expedited by force of law; whereas in others, such as Italy, many prominent poets and writers used the vernacular of their own accord – some of the most famous in Italy being Giacomo da Lentini and Dante Alighieri. Well before that, the vernacular was also used for practical purposes, such as the testimonies in the Placiti Cassinesi, written 960-963.

The invention of the printing press brought a tendency towards greater uniformity of standard languages within political boundaries, at the expense of other Romance languages and dialects less favored politically. In France, for instance, the dialect spoken in the region of Paris gradually spread to the entire country, and the Occitan of the south lost ground.

The Romance language most widely spoken natively today is Spanish, followed by Portuguese, French, Italian and Romanian, which together cover a vast territory in Europe and beyond, and work as official and national languages in dozens of countries.

French, Italian, Portuguese, Spanish, and Romanian are also official languages of the European Union. Spanish, Portuguese, French, Italian, Romanian, and Catalan are the official languages of the Latin Union; and French and Spanish are two of the six official languages of the United Nations. Outside Europe, French, Portuguese and Spanish are spoken and enjoy official status in various countries that emerged from the respective colonial empires.

Spanish is an official language in Spain and in nine countries of South America, home to about half that continent's population; in six countries of Central America (all except Belize); and in Mexico. In the Caribbean, it is official in Cuba, the Dominican Republic, and Puerto Rico. In all these countries, Latin American Spanish is the vernacular language of the majority of the population, giving Spanish the most native speakers of any Romance language. In Africa it is an official language of Equatorial Guinea.

Portuguese, in its original homeland, Portugal, is spoken by virtually the entire population of 10 million.
As the official language of Brazil, it is spoken by more than 200 million people in that country, as well as by neighboring residents of eastern Paraguay and northern Uruguay, accounting for a little more than half the population of South America, thus making Portuguese the most spoken official Romance language in a single country. It is the official language of six African countries (Angola, Cape Verde, Guinea-Bissau, Mozambique, Equatorial Guinea, and São Tomé and Príncipe), and is spoken as a first language by perhaps 30 million residents of that continent. In Asia, Portuguese is co-official with other languages in East Timor and Macau, while most Portuguese-speakers in Asia—some 400,000—are in Japan due to return immigration of Japanese Brazilians. In North America 1,000,000 people speak Portuguese as their home language.
In Oceania, Portuguese is the second most spoken Romance language, after French, due mainly to the number of speakers in East Timor. Its closest relative, Galician, has official status in the autonomous community of Galicia in Spain, together with Spanish.

Outside Europe, French is spoken natively most in the Canadian province of Quebec, and in parts of New Brunswick and Ontario. Canada is officially bilingual, with French and English being the official languages. In parts of the Caribbean, such as Haiti, French has official status, but most people speak creoles such as Haitian Creole as their native language. French also has official status in much of Africa, but relatively few native speakers. In France's overseas possessions, native use of French is increasing.

Although Italy also had some colonial possessions before World War II, its language did not remain official after the end of the colonial domination. As a result, Italian outside of Italy and Switzerland is now spoken only as a minority language by immigrant communities in North and South America and Australia. In some former Italian colonies in Africa—namely Libya, Eritrea and Somalia—it is spoken by a few educated people in commerce and government.

Romania did not establish a colonial empire, but beyond its native territory in southeastern Europe, the Romanian language is spoken as a minority language by autochthonous populations in Serbia, Bulgaria, and Hungary, and in some parts of the former Greater Romania (before 1945), as well as in Ukraine (Bukovina, Budjak) and in some villages between the Dniester and Bug rivers. The Aromanian language is spoken today by Aromanians in Bulgaria, Macedonia, Albania, Kosovo, and Greece. Romanian also spread to other countries on the Mediterranean (especially the other Romance-speaking countries, most notably Italy and Spain), and elsewhere such as Israel, where it is the native language of five percent of the population, and is spoken by many more as a secondary language. This is due to the large number of Romanian-born Jews who moved to Israel after World War II. And finally, some 2.6 million people in the former Soviet republic of Moldova speak a variety of Romanian, called variously Moldovan or Romanian by them.

The total native speakers of Romance languages are divided as follows (with their ranking within the languages of the world in brackets):

Catalan is the official language of Andorra. In Spain, it is co-official with Spanish in Catalonia, the Valencian Community, and the Balearic Islands, and it is recognized, but not official, in La Franja, and in Aragon. In addition, it is spoken by many residents of Alghero, on the island of Sardinia, and it is co-official in that city. Galician, with more than a million native speakers, is official together with Spanish in Galicia, and has legal recognition in neighbouring territories in Castilla y León. A few other languages have official recognition on a regional or otherwise limited level; for instance, Asturian and Aragonese in Spain; Mirandese in Portugal; Friulan, Sardinian and Franco-Provençal in Italy; and Romansh in Switzerland.

The remaining Romance languages survive mostly as spoken languages for informal contact. National governments have historically viewed linguistic diversity as an economic, administrative or military liability, as well as a potential source of separatist movements; therefore, they have generally fought to eliminate it, by extensively promoting the use of the official language, restricting the use of the other languages in the media, recognizing them as mere "dialects", or even persecuting them. As a result, all of these languages are considered endangered to varying degrees according to the UNESCO Red Book of Endangered Languages, ranging from "vulnerable" (e.g. Sicilian and Venetian) to "severely endangered" (Arpitan, most of the Occitan varieties). Since the late twentieth and early twenty-first centuries, increased sensitivity to the rights of minorities has allowed some of these languages to start recovering their prestige and lost rights. Yet it is unclear whether these political changes will be enough to reverse the decline of minority Romance languages.

The classification of the Romance languages is inherently difficult, because most of the linguistic area is a dialect continuum, and in some cases political biases can come into play. Along with Latin (which is not included among the Romance languages) and a few extinct languages of ancient Italy, they make up the Italic branch of the Indo-European family.

There are various schemes used to subdivide the Romance languages. Three of the most common schemes are as follows:

The main subfamilies that have been proposed by Ethnologue within the various classification schemes for Romance languages are:

This three-way division is made primarily based on the outcome of Vulgar Latin (Proto-Romance) vowels:
Italo-Western is in turn split along the so-called "La Spezia–Rimini Line" in northern Italy, which divides the central and southern Italian languages from the so-called Western Romance languages to the north and west. The primary characteristics dividing the two are:

The reality is somewhat more complex. All of the "southeast" characteristics apply to all languages southeast of the line, and all of the "northwest" characteristics apply to all languages in France and (most of) Spain. However, the Gallo-Italic languages are somewhere in between. All of these languages do have the "northwest" characteristics of lenition and loss of gemination. However:

On top of this, the ancient Mozarabic language in southern Spain, at the far end of the "northwest" group, had the "southeast" characteristics of lack of lenition and palatalization of /k/ to . Certain languages around the Pyrenees (e.g. some highland Aragonese dialects) also lack lenition, and northern French dialects such as Norman and Picard have palatalization of /k/ to (although this is possibly an independent, secondary development, since /k/ between vowels, i.e. when subject to lenition, developed to /dz/ rather than , as would be expected for a primary development).

The usual solution to these issues is to create various nested subgroups. Western Romance is split into the Gallo-Iberian languages, in which lenition happens and which include nearly all the Western Romance languages, and the Pyrenean-Mozarabic group, which includes the remaining languages without lenition (and is unlikely to be a valid clade; probably at least two clades, one for Mozarabic and one for Pyrenean). Gallo-Iberian is split in turn into the Iberian languages (e.g. Spanish and Portuguese), and the larger Gallo-Romance languages (stretching from eastern Spain to northeast Italy).

Probably a more accurate description, however, would be to say that there was a focal point of innovation located in central France, from which a series of innovations spread out as areal changes. The La Spezia–Rimini Line represents the farthest point to the southeast that these innovations reached, corresponding to the northern chain of the Apennine Mountains, which cuts straight across northern Italy and forms a major geographic barrier to further language spread.

This would explain why some of the "northwest" features (almost all of which can be characterized as innovations) end at differing points in northern Italy, and why some of the languages in geographically remote parts of Spain (in the south, and high in the Pyrenees) are lacking some of these features. It also explains why the languages in France (especially standard French) seem to have innovated earlier and more extensively than other Western Romance languages.

Many of the "southeast" features also apply to the Eastern Romance languages (particularly, Romanian), despite the geographic discontinuity. Examples are lack of lenition, maintenance of intertonic vowels, use of vowel-changing plurals, and palatalization of /k/ to . This has led some researchers to postulate a basic two-way East-West division, with the "Eastern" languages including Romanian and central and southern Italian, although this view is troubled by the contrast of numerous Romanian phonological developments with those found in Italy below the La Spezia-Rimini line. Among these features, in Romanian geminates reduced historically to single units — which may be an independent development or perhaps due to Slavic influence — and /kt/ developed into /pt/, whereas in central and southern Italy geminates are preserved and /kt/ underwent assimilation to /tt/.

Despite being the first Romance language to evolve from Vulgar Latin, Sardinian does not fit at all into this sort of division. It is clear that Sardinian became linguistically independent from the remainder of the Romance languages at an extremely early date, possibly already by the first century BC. Sardinian contains a large number of archaic features, including total lack of palatalization of /k/ and /g/ and a large amount of vocabulary preserved nowhere else, including some items already archaic by the time of Classical Latin (first century BC). Sardinian has plurals in /s/ but post-vocalic lenition of voiceless consonants is normally limited to the status of an allophonic rule (e.g. [k]"ane" 'dog' but "su" [g]"ane" or "su" [ɣ]"ane" 'the dog'), and there are a few innovations unseen elsewhere, such as a change of /au/ to /a/. Use of "su" < "ipsum" as an article is a retained archaic feature that also exists in the Catalan of the Balearic Islands and that used to be more widespread in Occitano-Romance, and is known as "" (literally the "salted article"), while Sardinian shares delabialization of earlier /kw/ and /gw/ with Romanian: Sard. "abba", Rum. "apă" 'water'; Sard. "limba", Rom. "limbă" 'language' (cf. Italian "acqua", "lingua").

Gallo-Romance can be divided into the following subgroups:
The following groups are also sometimes considered part of Gallo-Romance:

The Gallo-Romance languages are generally considered the most innovative (least conservative) among the Romance languages. Characteristic Gallo-Romance features generally developed earliest and appear in their most extreme manifestation in the Langue d'oïl, gradually spreading out along riverways and transalpine roads.

In some ways, however, the Gallo-Romance languages are conservative. The older stages of many of the languages preserved a two-case system consisting of nominative and oblique, fully marked on nouns, adjectives and determiners, inherited almost directly from the Latin nominative and accusative and preserving a number of different declensional classes and irregular forms. The languages closest to the oïl epicenter preserve the case system the best, while languages at the periphery lose it early.

Notable characteristics of the Gallo-Romance languages are:

Some Romance languages have developed varieties which seem dramatically restructured as to their grammars or to be mixtures with other languages. It is not always clear whether they should be classified as Romance, pidgins, creole languages, or mixed languages. Some other languages, such as Modern English, are sometimes thought of as creoles of semi-Romance ancestry. There are several dozens of creoles of French, Spanish, and Portuguese origin, some of them spoken as national languages in former European colonies.

Creoles of French:

Creoles of Spanish:

Creoles of Portuguese:

Latin and the Romance languages have also served as the inspiration and basis of numerous auxiliary and constructed languages, so-called "neo-romantic languages".

The concept was first developed in 1903 by Italian mathematician Giuseppe Peano, under the title Latino sine flexione. He wanted to create a "naturalistic" international language, as opposed to an autonomous constructed language like Esperanto or Volapuk which were designed for maximal simplicity of lexicon and derivation of words. Peano used Latin as the base of his language, because at the time of his flourishing it was the "de facto" international language of scientific communication.

Other languages developed since include Idiom Neutral, Occidental, Lingua Franca Nova, and most famously and successfully, Interlingua. Each of these languages has attempted to varying degrees to achieve a pseudo-Latin vocabulary as common as possible to living Romance languages.

There are also languages created for artistic purposes only, such as Talossan. Because Latin is a very well attested ancient language, some amateur linguists have even constructed Romance languages that mirror real languages that developed from other ancestral languages. These include Brithenig (which mirrors Welsh), Breathanach (mirrors Irish), Wenedyk (mirrors Polish), Þrjótrunn (mirrors Icelandic), and Helvetian (mirrors German).

Romance languages have a number of shared features across all languages:

The most significant changes between Classical Latin and Proto-Romance (and hence all the modern Romance languages) relate to the reduction or loss of the Latin case system, and the corresponding syntactic changes that were triggered.

The case system was drastically reduced from the six-case system of Classical Latin. Although five cases can be reconstructed for Vulgar Latin nouns (nominative, accusative, genitive, dative, and ablative), for Proto-Romance this had been reduced to three: nominative, accusative-ablative, and genitive-dative. This system is preserved best in pronouns. In the West, the genitive-dative disappeared with the genitive replaced by "de" + ablative and the dative by "ad" + accusative. This left only two cases: nominative and oblique. (However, a morphologically unmarked genitive, the so-called juxtaposition genitive, syntactically still discernible, survives in Old French and Old Occitan, also leaving traces in Old Italian and some modern Italian dialects.) Some of the older Gallo-Romance languages (in particular, Old French, Old Occitan, Old Sursilvan and Old Friulian, and in traces Old Catalan and Old Venetian) preserved this two-case system well into the literary period, and in Ibero-Romance languages, such as Spanish and Portuguese, as well as in Italian (see under Case), a couple of examples are found which preserve the old nominative. In the East, a genitive-dative made entirely of dative forms was retained but the nominative and accusative-ablative eventually merged.

Concomitant with the loss of cases, freedom of word order was greatly reduced. Classical Latin had a generally verb-final (SOV) but overall quite free word order, with a significant amount of word scrambling and mixing of left-branching and right-branching constructions. The Romance languages eliminated word scrambling and nearly all left-branching constructions, with most languages developing a rigid SVO, right-branching syntax. (Old French, however, had a freer word order due to the two-case system still present, as well as a predominantly verb-second word order developed under the influence of the Germanic languages.) Some freedom, however, is allowed in the placement of adjectives relative to their head noun. In addition, some languages (e.g. Spanish, Romanian) have an "accusative preposition" (Romanian "pe", Spanish "personal "a"") along with clitic doubling, which allows for some freedom in ordering the arguments of a verb.

The Romance languages developed grammatical articles where Latin had none. Articles are often introduced around the time a robust case system falls apart in order to disambiguate the remaining case markers (which are usually too ambiguous by themselves) and to serve as parsing clues that signal the presence of a noun (a function that used to be served by the case endings themselves).

This was the pattern followed by the Romance languages: In the Romance languages that still preserved a functioning nominal case system (e.g., Romanian and Old French), only the combination of article and case ending serves to uniquely identify number and case (compare the similar situation in modern German). All Romance languages have a definite article (originally developed from "ipse" "self" but replaced in nearly all languages by "ille" "that (over there)") and an indefinite article (developed from "ūnus" "one"). Many also have a partitive article ("dē" "of" + definite article).

Latin had a large number of syntactic constructions expressed through infinitives, participles, and similar nominal constructs. Examples are the ablative absolute, the accusative-plus-infinitive construction used for reported speech, gerundive constructions, and the common use of reduced relative clauses expressed through participles. All of these are replaced in the Romance languages by subordinate clauses expressed with finite verbs, making the Romance languages much more "verbal" and less "nominal" than Latin. Under the influence of the Balkan sprachbund, Romanian has progressed the furthest, largely eliminating the infinitive. (It is being revived, however, due to the increasing influence of other Romance languages.)


Every language has a different set of vowels from every other. Common characteristics are as follows:

Most Romance languages have similar sets of consonants. The following is a combined table of the consonants of the five major Romance languages (French, Spanish, Italian, Portuguese, Romanian).

Key:

Notable changes:

Most instances of most of the sounds below that occur (or used to occur, as described above) in all of the languages are cognate. However:

Word stress was rigorously predictable in classical Latin except in a very few exceptional cases, either on the penultimate syllable (second from last) or antepenultimate syllable (third from last), according to the syllable weight of the penultimate syllable. Stress in the Romance Languages mostly remains on the same syllable as in Latin, but various sound changes have made it no longer so predictable. Minimal pairs distinguished only by stress exist in some languages, e.g. Italian "Papa" "Pope" vs. "papà" "daddy", or Spanish "límite" "[a] limit", present subjunctive "limite" "[that] [I/he] limit" and preterite "limité" "[I] limited".

Erosion of unstressed syllables following the stress has caused most Spanish and Portuguese words to have either penultimate or ultimate stress: e.g. Latin "trēdecim" "thirteen" > Spanish "trece", Portuguese "treze"; Latin "amāre" "to love" > Spanish/Portuguese "amar". Most words with antepenultimate stress are learned borrowings from Latin, e.g. Spanish/Portuguese "fábrica" "factory" (the corresponding inherited word is Spanish "fragua", Portuguese "frágua" "forge"). This process has gone even farther in French, with deletion of all post-stressed vowels, leading to consistent, predictable stress on the last syllable: e.g. Latin "Stephanum" "Stephen" > Old French "Estievne" > French "Étienne" ; Latin "juvenis" "young" > Old French "juevne" > French "jeune" . This applies even to borrowings: e.g. Latin "fabrica" > French borrowing "fabrique" (the inherited word in this case being monosyllabic "forge" < Pre-French *"fauriga").

Other than French (with consistent final stress), the position of the stressed syllable generally falls on one of the last three syllables. Exceptions may be caused by clitics or (in Italian) certain verb endings, e.g. Italian "telefonano" "they telephone"; Spanish "entregándomelo" "delivering it to me"; Italian "mettiamocene" "let's put some of it in there"; Portuguese "dávamos-vo-lo" "we were giving it to you". Stress on verbs is almost completely predictable in Spanish and Portuguese, but less so in Italian.

Nouns, adjectives, and pronouns can be marked for gender, number and case. Adjectives and pronouns must agree in all features with the noun they are bound to.

The Romance languages inherited from Latin two grammatical numbers, singular and plural; the only trace of a dual number comes from Latin "ambō" > Spanish and Portuguese "ambos", Old Romanian "îmbi" > Romanian "ambii", Old French "ambe", Italian "ambedue, entrambi".

Most Romance languages have two grammatical genders, masculine and feminine. The gender of animate nouns is generally natural (i.e. nouns referring to men are generally masculine, and vice versa), but for nonanimate nouns it is arbitrary.

Although Latin had a third gender (neuter), there is little trace of this in most languages. The biggest exception is Romanian, where there is a productive class of "neuter" nouns, which include the descendants of many Latin neuter nouns and which behave like masculines in the singular and feminines in the plural, both in the endings used and in the agreement of adjectives and pronouns (e.g. "un deget" "one finger" vs. "două degete" "two fingers", cf. Latin "digitus", pl. "digiti"). This behavior happens also in Italian Language with a restricted number of words (e.g. "un uovo" "an egg", "il braccio" "the arm" masculine in the singular, "le uova" "the eggs", "le braccia" "the arms" feminine in the plural).

Such nouns arose because of the identity of the Latin neuter singular "-um" with the masculine singular, and the identity of the Latin neuter plural "-a" with the feminine singular. A similar class exists in Italian, although it is no longer productive (e.g. "il dito" "the finger" vs. "le dita" "the fingers", "l'uovo" "the egg" vs. "le uova" "the eggs"). A similar phenomenon may be observed in Albanian (which is heavily Romance-influenced), and the category remains highly productive with a number of new words loaned or coined in the neuter ("(një) hotel" one hotel(m) vs. "(tri) hotele" three hotels (f)). (A few isolated nouns in Latin had different genders in the singular and plural, but this was an unrelated phenomenon; this is similarly the case with a few French nouns, such as "amour", "délice", "orgue".)

Spanish also has vestiges of the neuter in the demonstrative adjectives: "esto", "eso", "aquello", the pronoun "ello" (meaning "it") and the article "lo" (used to intensify adjectives). Portuguese also has neuter demonstrative adjectives: "isto", "isso", "aquilo" (meaning "this [near me]", "this/that [near you]", "that [far from the both of us]").

Remnants of the neuter, interpretable now as "a sub-class of the non-feminine gender" (Haase 2000:233), are vigorous in Italy in an area running roughly from Ancona to Matera and just north of Rome to Naples. Oppositions with masculine typically have been recategorized, so that neuter signifies the referent in general, while masculine indicates a more specific instance, with the distinction marked by the definite article. In Southeast Umbrian, for example, neuter "lo pane" is 'the bread', while masculine "lu pane" refers to an individual piece or loaf of bread. Similarly, neuter "lo vinu" is wine in general, while masculine "lu vinu" is a specific sort of wine, with the consequence that mass "lo vinu" has no plural counterpart, but "lu vinu" can take a sortal plural form "li vini", referring to different types of wine. Phonological forms of articles vary by locale.

Latin had an extensive case system, where all nouns were declined in six cases (nominative, vocative, accusative, dative, genitive, and ablative) and two numbers. Many adjectives were additionally declined in three genders, leading to a possible 6 × 2 × 3 = 36 endings per adjective (although this was rarely the case). In practice, some category combinations had identical endings to other combinations, but a basic adjective like "bonus" "good" still had 14 distinct endings.

In all Romance languages, this system was drastically reduced. In most modern Romance languages, in fact, case is no longer marked at all on nouns, adjectives and determiners, and most forms are derived from the Latin accusative case. Much like English, however, case has survived somewhat better on pronouns.

Most pronouns have distinct nominative, accusative, genitive and possessive forms (cf. English "I, me, mine, my"). Many also have a separate dative form, a "disjunctive" form used after prepositions, and (in some languages) a special form used with the preposition "con" "with" (a conservative feature inherited from Latin forms such as "mēcum", "tēcum", "nōbīscum").

The system of inflectional classes is also drastically reduced. The basic system is most clearly indicated in Spanish, where there are only three classes, corresponding to the first, second and third declensions in Latin: plural in "-as" (feminine), plural in "-os" (masculine), plural in "-es" (either masculine or feminine). The singular endings exactly track the plural, except the singular "-e" is dropped after certain consonants.

The same system underlines many other modern Romance languages, such as Portuguese, French and Catalan. In these languages, however, further sound changes have resulted in various irregularities. In Portuguese, for example, loss of /l/ and /n/ between vowels (with nasalization in the latter case) produces various irregular plurals ("nação – nações" "nation(s)"; "hotel – hotéis" "hotel(s)").

In French and Catalan, loss of /o/ and /e/ in most unstressed final syllables has caused the "-os" and "-es" classes to merge. In French, merger of remaining /e/ with final /a/ into , and its subsequent loss, has completely obscured the original Romance system, and loss of final /s/ has caused most nouns to have identical pronunciation in singular and plural, although they are still marked differently in spelling (e.g. "femme – femmes" "woman – women", both pronounced ).

Noun inflection has survived in Romanian somewhat better than elsewhere. Determiners are still marked for two cases (nominative/accusative and genitive/dative) in both singular and plural, and feminine singular nouns have separate endings for the two cases. In addition, there is a separate vocative case, enriched with native development and Slavic borrowings (see some examples here) and the combination of noun with a following clitic definite article produces a separate set of "definite" inflections for nouns.

The inflectional classes of Latin have also survived more in Romanian than elsewhere, e.g. "om – oameni" "man – men" (Latin "homo" – "homines"); "corp – corpuri" "body – bodies" (Latin "corpus" – "corpora"). (Many other exceptional forms, however, are due to later sound changes or analogy, e.g. "casă – case" "house(s)" vs. "lună – luni" "moon(s)"; "frate – fraţi" "brother(s)" vs. "carte – cărţi" "book(s)" vs. "vale – văi" "valley(s)".)

In Italian, the situation is somewhere in between Spanish and Romanian. There are no case endings and relatively few classes, as in Spanish, but noun endings are generally formed with vowels instead of /s/, as in Romanian: "amico – amici" "friend(s) (masc.)", "amica – amiche" "friend(s) (fem.)"; "cane – cani" "dog(s)". The masculine plural "amici" is thought to reflect the Latin nominative plural "-ī" rather than accusative plural "-ōs" (Spanish "-os"); however, the other plurals are thought to stem from special developments of Latin "-ās" and "-ēs".

A different type of noun inflection survived into the medieval period in a number of western Romance languages (Old French, Old Occitan, and the older forms of a number of Rhaeto-Romance languages). This inflection distinguished nominative from oblique, grouping the accusative case with the oblique, rather than with the nominative as in Romanian.

The oblique case in these languages generally inherits from the Latin accusative; as a result, masculine nouns have distinct endings in the two cases while most feminine nouns do not.

A number of different inflectional classes are still represented at this stage. For example, the difference in the nominative case between masculine "li voisins" "the neighbor" and "li pere" "the father", and feminine "la riens" "the thing" vs. "la fame" "the woman", faithfully reflects the corresponding Latin inflectional differences ("vicīnus" vs. "pater", "fēmina" vs. "rēs").

A number of synchronically quite irregular differences between nominative and oblique reflect direct inheritances of Latin third-declension nouns with two different stems (one for the nominative singular, one for all other forms), most with of which had a stress shift between nominative and the other forms: "li ber – le baron" "baron" ("barō" – "barōnem"); "la suer – la seror" "sister" ("soror" – "sorōrem"); "li prestre – le prevoire" "priest" ("presbyter" – "presbyterem"); "li sire – le seigneur" "lord" ("senior" – "seniōrem"); "li enfes – l'enfant" "child" ("infāns" – "infantem").

A few of these multi-stem nouns derive from Latin forms without stress shift, e.g. "li om – le ome" "man" ("homō" – "hominem"). All of these multi-stem nouns refer to people; other nouns with stress shift in Latin (e.g. "amor" – "amōrem" "love") have not survived. Some of the same nouns with multiple stems in Old French or Old Occitan have come down in Italian in the nominative rather than the accusative (e.g. "uomo" "man" < "homō", "moglie" "wife" < "mulier"), suggesting that a similar system existed in pre-literary Italian.

The modern situation in Sursilvan (one of the Rhaeto-Romance languages) is unique in that the original nominative/oblique distinction has been reinterpreted as a predicative/attributive distinction:

As described above, case marking on pronouns is much more extensive than for nouns. Determiners (e.g. words such as "a", "the", "this") are also marked for case in Romanian.

Most Romance languages have the following sets of pronouns and determiners:

Unlike in English, a separate neuter personal pronoun ("it") generally does not exist, but the third-person singular and plural both distinguish masculine from feminine. Also, as described above, case is marked on pronouns even though it is not usually on nouns, similar to English. As in English, there are forms for nominative case (subject pronouns), oblique case (object pronouns), and genitive case (possessive pronouns); in addition, third-person pronouns distinguish accusative and dative. There is also an additional set of possessive determiners, distinct from the genitive case of the personal pronoun; this corresponds to the English difference between "my, your" and "mine, yours".

The Romance languages do not retain the Latin third-person personal pronouns, but have innovated a separate set of third-person pronouns by borrowing the demonstrative "ille" ("that (over there)"), and creating a separate reinforced demonstrative by attaching a variant of "ecce" "behold!" (or "here is ...") to the pronoun.

Similarly, in place of the genitive of the Latin pronouns, most Romance languages adopted the reflexive possessive, which then serves indifferently as both reflexive and non-reflexive possessive. Note that the reflexive, and hence the third-person possessive, is unmarked for the gender of the person being referred to. Hence, although gendered possessive forms do exist—e.g. Portuguese "seu" (masc.) vs. "sua" (fem.)—these refer to the gender of the object possessed, not the possessor.

The gender of the possessor needs to be made clear by a collocation such as French "la voiture à lui/elle", Portuguese "o carro dele/dela", literally "the car of him/her". (In spoken Brazilian Portuguese, these collocations are the usual way of expressing the third-person possessive, since the former possessive "seu carro" now has the meaning "your car".)

The same demonstrative "ille" is the source of the definite article in most Romance languages (see below), which explains the similarity in form between personal pronoun and definite article. When the two are different, it is usually because of differing degrees of phonological reduction. Generally, the personal pronoun is unreduced (beyond normal sound change), while the article has undergone various degrees of reduction, beginning with loss of one of the two original syllables, e.g. Spanish "ella" "she" < "illa" vs. "la" "the (fem.)" < "-la" < "illa", or masculine "el", developed from "il-" < "illud".

Object pronouns in Latin were normal words, but in the Romance languages they have become clitic forms, which must stand adjacent to a verb and merge phonologically with it. Originally, object pronouns could come either before or after the verb; sound change would often produce different forms in these two cases, with numerous additional complications and contracted forms when multiple clitic pronouns cooccurred.

Catalan still largely maintains this system with a highly complex clitic pronoun system. Most languages, however, have simplified this system by undoing some of the clitic mergers and requiring clitics to stand in a particular position relative to the verb (usually after imperatives, before other finite forms, and either before or after non-finite forms depending on the language).

When a pronoun cannot serve as a clitic, a separate disjunctive form is used. These result from dative object pronouns pronounced with stress (which causes them to develop differently from the equivalent unstressed pronouns), or from subject pronouns.

Most Romance languages are null subject languages. The subject pronouns are used only for emphasis and take the stress, and as a result are not clitics. In French, however (as in Friulian and in some Gallo-Italian languages of northern Italy), verbal agreement marking has degraded to the point that subject pronouns have become mandatory, and have turned into clitics. These forms cannot be stressed, so for emphasis the disjunctive pronouns must be used in combination with the clitic subject forms. Friulian and the Gallo-Italian languages have actually gone further than this and merged the subject pronouns onto the verb as a new type of verb agreement marking, which must be present even when there is a subject noun phrase. (Some non-standard varieties of French treat disjunctive pronouns as arguments and clitic pronouns as agreement markers.)

In medieval times, most Romance languages developed a distinction between familiar and polite second-person pronouns (a so-called "T-V distinction"), similar to the former English distinction between familiar "thou" and polite "you". This distinction was determined by the relationship between the speakers. As in English, this generally developed by appropriating the plural second-person pronoun to serve in addition as a polite singular. French is still at this stage, with familiar singular "tu" vs. formal or plural "vous". In cases like this, the pronoun requires plural agreement in all cases whenever a single affix marks both person and number (as in verb agreement endings and object and possessive pronouns), but singular agreement elsewhere where appropriate (e.g. "vous-même" "yourself" vs. "vous-mêmes" "yourselves").

Many languages, however, innovated further in developing an even more polite pronoun, generally composed of some noun phrases (e.g. Portuguese "vossa mercê" "your mercy", progressively reduced to "vossemecê", "vosmecê" and finally "você") and taking third-person singular agreement. A plural equivalent was created at the same time or soon after (Portuguese "vossas mercês", reduced to "vocês"), taking third-person plural agreement. Spanish innovated similarly, with "usted(es)" from earlier "vuestra(s) merced(es)".

In Portuguese and Spanish (as in other languages with similar forms), the "extra-polite" forms in time came to be the normal polite forms, and the former polite (or plural) second-person "vos" was displaced to a familiar form, either becoming a familiar plural (as in European Spanish) or a familiar singular (as in many varieties of Latin American Spanish). In the latter case, it either competes with the original familiar singular "tú" (as in Guatemala), displaces it entirely (as in Argentina), or is itself displaced (as in Mexico, except in Chiapas). In the Spanish of the Americas, the gap created by the loss of familiar plural "vos" was filled by originally polite "ustedes", with the result that there is no familiar/polite distinction in the plural, just as in the original "tú/vos" system.

A similar path was followed by Italian and Romanian. Romanian uses "dumneavoastră" "your lordship", while Italian the former polite phrase "sua eccellenza" "your excellency" has simply been supplanted by the corresponding pronoun "Ella" or "Lei" (literally "she", but capitalized when meaning "you"). As in European Spanish, the original second-person plural "voi" serves as familiar plural. (In Italy, during fascist times leading up to World War II, "voi" was resurrected as a polite singular, and discarded again afterwards, although it remains in some southern dialects.)

Portuguese innovated again in developing a new extra-polite pronoun "o senhor" "the sir", which in turn downgraded "você". Hence, modern European Portuguese has a three-way distinction between "familiar" "tu", "equalizing" "você" and "polite" "o senhor". (The original second-person plural "vós" was discarded centuries ago in speech, and is used today only in translations of the Bible, where "tu" and "vós" serve as universal singular and plural pronouns, respectively.)

Brazilian Portuguese, however, has diverged from this system, and most dialects simply use "você" (and plural "vocês") as a general-purpose second-person pronoun, combined with "te" (from "tu") as the clitic object pronoun. The form "o senhor" (and feminine "a senhora") is sometimes used in speech, but only in situations where an English speaker would say "sir" or "ma'am". The result is that second-person verb forms have disappeared, and the whole pronoun system has been radically realigned. However that is the case only in the spoken language of central and northern Brazil, with the northeastern and southern areas of the country still largely preserving the second-person verb form and the "tu" and "você" distinction.

Catalan still retains the plural form "vós" for formal distinction (similarly to French) but it is falling out of use, and nowadays is usually seen in extremely formal circumstances or in writing. Instead, (Central) or (Valencian) is normally used orally, which functions just like Spanish and Portuguese "usted/você".

Latin had no articles as such. The closest definite article was the non-specific demonstrative "is, ea, id" meaning approximately "this/that/the". The closest indefinite articles were the indefinite determiners "aliquī, aliqua, aliquod" "some (non-specific)" and "certus" "a certain".

Romance languages have both indefinite and definite articles, but none of the above words form the basis for either of these. Usually the definite article is derived from the Latin demonstrative "ille" ("that"), but some languages (e.g. Sardinian, Old Occitan, and Balearic Catalan) have forms from "ipse" (emphatic, as in "I myself"). The indefinite article everywhere is derived from the number "ūnus" ("one").

Some languages, e.g. French and Italian, have a partitive article that approximately translates as "some". This is used either with mass nouns or with plural nouns—both cases where the indefinite article cannot occur. A partitive article is used (and in French, required) whenever a bare noun refers to specific (but unspecified or unknown) quantity of the noun, but not when a bare noun refers to a class in general. For example, the partitive would be used in both of the following sentences:
But neither of these:
The sentence "Men arrived today", however, (presumably) means "some specific men arrived today" rather than "men, as a general class, arrived today" (which would mean that there were no men before today). On the other hand, "I hate men" does mean "I hate men, as a general class" rather than "I hate some specific men".

As in many other cases, French has developed the farthest from Latin in its use of articles. In French, nearly all nouns, singular and plural, must be accompanied by an article (either indefinite, definite, or partitive) or demonstrative pronoun.

Due to pervasive sound changes in French, most nouns are pronounced identically in the singular and plural, and there is often heavy homophony between nouns and identically pronounced words of other classes. For example, all of the following are pronounced : "sain" "healthy"; "saint" "saint, holy"; "sein" "breast"; "ceins" "(you) tie around, gird"; "ceint" "(he) ties around, girds"; "ceint" "tied around, girded"; and the equivalent noun and adjective plural forms "sains, saints, seins, ceints". The article helps identify the noun forms "saint" or "sein", and distinguish singular from plural; likewise, the mandatory subject of verbs helps identify the verb "ceint". In more conservative Romance languages, neither articles nor subject pronouns are necessary, since all of the above words are pronounced differently. In Italian, for example, the equivalents are "sano, santo, seno, cingi, cinge, cinto, sani, santi, seni, cinti", where all vowels and consonants are pronounced as written, and /s/ and /t͡ʃ/ are clearly distinct from each other.

Latin, at least originally, had a three-way distinction among demonstrative pronouns distinguished by distal value: "hic" 'this', "iste" 'that (near you)', "ille" 'that (over there)', similar to the distinction that used to exist in English as "this" vs. "that" vs. "yon(der)". In urban Latin of Rome, "iste" came to have a specifically derogatory meaning, but this innovation apparently did not reach the provinces and is not reflected in the modern Romance languages. A number of these languages still have such a three-way distinction, although "hic" has been lost and the other pronouns have shifted somewhat in meaning. For example, Spanish has "este" "this" vs. "ese" "that (near you)" vs. "aquel" (fem. "aquella") "that (over yonder)". The Spanish pronouns derive, respectively, from Latin "iste" "ipse" "accu"-"ille", where "accu-" is an emphatic prefix derived from "eccum" "behold (it!)" (still vigorous in Italy as "Ecco!" 'Behold!'), possibly with influence from "atque" "and".

Reinforced demonstratives such as "accu"-"ille" arose as "ille" came to be used as an article as well as a demonstrative. Such forms were often created even when not strictly needed to distinguish otherwise ambiguous forms. Italian, for example, has both "questo" "this" ("eccu"-"istum") and "quello" "that" ("eccu"-"illum"), in addition to dialectal "codesto" "that (near you)" (*"eccu-tē-istum"). French generally prefers forms derived from bare "ecce" "behold", as in the pronoun "ce" "this one/that one" (earlier "ço", from "ecce"-"hoc"; cf. Italian "ciò" 'that') and the determiner "ce/cet" "this/that" (earlier "cest", from "ecce"-"istum").

Reinforced forms are likewise common in locative adverbs (words such as English "here" and "there"), based on related Latin forms such as "hic" "this" vs. "hīc" "here", "hāc" "this way", and "ille" "that" vs. "illīc" "there", "illāc" "that way". Here again French prefers bare "ecce" while Spanish and Italian prefer "eccum" (French "ici" "here" vs. Spanish "aquí", Italian "qui"). In western languages such as Spanish, Portuguese and Catalan, doublets and triplets arose such as Portuguese "aqui, acá, cá" "(to) here" ("accu"-"hīc", "accu"-"hāc", "eccu"-"hāc"). From these, a prefix "a-" was extracted, from which forms like "aí" "there (near you)" ("a-(i)bi") and "ali" "there (over yonder)" ("a-(i)llīc") were created; compare Catalan neuter pronouns "açò" ("acce"-"hoc") "this", "això" ("a-(i)psum"-"hoc") "that (near you)", "allò" ("a-(i)llum"-"hoc") "that (yonder)".

Subsequent changes often reduced the number of demonstrative distinctions. Standard Italian, for example, has only a two-way distinction "this" vs. "that", as in English, with second-person and third-person demonstratives combined. In Catalan, however, a former three-way distinction "aquest, aqueix, aquell" has been reduced differently, with first-person and second-person demonstratives combined. Hence "aquest" means either "this" or "that (near you)"; on the phone, "aquest" is used to refer both to speaker and addressee.

Old French had a similar distinction to Italian ("cist/cest" vs. "cil/cel"), both of which could function as either adjectives or pronouns. Modern French, however, has no distinction between "this" and "that": "ce/cet, cette" < "cest, ceste" is only an adjective, and "celui, celle" < "cel lui, celle" is only a pronoun, and both forms indifferently mean either "this" or "that". (The distinction between "this" and "that" can be made, if necessary, by adding the suffixes "-ci" "here" or "-là" "there", e.g. "cette femme-ci" "this woman" vs. "cette femme-là" "that woman", but this is rarely done except when specifically necessary to distinguish two entities from each other.)

Verbs have many conjugations, including in most languages:

Several tenses and aspects, especially of the indicative mood, have been preserved with little change in most languages, as shown in the following table for the Latin verb "dīcere" (to say), and its descendants.
The main tense and mood distinctions that were made in classical Latin are generally still present in the modern Romance languages, though many are now expressed through compound rather than simple verbs. The passive voice, which was mostly synthetic in classical Latin, has been completely replaced with compound forms.
For a more detailed illustration of how the verbs have changed with respect to classical Latin, see Romance verbs.

Note that in Catalan, the synthetic preterite is predominantly a literary tense, except in Valencian; but an analytic preterite (formed using an auxiliary "vadō", which in other languages signals the future) persists in speech, with the same meaning. In Portuguese, a morphological present perfect does exist but has a different meaning (closer to "I have been doing").

The following are common features of the Romance languages (inherited from Vulgar Latin) that are different from Classical Latin:

Romance languages have borrowed heavily, though mostly from other Romance languages. However, some, such as Spanish, Portuguese, Romanian, and French, have borrowed heavily from other language groups. Vulgar Latin borrowed first from indigenous languages of the Roman empire, and during the Germanic folk movements, from Germanic languages, especially Gothic; for Eastern Romance languages, during Bulgarian Empires, from Slavic languages, especially Bulgarian. Notable examples are *"blancus" "white", replacing native "albus" (but Romansh "alv", Dalmatian "jualb", Romanian "alb"); *"guerra" "war", replacing native "bellum"; and the words for the cardinal directions, where cognates of English "north", "south", "east" and "west" replaced the native words "septentriō", "merīdiēs" (also "noon; midday nap"; cf. Romanian "meriză"), "oriens", and "occidens". (See History of French – The Franks.) Some Celtic words were incorporated into the core vocabulary, partly for words with no Latin equivalent ("betulla" "birch", "camisia" "shirt", "cerevisia" "beer"), but in some cases replacing Latin vocabulary ("gladius" "sword", replacing "ensis"; "cambiāre" "to exchange", replacing "mūtāre" except in Romanian and Portuguese; "carrus" "cart", replacing "currus"; "pettia" "piece", largely displacing "pars" (later resurrected) and eliminating "frustum"). Many Greek loans also entered the lexicon, e.g. "spatha" "sword" (Greek: "spáthē", replacing "gladius" which shifted to "iris", cf. French "épée", Spanish "espada", Italian "spada "and Romanian "spată"); "cara" "face" (Greek: κάρα "kára", partly replacing "faciēs"); "colpe" "blow" (Greek: κόλαφος "kólaphos", replacing "ictus", cf. Spanish "golpe", French "coup"); "cata" "each" (Greek: "katá", replacing "quisque"); common suffixes *"-ijāre/-izāre" (Greek: "-izein", French "oyer/-iser", Spanish "-ear/-izar", Italian "-eggiare/-izzare", etc.), "-ista" (Greek: "-istes").

Many basic nouns and verbs, especially those that were short or had irregular morphology, were replaced by longer derived forms with regular morphology. Nouns, and sometimes adjectives, were often replaced by diminutives, e.g. "auris" "ear" > "auricula" (orig. "outer ear") > "oricla" (Sardinian "origra", Italian "orecchia/o", Portuguese "orelha", etc.); "avis" "bird" > "avicellus" (orig. "chick, nestling") > "aucellu" (Occitan "aucèl", Friulian "ucel", Neapolitan "auciello", etc.); "caput" "head" > "capitium" (Portuguese "cabeça", Spanish "cabeza", French "chevet" "headboard"; but reflexes of "caput" were retained also, sometimes without change of meaning, as in Italian "capo" "head", alongside "testa"); "vetus" "old" > "vetulus" > "veclus" (Dalmatian "vieklo", Italian "vecchio", Portuguese "velho", etc.). Sometimes augmentative constructions were used instead: "piscis" "fish" > Old French "peis" > "peisson" (orig. "big fish") > French "poisson". Verbs were often replaced by frequentative constructions: "canere" "to sing" > "cantāre"; "iacere" "to throw" > "iactāre" > *"iectāre" (Italian "gettare", Portuguese "jeitar", Spanish "echar", etc.); "iuvāre" > "adiūtāre" (Italian "aiutare", Spanish "ayudar", French "aider", etc., meaning "help", alongside e.g. "iuvāre" > Italian "giovare" "to be of use"); "vēnārī" "hunt" (Romanian "vâna", Aromanian "avinari") > replaced by *"captiāre" "to hunt", frequentative of "capere" "to seize" (Italian "cacciare", Portuguese "caçar", Romansh "catschar", French "chasser", etc.).

Many Classical Latin words became archaic or poetic and were replaced by more colloquial terms: "equus" "horse" > "caballus" (orig. "nag") (but "equa" "mare" remains, cf. Spanish "yegua", Portuguese "égua", Sardinian "ebba", Romanian "iapă"); "domus" "house" > "casa" (orig. "hut"); "ignis" "fire" > "focus" (orig. "hearth"); "strāta" "street" > "rūga" (orig. "furrow") or "callis" (orig. "footpath") (but "strāta" is continued in Italian "strada"). In some cases, terms from common occupations became generalized: "invenīre" "to find" replaced by "afflāre" (orig. "to sniff out", in hunting, cf. Spanish "hallar", Portuguese "achar", Romansh dial. "anflar", Southern Italian "asciare", "acchiare", Romanian "afla" 'to find out'); "advenīre" "to arrive" gave way to "plicāre" (orig. "to fold (sails; tents)", cf. Spanish "llegar", Portuguese "chegar"; Romanian "pleca"), elsewhere "arripāre" (orig. "to harbor at a riverbank", cf. Italian "arrivare", French "arriver") ("advenīre" is continued with the meaning "to achieve, manage to do" as in Middle French "aveindre", or "to happen" in Italian "avvenire") . The same thing sometimes happened to religious terms, due to the pervasive influence of Christianity: "loquī" "to speak" succumbed to "parabolāre" (orig. "to tell parables", cf. Occitan "parlar", French "parler", Italian "parlare") or "fabulārī" ~ "fābellāre" (orig. "to tell stories", cf. Spanish "hablar", Dalmatian "favlur", Sardinian "faeddare"), based on Jesus' way of speaking in parables.

Many prepositions were used as verbal particles to make new roots and verb stems, e.g. Italian "estrarre", Aromanian "astragu", "astradziri" "to extract" from Latin "ex-" "out of" and "trahere" "to pull" (Italian "trarre" "draw, pull", Aromanian "tragu", "tradziri"), or to augment already existing words, e.g. French "coudre", Italian "cucire", Portuguese "coser" "to sew", from "cōnsuere" "to sew up", from "suere" "to sew", with total loss of the bare stem. Many prepositions and commonly became compounded, e.g. "de ex" > French "dès" "as of", "ab ante" > Italian "avanti" "forward". Some words derived from phrases, e.g. Portuguese "agora", Spanish "ahora" "now" < "hāc hōrā" "at this hour"; French "avec" "with" (prep.) < Old French "avuec" (adv.) < "apud hoc" "with that"; Spanish "tamaño", Portuguese "tamanho" "size" < "tam magnum" "so big"; Italian "codesto" "this, that" (near you) < Old Italian "cotevesto" < "eccum tibi istum" approx. "here's that thing of yours"; Portuguese "você" "you" < "vosmecê" < "vossemecê" < Galician-Portuguese "vossa mercee" "your mercy".

A number of common Latin words that have disappeared in many or most Romance languages have survived either in the periphery or in remote corners (especially Sardinia and Romania), or as secondary terms, sometimes differing in meaning. For example, Latin "caseum" "cheese" in the periphery (Portuguese "queijo", Spanish "queso", Romansh "caschiel", Sardinian "càsu", Romanian "caş"), but in the central areas has been replaced by "formāticum", originally "moulded (cheese)" (French "fromage", Occitan/Catalan "formatge", Italian "formaggio", with, however, "cacio" also available in much of Italy; similarly "(com)edere" "to eat (up)", which survives as Spanish/Portuguese "comer" but elsewhere is replaced by "mandūcāre", originally "to chew" (French "manger", Sardinian "mandicare" alongside "pappare", Romanian "mânca(re)"). In some cases, one language happens to preserve a word displaced elsewhere, e.g. Italian "ogni" "each, every" < "omnes", displaced elsewhere by "tōtum", originally "whole" or by a reflex of Greek "κατά" (e.g. Italian "ognuno", Catalan "tothom" "everyone"; Italian "ogni giorno", Spanish "cada día" "every day"); Friulian "vaî" "to cry" < "flere" "to weep"; Serbo-Croatian (Dubrovnik) "otijemna" "sail pole" < Dalmatian < "antenna" "yardarm". Sardinian in particular preserves many words entirely lost elsewhere, e.g. "eja" "yes" < "etiam" "also/yes/indeed", "emmo" "yes" < "immo" "rather/yes/no", "mannu" "big" < "magnus", "nàrrere" "to say" < "narrāre" "to tell", and "domo" "house" < (abl.) "domō" "at home". Sardinian preserves some words that were already archaic in Classical Latin, e.g. "àchina" "grape" < "acinam", while Sicily and Calabria typically have forms with initial /r/: "ràcina".

During the Middle Ages, scores of words were borrowed directly from Classical Latin (so-called Latinisms), either in their original form ("learned loans") or in a somewhat nativized form ("semi-learned loans"). These resulted in many doublets—pairs of inherited and learned words—such as those in the table below:

Sometimes triplets arise: Latin "articulus" "joint" > Portuguese "artículo" "joint, knuckle" (learned), "artigo" "article" (semi-learned), "artelho" "ankle" (inherited; archaic and dialectal). In many cases, the learned word simply displaced the original popular word: e.g. Spanish "crudo" "crude, raw" (Old Spanish "cruo"); French "légume" "vegetable" (Old French" leüm"); Portuguese "flor" "flower" (Galician-Portuguese "chor"). The learned loan always sounds (and, in writing, looks) more like the original than the inherited word does, because regular sound change has been bypassed; likewise, the learned word usually has a meaning closer to that of the original. In French, the stress of the modern form of the learned loan may be on the "wrong" syllable vis-à-vis Latin, whereas the stress of the inherited word always corresponds to the Latin stress: e.g. Latin "vipera" (stress on /i/) vs. French "vipère", learned loan, and "guivre/vouivre", inherited.

Borrowing from Classical Latin has produced a large number of suffix doublets. Examples from Spanish (learned form first): "-ción" vs. "-zon"; "-cia" vs. "-za"; "-ificar" vs. "-iguar"; "-izar" vs. "-ear"; "-mento" vs. "-miento"; "-tud" (< nominative "-tūdō") vs. "-dumbre" (< accusative "-tūdine"); "-ículo" vs. "-ejo"; etc. Similar examples can be found in all the other Romance languages.

This borrowing also introduced large numbers of classical prefixes in their original form ("dis-", "ex-", "post-", "trans"-) and reinforced many others ("re-", popular Spanish/Portuguese "des-" < "dis-", popular French "dé-" < "dis-", popular Italian "s-" < "ex-"). Many Greek prefixes and suffixes (hellenisms) also found their way into the lexicon: "tele-", "poli-/poly-", "meta-", "pseudo-", "-scope/scopo", "-logie/logia/logía", etc.

Significant sound changes affected the consonants of the Romance languages.

There was a tendency to eliminate final consonants in Vulgar Latin, either by dropping them (apocope) or adding a vowel after them (epenthesis).

Many final consonants were rare, occurring only in certain prepositions (e.g. "ad" "towards", "apud" "at, near (a person)"), conjunctions ("sed" "but"), demonstratives (e.g. "illud" "that (over there)", "hoc" "this"), and nominative singular noun forms, especially of neuter nouns (e.g. "lac" "milk", "mel" "honey", "cor" "heart"). Many of these prepositions and conjunctions were replaced by others, while the nouns were regularized into forms based on their oblique stems that avoided the final consonants (e.g. *"lacte", *"mele", *"core").

Final "-m" was dropped in Vulgar Latin. Even in Classical Latin, final "-am", "-em", "-um" (inflectional suffixes of the accusative case) were often elided in poetic meter, suggesting the "m" was weakly pronounced, probably marking the nasalisation of the vowel before it. This nasal vowel lost its nasalization in the Romance languages except in monosyllables, where it became e.g. Spanish "quien" < "quem" "whom", French "rien" "anything" < "rem" "thing"; note especially French and Catalan "mon" < "meum" "my (m.sg.)" pronounced as one syllable ( > *) but Spanish "mío" and Portuguese and Catalan "meu" < "meum" pronounced as two ( > *).

As a result, only the following final consonants occurred in Vulgar Latin:

Final "-t" was eventually dropped in many languages, although this often occurred several centuries after the Vulgar Latin period. For example, the reflex of "-t" was dropped in Old French and Old Spanish only around 1100. In Old French, this occurred only when a vowel still preceded the "t" (generally < Latin "a"). Hence "amat" "he loves" > Old French "aime" but "venit" "he comes" > Old French "vient": the was never dropped and survives into Modern French in liaison, e.g. "vient-il?" "is he coming?" (the corresponding in "aime-t-il?" is analogical, not inherited). Old French also kept the third-person plural ending "-nt" intact.

In Italo-Romance and the Eastern Romance languages, eventually "all" final consonants were either dropped or protected by an epenthetic vowel, except in clitic forms (e.g. prepositions "con", "per"). Modern Standard Italian still has almost no consonant-final words, although Romanian has resurfaced them through later loss of final and . For example, "amās" "you love" > "ame" > Italian "ami"; "amant" "they love" > *"aman" > Ital. "amano". On the evidence of "sloppily written" Lombardic language documents, however, the loss of final in Italy did not occur until the 7th or 8th century, after the Vulgar Latin period, and the presence of many former final consonants is betrayed by the syntactic gemination ("raddoppiamento sintattico") that they trigger. It is also thought that after a long vowel became rather than simply disappearing: "nōs" > "noi" "we", *ses > "sei" "you are", "crās" > "crai" "tomorrow" (southern Italian). In unstressed syllables, the resulting diphthongs were simplified: "canēs" > > "cani" "dogs"; "amīcās" > > "amiche" "(female) friends", where nominative "amīcae" should produce "**amice" rather than "amiche" (note masculine "amīcī" > "amici" not "**amichi").

Central Western Romance languages eventually regained a large number of final consonants through the general loss of final and , e.g. Catalan "llet" "milk" < "lactem", "foc" "fire" < "focum", "peix" "fish" < "piscem". In French, most of these secondary final consonants (as well as primary ones) were lost before around 1700, but tertiary final consonants later arose through the loss of < "-a". Hence masculine "frīgidum" "cold" > Old French "freit" > "froid" , feminine "frigidam" > Old French "freide" > "froide" .

Palatalization was one of the most important processes affecting consonants in Vulgar Latin. This eventually resulted in a whole series of "" and consonants in most Romance languages, e.g. Italian .

The following historical stages occurred:

Note how the environments become progressively less "palatal", and the languages affected become progressively fewer.

The outcomes of palatalization depended on the historical stage, the consonants involved, and the languages involved. The primary division is between the Western Romance languages, with resulting from palatalization of , and the remaining languages (Italo-Dalmatian and Eastern Romance), with resulting. It is often suggested that was the original result in all languages, with > a later innovation in the Western Romance languages. Evidence of this is the fact that Italian has both and as outcomes of palatalization in different environments, while Western Romance has only . Even more suggestive is the fact that the Mozarabic language in al-Andalus (modern southern Spain) had as the outcome despite being in the "Western Romance" area and geographically disconnected from the remaining areas; this suggests that Mozarabic was an outlying "relic" area where the change > failed to reach. (Northern French dialects, such as Norman and Picard, also had , but this may be a secondary development, i.e. due to a later sound change > .) Note that eventually became /s, z, ʒ/ in most Western Romance languages. Thus Latin "caelum" (sky, heaven), pronounced with an initial , became Italian "cielo" , Romanian "cer" , Spanish "cielo" /, French "ciel" , Catalan "cel" , and Portuguese "céu" .

The outcome of palatalized and is less clear:

This suggests that palatalized > > either or depending on location, while palatalized > ; after this, > in most areas, but Spanish and Gascon (originating from isolated districts behind the western Pyrenees) were relic areas unaffected by this change.

In French, the outcomes of palatalized by and by were different: "centum" "hundred" > "cent" but "cantum" "song" > "chant" . French also underwent palatalization of labials before : Vulgar Latin > Old French ("sēpia" "cuttlefish" > "seiche", "rubeus" "red" > "rouge", "sīmia" "monkey" > "singe").

The original outcomes of palatalization must have continued to be phonetically palatalized even after they had developed into //etc. consonants. This is clear from French, where all originally palatalized consonants triggered the development of a following glide in certain circumstances (most visible in the endings "-āre", "-ātum/ātam"). In some cases this came from a consonant palatalized by an adjoining consonant after the late loss of a separating vowel. For example, "mansiōnātam" > > > > early Old French "maisnieḍe" "household". Similarly, "mediētātem" > > > > early Old French "meitieḍ" > modern French "moitié" "half". In both cases, phonetic palatalization must have remained in primitive Old French at least through the time when unstressed intertonic vowels were lost (?8th century), well after the fragmentation of the Romance languages.

The effect of palatalization is indicated in the writing systems of almost all Romance languages, where the letters have the "hard" pronunciation in most situations, but a "soft" pronunciation (e.g. French/Portuguese , Italian/Romanian ) before . (This orthographic trait has passed into Modern English through Norman French-speaking scribes writing Middle English; this replaced the earlier system of Old English, which had developed its own hard-soft distinction with the soft representing .) This has the effect of keeping the modern spelling similar to the original Latin spelling, but complicates the relationship between sound and letter. In particular, the hard sounds must be written differently before (e.g. Italian , Portuguese ), and likewise for the soft sounds when not before these letters (e.g. Italian , Portuguese ). Furthermore, in Spanish, Catalan, Occitan and Brazilian Portuguese, the use of digraphs containing to signal the hard pronunciation before means that a different spelling is also needed to signal the sounds before these vowels (Spanish , Catalan, Occitan and Brazilian Portuguese ). This produces a number of orthographic alternations in verbs whose pronunciation is entirely regular. The following are examples of corresponding first-person plural indicative and subjunctive in a number of regular Portuguese verbs: "marcamos, marquemos" "we mark"; "caçamos, cacemos" "we hunt"; "chegamos, cheguemos" "we arrive"; "averiguamos, averigüemos" "we verify"; "adequamos, adeqüemos" "we adapt"; "oferecemos, ofereçamos" "we offer"; "dirigimos, dirijamos" "we drive" "erguemos, ergamos" "we raise"; "delinquimos, delincamos" "we commit a crime". In the case of Italian, the convention of digraphs <ch> and <gh> to represent /k/ and /g/ before written <e, i> results in similar orthographic alternations, such as "dimentico" 'I forget', "dimentichi" 'you forget', "baco" 'worm', "bachi" 'worms' with [k] or "pago" 'I pay', "paghi" 'you pay' and "lago" 'lake', "laghi" 'lakes' with [g]. The use in Italian of <ci> and <gi> to represent /tʃ/ or /dʒ/ before vowels written neatly distinguishes "dico" 'I say' with /k/ from "dici" 'you say' with /tʃ/ or "ghiro" 'dormouse' /g/ and "giro" 'turn, revolution' /dʒ/, but with orthographic <ci> and <gi> also representing the sequence of /tʃ/ or /dʒ/ and the actual vowel /i/ (/ditʃi/ "dici", /dʒiro/ "giro"), and no generally observed convention of indicating stress position, the status of "i" when followed by another vowel in spelling can be unrecognizable. For example, the written forms offer no indication that <cia> in "camicia" 'shirt' represents a single unstressed syllable /tʃa/ with no /i/ at any level (/kaˈmitʃa/ → [kaˈmiːtʃa] ~ [kaˈmiːʃa]), but that underlying the same spelling <cia> in "farmacia" 'pharmacy' is a bisyllabic sequence consisting of the stressed syllable /tʃi/ and syllabic /a/ (/farmaˈtʃia/ → [farmaˈtʃiːa] ~ [farmaˈʃiːa]).

Stop consonants shifted by lenition in Vulgar Latin in some areas.

The voiced labial consonants and (represented by and , respectively) both developed a fricative as an intervocalic allophone. This is clear from the orthography; in medieval times, the spelling of a consonantal is often used for what had been a in Classical Latin, or the two spellings were used interchangeably. In many Romance languages (Italian, French, Portuguese, Romanian, etc.), this fricative later developed into a ; but in others (Spanish, Galician, some Catalan and Occitan dialects, etc.) reflexes of and simply merged into a single phoneme.

Several other consonants were "softened" in intervocalic position in Western Romance (Spanish, Portuguese, French, Northern Italian), but normally not phonemically in the rest of Italy (except some cases of "elegant" or Ecclesiastical words), nor apparently at all in Romanian. The dividing line between the two sets of dialects is called the La Spezia–Rimini Line and is one of the most important isoglosses of the Romance dialects. The changes (instances of diachronic lenition resulting in phonological restructuring) are as follows:

Single voiceless plosives became voiced: "-p-, -t-, -c-" > "-b-, -d-, -g-". Subsequently, in some languages they were further weakened, either becoming fricatives or approximants, (as in Spanish) or disappearing entirely (as and , but not , in French). The following example shows progressive weakening of original /t/: e.g. "vītam" > Italian "vita" , Portuguese "vida" (European Portuguese ), Spanish "vida" (Southern Peninsular Spanish ), and French "vie" . Some scholars once speculated that these sound changes may be due in part to the influence of Continental Celtic languages, but scholarship of the past few decades challenges that hypothesis.

Consonant length is no longer phonemically distinctive in most Romance languages. However some languages of Italy (Italian, Sardinian, Sicilian, and numerous other varieties of central and southern Italy) do have long consonants like , , etc., where the doubling indicates either actual length or, in the case of plosives and affricates, a short hold before the consonant is released, in many cases with distinctive lexical value: e.g. "note" (notes) vs. "notte" (night), "cade" (s/he, it falls) vs. "cadde" (s/he, it fell), "caro" (dear, expensive) vs. "carro" (cart). They may even occur at the beginning of words in Romanesco, Neapolitan, Sicilian and other southern varieties, and are occasionally indicated in writing, e.g. Sicilian "cchiù" (more), and "ccà" (here). In general, the consonants , , and are long at the start of a word, while the archiphoneme is realised as a trill in the same position. In much of central and southern Italy, the affricates /t͡ʃ/ and /d͡ʒ/ weaken synchronically to fricative [ʃ] and [ʒ] between vowels, while their geminate congeners do not, e.g. "cacio" (cheese) vs. "caccio" (I chase).

A few languages have regained secondary geminate consonants. The double consonants of Piedmontese exist only after stressed , written "ë", and are not etymological: "vëdde" (Latin "vidēre", to see), "sëcca" (Latin "sicca", dry, feminine of "sech"). In standard Catalan and Occitan, there exists a geminate sound written "ŀl" (Catalan) or "ll" (Occitan), but it is usually pronounced as a simple sound in colloquial (and even some formal) speech in both languages.

In Late Latin a prosthetic vowel /i/ (lowered to /e/ in most languages) was inserted at the beginning of any word that began with (referred to as "s impura") and a voiceless consonant (#sC- > isC-): 
Prosthetic /i/ ~ /e/ in Romance languages may have been influenced by Continental Celtic languages, although the phenomenon exists or existed in some areas where Celtic was never present (e.g. Sardinia, southern Italy). While Western Romance words undergo prothesis, cognates in Balkan Romance and southern Italo-Romance do not, e.g. Italian "scrivere", "spada", "spirito", "Stefano", and "stato". In Italian, syllabification rules were preserved instead by vowel-final articles, thus feminine "spada" as "la spada", but instead of rendering the masculine "*il spaghetto", "lo spaghetto" came to be the norm. Though receding at present, Italian once had a prosthetic if a consonant preceded such clusters, so that 'in Switzerland' was "in" "Svizzera". Some speakers still use the prothetic productively, and it is fossilized in a few set locutions such as "in ispecie" 'especially' or "per iscritto" 'in writing' (although in this case its survival may be due partly to the influence of the separate word "iscritto" < Latin "īnscrīptus"). The association of /i/ ~ /j/ and /s/ also led to the vocalization of word-final -"s" in Italian, Romanian, certain Occitan dialects, and the Spanish dialect of Chocó in Colombia.

One profound change that affected Vulgar Latin was the reorganisation of its vowel system. Classical Latin had five short vowels, "ă, ĕ, ĭ, ŏ, ŭ", and five long vowels, "ā, ē, ī, ō, ū", each of which was an individual phoneme (see the table in the right, for their likely pronunciation in IPA), and four diphthongs, "ae", "oe", "au" and "eu" (five according to some authors, including "ui"). There were also long and short versions of "y", representing the rounded vowel in Greek borrowings, which however probably came to be pronounced even before Romance vowel changes started.

There is evidence that in the imperial period all the short vowels except "a" differed by quality as well as by length from their long counterparts. So, for example "ē" was pronounced close-mid while "ĕ" was pronounced open-mid , and "ī" was pronounced close while "ĭ" was pronounced near-close .

During the Proto-Romance period, phonemic length distinctions were lost. Vowels came to be automatically pronounced long in stressed, open syllables (i.e. when followed by only one consonant), and pronounced short everywhere else. This situation is still maintained in modern Italian: "cade" "he falls" vs. "cadde" "he fell".

The Proto-Romance loss of phonemic length originally produced a system with nine different quality distinctions in monophthongs, where only original had merged. Soon, however, many of these vowels coalesced:

The Proto-Romance allophonic vowel-length system was rephonemicized in the Gallo-Romance languages as a result of the loss of many final vowels. Some northern Italian languages (e.g. Friulan) still maintain this secondary phonemic length, but most languages dropped it by either diphthongizing or shortening the new long vowels.

French phonemicized a third vowel length system around AD 1300 as a result of the sound change /VsC/ > /VhC/ > (where "V" is any vowel and "C" any consonant). This vowel length was eventually lost by around AD 1700, but the former long vowels are still marked with a circumflex. A fourth vowel length system, still non-phonemic, has now arisen: All nasal vowels as well as the oral vowels (which mostly derive from former long vowels) are pronounced long in all stressed closed syllables, and all vowels are pronounced long in syllables closed by the voiced fricatives . This system in turn has been phonemicized in some non-standard dialects (e.g. Haitian Creole), as a result of the loss of final .

The Latin diphthongs "ae" and "oe", pronounced and in earlier Latin, were early on monophthongized.

"ae" became by the 1st century at the latest. Although this sound was still distinct from all existing vowels, the neutralization of Latin vowel length eventually caused its merger with < short "e": e.g. "caelum" "sky" > French "ciel", Spanish/Italian "cielo", Portuguese "céu" , with the same vowel as in "mele" "honey" > French/Spanish "miel", Italian "miele", Portuguese "mel" . Some words show an early merger of "ae" with , as in "praeda" "booty" > *"prēda" > French "proie" (vs. expected **"priée"), Italian "preda" (not **"prieda") "prey"; or "faenum" "hay" > *"fēnum" > Spanish "heno", French "foin" (but Italian "fieno" /fjɛno/).

"oe" generally merged with : "poenam" "punishment" > Romance * > Spanish/Italian "pena", French "peine"; "foedus" "ugly" > Romance * > Spanish "feo", Portuguese "feio". There are relatively few such outcomes, since "oe" was rare in Classical Latin (most original instances had become Classical "ū", as in Old Latin "oinos" "one" > Classical "ūnus") and so "oe" was mostly limited to Greek loanwords, which were typically learned (high-register) terms.

"au" merged with "ō" in the popular speech of Rome already by the 1st century . A number of authors remarked on this explicitly, e.g. Cicero's taunt that the populist politician Publius Clodius Pulcher had changed his name from "Claudius" to ingratiate himself with the masses. This change never penetrated far from Rome, however, and the pronunciation /au/ was maintained for centuries in the vast majority of Latin-speaking areas, although it eventually developed into some variety of "o" in many languages. For example, Italian and French have as the usual reflex, but this post-dates diphthongization of and the French-specific palatalization > (hence "causa" > French "chose", Italian "cosa" not **"cuosa"). Spanish has , but Portuguese spelling maintains , which has developed to (and still remains as in some dialects, and in others). Occitan, Romanian, southern Italian languages, and many other minority Romance languages still have . A few common words, however, show an early merger with "ō" , evidently reflecting a generalization of the popular Roman pronunciation: e.g. French "queue", Italian "coda" , Occitan "co(d)a", Romanian "coadă" (all meaning "tail") must all derive from "cōda" rather than Classical "cauda" (but notice Portuguese "cauda"). Similarly, Spanish "oreja", Portuguese "orelha", French "oreille", Romanian "ureche", and Sardinian "olícra", "orícla" "ear" must derive from "ōric(u)la" rather than Classical "auris" (Occitan "aurelha" was probably influenced by the unrelated "ausir" < "audīre" "to hear"), and the form "oricla" is in fact reflected in the Appendix Probi.

An early process that operated in all Romance languages to varying degrees was metaphony (vowel mutation), conceptually similar to the umlaut process so characteristic of the Germanic languages. Depending on the language, certain stressed vowels were raised (or sometimes diphthongized) either by a final /i/ or /u/ or by a directly following /j/. Metaphony is most extensive in the Italo-Romance languages, and applies to nearly all languages in Italy; however, it is absent from Tuscan, and hence from standard Italian. In many languages affected by metaphony, a distinction exists between final /u/ (from most cases of Latin "-um") and final /o/ (from Latin "-ō", "-ud" and some cases of "-um", esp. masculine "mass" nouns), and only the former triggers metaphony.

Some examples:

A number of languages diphthongized some of the free vowels, especially the open-mid vowels :

These diphthongizations had the effect of reducing or eliminating the distinctions between open-mid and close-mid vowels in many languages. In Spanish and Romanian, all open-mid vowels were diphthongized, and the distinction disappeared entirely. Portuguese is the most conservative in this respect, keeping the seven-vowel system more or less unchanged (but with changes in particular circumstances, e.g. due to metaphony). Other than before palatalized consonants, Catalan keeps intact, but split in a complex fashion into and then coalesced again in the standard dialect (Eastern Catalan) in such a way that most original have reversed their quality to become .

In French and Italian, the distinction between open-mid and close-mid vowels occurred only in closed syllables. Standard Italian more or less maintains this. In French, /e/ and merged by the twelfth century or so, and the distinction between and was eliminated without merging by the sound changes , . Generally this led to a situation where both and occur allophonically, with the close-mid vowels in open syllables and the open-mid vowels in closed syllables. This is still the situation in modern Spanish, for example. In French, however, both and were partly rephonemicized: Both and occur in open syllables as a result of , and both and occur in closed syllables as a result of .

Old French also had numerous falling diphthongs resulting from diphthongization before palatal consonants or from a fronted /j/ originally following palatal consonants in Proto-Romance or later: e.g. "pācem" /patsʲe/ "peace" > PWR */padzʲe/ (lenition) > OF "paiz" /pajts/; *"punctum" "point" > Gallo-Romance */ponʲto/ > */pojɲto/ (fronting) > OF "point" /põjnt/. During the Old French period, preconsonantal /l/ [ɫ] vocalized to /w/, producing many new falling diphthongs: e.g. "dulcem" "sweet" > PWR */doltsʲe/ > OF "dolz" /duɫts/ > "douz" /duts/; "fallet" "fails, is deficient" > OF "falt" > "faut" "is needed"; "bellus" "beautiful" > OF "bels" > "beaus" . By the end of the Middle French period, "all" falling diphthongs either monophthongized or switched to rising diphthongs: proto-OF > early OF > modern spelling > mod. French .

In both French and Portuguese, nasal vowels eventually developed from sequences of a vowel followed by a nasal consonant (/m/ or /n/). Originally, all vowels in both languages were nasalized before any nasal consonants, and nasal consonants not immediately followed by a vowel were eventually dropped. In French, nasal vowels before remaining nasal consonants were subsequently denasalized, but not before causing the vowels to lower somewhat, e.g. "dōnat" "he gives" > OF "dune" > "donne" , "fēminam" > "femme" . Other vowels remained diphthongized, and were dramatically lowered: "fīnem" "end" > "fin" (often pronounced ); "linguam" "tongue" > "langue" ; "ūnum" "one" > "un" .

In Portuguese, /n/ between vowels was dropped, and the resulting hiatus eliminated through vowel contraction of various sorts, often producing diphthongs: "manum, *manōs" > PWR *"manu, ˈmanos" "hand(s)" > "mão, mãos" ; "canem, canēs" "dog(s)" > PWR *"kane, ˈkanes" > *"can, ˈcanes" > "cão, cães" ; "ratiōnem, ratiōnēs" "reason(s)" > PWR *"raˈdʲzʲone, raˈdʲzʲones" > *"raˈdzon, raˈdzones" > "razão, razões" (Brazil), (Portugal). Sometimes the nasalization was eliminated: "lūna" "moon" > Galician-Portuguese "lũa" > "lua"; "vēna" "vein" > Galician-Portuguese "vẽa" > "veia". Nasal vowels that remained actually tend to be raised (rather than lowered, as in French): "fīnem" "end" > "fim" ; "centum" "hundred" > PWR "tʲsʲɛnto" > "cento" ; "pontem" "bridge" > PWR "pɔnte" > "ponte" (Brazil), (Portugal). In Portugal, vowels before a nasal consonant have become denasalized, but in Brazil they remain heavily nasalized.

Characteristic of the Gallo-Romance languages and Rhaeto-Romance languages are the front rounded vowels . All of these languages show an unconditional change /u/ > /y/, e.g. "lūnam" > French "lune" , Occitan . Many of the languages in Switzerland and Italy show the further change /y/ > /i/. Also very common is some variation of the French development (lengthened in open syllables) > > , with mid back vowels diphthongizing in some circumstances and then re-monophthongizing into mid-front rounded vowels. (French has both and , with developing from in certain circumstances.)

There was more variability in the result of the unstressed vowels. Originally in Proto-Romance, the same nine vowels developed in unstressed as stressed syllables, and in Sardinian, they coalesced into the same five vowels in the same way.

In Italo-Western Romance, however, vowels in unstressed syllables were significantly different from stressed vowels, with yet a third outcome for final unstressed syllables. In non-final unstressed syllables, the seven-vowel system of stressed syllables developed, but then the low-mid vowels merged into the high-mid vowels . This system is still preserved, largely or completely, in all of the conservative Romance languages (e.g. Italian, Spanish, Portuguese, Catalan).

In final unstressed syllables, results were somewhat complex. One of the more difficult issues is the development of final short "-u", which appears to have been raised to rather than lowered to , as happened in all other syllables. However, it is possible that in reality, final comes from "long" *"-ū" < "-um", where original final "-m" caused vowel lengthening as well as nasalization. Evidence of this comes from Rhaeto-Romance, in particular Sursilvan, which preserves reflexes of both final "-us" and "-um", and where the latter, but not the former, triggers metaphony. This suggests the development "-us" > > , but "-um" > > .

The original five-vowel system in final unstressed syllables was preserved as-is in some of the more conservative central Italian languages, but in most languages there was further coalescence:

Various later changes happened in individual languages, e.g.:

The so-called "intertonic vowels" are word-internal unstressed vowels, i.e. not in the initial, final, or "tonic" (i.e. stressed) syllable, hence intertonic. Intertonic vowels were the most subject to loss or modification. Already in Vulgar Latin intertonic vowels between a single consonant and a following /r/ or /l/ tended to drop: "vétulum" "old" > "veclum" > Dalmatian "vieklo", Sicilian "vecchiu", Portuguese "velho". But many languages ultimately dropped almost all intertonic vowels.

Generally, those languages south and east of the La Spezia–Rimini Line (Romanian and Central-Southern Italian) maintained intertonic vowels, while those to the north and west (Western Romance) dropped all except /a/. Standard Italian generally maintained intertonic vowels, but typically raised unstressed /e/ > /i/. Examples:
Portuguese is more conservative in maintaining some intertonic vowels other than /a/: e.g. *"offerḗscere" "to offer" > Portuguese "oferecer" vs. Spanish "ofrecer", French "offrir" (< *"offerīre"). French, on the other hand, drops even intertonic /a/ after the stress: "Stéphanum" "Stephen" > Spanish "Esteban" but Old French "Estievne" > French "Étienne". Many cases of /a/ before the stress also ultimately dropped in French: "sacraméntum" "sacrament" > Old French "sairement" > French "serment" "oath".

The Romance languages for the most part have kept the writing system of Latin, adapting it to their evolution.
One exception was Romanian before the nineteenth century, where, after the Roman retreat, literacy was reintroduced through the Romanian Cyrillic alphabet, a Slavic influence. A Cyrillic alphabet was also used for Romanian (Moldovan) in the USSR. The non-Christian populations of Spain also used the scripts of their religions (Arabic and Hebrew) to write Romance languages such as Ladino and Mozarabic in "aljamiado".

The Romance languages are written with the classical Latin alphabet of 23 letters – "A", "B", "C", "D", "E", "F", "G", "H", "I", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "V", "X", "Y", "Z" – subsequently modified and augmented in various ways. In particular, the single Latin letter "V" split into "V" (consonant) and "U" (vowel), and the letter "I" split into "I" and "J". The Latin letter "K" and the new letter "W", which came to be widely used in Germanic languages, are seldom used in most Romance languages – mostly for unassimilated foreign names and words. Indeed, in Italian prose is properly . Catalan eschews importation of "foreign" letters more than most languages. Thus Wikipedia is in Catalan but in Spanish.

While most of the 23 basic Latin letters have maintained their phonetic value, for some of them it has diverged considerably; and the new letters added since the Middle Ages have been put to different uses in different scripts. Some letters, notably "H" and "Q", have been variously combined in digraphs or trigraphs (see below) to represent phonetic phenomena that could not be recorded with the basic Latin alphabet, or to get around previously established spelling conventions. Most languages added auxiliary marks (diacritics) to some letters, for these and other purposes.

The spelling rules of most Romance languages are fairly simple, and consistent within any language. Since the spelling systems are based on phonemic structures rather than phonetics, however, the actual pronunciation of what is represented in standard orthography can be subject to considerable regional variation, as well as to allophonic differentiation by position in the word or utterance. Among the letters representing the most conspicuous phonological variations, between Romance languages or with respect to Latin, are the following:

Otherwise, letters that are not combined as digraphs generally represent the same phonemes as suggested by the International Phonetic Alphabet (IPA), whose design was, in fact, greatly influenced by Romance spelling systems.

Since most Romance languages have more sounds than can be accommodated in the Roman Latin alphabet they all resort to the use of digraphs and trigraphs – combinations of two or three letters with a single phonemic value. The concept (but not the actual combinations) is derived from Classical Latin, which used, for example, "TH", "PH", and "CH" when transliterating the Greek letters "θ", "ϕ" (later "φ"), and "χ". These were once aspirated sounds in Greek before changing to corresponding fricatives, and the "H" represented what sounded to the Romans like an following , , and respectively. Some of the digraphs used in modern scripts are:

While the digraphs "CH", "PH", "RH" and "TH" were at one time used in many words of Greek origin, most languages have now replaced them with "C/QU", "F", "R" and "T". Only French has kept these etymological spellings, which now represent or , , and , respectively.

Gemination, in the languages where it occurs, is usually indicated by doubling the consonant, except when it does not contrast phonemically with the corresponding short consonant, in which case gemination is not indicated. In Jèrriais, long consonants are marked with an apostrophe: is a long , is a long , and is a long . The phonemic contrast between geminate and single consonants is widespread in Italian, and normally indicated in the traditional orthography: 'done' vs. 'fate, destiny'; 's/he, it fell' vs. 's/he, it falls'. The double consonants in French orthography, however, are merely etymological. In Catalan, the gemination of is marked by a ("flying point"): .

Romance languages also introduced various marks (diacritics) that may be attached to some letters, for various purposes. In some cases, diacritics are used as an alternative to digraphs and trigraphs; namely to represent a larger number of sounds than would be possible with the basic alphabet, or to distinguish between sounds that were previously written the same. Diacritics are also used to mark word stress, to indicate exceptional pronunciation of letters in certain words, and to distinguish words with same pronunciation (homophones).

Depending on the language, some letter-diacritic combinations may be considered distinct letters, e.g. for the purposes of lexical sorting. This is the case, for example, of Romanian "ș" () and Spanish "ñ" ().

The following are the most common use of diacritics in Romance languages.


Most languages are written with a mixture of two distinct but phonetically identical variants or "cases" of the alphabet: majuscule ("uppercase" or "capital letters"), derived from Roman stone-carved letter shapes, and minuscule ("lowercase"), derived from Carolingian writing and Medieval quill pen handwriting which were later adapted by printers in the fifteenth and sixteenth centuries.

In particular, all Romance languages capitalize (use uppercase for the first letter of) the following words: the first word of each complete sentence, most words in names of people, places, and organizations, and most words in titles of books. The Romance languages do not follow the German practice of capitalizing all nouns including common ones. Unlike English, the names of months, days of the weeks, and derivatives of proper nouns are usually not capitalized: thus, in Italian one capitalizes "Francia" ("France") and "Francesco" ("Francis"), but not "francese" ("French") or "francescano" ("Franciscan"). However, each language has some exceptions to this general rule.

The tables below provide a vocabulary comparison that illustrates a number of examples of sound shifts that have occurred between Latin and Romance languages. Words are given in their conventional spellings. In addition, for French the actual pronunciation is given, due to the dramatic differences between spelling and pronunciation. (French spelling approximately reflects the pronunciation of Old French, c. 1200 AD.)


Overviews:

Phonology:

Lexicon:

French:

Portuguese:

Spanish:

Italian:

Rhaeto-Romance:



</doc>
<doc id="25402" url="https://en.wikipedia.org/wiki?curid=25402" title="Rugby football">
Rugby football

Rugby football is a collective name for the team sports of rugby league and rugby union, as well as the earlier forms of football from which both games evolved. Canadian football, and to a lesser extent American football were also broadly considered forms of rugby football but are seldom now referred to as such 

Rugby football started about 1845 at Rugby School in Rugby, Warwickshire, England, although forms of football in which the ball was carried and tossed date to medieval times. Rugby split into two sports in 1895, when twenty-one clubs split from the Rugby Football Union to form the Northern Rugby Football Union (later renamed the Rugby Football League in 1922) in the George Hotel, Huddersfield, over broken-time payments to players who took time off from 
work to play the sport, thus making rugby league the first code to turn professional and pay players. Rugby union turned professional one hundred years later in 1995, following the 1995 Rugby World Cup in South Africa. The respective world governing bodies are World Rugby (rugby union) and the Rugby League International Federation (rugby league).

Rugby football was one of many versions of football played at English public schools in the 19th century. Although rugby league initially used rugby union rules, they are now wholly separate sports. In addition to these two codes, both American and Canadian football evolved from rugby football in the beginning of the 20th century.

Following the 1895 split in rugby football, the two forms rugby league and rugby union differed in administration only. Soon the rules of rugby league were modified, resulting in two distinctly different forms of rugby. 100 years later, rugby union joined rugby league and most other forms of football as an openly professional sport.

The Olympic form of rugby is known as Rugby Sevens. In this form of the game, each team has seven players on the field at one time playing seven-minute halves. The rules and pitch size are the same as rugby union.

The Greeks and Romans are known to have played many ball games, some of which involved the use of the feet. The Roman game "harpastum" is believed to have been adapted from a Greek team game known as "ἐπίσκυρος" ("Episkyros") or "φαινίνδα" ("phaininda"), which is mentioned by a Greek playwright, Antiphanes (388–311 BC) and later referred to by the Christian theologian Clement of Alexandria (c.150-c.215 AD). These games appear to have resembled rugby football. The Roman politician Cicero (106–42 BC) describes the case of a man who was killed whilst having a shave when a ball was kicked into a barber's shop. Roman ball games already knew the air-filled ball, the follis. "Episkyros" is recognised as an early form of football by FIFA.

In 1871, English clubs met to form the Rugby Football Union (RFU). In 1892, after charges of professionalism (compensation of team members) were made against some clubs for paying players for missing work, the Northern Rugby Football Union, usually called the Northern Union (NU), was formed. The existing rugby union authorities responded by issuing sanctions against the clubs, players, and officials involved in the new organization. After the schism, the separate clubs were named "rugby league" and "rugby union".

Rugby union is both a professional and amateur game, and is dominated by the first tier unions: New Zealand, Ireland, Wales, England, South Africa, Australia, Argentina, Scotland, Italy and France. Second and third tier unions include Belgium, Brazil, Canada, Chile, Fiji, Georgia, Germany, Hong Kong, Japan, Kenya, Namibia, the Netherlands, Portugal, Romania, Russia, Samoa, Spain, Tonga, the United States and Uruguay. Rugby Union is administered by World Rugby (WR), whose headquarters are located in Dublin, Ireland. It is the national sport in New Zealand, Wales, Fiji, Samoa, Tonga, Georgia and Madagascar, and is the most popular form of rugby globally. The Olympic Games have admitted the seven-a-side version of the game, known as Rugby sevens, into the programme from Rio de Janeiro in 2016 onwards. There was a possibility sevens would be a demonstration sport at the 2012 London Olympics but many sports including sevens were dropped.

In Canada and the United States, rugby developed into gridiron football. During the late 1800s (and even the early 1900s), the two forms of the game were very similar (to the point where the United States was able to win the gold medal for rugby union at the 1924 Summer Olympics), but numerous rule changes have differentiated the gridiron-based game from its rugby counterpart, introduced by Walter Camp in the United States and John Thrift Meldrum Burnside in Canada. Among unique features of the North American game are the separation of play into downs instead of releasing the ball immediately upon tackling, the requirement that the team with the ball set into a set formation for at least one second before resuming play after a tackle (and the allowance of up to 40 seconds to do so), the allowance for one forward pass from behind the site of the last tackle on each down, the evolution of hard plastic equipment (particularly the football helmet and shoulder pads), a smaller and pointier ball that is favorable to being passed but makes drop kicks impractical, a generally smaller and narrower field measured in customary units instead of metric (in some variants of the American game a field can be as short as 50 yards between end zones), and a distinctive field (shaped like a gridiron, from which the code's nickname is derived) with lines marked in five-yard intervals.

Rugby league is also both a professional and amateur game, administered on a global level by the Rugby League International Federation. In addition to amateur and semi-professional competitions in the United States, Russia, Lebanon, Serbia, Europe and Australasia, there are two major professional competitions—the Australasian National Rugby League and the Super League. International Rugby League is dominated by Australia, England and New Zealand. In Papua New Guinea and New Zealand, it is the national sport. Other nations from the South Pacific and Europe also play in the Pacific Cup and European Cup respectively.

Distinctive features common to both rugby codes include the oval ball and throwing the ball forward is not allowed so that players can gain ground only by running with the ball or by kicking it. As the sport of rugby league moved further away from its union counterpart, rule changes were implemented with the aim of making a faster-paced and more try-oriented game. Unlike American and Canadian football, the players do not wear any sort of protection or armour.

The main differences between the two games, besides league having teams of 13 players and union of 15, involve the tackle and its aftermath:

Set pieces of the union code include the "scrum", which occurs after a minor infringement of the rules (most often a knock-on, when a player knocks the ball forward), where packs of opposing players push against each other for possession, and the "line-out", in which parallel lines of players from each team, arranged perpendicular to the touch-line, attempt to catch the ball thrown from touch. A rule has been added to line-outs which allows the jumper to be pulled down once a players' feet are on the ground.

In the league code, the scrum still exists, but with greatly reduced importance as it involves fewer players and is rarely contested. Set pieces are generally started from the play-the-ball situation. Many of the rugby league positions have names and requirements similar to rugby union positions, but there are no flankers in rugby league.

In England, rugby union is widely regarded as an "establishment" sport, played mostly by members of the upper and middle classes. For example, many pupils at public schools and grammar schools play rugby union, although the game (which had a long history of being played at state schools until the 1980s) is becoming increasingly popular in comprehensive schools. Despite this stereotype, the game, particularly in the West Country is popular amongst all classes. In contrast, rugby league has traditionally been seen as a working-class pursuit. Another exception to rugby union's upper-class stereotype is in Wales, where it has been traditionally associated with small village teams made up of coal miners and other industrial workers who played on their days off. In Ireland, both rugby union and rugby league are unifying forces across the national and sectarian divide, with the Ireland international teams representing both political entities.

In Australia, support for both codes is concentrated in New South Wales, Queensland and the Australian Capital Territory. The same perceived class barrier as exists between the two games in England also occurs in these states, fostered by rugby union's prominence and support at private schools.

Exceptions to the above include New Zealand (although rugby league is still considered to be a lower class game by many or a game for 'westies' referring to lower class western suburbs of Auckland and more recently, southern Auckland where the game is also popular), Wales, France (except Paris), Cornwall, Gloucestershire, Somerset, Scottish Borders, County Limerick (see Munster) and the Pacific Islands, where rugby union is popular in working class communities. Nevertheless, rugby league is perceived as the game of the working-class people in northern England and in the Australian states of New South Wales and Queensland.

In the United Kingdom, rugby union fans sometimes used the term "rugger" as an alternative name for the sport, (see Oxford '-er'), although this archaic expression has not had currency since the 1950s or earlier. New Zealanders refer to rugby union simply as either "rugby" or "union", or even simply "football", and to rugby league as "rugby league" or "league". In the U.S., people who play rugby are sometimes called "ruggers", a term little used elsewhere except facetiously.

In France, rugby is widely played and has a strong tradition in the Basque, Occitan and Catalan areas along the border regions between Spain and France. The game is very popular in South Africa, having been introduced by English-speaking settlers in the 19th century. British colonists also brought the game with them to Australia and New Zealand, where the game is widely played. It has spread thence to much of Polynesia, having particularly strong followings in Fiji, Samoa, and Tonga. Rugby union continues to grow in the Americas and parts of Asia as well.

About a quarter of rugby players are injured in each season.

A rugby ball, originally called a quanco, is a diamond shape ball used for easier passing.
Richard Lindon and Bernardo Solano started making balls for Rugby school out of hand stitched, four-panel, leather casings and pigs’ bladders. The rugby ball's distinctive shape is supposedly due to the pig's bladder, although early balls were more plum-shaped than oval. The balls varied in size in the beginning depending upon how large the pig's bladder was.

In rugby union, World Rugby regulates the size and shape of the ball under Law 2 (also known as Law E.R.B); an official rugby union ball is oval and made of four panels, has a length in-line of 280–300 millimetres, a circumference (end to end) of 740–770 millimetres, and a circumference (in width) of 580–620 millimetres. It is made of leather or suitable synthetic material and may be treated to make it water resistant and easier to grip. The rugby ball may not weigh more than 460 grams or less than 410 and has an air pressure of 65.71–68.75 kilopascals, or 0.67–0.70 kilograms per square centimetre, or 9.5–10.0 lbs per square inch. Spare balls are allowed under the condition that players or teams do not seek an advantage by changing the ball. Smaller sized balls may also be used in games between younger players.
Much larger versions of traditional balls are also available for purchase, but these are mainly for their novelty attraction.

The Rugby World Cup, which was first held in New Zealand and Australia in 1987, occurs every four years. It is an international tournament organized by World Rugby. The event is played in the union format and features the top 20 teams from around the world. The current world champions are South Africa, who won the 2019 Rugby World Cup, which was played in Japan.

The Rugby League World Cup was the first World Cup of either of the Rugby codes and was first held in France in 1954, and as of 2013 occurs on a 4-year cycle. It is an international tournament that is organized by the Rugby League International Federation. The event is played in the league format and features the top 14 teams from around the world. The current world champions are Australia, who won the World Cup in 2017, played in Australia, New Zealand and Papua New Guinea.

Rugby shirts were formerly made of cotton but are now made of a cotton and polyester mix. This material has the advantage of not absorbing as much water or mud as cotton alone. Owing to the more aggressive nature of the game, rugby clothing, in general, is designed to be much more robust and hardwearing than that worn for association football.

The rugby jerseys are slightly different depending on the type of rugby game played. The shirts worn by rugby league footballers commonly have a large "V" around the neck. The players in rugby union wear jerseys with a more traditional design, sometimes completely white (Cahors Rugby in France). The number of the player and his or her surname are placed on the upper back of the jersey (often name above number, with the number being significantly larger and more central), and the logo of the team on the upper left chest.

With the popularity of rugby over the years, many betting establishments have made it possible for viewers of the game to place wagers on games. The various types of wagers that can be placed on games vary, however, the main types of bets that can be placed are as follows:

Like most team sports, both forms of rugby are vulnerable to match-fixing, particularly bets involving easily manipulated outcomes, such as conceding penalties and first point scorer. A recent example is a deliberate infringement by Ryan Tandy in order for the first points scored to be a penalty goal in a 2010 NRL match; the attempt backfired when instead of taking a shot at goal, a try was scored.



</doc>
<doc id="25403" url="https://en.wikipedia.org/wiki?curid=25403" title="Russian">
Russian

Russian refers to anything related to Russia, including:


Russian may also refer to:



</doc>
<doc id="25405" url="https://en.wikipedia.org/wiki?curid=25405" title="Rugby union">
Rugby union

Rugby union, widely known simply as rugby, is a contact team sport that originated in England in the first half of the 19th century. One of the two codes of rugby football, it is based on running with the ball in hand. In its most common form, a game is played between two teams of 15 players using an oval-shaped ball on a rectangular field with H-shaped goalposts at either end.

Rugby union is a popular sport around the world, played by male and female players of all ages. In 2014, there were more than 6 million people playing worldwide, of whom 2.36 million were registered players. World Rugby, previously called the International Rugby Football Board (IRFB) and the International Rugby Board (IRB), has been the governing body for rugby union since 1886, and currently has 101 countries as full members and 18 associate members.

In 1845, the first football laws were written by pupils at Rugby School; other significant events in the early development of rugby include the decision by Blackheath F.C. to leave the Football Association in 1863 and the split between rugby union and rugby league in 1895. Historically an amateur sport, in 1995 restrictions on payments to players were removed, making the game openly professional at the highest level for the first time.

Rugby union spread from the Home Nations of Great Britain and Ireland and was embraced by many of the countries associated with the British Empire. Early exponents of the sport included Australia, New Zealand, South Africa and France. Countries that have adopted rugby union as their "de facto" national sport include Fiji, Georgia, Madagascar, New Zealand, Samoa, and Tonga.

International matches have taken place since 1871 when the first game was played between Scotland and England at Raeburn Place in Edinburgh. The Rugby World Cup, first held in 1987, is contested every four years. The Six Nations Championship in Europe and The Rugby Championship in the Southern Hemisphere are other major international competitions that are held annually.

National club and provincial competitions include the Premiership in England, the Top 14 in France, the Mitre 10 Cup in New Zealand, the National Rugby Championship in Australia, and the Currie Cup in South Africa. Other transnational club competitions include the European Rugby Champions Cup, the Pro14 in Europe and South Africa, and Super Rugby in the Southern Hemisphere and Japan.

The origin of rugby football is reputed to be an incident during a game of English school football at Rugby School in Warwickshire in 1823, when William Webb Ellis is said to have picked up the ball and run with it. Although the story may be apocryphal, it was immortalised at the school with a commemorative plaque that was unveiled in 1895, and the Rugby World Cup trophy is named after Webb Ellis. Rugby football stems from the form of the game played at Rugby School, which former pupils then introduced to their universities.

Former Rugby School student Albert Pell is credited with having formed the first "football" team while a student at Cambridge University. The schools used different rules during this early period, with former pupils from Rugby and Eton attempting to carry their preferred rules through to their universities. A significant event in the early development of rugby football was the production of a written set of rules at Rugby School in 1845, followed by the Cambridge Rules that were drawn up in 1848.

Formed in 1863, The Football Association (FA) began codifying a set of football rules. These rules did not allow players to run with the ball in hand and also banned hacking (kicking players in the shins). In protest of these two rules, the Blackheath Club left the FA followed by several other clubs that also favoured the "Rugby Rules". Although these clubs decided to ban hacking soon afterwards, the split was permanent and the FA's codified rules became "association football". Clubs that favoured the Rugby Rules formed the Rugby Football Union in 1871, and their code became known as "rugby football".

In 1895, there was a schism in England in which clubs from Northern England left the RFU over the issue of paying players. This led to the creation of the separate code of "rugby league". The existing sport took on the name "rugby union" to differentiate it from rugby league, but both versions of the sport are known simply as "rugby" throughout most of the world.

The first rugby football international was played on 27 March 1871 between Scotland and England in Edinburgh. Scotland won the game 1-0. By 1881 both Ireland and Wales had representative teams and in 1883 the first international competition, the Home Nations Championship had begun. 1883 is also the year of the first rugby sevens tournament, the Melrose Sevens, which is still held annually.

Two important overseas tours took place in 1888: a British Isles team visited Australia and New Zealand—although a private venture, it laid the foundations for future British and Irish Lions tours; and the 1888–89 New Zealand Native football team brought the first overseas team to British spectators.
During the early history of rugby union, a time before commercial air travel, teams from different continents rarely met. The first two notable tours both took place in 1888the British Isles team touring New Zealand and Australia, followed by the New Zealand team touring Europe. Traditionally the most prestigious tours were the Southern Hemisphere countries of Australia, New Zealand and South Africa making a tour of a Northern Hemisphere, and the return tours made by a joint British and Irish team. Tours would last for months, due to long traveling times and the number of games undertaken; the 1888 New Zealand team began their tour in Hawkes Bay in June and did not complete their schedule until August 1889, having played 107 rugby matches. Touring international sides would play Test matches against international opponents, including national, club and county sides in the case of Northern Hemisphere rugby, or provincial/state sides in the case of Southern Hemisphere rugby.

Between 1905 and 1908, all three major Southern Hemisphere rugby countries sent their first touring teams to the Northern Hemisphere: New Zealand in 1905, followed by South Africa in 1906 and Australia in 1908. All three teams brought new styles of play, fitness levels and tactics, and were far more successful than critics had expected.

The New Zealand 1905 touring team performed a haka before each match, leading Welsh Rugby Union administrator Tom Williams to suggest that Wales player Teddy Morgan lead the crowd in singing the Welsh National Anthem, "Hen Wlad Fy Nhadau", as a response. After Morgan began singing, the crowd joined in: the first time a national anthem was sung at the start of a sporting event. In 1905 France played England in its first international match.

Rugby union was included as an event in the Olympic Games four times during the early 20th century. No international rugby games and union-sponsored club matches were played during the First World War, but competitions continued through service teams such as the New Zealand Army team. During the Second World War no international matches were played by most countries, though Italy, Germany and Romania played a limited number of games, and Cambridge and Oxford continued their annual University Match.

The first officially sanctioned international rugby sevens tournament took place in 1973 at Murrayfield, one of Scotland's biggest stadiums, as part of the Scottish Rugby Union centenary celebrations.

In 1987 the first Rugby World Cup was held in Australia and New Zealand, and the inaugural winners were New Zealand. The first World Cup Sevens tournament was held at Murrayfield in 1993. Rugby Sevens was introduced into the Commonwealth Games in 1998 and was added to the Olympic Games of 2016. Both men and women's Sevens will again take place at the 2020 Olympic Games in Tokyo.

Rugby union was an amateur sport until the IRB declared the game "open" in August 1995 (shortly after the completion of the 1995 World Cup), removing restrictions on payments to players. However, the pre-1995 period of rugby union was marked by frequent accusations of "shamateurism", including an investigation in Britain by a House of Commons Select committee in early 1995. Following the introduction of professionalism trans-national club competitions were started, with the Heineken Cup in the Northern Hemisphere and Super Rugby in the Southern Hemisphere.

The Tri Nations, an annual international tournament involving Australia, New Zealand and South Africa, kicked off in 1996. In 2012, this competition was extended to include Argentina, a country whose impressive performances in international games (especially finishing in third place in the 2007 Rugby World Cup) was deemed to merit inclusion in the competition. As a result of the expansion to four teams, the tournament was renamed The Rugby Championship.

Each team starts the match with 15 players on the field and seven or eight substitutes. Players in a team are divided into eight forwards (two more than in rugby league) and seven backs.

The main responsibilities of the forward players are to gain and retain possession of the ball. Players in these positions are generally bigger and stronger and take part in the scrum and line-out. The forwards are often collectively referred to as the 'pack', especially when in the scrum formation.

Front row
The front row consists of three players: two props (the loosehead prop and the tighthead prop) and the hooker. The role of the two props is to support the hooker during scrums, to provide support for the jumpers during line-outs and to provide strength and power in rucks and mauls. The third position in the front row is the hooker. The hooker is a key position in attacking and defensive play and is responsible for winning the ball in the scrum. Hookers normally throw the ball in at line-outs.

Second row
The second row consists of two locks or lock forwards. Locks are usually the tallest players in the team, and specialise as line-out jumpers. The main role of the lock in line-outs is to make a standing jump, often supported by the other forwards, to either collect the thrown ball or ensure the ball comes down on their side. Locks also have an important role in the scrum, binding directly behind the three front row players and providing forward drive.

Back row
The back row, not to be confused with ‘Backs’, is the third and final row of the forward positions, who are often referred to as the loose forwards. The three positions in the back row are the two flankers and the number 8. The two flanker positions called the blindside flanker and openside flanker, are the final row in the scrum. They are usually the most mobile forwards in the game. Their main role is to win possession through 'turn overs'. The number 8 packs down between the two locks at the back of the scrum. The role of the number 8 in the scrum is to control the ball after it has been heeled back from the front of the pack, and the position provides a link between the forwards and backs during attacking phases.

The backs' role is to create and convert point-scoring opportunities. They are generally smaller, faster and more agile than the forwards. Another distinction between the backs and the forwards is that the backs are expected to have superior kicking and ball-handling skills, especially the fly-half, scrum-half, and full-back.

Half-backs
The half-backs consist of two positions, the scrum-half and the fly-half. The fly-half is crucial to a team's game plan, orchestrating the team's performance. They are usually the first to receive the ball from the scrum-half following a breakdown, lineout, or scrum, and need to be decisive with what actions to take and be effective at communicating with the outside backs. Many fly-halves are also their team's goal kickers. The scrum-half is the link between the forwards and the backs. They receive the ball from the lineout and remove the ball from the back of the scrum, usually passing it to the fly-half. They also feed the scrum and sometimes have to act as a fourth loose forward.

Three quarters
There are four three quarter positions, the inside centre, outside centre and left and right wings. The centres will attempt to tackle attacking players; whilst in attack, they should employ speed and strength to breach opposition defences. The wings are generally positioned on the outside of the backline. Their primary function is to finish off moves and score tries. Wings are usually the fastest players in the team and are elusive runners who use their speed to avoid tackles.

Fullback
The fullback normally positions themself several metres behind the back line. They often field opposition kicks and are usually the last line of defence should an opponent break through the back line. Two of the most important attributes of a good fullback are dependable catching skills and a good kicking game.

Rugby union is played between two teams – the one that scores more points wins the game. Points can be scored in several ways: a try, scored by grounding the ball in the in-goal area (between the goal line and the dead-ball line), is worth 5 points and a subsequent conversion kick scores 2 points; a successful penalty kick or a drop goal each score 3 points. The values of each of these scoring methods have been changed over the years. 
The field of play on a rugby pitch is as near as possible to a maximum of long by wide. In actual gameplay the length of a pitch can vary. There are typically 100 metres (109 yd) between the two try-lines, but it can be as short as 94 metres (103 yd). Anywhere between 6 and 22 metres behind each try line serves as the in-goal area. The pitch must be at least 68 metres (74 yd) wide, up to a maximum of 70 metres (76.5 yd) 

Rugby goalposts are H-shaped and are situated in the middle of the goal lines at each end of the field. They consist of two poles, 5.6 metres (6.1 yd) apart, connected by a horizontal crossbar 3 metres (3.3 yd) above the ground. The minimum height for posts is 3.4 metres (3.7 yd).

At the beginning of the game, the captains and the referee toss a coin to decide which team will kick off first. Play then starts with a dropkick, with the players chasing the ball into the opposition's territory, and the other side trying to retrieve the ball and advance it. If the ball does not reach the opponent's 10-metre line the opposing team has two choices: to have the ball kicked off again, or to have a scrum at the centre of the half-way line.
If the player with the ball is tackled, frequently a ruck will result.

Games are divided into 40-minute halves, with a break in the middle. The sides exchange ends of the field after the half-time break. Stoppages for injury or to allow the referee to take disciplinary action do not count as part of the playing time, so that the elapsed time is usually longer than 80 minutes. The referee is responsible for keeping time, even when—as in many professional tournaments—he is assisted by an official time-keeper. If time expires while the ball is in play, the game continues until the ball is "dead", and only then will the referee blow the whistle to signal half-time or full-time; but if the referee awards a penalty or free-kick, the game continues.

In the knockout stages of rugby competitions, most notably the Rugby World Cup, two extra time periods of 10 minutes periods are played (with an interval of 5 minutes in between) if the game is tied after full-time. If scores are level after 100 minutes then the rules call for 20 minutes of sudden-death extra time to be played. If the sudden-death extra time period results in no scoring a kicking competition is used to determine the winner. However, no match in the history of the Rugby World Cup has ever gone past 100 minutes into a sudden-death extra time period.

Forward passing (throwing the ball ahead to another player) is not allowed; the ball can be passed laterally or backwards. The ball tends to be moved forward in three ways — by kicking, by a player running with it or within a scrum or maul. Only the player with the ball may be tackled or rucked. When a ball is knocked forward by a player with their arms, a "knock-on" is committed, and play is restarted with a scrum.

Any player may kick the ball forward in an attempt to gain territory. When a player anywhere in the playing area kicks indirectly into touch so that the ball first bounces in the field of play, the throw-in is taken where the ball went into touch. If the player kicks directly into touch (i.e. without bouncing in-field first) from within one's own 22-metre line, the lineout is taken by the opposition where the ball went into touch, but if the ball is kicked into touch directly by a player outside the 22-metre line, the lineout is taken level to where the kick was taken.

The aim of the defending side is to stop the player with the ball, either by bringing them to ground (a tackle, which is frequently followed by a ruck) or by contesting for possession with the ball-carrier on their feet (a maul). Such a circumstance is called a breakdown and each is governed by a specific law.

Tackling

A player may tackle an opposing player who has the ball by holding them while bringing them to ground. Tacklers cannot tackle above the shoulder (the neck and head are out of bounds), and the tackler has to attempt to wrap their arms around the player being tackled to complete the tackle. It is illegal to push, shoulder-charge, or to trip a player using feet or legs, but hands may be used (this being referred to as a tap-tackle or ankle-tap). Tacklers may not tackle an opponent who has jumped to catch a ball until the player has landed.

Rucking and Mauling

Mauls occur after a player with the ball has come into contact with an opponent but the handler remains on his feet; once any combination of at least three players have bound themselves a maul has been set. A ruck is similar to the maul, but in this case the ball has gone to ground with at least three attacking players binding themselves on the ground in an attempt to secure the ball.

When the ball leaves the side of the field, a line-out is awarded against the team which last touched the ball. Forward players from each team line up a metre apart, perpendicular to the touchline and between 5 m and 15 m from the touchline. The ball is thrown from the touchline down the centre of the lines of forwards by a player (usually the hooker) from the team that did not play the ball into touch. The exception to this is when the ball went out from a penalty, in which case the side who gained the penalty throws the ball in.

Both sides compete for the ball and players may lift their teammates. A jumping player cannot be tackled until they stand and only shoulder-to-shoulder contact is allowed; deliberate infringement of this law is dangerous play, and results in a penalty kick.
A scrum is a way of restarting the game safely and fairly after a minor infringement. It is awarded when the ball has been knocked or passed forward, if a player takes the ball over his own try line and puts the ball down, when a player is accidentally offside or when the ball is trapped in a ruck or maul with no realistic chance of being retrieved. A team may also opt for a scrum if awarded a penalty.

A scrum is formed by the eight forwards from each team crouching down and binding together in three rows, before interlocking with the opposing team. For each team, the front row consists of two props (loosehead and tighthead) either side of the hooker. The second row consists of two locks and the two flankers. Behind the second row is the number 8. This formation is known as the 3–4–1 formation. Once a scrum is formed the scrum-half from the team awarded the "feed" rolls the ball into the gap between the two front-rows known as the "tunnel". The two hookers then compete for possession by hooking the ball backwards with their feet, while each pack tries to push the opposing pack backwards to help gain possession. The side that wins possession can either keep the ball under their feet while driving the opposition back, in order to gain ground, or transfer the ball to the back of the scrum where it can be picked up by the number 8 or by the scrum-half.

There are three match officials: a referee, and two assistant referees. The latter, formerly known as touch judges, had the primary function of indicating when the ball had gone into "touch"; their role has been expanded and they are now expected to assist the referee in a number of areas, such as watching for foul play and checking offside lines. In addition, for matches in high level competitions, there is often a television match official (TMO; popularly called the "video referee"), to assist with certain decisions, linked up to the referee by radio. The referees have a system of hand signals to indicate their decisions.

Common offences include tackling above the shoulders, collapsing a scrum, ruck or maul, not releasing the ball when on the ground, or being offside. The non-offending team has a number of options when awarded a penalty: a "tap" kick, when the ball is kicked a very short distance from hand, allowing the kicker to regather the ball and run with it; a punt, when the ball is kicked a long distance from hand, for field position; a place-kick, when the kicker will attempt to score a goal; or a scrum. Players may be sent off (signalled by a red card) or temporarily suspended ("sin-binned") for ten minutes (yellow card) for foul play or repeated infringements, and may not be replaced.

Occasionally, infringements are not caught by the referee during the match and these may be "cited" by the citing commissioner after the match and have punishments (usually suspension for a number of weeks) imposed on the infringing player.

During the match, players may be replaced (for injury) or substituted (for tactical reasons). A player who has been replaced may not rejoin play unless he was temporarily replaced to have bleeding controlled; a player who has been substituted may return temporarily, to replace a player who has a blood injury or has suffered a concussion, or permanently, if he is replacing a front-row forward. In international matches, eight replacements are allowed; in domestic or cross-border tournaments, at the discretion of the responsible national union(s), the number of replacements may be nominated to a maximum of eight, of whom three must be sufficiently trained and experienced to provide cover for the three front row positions.

Prior to 2016, all substitutions, no matter the cause, counted against the limit during a match. In 2016, World Rugby changed the law so that substitutions made to replace a player deemed unable to continue due to foul play by the opposition would no longer count against the match limit. This change was introduced in January of that year in the Southern Hemisphere and June in the Northern Hemisphere.

The most basic items of equipment for a game of rugby union are the ball itself, a rugby shirt (also known as a "jersey"), rugby shorts, socks, and boots. The rugby ball is oval in shape (technically a prolate spheroid), and is made up of four panels. The ball was historically made of leather, but in the modern era most games use a ball made from a synthetic material. World Rugby lays out specific dimensions for the ball, 280–300mm in length, 740–770mm in circumference of length and 580–620mm in circumference of width. Rugby boots have soles with studs to allow grip on the turf of the pitch. The studs may be either metal or plastic but must not have any sharp edges or ridges.

Protective equipment is optional and strictly regulated. The most common items are mouthguards, which are worn by almost all players, and are compulsory in some rugby-playing nations. Other protective items that are permitted include head gear; thin (not more than 10 mm thick), non-rigid shoulder pads and shin guards; which are worn underneath socks. Bandages or tape can be worn to support or protect injuries; some players wear tape around the head to protect the ears in scrums and rucks. Female players may also wear chest pads. Although not worn for protection, some types of fingerless mitts are allowed to aid grip.

It is the responsibility of the match officials to check players' clothing and equipment before a game to ensure that it conforms to the laws of the game.

The international governing body of rugby union (and associated games such as sevens) is World Rugby (WR). The WR headquarters are in Dublin, Ireland. WR, founded in 1886, governs the sport worldwide and publishes the game's laws and rankings. As of February 2014, WR (then known as the IRB, for International Rugby Board) recorded 119 unions in its membership, 101 full members and 18 associate member countries. According to WR, rugby union is played by men and women in over 100 countries. WR controls the Rugby World Cup, the Women's Rugby World Cup, Rugby World Cup Sevens, HSBC Sevens Series, HSBC Women's Sevens Series, World Under 20 Championship, World Under 20 Trophy, Nations Cup and the Pacific Nations Cup. WR holds votes to decide where each of these events are to be held, except in the case of the Sevens World Series for men and women, for which WR contracts with several national unions to hold individual events.

Six regional associations, which are members of WR, form the next level of administration; these are:

SANZAAR (South Africa, New Zealand, Australia and Argentina Rugby) is a joint venture of the South African Rugby Union, New Zealand Rugby, Rugby Australia and the Argentine Rugby Union (UAR) that operates Super Rugby and The Rugby Championship (formerly the Tri Nations before the entry of Argentina). Although UAR initially had no representation on the former SANZAR board, it was granted input into the organisation's issues, especially with regard to The Rugby Championship, and became a full SANZAAR member in 2016 (when the country entered Super Rugby).

National unions oversee rugby union within individual countries and are affiliated to WR. Since 2016, the WR Council has 40 seats. A total of 11 unions—the eight foundation unions of Scotland, Ireland, Wales, England, Australia, New Zealand, South Africa and France, plus Argentina, and —have two seats each. In addition, the six regional associations have two seats each. Four more unions—, , and the USA—have one seat each. Finally, the Chairman and Vice Chairman, who usually come from one of the eight foundation unions (although the current Vice Chairman, Agustín Pichot, is with the non-foundation Argentine union) have one vote each.

The earliest countries to adopt rugby union were England, the country of inception, and the other three Home Nations, Scotland, Ireland and Wales. The spread of rugby union as a global sport has its roots in the exporting of the game by British expatriates, military personnel, and overseas university students.
The first rugby club in France was formed by British residents in Le Havre in 1872, while the next year Argentina recorded its first game: 'Banks' v 'City' in Buenos Aires.

At least seven countries have adopted rugby union as their de facto national sport; they are Fiji, Georgia, Madagascar, New Zealand, Samoa, Tonga and Wales.

A rugby club was formed in Sydney, New South Wales, Australia in 1864; while the sport was said to have been introduced to New Zealand by Charles Monro in 1870, who played rugby while a student at Christ's College, Finchley.

Several island nations have embraced the sport of rugby. Rugby was first played in Fiji circa 1884 by European and Fijian soldiers of the Native Constabulary at Ba on Viti Levu island. Fiji then sent their first overseas team to Samoa in 1924, who in turn set up their own union in 1927. Along with Tonga, other countries to have national rugby teams in Oceania include the Cook Islands, Niue, Papua New Guinea and Solomon Islands.

In North America a club formed in Montreal in 1868, Canada's first club. The city of Montreal also played its part in the introduction of the sport in the United States, when students of McGill University played against a team from Harvard University in 1874.

Although the exact date of arrival of rugby union in Trinidad and Tobago is unknown, their first club Northern RFC was formed in 1923, a national team was playing by 1927 and due to a cancelled tour to British Guiana in 1933, switched their venue to Barbados; introducing rugby to the island. Other Atlantic countries to play rugby union include Jamaica and Bermuda.

The growth of rugby union in Europe outside the 6 Nations countries in terms of playing numbers has been sporadic. Historically, British and Irish home teams played the Southern Hemisphere teams of Australia, New Zealand, and South Africa, as well as France. The rest of Europe were left to play amongst themselves. During a period when it had been isolated by the British and Irish Unions, France, lacking international competition, became the only European team from the top tier to regularly play the other European countries; mainly Belgium, the Netherlands, Germany, Spain, Romania, Poland, Italy and Czechoslovakia. In 1934, instigated by the French Rugby Federation, FIRA (Fédération Internationale de Rugby Amateur) was formed to organise rugby union outside the authority of the IRFB. The founding members were , , , , , and .

Other European rugby playing nations of note include Russia, whose first officially recorded match is marked by an encounter between Dynamo Moscow and the Moscow Institute of Physical Education in 1933. Rugby union in Portugal also took hold between the First and Second World Wars, with a Portuguese National XV set up in 1922 and an official championship started in 1927.

In 1999, FIRA agreed to place itself under the auspices of the IRB, transforming itself into a strictly European organising body. Accordingly, it changed its name to FIRA–AER (Fédération Internationale de Rugby Amateur – Association Européenne de Rugby). It adopted its current name of Rugby Europe in 2014.

Although Argentina is the best-known rugby playing nation in South America, founding the Argentine Rugby Union in 1899, several other countries on the continent have a long history. Rugby had been played in Brazil since the end of the 19th century, but the game was played regularly only from 1926, when São Paulo beat Santos in an inter-city match. It took Uruguay several aborted attempts to adapt to rugby, led mainly by the efforts of the Montevideo Cricket Club; these efforts succeeded in 1951 with the formation of a national league and four clubs. Other South American countries that formed a rugby union include Chile (1948), and Paraguay (1968).

Many Asian countries have a tradition of playing rugby dating from the British Empire. India began playing rugby in the early 1870s, the Calcutta Football Club forming in 1873. However, with the departure of a local British army regiment, interest in rugby diminished in the area. In 1878, The Calcutta Football Club was disbanded, and rugby in India faltered. Sri Lanka claims to have founded their union in 1878, and although little official information from the period is available, the team won the All-India cup in Madras in 1920. The first recorded match in Malaysia was in 1892, but the first confirmation of rugby is the existence of the "HMS Malaya Cup" which was first presented in 1922 and is still awarded to the winners of the Malay sevens.

Rugby union was introduced to Japan in 1899 by two Cambridge students: Ginnosuke Tanaka and Edward Bramwell Clarke. The Japan RFU was founded in 1926 and its place in rugby history was cemented with the news that Japan will host the 2019 World Cup. It will be the first country outside the Commonwealth, Ireland and France to host the event, and this is viewed by the IRB as an opportunity for rugby union to extend its reach, particularly in Asia. Other Asian playing countries of note include Singapore, South Korea, China and The Philippines, while the former British colony of Hong Kong is notable within rugby for its development of the rugby sevens game, especially the Hong Kong Sevens tournament which was founded in 1976.

Rugby in the Middle East and the Gulf States has its history in the 1950s, with clubs formed by British and French Services stationed in the region after the Second World War. When these servicemen left, the clubs and teams were kept alive by young professionals, mostly Europeans, working in these countries. The official union of Oman was formed in 1971. Bahrain founded its union a year later, while in 1975 the Dubai Sevens, the Gulf's leading rugby tournament, was created. Rugby remains a minority sport in the region with Israel and the United Arab Emirates, as of 2019, being the only member union from the Middle East to be included in the IRB World Rankings.

In 1875, rugby was introduced to South Africa by British soldiers garrisoned in Cape Town. During the late 19th and early 20th century, the sport in Africa was spread by settlers and colonials who often adopted a "whites-only" policy to playing the game. This resulted in rugby being viewed as a bourgeois sport by the indigenous people with limited appeal. The earliest countries to see the playing of competitive rugby include South Africa, and neighbouring Rhodesia (modern-day Zimbabwe), which formed the Rhodesia Rugby Football Union in 1895.

In more recent times the sport has been embraced by several African nations. In the early 21st century Madagascar has experienced crowds of 40,000 at national matches, while Namibia, whose history of rugby can be dated from 1915, have qualified for the final stages of the World Cup four times since 1999. Other African nations to be represented in the World Rugby Rankings as Member Unions include Côte d'Ivoire, Kenya, Uganda and Zambia. South Africa and Kenya are among the 15 "core teams" that participate in every event of the men's World Rugby Sevens Series.

Records of women's rugby football date from the late 19th century, with the first documented source being Emily Valentine's writings, stating that she set up a rugby team in Portora Royal School in Enniskillen, Ireland in 1887. Although there are reports of early women's matches in New Zealand and France, one of the first notable games to prove primary evidence was the 1917 war-time encounter between Cardiff Ladies and Newport Ladies; a photo of which shows the Cardiff team before the match at the Cardiff Arms Park. In the past 30 years the game has grown in popularity among female athletes, and, according to WR, is now played in over 100 countries.

The English-based Women's Rugby Football Union (WRFU), responsible for women's rugby in England, Scotland, Ireland, and Wales, was founded in 1983, and is the oldest formally organised national governing body for women's rugby. This was replaced in 1994 by the Rugby Football Union for Women (RFUW) in England with each of the other Home Nations governing their own countries. The premier international competition in rugby union for women is the Women's Rugby World Cup, first held in 1991. From 1994 through 2014, it was held every four years. Following the 2014 event, a new four-year cycle was instituted, with the Women's World Cup held in the middle year of the men's World Cup cycle. The new Women's World Cup cycle began in 2017, with future competitions every four years thereafter.

The most important competition in rugby union is the Rugby World Cup, a men's tournament that has taken place every four years since the inaugural event in 1987. South Africa are the reigning champions, having defeated England in the final of the 2019 Rugby World Cup in Yokohama. New Zealand and South Africa have each won the title three times (New Zealand: 1987, 2011, 2015; South Africa: 1995, 2007, 2019), Australia have won twice (1991 and 1999), and England once (2003). England is the only team from the Northern Hemisphere to have won the Rugby World Cup.

The Rugby World Cup has continued to grow since its inception in 1987. The first tournament, in which 16 teams competed for the title, was broadcast to 17 countries with an accumulated total of 230 million television viewers. Ticket sales during the pool stages and finals of the same tournament was less than a million. The 2007 World Cup was contested by 94 countries with ticket sales of 3,850,000 over the pool and final stage. The accumulated television audience for the event, then broadcast to 200 countries, was a claimed 4.2 billion.

The 2019 Rugby World Cup took place in Japan between 20 September and 2 November. It was the ninth edition and the first time the tournament has been held in Asia.

Major international competitions are the Six Nations Championship and The Rugby Championship, held in Europe and the Southern Hemisphere respectively.

The Six Nations is an annual competition involving the European teams , , , , and . Each country plays the other five once. Following the first internationals between England and Scotland, Ireland and Wales began competing in the 1880s, forming the "Home International Championships". France joined the tournament in the 1900s and in 1910 the term "Five Nations" first appeared. However, the Home Nations (England, Ireland, Scotland, and Wales) excluded France in 1931 amid a run of poor results, allegations of professionalism and concerns over on-field violence. France then rejoined in 1939–1940, though World War II halted proceedings for a further eight years. France has played in all the tournaments since WWII, the first of which was played in 1947. In 2000, Italy became the sixth nation in the contest and Rome's Stadio Olimpico has replaced Stadio Flaminio as the venue for their home games since 2013. The current Six Nations champions are Wales.

The Rugby Championship is the Southern Hemisphere's annual international series for that region's top national teams. From its inception in 1996 through 2011, it was known as the Tri Nations, as it featured the hemisphere's traditional powers of Australia, New Zealand and South Africa. These teams have dominated world rankings in recent years, and many considered the Tri Nations to be the toughest competition in international rugby. The Tri Nations was initially played on a home and away basis with the three nations playing each other twice.

In 2006 a new system was introduced where each nation plays the others three times, though in 2007 and 2011 the teams played each other only twice, as both were World Cup years. Since Argentina's strong performances in the 2007 World Cup, after the 2009 Tri Nations tournament, SANZAR (South Africa, New Zealand and Australian Rugby) invited the Argentine Rugby Union (UAR) to join an expanded Four Nations tournament in 2012. The competition has been officially rechristened as The Rugby Championship beginning with the 2012 edition. The competition reverted to the Tri Nations' original home-and-away format, but now involving four teams. In World Cup years, an abbreviated tournament is held in which each team plays the others only once.

Rugby union was played at the Olympic Games in 1900, 1908, 1920 and 1924. As per Olympic rules, the nations of Scotland, Wales and England were not allowed to play separately as they are not sovereign states. In 1900, France won the gold, beating Great Britain 27 points to 8 and defeating Germany 27 points to 17. In 1908, Australia defeated Great Britain, claiming the gold medal, the score being 32 points to three. In 1920, the United States, fielding a team with many players new to the sport of rugby, upset France in a shock win, eight points to zero. In 1924, the United States again defeated France 17 to 3, becoming the only team to win gold twice in the sport.

In 2009 the International Olympic Committee voted with a majority of 81 to 8 that rugby union be reinstated as an Olympic sport in at least the 2016 and 2020 games, but in the sevens, 4-day tournament format. This is something the rugby world has aspired to for a long time and Bernard Lapasset, president of the International Rugby Board, said the Olympic gold medal would be considered to be "the pinnacle of our sport" (Rugby Sevens).

Rugby sevens has been played at the Commonwealth Games since the 1998 Games in Kuala Lumpur. The most gold medal holders are New Zealand who have won the competition on four successive occasions until South Africa beat them in 2014. Rugby union has also been an Asian Games event since the 1998 games in Bangkok, Thailand. In the 1998 and 2002 editions of the games, both the usual fifteen-a-side variety and rugby sevens were played, but from 2006 onwards, only rugby sevens was retained. In 2010, the women's rugby sevens event was introduced. The event is likely to remain a permanent fixture of the Asian Games due to elevation of rugby sevens as an Olympic sport from the 2016 Olympics onwards. The present gold medal holders in the sevens tournament, held in 2014, are Japan in the men's event and China in the women's.

Women's international rugby union began in 1982, with a match between France and the Netherlands played in Utrecht. As of 2009 over six hundred women's internationals have been played by over forty different nations.

The first Women's Rugby World Cup was held in Wales in 1991, and was won by the United States. The second tournament took place in 1994, and from that time through 2014 was held every four years. The New Zealand Women's team then won four straight World Cups (1998, 2002, 2006, 2010) before England won in 2014. Following the 2014 event, World Rugby moved the next edition of the event to 2017, with a new four-year cycle from that point forward. New Zealand are the current World Cup holders.

As well as the Women's Rugby World Cup there are also other regular tournaments, including a Six Nations, run in parallel to the men's competition. The Women's Six Nations, first played in 1996 has been dominated by England, who have won the tournament on 14 occasions, including a run of seven consecutive wins from 2006 to 2012. However, since then, England have won only in 2017; reigning champion France have won in each even-numbered year (2014, 2016, 2018) whilst Ireland won in 2013 and 2015.

Rugby union has been professionalised since 1995. The following table shows fully professional rugby competitions. (Semi-professional competitions are excluded.)

Rugby union has spawned several variants of the full-contact, 15-a-side game. The two most common differences in adapted versions are fewer players and reduced player contact.

The oldest variant is rugby sevens (sometimes 7s or VIIs), a fast-paced game which originated in Melrose, Scotland in 1883. In rugby sevens, there are only seven players per side, and each half is normally seven minutes. Major tournaments include the Hong Kong Sevens and Dubai Sevens, both held in areas not normally associated with the highest levels of the 15-a-side game.

A more recent variant of the sport is rugby tens (10s or Xs), a Malaysian invention with ten players per side.

Touch rugby, in which "tackles" are made by simply touching the ball carrier with two hands, is popular both as a training game and more formally as a mixed sex version of the sport played by both children and adults.

Several variants have been created to introduce the sport to children with a less physical contact. Mini rugby is a version aimed at fostering the sport in children. It is played with only eight players and on a smaller pitch.

Tag Rugby is a version in which the players wear a belt with two tags attached by velcro, the removal of either counting as a 'tackle'. Tag Rugby also varies in that kicking the ball is not allowed. Similar to Tag Rugby, American Flag Rugby, (AFR), is a mixed gender, non-contact imitation of rugby union designed for American children entering grades K-9. Both American Flag Rugby and Mini Rugby differ to Tag Rugby in that they introduce more advanced elements of rugby union as the participants age.

Other less formal variants include beach rugby and snow rugby.

Rugby league was formed after the Northern Union broke from the Rugby Football Union in a disagreement over payment to players. It went on to change its laws and became a code in its own right. The two sports continue to influence each other to this day.

American football and Canadian football are derived from early forms of rugby.

Australian rules football was influenced by rugby football and other games originating in English public schools.

James Naismith took aspects of many sports including rugby to invent basketball. The most obvious contribution is the jump ball's similarity to the line-out as well as the underhand shooting style that dominated the early years of the sport. Naismith played rugby at McGill University.

Swedish football was a code whose rules were a mix of Association and Rugby football rules.

Rugby lends its name to wheelchair rugby, a full-contact sport which contains elements of rugby such as crossing a try line with the ball to score.

According to a 2011 report by the Centre for the International Business of Sport, over four and a half million people play rugby union or one of its variants organised by the IRB. This is an increase of 19 percent since the previous report in 2007. The report also claimed that since 2007 participation has grown by 33 percent in Africa, 22 percent in South America and 18 percent in Asia and North America. In 2014 the IRB published a breakdown of the total number of players worldwide by national unions. It recorded a total of 6.6 million players globally, of those, 2.36 million were registered members playing for a club affiliated to their country's union. The 2016 World Rugby Year in Review reported 8.5 million players, of which 3.2 million were registered union players and 1.9 million were registered club players; 22% of all players were female.

The most capped international player from the tier 1 nations is former New Zealand openside flanker and captain Richie McCaw who has played in 148 internationals. While the top scoring tier 1 international player is New Zealand's Dan Carter, who has amassed 1442 points during his career. In April 2010 Lithuania which is a second tier rugby nation, broke the record of consecutive international wins for second tier rugby nations. In 2016, the All Blacks of New Zealand set the new record 18 consecutive test wins among tier 1 rugby nations, bettering their previous consecutive run of 17. This record was equalled by England on 11 March 2017 with a win over Scotland at Twickenham. The highest scoring international match between two recognised unions was Hong Kong's 164–13 victory over Singapore on 27 October 1994. While the largest winning margin of 152 points is held by two countries, Japan (a 155–3 win over Chinese Taipei) and Argentina (152–0 over Paraguay) both in 2002.

The record attendance for a rugby union game was set on 15 July 2000 in which New Zealand defeated Australia 39–35 in a Bledisloe Cup game at Stadium Australia in Sydney before 109,874 fans. The record attendance for a match in Europe of 104,000 (at the time a world record) was set on 1 March 1975 when Scotland defeated Wales 12–10 at Murrayfield in Edinburgh during the 1975 Five Nations Championship. The record attendance for a domestic club match is 99,124, set when Racing 92 defeated Toulon in the 2016 Top 14 final on 24 June at Camp Nou in Barcelona. The match had been moved from its normal site of Stade de France near Paris due to scheduling conflicts with France's hosting of UEFA Euro 2016.

Thomas Hughes 1857 novel "Tom Brown's Schooldays", set at Rugby School, includes a rugby football match, also portrayed in the 1940s film of the same name. James Joyce mentions Irish team Bective Rangers in several of his works, including Ulysses (1922) and Finnegans Wake (1939), while his 1916 semi-autobiographical work "A Portrait of the Artist as a Young Man" has an account of Ireland international James Magee. Sir Arthur Conan Doyle, in his 1924 Sherlock Holmes tale "The Adventure of the Sussex Vampire", mentions that Dr Watson played rugby for Blackheath.

Henri Rousseau's 1908 work "Joueurs de football" shows two pairs of rugby players competing. Other French artists to have represented the sport in their works include Albert Gleizes' "Les Joueurs de football" (1912), Robert Delaunay's "Football. L'Équipe de Cardiff" (1916) and André Lhote's "Partie de Rugby" (1917). The 1928 Gold Medal for Art at the Antwerp Olympics was won by Luxembourg's Jean Jacoby for his work "Rugby".

In film, Ealing Studios' 1949 comedy "A Run for Your Money" and the 1979 BBC Wales television film "Grand Slam" both centre on fans attending a match. Films that explore the sport in more detail include independent production "Old Scores" (1991) and "Forever Strong" (2008). "Invictus" (2009), based on John Carlin's book "Playing the Enemy", explores the events of the 1995 Rugby World Cup and Nelson Mandela's attempt to use the sport to connect South Africa's people post-apartheid.

In public art and sculpture there are many works dedicated to the sport. There is a 27 ft bronze statue of a rugby line-out by pop artist Gerald Laing at Twickenham and one of rugby administrator Sir Tasker Watkins at the Millennium Stadium. Rugby players to have been honoured with statues include Gareth Edwards in Cardiff and Danie Craven in Stellenbosch.





</doc>
<doc id="25406" url="https://en.wikipedia.org/wiki?curid=25406" title="Rugby World Cup">
Rugby World Cup

The Rugby World Cup is a men's rugby union tournament contested every four years between the top international teams. The tournament was first held in 1987, when the tournament was co-hosted by New Zealand and Australia.

The winners are awarded the Webb Ellis Cup, named after William Webb Ellis, the Rugby School pupil who, according to a popular legend, invented rugby by picking up the ball during a football game. Four countries have won the trophy; New Zealand and South Africa three times, Australia twice, and England once. South Africa are the current champions, having defeated England in the final of the 2019 tournament in Japan.

The tournament is administered by World Rugby, the sport's international governing body. Sixteen teams were invited to participate in the inaugural tournament in 1987, however since 1999 twenty teams have taken part. Japan hosted the 2019 Rugby World Cup and France will host the next in 2023.

On 21 August 2019, World Rugby announced that gender designations would be removed from the titles of the men's and women's World Cups. Accordingly, all future World Cups for men and women will officially bear the "Rugby World Cup" name. The first tournament to be affected by the new policy will be the next women's tournament to be held in New Zealand in 2021, which will officially be titled as "Rugby World Cup 2021".

Qualifying tournaments were introduced for the second tournament, where eight of the sixteen places were contested in a twenty-four-nation tournament. The inaugural World Cup in 1987, did not involve any qualifying process; instead, the 16 places were automatically filled by seven eligible International Rugby Football Board (IRFB, now World Rugby) member nations, and the rest by invitation.

In 2003 and 2007, the qualifying format allowed for eight of the twenty available positions to be filled by automatic qualification, as the eight quarter-finalists of the previous tournament enter its successor. The remaining twelve positions were filled by continental qualifying tournaments. Positions were filled by three teams from the Americas, one from Asia, one from Africa, three from Europe and two from Oceania. Another two places were allocated for repechage. The first repechage place was determined by a match between the runners-up from the Africa and Europe qualifying tournaments, with that winner then playing the Americas runner-up to determine the place. The second repechage position was determined between the runners-up from the Asia and Oceania qualifiers.

The current format allows for 12 of the 20 available positions to be filled by automatic qualification, as the teams who finish third or better in the group (pool) stages of the previous tournament enter its successor (where they will be seeded). The qualification system for the remaining eight places is region-based, with a total eight teams allocated for Europe, five for Oceania, three for the Americas, two for Africa, and one for Asia. The last place is determined by an intercontinental play-off.

The 2015 tournament involved twenty nations competing over six weeks. There were two stages, a pool and a knockout. Nations were divided into four pools, A through to D, of five nations each. The teams were seeded before the start of the tournament, with the seedings taken from the World Rankings in December 2012. The four highest-ranked teams were drawn into pools A to D. The next four highest-ranked teams were then drawn into pools A to D, followed by the next four. The remaining positions in each pool were filled by the qualifiers.

Nations play four pool games, playing their respective pool members once each. A bonus points system is used during pool play. If two or more teams are level on points, a system of criteria is used to determine the higher ranked; the sixth and final criterion decides the higher rank through the official World Rankings.

The winner and runner-up of each pool enter the knockout stage. The knockout stage consists of quarter- and semi-finals, and then the final. The winner of each pool is placed against a runner-up of a different pool in a quarter-final. The winner of each quarter-final goes on to the semi-finals, and the respective winners proceed to the final. Losers of the semi-finals contest for third place, called the 'Bronze Final'. If a match in the knockout stages ends in a draw, the winner is determined through extra time. If that fails, the match goes into sudden death and the next team to score any points is the winner. As a last resort, a kicking competition is used.

Prior to the Rugby World Cup, there was no truly global rugby union competition, but there were a number of other tournaments. One of the oldest is the annual Six Nations Championship, which started in 1883 as the Home Nations Championship, a tournament between England, Ireland, Scotland and Wales. It expanded to the Five Nations in 1910, when France joined the tournament. France did not participate from 1931 to 1939, during which period it reverted to a Home Nations championship. In 2000, Italy joined the competition, which became the Six Nations.

Rugby union was also played at the Summer Olympic Games, first appearing at the 1900 Paris games and subsequently at London in 1908, Antwerp in 1920, and Paris again in 1924. France won the first gold medal, then Australasia, with the last two being won by the United States. However rugby union ceased to be on Olympic program after 1924.

The idea of a Rugby World Cup had been suggested on numerous occasions going back to the 1950s, but met with opposition from most unions in the IRFB. The idea resurfaced several times in the early 1980s, with the Australian Rugby Union (ARU; now known as Rugby Australia) in 1983, and the New Zealand Rugby Union (NZRU; now known as New Zealand Rugby) in 1984 independently proposing the establishment of a world cup. A proposal was again put to the IRFB in 1985 and this time successfully passed 10–6. The delegates from Australia, France, New Zealand and South Africa all voted for the proposal, and the delegates from Ireland and Scotland against; the English and Welsh delegates were split, with one from each country for and one against.

The inaugural tournament, jointly hosted by Australia and New Zealand, was held in May and June 1987, with sixteen nations taking part. New Zealand became the first-ever champions, defeating France 29–9 in the final. The subsequent 1991 tournament was hosted by England, with matches played throughout Britain, Ireland and France. This tournament saw the introduction of a qualifying tournament; eight places were allocated to the quarter-finalists from 1987, and the remaining eight decided by a thirty-five nation qualifying tournament. Australia won the second tournament, defeating England 12–6 in the final.

In 1992, eight years after their last official series, South Africa hosted New Zealand in a one-off test match. The resumption of international rugby in South Africa came after the dismantling of the apartheid system, and was only done with permission of the African National Congress. With their return to test rugby, South Africa were selected to host the 1995 Rugby World Cup. After upsetting Australia in the opening match, South Africa continued to advance through the tournament until they met New Zealand in the final. After a tense final that went into extra time, South Africa emerged 15–12 winners, with then President Nelson Mandela, wearing a Springbok jersey, presenting the trophy to South Africa's captain, Francois Pienaar.

The tournament in 1999 was hosted by Wales with matches also being held throughout the rest of the United Kingdom, Ireland and France. The tournament included a repechage system, alongside specific regional qualifying places, and an increase from sixteen to twenty participating nations. Australia claimed their second title, defeating France in the final.

The 2003 event was hosted by Australia, although it was originally intended to be held jointly with New Zealand. England emerged as champions defeating Australia in extra time. England's win was unique in that it broke the southern hemisphere's dominance in the event. Such was the celebration of England's victory that an estimated 750,000 people gathered in central London to greet the team, making the day the largest sporting celebration of its kind ever in the United Kingdom.

The 2007 competition was hosted by France, with matches also being held in Wales and Scotland. South Africa claimed their second title by defeating defending champions England 15–6. The 2011 tournament was awarded to New Zealand in November 2005, ahead of bids from Japan and South Africa. The All Blacks reclaimed their place atop the rugby world with a narrow 8–7 win over France in the 2011 final.

In the 2015 edition of tournament, hosted by England, New Zealand once again won the final, this time against established rivals, Australia. In doing so, they became the first team in World Cup history to win three titles, as well as the first to successfully defend a title. It was also New Zealand's first title victory on foreign soil.

The 2019 World Cup, hosted by Japan, saw South Africa claim their third trophy to match New Zealand for the most Rugby World Cup titles. South Africa defeated England 32–12 in the final.

The Webb Ellis Cup is the prize presented to winners of the Rugby World Cup, named after William Webb Ellis. The trophy is also referred to simply as the "Rugby World Cup". The trophy was chosen in 1987 as an appropriate cup for use in the competition, and was created in 1906 by Garrard's Crown Jewellers. The trophy is restored after each game by fellow Royal Warrant holder Thomas Lyte. The words 'The International Rugby Football Board' and 'The Webb Ellis Cup' are engraved on the face of the cup. It stands thirty-eight centimetres high and is silver gilded in gold, and supported by two cast scroll handles, one with the head of a satyr, and the other a head of a nymph. In Australia the trophy is colloquially known as "Bill" — a reference to William Webb Ellis.

Tournaments are organised by Rugby World Cup Ltd (RWCL), which is itself owned by World Rugby. The selection of host is decided by a vote of World Rugby Council members. The voting procedure is managed by a team of independent auditors, and the voting kept secret. The allocation of a tournament to a host nation is now made five or six years prior to the commencement of the event, for example New Zealand were awarded the 2011 event in late 2005.

The tournament has been hosted by multiple nations. For example, the 1987 tournament was co-hosted by Australia and New Zealand. World Rugby requires that the hosts must have a venue with a capacity of at least 60,000 spectators for the final. Host nations sometimes construct or upgrade stadia in preparation for the World Cup, such as Millennium Stadium – purpose built for the 1999 tournament – and Eden Park, upgraded for 2011. The first country outside of the traditional rugby nations of SANZAAR or the Six Nations to be awarded the hosting rights was 2019 host Japan. France will host the 2023 tournament.

Organizers of the Rugby World Cup, as well as the Global Sports Impact, state that the Rugby World Cup is the third largest sporting event in the world, behind only the FIFA World Cup and the Olympics, although other sources question whether this is accurate.

Reports emanating from World Rugby and its business partners have frequently touted the tournament's media growth, with cumulative worldwide television audiences of 300 million for the inaugural 1987 tournament, 1.75 billion in 1991, 2.67 billion in 1995, 3 billion in 1999, 3.5 billion in 2003, and 4 billion in 2007. The 4 billion figure was widely dismissed as the global audience for television is estimated to be about 4.2 billion.

However, independent reviews have called into question the methodology of those growth estimates, pointing to factual inconsistencies. The event's supposed drawing power outside of a handful of rugby strongholds was also downplayed significantly, with an estimated 97 percent of the 33 million average audience produced by the 2007 final coming from Australasia, South Africa, the British Isles and France. Other sports have been accused of exaggerating their television reach over the years; such claims are not exclusive to the Rugby World Cup.

While the event's global popularity remains a matter of dispute, high interest in traditional rugby nations is well documented. The 2003 final, between Australia and England, became the most watched rugby union match in the history of Australian television.

†Typhoon Hagibis caused 3 group stage matches to be cancelled. As a result, only 45 of the scheduled 48 matches were played in the 2019 Rugby World Cup.

Notes:

Twenty-five nations have participated at the Rugby World Cup (excluding qualifying tournaments). The only nations to host and win a tournament are New Zealand (1987 and 2011) and South Africa (1995). The performance of other host nations includes England (1991 final hosts) and Australia (2003 hosts) both finishing runners-up, while France (2007 hosts) finished fourth, and Wales (1999 hosts) and Japan (2019 hosts) reached the quarter-finals. Wales became the first host nation to be eliminated at the pool stages in 1991 while England became the first solo host nation to be eliminated at the pool stages in 2015. Of the twenty-five nations that have participated in at least one tournament, eleven of them have never missed a tournament.

 South Africa was excluded from the first two tournaments due to a sporting boycott during the apartheid era.

The record for most points overall is held by English player Jonny Wilkinson, who scored 277 during his World Cup career. New Zealand All Black Grant Fox holds the record for most points in one competition, with 126 in 1987; Jason Leonard of England holds the record for most World Cup matches: 22 between 1991 and 2003. All Black Simon Culhane holds the record for most points in a match by one player, 45, as well as the record for most conversions in a match, 20. All Black Marc Ellis holds the record for most tries in a match, six, which he scored against Japan in 1995.

New Zealand All Black Jonah Lomu is the youngest player to appear in a final – aged 20 years and 43 days at the 1995 Final. Lomu (playing in two tournaments) and South African Bryan Habana (playing in three tournaments) share the record for most total World Cup tournament tries, both scoring 15. Lomu (in 1999) and Habana (in 2007) also share the record, along with All Black Julian Savea (in 2015), for most tries in a tournament, with 8 each. South Africa's Jannie de Beer kicked five drop-goals against England in 1999 – an individual record for a single World Cup match. The record for most penalties in a match is 8, held by Australian Matt Burke, Argentinian Gonzalo Quesada, Scotland's Gavin Hastings and France's Thierry Lacroix, with Quesada also holding the record for most penalties in a tournament, with 31.

The most points scored in a game is 145, by the All Blacks against Japan in 1995, while the widest winning margin is 142, held by Australia in a match against Namibia in 2003.

A total of 16 players have been sent off (red carded) in the tournament. Welsh lock Huw Richards was the first, while playing against New Zealand in 1987. No player has been red carded more than once.





</doc>
<doc id="25407" url="https://en.wikipedia.org/wiki?curid=25407" title="Recursion">
Recursion

Recursion (adjective: "recursive") occurs when a thing is defined in terms of itself or of its type. Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no infinite loop or infinite chain of references can occur.

In mathematics and computer science, a class of objects or methods exhibits recursive behavior when it can be defined by two properties:

For example, the following is a recursive definition of a person's "ancestor". One's ancestor is either:

The Fibonacci sequence is another classic example of recursion:

formula_1

formula_2

formula_3

Many mathematical axioms are based upon recursive rules. For example, the formal definition of the natural numbers by the Peano axioms can be described as: "Zero is a natural number, and each natural number has a successor, which is also a natural number." By this base case and recursive rule, one can generate the set of all natural numbers.

Other recursively defined mathematical objects include factorials, functions (e.g., recurrence relations), sets (e.g., Cantor ternary set), and fractals.

There are various more tongue-in-cheek definitions of recursion; see recursive humor.

Recursion is the process a procedure goes through when one of the steps of the procedure involves invoking the procedure itself. A procedure that goes through recursion is said to be 'recursive'.

To understand recursion, one must recognize the distinction between a procedure and the running of a procedure. A procedure is a set of steps based on a set of rules, while the running of a procedure involves actually following the rules and performing the steps. 

Recursion is related to, but not the same as, a reference within the specification of a procedure to the execution of some other procedure. 

When a procedure is defined as such, this immediately creates the possibility of an endless loop; recursion can only be properly used in a definition if the step in question is skipped in certain cases so that the procedure can complete. 

But even if it is properly defined, a recursive procedure is not easy for humans to perform, as it requires distinguishing the new from the old, partially executed invocation of the procedure; this requires some administration as to how far various simultaneous instances of the procedures have progressed. For this reason, recursive definitions are very rare in everyday situations.

Linguist Noam Chomsky, among many others, has argued that the lack of an upper bound on the number of grammatical sentences in a language, and the lack of an upper bound on grammatical sentence length (beyond practical constraints such as the time available to utter one), can be explained as the consequence of recursion in natural language. 

This can be understood in terms of a recursive definition of a syntactic category, such as a sentence. A sentence can have a structure in which what follows the verb is another sentence: "Dorothy thinks witches are dangerous", in which the sentence "witches are dangerous" occurs in the larger one. So a sentence can be defined recursively (very roughly) as something with a structure that includes a noun phrase, a verb, and optionally another sentence. This is really just a special case of the mathematical definition of recursion.

This provides a way of understanding the creativity of language—the unbounded number of grammatical sentences—because it immediately predicts that sentences can be of arbitrary length: "Dorothy thinks that Toto suspects that Tin Man said that...". There are many structures apart from sentences that can be defined recursively, and therefore many ways in which a sentence can embed instances of one category inside another. Over the years, languages in general have proved amenable to this kind of analysis.

Recently, however, the generally accepted idea that recursion is an essential property of human language has been challenged by Daniel Everett on the basis of his claims about the Pirahã language. Andrew Nevins, David Pesetsky and Cilene Rodrigues are among many who have argued against this. Literary self-reference can in any case be argued to be different in kind from mathematical or logical recursion.

Recursion plays a crucial role not only in syntax, but also in natural language semantics. The word "and", for example, can be construed as a function that can apply to sentence meanings to create new sentences, and likewise for noun phrase meanings, verb phrase meanings, and others. It can also apply to intransitive verbs, transitive verbs, or ditransitive verbs. In order to provide a single denotation for it that is suitably flexible, "and" is typically defined so that it can take any of these different types of meanings as arguments. This can be done by defining it for a simple case in which it combines sentences, and then defining the other cases recursively in terms of the simple one. 

A recursive grammar is a formal grammar that contains recursive production rules.

Recursion is sometimes used humorously in computer science, programming, philosophy, or mathematics textbooks, generally by giving a circular definition or self-reference, in which the putative recursive step does not get closer to a base case, but instead leads to an infinite regress. It is not unusual for such books to include a joke entry in their glossary along the lines of:

A variation is found on page 269 in the index of some editions of Brian Kernighan and Dennis Ritchie's book "The C Programming Language"; the index entry recursively references itself ("recursion 86, 139, 141, 182, 202, 269"). Early versions of this joke can be found in "Let's talk Lisp" by Laurent Siklóssy (published by Prentice Hall PTR on December, 1, 1975 with a copyright date of 1976) and in "Software Tools" by Kernighan and Plauger (published by Addison-Wesley Professional on January, 11, 1976). The joke also appears in "The UNIX Programming Environment" by Kernighan and Pike. It did not appear in the first edition of "The C Programming Language". The joke is part of the Functional programming folklore and was already widespread in the functional programming community before the publication of the aforementioned books. 

Another joke is that "To understand recursion, you must understand recursion." In the English-language version of the Google web search engine, when a search for "recursion" is made, the site suggests "Did you mean: "recursion"." An alternative form is the following, from Andrew Plotkin: ""If you already know what recursion is, just remember the answer. Otherwise, find someone who is standing closer to Douglas Hofstadter than you are; then ask him or her what recursion is.""

Recursive acronyms are other examples of recursive humor. PHP, for example, stands for "PHP Hypertext Preprocessor", WINE stands for "WINE Is Not an Emulator" GNU stands for "GNU's not Unix", and SPARQL denotes the "SPARQL Protocol and RDF Query Language".

The canonical example of a recursively defined set is given by the natural numbers:

In mathematical logic, the Peano axioms (or Peano postulates or Dedekind–Peano axioms), are axioms for the natural numbers presented in the 19th century by the German mathematician Richard Dedekind and by the Italian mathematician Giuseppe Peano. The Peano Axioms define the natural numbers referring to a recursive successor function and addition and multiplication as recursive functions. 

Another interesting example is the set of all "provable" propositions in an axiomatic system that are defined in terms of a proof procedure which is inductively (or recursively) defined as follows:


Finite subdivision rules are a geometric form of recursion, which can be used to create fractal-like images. A subdivision rule starts with a collection of polygons labelled by finitely many labels, and then each polygon is subdivided into smaller labelled polygons in a way that depends only on the labels of the original polygon. This process can be iterated. The standard `middle thirds' technique for creating the Cantor set is a subdivision rule, as is barycentric subdivision.

A function may be recursively defined in terms of itself. A familiar example is the Fibonacci number sequence: "F"("n") = "F"("n" − 1) + "F"("n" − 2). For such a definition to be useful, it must be reducible to non-recursively defined values: in this case "F"(0) = 0 and "F"(1) = 1.

A famous recursive function is the Ackermann function, which, unlike the Fibonacci sequence, cannot be expressed without recursion.

Applying the standard technique of proof by cases to recursively defined sets or functions, as in the preceding sections, yields structural induction — a powerful generalization of mathematical induction widely used to derive proofs in mathematical logic and computer science.

Dynamic programming is an approach to optimization that restates a multiperiod or multistep optimization problem in recursive form. The key result in dynamic programming is the Bellman equation, which writes the value of the optimization problem at an earlier time (or earlier step) in terms of its value at a later time (or later step).

In set theory, this is a theorem guaranteeing that recursively defined functions exist. Given a set "X", an element "a" of "X" and a function formula_7, the theorem states that there is a unique function formula_8 (where formula_4 denotes the set of natural numbers including zero) such that
for any natural number "n".

Take two functions formula_8 and formula_13 such that:

where "a" is an element of "X".

It can be proved by mathematical induction that formula_18 for all natural numbers "n":

By induction, formula_18 for all formula_25.

A common method of simplification is to divide a problem into subproblems of the same type. As a computer programming technique, this is called divide and conquer and is key to the design of many important algorithms. Divide and conquer serves as a top-down approach to problem solving, where problems are solved by solving smaller and smaller instances. A contrary approach is dynamic programming. This approach serves as a bottom-up approach, where problems are solved by solving larger and larger instances, until the desired size is reached.

A classic example of recursion is the definition of the factorial function, given here in C code:

The function calls itself recursively on a smaller version of the input and multiplies the result of the recursive call by , until reaching the base case, analogously to the mathematical definition of factorial.

Recursion in computer programming is exemplified when a function is defined in terms of simpler, often smaller versions of itself. The solution to the problem is then devised by combining the solutions obtained from the simpler versions of the problem. One example application of recursion is in parsers for programming languages. The great advantage of recursion is that an infinite set of possible sentences, designs or other data can be defined, parsed or produced by a finite computer program.

Recurrence relations are equations which define one or more sequences recursively. Some specific kinds of recurrence relation can be "solved" to obtain a non-recursive definition (e.g., a closed-form expression).

Use of recursion in an algorithm has both advantages and disadvantages. The main advantage is usually the simplicity of instructions. The main disadvantage is that the memory usage of recursive algorithms may grow very quickly, rendering them impractical for larger instances.

The Russian Doll or Matryoshka doll is a physical artistic example of the recursive concept.

Recursion has been used in paintings since Giotto's "Stefaneschi Triptych", made in 1320. Its central panel contains the kneeling figure of Cardinal Stefaneschi, holding up the triptych itself as an offering.

M. C. Escher's "Print Gallery" (1956) is a print which depicts a distorted city containing a gallery which recursively contains the picture, and so "ad infinitum".




</doc>
<doc id="25408" url="https://en.wikipedia.org/wiki?curid=25408" title="Robert Byrd">
Robert Byrd

Robert Carlyle Byrd (born Cornelius Calvin Sale Jr.; November 20, 1917June 28, 2010) was an American politician who served as a United States Senator from West Virginia for over 51 years, from 1959 until his death in 2010. A member of the Democratic Party, Byrd previously served as a U.S. Representative from 1953 until 1959. He is the longest-serving U.S. Senator in history, was the longest-serving member in the history of the United States Congress, until surpassed by Representative John Dingell of Michigan; the last remaining member of the U.S. Senate to have served during the presidency of Dwight Eisenhower; and the last remaining member of Congress to have served during the presidency of Harry S. Truman. Byrd is also the only West Virginian to have served in both chambers of the state legislature and both chambers of Congress.

Byrd served in the West Virginia House of Delegates from 1947 to 1950, and the West Virginia State Senate from 1950 to 1952. Initially elected to the United States House of Representatives in 1952, Byrd served there for six years before being elected to the Senate in 1958. He rose to become one of the Senate's most powerful members, serving as secretary of the Senate Democratic Caucus from 1967 to 1971 and—after defeating his longtime colleague, Ted Kennedy—as Senate Majority Whip from 1971 to 1977. Over the next three decades, Byrd led the Democratic caucus in numerous roles depending on whether his party held control of the Senate, including Senate Majority Leader, Senate Minority Leader, President pro tempore of the United States Senate and President pro tempore emeritus. As President pro tempore—a position he held four times in his career—he was third in the line of presidential succession, after the Vice President and the Speaker of the House of Representatives.

Serving three different tenures as Chairman of the United States Senate Committee on Appropriations enabled Byrd to steer a great deal of federal money toward projects in West Virginia. Critics derided his efforts as pork barrel spending, while Byrd argued that the many federal projects he worked to bring to West Virginia represented progress for the people of his state. He filibustered against the 1964 Civil Rights Act and supported the Vietnam War, but later renounced racism and segregation, and spoke in opposition to the Iraq War. Renowned for his knowledge of Senate precedent and parliamentary procedure, Byrd wrote a four-volume history of the Senate in later life.

Near the end of his life, Byrd was in declining health and was hospitalized several times. He died in office on June 28, 2010 at the age of 92 and became the oldest known member of Congress to die in office. He was buried at Columbia Gardens Cemetery in Arlington, Virginia.

Robert Byrd was born on November 20, 1917 as Cornelius Calvin Sale Jr. in North Wilkesboro, North Carolina, to Cornelius Calvin Sale Sr. and his wife Ada Mae (Kirby). When he was ten months old, his mother died in the 1918 flu pandemic. In accordance with his mother's wishes, his father dispersed their children among relatives. Calvin Jr. was adopted by his aunt and uncle, Titus and Vlurma Byrd, who changed his name to Robert Carlyle Byrd and raised him in the coal-mining region of southern West Virginia.

Byrd was valedictorian of his 1934 graduating class at Mark Twain High School in Tams, West Virginia.

On May 29, 1936, Byrd married Erma Ora James (June 12, 1917 – March 25, 2006) who was born to a coal mining family in Floyd County, Virginia. Her family moved to Raleigh County, West Virginia, where she met Byrd when they attended the same high school.

Robert Byrd had two daughters (Mona Byrd Fatemi and Marjorie Byrd Moore), six grandchildren, and seven great-grandchildren.

In the early 1940s, Byrd recruited 150 of his friends and associates to create a new chapter of the Ku Klux Klan in Sophia, West Virginia.

According to Byrd, a Klan official told him, "You have a talent for leadership, Bob ... The country needs young men like you in the leadership of the nation." Byrd later recalled, "Suddenly lights flashed in my mind! Someone important had recognized my abilities! I was only 23 or 24 years old, and the thought of a political career had never really hit me. But strike me that night, it did." Byrd became a recruiter and leader of his chapter. When it came time to elect the top officer (Exalted Cyclops) in the local Klan unit, Byrd won unanimously.

In December 1944, Byrd wrote to segregationist Mississippi Senator Theodore G. Bilbo:

In 1946, Byrd wrote a letter to a Grand Wizard stating, "The Klan is needed today as never before, and I am anxious to see its rebirth here in West Virginia and in every state in the nation." However, when running for the United States House of Representatives in 1952, he announced "After about a year, I became disinterested, quit paying my dues, and dropped my membership in the organization. During the nine years that have followed, I have never been interested in the Klan." He said he had joined the Klan because he felt it offered excitement and was anti-communist.

Byrd later called joining the KKK "the greatest mistake I ever made." In 1997, he told an interviewer he would encourage young people to become involved in politics but also warned, "Be sure you avoid the Ku Klux Klan. Don't get that albatross around your neck. Once you've made that mistake, you inhibit your operations in the political arena." In his last autobiography, Byrd explained that he was a KKK member because he "was sorely afflicted with tunnel vision—a jejune and immature outlook—seeing only what I wanted to see because I thought the Klan could provide an outlet for my talents and ambitions." Byrd also said in 2005, "I know now I was wrong. Intolerance had no place in America. I apologized a thousand times ... and I don't mind apologizing over and over again. I can't erase what happened."

Byrd worked as a gas station attendant, a grocery store clerk, a shipyard welder during World War II, and a butcher before he won a seat in the West Virginia House of Delegates in 1946, representing Raleigh County from 1947 to 1950. Byrd became a local celebrity after a radio station in Beckley began broadcasting his "fiery fundamentalist lessons." In 1950, he was elected to the West Virginia Senate, where he served from December 1950 to December 1952.

In 1951, Byrd was among the official witnesses of the execution of Harry Burdette and Fred Painter, which was the first use of the electric chair in West Virginia. In 1965 the state abolished capital punishment, with the last execution having occurred in 1959.

Early in his career Byrd attended Beckley College, Concord College, Morris Harvey College, Marshall College, and George Washington University Law School, and joined the Tau Kappa Epsilon fraternity.

Byrd began night classes at American University Washington College of Law in 1953, while a member of the United States House of Representatives. He earned his J.D. "cum laude" a decade later, by which time he was a U.S. Senator. President John F. Kennedy spoke at the commencement ceremony on June 10, 1963 and presented the graduates their diplomas, including Byrd. Byrd completed law school in an era when undergraduate degrees were not a requirement. He later decided to complete his Bachelor of Arts degree in political science, and in 1994 he graduated "summa cum laude" from Marshall University.

In 1952, Byrd was elected to the United States House of Representatives for West Virginia's 6th congressional district, succeeding E. H. Hedrick, who retired from the House to make an unsuccessful run for the Democratic nomination for Governor. Byrd was re-elected twice from this district, anchored in Charleston and also including his home in Sophia, serving from January 3, 1953 to January 3, 1959. Byrd defeated Republican incumbent W. Chapman Revercomb for the United States Senate in 1958. Revercomb's record supporting civil rights had become an issue, playing in Byrd's favor. Byrd was re-elected to the Senate eight times. He was West Virginia's junior senator for his first four terms; his colleague from 1959 to 1985 was Jennings Randolph, who had been elected on the same day as Byrd's first election in a special election to fill the seat of the late Senator Matthew Neely.
While Byrd faced some vigorous Republican opposition in his career, his last serious electoral opposition occurred in 1982 when he was challenged by freshman Congressman Cleve Benedict. Despite his tremendous popularity in the state, Byrd ran unopposed only once, in 1976. On three other occasions – in 1970, 1994 and 2000 – he won all 55 of West Virginia's counties. In his re-election bid in 2000, he won all but seven precincts. Congresswoman Shelley Moore Capito, the daughter of one of Byrd's longtime foes, former governor Arch Moore Jr., briefly considered a challenge to Byrd in 2006 but decided against it. Capito's district covered much of the territory Byrd had represented in the U.S. House.

In the 1960 Democratic presidential election primaries, Byrd – a close Senate ally of Lyndon B. Johnson – endorsed and campaigned for Hubert Humphrey over front-runner John F. Kennedy in the state's crucial primary. However, Kennedy won the state's primary and eventually the general election.

Byrd was elected to a record ninth consecutive full Senate term on November 7, 2006. He became the longest-serving senator in American history on June 12, 2006, surpassing Strom Thurmond of South Carolina with 17,327 days of service. On November 18, 2009, Byrd became the longest-serving member in congressional history, with 56 years, 320 days of combined service in the House and Senate, passing Carl Hayden of Arizona. Previously, Byrd had held the record for the longest unbroken tenure in the Senate (Thurmond resigned during his first term and was re-elected seven months later). He is the only senator ever to serve more than 50 years. Including his tenure as a state legislator from 1947 to 1953, Byrd's service on the political front exceeded 60 continuous years. Byrd, who never lost an election, cast his 18,000th vote on June 21, 2007, the most of any senator in history. John Dingell broke Byrd's record as longest-serving member of Congress on June 7, 2013.

Upon the death of former Florida Senator George Smathers on January 20, 2007, Byrd became the last living United States Senator from the 1950s.

Having taken part in the admission of Alaska and Hawaii to the union, Byrd was the last surviving senator to have voted on a bill granting statehood to a U.S. territory. At the time of Byrd's death, fourteen sitting or former members of the Senate had not been born when Byrd's tenure in the Senate began, President Barack Obama among them.

These are the committee assignments for Sen. Byrd's 9th and final term.

Byrd was a member of the wing of the Democratic Party that opposed federally-mandated desegregation and civil rights. However, despite his early career in the KKK, Byrd was linked to such senators as John C. Stennis, J. William Fulbright and George Smathers, who based their segregationist positions on their view of states' rights in contrast to senators like James Eastland, who held a reputation as a committed racist.

Byrd joined with Democratic senators to filibuster the Civil Rights Act of 1964, personally filibustering the bill for 14 hours, a move he later said he regretted. Despite an 83-day filibuster in the Senate, both parties in Congress voted overwhelmingly in favor of the Act (Democrats 47-16, Republicans 30-2), and President Johnson signed the bill into law. Byrd cast no vote on the Voting Rights Act of 1965, and voted against the confirmation of Thurgood Marshall to the U.S. Supreme Court. He did not sign the 1956 Southern Manifesto and voted for the Civil Rights Acts of 1957, 1960, and 1968. In 2005, Byrd told "The Washington Post" that his membership in the Baptist church led to a change in his views. In the opinion of one reviewer, Byrd, like other Southern and border-state Democrats, came to realize that he would have to temper "his blatantly segregationist views" and move to the Democratic Party mainstream if he wanted to play a role nationally.

In February 1968, Byrd questioned General Earle Wheeler during the latter's testimony to the Senate Armed Services Committee. During a White House meeting between President Johnson and congressional Democratic leaders on February 6, Byrd stated his concern for the ongoing Vietnam War, citing the US's lack of intelligence, preparation, underestimating of the morale and vitality of the Viet Cong, and overestimated how backed Americans would be by the South Vietnam government. President Johnson rejected Byrd's observations. "Anyone can kick a barn down. It takes a good carpenter to build one."

During the 1968 Democratic Party presidential primaries, Byrd supported the incumbent President Johnson. Of the challenging Robert F. Kennedy, Byrd said, "Bobby-come-lately has made a mistake. I won't even listen to him. There are many who liked his brother—as Bobby will find out—but who don't like him." Byrd praised Chicago Mayor Richard J. Daley's police response to protest activity at that year's Democratic National Convention, stating that the violence that resulted was the fault of the protesters, while the police only tried to restore order. Vice President Hubert H. Humphrey won the presidential nomination, and Byrd campaigned for him that fall.

Byrd served in the Senate Democratic leadership. He succeeded George Smathers as secretary of the Senate Democratic Conference from 1967 to 1971. He unseated Ted Kennedy in 1971 to become majority whip, or the second highest-ranking Democrat, until 1977. Smathers recalled that, "Ted was off playing. While Ted was away at Christmas, down in the islands, floating around having a good time with some of his friends, male and female, here was Bob up here calling on the phone. 'I want to do this, and would you help me?' He had it all committed so that when Teddy got back to town, Teddy didn't know what hit him, but it was already all over. That was Lyndon Johnson's style. Bob Byrd learned that from watching Lyndon Johnson." Byrd himself had told Smathers that " I have never in my life played a game of cards. I have never in my life had a golf club in my hand. I have never in life hit a tennis ball. I have—believe it or not—never thrown a line over to catch a fish. I don't do any of those things. I have only had to work all my life. And every time you told me about swimming, I don't know how to swim."

In 1976, Byrd was the "favorite son" Presidential candidate in West Virginia's primary. His easy victory gave him control of the delegation to the Democratic National Convention. Byrd had the inside track as majority whip but focused most of his time running for majority leader, more so than for re-election to the Senate, as he was virtually unopposed for his fourth term. By the time the vote for majority leader came, his lead was so secure that his lone rival, Minnesota's Hubert Humphrey, withdrew before the balloting took place. From 1977 to 1989 Byrd was the leader of the Senate Democrats, serving as majority leader from 1977 to 1981 and 1987 to 1989, and as minority leader from 1981 to 1987.

Byrd was well known for steering federal dollars to West Virginia, one of the country's poorest states. He was called the "King of Pork" by Citizens Against Government Waste. After becoming chair of the Appropriations Committee in 1989, Byrd set a goal securing a total of for public works in the state. He passed that mark in 1991, and funds for highways, dams, educational institutions, and federal agency offices flowed unabated over the course of his membership. More than 30 existing or pending federal projects bear his name. He commented on his reputation for attaining funds for projects in West Virginia in August 2006, when he called himself "Big Daddy" at the dedication for the Robert C. Byrd Biotechnology Science Center. Examples of this ability to claim funds and projects for his state include the Federal Bureau of Investigation's repository for computerized fingerprint records as well as several United States Coast Guard computing and office facilities.

Byrd also was known for using his knowledge of parliamentary procedure. Byrd frustrated Republicans with his encyclopedic knowledge of the inner workings of the Senate, particularly prior to the Reagan Revolution. From 1977 to 1979 he was described as "performing a procedural tap dance around the minority, outmaneuvering Republicans with his mastery of the Senate's arcane rules." In 1988, majority leader Byrd moved a call of the Senate, which was adopted by the majority present, in order to have the Sergeant-at-Arms arrest members not in attendance. One member (Robert Packwood, R-Oregon) was escorted back to the chamber by the Sergeant-at-Arms in order to obtain a quorum.

As the longest-serving Democratic senator, Byrd served as President pro tempore four times when his party was in the majority: from 1989 until the Republicans won control of the Senate in 1995; for 17 days in early 2001, when the Senate was evenly split between parties and outgoing Vice President Al Gore broke the tie in favor of the Democrats; when the Democrats regained the majority in June 2001 after Senator Jim Jeffords of Vermont left the Republican Party to become an independent; and again from 2007 to his death in 2010, as a result of the 2006 Senate elections. In this capacity, Byrd was third in the line of presidential succession at the time of his death, behind Vice President Joe Biden and House Speaker Nancy Pelosi.

In 1969, Byrd launched a Scholastic Recognition Award; he also began to present a savings bond to valedictorians from high schools—public and private—in West Virginia. In 1985 Congress approved the nation's only merit-based scholarship program funded through the U.S. Department of Education, a program which Congress later named in Byrd's honor. The Robert C. Byrd Honors Scholarship Program initially comprised a one-year, $1,500 award to students with "outstanding academic achievement" who had been accepted at a college or university. In 1993, the program began providing four-year scholarships.

In 2002 Byrd secured unanimous approval for a major national initiative to strengthen the teaching of "traditional American history" in K-12 public schools. The Department of Education competitively awards $50 to a year to school districts (in amounts of about $500,000 to ). The money goes to teacher training programs that are geared to improving the knowledge of history teachers. The Continuing Appropriations Act, 2011 eliminated funding for the Robert C. Byrd Honors Scholarship Program.

Television cameras were first introduced to the House of Representatives on March 19, 1979, by C-SPAN. Unsatisfied that Americans only saw Congress as the House of Representatives, Byrd and others pushed to televise Senate proceedings to prevent the Senate from becoming the "invisible branch" of government, succeeding in June 1986.

To help introduce the public to the inner workings of the legislative process, Byrd launched a series of one hundred speeches based on his examination of the Roman Republic and the intent of the Framers. Byrd published a four-volume series on Senate history: "The Senate: 1789–1989: Addresses on the History of the Senate". The first volume won the Henry Adams Prize of the Society for History in the Federal Government as "an outstanding contribution to research in the history of the Federal Government." He also published "The Senate of the Roman Republic: Addresses on the History of Roman Constitutionalism".

In 2004, Byrd received the American Historical Association's first Theodore Roosevelt-Woodrow Wilson Award for Civil Service; in 2007, Byrd received the Friend of History Award from the Organization of American Historians. Both awards honor individuals outside the academy who have made a significant contribution to the writing and/or presentation of history. In 2014, The Byrd Center for Legislative Studies began assessing the archiving of Senator Byrd's electronic correspondence and floor speeches in order to preserve these documents and make them available to the wider community.

On July 19, 2007, Byrd gave a 25-minute speech in the Senate against dog fighting, in response to the indictment of football player Michael Vick. In recognition of the speech, People for the Ethical Treatment of Animals named Byrd their "2007 Person of the Year".

For 2007, Byrd was deemed the fourteenth-most powerful senator, as well as the twelfth-most powerful Democratic senator.

On May 19, 2008, Byrd endorsed then-Senator Barack Obama for president. One week after the West Virginia Democratic Primary, in which Hillary Clinton defeated Obama by 67 to 25 percent, Byrd said, "Barack Obama is a noble-hearted patriot and humble Christian, and he has my full faith and support." When asked in October 2008 about the possibility that the issue of race would influence West Virginia voters, as Obama is an African-American, Byrd replied, "Those days are gone. Gone!" Obama lost West Virginia (by 13%) but won the election.

On January 26, 2009, Byrd was one of three Democrats to vote against the confirmation of Timothy Geithner as United States Secretary of the Treasury (along with Russ Feingold of Wisconsin and Tom Harkin of Iowa).

On February 26, 2009, Byrd was one of two Democrats to vote against the District of Columbia House Voting Rights Act of 2009, which if it had become law would have added a voting seat in the United States House of Representatives for the District of Columbia and add a seat for Utah, explaining that he supported the intent of the legislation, but regarded it as an attempt to solve with legislation an issue which required resolution with a Constitutional amendment. (Democrat Max Baucus of Montana also cast a "nay" vote.)

Although his health was poor, Byrd was present for every crucial vote during the December 2009 Senatorial healthcare debate; his vote was necessary so Democrats could obtain cloture to break a Republican filibuster. At the final vote on December 24, 2009, Byrd referenced recently deceased Senator Ted Kennedy, a devoted proponent, when casting his vote: "Mr. President, this is for my friend Ted Kennedy! Aye!"

Byrd initially compiled a mixed record on the subjects of race relations and desegregation. While he initially voted against civil rights legislation, in 1959 he hired one of the Capitol's first black congressional aides, and he also took steps to integrate the United States Capitol Police for the first time since Reconstruction. Beginning in the 1970s, Byrd explicitly renounced his earlier views favoring racial segregation. Byrd said that he regretted filibustering and voting against the Civil Rights Act of 1964 and would change it if he had the opportunity. Byrd also said that his views changed dramatically after his teenage grandson was killed in a 1982 traffic accident, which put him in a deep emotional valley. "The death of my grandson caused me to stop and think," said Byrd, adding he came to realize that African-Americans love their children as much as he does his. During debate in 1983 over the passage of the law creating the Martin Luther King Jr. Day holiday, Byrd grasped the symbolism of the day and its significance to his legacy, telling members of his staff "I'm the only one in the Senate who must vote for this bill".

Of the seven U.S. Senators to vote on the confirmations of both Thurgood Marshall and Clarence Thomas to the United States Supreme Court (the others being Daniel Inouye of Hawaii, Ted Kennedy of Massachusetts, Quentin Burdick of North Dakota, Mark Hatfield of Oregon, and Fritz Hollings and Strom Thurmond of South Carolina), Byrd was the only senator to vote against confirming both of the only two African-American nominees to the Court in its history. In Marshall's case, Byrd asked FBI Director J. Edgar Hoover to look into the possibility that Marshall had either connections to communists or a communist past. With respect to Thomas, Byrd stated that he was offended by Thomas's use of the phrase "high-tech lynching of uppity blacks" in his defense and that he was "offended by the injection of racism" into the hearing. He called Thomas's comments a "diversionary tactic" and said, "I thought we were past that stage." Regarding Anita Hill's sexual harassment charges against Thomas, Byrd supported Hill. Byrd joined 45 other Democrats in voting against confirming Thomas to the Supreme Court.

On March 29, 1968, Byrd criticized a Memphis, Tennessee protest: "It was a shameful and totally uncalled for outburst of lawfulness undoubtedly encouraged to some considerable degree, at least, by his [Dr. King's] words and actions, and his presence. There is no reason for us to believe that the same destructive rioting and violence cannot, or that it will not, happen here if King attempts his so-called Poor People's March, for what he plans in Washington appears to be something on a far greater scale than what he had indicated he planned to do in Memphis."

In a March 2, 2001 interview with Tony Snow, Byrd said of race relations:

Byrd's use of the term "white nigger" created immediate controversy. When asked about it, Byrd's office provided this in a written response,

For the 2003–2004 session, the National Association for the Advancement of Colored People (NAACP) rated Byrd's voting record as being 100% in line with the NAACP's position on the thirty-three Senate bills they evaluated. Sixteen other senators received that rating. In June 2005, Byrd proposed an additional $10,000,000 in federal funding for the Martin Luther King Jr. National Memorial in Washington, D.C., remarking that, "With the passage of time, we have come to learn that his Dream was the American Dream, and few ever expressed it more eloquently." Upon news of his death, the NAACP released a statement praising Byrd, saying that he "became a champion for civil rights and liberties" and "came to consistently support the NAACP civil rights agenda".

Byrd initially said that the impeachment proceedings against Clinton should be taken seriously. Although he harshly criticized any attempt to make light of the allegations, he made the motion to dismiss the charges and effectively end the matter. Even though he voted against both articles of impeachment, he was the sole Democrat to vote to censure Clinton.

Byrd strongly opposed Clinton's 1993 efforts to allow gays to serve in the military and supported efforts to limit gay marriage. In 1996, before the passage of the Defense of Marriage Act, he said, "The drive for same-sex marriage is, in effect, an effort to make a sneak attack on society by encoding this aberrant behavior in legal form before society itself has decided it should be legal. [...] Let us defend the oldest institution, the institution of marriage between male and female as set forth in the Holy Bible."

Despite his previous position, he later stated his opposition to the Federal Marriage Amendment and argued that it was unnecessary because the states already had the power to ban gay marriages. However, when the amendment came to the Senate floor, he was one of the two Democratic senators who voted in favor of cloture.

On March 11, 1982, Byrd voted against a measure sponsored by Senator Orrin Hatch that sought to reverse "Roe v. Wade" and allow Congress and individual states to adopt laws banning abortions. Its passing was the first time a congressional committee supported an anti-abortion amendment.

In 1995, Byrd voted against a ban on intact dilation and extraction, a late-term abortion procedure typically referred to by its opponents as "partial-birth abortion". In 2003, however, he voted for the Partial-Birth Abortion Ban Act, which prohibits intact dilation and extraction. Byrd also voted against the 2004 Unborn Victims of Violence Act, which recognizes a "child in utero" as a legal victim if he or she is injured or killed during the commission of a crime of violence.

In April 1970, the Senate Judiciary Committee approved a plan to replace the Electoral College with direct elections of presidents. Byrd initially opposed direct elections on the key vote and was one of two senators to switch votes in favor of the proposal during later votes.

In April 1970, as the Senate Judiciary Committee delayed a vote on Supreme Court nominee Harry Blackmun, Byrd stated that "no nomination should be voted on within 24 hours after the hearing" after the previous two Supreme Court nominees had delays and was one of the 17 committee members who went on record of assuring Blackmun's nomination would be reported favorably to the full Senate.

In October 1970, Byrd sponsored an amendment protecting members of Congress and those elected that have not yet assumed office. Byrd mentioned the 88 political assassinations in the United States and said state law was not adequate to handle the increase in political violence.

In February 1971, after Fred R. Harris and Charles Mathias requested the Senate Rules Committee change the rules to permit selection of committee chairmen on a basis aside from seniority, Byrd indicated through his line of questioning that he saw considerable value in the seniority system.

In April 1971, after Representative Hale Boggs stated that he had been tapped by the Federal Bureau of Investigation and called on FBI Director J. Edgar Hoover to resign, Byrd opined that Boggs' imagination was involved and called on him to reveal any possible "good, substantial, bona fide evidence".

In April 1971, Byrd met with President Nixon, Hugh Scott, and Robert P. Griffin for a briefing that after which Byrd, Scott, and Griffin asserted they had been told by Nixon of his intent to withdraw American forces from Indochina by a specific date. White House Press Secretary Ronald L. Ziegler disputed their claims by stating that the three had not been told anything by Nixon he had not mentioned in his speech the same day as the meeting.

In April 1971, Jacob Javits, Fred R. Harris, and Charles H. Percy circulated letters to their fellow Senators in an attempt to gain cosponsors for a resolution appoint the Senate's first girl pages. Byrd maintained that the Senate was ill-equipped for girl pages and was among those that cited the long hours of work, the carrying or sometimes heavy documents and the high crime rate in the Capitol area as among the reasons against it.

In September 1971, Representative Richard H. Poff was under consideration by President Nixon for a Supreme Court nomination, Byrd warning Poff that his nomination could be met with opposition by liberal senators and see a filibuster emerge. Within hours, Poff announced his declining of the nomination.

In April 1972, Senate Majority Leader Mansfield announced that he had authorized Byrd to present an amendment to the Senate for a fixed deadline for total troop withdrawal that the Nixon administration would be obligated to meet and that the measure would serve as an amendment to the State Department‐United States Information Agency authorization bill.

In April 1972, the Senate Judiciary Committee approved the nomination of Richard G. Kleindienst as United States Attorney General, Byrd being one of four Democrats to support the nomination. On June 7, Byrd announced that he would vote against Kleindienst, saying in a news release that this was Nixon's first nomination that he had not voted to confirm and that testimony at hearings investigating Kleindienst's tenure at the International Telephone and Telegraph Corporation displayed "a show of arrogance and deception and insensitivity to the people's right to know."

In a May 1972 luncheon speech, Byrd criticized American newspapers for "an increasing tendency toward shoddy technical production" and observed that there was "a greater schism between the Nixon Administration and the media, at least publicly, than at any previous time in our history."

In May 1972, Byrd introduced a proposal supported by the Nixon administration that would make cutting off all funding for American hostilities in Indochina conditional upon agreement on an internationally supervised cease‐fire. Byrd and Nixon supporters argued modification would bring the amendment more in line with President Nixon's proposal to withdraw all American forces from Vietnam the previous week and it was approved in the Senate by a vote of 47 to 43.

In September 1972, Edward Brooke attempted to reintroduce his war ending amendment that had been defeated earlier in the week as an addendum to a clean drinking water bill when he discovered that Byrd had arranged a unanimous consent free agreement prohibiting amendments that were not relevant to the subject. Brooke charged the Byrd agreements with impairing his senatorial prerogatives to introduce amendments.

During the 1972 general election campaign, Democratic nominee George McGovern advocated for partial amnesty for draft dodges. Byrd responded to the position in a November speech the day before the election without mentioning McGovern by name in saying, "How could we keep faith with the thousands of Americans we sent to Vietnam by giving a mere tap on the wrist to those who fled to Canada and Sweden?" Byrd said the welfare proposals were part of "pernicious doctrine that the Federal Government owes a living to people who don't want to work" and chastised individuals that had personal trips to Hanoi rather than official missions as "the Ramsey Clarks in our society who attempt to deal unilaterally with the enemy."

In January 1973, the Senate passed legislation containing an amendment Byrd offered requiring President Nixon to give Congress an accounting of all funds that he had impounded and appropriated by February 5. Byrd stated that President Nixon had been required to submit reports to Congress and that he had not done so since June, leaving Congress in the dark on the matter.

In February 1973, the Senate approved legislation requiring confirmation of the director and deputy director of the Office of Management and Budget in the White House in what was seen as "another battleground for the dispute between Congress and the White House over cuts in social spending programs in the current Federal budget and in the Nixon Administration's spending request for the fiscal year 1974, which begins next July 1". The legislation contained an amendment sponsored by Byrd limiting the budget officials to a maximum term of four years before having another confirmation proceeding. Byrd introduced another amendment that required all Cabinet officers be required to undergo reconfirmation by the Senate in the event that they are retained from one administration to another.

In March 1973, Byrd led Senate efforts to reject a proposal that would have made most critical committee meetings open to the public, arguing that tampering with "the rides of the Senate is to tamper with the Senate itself" and argued against changing "procedures which, over the long past, have contributed to stability and efficiency in the operation of the Senate." The Senate voted down the proposal 47 to 38 on March 7.

On May 2, 1973, the anniversary of FBI Director J. Edgar Hoover's death, Byrd called on President Nixon to appoint a permanent successor for Hoover.

In June 1973, Byrd sponsored a bill that would impose the first Tuesday in October as the date for all federal elections and mandate that states hold primary elections for federal elections between the first Tuesday in June and the first Tuesday in July. Senate Rules Committee approved the measure on June 13 and it was sent to the Senate floor for consideration.

In June 1973, along with Lloyd Bentsen, Mike Mansfield, John Tower, and Jennings Randolph, Byrd was one of five senators to switch their vote on the foreign military aid authorization bill to assure its passage after previously voting against it.

In October 1973, President Nixon vetoed the request of the United States Information Agency for 208 million for fiscal year 1974 on the grounds of a provision forcing the agency to provide any document or information demanded. Byrd introduced a bill identical to the one vetoed by Nixon the following month, differing in not containing the information provision as well as a ban on appropriating or spending more money than the annual budget called for, the Senate approving the legislation on November 13.

In November 1973, after the Senate rejected an amendment to the National Energy Emergency Act intending to direct President Nixon to put gasoline rationing into effect on January 15, Byrd indicated the final vote not coming for multiple days.

In June 1974, the Senate confirmed John C. Sawhill as Federal Energy Administrator only to rescind the confirmation hours later, the direct result of James Abourezk wanting to speak out and vote against the nomination due to the Nixon administration's refusal to roll back crude oil prices. Abourezk confirmed that he had asked Byrd for notice of when he could assume the Senate floor to deliver his remarks. Byrd was absent when present members passed the nomination as part of their efforts to clear the chamber's executive calendar and rescinded the confirmation.

In May 1974, the House Judiciary Committee opened impeachment hearings against President Nixon after the release of 1,200 pages of transcripts of White House conversations between him and his aides and the administration became engulfed in the scandal that would come to be known as "Watergate". That month, Byrd delivered a speech on the Senate floor opposing Nixon's potential resignation, saying it would serve only to convince the President's supporters that his enemies had driven him out of office: "The question of guilt or innocence would never be fully resolved. The country would remain polarized — more so than it is today. And confidence in government would remain unrestored." Most of the members of the Senate in attendance for the address were conservatives from both parties that shared opposition to Nixon being removed from office. Byrd was among multiple conservative senators who stated that they would not ask Nixon to resign. Later that month, Republican Attorney General Elliot L. Richardson termed Nixon "a law and order President who says subpoenas must he answered by everyone except himself," the comment being echoed by Byrd who additionally charged President Nixon with reneging on his public pledge that the independence of the special prosecutor to pursue the Watergate investigation would not be limited without the prior approval of a majority of Congressional leaders.

On July 29, Byrd met with Senate Majority Leader Mike Mansfield, Minority Leader Hugh Scott, and Republican whip Robert P. Griffin in the first formality by Senate leaders on the matter of President Nixon's impeachment. Byrd opposed Nixon being granted immunity. "The New York Times" noted that as Chairman of the Republican National Committee George H. W. Bush issued a formal statement indicating no chance for the Nixon administration to be salvaged, Byrd was advocating for President Nixon to face some punishment for the illegal activities of the administration and that former Vice President Spiro Agnew should have been imprisoned. The Senate leadership met throughout August 7 to discuss Nixon's fate, the topic of immunity being mentioned in the office of Hugh Scott. Nixon announced his resignation the following day and resigned on August 9. The resignation led to Congress rearranging their intent from an impeachment to the confirmation of a new vice presidential nominee and the Senate scheduled a recess between August 23 to September 14, Byrd opining, "What the country needs is for all of us to get out of Washington and let the country have a breath of fresh air." By August 11, Hugh Scott announced he was finding fewer members of Congress from either party committed to criminally prosecuting former President Nixon over "Watergate", Byrd and Majority Leader Mansfield both indicating their favoring for Nixon's culpability being left in the consideration of Special Prosecutor Leon Jaworski and the "Watergate" grand jury.

On November 22, 1974, the Senate Rules Committee voted unanimously to recommend the nomination of Nelson Rockefeller as Vice President of the United States to the full Senate. Byrd admitted that he had preferred sending the nomination with no recommendation but was worried the act would apply prejudice to the nominee.

In January 1975, after President Ford requested 300 million in additional military aid for South Vietnam and 222 million more for Cambodia from Congress, Byrd said Ford and Secretary of State Henry Kissinger had described the aid as "imperative" and that congressional leaders had been told North Vietnam would take over Saigon "little by little" if additional ammunition and other aid were not provided by the US to Saigon. In February, along with Mike Mansfield, Hugh Scott, and Robert P. Griffin, Byrd was one of four senators to sponsor a compromise modification of the Senate's filibuster rule where three-fifths of the total Senate membership would be adequate in invoking closure on any measure except a change in the Senate's rules. In March, while the Senate voted on reforming its filibuster rule, James B. Allen and other senators used their allotted time to speak at length and also force a series of votes. In response, Byrd said the group was engaging in an "exercise in futility" and that the chamber had already made up its mind. In April, after President Ford and his administration's lawyers contended that Ford had authority as president to use troops under the War Powers Act, Byrd and Thomas F. Eagleton objected by charging that Ford was establishing a dangerous precedent. Byrd issued a statement on the Senate floor admitting his "serious reservations" pertaining to the Ford administration's intent to bring roughly 130,000 South Vietnamese refugees to the United States, citing cultural differences and unemployment as raising "grave doubts about the wisdom of bringing any sizable number of evacuees here." In May, after President Ford appealed for Americans to support the resettlement of 130,000 Vietnamese and Cambodians in the US, Byrd told reporters that he believed that President Ford's request for 507 million for refugee transport and resettlement would be reduced, citing its lack of political support in the United States. In September, Byrd sponsored an amendment to the appropriations bill that if enacted would bar the education department from ordering busing to the school nearest to a pupil's home and sought to hold the Senate floor until there was an agreement among colleagues on his proposal. This failed, as the time limit for debating various proposals ran out. On November 10, Byrd met with President Ford for a discussion on the New York loan guarantee bill.

In April 1976, Byrd was one of five members of the Senate Select Committee to vote for a requirement that the proposed oversight committee would share Its jurisdiction with four committees that had authority over intelligence operations. In June, after the Senate Judiciary Committee voted to send a bill breaking up 18 large oil companies into separate production, refining and refining‐marketing entities to the Senate floor, Byrd announced his opposition to divestiture and joined Republicans Hugh Scott and Charles Mathias in confirming their votes were to report the bill. In September, Congress overrode President Ford's veto of a 56 billion appropriations bill for social services, Ford afterward telling Byrd and House Speaker Carl Albert that he would sign two bills supported by the Democrats.

Byrd was elected Majority Leader on January 4, 1977. On January 14, President Ford met with congressional leadership to announce his proposals for pay increases of high government officials, Byrd afterward telling reporters that the president had also stated his intent to recommend that the raises be linked to a code of conduct. Days later, after the Senate established a special 15‐member committee to draw up a code of ethics for senators, Byrd told reporters that he was supportive of the measure and it would be composed of eight Democrats and seven Republicans who would have until March 1 to issue a draft code that would then be subject to change by the full Senate.

In January 1977, after President-elect Carter announced his nomination of Theodore C. Sorensen to be Director of Central Intelligence, Byrd admitted to reporters that there could be difficulty securing a Senate confirmation. Conservative opposition to Sorenson's nomination led Carter to conclude that he could not be confirmed, and Carter withdrawing it without the Senate taking action.

On January 18, 1977, after the Senate established a special 15‐member committee to draw up a code of ethics for senators, Byrd and Senate Minority Leader Howard Baker announced their support for the resolution, Byrd adding that knowledge of the code of ethics being enacted in the Senate would be privy to the public, press, and members of the Senate. While eight of Carter's secretaries were confirmed within the first hours of his presidency, Byrd made an unsuccessful effort to secure a date and time limit for debate on the confirmation of F. Ray Marshall, Carter's nominee for United States Secretary of Labor.

Between January and February 1979, Byrd proposed outlawing tactics frequently used to prevent him from bringing a bill to the floor for consideration. He stated the filibuster tactics gave the Senate a bad reputation and rendered it ineffective. His proposals initially earned the opposition of Republicans and conservative Democrats until there was a compromise for the reform package to be split and have the less objectionable part come up first for consideration. The Senate passed legislation curtailing tactics that had been used in the past to continue filibusters after cloture had been invoked on February 22. In March, Byrd negotiated an agreement that a proposed amendment was referred to the Judiciary Committee and would be reported by April 10. The arrangement stated that Byrd could call up the proposed amendment any time following June 1 and his action would not be subject to a filibuster while the resolution embodying the amendment will.

In October 1977, Byrd stated his refusal to authorize the Senate dropping consideration of the natural gas legislation under any circumstances, predicting the matter would be settled in the coming days as a result of conversations with colleagues he had the night before and a growing disillusion with filibusters in place of action on legislation. Byrd added that the deregulation bill would not become law due to it being identical to the Carter administration's proposal and President Carter's prior statement that he would veto deregulation bills.

In May 1978, Byrd announced that he would not move to end a filibuster against the Carter administration's labor law revision bill until after the Memorial Day recess. The decision was seen as allowing wavering senators to not be cornered on their votes as lobbying efforts for both business and labor commenced and various opponents of the bill viewed Byrd's call as a sign of weakness toward the Carter administration. Byrd stated that his decision to wait was "to give ample time for debate on the measure" and that he was expecting the first petition to end the filibuster to come sometime following the Senate returning in June.

In March 1979, after Attorney General Griffin B. Bell named a special counsel in the Carter warehouse investigation, Byrd stated his dissatisfaction with the move in a Senate floor speech, citing the existence of legislation approved by Congress the previous year that would allow the appointment of a special prosecutor. In June, Director of Public Citizens Congress Watch Mark Green stated that President Carter had told him that Majority Leader Byrd had threatened that he would personally lead a filibuster against any attempt to extend controls on domestic oil prices. In response, Byrd press secretary Mike Willard confirmed that Byrd told President Carter he would not vote for cloture in the event of a filibuster. Days later, after the Senate voted to grant President Carter authority to set energy conservation targets for each of the 50 states and allow Carter to impose mandatory measures on any state that failed to implement a plan to meet the targets he set, Byrd reaffirmed his opposition to attempts aimed at President Carter's decision to remove price controls from crude oil produced within the United States. In November, Byrd stated that the United States did not have an alternative to coal when attempting to meet its energy needs and that the technology needed to turn coal into liquid fuel at a lower cost than that of producing gasoline had already been made available, opining that doing this would solve most environmental problems. Weeks later, Sergeant at Arms of the United States Senate F. Nordy Hoffman sent a letter to Byrd warning him to take precautions against possible attacks by religious fanatics and nationalist terrorists and advocating for senators "vary their daily routines, take different routes to and from the Senate, exchange their personalized license plates for those that provide anonymity and be generally alert to the possibility of attack." Byrd distributed the letter to the other members of the chamber of Congress. In December, the Senate voted on a Republican proposal to limit overall Government tax revenue that would also yield an annual tax cut of 39 billion to 55 billion over the course of the following four years. Republican William V. Roth Jr. sponsored an amendment that Byrd moved to table Senator Roth's request for a budget waiver and won by five votes. The Senate narrowly blocked the proposal. By December, congressional leadership was aiming for President Carter to sign a new synthetic fuels bill before Christmas, with Byrd wanting the bill to contain a 185 billion revenue that was achieved in a minimum tax provision. Later that month, after the Senate approved a 1.5 billion in Federal loan guarantees for the Chrysler Corporation tonight after defeating a proposal to provide emergency, Byrd confirmed that he had spoken with United States Secretary of the Treasury G. William Miller about what Byrd called "excellent" chances that the Senate would complete work on a federal loans guarantees bill for Chrysler.

In August 1980, Byrd stated that Congress was unlikely to pass a tax cut before the November elections despite the Senate being in the mood for passing one.

In July 1978, Byrd introduced and endorsed a proposal by George McGovern for an amendment to repeal the 42‐month‐old embargo on American military assistance for Turkey that also linked any future aid for that country to progress on a negotiated settlement of the Cyprus problem. The Senate approved the amendment in a vote of 57 to 42 as part of a 2.9 billion international security assistance bill. Byrd stated that every government in the NATO alliance except Greece favored repeal of the embargo.

In May 1979, Byrd stated that giving Turkey a grant should not be construed as retaliation against Greece and that aid for Turkey would improve Turkey's security in addition to that of Greece, NATO, and of American allies in the Middle East. Byrd mentioned his encouragement from the report on the Greek and Turkish Cypriot communities agreeing to resume negotiations on the island's future as well as reports that progress was also being made on the reintegration of Greece into NATO. Byrd furthered that American military installations in Turkey were "of major importance in the monitoring of Soviet strategic activities" and would have "obvious significance" in the goal of verifying compliance by the Soviet Union with the strategic arms treaty. The Senate approved the Turkey grant, to Byrd's wishes, but against that of both President Carter and the Senate Foreign Relations Committee.

On February 2, 1978, Byrd and Minority Leader Baker invited all other senators to join them in sponsoring two amendments to the Panama Canal neutrality treaty, the two party leaders sending copies of amendments recommended by the Foreign Relations Committee the previous week.

In January 1979, Byrd met with Deputy Prime Minister of China Deng Xiaoping for assurances by Deng that China hoped to unite Taiwan to the mainland by peaceful means and would fully respect "the present realities" on the island. Byrd afterward stated that his concern on the Taiwan question had been allayed. In June, Byrd opined that a decision by President Carter to not proceed with the new missile system would kill the strategic arms limitation treaty in the Senate. Byrd held meetings with Soviet leaders between July 3 to July 4. Following their conclusion, Byrd said he was still undecided on supporting the arms pact and that there had been talks on "the need on both sides for avoidance of inflammatory rhetoric which can only be counterproductive." On September 23, Byrd stated that it was possible the Senate could complete the strategic arms limitation treaty that year but a delay until the following year could result in its defeat, adding that senators might have to remain in session during Christmas to ensure the treaty was voted on before 1979's end. Byrd noted that he was opposed to the treaty being "held hostage to the Cuban situation" as American interests could be harmed in the event the treaty was defeated solely due to Soviet troops being in Cuba. In November, Byrd admitted to complaining to President Carter about Senate leadership receiving only occasional briefings about the Iranian hostage crisis and that Carter had agreed to daily consultations for Minority Leader Howard Baker, Chairman of the Foreign Relations Committee Frank Church, and ranking Republican member of the Foreign Relations Committee Jacob Javits. Byrd added that he did not disagree with the move by the Carter administration to admit Mohammad Reza Pahlavi for hospitalization and that the same action would extend to "Ayatollah Khomeini himself if he were needing medical treatment and had a terminal illness." On December 3, Byrd told reporters that the Iranian hostage crisis was making the Senate uninhabitable for a debate on the strategic arms treaty, noting that a discussion could still occur before the Senate adjourned on December 21 but that he did not believe he would call up the opportunity even if granted the chance. Days later, Byrd announced there was no chance that the Senate would take up debate on the strategic arms treaty that year while speaking to reporters, adding that he would see no harm in having the discussion on the treaty begin in January of the following year.

In July 1979, Senators Henry M. Jackson and George McGovern made comments expressing doubt on President Carter being assured as the Democratic nominee in the 1980 Presidential election. When asked about their comments by a reporter, Byrd referred to Jackson and McGovern as "two very strong voices and not at all to be considered men who have little background in politics" but stated it was too early to participate in "writing the political obituary of the President at this point." Byrd added that the powers of the presidency made it possible that Carter could have a comeback and cited the events in November and December as being telling of his prospects of achieving higher popularity.

On May 10, 1980, Byrd called for President Carter to debate Senator Ted Kennedy, who he complimented as having done a service for the US by raising key issues in his presidential campaign. On August 2, Byrd advocated for an open Democratic National Convention where the delegates were not bound to a single candidate. The endorsement was seen as a break from President Carter.
In September, Byrd said that Republican presidential nominee Ronald Reagan had made comments on the war between Iran and Iraq that were a disservice to the United States and that he was exercising "reckless political posturing" in foreign policy.

In early 1990, Byrd proposed an amendment granting special aid to coal miners who would lose their jobs in the event that Congress passed clean air legislation. Byrd was initially confident in the number of votes he needed to secure its passage being made available but this was prevented by a vote from Democrat Joe Biden who said the measure's passage would mean an assured veto by President Bush. Speaking to reporters after its defeat, Byrd stated his content with the results: "I made the supreme effort. I did everything I could and, therefore, I don't feel badly about it." The Senate passed clean air legislation within weeks of the vote on Byrd's amendment with the intent of reduction in acid rain, urban smog and toxic chemicals in the air and meeting the request by President Bush for a measure that was less costly than the initial plan while still performing the same tasks of combating clean air issues. Byrd was one of eleven senators to vote against the bill and said he "cannot vote for legislation that can bring economic ruin to communities throughout the Appalachian region and the Midwest."

In August 1990, after the Senate passed its first major campaign finance reform bill since the Watergate era that would prevent political action committees from federal campaigns, lend public money into congressional campaigns and bestow candidates vouchers for television advertising, Byrd stated that he believed the bill would "end the money chase."

Byrd authored an amendment to the National Endowment for the Arts that would bar the endowment from funding projects considered obscene such as depictions of sadomasochism, homo-eroticism, the sexual exploitation of children, or individuals engaged in sex acts while also requiring grant recipients to sign a pledge swearing their compliance with the restrictions. The October 1990 measure approved in the Senate was a bipartisan measure loosening government restrictions on art project funding and leaving courts to judge what art could be considered obscene.

President Bush nominated Clarence Thomas for the Supreme Court. In October 1991, Byrd stated his support in the credibility of Anita Hill: "I believe what she said. I did not see on that face the knotted brow of satanic revenge. I did not see a face that was contorted with hate. I did not hear a voice that was tremulous with passion. I saw the face of a woman, one of 13 in a family of Southern blacks who grew up on the farm and who belonged to the church." Byrd questioned how members of the Senate could be convinced that Thomas would serve as an objective judge when he could refuse to watch Hill's testimony against him.

In February 1992, the Senate turned down a Republican attempt sponsored by John McCain and Dan Coats to grant President Bush line-item veto authority and thereby be authorized to kill projects that he was opposed to, Byrd delivering an address defending congressional power over spending for eight hours afterward. The speech had been written by Byrd two years prior and he had at this point steered 1.5 billion to his state.

In 1992, there was an effort made to pass a constitutional amendment to ensure a balanced federal budget. Byrd called the amendment "a smokescreen that will allow lawmakers to claim action against the deficit while still postponing hard budgetary decision" and spoke to reporters on his feelings against the amendment being passed: "Once members are really informed as to the mischief this amendment could do, and the damage it could do to the country and to the Constitution. I just have faith that enough members will take a courageous stand against the amendment." The sponsor of the amendment, Paul Simon, admitted that Byrd's predicton was not off and that other senators speak "when the chairman of the Senate Appropriations Committee talks".

In a June 1992 debate, Byrd argued in favor of the United States withdrawing accepting immigrants that did not speak English, the comment being a response to a plan from the Bush administration that would enable former Soviet states to receive American assistance and allow immigrants from a variety of countries to receive welfare benefits. Byrd soon afterward apologized for the comment and said they were due to his frustration over the federal government's inability to afford several essential services.

In February 1994, the Senate passed a $10 billion spending bill that would mostly be allocated to Los Angeles, California earthquake victims and military operations abroad. Bob Dole, John Kerry, John McCain, and Russ Feingold partnered together to persuade the Senate in favor of cutting back the deficit expense. Byrd raised a procedural point to derail an attempt by Dole that would approve 50 billion in spending cuts over the following five years. McCain proposed killing highway demonstration projects with a 203 million price tag, leading Byrd to produce letters written by McCain that the latter had sent to the Appropriations Committee in 1991 in an attempt to gather highway grants for his home state of Arizona. Byrd said that McCain "is very considerate of the taxpayers when it comes to financing projects in other states, but he supports such projects in his own state."

In May 2000, Byrd and John Warner sponsored a provision threatening to withdraw American troops from Kosovo, the legislation if enacted cutting off funds for troops in Kosovo after July 1, 2001, without Congressional consent. The language would have also withheld 25 percent of the money for Kosovo in the bill unless the assertion that European countries were living up to their promises to provide reconstruction money for the province was certified by President Clinton by July 15. Byrd argued that lawmakers had never approved nor debate whether American troops should be stationed in Kosovo. The Senate Appropriations Committee approved the legislation in a vote of 23-to-3 that was said to reflect "widespread concern among lawmakers about an open-ended deployment of American soldiers".

In November 2000, Congress passed an amendment sponsored by Byrd diverting tariff revenues from the Treasury Department and instead allocating them to the industry complaining, the amount involved ranging from between 40 million and 200 million a year. The following month, Japan and the European Union led a group of countries in filing a joint complaint with the World Trade Organization to the law.

Byrd praised the nomination of John G. Roberts to fill the vacancy on the Supreme Court created by the death of Chief Justice William Rehnquist. Likewise, Byrd was one of four Democrats who supported the confirmation of Samuel Alito to replace retiring Associate Justice Sandra Day O'Connor.

Like most Democrats, Byrd opposed Bush's tax cuts and his proposals to change the Social Security program.

Byrd opposed the 2002 Homeland Security Act, which created the Department of Homeland Security, stating that the bill ceded too much authority to the executive branch.

On May 2, 2002, Byrd charged the White House with engaging in "sophomoric political antics", citing Homeland Security Advisor Tom Ridge briefing senators in another location instead of the Senate on how safe he felt the US was.

He also led the opposition to Bush's bid to win back the power to negotiate trade deals that Congress cannot amend, but lost overwhelmingly. In the 108th Congress, however, Byrd won his party's top seat on the new Homeland Security Appropriations Subcommittee.

In July 2004, Byrd released The New York Times best-selling book "Losing America: Confronting a Reckless and Arrogant Presidency", which criticized the Bush presidency and the war in Iraq.

Byrd led a filibuster against the resolution granting President George W. Bush broad power to wage a "preemptive" war against Iraq, but he could not get even a majority of his own party to vote against cloture.

Byrd was one of the Senate's most outspoken critics of the 2003 invasion of Iraq.

Byrd anticipated the difficulty of fighting an insurgency in Iraq, stating on March 13, 2003,

On March 19, 2003, when Bush ordered the invasion after receiving congressional approval, Byrd said,

Byrd also criticized Bush for his speech declaring the "end of major combat operations" in Iraq, which Bush made on the U.S.S. "Abraham Lincoln". Byrd stated on the Senate floor,

On October 17, 2003, Byrd delivered a speech expressing his concerns about the future of the nation and his unequivocal antipathy to Bush's policies. Referencing the Hans Christian Andersen children's tale "The Emperor's New Clothes", Byrd said of the president: "the emperor has no clothes." Byrd further lamented the "sheep-like" behavior of the "cowed Members of this Senate" and called on them to oppose the continuation of a "war based on falsehoods."

In April 2004, Byrd mentioned the possibility of the Bush administration violating law by its failure to inform leadership in Congress midway through 2002 about its use of emergency anti-terror dollars to begin preparations for an invasion of Iraq. Byrd stated that he had never been told of a shift in money, a charge reported in the Bob Woodward book "Plan of Attack", and its validation would mean "the administration failed to abide by the law to consult with and fully inform Congress."

Byrd accused the Bush administration of stifling dissent:

Of the more than 18,000 votes he cast as a senator, Byrd said he was proudest of his vote against the Iraq war resolution. Byrd also voted to tie a timetable for troop withdrawal to war funding.

On May 23, 2005, Byrd was one of 14 senators (who became known as the "Gang of 14") to forge a compromise on the judicial filibuster, thus securing up and down votes for many judicial nominees and ending the threat of the so-called nuclear option that would have eliminated the filibuster entirely. Under the agreement, the senators retained the power to filibuster a judicial nominee in only an "extraordinary circumstance." It ensured that the appellate court nominees (Janice Rogers Brown, Priscilla Owen and William Pryor) would receive votes by the full Senate.

In 1977, Byrd was one of five Democrats to vote against the nomination of F. Ray Marshall as United States Secretary of Labor. Marshall was opposed by conservatives in both parties because of his pro-labor positions, including support for repealing right to work laws. Marshall was confirmed and served until the end of Carter's term in 1981.

In February 1981, as the Senate voted on giving final approval to the 50 billion increase in the debt limit, Democrats initially opposed the measure as part of an effort to elicit the highest number of Republicans in support of the measure. Byrd proceeded to give a signal for Democrats that saw caucus members switch their votes in support of the increase.

President Reagan was injured during an assassination attempt in March 1981. Following the shooting, Byrd opined that the aftermath of the attempt had proven there were "holes that need to be plugged" in the constitution's handling of the presidential line of succession after a president's disability and stated his intent to introduce legislation calling for a mandatory life sentence for anyone attempting to assassinate a president, vice president, or member of Congress.

In March 1981, during a Capitol Hill interview, Byrd stated that the Reagan administration was promoting an economic package with assumptions for the national economy that might take a year for the public to see its difficulties and thereby lead to a political backlash. Byrd contented that President Reagan would win approval by Congress of 35 billion to 40 billion of the 48 billion in proposed budget cuts while having more difficulty in passing his tax-cut package, asserting Democratic opposition and some Republicans having misgivings about the approach as the reason Congress would block the plan and furthering that he would be surprised if a one-year-cut in rates lasted more than year. Byrd opined that it was time for "some tax reform" that would see loopholes closed for the rich dropped to bring in revenues and expressed belief in the likelihood of the administration dismantling existing energy programs: "Energy programs are not as catchy now as budget cuts. But if the gas lines begin to form again, or the overseas oil gets cut off, we will have lost the time, the momentum, the money. Basically, they have a wholesale dismantlement of the energy programs we spent several years creating around here."

In March 1981, during a news conference, Byrd stated that the Reagan administration had not established a coherent foreign policy. He credited conflicting statements from administration officials with having contributed to confusion in Western European capitals. Byrd also said, "We've seen these statements, and backing and filling, and the secretary of state has been kept pretty busy explaining and denying assertions and pronouncements by others, which indeed indicate that the administration has not yet got its foreign policy act together."

In May 1981, Byrd announced his support for the Reagan administration's proposed budget for the fiscal year 1982 during a weekly news conference, citing that the "people want the President to be given a chance with his budget." Byrd added that he did not believe a balanced budget would be achieved by 1984, calling the budget "a balanced budget on paper only, made up of juggled figures produced out of thin air", and charged the administration with making assumptions, his comments being seen as an indication that little opposition would amount from the Democrats to the Reagan budget.

In November 1981, as Senate leaders rejected the request of Senator Harrison A. Williams Jr. to introduce new evidence during the Senate's consideration of whether to expel him for his involvement in the Abscam case, Byrd and Majority Leader Baker informed Williams that he could have a lawyer that would have to remain wordless.

On December 2, 1981, Byrd voted in favor of an amendment to President Reagan's MX missiles proposal that would divert the silo system by $334 million as well as earmark further research for other methods that would allow giant missiles to be based. The vote was seen as a rebuff of the Reagan administration.

In February 1982, Byrd wrote a letter to President Reagan urging him to "withdraw the Administration's proposed fiscal 1983 budget, and resubmit a budget that provides for much lower deficits and makes use of more realistic assumptions", recalling his previous appeal to President Carter in 1980 amid the rise of soaring inflation rates and Carter afterward consulting with Democrats in Congress. Byrd stated that he was in favor of "a document we in Congress can work with, one based on realistic assumptions, one which shows a much clearer trend toward a balanced budget." Byrd had cautious praise for a proposal by Democrat Fritz Hollings called for a freeze on all benefit programs with the exception of food stamps, Medicare and Medicaid in addition to a freeze on military spending while eliminating a pay increase for federal employees.

In March 1982, Byrd announced he would introduce an amendment to the War Powers Act that would bar the president from being able to send combat troops to El Salvador without the approval of Congress. Byrd described the proposal as only allowing the president to act with independence in the event that Americans needed to evacuate El Salvador or if the United States was attacked. "It is my view that if Americans are to be asked to shed their blood in the jungles of El Salvador, all Americans should first have an opportunity to debate and carefully evaluate that action."

By March 1982, along with Alan Cranston, Byrd was one of two senators supporting both the measure sponsored by Henry M. Jackson and John W. Warner calling upon the United States and the Soviet Union to freeze their nuclear arsenals at "equal and sharply reduced levels" and the bill sponsored by Ted Kennedy and Mark Hatfield calling upon the two countries first to negotiate a freeze on nuclear forces at existing levels before following atomic arms reduction.

In January 1983, after President Reagan said during his 1983 State of the Union Address that he hoped for the same bipartisan support that had produced the Social Security recommendations would lead Congress during the year on other issues, Byrd and House Majority Leader Jim Wright assailed the unfairness of a six-month delay in the cost-of-living increases for Social Security recipients during a period of letting the wealthy reap the benefits of the general income tax cut for a third year. Byrd stated that he did not "want a six-month delay in Social Security while leaving in place the third year of the tax cut for upper-income people" and stated that Reagan's speech had been "'rhetorically good, but substantively lacking in measures that would deal now with the crises that millions of people are experiencing."

At the beginning of February 1983, House Democrats committed themselves "to an emergency economic assistance program that would create public service jobs, provide shelter and soup kitchens for the destitute and avert foreclosures of homes and farms." Concurrently, Byrd pledged to work with the House Democrats in developing legislation concerning jobs, proposing 5 billion to 10 billion be spent and introducing legislation intended to form a national investment corporation that would assist with underwriting faltering basic industries and starting new ones in areas of high unemployment.

In March 1984, Byrd voted against a proposed constitutional amendment authorizing periods in public school for silent prayer, and in favor of President Reagan's unsuccessful proposal for a constitutional amendment permitting organized school prayer in public schools.

In June 1984, Byrd was one of five Democrats to vote against the Lawton Chiles proposal to cease MX production for a year during study in search of a smaller and single-warhead missile. The 48 to 48 tie was broken by then-Vice President George H. W. Bush.

In September 1986, Byrd endorsed the death penalty for some drug pushers in anti-drug legislation that would order President Reagan to end drug trafficking within 45 days through using the military as a means of intercepting smugglers, and imposing the death penalty on those pushers who intentionally cause a death as part of their operations while providing funding for prevention, drug abuse treatment, and anti-drug laws enforcement that was estimated to cost 3 billion to 4 billion over three years. Byrd admitted that calling for the death penalty seemed harsh, but cautioned that children in some cases had their entire lives destroyed through using drugs and that Congress had been soft for too long without seeing a change in results.

In December 1986, Byrd announced that the Senate would convene a Watergate-type select committee to investigate the Iran-Contra affair the following year and that he had reached an agreement with Bob Dole for the committee to have six Democrats and five Republicans. Byrd and Dole disagreed on whether it was a necessity for Congress to be launched into a special session that month for the purpose of getting the investigative process moving. Naming members during December enabled participants to informally move ahead by selecting the staff and be prepared before the 100th United States Congress began.

In September 1988, in response to charges by Vice President Bush's presidential campaign that Democratic nominee Michael Dukakis was weak on defense, Byrd delivered a Senate speech in which he said that the Reagan administration "is living in a glass house when it throws a stone at the Democratic Party for its so-called Disneyland defense policies" and that the U.S. land-based missiles had grown in vulnerability due to the administration being "unable to produce an acceptable solution to make our missiles survivable." Byrd furthered, "Indeed, the Fantasyland exhibits of this White House's Defense Disneyland are loaded with the rejected systems that have been developed and discarded. If anything deserves the names 'Goofy' and 'Daffy' and 'Mickey Mouse,' it is those' basing proposals."

In October 1990, Byrd and James A. McClure served as floor managers for the appropriation bill for the National Endowment of the Arts, accepting an amendment by Jesse Helms prohibiting NEA support of work denigrating objects or beliefs of religions.

In November 1993, when the Senate voted to seek federal court enforcement of a subpoena for the diaries of Bob Packwood, Byrd stated the possibility of Americans becoming convinced that the Senate was delaying taking action to protect one of its own members. Byrd also called for Packwood to resign. "None of us is without flaws. But when those flaws damage the institution of the Senate, it is time to have the grace to go!" Packwood resigned in 1995.

In October 1999, Byrd was the only senator to vote present on the Comprehensive Test Ban Treaty. The treaty was designed to ban underground nuclear testing and was the first major international security pact to be defeated in the Senate since the Treaty of Versailles.

Byrd opposed the Flag Desecration Amendment, saying that, while he wanted to protect the American flag, he believed that amending the Constitution "is not the most expeditious way to protect this revered symbol of our Republic." As an alternative, Byrd cosponsored the Flag Protection Act of 2005 (S. 1370), a bill to prohibit destruction or desecration of the flag by anyone trying to incite violence or causing a breach of the peace, or who steals, damages, or destroys a flag on federal property, whether owned by the federal government or a private group or individual—can be imprisoned, fined or both. The bill did not pass.

In 2009, Byrd was one of three Democrats to oppose the confirmation of Secretary of the Treasury Timothy Geithner. After missing nearly two months while in hospital, Byrd returned to the Senate floor on July 21 to vote against the elimination of funding for the F-22 fighter plane.

Byrd received a 65% vote rating from the League of Conservation Voters for his support of environmentally friendly legislation. Additionally, he received a "liberal" rating of 65.5% by the "National Journal"—higher than six other Democratic senators.

In 2010, Byrd received a 70 percent lifetime rating from the American Civil Liberties Union for supporting rights-related legislation.

Byrd had an essential tremor; he was eventually confined to a wheelchair. His health declined through 2008, including several hospital admissions.

On January 20, 2009, Senator Ted Kennedy suffered a seizure during Barack Obama's inaugural luncheon and was taken away in an ambulance. Byrd, seated at the same table, became distraught and was himself removed to his office. Byrd's office reported that he was fine. On May 18, Byrd was admitted to the hospital after experiencing a fever due to a "minor infection", prolonged by a staphylococcus aureus infection. Byrd was released on June 30, 2009.

Byrd's final hospital stay began on June 27, 2010, at Inova Fairfax Hospital in Fairfax County, Virginia. He died at approximately EDT the next day at age 92 from natural causes.

Vice President Joe Biden recalled Byrd's standing in the rain with him as Biden buried his daughter when Biden had just been elected to the Senate. He called Byrd "a tough, compassionate, and outspoken leader and dedicated above all else to making life better for the people of the Mountain State." President Barack Obama said, "His profound passion for that body and its role and responsibilities was as evident behind closed doors as it was in the stemwinders he peppered with history. He held the deepest respect of members of both parties, and he was generous with his time and advice, something I appreciated greatly as a young senator." Senator Jay Rockefeller, who had served with Byrd since 1985, said, "I looked up to him, I fought next to him, and I am deeply saddened that he is gone." Former President Jimmy Carter noted, "He was my closest and most valuable adviser while I served as president. I respected him and attempted in every way to remain in his good graces. He was a giant among legislators, and was courageous in espousing controversial issues."

On July 1, 2010, Byrd lay in repose on the Lincoln Catafalque in the Senate chamber of the United States Capitol, becoming the first Senator to do so since his first year in the Senate, 1959. He was then flown to Charleston, West Virginia, where he lay in repose in the Lower Rotunda of the West Virginia State Capitol.

A funeral was held on July 2, 2010, on the grounds of the State Capitol where Byrd was eulogized by President Barack Obama, Vice President Joe Biden, Governor Joe Manchin, Senate Majority Leader Harry Reid, Senate Minority Leader Mitch McConnell, Speaker of the House of Representatives Nancy Pelosi, Senator Jay Rockefeller, Congressman Nick Rahall, Victoria Reggie Kennedy, and former President Bill Clinton. After the funeral services in Charleston, his body was returned to Arlington, Virginia, for funeral services on July 6, 2010, at Memorial Baptist Church. After the funeral in Arlington, Byrd was buried next to his wife Erma at Columbia Gardens Cemetery in Arlington, although family members have stated that both the senator and Mrs. Byrd will be reinterred somewhere in West Virginia once a site is determined.

The song "Take Me Home, Country Roads" was played at the end of the funeral in a bluegrass fashion as his casket was being carried back up the stairs and into the West Virginia State Capitol Building.

On September 30, 2010, Congress appropriated $193,400 to be paid equally among Byrd's children and grandchildren, representing the salary he would have earned in the next fiscal year, a common practice when members of Congress die in office.
Multiple political figures issued statements following Byrd's death:

Byrd had a prominent role in the 2008 Warner Bros. documentary "Body of War" directed by Phil Donahue. The film chronicles the life of Tomas Young, paralyzed from the chest down after a sniper shot him as he was riding in a vehicle in Iraq. Several long clips of Byrd show him passionately arguing against authorizing the use of force in Iraq. Later in the movie, Byrd has a one-on-one interview with Tomas Young in Byrd's Senate office, followed by a shot of Byrd walking beside the wheelchair-bound Young as they leave the Capitol.

A fictionalized version of Byrd, then the Senate Majority Leader, was a character in the Jeffrey Archer novel "Shall We Tell the President?"

Byrd was an avid fiddle player for most of his life, starting in his teens when he played in various square dance bands. Once he entered politics, his fiddling skills attracted attention and won votes. In 1978 when Byrd was Majority Leader, he recorded an album called "U.S. Senator Robert Byrd: Mountain Fiddler" (County, 1978). Byrd was accompanied by Country Gentlemen Doyle Lawson, James Bailey, and Spider Gilliam. Most of the LP consists of bluegrass music. Byrd covers "Don't Let Your Sweet Love Die", a Zeke Manners song, and "Will the Circle Be Unbroken". He had performed at the Kennedy Center, on the Grand Ole Opry and on "Hee Haw". He occasionally took a break from Senate business to entertain audiences with his fiddle. He stopped playing in 1982 when the symptoms of a benign essential tremor had begun to affect the use of his hands.

Byrd appeared in the Civil War movie "Gods and Generals" in 2003 along with then-Virginia senator George Allen. Both played Confederate States officers.


In 2002, the Robert C. Byrd Center for Legislative Studies (CLS) was opened on the campus of Shepherd University. Adjoining the University's Ruth Scarborough Library, the CLS "advances representative democracy by promoting a better understanding of the United States Congress and the Constitution through programs and research that engage citizens." The CLS is an archival research facility, housing the papers of Senator Robert C. Byrd in addition to the papers of Congressmen Harley O. Staggers Sr. and Harley O. Staggers Jr. and Scot Faulkner, the first Chief Administrative Officer of the United States House of Representatives. The CLS is a founding institution of the Association of Centers for the Study of Congress, "an independent alliance of organizations and institutions which promote the study of the U.S. Congress." 





</doc>
<doc id="25409" url="https://en.wikipedia.org/wiki?curid=25409" title="Reptile">
Reptile

Reptiles are tetrapod animals in the class Reptilia, comprising today's turtles, crocodilians, snakes, amphisbaenians, lizards, tuatara, and their extinct relatives. The study of these traditional reptile orders, historically combined with that of modern amphibians, is called herpetology.

Because some reptiles are more closely related to birds than they are to other reptiles (e.g., crocodiles are more closely related to birds than they are to lizards), the traditional groups of "reptiles" listed above do not together constitute a monophyletic grouping or clade (consisting of all descendants of a common ancestor). For this reason, many modern scientists prefer to consider the birds part of Reptilia as well, thereby making Reptilia a monophyletic class, including all living Diapsids. The term "reptiles" is sometimes used as shorthand for 'non-avian Reptilia'.

The earliest known proto-reptiles originated around 312 million years ago during the Carboniferous period, having evolved from advanced reptiliomorph tetrapods that became increasingly adapted to life on dry land. Some early examples include the lizard-like "Hylonomus" and "Casineria". In addition to the living reptiles, there are many diverse groups that are now extinct, in some cases due to mass extinction events. In particular, the Cretaceous–Paleogene extinction event wiped out the pterosaurs, plesiosaurs, ornithischians, and sauropods, as well as many species of theropods, including troodontids, dromaeosaurids, tyrannosaurids, and abelisaurids, along with many Crocodyliformes, and squamates (e.g. mosasaurids).

Modern non-avian reptiles inhabit all the continents except Antarctica, although some birds are found on the periphery of Antarctica. Several living subgroups are recognized: Testudines (turtles and tortoises), 350 species; Rhynchocephalia (tuatara from New Zealand), 1 species; Squamata (lizards, snakes, and worm lizards), over 10,200 species; Crocodilia (crocodiles, gharials, caimans, and alligators), 24 species; and Aves (birds), approximately 10,000 species.

Reptiles are tetrapod vertebrates, creatures that either have four limbs or, like snakes, are descended from four-limbed ancestors. Unlike amphibians, reptiles do not have an aquatic larval stage. Most reptiles are oviparous, although several species of squamates are viviparous, as were some extinct aquatic clades – the fetus develops within the mother, contained in a placenta rather than an eggshell. As amniotes, reptile eggs are surrounded by membranes for protection and transport, which adapt them to reproduction on dry land. Many of the viviparous species feed their fetuses through various forms of placenta analogous to those of mammals, with some providing initial care for their hatchlings. Extant reptiles range in size from a tiny gecko, "Sphaerodactylus ariasae", which can grow up to to the saltwater crocodile, "Crocodylus porosus", which can reach in length and weigh over .

In the 13th century the category of "reptile" was recognized in Europe as consisting of a miscellany of egg-laying creatures, including "snakes, various fantastic monsters, lizards, assorted amphibians, and worms", as recorded by Vincent of Beauvais in his "Mirror of Nature".
In the 18th century, the reptiles were, from the outset of classification, grouped with the amphibians. Linnaeus, working from species-poor Sweden, where the common adder and grass snake are often found hunting in water, included all reptiles and amphibians in class "III – Amphibia" in his "Systema Naturæ".
The terms "reptile" and "amphibian" were largely interchangeable, "reptile" (from Latin "repere", "to creep") being preferred by the French. Josephus Nicolaus Laurenti was the first to formally use the term "Reptilia" for an expanded selection of reptiles and amphibians basically similar to that of Linnaeus. Today, the two groups are still commonly treated under the same heading as herptiles.

It was not until the beginning of the 19th century that it became clear that reptiles and amphibians are, in fact, quite different animals, and Pierre André Latreille erected the class "Batracia" (1825) for the latter, dividing the tetrapods into the four familiar classes of reptiles, amphibians, birds, and mammals. The British anatomist Thomas Henry Huxley made Latreille's definition popular and, together with Richard Owen, expanded Reptilia to include the various fossil "antediluvian monsters", including dinosaurs and the mammal-like (synapsid) "Dicynodon" he helped describe. This was not the only possible classification scheme: In the Hunterian lectures delivered at the Royal College of Surgeons in 1863, Huxley grouped the vertebrates into mammals, sauroids, and ichthyoids (the latter containing the fishes and amphibians). He subsequently proposed the names of Sauropsida and Ichthyopsida for the latter two groups. In 1866, Haeckel demonstrated that vertebrates could be divided based on their reproductive strategies, and that reptiles, birds, and mammals were united by the amniotic egg.

The terms "Sauropsida" ("lizard faces") and "Theropsida" ("beast faces") were used again in 1916 by E.S. Goodrich to distinguish between lizards, birds, and their relatives on the one hand (Sauropsida) and mammals and their extinct relatives (Theropsida) on the other. Goodrich supported this division by the nature of the hearts and blood vessels in each group, and other features, such as the structure of the forebrain. According to Goodrich, both lineages evolved from an earlier stem group, Protosauria ("first lizards") in which he included some animals today considered reptile-like amphibians, as well as early reptiles.

In 1956, D.M.S. Watson observed that the first two groups diverged very early in reptilian history, so he divided Goodrich's Protosauria between them. He also reinterpreted Sauropsida and Theropsida to exclude birds and mammals, respectively. Thus his Sauropsida included Procolophonia, Eosuchia, Millerosauria, Chelonia (turtles), Squamata (lizards and snakes), Rhynchocephalia, Crocodilia, "thecodonts" (paraphyletic basal Archosauria), non-avian dinosaurs, pterosaurs, ichthyosaurs, and sauropterygians.

In the late 19th century, a number of definitions of Reptilia were offered. The traits listed by Lydekker in 1896, for example, include a single occipital condyle, a jaw joint formed by the quadrate and articular bones, and certain characteristics of the vertebrae. The animals singled out by these formulations, the amniotes other than the mammals and the birds, are still those considered reptiles today.

The synapsid/sauropsid division supplemented another approach, one that split the reptiles into four subclasses based on the number and position of temporal fenestrae, openings in the sides of the skull behind the eyes. This classification was initiated by Henry Fairfield Osborn and elaborated and made popular by Romer's classic "Vertebrate Paleontology". Those four subclasses were:

The composition of Euryapsida was uncertain. Ichthyosaurs were, at times, considered to have arisen independently of the other euryapsids, and given the older name Parapsida. Parapsida was later discarded as a group for the most part (ichthyosaurs being classified as "incertae sedis" or with Euryapsida). However, four (or three if Euryapsida is merged into Diapsida) subclasses remained more or less universal for non-specialist work throughout the 20th century. It has largely been abandoned by recent researchers: in particular, the anapsid condition has been found to occur so variably among unrelated groups that it is not now considered a useful distinction.

By the early 21st century, vertebrate paleontologists were beginning to adopt phylogenetic taxonomy, in which all groups are defined in such a way as to be monophyletic; that is, groups include all descendants of a particular ancestor. The reptiles as historically defined are paraphyletic, since they exclude both birds and mammals. These respectively evolved from dinosaurs and from early therapsids, which were both traditionally called reptiles. Birds are more closely related to crocodilians than the latter are to the rest of extant reptiles. Colin Tudge wrote:

Mammals are a clade, and therefore the cladists are happy to acknowledge the traditional taxon Mammalia; and birds, too, are a clade, universally ascribed to the formal taxon Aves. Mammalia and Aves are, in fact, subclades within the grand clade of the Amniota. But the traditional class Reptilia is not a clade. It is just a section of the clade Amniota: the section that is left after the Mammalia and Aves have been hived off. It cannot be defined by synapomorphies, as is the proper way. Instead, it is defined by a combination of the features it has and the features it lacks: reptiles are the amniotes that lack fur or feathers. At best, the cladists suggest, we could say that the traditional Reptilia are 'non-avian, non-mammalian amniotes'.
Despite the early proposals for replacing the paraphyletic Reptilia with a monophyletic Sauropsida, which includes birds, that term was never adopted widely or, when it was, was not applied consistently. When Sauropsida was used, it often had the same content or even the same definition as Reptilia. In 1988, Jacques Gauthier proposed a cladistic definition of Reptilia as a monophyletic node-based crown group containing turtles, lizards and snakes, crocodilians, and birds, their common ancestor and all its descendants. Because the actual relationship of turtles to other reptiles was not yet well understood at this time, Gauthier's definition came to be considered inadequate.

A variety of other definitions were proposed by other scientists in the years following Gauthier's paper. The first such new definition, which attempted to adhere to the standards of the PhyloCode, was published by Modesto and Anderson in 2004. Modesto and Anderson reviewed the many previous definitions and proposed a modified definition, which they intended to retain most traditional content of the group while keeping it stable and monophyletic. They defined Reptilia as all amniotes closer to "Lacerta agilis" and "Crocodylus niloticus" than to "Homo sapiens". This stem-based definition is equivalent to the more common definition of Sauropsida, which Modesto and Anderson synonymized with Reptilia, since the latter is better known and more frequently used. Unlike most previous definitions of Reptilia, however, Modesto and Anderson's definition includes birds, as they are within the clade that includes both lizards and crocodiles.

Classification to order level of the reptiles, after Benton, 2014.

The cladogram presented here illustrates the "family tree" of reptiles, and follows a simplified version of the relationships found by M.S. Lee, in 2013. All genetic studies have supported the hypothesis that turtles are diapsids; some have placed turtles within archosauriformes, though a few have recovered turtles as lepidosauriformes instead. The cladogram below used a combination of genetic (molecular) and fossil (morphological) data to obtain its results.

The placement of turtles has historically been highly variable. Classically, turtles were considered to be related to the primitive anapsid reptiles. Molecular work has usually placed turtles within the diapsids. As of 2013, three turtle genomes have been sequenced. The results place turtles as a sister clade to the archosaurs, the group that includes crocodiles, dinosaurs, and birds.

The origin of the reptiles lies about 310–320 million years ago, in the steaming swamps of the late Carboniferous period, when the first reptiles evolved from advanced reptiliomorphs.

The oldest known animal that may have been an amniote is "Casineria" (though it may have been a temnospondyl). A series of footprints from the fossil strata of Nova Scotia dated to show typical reptilian toes and imprints of scales. These tracks are attributed to "Hylonomus", the oldest unquestionable reptile known.
It was a small, lizard-like animal, about long, with numerous sharp teeth indicating an insectivorous diet. Other examples include "Westlothiana" (for the moment considered a reptiliomorph rather than a true amniote) and "Paleothyris", both of similar build and presumably similar habit.

The earliest amniotes, including stem-reptiles (those amniotes closer to modern reptiles than to mammals), were largely overshadowed by larger stem-tetrapods, such as "Cochleosaurus", and remained a small, inconspicuous part of the fauna until the Carboniferous Rainforest Collapse. This sudden collapse affected several large groups. Primitive tetrapods were particularly devastated, while stem-reptiles fared better, being ecologically adapted to the drier conditions that followed. Primitive tetrapods, like modern amphibians, need to return to water to lay eggs; in contrast, amniotes, like modern reptiles – whose eggs possess a shell that allows them to be laid on land – were better adapted to the new conditions. Amniotes acquired new niches at a faster rate than before the collapse and at a much faster rate than primitive tetrapods. They acquired new feeding strategies including herbivory and carnivory, previously only having been insectivores and piscivores. From this point forward, reptiles dominated communities and had a greater diversity than primitive tetrapods, setting the stage for the Mesozoic (known as the Age of Reptiles). One of the best known early stem-reptiles is "Mesosaurus", a genus from the Early Permian that had returned to water, feeding on fish.

It was traditionally assumed that the first reptiles retained an anapsid skull inherited from their ancestors. This type of skull has a skull roof with only holes for the nostrils, eyes and a pineal eye. The discoveries of synapsid-like openings (see below) in the skull roof of the skulls of several members of Parareptilia (the clade containing most of the amniotes traditionally referred to as "anapsids"), including lanthanosuchoids, millerettids, bolosaurids, some nycteroleterids, some procolophonoids and at least some mesosaurs made it more ambiguous and it's currently uncertain whether the ancestral amniote had an anapsid-like or synapsid-like skull. These animals are traditionally referred to as "anapsids", and form a paraphyletic basic stock from which other groups evolved. Very shortly after the first amniotes appeared, a lineage called Synapsida split off; this group was characterized by a temporal opening in the skull behind each eye to give room for the jaw muscle to move. These are the "mammal-like amniotes", or stem-mammals, that later gave rise to the true mammals. Soon after, another group evolved a similar trait, this time with a double opening behind each eye, earning them the name Diapsida ("two arches"). The function of the holes in these groups was to lighten the skull and give room for the jaw muscles to move, allowing for a more powerful bite.

Turtles have been traditionally believed to be surviving parareptiles, on the basis of their anapsid skull structure, which was assumed to be primitive trait. The rationale for this classification has been disputed, with some arguing that turtles are diapsids that evolved anapsid skulls in order to improve their armor. Later morphological phylogenetic studies with this in mind placed turtles firmly within Diapsida. All molecular studies have strongly upheld the placement of turtles within diapsids, most commonly as a sister group to extant archosaurs.

With the close of the Carboniferous, the amniotes became the dominant tetrapod fauna. While primitive, terrestrial reptiliomorphs still existed, the synapsid amniotes evolved the first truly terrestrial megafauna (giant animals) in the form of pelycosaurs, such as "Edaphosaurus" and the carnivorous "Dimetrodon". In the mid-Permian period, the climate became drier, resulting in a change of fauna: The pelycosaurs were replaced by the therapsids.

The parareptiles, whose massive skull roofs had no postorbital holes, continued and flourished throughout the Permian. The pareiasaurian parareptiles reached giant proportions in the late Permian, eventually disappearing at the close of the period (the turtles being possible survivors).

Early in the period, the modern reptiles, or crown-group reptiles, evolved and split into two main lineages: the Archosauromorpha (forebears of turtles, crocodiles, and dinosaurs) and the Lepidosauromorpha (predecessors of modern lizards and tuataras). Both groups remained lizard-like and relatively small and inconspicuous during the Permian.

The close of the Permian saw the greatest mass extinction known (see the Permian–Triassic extinction event), an event prolonged by the combination of two or more distinct extinction pulses. Most of the earlier parareptile and synapsid megafauna disappeared, being replaced by the true reptiles, particularly archosauromorphs. These were characterized by elongated hind legs and an erect pose, the early forms looking somewhat like long-legged crocodiles. The archosaurs became the dominant group during the Triassic period, though it took 30 million years before their diversity was as great as the animals that lived in the Permian. Archosaurs developed into the well-known dinosaurs and pterosaurs, as well as the ancestors of crocodiles. Since reptiles, first rauisuchians and then dinosaurs, dominated the Mesozoic era, the interval is popularly known as the "Age of Reptiles". The dinosaurs also developed smaller forms, including the feather-bearing smaller theropods. In the Cretaceous period, these gave rise to the first true birds.

The sister group to Archosauromorpha is Lepidosauromorpha, containing lizards and tuataras, as well as their fossil relatives. Lepidosauromorpha contained at least one major group of the Mesozoic sea reptiles: the mosasaurs, which lived during the Cretaceous period. The phylogenetic placement of other main groups of fossil sea reptiles – the ichthyopterygians (including ichthyosaurs) and the sauropterygians, which evolved in the early Triassic – is more controversial. Different authors linked these groups either to lepidosauromorphs or to archosauromorphs, and ichthyopterygians were also argued to be diapsids that did not belong to the least inclusive clade containing lepidosauromorphs and archosauromorphs.

The close of the Cretaceous period saw the demise of the Mesozoic era reptilian megafauna (see the Cretaceous–Paleogene extinction event). Of the large marine reptiles, only sea turtles were left; and of the non-marine large reptiles, only the semi-aquatic crocodiles and broadly similar choristoderes survived the extinction, with the latter becoming extinct in the Miocene. Of the great host of dinosaurs dominating the Mesozoic, only the small beaked birds survived. This dramatic extinction pattern at the end of the Mesozoic led into the Cenozoic. Mammals and birds filled the empty niches left behind by the reptilian megafauna and, while reptile diversification slowed, bird and mammal diversification took an exponential turn. However, reptiles were still important components of the megafauna, particularly in the form of large and giant tortoises.

After the extinction of most archosaur and marine reptile lines by the end of the Cretaceous, reptile diversification continued throughout the Cenozoic. Squamates took a massive hit during the KT-event, only recovering ten million years after it, but they underwent a great radiation event once they recovered, and today squamates make up the majority of living reptiles (> 95%). Approximately 10,000 extant species of traditional reptiles are known, with birds adding about 10,000 more, almost twice the number of mammals, represented by about 5,700 living species (excluding domesticated species).

All squamates and turtles have a three-chambered heart consisting of two atria, one variably partitioned ventricle, and two aortas that lead to the systemic circulation. The degree of mixing of oxygenated and deoxygenated blood in the three-chambered heart varies depending on the species and physiological state. Under different conditions, deoxygenated blood can be shunted back to the body or oxygenated blood can be shunted back to the lungs. This variation in blood flow has been hypothesized to allow more effective thermoregulation and longer diving times for aquatic species, but has not been shown to be a fitness advantage.
For example, Iguana hearts, like the majority of the squamates hearts, are composed of three chambers with two aorta and one ventricle, cardiac involuntary muscles. The main structures of the heart are the sinus venosus, the pacemaker, the left atrium, the right atruim, the atrioventricular valve, the cavum venosum, cavum arteriosum, the cavum pulmonale, the muscular ridge, the ventricular ridge, pulmonary veins, and paired aortic arches.

Some squamate species (e.g., pythons and monitor lizards) have three-chambered hearts that become functionally four-chambered hearts during contraction. This is made possible by a muscular ridge that subdivides the ventricle during ventricular diastole and completely divides it during ventricular systole. Because of this ridge, some of these squamates are capable of producing ventricular pressure differentials that are equivalent to those seen in mammalian and avian hearts.

Crocodilians have an anatomically four-chambered heart, similar to birds, but also have two systemic aortas and are therefore capable of bypassing their pulmonary circulation.

Modern non-avian reptiles exhibit some form of cold-bloodedness (i.e. some mix of poikilothermy, ectothermy, and bradymetabolism) so that they have limited physiological means of keeping the body temperature constant and often rely on external sources of heat. Due to a less stable core temperature than birds and mammals, reptilian biochemistry requires enzymes capable of maintaining efficiency over a greater range of temperatures than in the case for warm-blooded animals. The optimum body temperature range varies with species, but is typically below that of warm-blooded animals; for many lizards, it falls in the 24°–35 °C (75°–95 °F) range, while extreme heat-adapted species, like the American desert iguana "Dipsosaurus dorsalis", can have optimal physiological temperatures in the mammalian range, between 35° and 40 °C (95° and 104 °F). While the optimum temperature is often encountered when the animal is active, the low basal metabolism makes body temperature drop rapidly when the animal is inactive.

As in all animals, reptilian muscle action produces heat. In large reptiles, like leatherback turtles, the low surface-to-volume ratio allows this metabolically produced heat to keep the animals warmer than their environment even though they do not have a warm-blooded metabolism. This form of homeothermy is called gigantothermy; it has been suggested as having been common in large dinosaurs and other extinct large-bodied reptiles.

The benefit of a low resting metabolism is that it requires far less fuel to sustain bodily functions. By using temperature variations in their surroundings, or by remaining cold when they do not need to move, reptiles can save considerable amounts of energy compared to endothermic animals of the same size. A crocodile needs from a tenth to a fifth of the food necessary for a lion of the same weight and can live half a year without eating. Lower food requirements and adaptive metabolisms allow reptiles to dominate the animal life in regions where net calorie availability is too low to sustain large-bodied mammals and birds.

It is generally assumed that reptiles are unable to produce the sustained high energy output necessary for long distance chases or flying. Higher energetic capacity might have been responsible for the evolution of warm-bloodedness in birds and mammals. However, investigation of correlations between active capacity and thermophysiology show a weak relationship. Most extant reptiles are carnivores with a sit-and-wait feeding strategy; whether reptiles are cold blooded due to their ecology is not clear. Energetic studies on some reptiles have shown active capacities equal to or greater than similar sized warm-blooded animals.

All reptiles breathe using lungs. Aquatic turtles have developed more permeable skin, and some species have modified their cloaca to increase the area for gas exchange. Even with these adaptations, breathing is never fully accomplished without lungs. Lung ventilation is accomplished differently in each main reptile group. In squamates, the lungs are ventilated almost exclusively by the axial musculature. This is also the same musculature that is used during locomotion. Because of this constraint, most squamates are forced to hold their breath during intense runs. Some, however, have found a way around it. Varanids, and a few other lizard species, employ buccal pumping as a complement to their normal "axial breathing". This allows the animals to completely fill their lungs during intense locomotion, and thus remain aerobically active for a long time. Tegu lizards are known to possess a proto-diaphragm, which separates the pulmonary cavity from the visceral cavity. While not actually capable of movement, it does allow for greater lung inflation, by taking the weight of the viscera off the lungs.

Crocodilians actually have a muscular diaphragm that is analogous to the mammalian diaphragm. The difference is that the muscles for the crocodilian diaphragm pull the pubis (part of the pelvis, which is movable in crocodilians) back, which brings the liver down, thus freeing space for the lungs to expand. This type of diaphragmatic setup has been referred to as the "hepatic piston". The airways form a number of double tubular chambers within each lung. On inhalation and exhalation air moves through the airways in the same direction, thus creating a unidirectional airflow through the lungs. A similar system is found in birds, monitor lizards and iguanas.

Most reptiles lack a secondary palate, meaning that they must hold their breath while swallowing. Crocodilians have evolved a bony secondary palate that allows them to continue breathing while remaining submerged (and protect their brains against damage by struggling prey). Skinks (family Scincidae) also have evolved a bony secondary palate, to varying degrees. Snakes took a different approach and extended their trachea instead. Their tracheal extension sticks out like a fleshy straw, and allows these animals to swallow large prey without suffering from asphyxiation.

How turtles and tortoises breathe has been the subject of much study. To date, only a few species have been studied thoroughly enough to get an idea of how those turtles breathe. The varied results indicate that turtles and tortoises have found a variety of solutions to this problem.

The difficulty is that most turtle shells are rigid and do not allow for the type of expansion and contraction that other amniotes use to ventilate their lungs. Some turtles, such as the Indian flapshell ("Lissemys punctata"), have a sheet of muscle that envelops the lungs. When it contracts, the turtle can exhale. When at rest, the turtle can retract the limbs into the body cavity and force air out of the lungs. When the turtle protracts its limbs, the pressure inside the lungs is reduced, and the turtle can suck air in. Turtle lungs are attached to the inside of the top of the shell (carapace), with the bottom of the lungs attached (via connective tissue) to the rest of the viscera. By using a series of special muscles (roughly equivalent to a diaphragm), turtles are capable of pushing their viscera up and down, resulting in effective respiration, since many of these muscles have attachment points in conjunction with their forelimbs (indeed, many of the muscles expand into the limb pockets during contraction).

Breathing during locomotion has been studied in three species, and they show different patterns. Adult female green sea turtles do not breathe as they crutch along their nesting beaches. They hold their breath during terrestrial locomotion and breathe in bouts as they rest. North American box turtles breathe continuously during locomotion, and the ventilation cycle is not coordinated with the limb movements. This is because they use their abdominal muscles to breathe during locomotion. The last species to have been studied is the red-eared slider, which also breathes during locomotion, but takes smaller breaths during locomotion than during small pauses between locomotor bouts, indicating that there may be mechanical interference between the limb movements and the breathing apparatus. Box turtles have also been observed to breathe while completely sealed up inside their shells.

Reptilian skin is covered in a horny epidermis, making it watertight and enabling reptiles to live on dry land, in contrast to amphibians. Compared to mammalian skin, that of reptiles is rather thin and lacks the thick dermal layer that produces leather in mammals.
Exposed parts of reptiles are protected by scales or scutes, sometimes with a bony base, forming armor. In lepidosaurians, such as lizards and snakes, the whole skin is covered in overlapping epidermal scales. Such scales were once thought to be typical of the class Reptilia as a whole, but are now known to occur only in lepidosaurians. The scales found in turtles and crocodiles are of dermal, rather than epidermal, origin and are properly termed scutes. In turtles, the body is hidden inside a hard shell composed of fused scutes.

Lacking a thick dermis, reptilian leather is not as strong as mammalian leather. It is used in leather-wares for decorative purposes for shoes, belts and handbags, particularly crocodile skin.

Shedding. Reptiles shed their skin through a process called ecdysis which occurs continuously throughout their lifetime. In particular, younger reptiles tend to shed once every 5–6 weeks while adults shed 3–4 times a year. Younger reptiles shed more because of their rapid growth rate. Once full size, the frequency of shedding drastically decreases. The process of ecdysis involves forming a new layer of skin under the old one. Proteolytic enzymes and lymphatic fluid is secreted between the old and new layers of skin. Consequently, this lifts the old skin from the new one allowing shedding to occur. Snakes will shed from the head to the tail while lizards shed in a "patchy pattern". Dysecdysis, a common skin disease in snakes and lizards, will occur when ecdysis, or shedding, fails. There are numerous reasons why shedding fails and can be related to inadequate humidity and temperature, nutritional deficiencies, dehydration and traumatic injuries. Nutritional deficiencies decrease proteolytic enzymes while dehydration reduces lymphatic fluids to separate the skin layers. Traumatic injuries on the other hand, form scars that will not allow new scales to form and disrupt the process of ecdysis.

Excretion is performed mainly by two small kidneys. In diapsids, uric acid is the main nitrogenous waste product; turtles, like mammals, excrete mainly urea. Unlike the kidneys of mammals and birds, reptile kidneys are unable to produce liquid urine more concentrated than their body fluid. This is because they lack a specialized structure called a loop of Henle, which is present in the nephrons of birds and mammals. Because of this, many reptiles use the colon to aid in the reabsorption of water. Some are also able to take up water stored in the bladder. Excess salts are also excreted by nasal and lingual salt glands in some reptiles.

In all reptiles the urinogenital ducts and the anus both empty into an organ called a cloaca. In some reptiles, a midventral wall in the cloaca may open into a urinary bladder, but not all. It is present in all turtles and tortoises as well as most lizards, but is lacking in the monitor lizard, the legless lizards. It is absent in the snakes, alligators, and crocodiles.

Many turtles, tortoises, and lizards have proportionally very large bladders. Charles Darwin noted that the Galapagos tortoise had a bladder which could store up to 20% of its body weight. Such adaptations are the result of environments such as remote islands and deserts where water is very scarce. Other desert-dwelling reptiles have large bladders that can store a long-term reservoir of water for up to several months and aid in osmoregulation.

Turtles have two or more accessory urinary bladders, located lateral to the neck of the urinary bladder and dorsal to the pubis, occupying a significant portion of their body cavity. Their bladder is also usually bilobed with a left and right section. The right section is located under the liver, which prevents large stones from remaining in that side while the left section is more likely to have calculi.

Most reptiles are insectivorous or carnivorous and have simple and comparatively short digestive tracts due to meat being fairly simple to break down and digest. Digestion is slower than in mammals, reflecting their lower resting metabolism and their inability to divide and masticate their food. Their poikilotherm metabolism has very low energy requirements, allowing large reptiles like crocodiles and large constrictors to live from a single large meal for months, digesting it slowly.

While modern reptiles are predominantly carnivorous, during the early history of reptiles several groups produced some herbivorous megafauna: in the Paleozoic, the pareiasaurs; and in the Mesozoic several lines of dinosaurs. Today, turtles are the only predominantly herbivorous reptile group, but several lines of agamas and iguanas have evolved to live wholly or partly on plants.

Herbivorous reptiles face the same problems of mastication as herbivorous mammals but, lacking the complex teeth of mammals, many species swallow rocks and pebbles (so called gastroliths) to aid in digestion: The rocks are washed around in the stomach, helping to grind up plant matter. Fossil gastroliths have been found associated with both ornithopods and sauropods, though whether they actually functioned as a gastric mill in the latter is disputed. Salt water crocodiles also use gastroliths as ballast, stabilizing them in the water or helping them to dive. A dual function as both stabilizing ballast and digestion aid has been suggested for gastroliths found in plesiosaurs.

The reptilian nervous system contains the same basic part of the amphibian brain, but the reptile cerebrum and cerebellum are slightly larger. Most typical sense organs are well developed with certain exceptions, most notably the snake's lack of external ears (middle and inner ears are present). There are twelve pairs of cranial nerves. Due to their short cochlea, reptiles use electrical tuning to expand their range of audible frequencies.

Reptiles are generally considered less intelligent than mammals and birds. The size of their brain relative to their body is much less than that of mammals, the encephalization quotient being about one tenth of that of mammals, though larger reptiles can show more complex brain development. Larger lizards, like the monitors, are known to exhibit complex behavior, including cooperation. Crocodiles have relatively larger brains and show a fairly complex social structure. The Komodo dragon is even known to engage in play, as are turtles, which are also considered to be social creatures, and sometimes switch between monogamy and promiscuity in their sexual behavior. One study found that wood turtles were better than white rats at learning to navigate mazes. Another study found that giant tortoises are capable of learning through operant conditioning, visual discrimination and retained learned behaviors with long-term memory.

Most reptiles are diurnal animals. The vision is typically adapted to daylight conditions, with color vision and more advanced visual depth perception than in amphibians and most mammals. In some species, such as blind snakes, vision is reduced.

Some snakes have extra sets of visual organs (in the loosest sense of the word) in the form of pits sensitive to infrared radiation (heat). Such heat-sensitive pits are particularly well developed in the pit vipers, but are also found in boas and pythons. These pits allow the snakes to sense the body heat of birds and mammals, enabling pit vipers to hunt rodents in the dark.

Reptiles generally reproduce sexually, though some are capable of asexual reproduction. All reproductive activity occurs through the cloaca, the single exit/entrance at the base of the tail where waste is also eliminated. Most reptiles have copulatory organs, which are usually retracted or inverted and stored inside the body. In turtles and crocodilians, the male has a single median penis, while squamates, including snakes and lizards, possess a pair of hemipenes, only one of which is typically used in each session. Tuatara, however, lack copulatory organs, and so the male and female simply press their cloacas together as the male discharges sperm.

Most reptiles lay amniotic eggs covered with leathery or calcareous shells. An amnion, chorion, and allantois are present during embryonic life. The eggshell (1) protects the crocodile embryo (11) and keeps it from drying out, but it is flexible to allow gas exchange. The chorion (6) aids in gas exchange between the inside and outside of the egg. It allows carbon dioxide to exit the egg and oxygen gas to enter the egg. The albumin (9) further protects the embryo and serves as a reservoir for water and protein. The allantois (8) is a sac that collects the metabolic waste produced by the embryo. The amniotic sac (10) contains amniotic fluid (12) which protects and cushions the embryo. The amnion (5) aids in osmoregulation and serves as a saltwater reservoir. The yolk sac (2) surrounding the yolk (3) contains protein and fat rich nutrients that are absorbed by the embryo via vessels (4) that allow the embryo to grow and metabolize. The air space (7) provides the embryo with oxygen while it is hatching. This ensures that the embryo will not suffocate while it is hatching. There are no larval stages of development. Viviparity and ovoviviparity have evolved in many extinct clades of reptiles and in squamates. In the latter group, many species, including all boas and most vipers, utilize this mode of reproduction. The degree of viviparity varies; some species simply retain the eggs until just before hatching, others provide maternal nourishment to supplement the yolk, and yet others lack any yolk and provide all nutrients via a structure similar to the mammalian placenta. The earliest documented case of viviparity in reptiles is the Early Permian mesosaurs, although some individuals or taxa in that clade may also have been oviparous because a putative isolated egg has also been found. Several groups of Mesozoic marine reptiles also exhibited viviparity, such as mosasaurs, ichthyosaurs, and Sauropterygia, a group that include pachypleurosaurs and Plesiosauria.

Asexual reproduction has been identified in squamates in six families of lizards and one snake. In some species of squamates, a population of females is able to produce a unisexual diploid clone of the mother. This form of asexual reproduction, called parthenogenesis, occurs in several species of gecko, and is particularly widespread in the teiids (especially "Aspidocelis") and lacertids ("Lacerta"). In captivity, Komodo dragons (Varanidae) have reproduced by parthenogenesis.

Parthenogenetic species are suspected to occur among chameleons, agamids, xantusiids, and typhlopids.

Some reptiles exhibit temperature-dependent sex determination (TDSD), in which the incubation temperature determines whether a particular egg hatches as male or female. TDSD is most common in turtles and crocodiles, but also occurs in lizards and tuatara. To date, there has been no confirmation of whether TDSD occurs in snakes.

Many small reptiles, such as snakes and lizards that live on the ground or in the water, are vulnerable to being preyed on by all kinds of carnivorous animals. Thus avoidance is the most common form of defense in reptiles. At the first sign of danger, most snakes and lizards crawl away into the undergrowth, and turtles and crocodiles will plunge into water and sink out of sight.

Reptiles tend to avoid confrontation through camouflage. Two major groups of reptile predators are birds and other reptiles, both of which have well developed color vision. Thus the skins of many reptiles have cryptic coloration of plain or mottled gray, green, and brown to allow them to blend into the background of their natural environment. Aided by the reptiles' capacity for remaining motionless for long periods, the camouflage of many snakes is so effective that people or domestic animals are most typically bitten because they accidentally step on them.

When camouflage fails to protect them, blue-tongued skinks will try to ward off attackers by displaying their blue tongues, and the frill-necked lizard will display its brightly colored frill. These same displays are used in territorial disputes and during courtship. If danger arises so suddenly that flight is useless, crocodiles, turtles, some lizards, and some snakes hiss loudly when confronted by an enemy. Rattlesnakes rapidly vibrate the tip of the tail, which is composed of a series of nested, hollow beads to ward of approaching danger.

In contrast to the normal drab coloration of most reptiles, the lizards of the genus "Heloderma" (the Gila monster and the beaded lizard) and many of the coral snakes have high-contrast warning coloration, warning potential predators they are venomous. A number of non-venomous North American snake species have colorful markings similar to those of the coral snake, an oft cited example of Batesian mimicry.

Camouflage does not always fool a predator. When caught out, snake species adopt different defensive tactics and use a complicated set of behaviors when attacked. Some first elevate their head and spread out the skin of their neck in an effort to look large and threatening. Failure of this strategy may lead to other measures practiced particularly by cobras, vipers, and closely related species, which use venom to attack. The venom is modified saliva, delivered through fangs from a venom gland. Some non-venomous snakes, such as American hognose snakes or European grass snake, play dead when in danger; some, including the grass snake, exude a foul-smelling liquid to deter attackers.

When a crocodilian is concerned about its safety, it will gape to expose the teeth and yellow tongue. If this doesn't work, the crocodilian gets a little more agitated and typically begins to make hissing sounds. After this, the crocodilian will start to change its posture dramatically to make itself look more intimidating. The body is inflated to increase apparent size. If absolutely necessary it may decide to attack an enemy.

Some species try to bite immediately. Some will use their heads as sledgehammers and literally smash an opponent, some will rush or swim toward the threat from a distance, even chasing the opponent onto land or galloping after it. The main weapon in all crocodiles is the bite, which can generate very high bite force. Many species also possess canine-like teeth. These are used primarily for seizing prey, but are also used in fighting and display.

Geckos, skinks, and other lizards that are captured by the tail will shed part of the tail structure through a process called autotomy and thus be able to flee. The detached tail will continue to wiggle, creating a deceptive sense of continued struggle and distracting the predator's attention from the fleeing prey animal. The detached tails of leopard geckos can wiggle for up to 20 minutes. In many species the tails are of a separate and dramatically more intense color than the rest of the body so as to encourage potential predators to strike for the tail first. In the shingleback skink and some species of geckos, the tail is short and broad and resembles the head, so that the predators may attack it rather than the more vulnerable front part.

Reptiles that are capable of shedding their tails can partially regenerate them over a period of weeks. The new section will however contain cartilage rather than bone, and will never grow to the same length as the original tail. It is often also distinctly discolored compared to the rest of the body and may lack some of the external sculpting features seen in the original tail.

Dinosaurs have been widely depicted in culture since the English palaeontologist Richard Owen coined the name "dinosaur" in 1842. As soon as 1854, the Crystal Palace Dinosaurs were on display to the public in south London. One dinosaur appeared in literature even earlier, as Charles Dickens placed a "Megalosaurus" in the first chapter of his novel "Bleak House" in 1852. The dinosaurs featured in books, films, television programs, artwork, and other media have been used for both education and entertainment. The depictions range from the realistic, as in the television documentaries of the 1990s and first decade of the 21st century, or the fantastic, as in the monster movies of the 1950s and 1960s.

The snake or serpent has played a powerful symbolic role in different cultures. In Egyptian history, the Nile cobra adorned the crown of the pharaoh. It was worshipped as one of the gods and was also used for sinister purposes: murder of an adversary and ritual suicide (Cleopatra). In Greek mythology snakes are associated with deadly antagonists, as a chthonic symbol, roughly translated as "earthbound". The nine-headed Lernaean Hydra that Hercules defeated and the three Gorgon sisters are children of Gaia, the earth. Medusa was one of the three Gorgon sisters who Perseus defeated. Medusa is described as a hideous mortal, with snakes instead of hair and the power to turn men to stone with her gaze. After killing her, Perseus gave her head to Athena who fixed it to her shield called the Aegis. The Titans are depicted in art with their legs replaced by bodies of snakes for the same reason: They are children of Gaia, so they are bound to the earth. In Hinduism, snakes are worshipped as gods, with many women pouring milk on snake pits. The cobra is seen on the neck of Shiva, while Vishnu is depicted often as sleeping on a seven-headed snake or within the coils of a serpent. There are temples in India solely for cobras sometimes called "Nagraj" (King of Snakes), and it is believed that snakes are symbols of fertility. In the annual Hindu festival of Nag Panchami, snakes are venerated and prayed to. In religious terms, the snake and jaguar are arguably the most important animals in ancient Mesoamerica. "In states of ecstasy, lords dance a serpent dance; great descending snakes adorn and support buildings from Chichen Itza to Tenochtitlan, and the Nahuatl word "coatl" meaning serpent or twin, forms part of primary deities such as Mixcoatl, Quetzalcoatl, and Coatlicue." In Christianity and Judaism, a serpent appears in Genesis to tempt Adam and Eve with the forbidden fruit from the Tree of Knowledge of Good and Evil.

The turtle has a prominent position as a symbol of steadfastness and tranquility in religion, mythology, and folklore from around the world. A tortoise's longevity is suggested by its long lifespan and its shell, which was thought to protect it from any foe. In the cosmological myths of several cultures a "World Turtle" carries the world upon its back or supports the heavens.

Deaths from snakebites are uncommon in many parts of the world, but are still counted in tens of thousands per year in India. Snakebite can be treated with antivenom made from the venom of the snake. To produce antivenom, a mixture of the venoms of different species of snake is injected into the body of a horse in ever-increasing dosages until the horse is immunized. Blood is then extracted; the serum is separated, purified and freeze-dried. The cytotoxic effect of snake venom is being researched as a potential treatment for cancers.

Lizards such as the Gila monster produce toxins with medical applications. Gila toxin reduces plasma glucose; the substance is now synthesised for use in the anti-diabetes drug exenatide (Byetta). Another toxin from Gila monster saliva has been studied for use as an anti-Alzheimer's drug.

Geckos have also been used as medicine, especially in China. Turtles have been used in Chinese traditional medicine for thousands of years, with every part of the turtle believed to have medical benefits. There is a lack of scientific evidence that would correlate claimed medical benefits to turtle consumption. Growing demand for turtle meat has placed pressure on vulnerable wild populations of turtles.

Crocodiles are protected in many parts of the world, and are farmed commercially. Their hides are tanned and used to make leather goods such as shoes and handbags; crocodile meat is also considered a delicacy. The most commonly farmed species are the saltwater and Nile crocodiles. Farming has resulted in an increase in the saltwater crocodile population in Australia, as eggs are usually harvested from the wild, so landowners have an incentive to conserve their habitat. Crocodile leather is made into wallets, briefcases, purses, handbags, belts, hats, and shoes. Crocodile oil has been used for various purposes.

Snakes are also farmed, primarily in East and Southeast Asia, and their production has become more intensive in the last decade. Snake farming has been troubling for conservation in the past as it can lead to overexploitation of wild snakes and their natural prey to supply the farms. However, farming snakes can limit the hunting of wild snakes, while reducing the slaughter of higher-order vertebrates like cows. The energy efficiency of snakes is higher than expected for carnivores, due to their ectothermy and low metabolism. Waste protein from the poultry and pig industries is used as feed in snake farms. Snake farms produce meat, snake skin, and antivenom. 

Turtle farming is another known but controversial practice. Turtles have been farmed for a variety of reasons, ranging from food to traditional medicine, the pet trade, and scientific conservation. Demand for turtle meat and medicinal products is one of the main threats to turtle conservation in Asia. Though commercial breeding would seem to insulate wild populations, it can stoke the demand for them and increase wild captures. Even the potentially appealing concept of raising turtles at a farm to release into the wild is questioned by some veterinarians who have had some experience with farm operations. They caution that this may introduce into the wild populations infectious diseases that occur on the farm, but have not (yet) been occurring in the wild.

In the Western world, some snakes (especially docile species such as the ball python and corn snake) are kept as pets. Numerous species of lizard are kept as pets, including bearded dragons, iguanas, anoles, and geckos (such as the popular leopard gecko). 

Turtles and tortoises are an increasingly popular pet, but keeping them can be challenging due to particular requirements, such as temperature control and a varied diet, as well as the long lifespans of turtles, who can potentially outlive their owners. Good hygiene and significant maintenance is necessary when keeping reptiles, due to the risks of "Salmonella" and other pathogens.

A herpetarium is a zoological exhibition space for reptiles or amphibians.





</doc>
<doc id="25410" url="https://en.wikipedia.org/wiki?curid=25410" title="Rhode Island">
Rhode Island

Rhode Island (, like "road"), officially the State of Rhode Island and Providence Plantations, is a state in the New England region of the northeastern United States. It is the smallest U.S. state by area, the seventh least populous, but is also the second most densely populated. Rhode Island is bordered by Connecticut to the west, Massachusetts to the north and east, and the Atlantic Ocean to the south via Rhode Island Sound and Block Island Sound. It also shares a small maritime border with New York. Providence is the state capital and most populous city in Rhode Island.

On May 4, 1776, the Colony of Rhode Island was the first of the Thirteen Colonies to renounce its allegiance to the British Crown, and it was the fourth among the newly independent states to ratify the Articles of Confederation on February 9, 1778. The state boycotted the 1787 convention which drew up the United States Constitution and initially refused to ratify it; it was the last of the original 13 states to do so on May 29, 1790.

Rhode Island's official nickname is "The Ocean State", a reference to the large bays and inlets that amount to about 14 percent of its total area.

Despite its name, most of Rhode Island is located on the mainland of the United States. Its official name is "State of Rhode Island and Providence Plantations", which is derived from the merger of four Colonial settlements. The settlements of Newport and Portsmouth were situated on what is commonly called Aquidneck Island today but was called "Rhode Island" in Colonial times. "Providence Plantation" was the name of the colony founded by Roger Williams in the state's capital of Providence. This was adjoined by the settlement of Warwick; hence the plural Providence Plantations.

It is unclear how the island came to be named "Rhode Island", but two historical events may have been of influence:

The earliest documented use of the name "Rhode Island" for Aquidneck was in 1637 by Roger Williams. The name was officially applied to the island in 1644 with these words: "Aquethneck shall be henceforth called the Isle of Rodes or Rhode-Island." The name "Isle of Rodes" is used in a legal document as late as 1646. Dutch maps as early as 1659 call the island "Red Island" ("Roodt Eylant").

Roger Williams was a theologian who was forced out of the Massachusetts Bay Colony, seeking religious and political tolerance. He and others founded Providence Plantation as a free proprietary colony. "Providence" referred to the concept of divine providence, and "plantation" was an English term for a colony. However, in recent years, the word "plantation" in the state's name became a contested issue, and the Rhode Island General Assembly voted on June 25, 2009 to hold a general referendum determining whether "and Providence Plantations" would be dropped from the official name. 

Advocates for excising "plantation" claimed that the word symbolized an alleged legacy of disenfranchisement for many Rhode Islanders, as well as the proliferation of slavery in the colonies and in the post-colonial United States. Advocates for retaining the name argued that "plantation" was simply an archaic synonym for "colony" and bore no relation to slavery. The referendum election was held on November 2, 2010, and the people voted overwhelmingly (78% to 22%) to retain the entire original name.

In 1636, Roger Williams was banished from the Massachusetts Bay Colony for his religious views, and he settled at the top of Narragansett Bay on land sold or given to him by Narragansett sachem Canonicus. He named the site "Providence Plantations", "having a sense of God's merciful providence unto me in my distress", and it became a place of religious freedom where all were welcome.
In 1638 (after conferring with Williams), Anne Hutchinson, William Coddington, John Clarke, Philip Sherman, and other religious dissenters settled on Aquidneck Island (then known as Rhode Island), which was purchased from the local tribes who called it Pocasset. This settlement was called Portsmouth and was governed by the Portsmouth Compact. The southern part of the island became the separate settlement of Newport after disagreements among the founders.

Samuel Gorton purchased lands at Shawomet in 1642 from the Narragansetts, precipitating a dispute with the Massachusetts Bay Colony. In 1644, Providence, Portsmouth, and Newport united for their common independence as the Colony of Rhode Island and Providence Plantations, governed by an elected council and "president". Gorton received a separate charter for his settlement in 1648 which he named Warwick after his patron.

Brown University was founded in 1764 as the College in the English Colony of Rhode Island and Providence Plantations. It was one of nine Colonial colleges granted charters before the American Revolution, but was the first college in America to accept students regardless of religious affiliation.
Metacomet was the Wampanoag tribe's war leader, whom the colonists called King Philip. They invaded and burned down several of the towns in the area during King Philip's War (1675–1676), including Providence which was attacked twice. A force of Massachusetts, Connecticut, and Plymouth militia under General Josiah Winslow invaded and destroyed the fortified Narragansett Indian village in the Great Swamp in South Kingstown, Rhode Island on December 19, 1675. In one of the final actions of the war, an Indian associated with Benjamin Church killed King Philip in Bristol, Rhode Island.

The colony was amalgamated into the Dominion of New England in 1686, as King James II attempted to enforce royal authority over the autonomous colonies in British North America, but the colony regained its independence under the Royal Charter after the Glorious Revolution of 1688. Slaves were introduced in Rhode Island at this time, although there is no record of any law legalizing slave-holding. The colony later prospered under the slave trade, distilling rum to sell in Africa as part of a profitable triangular trade in slaves and sugar with the Caribbean.

Rhode Island's tradition of independence and dissent gave it a prominent role in the American Revolution. At approximately 2 a.m. on June 10, 1772, a band of Providence residents attacked the grounded revenue schooner "Gaspee", burning it to the waterline for enforcing unpopular trade regulations within Narragansett Bay. Rhode Island was the first of the thirteen colonies to renounce its allegiance to the British Crown on May 4, 1776. It was also the last of the thirteen colonies to ratify the United States Constitution on May 29, 1790, and only under threat of heavy trade tariffs from the other former colonies and after assurances were made that a Bill of Rights would become part of the Constitution. During the Revolution, the British occupied Newport in December 1776. A combined Franco-American force fought to drive them off Aquidneck Island. Portsmouth was the site of the first African-American military unit, the 1st Rhode Island Regiment, to fight for the U.S. in the unsuccessful Battle of Rhode Island of August 29, 1778. A month earlier, the appearance of a French fleet off Newport caused the British to scuttle some of their own ships in an attempt to block the harbor. The British abandoned Newport in October 1779, concentrating their forces in New York City. An expedition of 5,500 French troops under Count Rochambeau arrived in Newport by sea on July 10, 1780. The celebrated march to Yorktown, Virginia in 1781 ended with the defeat of the British at the Siege of Yorktown and the Battle of the Chesapeake.

Rhode Island was heavily involved in the slave trade during the post-revolution era. In 1774, the slave population of Rhode Island was 6.3% of the total, nearly twice as high as any other New England colony.

Rhode Island was also heavily involved in the Industrial Revolution, which began in America in 1787 when Thomas Somers reproduced textile machine plans which he imported from England. He helped to produce the Beverly Cotton Manufactory, in which Moses Brown of Providence took an interest. Moses Brown teamed up with Samuel Slater and helped to create the second cotton mill in America, a water-powered textile mill. The Industrial Revolution moved large numbers of workers into the cities, creating a permanently landless class who were therefore, by the law of the time, also voteless. By 1829, 60% of the state's free white males were ineligible to vote. Several attempts were unsuccessfully made to address this problem, and a new state constitution was passed in 1843 allowing landless men to vote if they could pay a $1 poll tax.

For the first several decades of statehood, Rhode Island was governed in accordance with the 1663 colonial charter. Voting rights were restricted to landowners holding at least $134 in property, disenfranchising well over half of the state's male citizens. The charter apportioned legislative seats equally among the state's towns, over-representing rural areas and under-representing the growing industrial centers. Additionally, the charter disallowed landless citizens from filing civil suits without endorsement from a landowner. Bills were periodically introduced in the legislature to expand suffrage, but they were invariably defeated. In 1841, activists led by Thomas W. Dorr organized an extralegal convention to draft a state constitution, arguing that the charter government violated the Guarantee Clause in Article Four, Section Four of the United States Constitution. In 1842, the charter government and Dorr's supporters held separate elections, and two rival governments claimed sovereignty over the state. Dorr's supporters led an armed rebellion against the charter government, and Dorr was arrested and imprisoned for treason against the state. Later that year, the legislature drafted a state constitution, removing property requirements for American-born citizens but keeping them in place for immigrants, and retaining urban under-representation in the legislature.

In the early 19th century, Rhode Island was subject to a tuberculosis outbreak which led to public hysteria about vampirism.

During the American Civil War, Rhode Island was the first Union state to send troops in response to President Lincoln's request for help from the states. Rhode Island furnished 25,236 fighting men, of whom 1,685 died. On the home front, Rhode Island and the other northern states used their industrial capacity to supply the Union Army with the materials that it needed to win the war. The United States Naval Academy moved to Rhode Island temporarily during the war.

In 1866, Rhode Island abolished racial segregation in the public schools throughout the state.

The 50 years following the Civil War were a time of prosperity and affluence that author William G. McLoughlin calls "Rhode Island's halcyon era." Rhode Island was a center of the Gilded Age and provided a home or summer home to many of the country's most prominent industrialists. This was a time of growth in textile mills and manufacturing and brought an influx of immigrants to fill those jobs, bringing population growth and urbanization. In Newport, New York's wealthiest industrialists created a summer haven to socialize and build grand mansions. Thousands of French-Canadian, Italian, Irish, and Portuguese immigrants arrived to fill jobs in the textile and manufacturing mills in Providence, Pawtucket, Central Falls, and Woonsocket.

During World War I, Rhode Island furnished 28,817 soldiers, of whom 612 died. After the war, the state was hit hard by the Spanish Influenza.

In the 1920s and 1930s, rural Rhode Island saw a surge in Ku Klux Klan membership, largely in reaction to large waves of immigrants moving to the state. The Klan is believed to be responsible for burning the Watchman Industrial School in Scituate, which was a school for African-American children.

Since the Great Depression, the Rhode Island Democratic Party has dominated local politics. Rhode Island has comprehensive health insurance for low-income children and a large social safety net. Many urban areas still have a high rate of children in poverty. Due to an influx of residents from Boston, increasing housing costs have resulted in more homelessness in Rhode Island.

The 350th Anniversary of the founding of Rhode Island was celebrated with a free concert held on the tarmac of the Quonset State Airport on August 31, 1986. Performers included Chuck Berry, Tommy James, and headliner Bob Hope.

In 2003, a nightclub fire in West Warwick claimed 100 lives and resulted in nearly twice as many injured, catching national attention. The fire resulted in criminal sentences.

In March 2010, areas of the state received record flooding due to rising rivers from heavy rain. The first period of rainy weather in mid-March caused localized flooding and, two weeks later, more rain caused more widespread flooding in many towns, especially south of Providence. Rain totals on March 29–30, 2010 exceeded 14 inches (35.5 cm) in many locales, resulting in the inundation of area rivers—especially the Pawtuxet River which runs through central Rhode Island. The overflow of the Pawtuxet River, nearly above flood stage, submerged a sewage treatment plant and closed a five-mile (8 km) stretch of Interstate 95. In addition, it flooded two shopping malls, numerous businesses, and many homes in the towns of Warwick, West Warwick, Cranston, and Westerly. Amtrak service was also suspended between New York and Boston during this period. Following the flood, Rhode Island was in a state of emergency for two days. The Federal Emergency Management Agency (FEMA) was called in to help flood victims.

Rhode Island covers an area of located within the New England region and is bordered on the north and east by Massachusetts, on the west by Connecticut, and on the south by Rhode Island Sound and the Atlantic Ocean. It shares a narrow maritime border with New York State between Block Island and Long Island. The mean elevation of the state is . It is only wide and long, yet the state has a tidal shoreline on Narragansett Bay and the Atlantic Ocean of .

Rhode Island is nicknamed the Ocean State and has a number of oceanfront beaches. It is mostly flat with no real mountains, and the state's highest natural point is Jerimoth Hill, above sea level. The state has two distinct natural regions. Eastern Rhode Island contains the lowlands of the Narragansett Bay, while Western Rhode Island forms part of the New England upland. Rhode Island's forests are part of the Northeastern coastal forests ecoregion.

Narragansett Bay is a major feature of the state's topography. There are more than 30 islands within the bay; the largest is Aquidneck Island which holds the municipalities of Newport, Middletown, and Portsmouth. The second-largest island is Conanicut, and the third is Prudence. Block Island lies about off the southern coast of the mainland and separates Block Island Sound and the Atlantic Ocean proper.

A rare type of rock called Cumberlandite is found only in Rhode Island specifically in the town of Cumberland and is the state rock. There were initially two known deposits of the mineral, but it is an ore of iron and one of the deposits was extensively mined for its ferrous content.

Most of Rhode Island has a humid continental climate, with warm summers and cold winters. The southern coastal portions of the state are the broad transition zone into subtropical climates, with hot summers and cool winters with a mix of rain and snow. Block Island has an oceanic climate. The highest temperature recorded in Rhode Island was , recorded on August 2, 1975 in Providence. The lowest recorded temperature in Rhode Island was on February 5, 1996 in Greene. Monthly average temperatures range from a high of to a low of .

Rhode Island is vulnerable to tropical storms and hurricanes due to its location in New England, catching the brunt of many storms blowing up the eastern seaboard. Some hurricanes that have done significant damage in the state are the 1938 New England hurricane, Hurricane Carol (1954), Hurricane Donna (1960), and Hurricane Bob (1991).

The capital of Rhode Island is Providence. The state's current governor is Gina Raimondo (D), and the lieutenant governor is Daniel McKee (D). Raimondo became Rhode Island's first female governor with a plurality of the vote in the November 2014 state elections. Its United States senators are Jack Reed (D) and Sheldon Whitehouse (D). Rhode Island's two United States representatives are David Cicilline (D-1) and Jim Langevin (D-2). "See congressional districts map." Rhode Island is one of a few states that do not have an official governor's residence. "See List of Rhode Island Governors."

The state legislature is the Rhode Island General Assembly, consisting of the 75-member House of Representatives and the 38-member Senate. Both houses of the bicameral body are currently dominated by the Democratic Party; the presence of the Republican Party is minor in the state government, with Republicans holding a handful of seats in both the Senate and House of Representatives.

Rhode Island's population barely crosses the threshold beyond the minimum of three for additional votes in both the federal House of Representatives and Electoral College; it is well represented relative to its population, with the eighth-highest number of electoral votes and second-highest number of House Representatives per resident. Based on its area, Rhode Island even has the highest density of electoral votes.

Federally, Rhode Island is a reliably Democratic state during presidential elections, usually supporting the Democratic Presidential nominee. The state voted for the Republican Presidential candidate until 1908. Since then, it has voted for the Republican nominee for President seven times, and the Democratic nominee 17 times. The last 16 presidential elections in Rhode Island have resulted in the Democratic Party winning the Ocean State's Electoral College votes 12 times. In the 1980 presidential election, Rhode Island was one of six states to vote against Republican Ronald Reagan. Reagan was the last Republican to win any of the state's counties in a Presidential election until Donald Trump won Kent County in 2016. In 1988, George H. W. Bush won over 40% of the state's popular vote, something that no Republican has done since.
Rhode Island was the Democrats' leading state in 1988 and 2000, and second-best in 1968, 1996, and 2004. Rhode Island's most one-sided Presidential election result was in 1964, with over 80% of Rhode Island's votes going for Lyndon B. Johnson. In 2004, Rhode Island gave John Kerry more than a 20-percentage-point margin of victory (the third-highest of any state), with 59.4% of its vote. All but three of Rhode Island's 39 cities and towns voted for the Democratic candidate. The exceptions were East Greenwich, West Greenwich, and Scituate. In 2008, Rhode Island gave Barack Obama a 28-percentage-point margin of victory (the third-highest of any state), with 63% of its vote. All but one of Rhode Island's 39 cities and towns voted for the Democratic candidate (the exception being Scituate).

Rhode Island is one of 21 states that have abolished capital punishment; it was second do so, just after Michigan, and carried out its last execution in the 1840s. Rhode Island was the second to last state to make prostitution illegal. Until November 2009 Rhode Island law made prostitution legal provided it took place indoors. In a 2009 study Rhode Island was listed as the 9th safest state in the country.

In 2011, Rhode Island became the third state in the United States to pass legislation to allow the use of medical marijuana. Additionally, the Rhode Island General Assembly passed civil unions, and it was signed into law by Governor Lincoln Chafee on July 2, 2011. Rhode Island became the eighth state to fully recognize either same-sex marriage or civil unions. Same-sex marriage became legal on May 2, 2013, and took effect August 1.

Rhode Island has some of the highest taxes in the country, particularly its property taxes, ranking seventh in local and state taxes, and sixth in real estate taxes.

The United States Census Bureau estimates that the population of Rhode Island was 1,057,315 on July 1, 2018, a 0.45% increase since the 2010 United States Census. The center of population of Rhode Island is located in Providence County, in the city of Cranston. A corridor of population can be seen from the Providence area, stretching northwest following the Blackstone River to Woonsocket, where 19th-century mills drove industry and development.

According to the 2010 Census, 81.4% of the population was White (76.4% non-Hispanic white), 5.7% was Black or African American, 0.6% American Indian and Alaska Native, 2.9% Asian, 0.1% Native Hawaiian and other Pacific Islander, 3.3% from two or more races. 12.4% of the total population was of Hispanic or Latino origin (they may be of any race).

Of the people residing in Rhode Island, 58.7% were born in Rhode Island, 26.6% were born in a different state, 2.0% were born in Puerto Rico, U.S. Island areas or born abroad to American parent(s), and 12.6% were foreign born.

According to the U.S. Census Bureau, , Rhode Island had an estimated population of 1,056,298, which is an increase of 1,125, or 0.10%, from the prior year and an increase of 3,731, or 0.35%, since the year 2010. This includes a natural increase since the last census of 15,220 people (that is 66,973 births minus 51,753 deaths) and an increase due to net migration of 14,001 people into the state. Immigration from outside the United States resulted in a net increase of 18,965 people, and migration within the country produced a net decrease of 4,964 people.

Hispanics in the state make up 12.8% of the population, predominantly Dominican, Puerto Rican, and Guatemalan populations.

According to the 2000 U.S. Census, 84% of the population aged 5 and older spoke only American English, while 8.07% spoke Spanish at home, 3.80% Portuguese, 1.96% French, 1.39% Italian and 0.78% speak other languages at home accordingly.

The state's most populous ethnic group, non-Hispanic white, has declined from 96.1% in 1970 to 76.5% in 2011. In 2011, 40.3% of Rhode Island's children under the age of one belonged to racial or ethnic minority groups, meaning that they had at least one parent who was not non-Hispanic white.

6.1% of Rhode Island's population were reported as under 5, 23.6% under 18, and 14.5% were 65 or older. Females made up approximately 52% of the population.

According to the 2010–2015 American Community Survey, the largest ancestry groups were Irish (18.3%), Italian (18.0%), English (10.5%), French (10.4%), and Portuguese (9.3%).

Rhode Island has a higher percentage of Americans of Portuguese ancestry, including Portuguese Americans and Cape Verdean Americans than any other state in the nation. Additionally, the state also has the highest percentage of Liberian immigrants, with more than 15,000 residing in the state. Italian Americans make up a plurality in central and southern Providence County and French-Canadian Americans form a large part of northern Providence County. Irish Americans have a strong presence in Newport and Kent counties. Americans of English ancestry still have a presence in the state as well, especially in Washington County, and are often referred to as "Swamp Yankees." African immigrants, including Cape Verdean Americans, Liberian Americans, Nigerian Americans and Ghanaian Americans, form significant and growing communities in Rhode Island.

Although Rhode Island has the smallest land area of all 50 states, it has the second highest population density of any state in the Union, second to that of New Jersey.


A Pew survey of Rhode Island residents' religious self-identification showed the following distribution of affiliations: Roman Catholic 43%, Protestant 27%, Jewish 1%, Orthodox 1%, Jehovah's Witnesses 1%, Buddhism 1%, Mormonism 0.5%, Hinduism 0.5%, Islam 0.5% and Non-religious 23%. The largest denominations are the Roman Catholic Church with 456,598 adherents, the Episcopal Church with 19,377, the American Baptist Churches USA with 15,220, and the United Methodist Church with 6,901 adherents.

Rhode Island has the highest proportion of Roman Catholic residents of any state, mainly due to large Irish, Italian, and French-Canadian immigration in the past; recently, significant Portuguese and various Hispanic communities have also been established in the state. Though it has the highest overall Catholic percentage of any state, none of Rhode Island's individual counties ranks among the 10 most Catholic in the United States, as Catholics are very evenly spread throughout the state.

The Jewish community of Rhode Island is centered in the Providence area, and emerged during a wave of Jewish immigration predominantly from Eastern Europeans shtetls between 1880 and 1920. The presence of the Touro Synagogue in Newport, the oldest existing synagogue in the United States, emphasizes that these second-wave immigrants did not create Rhode Island's first Jewish community; a comparatively smaller wave of Spanish and Portuguese Jews immigrated to Newport during the colonial era.

Rhode Island is divided into five counties but it has no county governments. The entire state is divided into municipalities, which handle all local government affairs.

There are 39 cities and towns in Rhode Island. Major population centers today result from historical factors; development took place predominantly along the Blackstone, Seekonk, and Providence Rivers with the advent of the water-powered mill. Providence is the base of a large metropolitan area.

The state's 18 largest municipalities ranked by population are :

Some of Rhode Island's cities and towns are further partitioned into villages, in common with many other New England states. Notable villages include Kingston in the town of South Kingstown, which houses the University of Rhode Island; Wickford in the town of North Kingstown, the site of an annual international art festival; and Wakefield where the Town Hall is located for the Town of South Kingstown.

The Rhode Island economy had a colonial base in fishing.

The Blackstone River Valley was a major contributor to the American Industrial Revolution. It was in Pawtucket that Samuel Slater set up Slater Mill in 1793, using the waterpower of the Blackstone River to power his cotton mill. For a while, Rhode Island was one of the leaders in textiles. However, with the Great Depression, most textile factories relocated to southern U.S. states. The textile industry still constitutes a part of the Rhode Island economy but does not have the same power that it once had.

Other important industries in Rhode Island's past included toolmaking, costume jewelry, and silverware. An interesting by-product of Rhode Island's industrial history is the number of abandoned factories, many of them now being used for condominiums, museums, offices, and low-income and elderly housing. Today, much of the economy of Rhode Island is based in services, particularly healthcare and education, and still manufacturing to some extent. The state's nautical history continues in the 21st century in the form of nuclear submarine construction.

Per the 2013 American Communities Survey, Rhode Island has the highest paid elementary school teachers in the country, with an average salary of $75,028 (adjusted to inflation).

The headquarters of Citizens Financial Group is located in Providence, the 14th largest bank in the United States. The Fortune 500 companies CVS Caremark and Textron are based in Woonsocket and Providence, respectively. FM Global, GTECH Corporation, Hasbro, American Power Conversion, Nortek, and Amica Mutual Insurance are all Fortune 1000 companies that are based in Rhode Island.

Rhode Island's 2000 total gross state production was $46.18 billion (adjusted to inflation), placing it 45th in the nation. Its 2000 "per capita" personal income was $41,484 (adjusted to inflation), 16th in the nation. Rhode Island has the lowest level of energy consumption per capita of any state. Additionally, Rhode Island is rated as the 5th most energy efficient state in the country. In December 2012, the state's unemployment rate was 10.2%.

Health services are Rhode Island's largest industry. Second is tourism, supporting 39,000 jobs, with tourism-related sales at $4.56 billion (adjusted to inflation) in the year 2000. The third-largest industry is manufacturing. Its industrial outputs are submarine construction, shipbuilding, costume jewelry, fabricated metal products, electrical equipment, machinery, and boatbuilding. Rhode Island's agricultural outputs are nursery stock, vegetables, dairy products, and eggs.

Rhode Island's taxes were appreciably higher than neighboring states, because Rhode Island's income tax was based on 25% of the payer's federal income tax payment. Former Governor Donald Carcieri claimed that the higher tax rate had an inhibitory effect on business growth in the state and called for reductions to increase the competitiveness of the state's business environment. In 2010, the Rhode Island General Assembly passed a new state income tax structure that was then signed into law on June 9, 2010 by Governor Carcieri. The income tax overhaul has now made Rhode Island competitive with other New England states by lowering its maximum tax rate to 5.99% and reducing the number of tax brackets to three. The state's first income tax was enacted in 1971.

, the largest employers in Rhode Island (excluding employees of municipalities) are the following:

The Rhode Island Public Transit Authority (RIPTA) operates statewide intra- and intercity bus transport from its hubs at Kennedy Plaza in Providence, Pawtucket, and Newport. RIPTA bus routes serve 38 of Rhode Island's 39 cities and towns. (New Shoreham on Block Island is not served). RIPTA currently operates 58 routes, including daytime trolley service (using trolley-style replica buses) in Providence and Newport.

From 2000 through 2008, RIPTA offered seasonal ferry service linking Providence and Newport (already connected by highway) funded by grant money from the United States Department of Transportation. Though the service was popular with residents and tourists, RIPTA was unable to continue on after the federal funding ended. Service was discontinued . The service was resumed in 2016 and has been successful. The privately run Block Island Ferry links Block Island with Newport and Narragansett with traditional and fast-ferry service, while the Prudence Island Ferry connects Bristol with Prudence Island. Private ferry services also link several Rhode Island communities with ports in Connecticut, Massachusetts, and New York. The Vineyard Fast Ferry offers seasonal service to Martha's Vineyard from Quonset Point with bus and train connections to Providence, Boston, and New York. Viking Fleet offers seasonal service from Block Island to New London, Connecticut, and Montauk, New York.

The MBTA Commuter Rail's Providence/Stoughton Line links Providence and T. F. Green Airport with Boston. The line was later extended southward to Wickford Junction, with service beginning April 23, 2012. The state hopes to extend the MBTA line to Kingston and Westerly. as well as explore the possibility of extending Connecticut's Shore Line East to T.F. Green Airport. Amtrak's Acela Express stops at Providence Station (the only Acela stop in Rhode Island), linking Providence to other cities in the Northeast Corridor. Amtrak's Northeast Regional service makes stops at Providence Station, Kingston, and Westerly.

Rhode Island's primary airport for passenger and cargo transport is T. F. Green Airport in Warwick, though most Rhode Islanders who wish to travel internationally on direct flights and those who seek a greater availability of flights and destinations often fly through Logan International Airport in Boston.

Interstate 95 (I-95) runs southwest to northeast across the state, linking Rhode Island with other states along the East Coast. I-295 functions as a partial beltway encircling Providence to the west. I-195 provides a limited-access highway connection from Providence (and Connecticut and New York via I-95) to Cape Cod. Initially built as the easternmost link in the (now cancelled) extension of I-84 from Hartford, Connecticut, a portion of U.S. Route 6 (US 6) through northern Rhode Island is limited-access and links I-295 with downtown Providence.

Several Rhode Island highways extend the state's limited-access highway network. Route 4 is a major north–south freeway linking Providence and Warwick (via I-95) with suburban and beach communities along Narragansett Bay. Route 10 is an urban connector linking downtown Providence with Cranston and Johnston. Route 37 is an important east–west freeway through Cranston and Warwick and links I-95 with I-295. Route 99 links Woonsocket with Providence (via Route 146). Route 146 travels through the Blackstone Valley, linking Providence and I-95 with Worcester, Massachusetts and the Massachusetts Turnpike. Route 403 links Route 4 with Quonset Point.

Several bridges cross Narragansett Bay connecting Aquidneck Island and Conanicut Island to the mainland, most notably the Claiborne Pell Newport Bridge and the Jamestown-Verrazano Bridge.

The East Bay Bike Path stretches from Providence to Bristol along the eastern shore of Narragansett Bay, while the Blackstone River Bikeway will eventually link Providence and Worcester. In 2011, Rhode Island completed work on a marked on-road bicycle path through Pawtucket and Providence, connecting the East Bay Bike Path with the Blackstone River Bikeway, completing a bicycle route through the eastern side of the state. The William C. O'Neill Bike Path (commonly known as the South County Bike Path) is an path through South Kingstown and Narragansett. The Washington Secondary Bike Path stretches from Cranston to Coventry, and the Ten Mile River Greenway path runs through East Providence and Pawtucket.

On May 29, 2014, Governor Lincoln D. Chafee announced that Rhode Island was one of eight states to release a collaborative Action Plan to put 3.3 million zero emission vehicles on the roads by 2025. The goal of the plan is to reduce greenhouse gas and smog-causing emissions. The Action Plan covers promoting zero emission vehicles and investing in the infrastructure to support them.

In 2014, Rhode Island received grants from the Environmental Protection Agency in the amount of $2,711,685 to clean up Brownfield sites in eight locations. The intent of the grants was to provide communities with the funding necessary to assess, clean up, and redevelop contaminated properties, boost local economies, and leverage jobs while protecting public health and the environment.

In 2013, the "Lots of Hope" program was established in the City of Providence to focus on increasing the city's green space and local food production, improve urban neighborhoods, promote healthy lifestyles and improve environmental sustainability. "Lots of Hope", supported by a $100,000 grant, will partner with the City of Providence, the Southside Community Land Trust and the Rhode Island Foundation to convert city-owned vacant lots into productive urban farms.

In 2012, Rhode Island passed bill S2277/H7412, "An act relating to Health and Safety – Environmental Cleanup Objectives for Schools", informally known as the "School Siting Bill." The bill, sponsored by Senator Juan Pichardo and Representative Scott Slater and signed into law by the Governor, made Rhode Island the first state in the US to prohibit school construction on Brownfield Sites where there is an ongoing potential for toxic vapors to negatively impact indoor air quality. It also creates a public participation process whenever a city or town considers building a school on any other kind of contaminated site.

Rhode Island has several colleges and universities:

Some Rhode Islanders speak with the distinctive, non-rhotic, traditional Rhode Island accent that many compare to a cross between the New York City and Boston accents (e.g., "water" sounds like "watuh"). Many Rhode Islanders distinguish a strong "aw" sound (i.e., do not exhibit the cot–caught merger) as one might hear in New Jersey or New York City; for example, the word "coffee" is pronounced . This type of accent may have been brought to the region by early settlers from eastern England in the Puritan migration to New England in the mid-17th century.

Rhode Islanders refer to a drinking fountain as a "bubbler" (sometimes pronounced "bubahluh") and sometimes call milkshakes "cabinets". A foot-long, overstuffed sandwich (of whatever kind) is called a "grinder."

Rhode Island, like the rest of New England, has a tradition of clam chowder. Both the white New England and the red Manhattan varieties are popular, but there is also a unique clear-broth chowder known as "Rhode Island Clam Chowder" available in many restaurants. A culinary tradition in Rhode Island is the "clam cake" (also known as a clam fritter outside of Rhode Island), a deep fried ball of buttery dough with chopped bits of clam inside. They are sold by the half-dozen or dozen in most seafood restaurants around the state, and the quintessential summer meal in Rhode Island is chowder and clam cakes.

The quahog is a large local clam usually used in a chowder. It is also ground and mixed with stuffing or spicy minced sausage, and then baked in its shell to form a "stuffie". Calamari (squid) is sliced into rings and fried as an appetizer in most Italian restaurants, typically served Sicilian-style with sliced banana peppers and marinara sauce on the side. Clams Casino originated in Rhode Island, invented by Julius Keller, the maitre d' in the original Casino next to the seaside Towers in Narragansett. Clams Casino resemble the beloved stuffed quahog but are generally made with the smaller littleneck or cherrystone clam and are unique in their use of bacon as a topping.

The official state drink of Rhode Island is "coffee milk", a beverage created by mixing milk with coffee syrup. This unique syrup was invented in the state and is sold in almost all Rhode Island supermarkets, as well as its bordering states. Johnnycakes have been a Rhode Island staple since Colonial times, made with corn meal and water then pan-fried much like pancakes.

Submarine sandwiches are called "grinders" throughout Rhode Island, and the Italian grinder is especially popular, made with cold cuts such as ham, prosciutto, capicola, salami, and Provolone cheese. Linguiça or chouriço is a spicy Portuguese sausage, frequently served with peppers among the state's large Portuguese community and eaten with hearty bread.

The Farrelly brothers and Seth MacFarlane depict Rhode Island in popular culture, often making comedic parodies of the state. MacFarlane's television series "Family Guy" is based in a fictional Rhode Island city named Quahog, and notable local events and celebrities are regularly lampooned. Peter Griffin is seen working at the Pawtucket brewery, and other state locations are mentioned.

The movie "High Society" (starring Bing Crosby, Grace Kelly, and Frank Sinatra) was set in Newport, Rhode Island.

The 1974 film adaptation of "The Great Gatsby" was also filmed in Newport.

Jacqueline Bouvier and John F. Kennedy were married at St. Mary's church in Newport. Their reception was held at Hammersmith Farm, the Bouvier summer home in Newport.

Cartoonist Don Bousquet, a state icon, has made a career out of Rhode Island culture, drawing Rhode Island-themed gags in "The Providence Journal" and "Yankee" magazine. These cartoons have been reprinted in the "Quahog" series of paperbacks ("I Brake for Quahogs", "Beware of the Quahog", and "The Quahog Walks Among Us".) Bousquet has also collaborated with humorist and "Providence Journal" columnist Mark Patinkin on two books: "The Rhode Island Dictionary" and "The Rhode Island Handbook".

The 1998 film "Meet Joe Black" was filmed at Aldrich Mansion in the Warwick Neck area of Warwick.

"Body of Proof"s first season was filmed entirely in Rhode Island. The show premiered on March 29, 2011.

The 2007 Steve Carell and Dane Cook film "Dan in Real Life" was filmed in various coastal towns in the state. The sunset scene with the entire family on the beach takes place at Napatree Point.

"Jersey Shore" star Pauly D filmed part of his spin-off "The Pauly D Project" in his hometown of Johnston.

The Comedy Central cable television series "Another Period" is set in Newport during the Gilded Age.

Rhode Island has been the first in a number of initiatives. The Colony of Rhode Island and Providence Plantations enacted the first law prohibiting slavery in America on May 18, 1652.

The first act of armed rebellion in America against the British Crown was the boarding and burning of the Revenue Schooner "Gaspee" in Narragansett Bay on June 10, 1772. The idea of a Continental Congress was first proposed at a town meeting in Providence on May 17, 1774. Rhode Island elected the first delegates (Stephen Hopkins and Samuel Ward) to the Continental Congress on June 15, 1774. The Rhode Island General Assembly created the first standing army in the colonies (1,500 men) on April 22, 1775. On June 15, 1775, the first naval engagement took place in the American Revolution between an American sloop commanded by Capt. Abraham Whipple and an armed tender of the British Frigate "Rose". The tender was chased aground and captured. Later in June, the General Assembly created the American Navy when it commissioned the sloops "Katy" and , armed with 24 guns and commanded by Abraham Whipple who was promoted to Commodore. Rhode Island was the first Colony to declare independence from Britain on May 4, 1776.

Slater Mill in Pawtucket was the first commercially successful cotton-spinning mill with a fully mechanized power system in America and was the birthplace of the Industrial Revolution in the US. The oldest Fourth of July parade in the country is still held annually in Bristol, Rhode Island. The first Baptist church in America was founded in Providence in 1638. Ann Smith Franklin of the Newport "Mercury" was the first female newspaper editor in America (August 22, 1762). Touro Synagogue was the first synagogue in America, founded in Newport in 1763.

Pelham Street in Newport was the first in America to be illuminated by gaslight in 1806. The first strike in the United States in which women participated occurred in Pawtucket in 1824. Watch Hill has the nation's oldest flying horses carousel that has been in continuous operation since 1850. The motion picture machine was patented in Providence on April 23, 1867. The first lunch wagon in America was introduced in Providence in 1872. The first nine-hole golf course in America was completed in Newport in 1890. The first state health laboratory was established in Providence on September 1, 1894. The Rhode Island State House was the first building with an all-marble dome to be built in the United States (1895–1901). The first automobile race on a track was held in Cranston on September 7, 1896. The first automobile parade was held in Newport on September 7, 1899 on the grounds of Belcourt Castle.

Rhode Island is nicknamed "The Ocean State", and the nautical nature of Rhode Island's geography pervades its culture. Newport Harbor, in particular, holds many pleasure boats. In the lobby of T. F. Green, the state's main airport, is a large life-sized sailboat, and the state's license plates depict an ocean wave or a sailboat.

Additionally, the large number of beaches in Washington County lures many Rhode Islanders south for summer vacation.

The state was notorious for organized crime activity from the 1950s into the 1990s when the Patriarca crime family held sway over most of New England from its Providence headquarters.

Rhode Islanders developed a unique style of architecture in the 17th century called the stone-ender.

Rhode Island is the only state to still celebrate Victory over Japan Day which is officially named "Victory Day" but is sometimes referred to as "VJ Day." It is celebrated on the second Monday in August.

Nibbles Woodaway, more commonly referred to as "The Big Blue Bug", is a 58-foot-long termite mascot for a Providence extermination business. Since its construction in 1980, it has been featured in several movies and television shows, and has come to be recognized as a cultural landmark by many locals.

Rhode Island has two professional sports teams, both of which are top-level minor league affiliates for teams in Boston. The Pawtucket Red Sox baseball team of the Triple-A International League are an affiliate of the Boston Red Sox. They play at McCoy Stadium in Pawtucket and have won four league titles, the Governors' Cup, in 1973, 1984, 2012, and 2014. McCoy Stadium also has the distinction of being home to the longest professional baseball game ever played – 33 innings.

The other professional minor league team is the Providence Bruins ice hockey team of the American Hockey League, who are an affiliate of the Boston Bruins. They play in the Dunkin' Donuts Center in Providence and won the AHL's Calder Cup during the 1998–99 AHL season.

The Providence Reds were a hockey team that played in the Canadian-American Hockey League (CAHL) between 1926 and 1936 and the American Hockey League (AHL) from 1936 to 1977, the last season of which they played as the Rhode Island Reds. The team won the Calder Cup in 1938, 1940, 1949, and 1956. The Reds played at the Rhode Island Auditorium, located on North Main Street in Providence, Rhode Island from 1926 through 1972, when the team affiliated with the New York Rangers and moved into the newly built Providence Civic Center. The team name came from the rooster known as the Rhode Island Red. They moved to New York in 1977, then to Connecticut in 1997, and are now called the Hartford Wolf Pack.

The Reds are the oldest continuously operating minor-league hockey franchise in North America, having fielded a team in one form or another since 1926 in the CAHL. It is also the only AHL franchise to have never missed a season. The AHL returned to Providence in 1992 in the form of the Providence Bruins.
Before the great expansion of athletic teams all over the country, Providence and Rhode Island in general played a great role in supporting teams. The Providence Grays won the first World Championship in baseball history in 1884. The team played their home games at the old Messer Street Field in Providence. The Grays played in the National League from 1878 to 1885. They defeated the New York Metropolitans of the American Association in a best of five game series at the Polo Grounds in New York. Providence won three straight games to become the first champions in major league baseball history. Babe Ruth played for the minor league Providence Grays of 1914 and hit his only official minor league home run for that team before being recalled by the Grays' parent club, the Boston Red Stockings.

The now-defunct professional football team the Providence Steam Roller won the 1928 NFL title. They played in a 10,000 person stadium called the Cycledrome. The Providence Steamrollers played in the Basketball Association of America which became the National Basketball Association.

Rhode Island is also home to a top semi-professional soccer club, the Rhode Island Reds, which compete in the National premier soccer league, in the fourth division of U.S. Soccer.

Rhode Island is home to one top level non-minor league team, the Rhode Island Rebellion rugby league team, a semi-professional rugby league team that competes in the USA Rugby League, the Top Competition in the United States for the Sport of Rugby League. The Rebellion play their home games at Classical High School in Providence.

There are four NCAA Division I schools in Rhode Island. All four schools compete in different conferences. The Brown University Bears compete in the Ivy League, the Bryant University Bulldogs compete in the Northeast Conference, the Providence College Friars compete in the Big East Conference, and the University of Rhode Island Rams compete in the Atlantic-10 Conference. Three of the schools' football teams compete in the Football Championship Subdivision, the second-highest level of college football in the United States. Brown plays FCS football in the Ivy League, Bryant plays FCS football in the Northeast Conference, and Rhode Island plays FCS football in the Colonial Athletic Association. All four of the Division I schools in the state compete in an intrastate all-sports competition known as the Ocean State Cup, with Bryant winning the most recent cup in 2011–12 academic year.

From 1930 to 1983, America's Cup races were sailed off Newport, and the extreme-sport X Games and Gravity Games were founded and hosted in the state's capital city.

The International Tennis Hall of Fame is in Newport at the Newport Casino, site of the first U.S. National Championships in 1881. The Hall of Fame and Museum were established in 1954 by James Van Alen as "a shrine to the ideals of the game".

Rhode Island is also home to the headquarters of the governing body for youth rugby league in the United States, the American Youth Rugby League Association or AYRLA. The AYRLA has started the first-ever Rugby League youth competition in Providence Middle Schools, a program at the RI Training School, in addition to starting the first High School Competition in the US in Providence Public High School.

The state capitol building is made of white Georgian marble. On top is the world's fourth largest self-supported marble dome. It houses the Rhode Island Charter granted by King Charles II in 1663, the Brown University charter, and other state treasures.

The First Baptist Church of Providence is the oldest Baptist church in the Americas, founded by Roger Williams in 1638.

The first fully automated post office in the country is located in Providence. There are many historic mansions in the seaside city of Newport, including The Breakers, Marble House, and Belcourt Castle. Also located there is the Touro Synagogue, dedicated on December 2, 1763, considered by locals to be the first synagogue within the United States (see below for information on New York City's claim), and still serving. The synagogue showcases the religious freedoms that were established by Roger Williams, as well as impressive architecture in a mix of the classic colonial and Sephardic style. The Newport Casino is a National Historic Landmark building complex that presently houses the International Tennis Hall of Fame and features an active grass-court tennis club.

Scenic Route 1A (known locally as Ocean Road) is in Narragansett. "The Towers" is also located in Narragansett featuring a large stone arch. It was once the entrance to a famous Narragansett casino that burned down in 1900. The Towers now serve as an event venue and host the local Chamber of Commerce, which operates a tourist information center.

The Newport Tower has been hypothesized to be of Viking origin, although most experts believe that it was a Colonial-era windmill.






</doc>
<doc id="25412" url="https://en.wikipedia.org/wiki?curid=25412" title="Rock and roll">
Rock and roll

Rock and roll (often written as rock & roll, rock 'n' roll or rock 'n roll) is a genre of popular music that originated and evolved in the United States during the late 1940s and early 1950s from musical styles such as gospel, jump blues, jazz, boogie woogie, and rhythm and blues, and country music. While elements of what was to become rock and roll can be heard in blues records from the 1920s and in country records of the 1930s, the genre did not acquire its name until 1954.

According to Greg Kot, "rock and roll" refers to a style of popular music originating in the U.S. in the 1950s prior to its development by the mid-1960s into "the more encompassing international style known as "rock music," though the latter also continued to be known as rock and roll." For the purpose of differentiation, this article deals with the first definition.

In the earliest rock and roll styles, either the piano or saxophone was typically the lead instrument, but these instruments were generally replaced or supplemented by guitar in the middle to late 1950s. The beat is essentially a dance rhythm with an accentuated backbeat, which is almost always provided by a snare drum. Classic rock and roll is usually played with one or two electric guitars (one lead, one rhythm), a double bass or string bass or (after the mid-1950s) an electric bass guitar, and a drum kit.

Beyond simply a musical style, rock and roll, as seen in movies, in fan magazines, and on television, influenced lifestyles, fashion, attitudes, and language. In addition, rock and roll may have contributed to the civil rights movement because both African-American and white American teenagers enjoyed the music. It went on to spawn various genres, often without the initially characteristic backbeat, that are now more commonly called rock music or simply "rock".

The term "rock and roll" is defined by "Encyclopædia Britannica" as the music that originated in the mid-1950s and later developed "into the more encompassing international style known as rock music". The term is sometimes also used as synonymous with "rock music" and is defined as such in some dictionaries. 

The phrase "rocking and rolling" originally described the movement of a ship on the ocean, but was used by the early twentieth century, both to describe the spiritual fervor of black church rituals and as a sexual analogy. Various gospel, blues and swing recordings used the phrase before it became used more frequently – but still intermittently – in the 1940s, on recordings and in reviews of what became known as "rhythm and blues" music aimed at a black audience.

In 1934, the song "Rock and Roll" by the Boswell Sisters appeared in the film "Transatlantic Merry-Go-Round". In 1942, "Billboard" magazine columnist Maurie Orodenker started to use the term "rock-and-roll" to describe upbeat recordings such as "Rock Me" by Sister Rosetta Tharpe. By 1943, the "Rock and Roll Inn" in South Merchantville, New Jersey, was established as a music venue. In 1951, Cleveland, Ohio, disc jockey Alan Freed began playing this music style while popularizing the phrase to describe it.

The origins of rock and roll have been fiercely debated by commentators and historians of music. There is general agreement that it arose in the Southern United States – a region that would produce most of the major early rock and roll acts – through the meeting of various influences that embodied a merging of the African musical tradition with European instrumentation. The migration of many former slaves and their descendants to major urban centers such as St. Louis, Memphis, New York City, Detroit, Chicago, Cleveland, and Buffalo (See: Second Great Migration (African American)) meant that black and white residents were living in close proximity in larger numbers than ever before, and as a result heard each other's music and even began to emulate each other's fashions. Radio stations that made white and black forms of music available to both groups, the development and spread of the gramophone record, and African-American musical styles such as jazz and swing which were taken up by white musicians, aided this process of "cultural collision".
The immediate roots of rock and roll lay in the rhythm and blues, then called "race music", and country music of the 1940s and 1950s. Particularly significant influences were jazz, blues, gospel, country, and folk. Commentators differ in their views of which of these forms were most important and the degree to which the new music was a re-branding of African-American rhythm and blues for a white market, or a new hybrid of black and white forms.

In the 1930s, jazz, and particularly swing, both in urban-based dance bands and blues-influenced country swing (Jimmie Rodgers, Moon Mullican and other similar singers), were among the first music to present African-American sounds for a predominantly white audience. One particularly noteworthy example of a jazz song with recognizably rock and roll elements is Big Joe Turner with pianist Pete Johnson's 1939 single "Roll 'Em Pete", which is regarded as an important precursor of rock and roll. The 1940s saw the increased use of blaring horns (including saxophones), shouted lyrics and boogie woogie beats in jazz-based music. During and immediately after World War II, with shortages of fuel and limitations on audiences and available personnel, large jazz bands were less economical and tended to be replaced by smaller combos, using guitars, bass and drums. In the same period, particularly on the West Coast and in the Midwest, the development of jump blues, with its guitar riffs, prominent beats and shouted lyrics, prefigured many later developments. In the documentary film "Hail! Hail! Rock 'n' Roll", Keith Richards proposes that Chuck Berry developed his brand of rock and roll by transposing the familiar two-note lead line of jump blues piano directly to the electric guitar, creating what is instantly recognizable as rock guitar. Similarly, country boogie and Chicago electric blues supplied many of the elements that would be seen as characteristic of rock and roll. Inspired by electric blues, Chuck Berry introduced an aggressive guitar sound to rock and roll, and established the electric guitar as its centrepiece, adapting his rock band instrumentation from the basic blues band instrumentation of a lead guitar, second chord instrument, bass and drums.
Rock and roll arrived at a time of considerable technological change, soon after the development of the electric guitar, amplifier and microphone, and the 45 rpm record. There were also changes in the record industry, with the rise of independent labels like Atlantic, Sun and Chess servicing niche audiences and a similar rise of radio stations that played their music. It was the realization that relatively affluent white teenagers were listening to this music that led to the development of what was to be defined as rock and roll as a distinct genre. Because the development of rock and roll was an evolutionary process, no single record can be identified as unambiguously "the first" rock and roll record. Contenders for the title of "first rock and roll record" include Sister Rosetta Tharpe's "Strange Things Happening Every Day" (1944), "That's All Right" by Arthur Crudup (1947), "The Fat Man" by Fats Domino (1949), Goree Carter's "Rock Awhile" (1949), Jimmy Preston's "Rock the Joint" (1949), which was later covered by Bill Haley & His Comets in 1952, "Rocket 88" by Jackie Brenston and his Delta Cats (Ike Turner and his band The Kings of Rhythm), recorded by Sam Phillips for Sun Records in March 1951. In terms of its wide cultural impact across society in the US and elsewhere, Bill Haley's "Rock Around the Clock", recorded in April 1954 but not a commercial success until the following year, is generally recognized as an important milestone, but it was preceded by many recordings from earlier decades in which elements of rock and roll can be clearly discerned.

Other artists with early rock and roll hits included Chuck Berry, Bo Diddley, Little Richard, Jerry Lee Lewis, and Gene Vincent. Chuck Berry's 1955 classic "Maybellene" in particular features a distorted electric guitar solo with warm overtones created by his small valve amplifier. However, the use of distortion was predated by electric blues guitarists such as Joe Hill Louis, Guitar Slim, Willie Johnson of Howlin' Wolf's band, and Pat Hare; the latter two also made use of distorted power chords in the early 1950s. Also in 1955, Bo Diddley introduced the "Bo Diddley beat" and a unique electric guitar style, influenced by African and Afro-Cuban music and in turn influencing many later artists.

"Rockabilly" usually (but not exclusively) refers to the type of rock and roll music which was played and recorded in the mid-1950s primarily by white singers such as Elvis Presley, Carl Perkins, Johnny Cash, and Jerry Lee Lewis, who drew mainly on the country roots of the music. Elvis Presley was greatly influenced and incorporated his style of music with some of the greatest African American musicians like BB King, Chuck Berry and Fats Domino. His style of music combined with black influences created controversy during a turbulent time in history. Many other popular rock and roll singers of the time, such as Fats Domino and Little Richard, came out of the black rhythm and blues tradition, making the music attractive to white audiences, and are not usually classed as "rockabilly".

Bill Flagg who is a Connecticut resident, began referring to his mix of hillbilly and rock 'n' roll music as rockabilly around 1953. His song "Guitar Rock" is considered as classic rockabilly.

In July 1954, Elvis Presley recorded the regional hit "That's All Right" at Sam Phillips' Sun Studio in Memphis. Three months earlier, on April 12, 1954, Bill Haley & His Comets recorded "Rock Around the Clock". Although only a minor hit when first released, when used in the opening sequence of the movie "Blackboard Jungle" a year later, it set the rock and roll boom in motion. The song became one of the biggest hits in history, and frenzied teens flocked to see Haley and the Comets perform it, causing riots in some cities. "Rock Around the Clock" was a breakthrough for both the group and for all of rock and roll music. If everything that came before laid the groundwork, "Rock Around the Clock" introduced the music to a global audience.

In 1956, the arrival of rockabilly was underlined by the success of songs like "Folsom Prison Blues" by Johnny Cash, "Blue Suede Shoes" by Perkins and the No. 1 hit "Heartbreak Hotel" by Presley. For a few years it became the most commercially successful form of rock and roll. Later rockabilly acts, particularly performing songwriters like Buddy Holly, would be a major influence on British Invasion acts and particularly on the song writing of the Beatles and through them on the nature of later rock music.

Doo-wop was one of the most popular forms of 1950s rhythm and blues, often compared with rock and roll, with an emphasis on multi-part vocal harmonies and meaningless backing lyrics (from which the genre later gained its name), which were usually supported with light instrumentation. Its origins were in African-American vocal groups of the 1930s and 40s, such as the Ink Spots and the Mills Brothers, who had enjoyed considerable commercial success with arrangements based on close harmonies. They were followed by 1940s R&B vocal acts such as the Orioles, the Ravens and the Clovers, who injected a strong element of traditional gospel and, increasingly, the energy of jump blues. By 1954, as rock and roll was beginning to emerge, a number of similar acts began to cross over from the R&B charts to mainstream success, often with added honking brass and saxophone, with the Crows, the Penguins, the El Dorados and the Turbans all scoring major hits. Despite the subsequent explosion in records from doo wop acts in the later '50s, many failed to chart or were one-hit wonders. Exceptions included the Platters, with songs including "The Great Pretender" (1955) and the Coasters with humorous songs like "Yakety Yak" (1958), both of which ranked among the most successful rock and roll acts of the era. Towards the end of the decade there were increasing numbers of white, particularly Italian-American, singers taking up Doo Wop, creating all-white groups like the Mystics and Dion and the Belmonts and racially integrated groups like the Del-Vikings and the Impalas. Doo-wop would be a major influence on vocal surf music, soul and early Merseybeat, including the Beatles.

Many of the earliest white rock and roll hits were covers or partial re-writes of earlier black rhythm and blues or blues songs. Through the late 1940s and early 1950s, R&B music had been gaining a stronger beat and a wilder style, with artists such as Fats Domino and Johnny Otis speeding up the tempos and increasing the backbeat to great popularity on the juke joint circuit. Before the efforts of Freed and others, black music was taboo on many white-owned radio outlets, but artists and producers quickly recognized the potential of rock and roll. Some of Presley's early recordings were covers of black rhythm and blues or blues songs, such as "That's All Right" (a countrified arrangement of a blues number), "Baby Let's Play House", "Lawdy Miss Clawdy" and "Hound Dog". The racial lines, however, are rather more clouded by the fact that some of these R&B songs originally recorded by black artists had been written by white songwriters, such as the team of Jerry Leiber and Mike Stoller. Songwriting credits were often unreliable; many publishers, record executives, and even managers (both white and black) would insert their name as a composer in order to collect royalty checks.
Covers were customary in the music industry at the time; it was made particularly easy by the compulsory license provision of United States copyright law (still in effect). One of the first relevant successful covers was Wynonie Harris's transformation of Roy Brown's 1947 original jump blues hit "Good Rocking Tonight" into a more showy rocker and the Louis Prima rocker "Oh Babe" in 1950, as well as Amos Milburn's cover of what may have been the first white rock and roll record, Hardrock Gunter's "Birmingham Bounce" in 1949. The most notable trend, however, was white pop covers of black R&B numbers. The more familiar sound of these covers may have been more palatable to white audiences, there may have been an element of prejudice, but labels aimed at the white market also had much better distribution networks and were generally much more profitable. Famously, Pat Boone recorded sanitized versions of songs recorded by the likes of Fats Domino, Little Richard, the Flamingos and Ivory Joe Hunter. Later, as those songs became popular, the original artists' recordings received radio play as well.

The cover versions were not necessarily straightforward imitations. For example, Bill Haley's incompletely bowdlerized cover of "Shake, Rattle and Roll" transformed Big Joe Turner's humorous and racy tale of adult love into an energetic teen dance number, while Georgia Gibbs replaced Etta James's tough, sarcastic vocal in "Roll With Me, Henry" (covered as "Dance With Me, Henry") with a perkier vocal more appropriate for an audience unfamiliar with the song to which James's song was an answer, Hank Ballard's "Work With Me, Annie". Elvis' rock and roll version of "Hound Dog", taken mainly from a version recorded by the pop band Freddie Bell and the Bellboys, was very different from the blues shouter that Big Mama Thornton had recorded four years earlier. Other white artists who recorded cover versions of rhythm & blues songs included Gale Storm [Smiley Lewis' "I Hear You Knockin'"], the Diamonds [The Gladiolas' "Little Darlin'" and Frankie Lymon & the Teenagers' "Why Do Fools Fall in Love?"], the Crew Cuts [the Chords' "Sh-Boom" and Nappy Brown's "Don't Be Angry"], the Fountain Sisters [The Jewels' "Hearts of Stone"] and the Maguire Sisters [The Moonglows' "Sincerely"].

Some commentators have suggested a decline of rock and roll in the late 1950s and early 1960s. By 1959, the deaths of Buddy Holly, The Big Bopper and Ritchie Valens in a plane crash (February 1959), the departure of Elvis for service in the United States Army (March 1958), the retirement of Little Richard to become a preacher (October 1957), the scandal surrounding Jerry Lee Lewis' marriage to his thirteen-year-old cousin (May 1958), the arrest of Chuck Berry (December 1959), and the breaking of the Payola scandal implicating major figures, including Alan Freed, in bribery and corruption in promoting individual acts or songs (November 1959), gave a sense that the initial phase of rock and roll had come to an end.

During the late 1950s and early 1960s, the rawer sounds of Elvis Presley, Gene Vincent, Jerry Lee Lewis and Buddy Holly were commercially superseded by a more polished, commercial style of rock and roll. Marketing frequently emphasized the physical looks of the artist rather than the music, contributing to the successful careers of Ricky Nelson, Tommy Sands, Bobby Vee and the Philadelphia trio of Bobby Rydell, Frankie Avalon and Fabian, who all became "teen idols."

Some music historians have also pointed to important and innovative developments that built on rock and roll in this period, including multitrack recording, developed by Les Paul, the electronic treatment of sound by such innovators as Joe Meek, and the "Wall of Sound" productions of Phil Spector, continued desegregation of the charts, the rise of surf music, garage rock and the Twist dance craze. Surf rock in particular, noted for the use of reverb-drenched guitars, became one of the most popular forms of American rock of the 1960s.

In the 1950s, Britain was well placed to receive American rock and roll music and culture. It shared a common language, had been exposed to American culture through the stationing of troops in the country, and shared many social developments, including the emergence of distinct youth sub-cultures, which in Britain included the Teddy Boys and the rockers. Trad Jazz became popular, and many of its musicians were influenced by related American styles, including boogie woogie and the blues. The skiffle craze, led by Lonnie Donegan, utilised amateurish versions of American folk songs and encouraged many of the subsequent generation of rock and roll, folk, R&B and beat musicians to start performing. At the same time British audiences were beginning to encounter American rock and roll, initially through films including "Blackboard Jungle" (1955) and "Rock Around the Clock" (1955). Both movies contained the Bill Haley & His Comets hit "Rock Around the Clock", which first entered the British charts in early 1955 – four months before it reached the US pop charts – topped the British charts later that year and again in 1956, and helped identify rock and roll with teenage delinquency. American rock and roll acts such as Elvis Presley, Little Richard, Buddy Holly, Chuck Berry and Carl Perkins thereafter became major forces in the British charts.

The initial response of the British music industry was to attempt to produce copies of American records, recorded with session musicians and often fronted by teen idols. More grassroots British rock and rollers soon began to appear, including Wee Willie Harris and Tommy Steele. During this period American Rock and Roll remained dominant; however, in 1958 Britain produced its first "authentic" rock and roll song and star, when Cliff Richard reached number 2 in the charts with "Move It". At the same time, TV shows such as "Six-Five Special" and "Oh Boy!" promoted the careers of British rock and rollers like Marty Wilde and Adam Faith. Cliff Richard and his backing band, the Shadows, were the most successful home grown rock and roll based acts of the era. Other leading acts included Billy Fury, Joe Brown, and Johnny Kidd & the Pirates, whose 1960 hit song "Shakin' All Over" became a rock and roll standard.

As interest in rock and roll was beginning to subside in America in the late 1950s and early 1960s, it was taken up by groups in major British urban centres like Liverpool, Manchester, Birmingham, and London. About the same time, a British blues scene developed, initially led by purist blues followers such as Alexis Korner and Cyril Davies who were directly inspired by American musicians such as Robert Johnson, Muddy Waters and Howlin' Wolf. Many groups moved towards the beat music of rock and roll and rhythm and blues from skiffle, like the Quarrymen who became the Beatles, producing a form of rock and roll revivalism that carried them and many other groups to national success from about 1963 and to international success from 1964, known in America as the British Invasion. Groups that followed the Beatles included the beat-influenced Freddie and the Dreamers, Wayne Fontana and the Mindbenders, Herman's Hermits and the Dave Clark Five. Early British rhythm and blues groups with more blues influences include the Animals, the Rolling Stones, and the Yardbirds.

Rock and roll influenced lifestyles, fashion, attitudes, and language. In addition, rock and roll may have contributed to the civil rights movement because both African-American and white American teens enjoyed the music.

Many early rock and roll songs dealt with issues of cars, school, dating, and clothing. The lyrics of rock and roll songs described events and conflicts that most listeners could relate to through personal experience. Topics such as sex that had generally been considered taboo began to appear in rock and roll lyrics. This new music tried to break boundaries and express emotions that people were actually feeling but had not talked about. An awakening began to take place in American youth culture.

In the crossover of African-American "race music" to a growing white youth audience, the popularization of rock and roll involved both black performers reaching a white audience and white musicians performing African-American music. Rock and roll appeared at a time when racial tensions in the United States were entering a new phase, with the beginnings of the civil rights movement for desegregation, leading to the U.S. Supreme Court ruling that abolished the policy of "separate but equal" in 1954, but leaving a policy which would be extremely difficult to enforce in parts of the United States. The coming together of white youth audiences and black music in rock and roll inevitably provoked strong white racist reactions within the US, with many whites condemning its breaking down of barriers based on color. Many observers saw rock and roll as heralding the way for desegregation, in creating a new form of music that encouraged racial cooperation and shared experience. Many authors have argued that early rock and roll was instrumental in the way both white and black teenagers identified themselves.

Several rock historians have claimed that rock and roll was one of the first music genres to define an age group. It gave teenagers a sense of belonging, even when they were alone. Rock and roll is often identified with the emergence of teen culture among the first baby boomer generation, who had greater relative affluence and leisure time and adopted rock and roll as part of a distinct subculture. This involved not just music, absorbed via radio, record buying, jukeboxes and TV programs like "American Bandstand", but also extended to film, clothes, hair, cars and motorbikes, and distinctive language. The youth culture exemplified by rock and roll was a recurring source of concern for older generations, who worried about juvenile delinquency and social rebellion, particularly because to a large extent rock and roll culture was shared by different racial and social groups.

In America, that concern was conveyed even in youth cultural artifacts such as comic books. In "There's No Romance in Rock and Roll" from "True Life Romance" (1956), a defiant teen dates a rock and roll-loving boy but drops him for one who likes traditional adult music—to her parents' relief. In Britain, where postwar prosperity was more limited, rock and roll culture became attached to the pre-existing Teddy Boy movement, largely working class in origin, and eventually to the rockers. Rock and roll has been seen as reorienting popular music toward a youth market, as in Dion and the Belmonts' "A Teenager in Love" (1960).

From its early 1950s beginnings through the early 1960s, rock and roll spawned new dance crazes including the twist. Teenagers found the syncopated backbeat rhythm especially suited to reviving Big Band-era jitterbug dancing. Sock hops, school and church gym dances, and home basement dance parties became the rage, and American teens watched Dick Clark's "American Bandstand" to keep up on the latest dance and fashion styles. From the mid-1960s on, as "rock and roll" was rebranded as "rock," later dance genres followed, leading to funk, disco, house, techno, and hip hop.



</doc>
<doc id="25414" url="https://en.wikipedia.org/wiki?curid=25414" title="Religion">
Religion

Religion is a social-cultural system of designated behaviors and practices, morals, worldviews, texts, sanctified places, prophecies, ethics, or organizations, that relates humanity to supernatural, transcendental, or spiritual elements. However, there is no scholarly consensus over what precisely constitutes a religion.

Different religions may or may not contain various elements ranging from the divine, sacred things, faith, a supernatural being or supernatural beings or "some sort of ultimacy and transcendence that will provide norms and power for the rest of life". Religious practices may include rituals, sermons, commemoration or veneration (of deities), sacrifices, festivals, feasts, trances, initiations, funerary services, matrimonial services, meditation, prayer, music, art, dance, public service, or other aspects of human culture. Religions have sacred histories and narratives, which may be preserved in sacred scriptures, and symbols and holy places, that aim mostly to give a meaning to life. Religions may contain symbolic stories, which are sometimes said by followers to be true, that have the side purpose of explaining the origin of life, the universe, and other things. Traditionally, faith, in addition to reason, has been considered a source of religious beliefs.

There are an estimated 10,000 distinct religions worldwide, but about 84% of the world's population is affiliated with one of the five largest religion groups, namely Christianity, Islam, Hinduism, Buddhism or forms of folk religion. The religiously unaffiliated demographic includes those who do not identify with any particular religion, atheists, and agnostics. While the religiously unaffiliated have grown globally, many of the religiously unaffiliated still have various religious beliefs.<ref name="Pew Global Unaffiliated 12/2012"></ref>

The study of religion encompasses a wide variety of academic disciplines, including theology, comparative religion and social scientific studies. Theories of religion offer various explanations for the origins and workings of religion, including the ontological foundations of religious being and belief.

"Religion" (from O.Fr. "religion" religious community, from L. "religionem" (nom. "religio") "respect for what is sacred, reverence for the gods, sense of right, moral obligation, sanctity", "obligation, the bond between man and the gods") is derived from the Latin "religiō", the ultimate origins of which are obscure. One possible interpretation traced to Cicero, connects ' read, i.e. "re" (again) with "lego" in the sense of choose, go over again or consider carefully. The definition of "religio" by Cicero is "cultum deorum", "the proper performance of rites in veneration of the gods." Julius Caesar used "religio" to mean "obligation of an oath" when discussing captured soldiers making an oath to their captors. The Roman naturalist Pliny the Elder used the term "religio" on elephants in that they venerate the sun and the moon. Modern scholars such as Tom Harpur and Joseph Campbell favor the derivation from ' bind, connect, probably from a prefixed "", i.e. "re" (again) + "ligare" or to reconnect, which was made prominent by St. Augustine, following the interpretation given by Lactantius in "Divinae institutiones", IV, 28. The medieval usage alternates with "order" in designating bonded communities like those of monastic orders: "we hear of the 'religion' of the Golden Fleece, of a knight 'of the religion of Avys'".

In classic antiquity, 'religio' broadly meant conscientiousness, sense of right, moral obligation, or duty to anything. In the ancient and medieval world, the etymological Latin root "religio" was understood as an individual virtue of worship in mundane contexts; never as doctrine, practice, or actual source of knowledge. In general, "religio" referred to broad social obligations towards anything including family, neighbors, rulers, and even towards God. "Religio" was most often used by the ancient Romans not in the context of a relation towards gods, but as a range of general emotions such as hesitation, caution, anxiety, fear; feelings of being bound, restricted, inhibited; which arose from heightened attention in any mundane context. The term was also closely related to other terms like "scrupulus" which meant "very precisely" and some Roman authors related the term "superstitio", which meant too much fear or anxiety or shame, to "religio" at times. When "religio" came into English around the 1200s as religion, it took the meaning of "life bound by monastic vows" or monastic orders. The compartmentalized concept of religion, where religious things were separated from worldly things, was not used before the 1500s. The concept of religion was first used in the 1500s to distinguish the domain of the church and the domain of civil authorities.

In the ancient Greece, the Greek term "threskeia" was loosely translated into Latin as "religio" in late antiquity. The term was sparsely used in classical Greece but became more frequently used in the writings of Josephus in the first century CE. It was used in mundane contexts and could mean multiple things from respectful fear to excessive or harmfully distracting practices of others; to cultic practices. It was often contrasted with the Greek word "deisidaimonia" which meant too much fear.

The modern concept of religion, as an abstraction that entails distinct sets of beliefs or doctrines, is a recent invention in the English language. Such usage began with texts from the 17th century due to events such the splitting of Christendom during the Protestant Reformation and globalization in the age of exploration, which involved contact with numerous foreign cultures with non-European languages.
Some argue that regardless of its definition, it is not appropriate to apply the term religion to non-Western cultures. Others argue that using religion on non-Western cultures distorts what people do and believe.

The concept of religion was formed in the 16th and 17th centuries, despite the fact that ancient sacred texts like the Bible, the Quran, and others did not have a word or even a concept of religion in the original languages and neither did the people or the cultures in which these sacred texts were written. For example, there is no precise equivalent of religion in Hebrew, and Judaism does not distinguish clearly between religious, national, racial, or ethnic identities. One of its central concepts is "halakha", meaning the walk or path sometimes translated as law, which guides religious practice and belief and many aspects of daily life. Even though the beliefs and traditions of Judaism are found in the ancient world, ancient Jews saw Jewish identity as being about an ethnic or national identity and did not entail a compulsory belief system or regulated rituals. Even in the 1st century CE, Josephus had used the Greek term "ioudaismos", which some translate as Judaism today, even though he used it as an ethnic term, not one linked to modern abstract concepts of religion as a set of beliefs. It was in the 19th century that Jews began to see their ancestral culture as a religion analogous to Christianity. The Greek word "threskeia", which was used by Greek writers such as Herodotus and Josephus, is found in the New Testament. "Threskeia" is sometimes translated as religion in today's translations, however, the term was understood as worship well into the medieval period. In the Quran, the Arabic word "din" is often translated as religion in modern translations, but up to the mid-1600s translators expressed "din" as law.

The Sanskrit word dharma, sometimes translated as religion, also means law. Throughout classical South Asia, the study of law consisted of concepts such as penance through piety and ceremonial as well as practical traditions. Medieval Japan at first had a similar union between imperial law and universal or Buddha law, but these later became independent sources of power.

Throughout the Americas, Native Americans never had a concept of "religion" and any suggestion otherwise is a colonial imposition by Christians.

Though traditions, sacred texts, and practices have existed throughout time, most cultures did not align with Western conceptions of religion since they did not separate everyday life from the sacred. In the 18th and 19th centuries, the terms Buddhism, Hinduism, Taoism, Confucianism, and world religions first entered the English language. No one self-identified as a Hindu or Buddhist or other similar terms before the 1800s. "Hindu" has historically been used as a geographical, cultural, and later religious identifier for people indigenous to the Indian subcontinent. Throughout its long history, Japan had no concept of religion since there was no corresponding Japanese word, nor anything close to its meaning, but when American warships appeared off the coast of Japan in 1853 and forced the Japanese government to sign treaties demanding, among other things, freedom of religion, the country had to contend with this Western idea.

According to the philologist Max Müller in the 19th century, the root of the English word religion, the Latin "religio", was originally used to mean only reverence for God or the gods, careful pondering of divine things, piety (which Cicero further derived to mean diligence). Max Müller characterized many other cultures around the world, including Egypt, Persia, and India, as having a similar power structure at this point in history. What is called ancient religion today, they would have only called law.

Scholars have failed to agree on a definition of religion. There are, however, two general definition systems: the sociological/functional and the phenomenological/philosophical.

Religion is a modern Western concept. Parallel concepts are not found in many current and past cultures; there is no equivalent term for religion in many languages. Scholars have found it difficult to develop a consistent definition, with some giving up on the possibility of a definition. Others argue that regardless of its definition, it is not appropriate to apply it to non-Western cultures.

An increasing number of scholars have expressed reservations about ever defining the essence of religion. They observe that the way we use the concept today is a particularly modern construct that would not have been understood through much of history and in many cultures outside the West (or even in the West until after the Peace of Westphalia). The MacMillan Encyclopedia of Religions states:
The anthropologist Clifford Geertz defined religion as a
Alluding perhaps to Tylor's "deeper motive", Geertz remarked that
The theologian Antoine Vergote took the term supernatural simply to mean whatever transcends the powers of nature or human agency. He also emphasized the cultural reality of religion, which he defined as
Peter Mandaville and Paul James intended to get away from the modernist dualisms or dichotomous understandings of immanence/transcendence, spirituality/materialism, and sacredness/secularity. They define religion as
According to the MacMillan Encyclopedia of Religions, there is an experiential aspect to religion which can be found in almost every culture:

Friedrich Schleiermacher in the late 18th century defined religion as "das schlechthinnige Abhängigkeitsgefühl", commonly translated as "the feeling of absolute dependence".

His contemporary Georg Wilhelm Friedrich Hegel disagreed thoroughly, defining religion as "the Divine Spirit becoming conscious of Himself through the finite spirit."

Edward Burnett Tylor defined religion in 1871 as "the belief in spiritual beings". He argued that narrowing the definition to mean the belief in a supreme deity or judgment after death or idolatry and so on, would exclude many peoples from the category of religious, and thus "has the fault of identifying religion rather with particular developments than with the deeper motive which underlies them". He also argued that the belief in spiritual beings exists in all known societies.

In his book "The Varieties of Religious Experience", the psychologist William James defined religion as "the feelings, acts, and experiences of individual men in their solitude, so far as they apprehend themselves to stand in relation to whatever they may consider the divine". By the term divine James meant "any object that is god"like", whether it be a concrete deity or not" to which the individual feels impelled to respond with solemnity and gravity.

The sociologist Émile Durkheim, in his seminal book "The Elementary Forms of the Religious Life", defined religion as a "unified system of beliefs and practices relative to sacred things". By sacred things he meant things "set apart and forbidden—beliefs and practices which unite into one single moral community called a Church, all those who adhere to them". Sacred things are not, however, limited to gods or spirits. On the contrary, a sacred thing can be "a rock, a tree, a spring, a pebble, a piece of wood, a house, in a word, anything can be sacred". Religious beliefs, myths, dogmas and legends are the representations that express the nature of these sacred things, and the virtues and powers which are attributed to them.

Echoes of James' and Durkheim's definitions are to be found in the writings of, for example, Frederick Ferré who defined religion as "one's way of valuing most comprehensively and intensively". Similarly, for the theologian Paul Tillich, faith is "the state of being ultimately concerned", which "is itself religion. Religion is the substance, the ground, and the depth of man's spiritual life."

When religion is seen in terms of sacred, divine, intensive valuing, or ultimate concern, then it is possible to understand why scientific findings and philosophical criticisms (e.g., those made by Richard Dawkins) do not necessarily disturb its adherents.

Traditionally, faith, in addition to reason, has been considered a source of religious beliefs. The interplay between faith and reason, and their use as perceived support for religious beliefs, have been a subject of interest to philosophers and theologians.

The word "myth" has several meanings.

Ancient polytheistic religions, such as those of Greece, Rome, and Scandinavia, are usually categorized under the heading of mythology. Religions of pre-industrial peoples, or cultures in development, are similarly called myths in the anthropology of religion. The term myth can be used pejoratively by both religious and non-religious people. By defining another person's religious stories and beliefs as mythology, one implies that they are less real or true than one's own religious stories and beliefs. Joseph Campbell remarked, "Mythology is often thought of as "other people's" religions, and religion can be defined as mis-interpreted mythology."

In sociology, however, the term myth has a non-pejorative meaning. There, myth is defined as a story that is important for the group whether or not it is objectively or provably true. Examples include the resurrection of their real-life founder Jesus, which, to Christians, explains the means by which they are freed from sin, is symbolic of the power of life over death, and is also said to be a historical event. But from a mythological outlook, whether or not the event actually occurred is unimportant. Instead, the symbolism of the death of an old life and the start of a new life is what is most significant. Religious believers may or may not accept such symbolic interpretations.

Religions have sacred histories, narratives, and mythologies which may be preserved in sacred scriptures, and symbols and holy places, that aim to explain the meaning of life, the origin of life, or the Universe.

The practices of a religion may include rituals, sermons, commemoration or veneration (of a deity, gods, or goddesses), sacrifices, festivals, feasts, trances, initiations, funerary services, matrimonial services, meditation, prayer, religious music, religious art, sacred dance, public service, or other aspects of human culture.

Religions have a societal basis, either as a living tradition which is carried by lay participants, or with an organized clergy, and a definition of what constitutes adherence or membership.

A number of disciplines study the phenomenon of religion: theology, comparative religion, history of religion, evolutionary origin of religions, anthropology of religion, psychology of religion (including neuroscience of religion and evolutionary psychology of religion), law and religion, and sociology of religion.

Daniel L. Pals mentions eight classical theories of religion, focusing on various aspects of religion: animism and magic, by E.B. Tylor and J.G. Frazer; the psycho-analytic approach of Sigmund Freud; and further Émile Durkheim, Karl Marx, Max Weber, Mircea Eliade, E.E. Evans-Pritchard, and Clifford Geertz.

Michael Stausberg gives an overview of contemporary theories of religion, including cognitive and biological approaches.

Sociological and anthropological theories of religion generally attempt to explain the origin and function of religion. These theories define what they present as universal characteristics of religious belief and practice.

The origin of religion is uncertain. There are a number of theories regarding the subsequent origins of religious practices.

According to anthropologists John Monaghan and Peter Just, "Many of the great world religions appear to have begun as revitalization movements of some sort, as the vision of a charismatic prophet fires the imaginations of people seeking a more comprehensive answer to their problems than they feel is provided by everyday beliefs. Charismatic individuals have emerged at many times and places in the world. It seems that the key to long-term success—and many movements come and go with little long-term effect—has relatively little to do with the prophets, who appear with surprising regularity, but more to do with the development of a group of supporters who are able to institutionalize the movement."

The development of religion has taken different forms in different cultures. Some religions place an emphasis on belief, while others emphasize practice. Some religions focus on the subjective experience of the religious individual, while others consider the activities of the religious community to be most important. Some religions claim to be universal, believing their laws and cosmology to be binding for everyone, while others are intended to be practiced only by a closely defined or localized group. In many places, religion has been associated with public institutions such as education, hospitals, the family, government, and political hierarchies.

Anthropologists John Monoghan and Peter Just state that, "it seems apparent that one thing religion or belief helps us do is deal with problems of human life that are significant, persistent, and intolerable. One important way in which religious beliefs accomplish this is by providing a set of ideas about how and why the world is put together that allows people to accommodate anxieties and deal with misfortune."

While religion is difficult to define, one standard model of religion, used in religious studies courses, was proposed by Clifford Geertz, who simply called it a "cultural system". A critique of Geertz's model by Talal Asad categorized religion as "an anthropological category". Richard Niebuhr's (1894–1962) five-fold classification of the relationship between Christ and culture, however, indicates that religion and culture can be seen as two separate systems, though not without some interplay.

One modern academic theory of religion, social constructionism, says that religion is a modern concept that suggests all spiritual practice and worship follows a model similar to the Abrahamic religions as an orientation system that helps to interpret reality and define human beings. Among the main proponents of this theory of religion are Daniel Dubuisson, Timothy Fitzgerald, Talal Asad, and Jason Ānanda Josephson. The social constructionists argue that religion is a modern concept that developed from Christianity and was then applied inappropriately to non-Western cultures.

Cognitive science of religion is the study of religious thought and behavior from the perspective of the cognitive and evolutionary sciences. The field employs methods and theories from a very broad range of disciplines, including: cognitive psychology, evolutionary psychology, cognitive anthropology, artificial intelligence, cognitive neuroscience, neurobiology, zoology, and ethology. Scholars in this field seek to explain how human minds acquire, generate, and transmit religious thoughts, practices, and schemas by means of ordinary cognitive capacities.

Hallucinations and delusions related to religious content occurs in about 60% of people with schizophrenia. While this number varies across cultures, this had led to theories about a number of influential religious phenomenon and possible relation to psychotic disorders. A number of prophetic experiences are consistent with psychotic symptoms, although retrospective diagnoses are practically impossible. Schizophrenic episodes are also experienced by people who do not have belief in gods.

Religious content is also common in temporal lobe epilepsy, and obsessive-compulsive disorder. Atheistic content is also found to be common with temporal lobe epilepsy.

Comparative religion is the branch of the study of religions concerned with the systematic comparison of the doctrines and practices of the world's religions. In general, the comparative study of religion yields a deeper understanding of the fundamental philosophical concerns of religion such as ethics, metaphysics, and the nature and form of salvation. Studying such material is meant to give one a richer and more sophisticated understanding of human beliefs and practices regarding the sacred, numinous, spiritual and divine.

In the field of comparative religion, a common geographical classification of the main world religions includes Middle Eastern religions (including Zoroastrianism and Iranian religions), Indian religions, East Asian religions, African religions, American religions, Oceanic religions, and classical Hellenistic religions.

In the 19th and 20th centuries, the academic practice of comparative religion divided religious belief into philosophically defined categories called world religions. Some academics studying the subject have divided religions into three broad categories:

Some recent scholarship has argued that not all types of religion are necessarily separated by mutually exclusive philosophies, and furthermore that the utility of ascribing a practice to a certain philosophy, or even calling a given practice religious, rather than cultural, political, or social in nature, is limited. The current state of psychological study about the nature of religiousness suggests that it is better to refer to religion as a largely invariant phenomenon that should be distinguished from cultural norms (i.e. religions).

Some scholars classify religions as either "universal religions" that seek worldwide acceptance and actively look for new converts, or "ethnic religions" that are identified with a particular ethnic group and do not seek converts. Others reject the distinction, pointing out that all religious practices, whatever their philosophical origin, are ethnic because they come from a particular culture. Christianity, Islam, Buddhism and Jainism are universal religions while Hinduism and Judaism are ethnic religions.

The five largest religious groups by world population, estimated to account for 5.8 billion people and 84% of the population, are Christianity, Islam, Buddhism, Hinduism (with the relative numbers for Buddhism and Hinduism dependent on the extent of syncretism) and traditional folk religion.

A global poll in 2012 surveyed 57 countries and reported that 59% of the world's population identified as religious, 23% as not religious, 13% as convinced atheists, and also a 9% decrease in identification as religious when compared to the 2005 average from 39 countries. A follow-up poll in 2015 found that 63% of the globe identified as religious, 22% as not religious, and 11% as convinced atheists. On average, women are more religious than men. Some people follow multiple religions or multiple religious principles at the same time, regardless of whether or not the religious principles they follow traditionally allow for syncretism.

Abrahamic religions are monotheistic religions which believe they descend from Abraham.

Judaism is the oldest Abrahamic religion, originating in the people of ancient Israel and Judea. The Torah is its foundational text, and is part of the larger text known as the Tanakh or Hebrew Bible. It is supplemented by oral tradition, set down in written form in later texts such as the Midrash and the Talmud. Judaism includes a wide corpus of texts, practices, theological positions, and forms of organization. Within Judaism there are a variety of movements, most of which emerged from Rabbinic Judaism, which holds that God revealed his laws and commandments to Moses on Mount Sinai in the form of both the Written and Oral Torah; historically, this assertion was challenged by various groups. The Jewish people were scattered after the destruction of the Temple in Jerusalem in 70 CE. Today there are about 13 million Jews, about 40 per cent living in Israel and 40 per cent in the United States. The largest Jewish religious movements are Orthodox Judaism (Haredi Judaism and Modern Orthodox Judaism), Conservative Judaism and Reform Judaism.

Christianity is based on the life and teachings of Jesus of Nazareth (1st century) as presented in the New Testament. The Christian faith is essentially faith in Jesus as the Christ, the Son of God, and as Savior and Lord. Almost all Christians believe in the Trinity, which teaches the unity of Father, Son (Jesus Christ), and Holy Spirit as three persons in one Godhead. Most Christians can describe their faith with the Nicene Creed. As the religion of Byzantine Empire in the first millennium and of Western Europe during the time of colonization, Christianity has been propagated throughout the world. The main divisions of Christianity are, according to the number of adherents:

There are also smaller groups, including:

Islam is based on the Qur'an, one of the holy books considered by Muslims to be revealed by God, and on the teachings (hadith) of the Islamic prophet Muhammad, a major political and religious figure of the 7th century CE. Islam is based on the unity of all religious philosophies and accepts all of the Abrahamic prophets of Judaism, Christianity and other Abrahamic religions before Muhammad. It is the most widely practiced religion of Southeast Asia, North Africa, Western Asia, and Central Asia, while Muslim-majority countries also exist in parts of South Asia, Sub-Saharan Africa, and Southeast Europe. There are also several Islamic republics, including Iran, Pakistan, Mauritania, and Afghanistan.

Other denominations of Islam include Nation of Islam, Ibadi, Sufism, Quranism, Mahdavia, and non-denominational Muslims. Wahhabism is the dominant Muslim schools of thought in the Kingdom of Saudi Arabia.

Whilst Judaism, Christianity and Islam are commonly seen as the three Abrahamic faiths, there are smaller and newer traditions which lay claim to the designation as well.

For example, the Bahá'í Faith is a new religious movement that has links to the major Abrahamic religions as well as other religions (e.g. of Eastern philosophy). Founded in 19th-century Iran, it teaches the unity of all religious philosophies and accepts all of the prophets of Judaism, Christianity, and Islam as well as additional prophets (Buddha, Mahavira), including its founder Bahá'u'lláh. It is an offshoot of Bábism. One of its divisions is the Orthodox Bahá'í Faith.

Even smaller regional Abrahamic groups also exist, including Samaritanism (primarily in Israel and the West Bank), the Rastafari movement (primarily in Jamaica), and Druze (primarily in Syria and Lebanon).

East Asian religions (also known as Far Eastern religions or Taoic religions) consist of several religions of East Asia which make use of the concept of Tao (in Chinese) or Dō (in Japanese or Korean). They include:



Indian religions are practiced or were founded in the Indian subcontinent. They are sometimes classified as the "dharmic religions", as they all feature dharma, the specific law of reality and duties expected according to the religion.




Indigenous religions or folk religions refers to a broad category of traditional religions that can be characterised by shamanism, animism and ancestor worship, where traditional means "indigenous, that which is aboriginal or foundational, handed down from generation to generation…". These are religions that are closely associated with a particular group of people, ethnicity or tribe; they often have no formal creeds or sacred texts. Some faiths are syncretic, fusing diverse religious beliefs and practices.

Folk religions are often omitted as a category in surveys even in countries where they are widely practiced, e.g. in China.

African traditional religion encompasses the traditional religious beliefs of people in Africa. In West Africa, these religions include the Akan religion, Dahomey (Fon) mythology, Efik mythology, Odinani, Serer religion (A ƭat Roog), and Yoruba religion, while Bushongo mythology, Mbuti (Pygmy) mythology, Lugbara mythology, Dinka religion, and Lotuko mythology come from central Africa. Southern African traditions include Akamba mythology, Masai mythology, Malagasy mythology, San religion, Lozi mythology, Tumbuka mythology, and Zulu mythology. Bantu mythology is found throughout central, southeast, and southern Africa. In north Africa, these traditions include Berber and ancient Egyptian.

There are also notable African diasporic religions practiced in the Americas, such as Santeria, Candomble, Vodun, Lucumi, Umbanda, and Macumba.
Iranian religions are ancient religions whose roots predate the Islamization of Greater Iran. Nowadays these religions are practiced only by minorities.

Zoroastrianism is based on the teachings of prophet Zoroaster in the 6th century BCE. Zoroastrians worship the creator Ahura Mazda. In Zoroastrianism, good and evil have distinct sources, with evil trying to destroy the creation of Mazda, and good trying to sustain it.

Mandaeism is a monotheistic religion with a strongly dualistic worldview. Mandaeans are sometime labeled as the Last Gnostics.

Kurdish religions include the traditional beliefs of the Yazidi, Alevi, and Ahl-e Haqq. Sometimes these are labeled Yazdânism.


Sociological classifications of religious movements suggest that within any given religious group, a community can resemble various types of structures, including churches, denominations, sects, cults, and institutions.

The study of law and religion is a relatively new field, with several thousand scholars involved in law schools, and academic departments including political science, religion, and history since 1980. Scholars in the field are not only focused on strictly legal issues about religious freedom or non-establishment, but also study religions as they are qualified through judicial discourses or legal understanding of religious phenomena. Exponents look at canon law, natural law, and state law, often in a comparative perspective. Specialists have explored themes in Western history regarding Christianity and justice and mercy, rule and equity, and discipline and love. Common topics of interest include marriage and the family and human rights. Outside of Christianity, scholars have looked at law and religion links in the Muslim Middle East and pagan Rome.

Studies have focused on secularization. In particular, the issue of wearing religious symbols in public, such as headscarves that are banned in French schools, have received scholarly attention in the context of human rights and feminism.

Science acknowledges reason, empiricism, and evidence; and religions include revelation, faith and sacredness whilst also acknowledging philosophical and metaphysical explanations with regard to the study of the universe. Both science and religion are not monolithic, timeless, or static because both are complex social and cultural endeavors that have changed through time across languages and cultures.

The concepts of science and religion are a recent invention: the term religion emerged in the 17th century in the midst of colonization and globalization and the Protestant Reformation. The term science emerged in the 19th century out of natural philosophy in the midst of attempts to narrowly define those who studied nature (natural science), and the phrase religion and science emerged in the 19th century due to the reification of both concepts. It was in the 19th century that the terms Buddhism, Hinduism, Taoism, and Confucianism first emerged. In the ancient and medieval world, the etymological Latin roots of both science ("scientia") and religion ("religio") were understood as inner qualities of the individual or virtues, never as doctrines, practices, or actual sources of knowledge.

In general the scientific method gains knowledge by testing hypotheses to develop theories through elucidation of facts or evaluation by experiments and thus only answers cosmological questions about the universe that can be observed and measured. It develops theories of the world which best fit physically observed evidence. All scientific knowledge is subject to later refinement, or even rejection, in the face of additional evidence. Scientific theories that have an overwhelming preponderance of favorable evidence are often treated as "de facto" verities in general parlance, such as the theories of general relativity and natural selection to explain respectively the mechanisms of gravity and evolution.

Religion does not have a method per se partly because religions emerge through time from diverse cultures and it is an attempt to find meaning in the world, and to explain humanity's place in it and relationship to it and to any posited entities. In terms of Christian theology and ultimate truths, people rely on reason, experience, scripture, and tradition to test and gauge what they experience and what they should believe. Furthermore, religious models, understanding, and metaphors are also revisable, as are scientific models.

Regarding religion and science, Albert Einstein states (1940): "For science can only ascertain what is, but not what should be, and outside of its domain value judgments of all kinds remain necessary. Religion, on the other hand, deals only with evaluations of human thought and action; it cannot justifiably speak of facts and relationships between facts…Now, even though the realms of religion and science in themselves are clearly marked off from each other, nevertheless there exist between the two strong reciprocal relationships and dependencies. Though religion may be that which determine the goals, it has, nevertheless, learned from science, in the broadest sense, what means will contribute to the attainment of the goals it has set up." 

Many religions have value frameworks regarding personal behavior meant to guide adherents in determining between right and wrong. These include the Triple Jems of Jainism, Judaism's Halacha, Islam's Sharia, Catholicism's Canon Law, Buddhism's Eightfold Path, and Zoroastrianism's good thoughts, good words, and good deeds concept, among others.

Religion and morality are not synonymous. While it is "an almost automatic assumption." in Christianity, morality can have a secular basis.

The study of religion and morality can be contentious due to ethnocentric views on morality, failure to distinguish between in group and out group altruism, and inconsistent definitions of religiosity.

Religion has had a significant impact on the political system in many countries. Notably, most Muslim-majority countries adopt various aspects of sharia, the Islamic law. Some countries even define themselves in religious terms, such as The Islamic Republic of Iran. The sharia thus affects up to 23% of the global population, or 1.57 billion people who are Muslims. However, religion also affects political decisions in many western countries. For instance, in the United States, 51% of voters would be less likely to vote for a presidential candidate who did not believe in God, and only 6% more likely. Christians make up 92% of members of the US Congress, compared with 71% of the general public (as of 2014). At the same time, while 23% of U.S. adults are religiously unaffiliated, only one member of Congress (Kyrsten Sinema, D-Arizona), or 0.2% of that body, claims no religious affiliation. In most European countries, however, religion has a much smaller influence on politics although it used to be much more important. For instance, same-sex marriage and abortion were illegal in many European countries until recently, following Christian (usually Catholic) doctrine. Several European leaders are atheists (e.g. France’s former president Francois Hollande or Greece's prime minister Alexis Tsipras). In Asia, the role of religion differs widely between countries. For instance, India is still one of the most religious countries and religion still has a strong impact on politics, given that Hindu nationalists have been targeting minorities like the Muslims and the Christians, who historically belonged to the lower castes. By contrast, countries such as China or Japan are largely secular and thus religion has a much smaller impact on politics.

Secularization is the transformation of the politics of a society from close identification with a particular religion's values and institutions toward nonreligious values and secular institutions. The purpose of this is frequently modernization or protection of the populations religious diversity.

One study has found there is a negative correlation between self-defined religiosity and the wealth of nations. In other words, the richer a nation is, the less likely its inhabitants to call themselves religious, whatever this word means to them (Many people identify themselves as part of a religion (not irreligion) but do not self-identify as religious).

Sociologist and political economist Max Weber has argued that Protestant Christian countries are wealthier because of their Protestant work ethic.

According to a study from 2015, Christians hold the largest amount of wealth (55% of the total world wealth), followed by Muslims (5.8%), Hindus (3.3%) and Jews (1.1%). According to the same study it was found that adherents under the classification Irreligion or other religions hold about 34.8% of the total global wealth.

Mayo Clinic researchers examined the association between religious involvement and spirituality, and physical health, mental health, health-related quality of life, and other health outcomes. The authors reported that: "Most studies have shown that religious involvement and spirituality are associated with better health outcomes, including greater longevity, coping skills, and health-related quality of life (even during terminal illness) and less anxiety, depression, and suicide."

The authors of a subsequent study concluded that the influence of religion on health is largely beneficial, based on a review of related literature. According to academic James W. Jones, several studies have discovered "positive correlations between religious belief and practice and mental and physical health and longevity." 

An analysis of data from the 1998 US General Social Survey, whilst broadly confirming that religious activity was associated with better health and well-being, also suggested that the role of different dimensions of spirituality/religiosity in health is rather more complicated. The results suggested "that it may not be appropriate to generalize findings about the relationship between spirituality/religiosity and health from one form of spirituality/religiosity to another, across denominations, or to assume effects are uniform for men and women.

Critics like Hector Avalos Regina Schwartz, Christopher Hitchens and Richard Dawkins have argued that religions are inherently violent and harmful to society by using violence to promote their goals, in ways that are endorsed and exploited by their leaders.

Anthropologist Jack David Eller asserts that religion is not inherently violent, arguing "religion and violence are clearly compatible, but they are not identical." He asserts that "violence is neither essential to nor exclusive to religion" and that "virtually every form of religious violence has its nonreligious corollary."

Done by some (but not all) religions, animal sacrifice is the ritual killing and offering of an animal to appease or maintain favour with a deity. It has been banned in India.

Greek and Roman pagans, who saw their relations with the gods in political and social terms, scorned the man who constantly trembled with fear at the thought of the gods ("deisidaimonia"), as a slave might fear a cruel and capricious master. The Romans called such fear of the gods "superstitio". Ancient Greek historian Polybius described superstition in Ancient Rome as an "instrumentum regni", an instrument of maintaining the cohesion of the Empire.

Superstition has been described as the non rational establishment of cause and effect. Religion is more complex and is often composed of social institutions and has a moral aspect. Some religions may include superstitions or make use of magical thinking. Adherents of one religion sometimes think of other religions as superstition. Some atheists, deists, and skeptics regard religious belief as superstition.

The Roman Catholic Church considers superstition to be sinful in the sense that it denotes a lack of trust in the divine providence of God and, as such, is a violation of the first of the Ten Commandments. The Catechism of the Catholic Church states that superstition "in some sense represents a perverse excess of religion" (para. #2110). "Superstition," it says, "is a deviation of religious feeling and of the practices this feeling imposes. It can even affect the worship we offer the true God, e.g., when one attributes an importance in some way magical to certain practices otherwise lawful or necessary. To attribute the efficacy of prayers or of sacramental signs to their mere external performance, apart from the interior dispositions that they demand is to fall into superstition. Cf. Matthew 23:16–22" (para. #2111)

The terms atheist (lack of belief in any gods) and agnostic (belief in the unknowability of the existence of gods), though specifically contrary to theistic (e.g. Christian, Jewish, and Muslim) religious teachings, do not by definition mean the opposite of religious. There are religions (including Buddhism, Taoism, and Hinduism), in fact, that classify some of their followers as agnostic, atheistic, or nontheistic. The true opposite of religious is the word irreligious. Irreligion describes an absence of any religion; antireligion describes an active opposition or aversion toward religions in general.

Because religion continues to be recognized in Western thought as a universal impulse, many religious practitioners have aimed to band together in interfaith dialogue, cooperation, and religious peacebuilding. The first major dialogue was the Parliament of the World's Religions at the 1893 Chicago World's Fair, which affirmed universal values and recognition of the diversity of practices among different cultures. The 20th century has been especially fruitful in use of interfaith dialogue as a means of solving ethnic, political, or even religious conflict, with Christian–Jewish reconciliation representing a complete reverse in the attitudes of many Christian communities towards Jews.

Recent interfaith initiatives include A Common Word, launched in 2007 and focused on bringing Muslim and Christian leaders together, the "C1 World Dialogue", the Common Ground initiative between Islam and Buddhism, and a United Nations sponsored "World Interfaith Harmony Week".

Culture and religion have usually been seen as closely related. Paul Tillich looked at religion as the soul of culture and culture as the form or framework of religion. In his own words:
Religion as ultimate concern is the meaning-giving substance of culture, and culture is the totality of forms in which the basic concern of religion expresses itself. In abbreviation: religion is the substance of culture, culture is the form of religion. Such a consideration definitely prevents the establishment of a dualism of religion and culture. Every religious act, not only in organized religion, but also in the most intimate movement of the soul, is culturally formed.
Ernst Troeltsch, similarly, looked at culture as the soil of religion and thought that, therefore, transplanting a religion from its original culture to a foreign culture would actually kill it in the same manner that transplanting a plant from its natural soil to an alien soil would kill it. However, there have been many attempts in the modern pluralistic situation to distinguish culture from religion. Domenic Marbaniang has argued that elements grounded on beliefs of a metaphysical nature (religious) are distinct from elements grounded on nature and the natural (cultural). For instance, language (with its grammar) is a cultural element while sacralization of language in which a particular religious scripture is written is more often a religious practice. The same applies to music and the arts.

Criticism of religion is criticism of the ideas, the truth, or the practice of religion, including its political and social implications.







</doc>
<doc id="25417" url="https://en.wikipedia.org/wiki?curid=25417" title="Reed College">
Reed College

Reed College is a private liberal arts college in Portland, Oregon. Founded in 1908, Reed is a residential college with a campus in the Eastmoreland neighborhood, with Tudor-Gothic style architecture, and a forested canyon nature preserve at its center.

Reed is known for its academic rigor, mandatory freshman humanities program, senior thesis, and unusually high proportion of graduates who go on to earn doctorates and other postgraduate degrees. The college has many prominent alumni, including over a hundred Fulbright Scholars, 67 Watson Fellows, and three Winston Churchill Scholars; its 32 Rhodes Scholars are the second-highest count for a liberal arts college. Reed is ranked fourth in the U.S. of all colleges for the percentage of its graduates who go on to earn a PhD.

The Reed Institute (the legal name of the college) was founded in 1908, and held its first classes in 1911. Reed is named for Oregon pioneers Simeon Gannett Reed (1830–1895) and Amanda Reed (died 1904). Simeon was an entrepreneur involved in several enterprises, including trade on the Willamette and Columbia Rivers with his close friend and associate, former Portland Mayor William S. Ladd (for whom Ladd's Addition is named). Unitarian minister Thomas Lamb Eliot, who knew the Reeds from the church choir, is credited with convincing Reed of the need for "a lasting legacy, a 'Reed Institute of Lectures,' and joked it would 'need a mine to run it.'" Reed's will suggested his wife could "devote some portion of my estate to benevolent objects, or to the cultivation, illustration, or development of the fine arts in the city of Portland, or to some other suitable purpose, which shall be of permanent value and contribute to the beauty of the city and to the intelligence, prosperity, and happiness of the inhabitants". Ladd's son, William Mead Ladd, donated 40 acres from the Ladd Estate Company to build the new college. Reed's first president (1910–1919) was William Trufant Foster, a former professor at Bates College and Bowdoin College in Maine.

Contrary to popular belief, the college did not grow out of student revolts and experimentation, but out of a desire to provide a "more flexible, individualized approach to a rigorous liberal arts education". Founded explicitly in reaction to the "prevailing model of East Coast, Ivy League education", the college's lack of varsity athletics, fraternities, and exclusive social clubs – as well as its coeducational, nonsectarian, and egalitarian status—gave way to an intensely academic and intellectual college whose purpose was to devote itself to "the life of the mind", that life being understood primarily as the academic life.

During the 1930s, President Dexter Keezer became very concerned about what he considered to be dishonorable behavior at Reed. Foremost among these behaviors was fraternization among male and female students but the consumption of alcohol was also an issue. A large portion of the Student Council even took the position that Oregon's liquor laws did not apply to Reed's campus. Policies restricting the ability of student's from visiting the dormitories of the opposite sex were fiercely resisted.

The college has a reputation for political liberalism.

According to sociologist Burton Clark, Reed is one of the most unusual institutions of higher learning in the United States, featuring a traditional liberal arts and natural sciences curriculum. It requires freshmen to take Humanities 110, an intensive introduction to multidisciplinary inquiry, covering ancient Greece and Rome, the Hebrew Bible and ancient Jewish history, and as of 2019 Tenochtitlan/Mexico City and the Harlem Renaissance. Its program in the sciences is likewise unusual with its TRIGA research reactor making it the only school in the United States to have a nuclear reactor operated primarily by undergraduates. Reed also requires all students to complete a thesis (a two-semester-long research project conducted under the guidance of professors) during the senior year as a prerequisite of graduation with successful completion of a junior qualifying exam at the end of the junior year a prerequisite to beginning the thesis. Upon completion of the senior thesis, students must also pass an oral exam that may encompass questions not only about the thesis but also about any course previously taken.

Reed maintains a 9:1 student-to-faculty ratio, and its small classes emphasize a "conference" style where the teacher often acts as a mediator for discussion rather than a lecturer. While large lecture-style classes exist, Reed emphasizes its smaller lab and conference sections.
Although letter grades are given to students, grades are de-emphasized at Reed and focus is placed on a narrative evaluation. According to the school, "A conventional letter grade for each course is recorded for every student, but the registrar's office does not distribute grades to students, provided that work continues at satisfactory (C or higher) levels. Unsatisfactory grades are reported directly to the student and the student's adviser. Papers and exams are generally returned to students with lengthy comments but without grades affixed." There is no dean's list or honor roll "per se", but students who maintain a GPA of 3.5 or above for an academic year receive academic commendations at the end of the spring semester which are noted on their transcripts. Many Reed students graduate without knowing their cumulative GPA or their grades in individual classes. Reed also claims to have experienced very little grade inflation over the years, noting, for example, that only ten students graduated with a perfect 4.0 GPA in the period from 1983 to 2012. (Transcripts are accompanied by a card explaining Reed's relatively tough grading system so as not to penalize students applying to graduate schools.) Although Reed does not award Latin honors to graduates, it confers several awards for academic achievement at commencement, including naming students to Phi Beta Kappa.

Reed has no fraternities or sororities and few NCAA sports teams although physical education classes (which range from kayaking to juggling to capoeira) are required for graduation. Reed also has several intercollegiate athletic clubs, most notably the rugby, Ultimate Frisbee, and soccer teams.
Reed's ethical code is known as "The Honor Principle". First introduced as an agreement to promote ethical academic behavior with the explicit end of relieving the faculty of policing student behavior, the Honor Principle was extended to cover all aspects of student life. While inspired by traditional honor systems, Reed's Honor Principle differs from these in that it is a guide for ethical standards themselves and not just their enforcement. Under the Honor Principle, there are no codified rules governing behavior. Rather, the onus is on students individually and as a community to define which behaviors are acceptable and which are not.

Discrete cases of grievance, known as "Honor Cases," are adjudicated by a Judicial Board of twelve full-time students. There is also an "Honor Council" of students, faculty, and staff who educate the community on the Honor Principle and mediate conflict between individuals.

Reed categorizes its academic program into five Divisions and the Humanities program. Overall, Reed offers five Humanities courses, twenty-six department majors, twelve interdisciplinary majors, six dual-degree programs with other colleges and universities, and programs for pre-medical and pre-veterinary students.


Reed President Richard Scholz in 1922 called the educational program as a whole "an honest effort to disregard old historic rivalries and hostilities between the sciences and the arts, between professional and cultural subjects, and, ... the formal chronological cleavage between the graduate and the undergraduate attitude of mind". The Humanities program, which came into being in 1943 (as the union of two year-long courses, one in "world" literature, the other in "world" history) is one manifestation of this effort. One change to the program was the addition of a course in Chinese Civilization in 1995. The faculty has also recently approved several significant changes to the introductory syllabus. These changes include expanding the parameters of the course to include more material regarding urban and cultural environments.

Reed's Humanities program includes the mandatory freshman course "Introduction to Western Humanities" covering ancient Greek and Roman literature, history, art, religion, and philosophy. Sophomores, juniors, and seniors may take "Early Modern Europe" covering Renaissance thought and literature; "Modern Humanities" covering the Enlightenment, the French Revolution, the Industrial Revolution, and Modernism, and/or "Foundations of Chinese Civilization". There is also a Humanities Senior Symposium.

Reed also offers interdisciplinary programs in American studies, Environmental Studies, Biochemistry and Molecular Biology, Chemistry-Physics, Classics-Religion, Dance/Theatre, History-Literature, International and Comparative Policy Studies (ICPS), Literature-Theatre, Mathematics-Economics, and Mathematics-Physics.

Reed offers dual-degree programs in Computer Science (with University of Washington), Engineering (with Caltech, Columbia University, and Rensselaer Polytechnic Institute), Forestry or Environmental Management (with Duke University), and Fine Art (with the Pacific Northwest College of Art).

For Fall 2016, the freshman class had 357 students. 10% were valedictorians of their high school classes and another 2% were salutatorians. 32% ranked in the top 5% of their class. The median scores on their SAT tests were 680 math, 710 verbal, and 680 writing, which puts them at the 96th percentile. The class was drawn from the largest pool ever—5,705 applicants—and was the most selective in Reed's history, with an admittance rate of 31%. To increase student enrollment from historically underrepresented minorities, Reed offers an all-inclusive, all-expenses-paid "Discover Reed Fly-In Program" to non-white US citizens and permanent residents.

The total direct cost for the 2018–19 academic year, including tuition, fees and room-and-board, is $70,550. Indirect costs (books, supplies, transportation, personal expenses) can tack on another $3,950. For the 2017–18 academic year, the average financial aid package – including grants, loans, and work opportunities – was approximately $45,325". In 2017–18 about half of students received financial aid from the college. In 2004 (the most recent data available), 1.4% of Reed graduates defaulted on their student loans – below the national Cohort Default Rate average of 5.1%.

Reed's endowment as of June 30, 2014, was $543 million. In the economic downturn that began in late 2007, Reed's total endowment had declined from $455 million in June 2007 to $311 million in June 2009. By the end of 2013, however, the endowment surpassed the $500 million mark.

In 1995, Reed College refused to participate in the "U.S. News & World Report" "best colleges" rankings, making it the first educational institution in the United States to refuse to participate in college rankings. According to Reed's Office of Admissions the school's refusal to participate is based in 1994 disclosures by the "Wall Street Journal" about institutions flagrantly manipulating data in order to move up in the rankings in "U.S. News" and other popular college guides. "U.S. News" maintains that their rankings are "a very legitimate tool for getting at a certain level of knowledge about colleges." In 2019, a team of statistics students recreated the formula used by "U.S. News" and were able to identify and quantify the penalty imposed on Reed, finding the college to be ranked over 100 places below what their system would otherwise find.

In 2015, "Money" magazine ranked Reed College 196th among U.S. colleges with an overall score of C+ based on its aggregate score on measures of educational quality, tuition costs, and post-graduation alumni earnings.

Reed is ranked as tied for the 93rd best liberal arts college by "U.S. News & World Report" in its 2016 rankings, and tied for 18th in its high school counselor rankings, although the former has been harshly criticized by the college.

In 2006, "Newsweek" magazine named Reed as one of twenty-five "New Ivies", listing it among "the nation's elite colleges." In 2012, "Newsweek" ranked Reed the 15th "most rigorous" college in the nation.

Reed College ranked in the bottom 6% of four year colleges nationwide in the Brookings Institute's rating of U.S. colleges by incremental impact on alumni earnings 10 years post-enrollment.

Reed has produced the second-highest number of Rhodes scholars for any liberal arts college—32—as well as over fifty Fulbright Scholars, over sixty Watson Fellows, and two MacArthur ("Genius") Award winners. A very high proportion of Reed graduates go on to earn PhDs, particularly in the sciences, history, political science, and philosophy. Reed is third in percentage of its graduates who go on to earn PhDs in all disciplines, after only Caltech and Harvey Mudd. In 1961, "Scientific American" declared that second only to Caltech, "This small college in Oregon has been far and away more productive of future scientists than any other institution in the U.S." Reed is first in this percentage in biology, second in chemistry and humanities, third in history, foreign languages, and political science, fourth in science and mathematics, fifth in physics and social sciences, sixth in anthropology, seventh in area and ethnic studies and linguistics, and eighth in English literature and medicine.

Reed's debating team, which had existed for only two years at the time, was awarded the first place sweepstakes trophy for Division II schools at the final tournament of the Northwest Forensics Conference in February 2004.

Loren Pope, former education editor for "The New York Times," writes about Reed in "Colleges That Change Lives," saying, "If you're a genuine intellectual, live the life of the mind, and want to learn for the sake of learning, the place most likely to empower you is not Harvard, Yale, Princeton, Chicago, or Stanford. It is the most intellectual college in the country—Reed in Portland, Oregon."

Since the 1960s, Reed has had a reputation for tolerating open drug use among its students. "The Insider's Guide to the Colleges", written by the staff of "Yale Daily News", notes an impression among students of institutional permissiveness: "According to students, the school does not bust students for drug or alcohol use unless they cause harm or embarrassment to another student."

In April 2008, student Alex Lluch died of a heroin overdose in his on-campus dorm room. His death prompted revelations of several previous incidents, including the near-death heroin overdose of another student only months earlier. College President Colin Diver said "I don't honestly know" whether the drug death was an isolated incident or part of a larger problem. "When you say Reed," Diver said, "two words often come to mind. One is brains. One is drugs." Local reporter James Pitkin of the newspaper "Willamette Week" editorialized that "Reed College, a private school with one of the most prestigious academic programs in the U.S., is one of the last schools in the country where students enjoy almost unlimited freedom to experiment openly with drugs, with little or no hassles from authorities," though "Willamette Week" stated the following week concerning Pitkin's editorial: "As of press time, almost 500 responses, many expressing harsh criticism of "Willamette Week", had been posted on our website."

In March 2010, another student died of drug-related causes in his off-campus residence. This led "The New York Times" to conclude that "Reed…has long been known almost as much for its unusually permissive atmosphere as for its impressively rigorous academics." Law enforcement authorities promised to take action, including sending undercover agents to Reed's annual Renn Fayre celebration.

In February 2012, the Reed administration chose to call the police following the discovery of "two to three pounds of marijuana and a small amount of ecstasy and LSD in the on-campus apartment of two juniors." Following campus debate, Reed's president at the time, Colin Diver, issued a letter to students and staff, saying the college would not tolerate illegal drug use on campus: "Such behavior endangers the health and welfare of the entire community, attracts potentially dangerous criminal activity on campus, undermines the academic mission of the college, and violates the college's obligations under state and federal law." 

Reed has a reputation for being politically liberal.

During the McCarthy era of the 1950s, then-President Duncan Ballantine fired Marxist philosopher Stanley Moore, a tenured professor, for his failure to cooperate with the House Un-American Activities Committee (HUAC) investigation. According to an article in the college's alumni magazine, "because of the decisive support expressed by Reed's faculty, students, and alumni for the three besieged teachers and for the principle of academic freedom, Reed College's experience with McCarthyism stands apart from that of most other American colleges and universities. Elsewhere in the academic world both tenured and nontenured professors with alleged or admitted communist party ties were fired with relatively little fuss or protest. At Reed, however, opposition to the political interrogations of the teachers was so strong that some believed the campus was in danger of closure." A statement of "regret" by the Reed administration and Board of Trustees was published in 1981, formally revising the judgment of the 1954 trustees. In 1993, then-President Steve Koblik invited Moore to visit the College, and in 1995 the last surviving member of the Board that fired Moore expressed his regret and apologized to him.

On September 26, 2016, students organized a boycott of all college operations in participation with the National Day of Boycott, a national day of protest which was proposed by actor Isaiah Washington on Twitter in response to the issue of police brutality against African-Americans. Following the boycott, students created an activist group called Reedies Against Racism (RAR) and presented a list of demands for the college purportedly on behalf of students from marginalized backgrounds. The primary demand concerned Reed's mandatory freshman Humanities course, proposing that the course either be changed to be more inclusive of world literature and classics or to be made not mandatory. One element of the class deemed racist by the protestors was the use of the 1978 Steve Martin song "King Tut" in a discussion about cultural appropriation. Students began a protest campaign against the curriculum by sitting in during lectures with signs with quotations from various African-American and non-white academics. Other protests separate from the Humanities course also included efforts to shout down speakers, including Kimberly Peirce after she was accused of profiting from transphobia while making the film "Boys Don't Cry". The group eventually focused on Reed's banking relationship with Wells Fargo, based on allegations that the bank had invested in the Dakota Access Pipeline project and the private prison industry, and staged an occupation of Reed's Eliot Hall.

There was some opposition to the lecture protests, notably by Reed professor of English Lucía Martínez Valdivia, who stated that a protest during her lecture on Sappho would amplify her pre-existing case of PTSD. In November 2017, Chris Bodenner of "The Atlantic" wrote about growing student resentment toward the tactics of RAR. In response to protests the faculty decided to undergo the decennial review process a year early, as well as to complete the process in three months instead of the usual year. In January 2018, Humanities 110 Chair professor Libby Drumm announced in a campus-wide email that the course curriculum would be restructured after years of faculty discussion and in response to student feedback as well as input from an external review committee composed of humanities faculty from other institutes, adopting a "four-module structure" that would include texts from the Americas and allow greater flexibility in the curriculum which would be integrated beginning fall 2018. The external review had not in fact been completed nor reviewed at the time of the announcement.

Following "a contentious year of protests, including an anti-racism sit-in in Kroger’s office," college president John Kroger resigned, effective June 2018.

The Reed College campus was established on a tract of land in southeast Portland known in 1910 as Crystal Springs Farm, a part of the Ladd Estate, formed in the 1870s from original land claims. The college's grounds include of contiguous land, including a wooded wetland known as Reed Canyon.

Portland architect A. E. Doyle developed a plan, never implemented in full, modeled on the University of Oxford's St. John's College. The original campus buildings (including the Library, the Old Dorm Block, and what is now the primary administration building, Eliot Hall) are brick Tudor Gothic buildings in a style similar to Ivy League campuses. In contrast, the science section of campus, including the physics, biology, and psychology (originally chemistry) buildings, were designed in the Modernist style. The Psychology Building, completed in 1949, was designed by Modernist architect Pietro Belluschi at the same time as his celebrated Equitable Building in downtown Portland.

The campus and buildings have undergone several phases of growth, and there are now 21 academic and administrative buildings and 18 residence halls. Since 2004, Reed's campus has expanded to include adjacent properties beyond its historic boundaries, such as the Birchwood Apartments complex and former medical administrative offices on either side of SE 28th Avenue, and the Parker House, across SE Woodstock from Prexy. At the same time the Willard House (donated to Reed in 1964), across from the college's main entrance at SE Woodstock and SE Reed College Place, was converted from faculty housing to administrative use. Reed announced on July 13, 2007, that it had purchased the Rivelli farm, a tract of land south of the Garden House and west of Botsford Drive. Reed's "immediate plans for the acquired property include housing a small number of students in the former Rivelli home during the 2007–08 academic year. Longer term, the college anticipates that it may seek to develop the northern portion of the property for additional student housing".

Reed houses 946 students in 18 residence halls on campus and several college-owned houses and apartment buildings on or adjacent to campus. Residence halls on campus range from the traditional (i.e., Gothic Old Dorm Block, referred to as "ODB") to the eclectic (e.g., Anna Mann, a Tudor-style cottage built in the 1920s by Reed's founding architect A. E. Doyle, originally used as a women's hall), language houses (Spanish, Russian, French, German, and Chinese), "temporary" housing, built in the 1960s (Cross Canyon – Chittick, Woodbridge, McKinley, Griffin), to more recently built dorms (Bragdon, Naito, Sullivan). There are also theme residence halls including everything from substance-free living to Japanese culture to music to a dorm for students interested in outdoors activities (hiking, climbing, bicycling, kayaking, skiing, etc.). The college's least-loved complex (as measured by applications to the College's housing lottery), MacNaughton and Foster-Scholz, is known on campus as "Asylum Block" because of its post-World War II modernist architecture and interior spaces dominated by long, straight corridors lined with identical doors, said by students to resemble that of an insane asylum. Until 2006, it was thought that these residence halls had been designed by architect Pietro Belluschi.

Under the 10-year Campus Master Plan adopted in 2006, Foster-Scholz is scheduled to be demolished and replaced, and MacNaughton to be remodeled. According to the master plan, "The College's goal is to provide housing on or adjacent to the campus that accommodates 75% of the [full-time] student population. At present, the College provides on-campus housing for 838 students".

In Spring 2007, the College broke ground on the construction of a new quadrangle called the Grove with four new Leed certified residence halls (Aspen, Sequoia, Sitka, Bidwell) on the northwest side of the campus, which opened in Fall 2008. A new Spanish House residence was completed. Together, the five new residences added 142 new beds.

Reed also has off-campus housing. Many houses in the Woodstock and Eastmoreland Portland neighborhoods are traditionally rented to Reed students.

On February 21, 2018, Reed announced the construction of the "largest residence hall in its history." Set to be complete by Fall 2019, it will house an additional 180 students, boosting Reed's housing capacity to nearly 80% of the student body, up from 68%. This will guarantee housing for both freshman and sophomores, as students were formerly subjected to a housing lottery after freshman year. The new building is also designed to meet "LEED Platinum standards," and Reed is currently evaluating proposals to put solar panels on the roof.

The Reed College Canyon, a natural area and national wildlife preserve, bisects the campus, separating the academic buildings from many of the residence halls (the so-called "cross-canyon halls"). The canyon is filled by Crystal Creek Springs, a natural spring that drains into Johnson Creek.

Canyon Day, a tradition dating back to 1915, is held twice a year. On Canyon Day students and Reed neighbors join canyon crew workers to spend a day helping with restoration efforts.

A landmark of the campus, the Blue Bridge, spans the canyon. This bridge replaced the unique cantilevered bridge that served in that spot between 1959 and 1991, which "featured stressed plywood girders – the first time this construction had been used on a span of this size: a straight bridge long and high. It attracted great architectural interest during its lifetime".

A new pedestrian and bicycle bridge spanning the canyon was opened in Fall 2008. This bridge, dubbed the "Bouncy Bridge" or "Amber Bridge" by students, is long, about a third longer than the Blue Bridge, and "connect[s] the new north campus quad to Gray Campus Center, the student union, the library, and academic buildings on the south side of campus".

Reed's Cooley Gallery is an internationally recognized contemporary art space located at the entrance to the Eric V. Hauser Memorial Library. It was established in 1988 as the result of a gift from Susan and Edward Cooley in honor of their late son. The Cooley Gallery has exhibited international artists such as Mona Hatoum, Al Held, David Reed and Gregory Crewdson as well as the contemporary art collection of Michael Ovitz. In pursuit of its mission to support the curriculum of the art, art history, and humanities programs at Reed, the gallery produces three or four exhibitions each year, along with lectures, colloquia, and artist visits. The gallery is currently under the directorship of Stephanie Snyder, who succeeded founding director Susan Fillin-Yeh in 2004.

The cafeteria, known simply as "Commons," has a reputation for ecologically sustainable food services. The commons dining hall is operated by Bon Appétit, and food is purchased on an item-by-item basis. Suiting the student body, vegan and vegetarian dishes feature heavily on the menu. It is currently the only cafeteria on the small campus, with the exception of Caffe Circo (formerly Caffe Paradiso), a small cafe on the other side of campus which also operated by board points. Scrounging is a long tradition at Reed College allowing students to offer unfinished Commons' food to students without board points from their trays as they are returned to be washed.

The Reed College Co-ops are a theme community that reside in the Farm and Garden Houses, after many years on the first floor of MacNaughton Hall. These are the only campus dorms that are independent of the school's board plan. They traditionally throw an alternative "Thanksgiving" celebration that has sometimes included a square-dance. The Co-ops house students who purchase and prepare food together, sharing chores and conducting weekly, consensus-based meetings. It is a close community valuing sustainability, organic food, consensus-based decisions, self-government, music, and plants.

The Paradox ("Est. in the 80s") is a student-run coffee shop located on campus. In 2003 the Paradox opened a second coffee shop, dubbing it the "Paradox Lost" (an allusion to John Milton's "Paradise Lost,") at the southern end of the biology building, in the space commonly called the "Bio Fishbowl." The new north-campus dorms, which opened in Fall 2008, feature yet another small cafe, originally dubbed "Cafe Paradiso," thereby providing three coffee shops within a campus. The recent addition of a circus-themed mural to the cafe prompted a name change, and it now operates as Caffe Circo. This third shop is not student-run, but is operated by Bon Appétit. Bon Appétit has a monopoly on the food services at Reed as they are the only ones who accept board points; written into their contract is the prohibition of food carts on campus.

The official mascot of Reed is the griffin. In mythology, the griffin often pulled the chariot of the sun; in canto 32 of Dante's "Commedia" the griffin is associated with the Tree of Knowledge. The griffin was featured on the coat-of-arms of founder Simeon Reed and is now on the official seal of Reed College.

The official school color of Reed is Richmond Rose. Over the years, institutional memory of this fact has faded and the color appearing on the school's publications and merchandise has darkened to a shade of maroon. The most common examples of "Richmond Rose" are the satin tapes securing the degree certificate inside a Reed College diploma.

The school song, "Fair Reed," is sung to the tune of the 1912 popular song "Believe Me, if All Those Endearing Young Charms." It may be imitative of the Harvard anthem "Fair Harvard," which is also sung to the tune of "Believe Me, if All Those Endearing Young Charms." It was composed by former president William Trufant Foster shortly after Reed's founding, and is rarely heard today.

An unofficial Reed Alma Mater, "Epistemology Forever," sung to the tune of "The Battle Hymn of the Republic," has been sung by Reed students since the 1950s.

Reed students and alumni referred to themselves as "Reedites" in the early years of the college. This term faded out in favor of the now ubiquitous "Reedie" after World War II. Around campus, prospective students are called "prospies."

An unofficial motto of Reed is "Communism, Atheism, Free Love," and can be found in the Reed College Bookstore on sweaters, T-shirts, etc. It was a label that the Reed community claimed from critics during the 1920s as a "tongue-in-cheek slogan" in reference to Reed's nonconformism. Reed's founding president William T. Foster's outspoken opposition against the entrance of the United States into World War I, as well as the college's support for feminism, its adherence to academic freedom (i.e., inviting a leader of the Socialist Party of America to speak on campus about the Russian Revolution’s potential effect on militarism, emancipation of women, and ending the persecution of Jews), and its nonsectarian status made the college a natural target for what was originally meant to be a pejorative slur.

The faux Reed Seal has changed over the years. In its original form the griffin was holding a hammer and sickle in its paws. Later versions had the griffin wearing boxing gloves.

One of the unofficial symbols of Reed is the Doyle Owl, a roughly concrete statue that has been continuously stolen and re-stolen since about 1919. The original Doyle Owl (originally "House F Owl" after the dormitory named House F that later became Doyle dormitory) was a garden sculpture from the neighborhood stolen by House F residents as a prank (there is a photo of House F residents around the original owl that has been made into a T-shirt). The on-campus folklore of events surrounding the Doyle Owl is sufficiently large that, in 1983, a senior thesis was written on the topic of the Owl's oral history. The original Doyle Owl was destroyed many years ago; the current avatar is Doyle Owl number 13, plus or minus 11. At the present time only one Owl is being shown.

Each January, before the beginning of second-semester classes, the campus holds an interim period called Paideia (drawn from the Greek, meaning 'education'). Originally conceived and approved by the faculty in 1968 for unstructured independent study or "UIS," Paideia ran for the full month of January from 1969–1981, supervised by a committee of faculty, staff and students. This festival of learning takes the form of classes and seminars put on by anyone who wishes to teach, including students, professors, staff members, and outside educators invited on-campus by members of the Reed Community. The classes are intended to be informal, yet intellectual activities free of the usual academic pressure endemic to Reed. Many such classes are explicitly trivial (one long-running tradition is to hold an underwater basket weaving class), while others are trivially academic (such as "Giant Concrete Gnome Construction," a class that, incidental to building monolithic gnomes, includes some content relating to the construction of pre-Christian monoliths). More structured classes (such as martial arts seminars and mini-classes on obscure academic topics), tournaments, and film festivals round out the schedule, which is different every year. The objective of Paideia is not only to learn new (possibly non-useful) things, but to turn the tables on students and encourage them to teach.

In his 2005 Stanford commencement lecture, Apple Inc. founder and Reed dropout Steve Jobs credited a Reed calligraphy class taught by Robert Palladino for his focus on choosing quality typefaces for the Macintosh. While the full calligraphy course is no longer taught at Reed, Paideia usually features a short course on the subject in addition to the informal, weekly gatherings (currently held every Thursday night) of aspiring calligraphy enthusiasts.

Renn Fayre is an annual three-day celebration with a different theme each year. Born in the 1960s as an actual renaissance fair, it has long since lost all connection to anachronism and the Renaissance, although its name has persisted. The event is initiated by a procession of seniors throwing their thesis notes in a large bonfire after the completed theses are submitted.

Reed Arts Week is a week-long celebration of the arts at Reed. It features music, dance, film, creative writing, and the visual arts.

According to Reed's website, each semester, a $130 student body fee "is collected from each full-time student by the business office, acting as agent for the student senate. The fee underwrites publication of the student newspaper and extracurricular activities, and partially supports the student union and ski cabin."
Student body funds (totaling roughly $370,000 annually) are distributed each semester to groups that place among the top 40 organizations in the semester's funding poll. The funding poll uses a voting system in which each organization provides a description that is ranked by each member of the student body with either 'top six,' 'approve,' 'no opinion,' 'disapprove,' or 'deep six.' These ranks are then tabulated by assigning numbers to each rank and summing across all voters. Afterwards, the top forty organizations present their budgets to the student body senate during Funding Circus. The following day the senate makes decisions about each budget in a process called Funding Hell.

The school's student-run newspaper, "The Reed College Quest "or simply the "Quest," has been published since 1913, and its radio station, KRRC has been broadcasting, with a few interruptions, since 1955.

Although some that partner with outside groups such as Oxfam or Planned Parenthood are more structured, most organizations are highly informal. There is no formal process for forming a student organization at Reed; a group of students (or a single student) announcing themselves as or just considering themselves a student organization is enough. Groups that want funding from the school's Student Activities office or Student Body Fees, however, must register with Student Activities or through the Student Senate. The Reed archive of comic books and graphic novels, the MLLL (Comic Book Reading Room), is well into its fourth decade, and Beer Nation, the student group that organizes and manages various beer gardens throughout the year and during Renn Fayre, has existed for many years. Some organizations, such as the Motorized Couch Collective – dedicated to installing motors and wheels into furniture – have become more Reed myth than reality in recent years.

Reed has ample recreational facilities on campus, a ski cabin on Mount Hood, recreational clubs such as the Reed Outing Club (ROC), and Club Sports (with college-paid coaches), including ultimate frisbee, co-ed soccer, rugby, basketball, and squash.

According to a "Washington Post" analysis of federal campus safety data from 2014, Reed College had 12.9 reports of rape per 1,000 students, the "highest total of reports of rape" per 1,000 students of any college in the nation on its main campus.

In 2012, Reed College had the third highest reported sexual assault rate among U.S. colleges and universities. It is unclear whether this high reporting rate arises from an environment that is more supportive of reporting by crime victims or due to a higher underlying rate of sexual assault. in 2013 there were 19 reported forcible sexual offenses among the approximately 1,400 students at the college. In 2011 a student member of Reed's Judicial Board resigned over the college's handling of sexual assault cases. An investigation by the Center for Public Integrity found that those found responsible in cases of sexual assault frequently faced few consequences, while the lives of the victims were left in turmoil.

Notable Reed alumni include Tektronix co-founder Howard Vollum (1936), businessman John Sperling (1948), Pulitzer Prize-winning poet Gary Snyder (1951), fantasy author David Eddings (1954), distance learning pioneer John Bear (1959), socialist and feminist activist and author Barbara Ehrenreich (1963), radio personality Dr. Demento (1963), programmer, software publisher, author, and philanthropist Peter Norton (1965), former U.S. Secretary of the Navy Richard Danzig (1965), alpinist and biophysical chemist Arlene Blum (1966), chemist Mary Jo Ondrechen (1974), computer engineer Daniel Kottke (1976), and Wikipedia co-founder Larry Sanger (1991).

Among those who attended but did not graduate from Reed are Academy Award-nominated actress Hope Lange, chef James Beard, and Apple co-founder and CEO Steve Jobs.

Notable Reed faculty of the past and present include former U.S. Senator from Illinois Paul Douglas, and physicists Richard Crandall and David Griffiths.



</doc>
<doc id="25418" url="https://en.wikipedia.org/wiki?curid=25418" title="Proof by contradiction">
Proof by contradiction

In logic and mathematics, proof by contradiction is a form of proof that establishes the truth or the validity of a proposition, by showing that assuming the proposition to be false leads to a contradiction. Proof by contradiction is also known as indirect proof, proof by assuming the opposite, and reductio ad impossibile. It is a particular kind of the more general form of argument known as "reductio ad absurdum".

G. H. Hardy described proof by contradiction as "one of a mathematician's finest weapons", saying "It is a far finer gambit than any chess gambit: a chess player may offer the sacrifice of a pawn or even a piece, but a mathematician offers the game."

Proof by contradiction is based on the law of noncontradiction as first formalized as a metaphysical principle by Aristotle. Noncontradiction is also a theorem in propositional logic. This states that an assertion or mathematical statement cannot be both true and false. That is, a proposition "Q" and its negation formula_1"Q" ("not-"Q"") cannot both be true. In a proof by contradiction, it is shown that the denial of the statement being proved results in such a contradiction. It has the form of a "reductio ad absurdum" argument, and usually proceeds as follows:


An alternate form of proof by contradiction derives a contradiction with the statement to be proved itself by showing
that formula_1"P" implies "P". Thus formula_1"P" must be false, which implies that "P" must be true.

An existence proof by contradiction assumes that some object doesn't exist, and then proves that this would lead to a contradiction; thus, such an object must exist. Although it is quite freely used in mathematical proofs, not every school of mathematical thought accepts this kind of nonconstructive proof as universally valid.

Proof by contradiction also depends on the law of the excluded middle, also first formulated by Aristotle. This states that either an assertion or its negation must be true
That is, there is no other truth value besides "true" and "false" that a proposition can take. Combined with the principle of noncontradiction, this means that exactly one of formula_9 and formula_10 is true. In proof by contradiction, this permits the conclusion that since the possibility of formula_10 has been excluded, formula_9 must be true.

The law of the excluded middle is accepted in virtually all formal logics; however, some intuitionist mathematicians do not accept it, and thus reject proof by contradiction as a viable proof technique.

Proof by contradiction is closely related to proof by contrapositive, and the two are sometimes confused, though they are distinct methods. The main distinction is that a proof by contrapositive applies only to statements formula_9 that can be written in the form formula_14 (i.e., implications), whereas the technique of proof by contradiction applies to statements formula_9 of any form:

In the case where the statement to be proven "is" an implication formula_19, then the differences between direct proof, proof by contrapositive, and proof by contradiction can be outlined as follows:

A classic proof by contradiction from mathematics is the proof that the square root of 2 is irrational. If it were rational, it would be expressible as a fraction "a"/"b" in lowest terms, where "a" and "b" are integers, at least one of which is odd. But if "a"/"b" = , then "a" = 2"b". Therefore, "a" must be even, and because the square of an odd number is odd, that in turn implies that "a" is itself even — which means that "b" must be odd because a/b is in lowest terms.

On the other hand, if "a" is even, then "a" is a multiple of 4. If "a" is a multiple of 4 and "a" = 2"b", then 2"b" is a multiple of 4, and therefore "b" must be even, which means that so is "b" too.

So "b" is both odd and even, a contradiction. Therefore, the initial assumption—that can be expressed as a fraction—must be false.

The method of proof by contradiction has also been used to show that for any non-degenerate right triangle, the length of the hypotenuse is less than the sum of the lengths of the two remaining sides. By letting "c" be the length of the hypotenuse and "a" and "b" be the lengths of the legs, one can also express the claim more succinctly as "a" + "b" > "c". In which case, a proof by contradiction can then be made by appealing to the Pythagorean theorem. 

First, the claim is negated to assume that "a" + "b" ≤ "c". In which case, squaring both sides would yield that ("a" + "b") ≤ "c", or equivalently, "a" + 2"ab" + "b" ≤ "c". A triangle is non-degenerate if each of its edges has positive length, so it may be assumed that both "a" and "b" are greater than 0. Therefore, "a" + "b" < "a" + 2"ab" + "b" ≤ "c", and the transitive relation may be reduced further to "a" + "b" < "c".

On the other hand, it is also known from the Pythagorean theorem that "a" + "b" = "c". This would result in a contradiction since strict inequality and equality are mutually exclusive. The contradiction means that it is impossible for both to be true and it is known that the Pythagorean theorem holds. It follows from there that the assumption "a" + "b" ≤ "c" must be false and hence "a" + "b" > "c", proving the claim.

Consider the proposition, "P": "there is no smallest rational number greater than 0". In a proof by contradiction, we start by assuming the opposite, ¬"P": that there "is" a smallest rational number, say, "r".

Now, "r"/2 is a rational number greater than 0 and smaller than "r". But that contradicts the assumption that "r" was the "smallest" rational number (if ""r" is the smallest rational number" were "Q, then" one can infer from ""r"/2 is a rational number smaller than "r"" that ¬"Q".) This contradictions shows that the original proposition, "P", must be true. That is, that "there is no smallest rational number greater than 0".
For other examples, see proof that the square root of 2 is not rational (where indirect proofs different from the one above can be found) and Cantor's diagonal argument.

Proofs by contradiction sometimes end with the word "Contradiction!". Isaac Barrow and Baermann used the notation Q.E.A., for ""quod est absurdum"" ("which is absurd"), along the lines of Q.E.D., but this notation is rarely used today. A graphical symbol sometimes used for contradictions is a downwards zigzag arrow "lightning" symbol (U+21AF: ↯), for example in Davey and Priestley. Others sometimes used include a pair of opposing arrows (as formula_28 or formula_29), struck-out arrows (formula_30), a stylized form of hash (such as U+2A33: ⨳), or the "reference mark" (U+203B: ※). The "up tack" symbol (U+22A5: ⊥) used by philosophers and logicians (see contradiction) also appears, but is often avoided due to its usage for orthogonality.

A curious logical consequence of the principle of non-contradiction is that a contradiction implies any statement; if a contradiction is accepted as true, any proposition (including its negation) can be proved from it. This is known as the principle of explosion (, "from a falsehood, anything [follows]", or "", "from a contradiction, anything follows"), or the principle of pseudo-scotus.
Thus a contradiction in a formal axiomatic system is disastrous; since any theorem can be proven true, it destroys the conventional meaning of truth and falsity.

The discovery of contradictions at the foundations of mathematics at the beginning of the 20th century, such as Russell's paradox, threatened the entire structure of mathematics due to the principle of explosion. This motivated a great deal of work during the 20th century to create consistent axiomatic systems to provide a logical underpinning for mathematics. This has also led a few philosophers such as Newton da Costa, Walter Carnielli and Graham Priest to reject the principle of non-contradiction, giving rise to theories such as paraconsistent logic and dialethism, which accepts that there exist statements that are both true and false.




</doc>
<doc id="25420" url="https://en.wikipedia.org/wiki?curid=25420" title="Reversible error">
Reversible error

In United States law, a reversible error is an error of sufficient gravity to warrant reversal of a judgment on appeal. It is an error by the trier of law (judge), or the trier of fact (the jury, or the judge if it is a bench trial), or malfeasance by one of the trying attorneys, which results in an unfair trial. It is to be distinguished from harmless errors which do not rise to a level which brings the validity of the judgment into question and thus do not lead to a reversal upon appeal.

A finding of reversible error requires that one or more of the appellant's "substantial rights" be affected, or the evidence in question be of such character as to have affected the outcome of the trial. (See e.g., "Montana Petroleum Tank Release Compensation Bd. v. Crumley's, Inc.", 174 P.3d 948 (Mont. 2008).) The criteria for determining what constitutes a "substantial right" is somewhat vague however, being that it varies from case to case, each presenting a slightly different interpretation of which rights are essential, or significant enough to warrant this sort of legal protection. Therefore, reversible errors resulting from the violation of an individual's "substantial right(s)" must be considered on an individual basis.

Reversible errors include, but are not limited to:

If an appellate court determines that reversible error occurred, it may reverse the judgment of the lower court and order a new trial on such terms and conditions as are found to be just.

Technically, attorney misconduct is not reversible error. Failure of the judge to remedy it during the trial is reversible error. In cases such as unfairly or illegally concealing evidence, there is no error on the part of the court but the court's decision may still be vacated and the matter returned for a new trial, because there is no other way for justice to be granted.


</doc>
<doc id="25421" url="https://en.wikipedia.org/wiki?curid=25421" title="Rapping">
Rapping

Rapping (or rhyming, spitting, emceeing, MCing) is a musical form of vocal delivery that incorporates "rhyme, rhythmic speech, and street vernacular", which is performed or chanted in a variety of ways, usually over a backing beat or musical accompaniment. The components of rap include "content" (what is being said), "flow" (rhythm, rhyme), and "delivery" (cadence, tone). Rap differs from spoken-word poetry in that it is usually performed in time to musical accompaniment. Rap being a primary ingredient of hip hop music, it is commonly associated with that genre in particular; however, the origins of rap precede hip-hop culture. The earliest precursor to modern rap is the West African griot tradition, in which "oral historians", or "praise-singers", would disseminate oral traditions and genealogies, or use their rhetorical techniques for gossip or to "praise or critique individuals." Griot traditions connect to rap along a lineage of black verbal reverence, through James Brown interacting with the crowd and the band between songs, to Muhammad Ali's verbal taunts and the poems of The Last Poets. Therefore, rap lyrics and music are part of the "Black rhetorical continuum", and aim to reuse elements of past traditions while expanding upon them through "creative use of language and rhetorical styles and strategies". The person credited with originating the style of "delivering rhymes over extensive music", that would become known as rap, was Anthony "DJ Hollywood" Holloway from Harlem, New York.

Rap is usually delivered over a beat, typically provided by a DJ, turntablist, beatboxer, or performed a cappella without accompaniment. Stylistically, rap occupies a gray area between speech, prose, poetry, and singing. The word, which predates the musical form, originally meant "to lightly strike", and is now used to describe quick speech or repartee. The word had been used in British English since the 16th century. It was part of the African American dialect of English in the 1960s meaning "to converse", and very soon after that in its present usage as a term denoting the musical style. Today, the term rap is so closely associated with hip-hop music that many writers use the terms interchangeably.

The English verb "rap" has various meanings, these include "to strike, especially with a quick, smart, or light blow", as well "to utter sharply or vigorously: to rap out a command". The "Shorter Oxford English Dictionary" gives a date of 1541 for the first recorded use of the word with the meaning "to utter (esp. an oath) sharply, vigorously, or suddenly". Wentworth and Flexner's "Dictionary of American Slang" gives the meaning "to speak to, recognize, or acknowledge acquaintance with someone", dated 1932, and a later meaning of "to converse, esp. in an open and frank manner". It is these meanings from which the musical form of "rapping" derives, and this definition may be from a shortening of repartee. A "rapper" refers to a performer who "raps". By the late 1960s, when Hubert G. Brown changed his name to H. Rap Brown, "rap" was a slang term referring to an oration or speech, such as was common among the "hip" crowd in the protest movements, but it did not come to be associated with a musical style for another decade.

"Rap" was used to describe talking on records as early as 1971, on Isaac Hayes' album "Black Moses" with track names such as "Ike's Rap", "Ike's Rap II", "Ike's Rap III", and so on. Hayes' "husky-voiced sexy spoken 'raps' became key components in his signature sound".
Del the Funky Homosapien similarly states that "rap" was used to refer to talking in a stylistic manner in the early 1970s: "I was born in '72 ... back then what rapping meant, basically, was you trying to convey something—you're trying to convince somebody. That's what rapping is, it's in the way you talk."

Rapping can be traced back to its African roots. Centuries before hip-hop music existed, the griots of West Africa were delivering stories rhythmically, over drums and sparse instrumentation. Such connections have been acknowledged by many modern artists, modern day "griots", spoken word artists, mainstream news sources, and academics.

Blues music, rooted in the work songs and spirituals of slavery and influenced greatly by West African musical traditions, was first played by blacks, and later by some whites, in the Mississippi Delta region of the United States around the time of the Emancipation Proclamation. Grammy-winning blues musician/historian Elijah Wald and others have argued that the blues were being rapped as early as the 1920s. Wald went so far as to call hip hop "the living blues." A notable recorded example of rapping in blues music was the 1950 song "Gotta Let You Go" by Joe Hill Louis.

Jazz, which developed from the blues and other African-American and European musical traditions and originated around the beginning of the 20th century, has also influenced hip hop and has been cited as a precursor of hip hop. Not just jazz music and lyrics but also jazz poetry. According to John Sobol, the jazz musician and poet who wrote "Digitopia Blues", rap "bears a striking resemblance to the evolution of jazz both stylistically and formally". Boxer Muhammad Ali anticipated elements of rap, often using rhyme schemes and spoken word poetry, both for when he was trash talking in boxing and as political poetry for his activism outside of boxing, paving the way for The Last Poets in 1968, Gil Scott-Heron in 1970, and the emergence of rap music in the 1970s.

Precursors also exist in non-African/African-American traditions, especially in vaudeville and musical theater. One such tradition is the patter song exemplified by Gilbert and Sullivan but that has origins in earlier Italian opera. "Rock Island" from Meridith Wilson's "The Music Man" is wholly spoken by an ensemble of travelling salesmen, as are most of the numbers for British actor Rex Harrison in the 1964 Lerner and Loewe musical My Fair Lady. Glenn Miller's "The Lady's in Love with You" and "The Little Man Who Wasn't There" (both 1939), each contain distinctly rap-like sequences set to a driving beat as does the 1937 song "Doin' the Jive". In musical theater, the term "vamp" is identical to its meaning in jazz, gospel, and funk, and it fulfills the same function. Semi-spoken music has long been especially popular in British entertainment, and such examples as David Croft's theme to the 1970s' sitcom "Are You Being Served?" have elements indistinguishable from modern rap.

In classical music, semi-spoken music was popular stylized by composer Arnold Schoenberg as Sprechstimme, and famously used in Ernst Toch's 1924 "Geographical Fugue" for spoken chorus and the final scene in Darius Milhaud's 1915 ballet "Les Choéphores". In the French chanson field, irrigated by a strong poetry tradition, such singer-songwriters as Léo Ferré or Serge Gainsbourg made their own use of spoken word over rock or symphonic music from the very beginning of the 1970s. Although these probably did not have a direct influence on rap's development in the African-American cultural sphere, they paved the way for acceptance of spoken word music in the media market.

With the decline of disco in the early 1980s rap became a new form of expression. Rap arose from musical experimentation with rhyming, rhythmic speech. Rap was a departure from disco. Sherley Anne Williams refers to the development of rap as "anti-Disco" in style and means of reproduction. The early productions of Rap after Disco sought a more simplified manner of producing the tracks they were to sing over. Williams explains how Rap composers and DJ's opposed the heavily orchestrated and ritzy multi-tracks of Disco for "break beats" which were created from compiling different records from numerous genres and did not require the equipment from professional recording studios. Professional studios were not necessary therefore opening the production of rap to the youth who as Williams explains felt "locked out" because of the capital needed to produce Disco records.

More directly related to the African-American community were items like schoolyard chants and taunts, clapping games, jump-rope rhymes, some with unwritten folk histories going back hundreds of years across many nationalities. Sometimes these items contain racially offensive lyrics. A related area that is not strictly folklore is rhythmical cheering and cheerleading for military and sports.

Art forms such as spoken word jazz poetry and comedy records had an influence on the first rappers. Coke La Rock, often credited as hip-hop's first MC cites the Last Poets among his influences, as well as comedians such as Wild Man Steve and Richard Pryor. Comedian Rudy Ray Moore released under the counter albums in the 1960s and 1970s such as "This Pussy Belongs To Me" (1970), which contained "raunchy, sexually explicit rhymes that often had to do with pimps, prostitutes, players, and hustlers", and which later led to him being called "The Godfather of Rap".

Gil Scott-Heron, a jazz poet/musician, has been cited as an influence on rappers such as Chuck D and KRS-One. Scott-Heron himself was influenced by Melvin Van Peebles, whose first album was 1968's "Brer Soul". Van Peebles describes his vocal style as "the old Southern style", which was influenced by singers he had heard growing up in South Chicago. Van Peebles also said that he was influenced by older forms of African-American music: "... people like Blind Lemon Jefferson and the field hollers. I was also influenced by spoken word song styles from Germany that I encountered when I lived in France."

During the mid-20th century, the musical culture of the Caribbean was constantly influenced by the concurrent changes in American music. As early as 1956, deejays were toasting (an African tradition of "rapped out" tales of heroism) over dubbed Jamaican beats. It was called "rap", expanding the word's earlier meaning in the African-American community—"to discuss or debate informally."

The early rapping of hip-hop developed out of DJ and Master of Ceremonies' announcements made over the microphone at parties, and later into more complex raps. Grandmaster Caz states: "The microphone was just used for making announcements, like when the next party was gonna be, or people's moms would come to the party looking for them, and you have to announce it on the mic. Different DJs started embellishing what they were saying. I would make an announcement this way, and somebody would hear that and they add a little bit to it. I'd hear it again and take it a little step further 'til it turned from lines to sentences to paragraphs to verses to rhymes."

One of the first rappers at the beginning of the hip hop period, at the end of the 1970s, was also hip hop's first DJ, DJ Kool Herc. Herc, a Jamaican immigrant, started delivering simple raps at his parties, which some claim were inspired by the Jamaican tradition of toasting. However, Kool Herc himself denies this link (in the 1984 book "Hip Hop"), saying, "Jamaican toasting? Naw, naw. No connection there. I couldn't play reggae in the Bronx. People wouldn't accept it. The inspiration for rap is James Brown and the album "Hustler's Convention"". Herc also suggests he was too young while in Jamaica to get into sound system parties: "I couldn't get in. Couldn't get in. I was ten, eleven years old," and that while in Jamaica, he was listening to James Brown: "I was listening to American music in Jamaica and my favorite artist was James Brown. That's who inspired me. A lot of the records I played were by James Brown."

However, in terms of what we identify in the 2010s as "rap" the source came from Manhattan. Pete DJ Jones said the first person he heard rap was DJ Hollywood, a Harlem (not Bronx) native who was the house DJ at the Apollo Theater. Kurtis Blow also says the first person he heard rhyme was DJ Hollywood. In a 2014 interview, Hollywood said: "I used to like the way Frankie Crocker would ride a track, but he wasn't syncopated to the track though. I liked [WWRL DJ] Hank Spann too, but he wasn't on the one. Guys back then weren't concerned with being musical. I wanted to flow with the record". And in 1975, he ushered in what became known as the Hip Hop style by rhyming syncopated to the beat of an existing record uninterruptedly for nearly a minute. He adapted the lyrics of Isaac Hayes "Good Love 6-9969" and rhymed it to the breakdown part of "Love is the Message". His partner Kevin Smith, better known as Lovebug Starski, took this new style and introduced it to the Bronx Hip Hop set that until then was composed of DJing and B-boying, with traditional "shout out" style rapping.

The style that Hollywood created and his partner introduced to the Hip Hop set quickly became the standard. What actually did Hollywood do? He created "flow." Before then all MCs rhymed based on radio DJs. This usually consisted of short patters that were disconnected thematically; they were separate unto themselves. But by Hollywood using song lyrics, he had an inherent flow and theme to his rhyme. This was the game changer. By the end of the 1970s, artists such as Kurtis Blow and The Sugarhill Gang were just starting to receive radio airplay and make an impact far outside of New York City, on a national scale. Blondie's 1981 single, "Rapture", was one of the first songs featuring rap to top the U.S. "Billboard" Hot 100 chart.

Old school rap (1979–84) was "easily identified by its relatively simple raps" according to AllMusic, "the emphasis was not on lyrical technique, but simply on good times", one notable exception being Melle Mel, who set the way for future rappers through his socio-political content and creative wordplay.

Golden age hip hop (the mid-1980s to early '90s) was the time period where hip-hop lyricism went through its most drastic transformation – writer William Jelani Cobb says "in these golden years, a critical mass of mic prodigies were literally creating themselves and their art form at the same time" and Allmusic writes, "rhymers like PE's Chuck D, Big Daddy Kane, KRS-One, and Rakim basically invented the complex wordplay and lyrical kung-fu of later hip-hop". The golden age is considered to have ended around 1993–94, marking the end of rap lyricism's most innovative period.

"Flow" is defined as "the rhythms and rhymes" of a hip-hop song's lyrics and how they interact – the book "How to Rap" breaks flow down into rhyme, rhyme schemes, and rhythm (also known as cadence). 'Flow' is also sometimes used to refer to elements of the delivery (pitch, timbre, volume) as well, though often a distinction is made between the flow and the delivery.

Staying on the beat is central to rap's flow – many MCs note the importance of staying on-beat in "How to Rap" including Sean Price, Mighty Casey, Zion I, Vinnie Paz, Fredro Starr, Del The Funky Homosapien, Tech N9ne, People Under The Stairs, Twista, B-Real, Mr Lif, 2Mex, and Cage.

MCs stay on beat by stressing syllables in time to the four beats of the musical backdrop. Poetry scholar Derek Attridge describes how this works in his book "Poetic Rhythm" – "rap lyrics are written to be performed to an accompaniment that emphasizes the metrical structure of the verse". He says rap lyrics are made up of, "lines with four stressed beats, separated by other syllables that may vary in number and may include other stressed syllables. The strong beat of the accompaniment coincides with the stressed beats of the verse, and the rapper organizes the rhythms of the intervening syllables to provide variety and surprise".

The same technique is also noted in the book "How to Rap", where diagrams are used to show how the lyrics line up with the beat – "stressing a syllable on each of the four beats gives the lyrics the same underlying rhythmic pulse as the music and keeps them in rhythm ... other syllables in the song may still be stressed, but the ones that fall in time with the four beats of a bar are the only ones that need to be emphasized in order to keep the lyrics in time with the music".

In rap terminology, 16-bars is the amount of time that rappers are generally given to perform a guest verse on another artist's song; one bar is typically equal to four beats of music.

Old school flows were relatively basic and used only few syllables per bar, simple rhythmic patterns, and basic rhyming techniques and rhyme schemes.
Melle Mel is cited as an MC who epitomizes the old school flow – Kool Moe Dee says, "from 1970 to 1978 we rhymed one way [then] Melle Mel, in 1978, gave us the new cadence we would use from 1978 to 1986". He's the first emcee to explode in a new rhyme cadence, and change the way every emcee rhymed forever. Rakim, The Notorious B.I.G., and Eminem have flipped the flow, but Melle Mel's downbeat on the two, four, kick to snare cadence is still the rhyme foundation all emcees are building on".

Artists and critics often credit Rakim with creating the overall shift from the more simplistic old school flows to more complex flows near the beginning of hip hop's new school – Kool Moe Dee says, "any emcee that came after 1986 had to study Rakim just to know what to be able to do. Rakim, in 1986, gave us flow and that was the rhyme style from 1986 to 1994. from that point on, anybody emceeing was forced to focus on their flow". Kool Moe Dee explains that before Rakim, the term 'flow' wasn't widely used – "Rakim is basically the inventor of flow. We were not even using the word flow until Rakim came along. It was called rhyming, it was called cadence, but it wasn't called flow. Rakim created flow!" He adds that while Rakim upgraded and popularized the focus on flow, "he didn't invent the word".

Kool Moe Dee states that Biggie introduced a newer flow which "dominated from 1994 to 2002", and also says that Method Man was "one of the emcees from the early to mid-'90s that ushered in the era of flow ... Rakim invented it, Big Daddy Kane, KRS-One, and Kool G Rap expanded it, but Biggie and Method Man made flow the single most important aspect of an emcee's game". He also cites Craig Mack as an artist who contributed to developing flow in the '90s.

Music scholar Adam Krims says, "the flow of MCs is one of the profoundest changes that separates out new-sounding from older-sounding music ... it is widely recognized and remarked that rhythmic styles of many commercially successful MCs since roughly the beginning of the 1990s have progressively become faster and more 'complex'". He cites "members of the Wu-Tang Clan, Nas, AZ, Big Pun, and Ras Kass, just to name a few" as artists who exemplify this progression.

Kool Moe Dee adds, "in 2002 Eminem created the song that got the first Oscar in Hip-Hop history [Lose Yourself] ... and I would have to say that his flow is the most dominant right now (2003)".

There are many different styles of flow, with different terminology used by different people – stic.man of Dead Prez uses the following terms –

Alternatively, music scholar Adam Krims uses the following terms –

MCs use many different rhyming techniques, including complex rhyme schemes, as Adam Krims points out – "the complexity ... involves multiple rhymes in the same rhyme complex (i.e. section with consistently rhyming words), internal rhymes, [and] offbeat rhymes". There is also widespread use of multisyllabic rhymes, by artists such as Kool G Rap, Big Daddy Kane, Rakim, Big L, Nas and Eminem.

It has been noted that rap's use of rhyme is some of the most advanced in all forms of poetry – music scholar Adam Bradley notes, "rap rhymes so much and with such variety that it is now the largest and richest contemporary archive of rhymed words. It has done more than any other art form in recent history to expand rhyme's formal range and expressive possibilities".

In the book "How to Rap", Masta Ace explains how Rakim and Big Daddy Kane caused a shift in the way MCs rhymed: "Up until Rakim, everybody who you heard rhyme, the last word in the sentence was the rhyming [word], the connection word. Then Rakim showed us that you could put rhymes within a rhyme ... now here comes Big Daddy Kane — instead of going three words, he's going multiple". "How to Rap" explains that "rhyme is often thought to be the most important factor in rap writing ... rhyme is what gives rap lyrics their musicality.

Many of the rhythmic techniques used in rapping come from percussive techniques and many rappers compare themselves to percussionists. "How to Rap 2" identifies all the rhythmic techniques used in rapping such as triplets, flams, 16th notes, 32nd notes, syncopation, extensive use of rests, and rhythmic techniques unique to rapping such as West Coast "lazy tails", coined by Shock G. Rapping has also been done in various time signatures, such as 3/4 time.

Since the 2000s, rapping has evolved into a style of rap that spills over the boundaries of the beat, closely resembling spoken English. Rappers like MF Doom and Eminem have exhibited this style, and since then, rapping has been difficult to notate. The American hip-hop group Crime Mob exhibited a new rap flow in songs such as "Knuck If You Buck", heavily dependent on triplets. Rappers including Drake, Kanye West, Rick Ross, Young Jeezy and more have included this influence in their music. In 2014, an American hip-hop collective from Atlanta, Migos, popularized this flow, and is commonly referred to as the "Migos Flow" (a term that is contentious within the hip-hop community).

The standard form of rap notation is the flow diagram, where rappers line-up their lyrics underneath "beat numbers". Different rappers have slightly different forms of flow diagram that they use: Del the Funky Homosapien says, "I'm just writing out the rhythm of the flow, basically. Even if it's just slashes to represent the beats, that's enough to give me a visual path.", Vinnie Paz states, "I've created my own sort of writing technique, like little marks and asterisks to show like a pause or emphasis on words in certain places.", and Aesop Rock says, "I have a system of maybe 10 little symbols that I use on paper that tell me to do something when I'm recording."

Hip-hop scholars also make use of the same flow diagrams: the books "How to Rap" and "How to Rap 2" use the diagrams to explain rap's triplets, flams, rests, rhyme schemes, runs of rhyme, and breaking rhyme patterns, among other techniques. Similar systems are used by PhD musicologists Adam Krims in his book "Rap Music and the Poetics of Identity" and Kyle Adams in his academic work on flow.

Because rap revolves around a strong 4/4 beat, with certain syllables said in time to the beat, all the notational systems have a similar structure: they all have the same 4 beat numbers at the top of the diagram, so that syllables can be written in-line with the beat numbers. This allows devices such as rests, "lazy tails", flams, and other rhythmic techniques to be shown, as well as illustrating where different rhyming words fall in relation to the music.

To successfully deliver a rap, a rapper must also develop vocal presence, enunciation, and breath control. Vocal presence is the distinctiveness of a rapper's voice on record. Enunciation is essential to a flowing rap; some rappers choose also to exaggerate it for comic and artistic effect. Breath control, taking in air without interrupting one's delivery, is an important skill for a rapper to master, and a must for any MC. An MC with poor breath control cannot deliver difficult verses without making unintentional pauses.

Raps are sometimes delivered with melody. West Coast rapper Egyptian Lover was the first notable MC to deliver "sing-raps". Popular rappers such as 50 Cent and Ja Rule add a slight melody to their otherwise purely percussive raps whereas some rappers such as Cee-Lo Green are able to harmonize their raps with the beat. The Midwestern group Bone Thugs-n-Harmony was one of the first groups to achieve nationwide recognition for using the fast-paced, melodic and harmonic raps that are also practiced by Do or Die, another Midwestern group. Another rapper that harmonized his rhymes was Nate Dogg, a rapper part of the group 213. Rakim experimented not only with following the beat, but also with complementing the song's melody with his own voice, making his flow sound like that of an instrument (a saxophone in particular).

The ability to rap quickly and clearly is sometimes regarded as an important sign of skill. In certain hip-hop subgenres such as chopped and screwed, slow-paced rapping is often considered optimal. The current record for fastest rapper is held by Spanish rapper Domingo Edjang Moreno, known by his alias Chojin, who rapped 921 syllables in one minute on December 23, 2008.

In the late 1970s, the term Emcee, MC or M.C. became an alternative title for a rapper, and for their role within hip-hop music and culture. An MC uses rhyming verses, pre-written or ad lib ('freestyled'), to introduce the DJ with whom they work, to keep the crowd entertained or to glorify themselves. As hip hop progressed, the title MC acquired backronyms such as 'mike chanter' 'microphone controller', 'microphone checker', 'music commentator', and one who 'moves the crowd'. A recent neologistic acronym, gaining use, is 'mentor to child'. Some use this word interchangeably with the term "rapper", while for others the term denotes a superior level of skill and connection to the wider culture.

MC can often be used as a term of distinction; referring to an artist with good performance skills. As Kool G Rap notes, "masters of ceremony, where the word 'M.C.' comes from, means just keeping the party alive" Sic. Many people in hiphop including DJ Premier and KRS-One feel that James Brown was the first MC. James Brown had the lyrics, moves, and soul that greatly influenced a lot of rappers in Hip-Hop, and arguably even started the first MC rhyme.

For some rappers, there was a distinction to the term, such as for MC Hammer who acquired the nickname "MC" for being a "Master of Ceremonies" which he used when he began performing at various clubs while on the road with the Oakland As and eventually in the military (United States Navy). It was within the lyrics of a rap song called "This Wall" that Hammer first identified himself as M.C. Hammer and later marketed it on his debut album "Feel My Power".

Uncertainty over the acronym's expansion may be considered evidence for its ubiquity: the full term "Master of Ceremonies" is very rarely used in the hip-hop scene. This confusion prompted the hip-hop group A Tribe Called Quest to include this statement in the liner notes to their 1993 album "Midnight Marauders:

The use of the term MC when referring to a rhyming wordsmith originates from the dance halls of Jamaica. At each event, there would be a master of ceremonies who would introduce the different musical acts and would say a toast in style of a rhyme, directed at the audience and to the performers. He would also make announcements such as the schedule of other events or advertisements from local sponsors. The term MC continued to be used by the children of women who moved to New York City to work as maids in the 1970s. These MCs eventually created a new style of music called hip-hop based on the rhyming they used to do in Jamaica and the breakbeats used in records. MC has also recently been accepted to refer to all who engineer music.
"Party rhymes", meant to pump up the crowd at a party, were nearly the exclusive focus of old school hip hop, and they remain a staple of hip-hop music to this day. In addition to party raps, rappers also tend to make references to love and sex. Love raps were first popularized by Spoonie Gee of the Treacherous Three, and later, in the golden age of hip hop, Big Daddy Kane, Heavy D, and LL Cool J would continue this tradition.
Hip-hop artists such as KRS-One, Hopsin, Public Enemy, Lupe Fiasco, Mos Def, Talib Kweli, Jay-Z, Nas, The Notorious B.I.G. (Biggie), and dead prez are known for their sociopolitical subject matter. Their West Coast counterparts include Emcee Lynx, The Coup, Paris, and Michael Franti. Tupac Shakur was also known for rapping about social issues such as police brutality, teenage pregnancy, and racism.

Other rappers take a less critical approach to urbanity, sometimes even embracing such aspects as crime. Schoolly D was the first notable MC to rap about crime. Early on KRS-One was accused of celebrating crime and a hedonistic lifestyle, but after the death of his DJ, Scott La Rock, KRS-One went on to speak out against violence in hip hop and has spent the majority of his career condemning violence and writing on issues of race and class. Ice-T was one of the first rappers to call himself a "playa" and discuss guns on record, but his theme tune to the 1988 film "Colors" contained warnings against joining gangs. Gangsta rap, made popular largely because of N.W.A, brought rapping about crime and the gangster lifestyle into the musical mainstream.

Materialism has also been a popular topic in hip-hop since at least the early 1990s, with rappers boasting about their own wealth and possessions, and name-dropping specific brands: liquor brands Cristal and Rémy Martin, car manufacturers Bentley and Mercedes-Benz and clothing brands Gucci and Versace have all been popular subjects for rappers.

Various politicians, journalists, and religious leaders have accused rappers of fostering a culture of violence and hedonism among hip-hop listeners through their lyrics. However, there are also rappers whose messages may not be in conflict with these views, for example Christian hip hop. Others have praised the "political critique, innuendo and sarcasm" of hip-hop music.

In contrast to the more hedonistic approach of gangsta rappers, some rappers have a spiritual or religious focus. Christian rap is currently the most commercially successful form of religious rap. With Christian rappers like Lecrae, Thi'sl and Hostyle Gospel winning national awards and making regular appearances on television, Christian hip hop seem to have found its way in the hip-hop family. Aside from Christianity, the Five Percent Nation, an Islamic esotericist religious/spiritual group, has been represented more than any religious group in popular hip hop. Artists such as Rakim, the members of the Wu-Tang Clan, Brand Nubian, X-Clan and Busta Rhymes have had success in spreading the theology of the Five Percenters.

Rappers use the literary techniques of double entendres, alliteration, and forms of wordplay that are found in classical poetry. Similes and metaphors are used extensively in rap lyrics; rappers such as Fabolous and Lloyd Banks have written entire songs in which every line contains similes, whereas MCs like Rakim, GZA, and Jay-Z are known for the metaphorical content of their raps. Rappers such as Lupe Fiasco are known for the complexity of their songs that contain metaphors within extended metaphors.

Many hip-hop listeners believe that a rapper's lyrics are enhanced by a complex vocabulary. Kool Moe Dee claims that he appealed to older audiences by using a complex vocabulary in his raps. Rap is famous, however, for having its own vocabulary—from international hip-hop slang to regional slang. Some artists, like the Wu-Tang Clan, develop an entire lexicon among their clique. African-American English has always had a significant effect on hip-hop slang and vice versa. Certain regions have introduced their unique regional slang to hip-hop culture, such as the Bay Area (Mac Dre, E-40), Houston (Chamillionaire, Paul Wall), Atlanta (Ludacris, Lil Jon, T.I.), and Kentucky (Nappy Roots). The Nation of Gods and Earths, aka The Five Percenters, has influenced mainstream hip-hop slang with the introduction of phrases such as "word is bond" that have since lost much of their original spiritual meaning. Preference toward one or the other has much to do with the individual; GZA, for example, prides himself on being very visual and metaphorical but also succinct, whereas underground rapper MF DOOM is known for heaping similes upon similes. In still another variation, 2Pac was known for saying exactly what he meant, literally and clearly.

Rap music's development into popular culture in the 1990s can be accredited to the album "Niggaz4life" by artists Niggaz With Attitude, the first rap group to ever take the top spot of the Billboard's Top 200 in 1991, in the United States.   With this victory, came the beginning of an era of popular culture guided by the musical influences of hip-hop and rap itself, moving away from the influences of rock music. As rap continued to develop and further disseminate, it went on to influence clothing brands, movies, sports, and dancing through popular culture. As rap has developed to become more of a presence in popular culture, it has focused itself on a particular demographic, adolescent and young adults. As such, it has had a significant impact on the modern vernacular of this portion of the population, which has diffused throughout society.

The effects of rap music on modern vernacular can be explored through the study of semiotics. Semiotics is the study of signs and symbols, or the study of language as a system. French literary theorist Roland Barthes furthers this study with this own theory of myth. He maintains that the first order of signification is language and that the second is "myth", arguing that a word has both its literal meaning, and its mythical meaning, which is heavily dependent on socio-cultural context. To illustrate, Barthes uses the example of a rat: it has a literal meaning (a physical, objective description) and it has a greater socio-cultural understanding. This contextual meaning is subjective and is dynamic within society.

Through Barthes' semiotic theory of language and myth, it can be shown that rap music has culturally influenced the language of its listeners, as they influence the connotative message to words that already exist. As more people listen to rap, the words that are used in the lyrics become culturally bound to the song, and then are disseminated through the conversations that people have using these words.

Most often, the terms that rappers use are pre-established words that have been prescribed new meaning through their music, that are eventually disseminated through social spheres. This newly contextualized word is called a neosemanticism. Neosemanticisms are forgotten words that are often brought forward from subcultures that attract the attention of members of the reigning culture of their time, then they are brought forward by the influential voices in society – in this case, these figures are rappers. To illustrate, the acronym YOLO was popularized by rapper, actor and RNB singer Drake in 2012 when he featured it in his own song, "The Motto". That year the term YOLO was so popular that it was printed on t-shirts, became a trending hashtag on Twitter, and was even considered as the inspiration for several tattoos. However, although the rapper may have come up with the acronym, the motto itself was in no way first established by Drake. Similar messages can be seen in many well-known sayings, or as early as 1896, in the English translation of "La Comédie Humaine", by Honoré de Balzac where one of his free-spirited characters tells another, "You Only Live Once!". Another example of a neosemanticism is the word "broccoli". Rapper E-40 initially uses the word "broccoli" to refer to marijuana, on his hit track "Broccoli" in 1993. In contemporary society, artists D.R.A.M. and Lil Yachty are often accredited for this slang on for "their" hit song, also titled "Broccoli".

With the rise in technology and mass media, the dissemination of subcultural terms has only become easier. Dick Hebdige, author of "," merits that subcultures often use music to vocalize the struggles of their experiences. As rap is also the culmination of a prevalent sub-culture in African-American social spheres, often their own personal cultures are disseminated through rap lyrics.

It is here that lyrics can be categorized as either historically influenced or (more commonly) considered as slang. Vernon Andrews, the professor of the course "American Studies 111: Hip-Hop Culture", suggests that many words, such as "hood", "homie", and "dope", are historically influenced. Most importantly, this also brings forward the anarchistic culture of rap music. Common themes from rap are anti-establishment and instead, promote black excellence and diversity. It is here that rap can be seen to reclaim words, namely, "nigga", a historical term used to subjugate and oppress Black people in America. This word has been reclaimed by Black Americans and is heavily used in rap music. Niggaz With Attitude embodies this notion by using it as the first word of their influential rap group name.

There are two kinds of freestyle rap: one is scripted (recitation), but having no particular overriding subject matter, the second typically referred to as "freestyling" or "spitting", is the improvisation of rapped lyrics. When freestyling, some rappers inadvertently reuse old lines, or even "cheat" by preparing segments or entire verses in advance. Therefore, freestyles with proven spontaneity are valued above generic, always usable lines. Rappers will often reference places or objects in their immediate setting, or specific (usually demeaning) characteristics of opponents, to prove their authenticity and originality.

Battle rapping, which can be freestyled, is the competition between two or more rappers in front of an audience. The tradition of insulting one's friends or acquaintances in rhyme goes back to the dozens, and was portrayed famously by Muhammad Ali in his boxing matches. The winner of a battle is decided by the crowd and/or preselected judges. According to Kool Moe Dee, a successful battle rap focuses on an opponent's weaknesses, rather than one's own strengths. Television shows such as MTV's "DFX" and BET's "106 and Park" host weekly freestyle battles live on the air. Battle rapping gained widespread public recognition outside of the African-American community with rapper Eminem's movie "8 Mile".

The strongest battle rappers will generally perform their rap fully freestyled. This is the most effective form in a battle as the rapper can comment on the other person, whether it be what they look like, or how they talk, or what they wear. It also allows the rapper to reverse a line used to "diss" him or her if they are the second rapper to battle. This is known as a "flip". Jin The Emcee was considered "World Champion" battle rapper in the mid-2000s.

Throughout hip hop's history, new musical styles and genres have developed that contain rapping. Entire genres, such as rap rock and its derivatives rapcore and rap metal (rock/metal/punk with rapped vocals), or hip house have resulted from the fusion of rap and other styles. Many popular music genres with a focus on percussion have contained rapping at some point; be it disco (DJ Hollywood), jazz (Gang Starr), new wave (Blondie), funk (Fatback Band), contemporary R&B (Mary J. Blige), reggaeton (Daddy Yankee), or even Japanese dance music (Soul'd Out). UK garage music has begun to focus increasingly on rappers in a new subgenre called grime which emerged in London in the early 2000s and was pioneered and popularized by the MC Dizzee Rascal. Increased popularity with the music has shown more UK rappers going to America as well as tour there, such as Sway DaSafo possibly signing with Akon's label Konvict. Hyphy is the latest of these spin-offs. It is typified by slowed-down atonal vocals with instrumentals that borrow heavily from the hip-hop scene and lyrics centered on illegal street racing and car culture. Another Oakland, California group, Beltaine's Fire, has recently gained attention for their Celtic fusion sound which blends hip-hop beats with Celtic melodies. Unlike the majority of hip-hop artists, all their music is performed live without samples, synths, or drum machines, drawing comparisons to The Roots and Rage Against the Machine.

Bhangra, a widely popular style of music from Punjab, India has been mixed numerous times with reggae and hip-hop music. The most popular song in this genre in the United States was "Mundian to Bach Ke" or "Beware the Boys" by Panjabi MC and Jay-Z. Although "Mundian To Bach Ke" had been released previously, the mixing with Jay-Z popularized the genre further.

Although the majority of rappers are male, there have been a number of female rap stars, including Lauryn Hill, MC Lyte, Lil' Kim, Missy Elliott, Queen Latifah, Da Brat, Eve, Trina, Nicki Minaj, Khia, M.I.A., CL from 2NE1, Foxy Brown, Iggy Azalea, and Lisa Lopes from TLC. There is also deaf rap artist Signmark.




</doc>
<doc id="25423" url="https://en.wikipedia.org/wiki?curid=25423" title="Rock music">
Rock music

Rock music is a broad genre of popular music that originated as "rock and roll" in the United States in the early 1950s, and developed into a range of different styles in the 1960s and later, particularly in the United States and the United Kingdom. It has its roots in 1940s and 1950s rock and roll, a style which drew heavily from the genres of blues, rhythm and blues, and from country music. Rock music also drew strongly from a number of other genres such as electric blues and folk, and incorporated influences from jazz, classical and other musical styles. Musically, rock has centered on the electric guitar, usually as part of a rock group with electric bass, drums, and one or more singers. Usually, rock is song-based music with a 4/4 time signature using a verse–chorus form, but the genre has become extremely diverse. Like pop music, lyrics often stress romantic love but also address a wide variety of other themes that are frequently social or political.

By the late 1960s "classic rock" period, a number of distinct rock music subgenres had emerged, including hybrids like blues rock, folk rock, country rock, southern rock, raga rock, and jazz-rock, many of which contributed to the development of psychedelic rock, which was influenced by the countercultural psychedelic and hippie scene. New genres that emerged included progressive rock, which extended the artistic elements; glam rock, which highlighted showmanship and visual style; and the diverse and enduring subgenre of heavy metal, which emphasized volume, power, and speed. In the second half of the 1970s, punk rock reacted by producing stripped-down, energetic social and political critiques. Punk was an influence in the 1980s on new wave, post-punk and eventually alternative rock. From the 1990s alternative rock began to dominate rock music and break into the mainstream in the form of grunge, Britpop, and indie rock. Further fusion subgenres have since emerged, including pop punk, electronic rock, rap rock, and rap metal, as well as conscious attempts to revisit rock's history, including the garage rock/post-punk and techno-pop revivals at the beginning of the 2000s. The 2010s saw a slow decline in the cultural relevancy of the genre, being usurped by hip-hop as the most popular genre in the United States in 2017.

Rock music has also embodied and served as the vehicle for cultural and social movements, leading to major subcultures including mods and rockers in the UK and the hippie counterculture that spread out from San Francisco in the US in the 1960s. Similarly, 1970s punk culture spawned the goth, punk, and emo subcultures. Inheriting the folk tradition of the protest song, rock music has been associated with political activism as well as changes in social attitudes to race, sex and drug use, and is often seen as an expression of youth revolt against adult consumerism and conformity.

The sound of rock is traditionally centered on the amplified electric guitar, which emerged in its modern form in the 1950s with the popularity of rock and roll. Also, it was influenced by the sounds of electric blues guitarists. The sound of an electric guitar in rock music is typically supported by an electric bass guitar, which pioneered in jazz music in the same era, and percussion produced from a drum kit that combines drums and cymbals. This trio of instruments has often been complemented by the inclusion of other instruments, particularly keyboards such as the piano, the Hammond organ, and the synthesizer. The basic rock instrumentation was derived from the basic blues band instrumentation (prominent lead guitar, second chordal instrument, bass, and drums). A group of musicians performing rock music is termed as a rock band or a rock group. Furthermore, it typically consists of between three (the power trio) and five members. Classically, a rock band takes the form of a quartet whose members cover one or more roles, including vocalist, lead guitarist, rhythm guitarist, bass guitarist, drummer, and often keyboard player or other instrumentalist.
Rock music is traditionally built on a foundation of simple unsyncopated rhythms in a 4/4 meter, with a repetitive snare drum back beat on beats two and four. Melodies often originate from older musical modes such as the Dorian and Mixolydian, as well as major and minor modes. Harmonies range from the common triad to parallel perfect fourths and fifths and dissonant harmonic progressions. Since the late 1950s and particularly from the mid 1960s onwards, rock music often used the verse-chorus structure derived from blues and folk music, but there has been considerable variation from this model. Critics have stressed the eclecticism and stylistic diversity of rock. Because of its complex history and its tendency to borrow from other musical and cultural forms, it has been argued that "it is impossible to bind rock music to a rigidly delineated musical definition."
Unlike many earlier styles of popular music, rock lyrics have dealt with a wide range of themes, including romantic love, sex, rebellion against "The Establishment", social concerns, and life styles. These themes were inherited from a variety of sources such as the Tin Pan Alley pop tradition, folk music, and rhythm and blues. Music journalist Robert Christgau characterizes rock lyrics as a "cool medium" with simple diction and repeated refrains, and asserts that rock's primary "function" "pertains to music, or, more generally, noise." The predominance of white, male, and often middle class musicians in rock music has often been noted, and rock has been seen as an appropriation of black musical forms for a young, white and largely male audience. As a result, it has also been seen to articulate the concerns of this group in both style and lyrics. Christgau, writing in 1972, said in spite of some exceptions, "rock and roll usually implies an identification of male sexuality and aggression".

Since the term "rock" started being used in preference to "rock and roll" from the late-1960s, it has usually been contrasted with pop music, with which it has shared many characteristics, but from which it is often distanced by an emphasis on musicianship, live performance, and a focus on serious and progressive themes as part of an ideology of authenticity that is frequently combined with an awareness of the genre's history and development. According to Simon Frith, rock was "something more than pop, something more than rock and roll" and "[r]ock musicians combined an emphasis on skill and technique with the romantic concept of art as artistic expression, original and sincere".

In the new millennium, the term "rock" has occasionally been used as a blanket term including forms like pop music, reggae music, soul music, and even hip hop, which it has been influenced with but often contrasted through much of its history. Christgau has used the term broadly to refer to popular and semipopular music that cater to his sensibility as "a rock-and-roller", including a fondness for a good beat, a meaningful lyric with some wit, and the theme of youth, which holds an "eternal attraction" so objective "that all youth music partakes of sociology and the field report." Writing in "" (1990), he said this sensibility is evident in the music of folk singer-songwriter Michelle Shocked, rapper LL Cool J, and synth-pop duo Pet Shop Boys—"all kids working out their identities"—as much as it is in the music of Chuck Berry, the Ramones, and the Replacements.

The foundations of rock music are in rock and roll, which originated in the United States during the late 1940s and early 1950s, and quickly spread to much of the rest of the world. Its immediate origins lay in a melding of various black musical genres of the time, including rhythm and blues and gospel music, with country and western. In 1951, Cleveland, Ohio disc jockey Alan Freed began playing rhythm and blues music (then termed "race music") for a multi-racial audience, and is credited with first using the phrase "rock and roll" to describe the music.

Debate surrounds which record should be considered the first rock and roll record. Contenders include Goree Carter's "" (1949); Jimmy Preston's "Rock the Joint" (1949), which was later covered by Bill Haley & His Comets in 1952; and "Rocket 88" by Jackie Brenston and his Delta Cats (in fact, Ike Turner and his band the Kings of Rhythm), recorded by Sam Phillips for Sun Records in 1951. Four years later, Bill Haley's "Rock Around the Clock" (1955) became the first rock and roll song to top "Billboard" magazine's main sales and airplay charts, and opened the door worldwide for this new wave of popular culture.
It also has been argued that "That's All Right (Mama)" (1954), Elvis Presley's first single for Sun Records in Memphis, could be the first rock and roll record, but, at the same time, Big Joe Turner's "Shake, Rattle & Roll", later covered by Haley, was already at the top of the Billboard R&B charts. Other artists with early rock and roll hits included Chuck Berry, Bo Diddley, Fats Domino, Little Richard, Jerry Lee Lewis, and Gene Vincent. Soon rock and roll was the major force in American record sales and crooners, such as Eddie Fisher, Perry Como, and Patti Page, who had dominated the previous decade of popular music, found their access to the pop charts significantly curtailed.

Rock and roll has been seen as leading to a number of distinct subgenres, including rockabilly, combining rock and roll with "hillbilly" country music, which was usually played and recorded in the mid-1950s by white singers such as Carl Perkins, Jerry Lee Lewis, Buddy Holly and with the greatest commercial success, Elvis Presley. In contrast doo wop placed an emphasis on multi-part vocal harmonies and meaningless backing lyrics (from which the genre later gained its name), which were usually supported with light instrumentation and had its origins in 1930s and 1940s African American vocal groups. Acts like the Crows, the Penguins, the El Dorados and the Turbans all scored major hits, and groups like the Platters, with songs including "The Great Pretender" (1955), and the Coasters with humorous songs like "Yakety Yak" (1958), ranked among the most successful rock and roll acts of the period.

The era also saw the growth in popularity of the electric guitar, and the development of a specifically rock and roll style of playing through such exponents as Chuck Berry, Link Wray, and Scotty Moore. The use of distortion, pioneered by electric blues guitarists such as Guitar Slim, Willie Johnson and Pat Hare in the early 1950s, was popularized by Chuck Berry in the mid-1950s. The use of power chords, pioneered by Willie Johnson and Pat Hare in the early 1950s, was popularized by Link Wray in the late 1950s.

In the United Kingdom, the trad jazz and folk movements brought visiting blues music artists to Britain. Lonnie Donegan's 1955 hit "Rock Island Line" was a major influence and helped to develop the trend of skiffle music groups throughout the country, many of which, including John Lennon's Quarrymen, moved on to play rock and roll.

Commentators have traditionally perceived a decline of rock and roll in the late 1950s and early 1960s. By 1959, the death of Buddy Holly, The Big Bopper and Ritchie Valens in a plane crash, the departure of Elvis for the army, the retirement of Little Richard to become a preacher, prosecutions of Jerry Lee Lewis and Chuck Berry and the breaking of the payola scandal (which implicated major figures, including Alan Freed, in bribery and corruption in promoting individual acts or songs), gave a sense that the rock and roll era established at that point had come to an end.

The term "pop" has been used since the early 20th century to refer to popular music in general, but from the mid-1950s it began to be used for a distinct genre, aimed at a youth market, often characterized as a softer alternative to rock and roll. From about 1967, it was increasingly used in opposition to the term rock music, to describe a form that was more commercial, ephemeral and accessible. In contrast rock music was seen as focusing on extended works, particularly albums, was often associated with particular sub-cultures (like the counterculture of the 1960s), placed an emphasis on artistic values and "authenticity", stressed live performance and instrumental or vocal virtuosity and was often seen as encapsulating progressive developments rather than simply reflecting existing trends. Nevertheless, much pop and rock music has been very similar in sound, instrumentation and even lyrical content.
The period of the later 1950s and early 1960s has traditionally been seen as an era of hiatus for rock and roll. More recently some authors have emphasised important innovations and trends in this period without which future developments would not have been possible. While early rock and roll, particularly through the advent of rockabilly, saw the greatest commercial success for male and white performers, in this era the genre was dominated by black and female artists. Rock and roll had not disappeared at the end of the 1950s and some of its energy can be seen in the Twist dance craze of the early 1960s, mainly benefiting the career of Chubby Checker.

Cliff Richard had the first British rock and roll hit with "Move It", effectively ushering in the sound of British rock. At the start of the 1960s, his backing group the Shadows was the most successful group recording instrumentals. While rock 'n' roll was fading into lightweight pop and ballads, British rock groups at clubs and local dances, heavily influenced by blues-rock pioneers like Alexis Korner, were starting to play with an intensity and drive seldom found in white American acts.

Also significant was the advent of soul music as a major commercial force. Developing out of rhythm and blues with a re-injection of gospel music and pop, led by pioneers like Ray Charles and Sam Cooke from the mid-1950s, by the early 1960s figures like Marvin Gaye, James Brown, Aretha Franklin, Curtis Mayfield and Stevie Wonder were dominating the R&B charts and breaking through into the main pop charts, helping to accelerate their desegregation, while Motown and Stax/Volt Records were becoming major forces in the record industry. Some historians of music have also pointed to important and innovative technical developments that built on rock and roll in this period, including the electronic treatment of sound by such innovators as Joe Meek, and the elaborate production methods of the Wall of Sound pursued by Phil Spector.

The instrumental rock and roll of performers such as Duane Eddy, Link Wray and the Ventures was developed by Dick Dale, who added distinctive "wet" reverb, rapid alternate picking, and Middle Eastern and Mexican influences. He produced the regional hit "Let's Go Trippin'" in 1961 and launched the surf music craze, following up with songs like "Misirlou" (1962). Like Dale and his Del-Tones, most early surf bands were formed in Southern California, including the Bel-Airs, the Challengers, and Eddie & the Showmen. The Chantays scored a top ten national hit with "Pipeline" in 1963 and probably the best known surf tune was 1963's "Wipe Out", by the Surfaris, which hit number 2 and number 10 on the "Billboard" charts in 1965.

Surf music achieved its greatest commercial success as vocal music, particularly the work of the Beach Boys, formed in 1961 in Southern California. Their early albums included both instrumental surf rock (among them covers of music by Dick Dale) and vocal songs, drawing on rock and roll and doo wop and the close harmonies of vocal pop acts like the Four Freshmen. The Beach Boys first chart hit, "Surfin'" in 1962 reached the "Billboard" top 100 and helped make the surf music craze a national phenomenon. It is often argued that the surf music craze and the careers of almost all surf acts was effectively ended by the arrival of the British Invasion from 1964, because most surf music hits were recorded and released between 1961 and 1965.

By the end of 1962, what would become the British rock scene had started with beat groups like the Beatles, Gerry & the Pacemakers and the Searchers from Liverpool and Freddie and the Dreamers, Herman's Hermits and the Hollies from Manchester. They drew on a wide range of American influences including 1950s rock and roll, soul, rhythm and blues, and surf music, initially reinterpreting standard American tunes and playing for dancers. Bands like the Animals from Newcastle and Them from Belfast, and particularly those from London like the Rolling Stones and the Yardbirds, were much more directly influenced by rhythm and blues and later blues music. Soon these groups were composing their own material, combining US forms of music and infusing it with a high energy beat. Beat bands tended towards "bouncy, irresistible melodies", while early British blues acts tended towards less sexually innocent, more aggressive songs, often adopting an anti-establishment stance. There was, however, particularly in the early stages, considerable musical crossover between the two tendencies. By 1963, led by the Beatles, beat groups had begun to achieve national success in Britain, soon to be followed into the charts by the more rhythm and blues focused acts.

"I Want to Hold Your Hand" was the Beatles' first number 1 hit on the "Billboard" Hot 100, spending 7 weeks at the top and a total of 15 weeks on the chart. Their first appearance on "The Ed Sullivan Show" on 9 February 1964, drawing an estimated 73 million viewers (at the time a record for an American television program) is often considered a milestone in American pop culture. During the week of 4 April 1964, the Beatles held twelve positions on the "Billboard" Hot 100 singles chart, including the entire top five. The Beatles went on to become the biggest selling rock band of all time and they were followed into the US charts by numerous British bands. During the next two years British acts dominated their own and the US charts with Peter and Gordon, the Animals, Manfred Mann, Petula Clark, Freddie and the Dreamers, Wayne Fontana and the Mindbenders, Herman's Hermits, the Rolling Stones, the Troggs, and Donovan all having one or more number one singles. Other major acts that were part of the invasion included the Kinks and the Dave Clark Five.

The British Invasion helped internationalize the production of rock and roll, opening the door for subsequent British (and Irish) performers to achieve international success. In America it arguably spelled the end of instrumental surf music, vocal girl groups and (for a time) the teen idols, that had dominated the American charts in the late 1950s and 1960s. It dented the careers of established R&B acts like Fats Domino and Chubby Checker and even temporarily derailed the chart success of surviving rock and roll acts, including Elvis. The British Invasion also played a major part in the rise of a distinct genre of rock music, and cemented the primacy of the rock group, based on guitars and drums and producing their own material as singer-songwriters.

Garage rock was a raw form of rock music, particularly prevalent in North America in the mid-1960s and so called because of the perception that it was rehearsed in the suburban family garage. Garage rock songs often revolved around the traumas of high school life, with songs about "lying girls" and unfair social circumstances being particularly common. The lyrics and delivery tended to be more aggressive than was common at the time, often with growled or shouted vocals that dissolved into incoherent screaming. They ranged from crude one-chord music (like the Seeds) to near-studio musician quality (including the Knickerbockers, the Remains, and the Fifth Estate). There were also regional variations in many parts of the country with flourishing scenes particularly in California and Texas. The Pacific Northwest states of Washington and Oregon had perhaps the most defined regional sound.
The style had been evolving from regional scenes as early as 1958. "Tall Cool One" (1959) by The Wailers and "Louie Louie" by the Kingsmen (1963) are mainstream examples of the genre in its formative stages. By 1963, garage band singles were creeping into the national charts in greater numbers, including Paul Revere and the Raiders (Boise), the Trashmen (Minneapolis) and the Rivieras (South Bend, Indiana). Other influential garage bands, such as the Sonics (Tacoma, Washington), never reached the "Billboard" Hot 100.

The British Invasion greatly influenced garage bands, providing them with a national audience, leading many (often surf or hot rod groups) to adopt a British influence, and encouraging many more groups to form. Thousands of garage bands were extant in the US and Canada during the era and hundreds produced regional hits. Despite scores of bands being signed to major or large regional labels, most were commercial failures. It is generally agreed that garage rock peaked both commercially and artistically around 1966. By 1968 the style largely disappeared from the national charts and at the local level as amateur musicians faced college, work or the draft. New styles had evolved to replace garage rock.

Although the first impact of the British Invasion on American popular music was through beat and R&B based acts, the impetus was soon taken up by a second wave of bands that drew their inspiration more directly from American blues, including the Rolling Stones and the Yardbirds. British blues musicians of the late 1950s and early 1960s had been inspired by the acoustic playing of figures such as Lead Belly, who was a major influence on the Skiffle craze, and Robert Johnson. Increasingly they adopted a loud amplified sound, often centered on the electric guitar, based on the Chicago blues, particularly after the tour of Britain by Muddy Waters in 1958, which prompted Cyril Davies and guitarist Alexis Korner to form the band Blues Incorporated. The band involved and inspired many of the figures of the subsequent British blues boom, including members of the Rolling Stones and Cream, combining blues standards and forms with rock instrumentation and emphasis.
The other key focus for British blues was John Mayall; his band, the Bluesbreakers, included Eric Clapton (after Clapton's departure from the Yardbirds) and later Peter Green. Particularly significant was the release of "Blues Breakers with Eric Clapton (Beano)" album (1966), considered one of the seminal British blues recordings and the sound of which was much emulated in both Britain and the United States. Eric Clapton went on to form supergroups Cream, Blind Faith, and Derek and the Dominos, followed by an extensive solo career that helped bring blues rock into the mainstream. Green, along with the Bluesbreaker's rhythm section Mick Fleetwood and John McVie, formed Peter Green's Fleetwood Mac, who enjoyed some of the greatest commercial success in the genre. In the late 1960s Jeff Beck, also an alumnus of the Yardbirds, moved blues rock in the direction of heavy rock with his band, the Jeff Beck Group. The last Yardbirds guitarist was Jimmy Page, who went on to form "The New Yardbirds" which rapidly became Led Zeppelin. Many of the songs on their first three albums, and occasionally later in their careers, were expansions on traditional blues songs.

In America, blues rock had been pioneered in the early 1960s by guitarist Lonnie Mack, but the genre began to take off in the mid-1960s as acts developed a sound similar to British blues musicians. Key acts included Paul Butterfield (whose band acted like Mayall's Bluesbreakers in Britain as a starting point for many successful musicians), Canned Heat, the early Jefferson Airplane, Janis Joplin, Johnny Winter, the J. Geils Band and Jimi Hendrix with his power trios, the Jimi Hendrix Experience (which included two British members, and was founded in Britain), and Band of Gypsys, whose guitar virtuosity and showmanship would be among the most emulated of the decade. Blues rock bands from the southern states, like the Allman Brothers Band, Lynyrd Skynyrd, and ZZ Top, incorporated country elements into their style to produce the distinctive genre Southern rock.

Early blues rock bands often emulated jazz, playing long, involved improvisations, which would later be a major element of progressive rock. From about 1967 bands like Cream and the Jimi Hendrix Experience had moved away from purely blues-based music into psychedelia. By the 1970s, blues rock had become heavier and more riff-based, exemplified by the work of Led Zeppelin and Deep Purple, and the lines between blues rock and hard rock "were barely visible", as bands began recording rock-style albums. The genre was continued in the 1970s by figures such as George Thorogood and Pat Travers, but, particularly on the British scene (except perhaps for the advent of groups such as Status Quo and Foghat who moved towards a form of high energy and repetitive boogie rock), bands became focused on heavy metal innovation, and blues rock began to slip out of the mainstream.

By the 1960s, the scene that had developed out of the American folk music revival had grown to a major movement, utilising traditional music and new compositions in a traditional style, usually on acoustic instruments. In America the genre was pioneered by figures such as Woody Guthrie and Pete Seeger and often identified with progressive or labor politics. In the early sixties figures such as Joan Baez and Bob Dylan had come to the fore in this movement as singer-songwriters. Dylan had begun to reach a mainstream audience with hits including "Blowin' in the Wind" (1963) and "Masters of War" (1963), which brought "protest songs" to a wider public, but, although beginning to influence each other, rock and folk music had remained largely separate genres, often with mutually exclusive audiences.

Early attempts to combine elements of folk and rock included the Animals' "House of the Rising Sun" (1964), which was the first commercially successful folk song to be recorded with rock and roll instrumentation and the Beatles "I'm a Loser" (1964), arguably the first Beatles song to be influenced directly by Dylan. The folk rock movement is usually thought to have taken off with The Byrds' recording of Dylan's "Mr. Tambourine Man" which topped the charts in 1965. With members who had been part of the cafe-based folk scene in Los Angeles, the Byrds adopted rock instrumentation, including drums and 12-string Rickenbacker guitars, which became a major element in the sound of the genre. Later that year Dylan adopted electric instruments, much to the outrage of many folk purists, with his "Like a Rolling Stone" becoming a US hit single. Folk rock particularly took off in California, where it led acts like the Mamas & the Papas and Crosby, Stills and Nash to move to electric instrumentation, and in New York, where it spawned performers including The Lovin' Spoonful and Simon and Garfunkel, with the latter's acoustic "The Sounds of Silence" (1965) being remixed with rock instruments to be the first of many hits.

These acts directly influenced British performers like Donovan and Fairport Convention. In 1969 Fairport Convention abandoned their mixture of American covers and Dylan-influenced songs to play traditional English folk music on electric instruments. This British folk-rock was taken up by bands including Pentangle, Steeleye Span and the Albion Band, which in turn prompted Irish groups like Horslips and Scottish acts like the JSD Band, Spencer's Feat and later Five Hand Reel, to use their traditional music to create a brand of Celtic rock in the early 1970s.

Folk-rock reached its peak of commercial popularity in the period 1967–68, before many acts moved off in a variety of directions, including Dylan and the Byrds, who began to develop country rock. However, the hybridization of folk and rock has been seen as having a major influence on the development of rock music, bringing in elements of psychedelia, and helping to develop the ideas of the singer-songwriter, the protest song, and concepts of "authenticity".

Psychedelic music's LSD-inspired vibe began in the folk scene. The first group to advertise themselves as psychedelic rock were the 13th Floor Elevators from Texas. The Beatles introduced many of the major elements of the psychedelic sound to audiences in this period, such as guitar feedback, the Indian sitar and backmasking sound effects. Psychedelic rock particularly took off in California's emerging music scene as groups followed the Byrds's shift from folk to folk rock from 1965. The psychedelic lifestyle, which revolved around hallucinogenic drugs, had already developed in San Francisco and particularly prominent products of the scene were Big Brother and the Holding Company, the Grateful Dead and Jefferson Airplane. The Jimi Hendrix Experience's lead guitarist, Jimi Hendrix did
extended distorted, feedback-filled jams which became a key feature of psychedelia. Psychedelic rock reached its apogee in the last years of the decade. 1967 saw the Beatles release their definitive psychedelic statement in "Sgt. Pepper's Lonely Hearts Club Band", including the controversial track "Lucy in the Sky with Diamonds", the Rolling Stones responded later that year with "Their Satanic Majesties Request", and the Pink Floyd debuted with "The Piper at the Gates of Dawn". Key recordings included Jefferson Airplane's "Surrealistic Pillow" and the Doors' "Strange Days". These trends peaked in the 1969 Woodstock festival, which saw performances by most of the major psychedelic acts.

Progressive rock, a term sometimes used interchangeably with art rock, moved beyond established musical formulas by experimenting with different instruments, song types, and forms. From the mid-1960s the Left Banke, the Beatles, Queen, the Rolling Stones and the Beach Boys, had pioneered the inclusion of harpsichords, wind, and string sections on their recordings to produce a form of Baroque rock and can be heard in singles like Procol Harum's "A Whiter Shade of Pale" (1967), with its Bach-inspired introduction. The Moody Blues used a full orchestra on their album "Days of Future Passed" (1967) and subsequently created orchestral sounds with synthesizers. Classical orchestration, keyboards, and synthesizers were a frequent addition to the established rock format of guitars, bass, and drums in subsequent progressive rock.
Instrumentals were common, while songs with lyrics were sometimes conceptual, abstract, or based in fantasy and science fiction. The Pretty Things' "SF Sorrow" (1968), and the Kinks' "Arthur (Or the Decline and Fall of the British Empire)" (1969) introduced the format of rock operas and opened the door to concept albums, often telling an epic story or tackling a grand overarching theme. King Crimson's 1969 début album, "In the Court of the Crimson King", which mixed powerful guitar riffs and mellotron, with jazz and symphonic music, is often taken as the key recording in progressive rock, helping the widespread adoption of the genre in the early 1970s among existing blues-rock and psychedelic bands, as well as newly formed acts. The vibrant Canterbury scene saw acts following Soft Machine from psychedelia, through jazz influences, toward more expansive hard rock, including Caravan, Hatfield and the North, Gong, and National Health.

Greater commercial success was enjoyed by Pink Floyd, who also moved away from psychedelia after the departure of Syd Barrett in 1968, with "The Dark Side of the Moon" (1973), seen as a masterpiece of the genre, becoming one of the best-selling albums of all time. There was an emphasis on instrumental virtuosity, with Yes showcasing the skills of both guitarist Steve Howe and keyboard player Rick Wakeman, while Emerson, Lake & Palmer were a supergroup who produced some of the genre's most technically demanding work. Jethro Tull and Genesis both pursued very different, but distinctly English, brands of music. Renaissance, formed in 1969 by ex-Yardbirds Jim McCarty and Keith Relf, evolved into a high-concept band featuring the three-octave voice of Annie Haslam. Most British bands depended on a relatively small cult following, but a handful, including Pink Floyd, Genesis, and Jethro Tull, managed to produce top ten singles at home and break the American market. The American brand of progressive rock varied from the eclectic and innovative Frank Zappa, Captain Beefheart and Blood, Sweat & Tears, to more pop rock orientated bands like Boston, Foreigner, Kansas, Journey, and Styx. These, beside British bands Supertramp and ELO, all demonstrated a prog rock influence and while ranking among the most commercially successful acts of the 1970s, heralding the era of "pomp" or "arena rock", which would last until the costs of complex shows (often with theatrical staging and special effects), would be replaced by more economical rock festivals as major live venues in the 1990s.

The instrumental strand of the genre resulted in albums like Mike Oldfield's "Tubular Bells" (1973), the first record, and worldwide hit, for the Virgin Records label, which became a mainstay of the genre. Instrumental rock was particularly significant in continental Europe, allowing bands like Kraftwerk, Tangerine Dream, Can, and Faust to circumvent the language barrier. Their synthesiser-heavy "krautrock", along with the work of Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent electronic rock. With the advent of punk rock and technological changes in the late 1970s, progressive rock was increasingly dismissed as pretentious and overblown. Many bands broke up, but some, including Genesis, ELP, Yes, and Pink Floyd, regularly scored top ten albums with successful accompanying worldwide tours. Some bands which emerged in the aftermath of punk, such as Siouxsie and the Banshees, Ultravox, and Simple Minds, showed the influence of progressive rock, as well as their more usually recognized punk influences.

In the late 1960s, jazz-rock emerged as a distinct subgenre out of the blues-rock, psychedelic, and progressive rock scenes, mixing the power of rock with the musical complexity and improvisational elements of jazz. AllMusic states that the term jazz-rock "may refer to the loudest, wildest, most electrified fusion bands from the jazz camp, but most often it describes performers coming from the rock side of the equation." Jazz-rock "...generally grew out of the most artistically ambitious rock subgenres of the late '60s and early '70s", including the singer-songwriter movement. Many early US rock and roll musicians had begun in jazz and carried some of these elements into the new music. In Britain the subgenre of blues rock, and many of its leading figures, like Ginger Baker and Jack Bruce of the Eric Clapton-fronted band Cream, had emerged from the British jazz scene. Often highlighted as the first true jazz-rock recording is the only album by the relatively obscure New York-based the Free Spirits with "Out of Sight and Sound" (1966). The first group of bands to self-consciously use the label were R&B oriented white rock bands that made use of jazzy horn sections, like Electric Flag, Blood, Sweat & Tears and Chicago, to become some of the most commercially successful acts of the later 1960s and the early 1970s.

British acts to emerge in the same period from the blues scene, to make use of the tonal and improvisational aspects of jazz, included Nucleus and the Graham Bond and John Mayall spin-off Colosseum. From the psychedelic rock and the Canterbury scenes came Soft Machine, who, it has been suggested, produced one of the artistically successfully fusions of the two genres. Perhaps the most critically acclaimed fusion came from the jazz side of the equation, with Miles Davis, particularly influenced by the work of Hendrix, incorporating rock instrumentation into his sound for the album "Bitches Brew" (1970). It was a major influence on subsequent rock-influenced jazz artists, including Herbie Hancock, Chick Corea and Weather Report. The genre began to fade in the late 1970s, as a mellower form of fusion began to take its audience, but acts like Steely Dan, Frank Zappa and Joni Mitchell recorded significant jazz-influenced albums in this period, and it has continued to be a major influence on rock music.

Reflecting on developments in rock music at the start of the 1970s, Robert Christgau later wrote in "" (1981):

Rock saw greater commodification during this decade, turning into a multibillion-dollar industry and doubling its market while, as Christgau noted, suffering a significant "loss of cultural prestige". "Maybe the Bee Gees became more popular than the Beatles, but they were never more popular than Jesus", he said. "Insofar as the music retained any mythic power, the myth was self-referential — there were lots of songs about the rock and roll life but very few about how rock could change the world, except as a new brand of painkiller ... In the '70s the powerful took over, as rock industrialists capitalized on the national mood to reduce potent music to an often reactionary species of entertainment—and to transmute rock's popular base from the audience to market."

Roots rock is the term now used to describe a move away from what some saw as the excesses of the psychedelic scene, to a more basic form of rock and roll that incorporated its original influences, particularly country and folk music, leading to the creation of country rock and Southern rock. In 1966 Bob Dylan went to Nashville to record the album "Blonde on Blonde". This, and subsequent more clearly country-influenced albums, have been seen as creating the genre of country folk, a route pursued by a number of largely acoustic folk musicians. Other acts that followed the back-to-basics trend were the Canadian group the Band and the California-based Creedence Clearwater Revival, both of which mixed basic rock and roll with folk, country and blues, to be among the most successful and influential bands of the late 1960s. The same movement saw the beginning of the recording careers of Californian solo artists like Ry Cooder, Bonnie Raitt and Lowell George, and influenced the work of established performers such as the Rolling Stones' "Beggar's Banquet" (1968) and the Beatles' "Let It Be" (1970).
In 1968, Gram Parsons recorded "Safe at Home" with the International Submarine Band, arguably the first true country rock album. Later that year he joined the Byrds for "Sweetheart of the Rodeo" (1968), generally considered one of the most influential recordings in the genre. The Byrds continued in the same vein, but Parsons left to be joined by another ex-Byrds member Chris Hillman in forming the Flying Burrito Brothers who helped establish the respectability and parameters of the genre, before Parsons departed to pursue a solo career. Bands in California that adopted country rock included Hearts and Flowers, Poco, New Riders of the Purple Sage, the Beau Brummels, and the Nitty Gritty Dirt Band. Some performers also enjoyed a renaissance by adopting country sounds, including: the Everly Brothers; one-time teen idol Rick Nelson who became the frontman for the Stone Canyon Band; former Monkee Mike Nesmith who formed the First National Band; and Neil Young. The Dillards were, unusually, a country act, who moved towards rock music. The greatest commercial success for country rock came in the 1970s, with artists including the Doobie Brothers, Emmylou Harris, Linda Ronstadt and the Eagles (made up of members of the Burritos, Poco, and Stone Canyon Band), who emerged as one of the most successful rock acts of all time, producing albums that included "Hotel California" (1976).

The founders of Southern rock are usually thought to be the Allman Brothers Band, who developed a distinctive sound, largely derived from blues rock, but incorporating elements of boogie, soul, and country in the early 1970s. The most successful act to follow them were Lynyrd Skynyrd, who helped establish the "Good ol' boy" image of the subgenre and the general shape of 1970s' guitar rock. Their successors included the fusion/progressive instrumentalists Dixie Dregs, the more country-influenced Outlaws, jazz-leaning Wet Willie and (incorporating elements of R&B and gospel) the Ozark Mountain Daredevils. After the loss of original members of the Allmans and Lynyrd Skynyrd, the genre began to fade in popularity in the late 1970s, but was sustained the 1980s with acts like .38 Special, Molly Hatchet and the Marshall Tucker Band.

Glam rock emerged from the English psychedelic and art rock scenes of the late 1960s and can be seen as both an extension of and reaction against those trends. Musically diverse, varying between the simple rock and roll revivalism of figures like Alvin Stardust to the complex art rock of Roxy Music, and can be seen as much as a fashion as a musical subgenre. Visually it was a mesh of various styles, ranging from 1930s Hollywood glamor, through 1950s pin-up sex appeal, pre-war Cabaret theatrics, Victorian literary and symbolist styles, science fiction, to ancient and occult mysticism and mythology; manifesting itself in outrageous clothes, makeup, hairstyles, and platform-soled boots. Glam is most noted for its sexual and gender ambiguity and representations of androgyny, beside extensive use of theatrics. It was prefigured by the showmanship and gender-identity manipulation of American acts such as the Cockettes and Alice Cooper.

The origins of glam rock are associated with Marc Bolan, who had renamed his folk duo to T. Rex and taken up electric instruments by the end of the 1960s. Often cited as the moment of inception is his appearance on the UK TV programme "Top of the Pops" in December 1970 wearing glitter, to perform what would be his first number 1 single "Ride a White Swan". From 1971, already a minor star, David Bowie developed his Ziggy Stardust persona, incorporating elements of professional make up, mime and performance into his act. These performers were soon followed in the style by acts including Roxy Music, Sweet, Slade, Mott the Hoople, Mud and Alvin Stardust. While highly successful in the single charts in the UK, very few of these musicians were able to make a serious impact in the United States; Bowie was the major exception becoming an international superstar and prompting the adoption of glam styles among acts like Lou Reed, Iggy Pop, New York Dolls and Jobriath, often known as "glitter rock" and with a darker lyrical content than their British counterparts. In the UK the term glitter rock was most often used to refer to the extreme version of glam pursued by Gary Glitter and his support musicians the Glitter Band, who between them achieved eighteen top ten singles in the UK between 1972 and 1976. A second wave of glam rock acts, including Suzi Quatro, Roy Wood's Wizzard and Sparks, dominated the British single charts from about 1974 to 1976. Existing acts, some not usually considered central to the genre, also adopted glam styles, including Rod Stewart, Elton John, Queen and, for a time, even the Rolling Stones. It was also a direct influence on acts that rose to prominence later, including Kiss and Adam Ant, and less directly on the formation of gothic rock and glam metal as well as on punk rock, which helped end the fashion for glam from about 1976. Glam has since enjoyed sporadic modest revivals through bands such as Chainsaw Kittens, the Darkness and in R n' B crossover act Prince.

From the late 1960s it became common to divide mainstream rock music into soft and hard rock. Soft rock was often derived from folk rock, using acoustic instruments and putting more emphasis on melody and harmonies. Major artists included Carole King, Cat Stevens and James Taylor. It reached its commercial peak in the mid- to late 1970s with acts like Billy Joel, America and the reformed Fleetwood Mac, whose "Rumours" (1977) was the best-selling album of the decade. In contrast, hard rock was more often derived from blues-rock and was played louder and with more intensity. It often emphasised the electric guitar, both as a rhythm instrument using simple repetitive riffs and as a solo lead instrument, and was more likely to be used with distortion and other effects. Key acts included British Invasion bands like the Kinks, as well as psychedelic era performers like Cream, Jimi Hendrix and the Jeff Beck Group. Hard rock-influenced bands that enjoyed international success in the later 1970s included Queen, Thin Lizzy, Aerosmith, AC/DC, and Van Halen.

From the late 1960s the term "heavy metal" began to be used to describe some hard rock played with even more volume and intensity, first as an adjective and by the early 1970s as a noun. The term was first used in music in Steppenwolf's "Born to Be Wild" (1967) and began to be associated with pioneer bands like San Francisco's Blue Cheer, Cleveland's James Gang and Michigan's Grand Funk Railroad. By 1970 three key British bands had developed the characteristic sounds and styles which would help shape the subgenre. Led Zeppelin added elements of fantasy to their riff laden blues-rock, Deep Purple brought in symphonic and medieval interests from their progressive rock phase and Black Sabbath introduced facets of the gothic and modal harmony, helping to produce a "darker" sound. These elements were taken up by a "second generation" of heavy metal bands into the late 1970s, including: Judas Priest, UFO, Motörhead and Rainbow from Britain; Kiss, Ted Nugent, and Blue Öyster Cult from the US; Rush from Canada and Scorpions from Germany, all marking the expansion in popularity of the subgenre. Despite a lack of airplay and very little presence on the singles charts, late-1970s heavy metal built a considerable following, particularly among adolescent working-class males in North America and Europe.

Rock, mostly the heavy metal genre, has been criticized by some Christian leaders, who have condemned it as immoral, anti-Christian and even demonic. However, Christian rock began to develop in the late 1960s, particularly out of the Jesus movement beginning in Southern California, and emerged as a subgenre in the 1970s with artists like Larry Norman, usually seen as the first major "star" of Christian rock. The genre has been particularly popular in the United States. Many Christian rock performers have ties to the contemporary Christian music scene, while other bands and artists are closely linked to independent music. Since the 1980s Christian rock performers have gained mainstream success, including figures such as the American gospel-to-pop crossover artist Amy Grant and the British singer Cliff Richard. While these artists were largely acceptable in Christian communities the adoption of heavy rock and glam metal styles by bands like Petra and Stryper, who achieved considerable mainstream success in the 1980s, was more controversial. From the 1990s there were increasing numbers of acts who attempted to avoid the Christian band label, preferring to be seen as groups who were also Christians, including P.O.D and Collective Soul.

Punk rock was developed between 1974 and 1976 in the United States and the United Kingdom. Rooted in garage rock and other forms of what is now known as protopunk music, punk rock bands eschewed the perceived excesses of mainstream 1970s rock. They created fast, hard-edged music, typically with short songs, stripped-down instrumentation, and often political, anti-establishment lyrics. Punk embraces a DIY (do it yourself) ethic, with many bands self-producing their recordings and distributing them through informal channels.

By late 1976, acts such as the Ramones and Patti Smith, in New York City, and the Sex Pistols and the Clash, in London, were recognized as the vanguard of a new musical movement. The following year saw punk rock spreading around the world. Punk quickly, though briefly, became a major cultural phenomenon in the United Kingdom. For the most part, punk took root in local scenes that tended to reject association with the mainstream. An associated punk subculture emerged, expressing youthful rebellion and characterized by distinctive clothing styles and a variety of anti-authoritarian ideologies.

By the beginning of the 1980s, faster, more aggressive styles such as hardcore and Oi! had become the predominant mode of punk rock. This has resulted in several evolved strains of hardcore punk, such as D-beat (a distortion-heavy subgenre influenced by the UK band Discharge), anarcho-punk (such as Crass), grindcore (such as Napalm Death), and crust punk. Musicians identifying with or inspired by punk also pursued a broad range of other variations, giving rise to New wave, post-punk and the alternative rock movement.

Although punk rock was a significant social and musical phenomenon, it achieved less in the way of record sales (being distributed by small specialty labels such as Stiff Records), or American radio airplay (as the radio scene continued to be dominated by mainstream formats such as disco and album-oriented rock). Punk rock had attracted devotees from the art and collegiate world and soon bands sporting a more literate, arty approach, such as Talking Heads and Devo began to infiltrate the punk scene; in some quarters the description "new wave" began to be used to differentiate these less overtly punk bands. Record executives, who had been mostly mystified by the punk movement, recognized the potential of the more accessible new wave acts and began aggressively signing and marketing any band that could claim a remote connection to punk or new wave. Many of these bands, such as the Cars and the Go-Go's can be seen as pop bands marketed as new wave; other existing acts, including the Police, the Pretenders and Elvis Costello, used the new wave movement as the springboard for relatively long and critically successful careers, while "skinny tie" bands exemplified by the Knack, or the photogenic Blondie, began as punk acts and moved into more commercial territory.

Between 1979 and 1985, influenced by Kraftwerk, Yellow Magic Orchestra, David Bowie and Gary Numan, British new wave went in the direction of such New Romantics as Spandau Ballet, Ultravox, Japan, Duran Duran, A Flock of Seagulls, Culture Club, Talk Talk and the Eurythmics, sometimes using the synthesizer to replace all other instruments. This period coincided with the rise of MTV and led to a great deal of exposure for this brand of synth-pop, creating what has been characterised as a second British Invasion. Some more traditional rock bands adapted to the video age and profited from MTV's airplay, most obviously Dire Straits, whose "Money for Nothing" gently poked fun at the station, despite the fact that it had helped make them international stars, but in general, guitar-oriented rock was commercially eclipsed.

If hardcore most directly pursued the stripped down aesthetic of punk, and new wave came to represent its commercial wing, post-punk emerged in the later 1970s and early 1980s as its more artistic and challenging side. Major influences beside punk bands were the Velvet Underground, Frank Zappa and Captain Beefheart, and the New York-based no wave scene which placed an emphasis on performance, including bands such as James Chance and the Contortions, DNA and Sonic Youth. Early contributors to the genre included the US bands Pere Ubu, Devo, the Residents and Talking Heads.

The first wave of British post-punk included Gang of Four, Siouxsie and the Banshees and Joy Division, who placed less emphasis on art than their US counterparts and more on the dark emotional qualities of their music. Bands like Siouxsie and the Banshees, Bauhaus, the Cure, and the Sisters of Mercy, moved increasingly in this direction to found Gothic rock, which had become the basis of a major sub-culture by the early 1980s. Similar emotional territory was pursued by Australian acts like the Birthday Party and Nick Cave. Members of Bauhaus and Joy Division explored new stylistic territory as Love and Rockets and New Order respectively. Another early post-punk movement was the industrial music developed by British bands Throbbing Gristle and Cabaret Voltaire, and New York-based Suicide, using a variety of electronic and sampling techniques that emulated the sound of industrial production and which would develop into a variety of forms of post-industrial music in the 1980s.

The second generation of British post-punk bands that broke through in the early 1980s, including the Fall, the Pop Group, the Mekons, Echo and the Bunnymen and the Teardrop Explodes, tended to move away from dark sonic landscapes. Arguably the most successful band to emerge from post-punk was Ireland's U2, who incorporated elements of religious imagery together with political commentary into their often anthemic music, and by the late 1980s had become one of the biggest bands in the world. Although many post-punk bands continued to record and perform, it declined as a movement in the mid-1980s as acts disbanded or moved off to explore other musical areas, but it has continued to influence the development of rock music and has been seen as a major element in the creation of the alternative rock movement.

American working-class oriented heartland rock, characterized by a straightforward musical style, and a concern with the lives of ordinary, blue-collar American people, developed in the second half of the 1970s. The term heartland rock was first used to describe Midwestern arena rock groups like Kansas, REO Speedwagon and Styx, but which came to be associated with a more socially concerned form of roots rock more directly influenced by folk, country and rock and roll. It has been seen as an American Midwest and Rust Belt counterpart to West Coast country rock and the Southern rock of the American South. Led by figures who had initially been identified with punk and New Wave, it was most strongly influenced by acts such as Bob Dylan, the Byrds, Creedence Clearwater Revival and Van Morrison, and the basic rock of 1960s garage and the Rolling Stones.

Exemplified by the commercial success of singer songwriters Bruce Springsteen, Bob Seger, and Tom Petty, along with less widely known acts such as Southside Johnny and the Asbury Jukes and Joe Grushecky and the Houserockers, it was partly a reaction to post-industrial urban decline in the East and Mid-West, often dwelling on issues of social disintegration and isolation, beside a form of good-time rock and roll revivalism. The genre reached its commercial, artistic and influential peak in the mid-1980s, with Springsteen's "Born in the USA" (1984), topping the charts worldwide and spawning a series of top ten singles, together with the arrival of artists including John Mellencamp, Steve Earle and more gentle singer-songwriters such as Bruce Hornsby. It can also be heard as an influence on artists as diverse as Billy Joel, Kid Rock and the Killers.

Heartland rock faded away as a recognized genre by the early 1990s, as rock music in general, and blue-collar and white working class themes in particular, lost influence with younger audiences, and as heartland's artists turned to more personal works. Many heartland rock artists continue to record today with critical and commercial success, most notably Bruce Springsteen, Tom Petty, and John Mellencamp, although their works have become more personal and experimental and no longer fit easily into a single genre. Newer artists whose music would perhaps have been labeled heartland rock had it been released in the 1970s or 1980s, such as Missouri's Bottle Rockets and Illinois' Uncle Tupelo, often find themselves labeled alt-country.

The term alternative rock was coined in the early 1980s to describe rock artists who did not fit into the mainstream genres of the time. Bands dubbed "alternative" had no unified style, but were all seen as distinct from mainstream music. Alternative bands were linked by their collective debt to punk rock, through hardcore, New Wave or the post-punk movements. Important alternative rock bands of the 1980s in the US included R.E.M., Hüsker Dü, Jane's Addiction, Sonic Youth, and the Pixies, and in the UK the Cure, New Order, the Jesus and Mary Chain, and the Smiths. Artists were largely confined to independent record labels, building an extensive underground music scene based on college radio, fanzines, touring, and word-of-mouth. They rejected the dominant synth-pop of the early 1980s, marking a return to group-based guitar rock.

Few of these early bands achieved mainstream success, although exceptions to this rule include R.E.M., the Smiths, and the Cure. Despite a general lack of spectacular album sales, the original alternative rock bands exerted a considerable influence on the generation of musicians who came of age in the 1980s and ended up breaking through to mainstream success in the 1990s. Styles of alternative rock in the U.S. during the 1980s included jangle pop, associated with the early recordings of R.E.M., which incorporated the ringing guitars of mid-1960s pop and rock, and college rock, used to describe alternative bands that began in the college circuit and college radio, including acts such as 10,000 Maniacs and the Feelies. In the UK Gothic rock was dominant in the early 1980s, but by the end of the decade indie or dream pop like Primal Scream, Bogshed, Half Man Half Biscuit and the Wedding Present, and what were dubbed shoegaze bands like My Bloody Valentine, Slowdive, Ride and Lush. Particularly vibrant was the Madchester scene, produced such bands as Happy Mondays, Inspiral Carpets and the Stone Roses. The next decade would see the success of grunge in the United States and Britpop in the United Kingdom, bringing alternative rock into the mainstream.

Disaffected by commercialized and highly produced pop and rock in the mid-1980s, bands in Washington state (particularly in the Seattle area) formed a new style of rock which sharply contrasted with the mainstream music of the time. The developing genre came to be known as "grunge", a term descriptive of the dirty sound of the music and the unkempt appearance of most musicians, who actively rebelled against the over-groomed images of other artists. Grunge fused elements of hardcore punk and heavy metal into a single sound, and made heavy use of guitar distortion, fuzz and feedback. The lyrics were typically apathetic and angst-filled, and often concerned themes such as social alienation and entrapment, although it was also known for its dark humor and parodies of commercial rock.

Bands such as Green River, Soundgarden, Melvins and Skin Yard pioneered the genre, with Mudhoney becoming the most successful by the end of the decade. Grunge remained largely a local phenomenon until 1991, when Nirvana's album "Nevermind" became a huge success, containing the anthemic song "Smells Like Teen Spirit". "Nevermind" was more melodic than its predecessors, by signing to Geffen Records the band was one of the first to employ traditional corporate promotion and marketing mechanisms such as an MTV video, in store displays and the use of radio "consultants" who promoted airplay at major mainstream rock stations. During 1991 and 1992, other grunge albums such as Pearl Jam's "Ten", Soundgarden's "Badmotorfinger" and Alice in Chains' "Dirt", along with the "Temple of the Dog" album featuring members of Pearl Jam and Soundgarden, became among the 100 top-selling albums. Major record labels signed most of the remaining grunge bands in Seattle, while a second influx of acts moved to the city in the hope of success. However, with the death of Kurt Cobain and the subsequent break-up of Nirvana in 1994, touring problems for Pearl Jam and the departure of Alice in Chains' lead singer Layne Staley in 1998, the genre began to decline, partly to be overshadowed by Britpop and more commercial sounding post-grunge.

Britpop emerged from the British alternative rock scene of the early 1990s and was characterised by bands particularly influenced by British guitar music of the 1960s and 1970s. The Smiths were a major influence, as were bands of the Madchester scene, which had dissolved in the early 1990s. The movement has been seen partly as a reaction against various U.S.-based, musical and cultural trends in the late 1980s and early 1990s, particularly the grunge phenomenon and as a reassertion of a British rock identity. Britpop was varied in style, but often used catchy tunes and hooks, beside lyrics with particularly British concerns and the adoption of the iconography of the 1960s British Invasion, including the symbols of British identity previously utilised by the mods. It was launched around 1993 with releases by groups such as Suede and Blur, who were soon joined by others including Oasis, Pulp, Supergrass, and Elastica, who produced a series of successful albums and singles. For a while the contest between Blur and Oasis was built by the popular press into the "Battle of Britpop", initially won by Blur, but with Oasis achieving greater long-term and international success, directly influencing later Britpop bands, such as Ocean Colour Scene and Kula Shaker. Britpop groups brought British alternative rock into the mainstream and formed the backbone of a larger British cultural movement known as Cool Britannia. Although its more popular bands, particularly Blur and Oasis, were able to spread their commercial success overseas, especially to the United States, the movement had largely fallen apart by the end of the decade.

The term post-grunge was coined for the generation of bands that followed the emergence into the mainstream and subsequent hiatus of the Seattle grunge bands. Post-grunge bands emulated their attitudes and music, but with a more radio-friendly commercially oriented sound. Often they worked through the major labels and came to incorporate diverse influences from jangle pop, pop-punk, alternative metal or hard rock. The term post-grunge originally was meant to be pejorative, suggesting that they were simply musically derivative, or a cynical response to an "authentic" rock movement. Originally, grunge bands that emerged when grunge was mainstream and were suspected of emulating the grunge sound were pejoratively labelled as post-grunge. From 1994, former Nirvana drummer Dave Grohl's new band, the Foo Fighters, helped popularize the genre and define its parameters.

Some post-grunge bands, like Candlebox, were from Seattle, but the subgenre was marked by a broadening of the geographical base of grunge, with bands like Los Angeles' Audioslave, and Georgia's Collective Soul and beyond the US to Australia's Silverchair and Britain's Bush, who all cemented post-grunge as one of the most commercially viable subgenres of the late 1990s. Although male bands predominated post-grunge, female solo artist Alanis Morissette's 1995 album "Jagged Little Pill", labelled as post-grunge, also became a multi-platinum hit. Post-grunge morphed during the late 1990s as post-grunge bands like Creed and Nickelback emerged. Bands like Creed and Nickelback took post-grunge into the 21st century with considerable commercial success, abandoning most of the angst and anger of the original movement for more conventional anthems, narratives and romantic songs, and were followed in this vein by newer acts including Shinedown, Seether, 3 Doors Down and Puddle of Mudd.

The origins of 1990s pop punk can be seen in the more song-oriented bands of the 1970s punk movement like Buzzcocks and the Clash, commercially successful new wave acts such as the Jam and the Undertones, and the more hardcore-influenced elements of alternative rock in the 1980s. Pop-punk tends to use power-pop melodies and chord changes with speedy punk tempos and loud guitars. Punk music provided the inspiration for some California-based bands on independent labels in the early 1990s, including Rancid, Pennywise, Weezer and Green Day. In 1994 Green Day moved to a major label and produced the album "Dookie", which found a new, largely teenage, audience and proved a surprise diamond-selling success, leading to a series of hit singles, including two number ones in the US. They were soon followed by the eponymous debut from Weezer, which spawned three top ten singles in the US. This success opened the door for the multi-platinum sales of metallic punk band the Offspring with "Smash" (1994). This first wave of pop punk reached its commercial peak with Green Day's "Nimrod" (1997) and The Offspring's "Americana" (1998).

A second wave of pop punk was spearheaded by Blink-182, with their breakthrough album "Enema of the State" (1999), followed by bands such as Good Charlotte, Simple Plan and Sum 41, who made use of humour in their videos and had a more radio-friendly tone to their music, while retaining the speed, some of the attitude and even the look of 1970s punk. Later pop-punk bands, including All Time Low, 5 Seconds Of Summer, the All-American Rejects and Fall Out Boy, had a sound that has been described as closer to 1980s hardcore, while still achieving commercial success.

In the 1980s the terms indie rock and alternative rock were used interchangeably. By the mid-1990s, as elements of the movement began to attract mainstream interest, particularly grunge and then Britpop, post-grunge and pop-punk, the term alternative began to lose its meaning. Those bands following the less commercial contours of the scene were increasingly referred to by the label indie. They characteristically attempted to retain control of their careers by releasing albums on their own or small independent labels, while relying on touring, word-of-mouth, and airplay on independent or college radio stations for promotion. Linked by an ethos more than a musical approach, the indie rock movement encompassed a wide range of styles, from hard-edged, grunge-influenced bands like the Cranberries and Superchunk, through do-it-yourself experimental bands like Pavement, to punk-folk singers such as Ani DiFranco. It has been noted that indie rock has a relatively high proportion of female artists compared with preceding rock genres, a tendency exemplified by the development of feminist-informed Riot Grrrl music. Many countries have developed an extensive local indie scene, flourishing with bands with enough popularity to survive inside the respective country, but virtually unknown outside them.

By the end of the 1990s many recognisable subgenres, most with their origins in the late 1980s alternative movement, were included under the umbrella of indie. Lo-fi eschewed polished recording techniques for a D.I.Y. ethos and was spearheaded by Beck, Sebadoh and Pavement. The work of Talk Talk and Slint helped inspire both post rock, an experimental style influenced by jazz and electronic music, pioneered by Bark Psychosis and taken up by acts such as Tortoise, Stereolab, and Laika, as well as leading to more dense and complex, guitar-based math rock, developed by acts like Polvo and Chavez. Space rock looked back to progressive roots, with drone heavy and minimalist acts like Spacemen 3, the two bands created out of its split, Spectrum and Spiritualized, and later groups including Flying Saucer Attack, Godspeed You! Black Emperor and Quickspace. In contrast, Sadcore emphasised pain and suffering through melodic use of acoustic and electronic instrumentation in the music of bands like American Music Club and Red House Painters, while the revival of Baroque pop reacted against lo-fi and experimental music by placing an emphasis on melody and classical instrumentation, with artists like Arcade Fire, Belle and Sebastian and Rufus Wainwright.

Alternative metal emerged from the hardcore scene of alternative rock in the US in the later 1980s, but gained a wider audience after grunge broke into the mainstream in the early 1990s. Early alternative metal bands mixed a wide variety of genres with hardcore and heavy metal sensibilities, with acts like Jane's Addiction and Primus utilizing progressive rock, Soundgarden and Corrosion of Conformity using garage punk, the Jesus Lizard and Helmet mixing noise rock, Ministry and Nine Inch Nails influenced by industrial music, Monster Magnet moving into psychedelia, Pantera, Sepultura and White Zombie creating groove metal, while Biohazard and Faith No More turned to hip hop and rap.
Hip hop had gained attention from rock acts in the early 1980s, including The Clash with "The Magnificent Seven" (1980) and Blondie with "Rapture" (1980). Early crossover acts included Run DMC and the Beastie Boys. Detroit rapper Esham became known for his "acid rap" style, which fused rapping with a sound that was often based in rock and heavy metal. Rappers who sampled rock songs included Ice-T, The Fat Boys, LL Cool J, Public Enemy and Whodini. The mixing of thrash metal and rap was pioneered by Anthrax on their 1987 comedy-influenced single "I'm the Man".

In 1990, Faith No More broke into the mainstream with their single "Epic", often seen as the first truly successful combination of heavy metal with rap. This paved the way for the success of existing bands like 24-7 Spyz and Living Colour, and new acts including Rage Against the Machine and Red Hot Chili Peppers, who all fused rock and hip hop among other influences. Among the first wave of performers to gain mainstream success as rap rock were 311, Bloodhound Gang, and Kid Rock. A more metallic sound"nu metal"was pursued by bands including Limp Bizkit, Korn and Slipknot. Later in the decade this style, which contained a mix of grunge, punk, metal, rap and turntable scratching, spawned a wave of successful bands like Linkin Park, P.O.D. and Staind, who were often classified as rap metal or nu metal, the first of which are the best-selling band of the genre.

In 2001, nu metal reached its peak with albums like Staind's "Break the Cycle", P.O.D's "Satellite", Slipknot's "Iowa" and Linkin Park's "Hybrid Theory". New bands also emerged like Disturbed, Godsmack and Papa Roach, whose major label début "Infest" became a platinum hit. Korn's long-awaited fifth album "Untouchables", and Papa Roach's second album "Lovehatetragedy", did not sell as well as their previous releases, while nu metal bands were played more infrequently on rock radio stations and MTV began focusing on pop punk and emo. Since then, many bands have changed to a more conventional hard rock, heavy metal, or electronic music sound.

From about 1997, as dissatisfaction grew with the concept of Cool Britannia, and Britpop as a movement began to dissolve, emerging bands began to avoid the Britpop label while still producing music derived from it. Many of these bands tended to mix elements of British traditional rock (or British trad rock), particularly the Beatles, Rolling Stones and Small Faces, with American influences, including post-grunge. Drawn from across the United Kingdom (with several important bands emerging from the north of England, Scotland, Wales and Northern Ireland), the themes of their music tended to be less parochially centered on British, English and London life and more introspective than had been the case with Britpop at its height. This, beside a greater willingness to engage with the American press and fans, may have helped some of them in achieving international success.

Post-Britpop bands have been seen as presenting the image of the rock star as an ordinary person and their increasingly melodic music was criticised for being bland or derivative. Post-Britpop bands like Travis from "The Man Who" (1999), Stereophonics from "Performance and Cocktails" (1999), Feeder from "Echo Park" (2001), and particularly Coldplay from their debut album "Parachutes" (2000), achieved much wider international success than most of the Britpop groups that had preceded them, and were some of the most commercially successful acts of the late 1990s and early 2000s, arguably providing a launchpad for the subsequent garage rock or post-punk revival, which has also been seen as a reaction to their introspective brand of rock.

Post-hardcore developed in the US, particularly in the Chicago and Washington, DC areas, in the early to mid-1980s, with bands that were inspired by the do-it-yourself ethics and guitar-heavy music of hardcore punk, but influenced by post-punk, adopting longer song formats, more complex musical structures and sometimes more melodic vocal styles.

Emo also emerged from the hardcore scene in 1980s Washington, D.C., initially as "emocore", used as a term to describe bands who favored expressive vocals over the more common abrasive, barking style. The early emo scene operated as an underground, with short-lived bands releasing small-run vinyl records on tiny independent labels. Emo broke into mainstream culture in the early 2000s with the platinum-selling success of Jimmy Eat World's "Bleed American" (2001) and Dashboard Confessional's "The Places You Have Come to Fear the Most" (2003). The new emo had a much more mainstream sound than in the 1990s and a far greater appeal amongst adolescents than its earlier incarnations. At the same time, use of the term emo expanded beyond the musical genre, becoming associated with fashion, a hairstyle and any music that expressed emotion. By 2003 post-hardcore bands had also caught the attention of major labels and began to enjoy mainstream success in the album charts. A number of these bands were seen as a more aggressive offshoot of emo and given the often vague label of screamo.

In the early 2000s, a new group of bands that played a stripped down and back-to-basics version of guitar rock, emerged into the mainstream. They were variously characterised as part of a garage rock, post-punk or new wave revival. Because the bands came from across the globe, cited diverse influences (from traditional blues, through New Wave to grunge), and adopted differing styles of dress, their unity as a genre has been disputed. There had been attempts to revive garage rock and elements of punk in the 1980s and 1990s and by 2000 scenes had grown up in several countries.

The commercial breakthrough from these scenes was led by four bands: the Strokes, who emerged from the New York club scene with their début album "Is This It" (2001); the White Stripes, from Detroit, with their third album "White Blood Cells" (2001); the Hives from Sweden after their compilation album "Your New Favourite Band" (2001); and the Vines from Australia with "Highly Evolved" (2002). They were christened by the media as the "The" bands, and dubbed "The saviours of rock 'n' roll", leading to accusations of hype. A second wave of bands that gained international recognition due to the movement included Black Rebel Motorcycle Club, the Killers, Interpol and Kings of Leon from the US, the Libertines, Arctic Monkeys, Bloc Party, Kaiser Chiefs and Franz Ferdinand from the UK, Jet from Australia, and the Datsuns and the D4 from New Zealand.

In the 2000s, as computer technology became more accessible and music software advanced, it became possible to create high quality music using little more than a single laptop computer. This resulted in a massive increase in the amount of home-produced electronic music available to the general public via the expanding internet, and new forms of performance such as laptronica and live coding. These techniques also began to be used by existing bands and by developing genres that mixed rock with digital techniques and sounds, including indie electronic, electroclash, dance-punk and new rave.

During the 2010s, rock music saw a decline in mainstream popularity, with hip hop music surpassing it as the most consumed musical genre in the United States in 2017. Critics in the latter half of the decade took notice of the genre's waning popularity, increasing vagueness, a perceived inability by newer artists to evolve the genre, and changing attitudes in music creation. Bill Flanagan, in an opinion piece to the New York Times in 2016, compared the state of rock during this period to the state of jazz in the early 1980s, 'slowing down and looking back'. Vice suggests that this decline in popularity could actually benefit the genre by attracting outsiders with 'something to prove and nothing to gain'.

Different subgenres of rock were adopted by, and became central to, the identity of a large number of sub-cultures. In the 1950s and 1960s, respectively, British youths adopted the Teddy Boy and Rocker subcultures, which revolved around US rock and roll. The counterculture of the 1960s was closely associated with psychedelic rock. The mid-1970s punk subculture began in the US, but it was given a distinctive look by British designer Vivienne Westwood, a look which spread worldwide. Out of the punk scene, the Goth and Emo subcultures grew, both of which presented distinctive visual styles.
When an international rock culture developed, it supplanted cinema as the major sources of fashion influence. Paradoxically, followers of rock music have often mistrusted the world of fashion, which has been seen as elevating image above substance. Rock fashions have been seen as combining elements of different cultures and periods, as well as expressing divergent views on sexuality and gender, and rock music in general has been noted and criticised for facilitating greater sexual freedom. Rock has also been associated with various forms of drug use, including the amphetamines taken by mods in the early to mid-1960s, through the LSD, mescaline, hashish and other hallucinogenic drugs linked with psychedelic rock in the late 1960s and early 1970s; and sometimes to cannabis, cocaine and heroin, all of which have been eulogised in song.

Rock has been credited with changing attitudes to race by opening up African-American culture to white audiences; but at the same time, rock has been accused of appropriating and exploiting that culture. While rock music has absorbed many influences and introduced Western audiences to different musical traditions, the global spread of rock music has been interpreted as a form of cultural imperialism. Rock music inherited the folk tradition of protest song, making political statements on subjects such as war, religion, poverty, civil rights, justice and the environment. Political activism reached a mainstream peak with the "Do They Know It's Christmas?" single (1984) and Live Aid concert for Ethiopia in 1985, which, while successfully raising awareness of world poverty and funds for aid, have also been criticised (along with similar events), for providing a stage for self-aggrandisement and increased profits for the rock stars involved.

Since its early development rock music has been associated with rebellion against social and political norms, most obviously in early rock and roll's rejection of an adult-dominated culture, the counterculture's rejection of consumerism and conformity and punk's rejection of all forms of social convention, however, it can also be seen as providing a means of commercial exploitation of such ideas and of diverting youth away from political action.

Professional women instrumentalists are uncommon in rock genres such as heavy metal although bands such as Within Temptation have featured women as lead singers with men playing instruments. According to Schaap and Berkers, "playing in a band is largely a male homosocial activity, that is, learning to play in a band is largely a peer-based ... experience, shaped by existing sex-segregated friendship networks. They note that rock music "is often defined as a form of male rebellion vis-à-vis female bedroom culture." (The theory of "bedroom culture" argues that society influences girls to not engage in crime and deviance by virtually trapping them in their bedroom; it was developed by a sociologist named Angela McRobbie.) In popular music, there has been a gendered "distinction between public (male) and private (female) participation" in music. "Several scholars have argued that men exclude women from bands or from the bands' rehearsals, recordings, performances, and other social activities". "Women are mainly regarded as passive and private consumers of allegedly slick, prefabricatedhence, inferiorpop music ..., excluding them from participating as high status rock musicians". One of the reasons that there are rarely mixed gender bands is that "bands operate as tight-knit units in which homosocial solidaritysocial bonds between people of the same sex ... plays a crucial role". In the 1960s rock music scene, "singing was sometimes an acceptable pastime for a girl, but playing an instrument ... simply wasn't done".

"The rebellion of rock music was largely a male rebellion; the womenoften, in the 1950s and '60s, girls in their teensin rock usually sang songs as personæ utterly dependent on their macho boyfriends ...". Philip Auslander says that "Although there were many women in rock by the late 1960s, most performed only as singers, a traditionally feminine position in popular music". Though some women played instruments in American all-female garage rock bands, none of these bands achieved more than regional success. So they "did not provide viable templates for women's on-going participation in rock". In relation to the gender composition of heavy metal bands, it has been said that "[h]eavy metal performers are almost exclusively male" "...at least until the mid-1980s" apart from "...exceptions such as Girlschool". However, "...now [in the 2010s] maybe more than ever–strong metal women have put up their dukes and got down to it", "carv[ing] out a considerable place for [them]selves." When Suzi Quatro emerged in 1973, "no other prominent female musician worked in rock simultaneously as a singer, instrumentalist, songwriter, and bandleader". According to Auslander, she was "kicking down the male door in rock and roll and proving that a female "musician" ... and this is a point I am extremely concerned about ... could play as well if not better than the boys".

An all-female band is a musical group in genres such as rock and blues which is exclusively composed of female musicians. This is distinct from a girl group, in which the female members are solely vocalists, though this terminology is not universally followed.





</doc>
<doc id="25424" url="https://en.wikipedia.org/wiki?curid=25424" title="Retronym">
Retronym

A retronym is a newer name for an existing thing that differentiates the original form or version from a more recent one. It is thus a word or phrase created to differentiate between two types, whereas previously (before there were two types) no clarification was required.

Advances in technology are often responsible for the coinage of retronyms. For example, the term "acoustic guitar" was coined at the advent of electric guitars; analog watches were renamed to distinguish them from digital watches once the latter were invented; association football was coined to distinguish from the later sports of Rugby football; and "push bike" was created to distinguish from motorbikes and motorized bicycles.

The first bicycles with two wheels of equal size were called "safety bicycles" because they were easier to handle than the then-dominant style that had one large wheel and one small wheel, which then became known as an "ordinary" bicycle. Since the end of the 19th century, most bicycles have been expected to have two equal sized wheels, and the other type has been renamed "penny-farthing" or "high-wheeler" bicycle.

The Atari Video Computer System platform was rebranded the "Atari 2600" (after its product code, CX-2600) in 1982 following the launch of its successor, the Atari 5200, and all hardware and software related to the platform were released under this new branding from that point on.

The original Game Boy was referred to as "Game Boy Classic" after the release of Game Boy Color. Another game console example is the original Xbox being referred to as the "Xbox 1" prior to the release of the Xbox One. After the Xbox One release, the first Xbox has been commonly referred to as the "original Xbox" instead.

The term "retronym", a neologism composed of the combining forms "retro-" (from Latin "retro"", "before")" + "-nym" (from Greek ónoma, "“name”"), was coined by Frank Mankiewicz in 1980 and popularized by William Safire in "The New York Times Magazine".

In 2000 "The American Heritage Dictionary" (4th edition) became the first major dictionary to include the word "retronym".



</doc>
<doc id="25427" url="https://en.wikipedia.org/wiki?curid=25427" title="Rush Limbaugh">
Rush Limbaugh

Rush Hudson Limbaugh III ( ; born January 12, 1951) is an American radio personality, conservative political commentator, author, and former television show host. He is best known as the host of his longtime radio show "The Rush Limbaugh Show", which entered national syndication on AM and FM radio stations in 1988. The show has aired live from Limbaugh's home studio in West Palm Beach, Florida since 1996. Limbaugh began his career in 1967 as a radio DJ at various stations in Pittsburgh and Missouri. After a brief hiatus, Limbaugh returned to radio in 1984 at KFBK-AM in Sacramento, California, adopting a talk, political commentary, and listener phone-in format to his show. In 1988, Limbaugh started at WABC-AM in New York City where he became a prominent media figure. 

In addition to his radio show, Limbaugh hosted a national television show from 1992 to 1996. He has written seven books; his first two, "The Way Things Ought to Be" (1992) and "See, I Told You So" (1993), made "The New York Times" Best Seller list. Limbaugh is among the highest-paid radio figures. In 2008, he signed an eight-year deal with Clear Channel Communications worth $400 million to continue his radio show on its network. In 2018, "Forbes" listed his earnings at $84.5 million , up slightly from 2017 when he was ranked as the 11th highest-earning celebrity. In 2015, "Talkers Magazine" estimated that Limbaugh's show attracted a cumulative weekly audience of 13.25 million listeners to become the most-listened-to radio show in the US. Limbaugh has mentioned his audience has continued to grow to 14 million listeners each day and 27 million each week. He is a critic of liberalism in the US and liberal bias in the widespread media.

Limbaugh was born on January 12, 1951 in Cape Girardeau, Missouri to parents Rush Hudson Limbaugh Jr. and Mildred Carolyn ("née" Armstrong) Limbaugh. He and his younger brother David were born into the Limbaugh family; his father was a lawyer and a U.S. fighter pilot who served in the China Burma India Theater of World War II. His mother was from Searcy, Arkansas. The name "Rush" was originally chosen for his grandfather to honor the maiden name of a family member, Edna Rush.

Limbaugh is partly of German ancestry. The family includes many lawyers, including his grandfather, father and brother; his uncle, Stephen N. Limbaugh Sr., was a federal judge in the United States District Court for the Eastern District of Missouri. His cousin, Stephen N. Limbaugh Jr., is a judge in the same court, appointed by George W. Bush. Limbaugh's grandfather, Rush Limbaugh Sr., was a Missouri prosecutor, judge, special commissioner, member of the Missouri House of Representatives in the 1930s and longtime president of the Missouri Historical Society.

In 1969, Limbaugh graduated from Cape Girardeau Central High School. He played football. During this time, at age 16 he worked his first radio job at KGMO-AM, a local radio station in Cape Girardeau. He used the airname Rusty Sharpe having found "Sharpe" in a telephone book. Limbaugh later cited Chicago DJ Larry Lujack as a major influence on him, "the only person I ever copied." Because of his parents' desire to see him attend college, he enrolled at Southeast Missouri State University but dropped out after two semesters. According to his mother, "he flunked everything [...] he just didn't seem interested in anything except radio." Biographer Zev Chafets believes that a large part of Limbaugh's life has been dedicated to gaining his father's respect and approval.

In February 1971, after dropping out of university, the 20-year-old Limbaugh accepted an offer to DJ at WIXZ-AM, a Top 40 station in McKeesport, Pennsylvania. He adopted the airname "Bachelor Jeff" Christie and worked afternoons before moving to morning drive. The station's general manager compared Limbaugh's style at this time to "early Imus." In 1973, after eighteen months at WIXZ, Limbaugh was fired from the station due to "personality conflict" with the program director. He then started a nighttime position at KQV-AM in Pittsburgh, succeeding Jim Quinn. In late 1974, Limbaugh was dismissed after new management put pressure on the program director to fire him. Limbaugh recalled the general manager telling him that he would never land success as an air personality and suggested a career in radio sales. After rejecting his only offer at the time, a position in Neenah, Wisconsin, Limbaugh returned to living with his parents in Cape Girardeau. During this time, he became a lifelong fan of the Pittsburgh Steelers.

In 1975, Limbaugh began an afternoon show at the Top 40 station KUDL in Kansas City, Missouri. He soon became the host of a public service talk program that aired on weekend mornings which allowed him to develop his style and present more controversial ideas. In 1977, he was let go from the station but remained in Kansas City to start an evening show at KFIX. The stint was short-lived, however, and disagreements with management led to his dismissal weeks after. By this time, Limbaugh had become disillusioned with radio and felt pressure to pursue a different career. He looked back on himself as "a moderate failure [...] as a deejay". In 1979, he accepted a part-time role in group sales for the Kansas City Royals baseball team which developed into a full-time position as director of group sales and special events. He worked from the Royals Stadium. There he developed a close friendship with then-Royals star third baseman and future Hall of Famer George Brett; the two remain close friends. Limbaugh claimed that business trips to Europe and Asia during this time developed his conservative views as he considered these countries having lower standards of living than the US.

In November 1983, Limbaugh returned to radio with a year's stint at KMBZ-AM in Kansas City. He decided to drop his on-air moniker and broadcast under his real name. He was fired from the station, but weeks later he landed a spot on KFBK-AM in Sacramento, California, replacing Morton Downey Jr. The show launched on October 14, 1984. The repeal of the Fairness Doctrine—which had required that stations provide free air time for responses to any controversial opinions that were broadcast—by the FCC on August 5, 1987 meant stations could broadcast editorial commentary without having to present opposing views. Daniel Henninger wrote, in a "Wall Street Journal" editorial, "Ronald Reagan tore down this wall (the Fairness Doctrine) in 1987 ... and Rush Limbaugh was the first man to proclaim himself liberated from the East Germany of liberal media domination."

In July 1988, after his success in Sacramento caught the attention of former ABC Radio President Edward McLaughlin, Limbaugh started a new show at WABC-AM in New York City. He debuted just weeks after the Democratic National Convention, and just weeks before the Republican National Convention. Limbaugh's radio home in New York City was the talk-formatted WABC (AM), and this remained his flagship station for many years, even after Limbaugh moved to West Palm Beach, Fla., from where he continues to broadcast his show. Limbaugh's show moved on January 1, 2014 to WABC's cross-town rival WOR (AM), its current New York outlet.

By 1990, Limbaugh had been on his Rush to Excellence Tour, a series of personal appearances in cities nationwide, for two years. For the 45 shows he completed that year alone, he was estimated to make around $360,000.

In December 1990, journalist Lewis Grossberger wrote in "The New York Times" that Limbaugh had "more listeners than any other talk show host" and described Limbaugh's style as "bouncing between earnest lecturer and political vaudevillian." Limbaugh's rising profile coincided with the Persian Gulf War, and his support for the war effort and his relentless ridicule of peace activists. The program was moved to stations with larger audiences, eventually being broadcast on over 650 radio stations nationwide.

In 1992, Democrat Bill Clinton was elected president of the United States. Limbaugh satirized the policies of Clinton and First Lady Hillary Clinton, as well as those of the Democratic Party. When the Republican Party won control of Congress in the 1994 midterm elections, the freshman Republican class awarded Limbaugh an honorary membership in their caucus believing he had a role in their success.

Limbaugh had publicized personal difficulties in the 2000s. In late 2001, he acknowledged that he had become almost completely deaf, although he continued his show. He was able to regain much of his hearing with the help of a cochlear implant in 2001.

In 2003, Limbaugh had a brief stint as a professional football commentator with ESPN. He resigned a few weeks into the 2003 NFL season after making comments about the press coverage for quarterback Donovan McNabb that caused controversy and accusations of racism on the part of Limbaugh. His comment about McNabb was:

I don't think he's been that good from the get-go. I think what we've had here is a little social concern in the NFL. I think the media has been very desirous that a black quarterback do well. They're interested in black coaches and black quarterbacks doing well. I think there's a little hope invested in McNabb and he got a lot of credit for the performance of his team that he really didn't deserve. The defense carried this team.

The sportwriter Peter King construed the comment as "boneheaded". The sports analyst Allen Barra wrote Limbaugh's viewpoint was shared by "many football fans and analysts" and "it is... absurd to say that the sports media haven't overrated Donovan McNabb because he's black".

In 2003, Limbaugh stated that he was addicted to pain medication, and sought treatment. In April 2006, Limbaugh turned himself in to authorities, on a warrant issued by the Palm Beach County state attorney's office, and was arrested "on a single charge of prescription fraud". His record was later expunged.

In 2013, news reports indicated that Cumulus Media, some of whose stations carried Limbaugh's program in certain major markets, including New York, Chicago, Dallas, Washington D.C. and Detroit, was considering dropping his show when its contract with Limbaugh expired at the end of that year, reportedly because the company believed that its advertising revenues had been hurt by listener reaction to controversial Limbaugh comments. Limbaugh himself said that the reports were overblown and that it was a matter of routine dollars-and-cents negotiations between Cumulus and his network syndication partner, Premiere Networks, a unit of Clear Channel Communications. Ultimately, the parties reached agreement on a new contract, with Limbaugh's show moving from its long-time flagship outlet in New York, the Cumulus-owned WABC, to the latter's cross-town rival, the Clear Channel-owned WOR, starting January 1, 2014, but remaining on the Cumulus-owned stations it was being carried on in other markets.

Limbaugh's radio show airs for three hours each weekday beginning at noon Eastern Time on both AM and FM radio. The program is also broadcast worldwide on the Armed Forces Radio Network.

Radio broadcasting shifted from AM to FM in the late 1970s because of the opportunity to broadcast music in stereo with better fidelity. Limbaugh's show was first nationally syndicated in August 1988, in a later stage of AM's decline. Limbaugh's popularity paved the way for other conservative talk radio programming to become commonplace on AM radio. The show increased its audience in the 1990s to the extent that even some FM stations picked it up, even though AM's poor sound quality and lack of stereo make AM preferable for a talk show like Limbaugh's. about half of Limbaugh's affiliate stations are on the FM dial.

In March 2006, WBAL in Baltimore became the first major market radio station in the country to drop Limbaugh's nationally syndicated radio program. In 2007, "Talkers" magazine again named him No.1 in its "Heavy Hundred" most important talk show hosts.

Limbaugh frequently mentions the EIB (Excellence In Broadcasting) Network, trademarked in 1990. In the beginning, his show was co-owned and first syndicated by Edward F. McLaughlin, former president of ABC, who founded EFM Media in 1988, with Limbaugh's show as his first product. In 1997, McLaughlin sold EFM to Jacor Communications, which was ultimately bought up by Clear Channel Communications. Today, Limbaugh owns a majority of the show, which is syndicated by the Premiere Radio Networks.

According to a 2001 article in "U.S. News & World Report", Limbaugh had an eight-year contract, at the rate of $31.25 million a year. In 2007, Limbaugh earned $33 million. A November 2008 poll by Zogby International found that Rush Limbaugh was the most trusted news personality in the nation, garnering 12.5 percent of poll responses.

Limbaugh signed a $400 million, eight-year contract in 2008 with what was then Clear Channel Communications, making him the highest-paid broadcaster on terrestrial radio. On August 2, 2016, Limbaugh signed a four-year extension of the 2008 contract. At the announcement of the extension, Premiere Radio Networks and iHeartMedia announced that his show experienced audience growth with 18% growth in adults 25–54, 27% growth with 25–54 women, and ad revenue growth of 20% year over year.

In 2018, Limbaugh was the world's second (behind Howard Stern) highest-paid radio host, reportedly earning $84.5 million. 

On January 5, 2020, Limbaugh renewed his contract again. Though media reports said it was "a long-term" renewal, (with no length specified), according to Donald Trump it was a four-year deal.

Limbaugh had a syndicated half-hour television show from 1992 through 1996, produced by Roger Ailes. The show discussed many of the topics on his radio show, and was taped in front of an audience. Rush Limbaugh says he loves doing his radio show, but not a TV show.

Limbaugh's first television hosting experience came March 30, 1990, as a guest host on Pat Sajak's CBS late-night talk show, "The Pat Sajak Show". ACT UP activists in the audience heckled Limbaugh repeatedly; ultimately the entire studio audience was cleared. In 2001, Sajak said the incident was "legendary around CBS".

On December 17, 1993, Limbaugh appeared on the "Late Show with David Letterman". Limbaugh also guest-starred (as himself) on a 1994 episode of "Hearts Afire". He appeared in the 1995 Billy Crystal film "Forget Paris", and in 1998 on an episode of "The Drew Carey Show".

In 2007, Limbaugh made cameo appearances on Fox News Channel's short-lived "The 1/2 Hour News Hour" in a series of parodies portraying him as the future President of the United States. In the parodies, his vice president was fellow conservative pundit Ann Coulter. That year, he also made a cameo in the "Family Guy" episode "Blue Harvest", a parody of "Star Wars" in which Limbaugh can be heard on the radio claiming that the "liberal galactic media" were lying about climate change on the planet Hoth, and that Lando Calrissian's administrative position on Cloud City was a result of affirmative action. More recent "Family Guy" appearances have happened in the 2010 episode "Excellence in Broadcasting", and 2011's "Episode VI: It's a Trap!", a parody of "Return of the Jedi".

Limbaugh has become widely recognized as one of the premiere voices of the conservative movement in the United States since the 1990s. In a 1992 letter, President Reagan thanked him, "for all you're doing to promote Republican and conservative principles... [and] you have become the Number One voice for conservatism in our Country." In 1994, Republicans in the U.S. House of Representatives made Limbaugh an honorary member.

In 1995, Rush Limbaugh was profiled on the PBS series "Frontline" in a one-hour documentary called "Rush Limbaugh's America." Limbaugh refused to be interviewed, but his mother, brother and many Republican supporters took part, as well as critics and opponents.

Since the 1990s, Limbaugh has become known for his love of cigars, saying, "I think cigars are just a tremendous addition to the enjoyment of life." During his syndicated television program from 1992 to 1996, he also become known for wearing distinctive neckties. In response to viewer interest, Limbaugh launched a series of ties designed primarily by his then-wife Marta. Limbaugh is also known for using props, songs and photos to introduce his monologues on various topics. On his radio show, news about the homeless has often been preceded with the Clarence "Frogman" Henry song "Ain't Got No Home." For a time, Dionne Warwick's song, "I Know I'll Never Love This Way Again" preceded reports about people with HIV/AIDS. These later became "condom updates" preceded by Fifth Dimension's song, "Up, Up and Away". For two weeks in 1989, on his Sacramento radio show, Limbaugh performed "caller abortions" where he would end a call suddenly to the sounds of a vacuum cleaner and a scream. He would then deny that he had "hung up" on the caller, which he had promised not to do. Limbaugh claims that he used this gag to illustrate "the tragedy of abortion" as well as to highlight the question of whether abortion constitutes murder. During the Clinton administration, while filming his television program, Limbaugh referred to media coverage of Socks, the Clintons' cat. He then stated, "But did you know there is also a White House dog?" and a picture of Chelsea Clinton was shown. When questioned about it, Limbaugh claimed that it was an accident and that without his permission some technician had put up the picture of Chelsea.

Limbaugh was awarded the Marconi Radio Award for Syndicated Radio Personality of the Year by the National Association of Broadcasters five times – 1992, 1995, 2000, 2005, and 2014 (given by the National Association of Broadcasters). He was inducted into the National Radio Hall of Fame in 1993 and the National Association of Broadcasters Hall of Fame in 1998. By 2001, he inked a $285 million contract for eight years, which was renewed in 2008 for another eight years at $400 million. By 2017, Limbaugh was the second highest paid radio host in the United States, earning an annual salary of $84 million – second only to Howard Stern. "Talkers Magazine" ranked him as the greatest radio talk show host of all time in 2002, and in 2017, he was the most-listened-to radio host in the United States with 14 million listeners.

Limbaugh was awarded the inaugural William F. Buckley Jr. Award for Media Excellence by the Media Research Center, a conservative media analysis group in 2007. Conservative magazine "Human Events" also announced Limbaugh as their 2007 Man of the Year. Later that same year, Barbara Walters featured Limbaugh as one of the most fascinating people of the year in a special that aired on December 4, 2008.

On February 28, 2009, following his self-described "first address to the nation" lasting 90 minutes, carried live on CNN and Fox News and recorded for C-SPAN, Limbaugh received CPAC's "Defender of the Constitution Award", a document originally signed by Benjamin Franklin, given to someone "who has stood up for the First Amendment ... Rush Limbaugh is for America, exactly what Benjamin Franklin did for the Founding Fathers ... the only way we will be successful is if we listen to Rush Limbaugh."

In his 2010 book, "Rush Limbaugh: An Army of One", Ze'ev Chafets cited Limbaugh as, "the brains and the spirit behind" the Republican Party's resurgence in the 2010 midterm elections in the wake of the election of President Obama. Chafets pointed, among others, to Sen. Arlen Specter's defeat, after being labeled by Limbaugh as a "Republican in Name Only", and to Sarah Palin, whose "biggest current applause line – Republicans are not just the party of no, but the party of hell no – came courtesy of Mr. Limbaugh." Limbaugh has argued the party-of-no Ronald Reagan conservative course for the Republicans vigorously, notably since six weeks after the Obama inauguration, and has been fundamental to, and encouraging to, the more prominently noted Tea Party movement.

Rush Limbaugh was inducted into the Hall of Famous Missourians on May 14, 2012, in a secret ceremony announced only 20 minutes before it began to prevent negative media attention. A bronze bust of Limbaugh is on display at the Missouri State Capitol building in Jefferson City, along with 40 other awardees. Limbaugh's bust includes a security camera to prevent vandalism.

In his first "New York Times" best seller, Limbaugh describes himself as conservative, and is critical of broadcasters in many media outlets for claiming to be objective. He has criticized political centrists, independents, and moderate conservatives, claiming they are responsible for Democrat Barack Obama's victory over Republican John McCain in the 2008 United States presidential election and inviting them to leave the Republican Party. He calls for the adoption of core conservative philosophies in order to ensure the survival of the Republican Party. Limbaugh is a proponent of American exceptionalism, and he often criticizes politicians he sees as rejecting this notion as unpatriotic or anti-American.

Limbaugh considers "Roe v. Wade" "bad law" and supports overturning it. He has compared support for abortion with Nazism, saying that abortion is "a modern-day holocaust" and that for feminists abortion is "a kind of sacrament for their religion/politics of alienation and bitterness". During the 2008 Republican Party vice presidential candidate selection Limbaugh strongly opposed Tom Ridge due to his pro-abortion views.

Limbaugh is known for making controversial race-related statements with regard to African-Americans. He once opined that all newspaper composite pictures of wanted criminals resembled Jesse Jackson, and another time that "the NFL all too often looks like a game between the Bloods and the Crips without any weapons." While employed as what he describes as an "insult-radio" DJ, he used a derogatory racial stereotype to characterize a black caller he could not understand, telling the caller to "take that bone out of your nose and call me back," although he expressed guilt over this when recounting it. In March 2010, Limbaugh used the similarity of recently resigned Rep. Eric Massa's surname to the slavery-era African-American pronunciation of "master" to make a pun on the possibility that Gov. David Paterson, New York's first African-American governor, would pick Massa's replacement: "Let's assume you're right [caller]. So, David Paterson will become the massa who gets to appoint whoever gets to take Massa's place. So, for the first time in his life, Paterson's gonna be a massa. Interesting, interesting."

Limbaugh has asserted that African-Americans, in contrast with other minority groups, are "left behind" socially because they have been systematically trained from a young age to hate the United States because of the welfare state.

Limbaugh has argued that liberal politicians have encouraged immigration from Latin America but have discouraged their assimilation to deliberately create racial inequality to manipulate as a voter base, and that their continued admission will cause a collapse of representative democracy and rule of law in the United States. He has criticized the Immigration and Nationality Act of 1965 for this reason.

Limbaugh, who has expressed anti-LGBT rhetoric in the past and views homosexual sexual practices as unhygienic, made serophobic statements about HIV/AIDS victims in the 1990s, and called the virus "Rock Hudson's disease" and "the only federally-protected virus." Limbaugh claimed in 2007 while defending President Reagan's response to the HIV/AIDS epidemic during the 1980s that it did not "spread to the heterosexual community." Limbaugh, who still opposes homosexuality, has since called his statements "the single most regretful thing I have ever done." In 2013, Limbaugh commented on same-sex marriage by saying, "This issue is lost. I don’t care what the Supreme Court does. This is inevitable. And it’s inevitable because we lost the language on this. As far as I’m concerned, once we started talking about gay marriage, traditional marriage, opposite-sex marriage, same-sex marriage, hetero marriage, we lost. It was over.”

Limbaugh supports capital punishment. Referring to Robert Alton Harris, who was escorted to the gas chamber before receiving a fourth stay of execution, Limbaugh wrote "the only thing cruel about the death penalty is last-minute stays."

Limbaugh dismisses the concept of consent in sexual relations. He views consent as "the magic key to the left." In 2014, Limbaugh criticized a policy at Ohio State University encouraging students to obtain verbal consent, saying "How many of you guys . . . have learned that 'no' means 'yes' if you know how to spot it?” The Democratic Congressional Campaign Committee used these statements to advocate a boycott of Limbaugh's show and advertisers, claiming that the statements were tantamount to an endorsement of sexual assault. Limbaugh denied this, and his spokesman Brian Glicklick and lawyer Patricia Glaser threatened a defamation lawsuit against the DCCC .

Limbaugh has been an outspoken critic of what he sees as leniency towards criminal drug use in America. On his television show on October 5, 1995, Limbaugh stated, "too many whites are getting away with drug use" and illegal drug trafficking. Limbaugh proposed that the racial disparity in drug enforcement could be fixed if authorities increased detection efforts, conviction rates, and jail time for whites involved in illegal drugs. He defended mandatory-minimum sentencing as an effective tool against the crack cocaine epidemic of the 1980s. Limbaugh has accused advocates of legalization of non-medical cannabis in the United States of hypocrisy due to their advocacy of tobacco control and backlash against electronic cigarettes, and compared the advocates for its legalization in Colorado to Big Tobacco.

Limbaugh is critical of environmentalism and climate science. He rejects the scientific consensus on climate change, and the relationship between CFCs and depletion of the ozone layer, saying the scientific evidence does not support them. Limbaugh has argued against the scientific consensus on climate change saying it is "just a bunch of scientists organized around a political proposition." He has also argued that projections of climate change are the product of ideologically-motivated computer simulations without the proper support of empirical data, a claim which has been widely debunked. Limbaugh has used the term "environmentalist wacko" when referring to left-leaning environmental advocates. As a rhetorical device, he has also used the term to refer to more mainstream climate scientists and other environmental scientists and advocates with whom he disagrees. Limbaugh opposed pollution credits, including a carbon cap-and-trade system, as a way to disproportionately benefit major American investment banks, particularly Goldman Sachs, and claimed that it would destroy the American national economy.

Limbaugh has written that "there are more acres of forestland in America today than when Columbus discovered the continent ["sic"] in 1492," a claim that is disputed by the United States Forest Service and the American Forestry Association, which state that the precolonial forests have been reduced by about 24 percent or nearly 300 million acres.

Limbaugh strongly opposed the proposed Green New Deal and its sponsor Alexandria Ocasio-Cortez.

Limbaugh is critical of feminism, which he views as advancing only liberals and not women in general. During an interview with "Time" magazine during the 1992 presidential election he stated that it "was established so as to allow unattractive women easier access to the mainstream of society." He has criticized Democratic congressmen calling for more women in Congress as hypocritical due to their opposition to female Republican candidates. He has also regularly used the term "feminazi", described by "The New York Times" in 1994 as one of his "favorite epithets for supporters of women's rights". According to Limbaugh in 1992, for certain feminists, the "most important thing in life is ensuring that as many abortions as possible occur." He also used the term referring to the half-million large 2017 Women's March as the "Deranged Feminazi March". He credited his friend Tom Hazlett, a professor of law and economics at George Mason University, with coining the term.

Limbaugh first rose to prominence in 1991 for his vocal support for the Persian Gulf War and criticism of opponents of the war. Limbaugh later accused the media, in particular Sam Donaldson, of deliberately overestimating in their predictions of the amount of American casualties caused by the war and overstating the Iraqi Armed Forces's military preparedness. 

Limbaugh was supportive of the Iraq War, and first suggested bombing Ba'athist Iraq in 2002 in revenge for the September 11 attacks. Even after no Iraqi weapons of mass destruction were found, he supported theories that they had existed. On the Abu Ghraib torture and prisoner abuse scandal, Limbaugh said, "This is no different than what happens at the Skull and Bones initiation ... And we're going to ruin people's lives over it and we're going to hamper our military effort, and then we are going to really hammer them because they had a good time." Speaking at the 2009 Conservative Political Action Conference, Limbaugh accused Democratic congressional leaders such as Harry Reid of deliberately undermining the war effort.

In 2018, Limbaugh speculated that evidence of Iraqi weapons of mass destruction had been fabricated by the U.S. intelligence community to embarrass President Bush.

During the 2019-20 Persian Gulf crisis, Limbaugh praised the 2020 Baghdad International Airport drone strike that resulted in the death of the Islamic Revolutionary Guard Corps's commander Major General Qasem Soleimani, and accused opponents of the strike of supporting Iran over the United States. On January 6, 2020, he held an interview with President Donald Trump on his show commending him for the strike. 

In 1993, Limbaugh supported the North American Free Trade Agreement, joking in response to claims that it would lead to a transfer of unskilled labor to Mexico that this would only leave the United States with better jobs. During a 1993 televised debate against H. Ross Perot over NAFTA, Vice President Al Gore complimented Limbaugh as one of the "distinguished Americans" who pushed NAFTA forward in spite of the intense animosity between Limbaugh and the administration of President Bill Clinton. He later became more critical of NAFTA and trade agreements in general, claiming that they had reduced national sovereignty by "subordinating" America to "world tribunals, like the World Trade Organization and the International Criminal Court and this kind of thing." He also claimed that promises to stem mass migration by invigorating the Latin American economy had failed. He supported a renegotiation of NAFTA and the eventual United States–Mexico–Canada Agreement.

Limbaugh defended the Trump tariffs and the China–United States trade war as a legitimate response to predatory Chinese trade practices and its Communist command economy.

Rush Limbaugh strongly opposed Barack Obama during the 2008 presidential election, and famously urged Republicans to vote for his chief rival Hillary Rodham Clinton in the Democratic Party primaries to prevent his nomination in an operation he referred to as "Operation Chaos." Limbaugh later predicted that Obama would be unable to win the election. On January 16, 2009, Limbaugh commented on the then-upcoming Obama presidency, "I hope he fails." Limbaugh later said that he wants to see Obama's "policies" fail, not the man himself. Speaking of Obama, Limbaugh said, "He's my president, he's a human being, and his ideas and policies are what count for me." Limbaugh later discouraged efforts to impeach Barack Obama as politically unrealistic.

Limbaugh accused Obama of using his race to prevent criticism of his policies, and said he was successful in his first year in office only because conservative members of the 111th Congress feared accusations of racism. Limbaugh featured a recurring skit in which his colleague James Golden, who described himself as an "African-American-in-good-standing-and-certified-black-enough-to-criticize-Obama guy," appeared in a cameo as the "Official EIB Obama Criticizer."

Limbaugh blamed Obama's foreign policy, including the withdrawal of U.S. troops from Iraq, for allowing the rise of the Islamic State of Iraq and the Levant. Limbaugh also claimed that the 2012 Benghazi attack occurred due to a secret arms trafficking operation to the Syrian opposition authorized by Obama and coordinated by Ambassador J. Christopher Stevens, speculating that the 2016 Democratic National Committee email leak would reveal evidence of it. Limbaugh also criticized the Russian reset, seeing Vladimir Putin's rule in the Russian Federation as a thinly-veiled continuation of the Soviet Union and Marxism–Leninism. He was also critical of the Joint Comprehensive Plan of Action, including of Obama's decision to ratify it as an executive agreement, and claims that it was used as a pretext for surveillance against Obama's political opponents. Limbaugh argued that side agreements of the JCPOA limited transparency and would obligate the United States to militarily defend Iran against an Israeli offensive, including a preemptive strike to prevent nuclear weapons development.

During the West African Ebola virus epidemic, Limbaugh blamed Obama for allowing the spread of the disease to the United States in 2014, claiming that he should have stopped air travel to West Africa. He claimed that both the media and the government, including the Centers for Disease Control and Prevention, deliberately downplayed its symptoms, expressing skepticism over the scientific consensus that the disease could be spread only through contact with bodily fluids and was not aerosol transmissible. When David Quammen criticized the idea of ending air travel to West Africa by pointing out that Liberia was founded due to slavery in the United States on "Anderson Cooper 360°", Limbaugh suggested in response that the Obama administration was deliberately allowing Ebola to be transmitted to the United States due to its guilt over slavery, stating "People at the highest levels of our government say 'Why, why shouldn't we get it? Why should only those three nations in Africa get it? We're no better than they are.' And they have this attitude, 'Well, if they have it in Africa, by God, we deserve to get it, because they're in Africa because of us and because of slavery.'"

Limbaugh has been consistently supportive of the candidacy and presidency of Donald Trump, although he endorsed Ted Cruz during the 2016 Republican Party presidential primaries and took issue with Trump's treatment of Cruz. Limbaugh later criticized Cruz's hesitance to endorse Trump after his nomination at the 2016 Republican National Convention, comparing it to Ted Kennedy's lukewarm support of Jimmy Carter at the 1980 Democratic National Convention. After the election he became supportive of deep-state conspiracy theories, claiming that the United States has entered a "Cold Civil War" in which the Democratic Party is attempting to illegitimately overturn the election results and that it is part of a trend of Democrats contesting elections beginning with the 2000 Florida election recount intended to eventually eliminate free elections in the United States.

In December 2018, Limbaugh criticized Trump for preparing to accept a continuing resolution that would fund the government through February 8, 2019, but included no funding for a border wall on the Mexico–United States border, a campaign promise repeatedly emphasized by Trump. Trump would subsequently make a surprise telephone call to Limbaugh announcing his intent to veto the bill, a decision that would lead to the 2018–19 United States federal government shutdown. Limbaugh would go on to support the shutdown, stating "We have a president keeping promises left and right. And isn't it interesting to see how trivial Washington thinks that is?” After Trump declared the National Emergency Concerning the Southern Border of the United States and the 116th Congress failed in its attempt to override it, Limbaugh called on him to completely close the border with Mexico.

Limbaugh has been dismissive of controversies over links between Trump associates and Russian officials. He claims that the FBI investigations of Michael Flynn and Paul Manafort as well as the subsequent Special Counsel investigation directed by Robert Mueller were orchestrated by Barack Obama and the Democratic Party to undermine the legitimacy of Trump's presidency and constituted an illegal coup d'état. Limbaugh claimed that George Papadopoulos was entrapped by the FBI, which he claims Joseph Mifsud was an informant for, through Stefan Halper as part of an "insurance policy" against Trump's election by the Five Eyes intelligence alliance. Limbaugh has advocated a full presidential pardon for all suspects indicted or convicted by the investigation. After the release of the Mueller Report, he disputed its conclusion that WikiLeaks obtained the Democratic National Committee's emails from the Russian government and its depiction of Donald Trump Jr.'s Trump Tower meeting. He claimed that allegations of obstruction of justice were leveled at Trump due to the Report's conclusion that Trump did not directly collude with Russian officials and that Trump's intent to fire Mueller and Attorney General Jeff Sessions were legitimate.

Limbaugh supported the Presidential Advisory Commission on Election Integrity as well as Trump's claims that he lost the popular vote due to voter impersonation by illegal immigrants.

After the House of Representatives commenced a formal impeachment inquiry against President Trump due to the scandal over a 2019 telephone call to Ukrainian President Volodomyr Zelensky pressuring his government to prosecute Biden shortly after a freeze of military aid, Limbaugh argued that the two events were unrelated since Trump had made a decision to withhold military funds a month in advance. He additionally claimed that Trump's desire for the Ukrainian government to prosecute Biden was legally justified by a 1999 mutual legal assistance treaty with Ukraine and "was following the law to the letter when it comes to unearthing the long-standing corruption that has swirled in Ukraine and allegedly involves powerful Democrats like Joe Biden."

In 2010, after the "Deepwater Horizon" oil spill in the Gulf of Mexico, Limbaugh speculated on his show that eco-terrorists deliberately destroyed the oil well to justify President Obama's deepwater drilling moratorium. Limbaugh also claimed that the media was exaggerating the environmental effects of the disaster. 

After the Unite the Right rally and vehicle-ramming attack in Charlottesville, Virginia, Limbaugh defended Trump's controversial response to the rally and claimed that the violence had been provoked by Black Lives Matter activists, Antifa, and Robert Creamer. He also claimed without evidence that the police response had been deliberately restrained by Terry McAuliffe as a botched attempt to start a presidential bid in the 2020 Democratic Party presidential primaries, and that it was part of a campaign by "international financiers" such as George Soros to start a second civil war in the United States to remove its status as a global superpower. After attention on Trump's comments renewed when Joe Biden criticized them in the announcement of his 2020 presidential campaign, Limbaugh again defended them by repeating claims that some of the protesters were not white supremacists and were protesting the removal of the statue of Robert E. Lee.

Limbaugh claimed that the October 2018 United States mail bombing attempts were perpetuated as a false-flag operation to draw public attention away from Central American migrant caravans. He reiterated these claims two weeks after the arrest of the primary suspect Cesar Sayoc, a registered Republican.

On his show, Limbaugh has said that the Christchurch mosque shootings of March 2019 may have been a false-flag operation. Limbaugh described "an ongoing theory" that the shooter was actually "a leftist" trying to smear the right. Despite providing no source or evidence, Limbaugh continued: "...you can't immediately discount this. The left is this insane, they are this crazy."

The July–August 1994 issue of "Extra!", a publication of Fairness and Accuracy in Reporting (FAIR), alleges 50 different inaccuracies and distortions in Limbaugh's commentary. Comedian Al Franken, who later became a Senator, wrote a satirical book ("Rush Limbaugh Is a Big Fat Idiot and Other Observations") in which he accused Limbaugh of distorting facts to serve his own political biases.

Of Limbaugh's controversial statements and allegations they have investigated, Politifact has rated 84% as ranging from "Mostly False" to "Pants-On-Fire" (a signification for extremely false), with 5% of Limbaugh's contested statements rising to the level of "Mostly True" and 0% rated "True." These debunked allegations by Limbaugh include suggestions that the existence of gorillas disproves the theory of evolution, that Ted Kennedy sent a letter to Soviet General Secretary Yuri Andropov seeking to undercut President Reagan, that a recent lack of hurricanes disproves climate change, and that President Obama wanted to mandate circumcision.

Limbaugh has been criticized for inaccuracies by the Environmental Defense Fund. A defense fund report authored by Princeton University endowed geoscience professor Michael Oppenheimer and professor of biology David Wilcove lists 14 significant scientific facts that, the authors allege, Limbaugh misrepresented in his book "The Way Things Ought to Be". The authors conclude that "Rush Limbaugh ... allows his political bias to distort the truth about a whole range of important scientific issues."

On October 14, 2011, Limbaugh questioned the U.S. military initiative against Joseph Kony and his Lord's Resistance Army (LRA), based on the assumption that they were Christians. "They are fighting the Muslims in Sudan. And Obama has sent troops, United States troops to remove them from the battlefield, which means kill them." Upon learning about the accusations leveled against Kony, which included kidnapping whole schools of young children for use as child soldiers, Limbaugh stated that he would research the group. The show's written transcript on his website was not changed.

In October 2006, Limbaugh said Michael J. Fox, who suffers from Parkinson's disease, had exaggerated the effects of his affliction in a political TV advertisement advocating for funding of stem cell research. Limbaugh said that Fox in the ad had been "shameless" in "moving all around and shaking", and that Fox had not taken "his medication or he's acting, one of the two". Fox said "the irony of it is I was too medicated," adding that there was no way to predict how his symptoms would manifest. Limbaugh said he would apologize to Fox "bigly, hugely... if I am wrong in characterizing his behavior on this commercial as an act." In 2012, Fox said Limbaugh in 2006 had acted on "bullying instincts" when "he said I faked it. I didn't fake it," and said Limbaugh's goal was to have him marginalized and shut down for his stem cell stance.

In 2007, Media Matters' reported that Limbaugh had categorized Iraq War veterans opposed to the war as "the phony soldiers." Limbaugh later said that he was speaking of Jesse MacBeth, a soldier who falsely claimed to have been decorated for valor but, in fact, had never seen combat. Limbaugh said Media Matters was trying to smear him with out-of-context and selectively edited comments. After Limbaugh published what he claimed was the entire transcript of phony soldiers discussion, Media Matters said that over a minute and 30 seconds of the transcript was omitted without "notation or ellipsis to indicate that there is, in fact, a break in the transcript." Limbaugh said during the minute and a half gap Media Matters had pointed out, he was waiting for relevant ABC news copy on the topic, and the transcript and audio edits were "for space and relevance reasons, not to hide anything." Senator Harry Reid and 41 Democrats, including Hillary Clinton, signed a letter asking the CEO of Clear Channel to denounce Limbaugh. Instead, he gave the letter to Limbaugh to auction. It raised over $2 million for the Marine Corps-Law Enforcement Foundation.

On February 29, 2012, Limbaugh, while talking about contraceptive mandates, included remarks about law student Sandra Fluke as a "slut" and "prostitute." Limbaugh was commenting on Fluke's speech the previous week to House Democrats in support of mandating insurance coverage for contraceptives. Limbaugh made numerous similar statements over the next two days, leading to the loss of 45 to "more than 100" local and national sponsors and Limbaugh's apology on his show for some of his comments. Susan McMillan Emry co-organized a public relations campaign called Rock the Slut Vote as a response to Limbaugh's remarks.

Limbaugh holds an annual fundraising telethon called the "EIB Cure-a-Thon" for the Leukemia & Lymphoma Society. In 2006, the EIB Cure-a-Thon conducted its 16th annual telethon, raising $1.7 million, totaling over $15 million since the first cure-a-thon. According to Leukemia and Lymphoma Society annual reports, Limbaugh personally contributed between $100,000 and $499,999 from 2000–2005 and 2007, and Limbaugh said that he contributed around $250,000 in 2003, 2004 and 2005. NewsMax reported Limbaugh donated $250,000 in 2006, and the Society's 2006 annual report placed him in the $500,000 to $999,999 category. Limbaugh donated $320,000 during the 2007 Cure-a-Thon, which the Leukemia and Lymphoma Society reported had raised $3.1 million. On his radio program April 18, 2008, Limbaugh pledged $400,000 to the Leukemia and Lymphoma Society after being challenged by two listeners to increase his initial pledge of $300,000.

Limbaugh conducts an annual drive to help the Marine Corps–Law Enforcement Foundation collect contributions to provide scholarships for children of Marines and law enforcement officers and agents who have died in the line of duty. The foundation was the beneficiary of a record $2.1 million eBay auction in October 2007 after Limbaugh listed for sale a letter critical of him signed by 41 Democratic senators and pledged to match the selling price.
With the founding of his and his wife's company Two if by Tea, they pledged to donate at least $100,000 to the MC–LEF beginning in June 2011.

In July 2019 Nike announced a special Fourth of July edition of their Air Max1 Quick Strike sneaker that featured the thirteen-star Betsy Ross flag. The company withdrew the sneaker after their spokesman Colin Kaepernick raised concerns that the symbol represented an era of black enslavement. In response Limbaugh's radio program introduced a t-shirt imprinted "Stand up for Betsy Ross" with sale proceeds to benefit the Tunnel to Towers Foundation. As of December 2019 the sales have earned over $5M USD for the foundation.

Limbaugh has had four marriages, three divorces, and no children. He was first married at the age of 26 to Roxy Maxine McNeely, a sales secretary at radio station WHB in Kansas City, Missouri. The couple married at the Centenary United Methodist Church in Limbaugh's hometown of Cape Girardeau on September 24, 1977. McNeely filed for divorce in March 1980, citing "incompatibility." They were formally divorced on July 10, 1980.

In 1983, Limbaugh married Michelle Sixta, a college student and usherette at the Kansas City Royals Stadium Club. They divorced in 1990, and she remarried the following year.

On May 27, 1994, Limbaugh married Marta Fitzgerald, a 35-year-old aerobics instructor whom he met on the online service CompuServe in 1990. They married at the house of U.S. Supreme Court Justice Clarence Thomas, who officiated. The couple separated on June 11, 2004. Limbaugh announced his divorce on the air. It was finalized in December 2004. In September 2004, Limbaugh became romantically involved with then-CNN news anchor Daryn Kagan; the relationship ended in February 2006.

Limbaugh has lived in Palm Beach since 1996. A friend recalls that Limbaugh "fell in love with Palm Beach... after visiting her over Memorial Day weekend in 1995." Unlike New York, Florida does not tax income, the stated reason Limbaugh moved his residence and established his "Southern Command".

On December 30, 2009, while vacationing in Honolulu, Hawaii, Limbaugh was admitted to Queen's Medical Center with intense chest pains. His doctors attributed the pain to angina pectoris.

He dated Kathryn Rogers, a party planner from Florida, for three years before he married her on June 5, 2010. During the wedding reception after the ceremony, Elton John entertained the wedding guests for a reported $1 million fee; however, Limbaugh himself denied that the $1 million figure was accurate on his September 7, 2010, radio show.

Through a holding company, KARHL Holdings (KARHL meaning "Kathryn and Rush Hudson Limbaugh"), Limbaugh launched a line of bottled iced tea beverages called "Two if by Tea", a play on the line from Henry Wadsworth Longfellow's "Paul Revere's Ride" "one if by land, two if by sea". KARHL Holdings features a Rush Revere website where children can send notes to Liberty, the time-traveling, talking horse.

On October 3, 2003, the "National Enquirer" reported that Limbaugh was being investigated for illegally obtaining the prescription drugs oxycodone and hydrocodone. Other news outlets quickly confirmed the investigation. He admitted to listeners on his radio show on October 10, 2003, that he was addicted to prescription painkillers and stated that he would enter inpatient treatment for 30 days, immediately after the broadcast. Limbaugh stated his addiction to painkillers resulted from several years of severe back pain heightened by a botched surgery intended to correct those problems.

A subsequent investigation into whether Limbaugh had violated Florida's doctor shopping laws was launched by the Palm Beach State Attorney, which raised privacy issues when investigators seized Limbaugh's private medical records looking for evidence of crimes. Roy Black, one of Limbaugh's attorneys, stated that "Rush Limbaugh was singled out for prosecution because of who he is. We believe the state attorney's office is applying a double standard." On November 9, 2005, following two years of investigations, Assistant State Attorney James L. Martz requested that the court set aside Limbaugh's doctor–patient confidentiality rights and allow the state to question his physicians. Limbaugh's attorney opposed the prosecutor's efforts to interview his doctors on the basis of patient privacy rights, and argued that the prosecutor had violated Limbaugh's Fourth Amendment rights by illegally seizing his medical records. The American Civil Liberties Union issued a statement in agreement and filed an amicus curiae brief in support of Limbaugh. On December 12, 2005, Judge David F. Crow delivered a ruling prohibiting the State of Florida from questioning Limbaugh's physicians about "the medical condition of the patient and any information disclosed to the health care practitioner by the patient in the course of the care and treatment of the patient."

On April 28, 2006, a warrant was issued for his arrest on the charge of doctor shopping. According to Teri Barbera, spokeswoman for the sheriff, during his arrest, Limbaugh was booked, photographed, and fingerprinted, but not handcuffed. He was then released after about an hour on $3,000 bail. After his surrender, he filed a "not guilty" plea to the charge. Prosecutors explained that the charges were brought after they discovered he received about 2,000 painkillers, prescribed by four doctors in six months, at a pharmacy near his Palm Beach mansion. In 2009, after three years of prolonged discussion regarding a settlement, prosecutors agreed to drop the charge if Limbaugh paid $30,000 to defray the cost of the investigation, completed an 18-month therapy regimen with his physician, submitted to random drug testing, and gave up his right to own a firearm for eighteen months. Limbaugh agreed to the settlement, though he continued to maintain his innocence of doctor shopping and asserted that the state's offer resulted from a lack of evidence supporting the charge.

Before his addiction became known, Limbaugh had condemned illegal drug use on his television program, stating that "Drug use, some might say, is destroying this country... And so if people are violating the law by doing drugs, they ought to be accused and they ought to be convicted and they ought to be sent up."

In June 2006, Limbaugh was detained by drug enforcement agents at Palm Beach International Airport. Customs officials confiscated Viagra from Limbaugh's luggage as he was returning from the Dominican Republic. The prescription was not in Limbaugh's name. After he was released with no charges filed, Limbaugh joked about the incident on his radio show, claiming that he got the Viagra at the Clinton Library and was told they were blue M&M's. He also stated that "I had a great time in the Dominican Republic. Wish I could tell you about it."

Rush Limbaugh has described himself as being "100 percent, totally deaf." In 2001, Limbaugh announced that he had lost most of his ability to hear: "I cannot hear television. I cannot hear music. I am, for all practical purposes, deaf – and it's happened in three months." He said that the condition was not genetic. He was diagnosed with autoimmune inner ear disease (AIED) and medications failed to work. On December 19, 2001, doctors at the House Ear Clinic in Los Angeles were able to successfully restore a measure of his hearing through cochlear implant surgery. Limbaugh received a Clarion CII Bionic Ear.

When questioned whether Limbaugh's sudden hearing loss was caused by his addiction to opioids, his cochlear implant doctor, otolaryngologist Jennifer Derebery, said that it was possible but that there is no way to know for sure without performing tests that would destroy Limbaugh's hearing completely. "We don't know why some people, but apparently not most, who take large doses may lose their hearing".

In 2005, Limbaugh was forced to undergo "tuning" due to an "eye twitch," an apparent side-effect of cochlear implants.

On April 8, 2014, on his radio program, Limbaugh announced his decision to 'go bilateral.' "I'm going to get an implant on the right side," he said. After bilateral tuning, there was 100% improvement. "Coming from total deafness, it is miraculous! How can you not believe in God?" Limbaugh said in his national daily broadcast.


In 1992, Limbaugh published his first book, "The Way Things Ought to Be", followed by "See, I Told You So", the following year. Both titles were number one on the "New York Times" Best Seller list for 24 weeks. His first book was dictated by himself, and transcribed and edited by "Wall Street Journal Journal" writer John Fund.

In 2013, Limbaugh authored his first children's book entitled, "Rush Revere and the Brave Pilgrims: Time-Travel with Exceptional Americans". He received the Author of the Year Award from the Children's Book Council for this work. Limbaugh's second children's book was released the following year, entitled, "Rush Revere and the First Patriots: Time-Travel with Exceptional Americans". This book was nominated as an author-of-the year finalist for the annual Children's and Teen Choice Book Awards. Limbaugh's third children's book was released later this same year, written with his wife, Kathryn, and entitled "Rush Revere and the American Revolution". The Limbaugh's dedicated this to the U.S. military and their families.

Sources




</doc>
<doc id="25428" url="https://en.wikipedia.org/wiki?curid=25428" title="Roman Polanski">
Roman Polanski

Roman Polański ( , ; born 18 August 1933 in Paris; original name Raymond Thierry Liebling) is a French-Polish film director, producer, writer, and actor. Since 1978, Polanski has been a fugitive from the U.S. criminal justice system; he fled the country while awaiting sentencing in his sexual abuse case, in which he pleaded guilty to statutory rape.

His Polish-Jewish parents moved the family back from Paris to Kraków in 1937. Two years later, Poland was invaded by Nazi Germany starting World War II and the Polanskis found themselves trapped in the Kraków Ghetto. After his mother and father were taken in raids, Polanski spent his formative years in foster homes under an adopted identity, trying to survive the Holocaust.
Polanski's first feature-length film, "Knife in the Water" (1962), was made in Poland and was nominated for a United States Academy Award for Best Foreign Language Film. He has since received five more Oscar nominations, along with two BAFTAs, four Césars, a Golden Globe Award and the Palme d'Or of the Cannes Film Festival in France. In the United Kingdom he directed three films, beginning with "Repulsion" (1965). In 1968, he moved to the United States and cemented his status by directing the horror film "Rosemary's Baby" (1968).

A turning point in his life took place in 1969, when his pregnant wife, Sharon Tate, and four friends were brutally murdered by members of the Manson Family. Following her death, Polanski returned to Europe and eventually continued directing. He made "Macbeth" (1971) in England and back in Hollywood, "Chinatown" (1974), which was nominated for eleven Academy Awards. In 1977, Polanski was arrested and charged with drugging and raping a 13-year-old girl. He subsequently pled guilty to the lesser offence of unlawful sex with a minor. After spending 42 days undergoing psychiatric evaluation in prison in preparation for sentencing, Polanski, who had expected to be put on probation, fled to Paris after learning that the judge planned to reject his plea deal and impose a prison term.

In Europe, Polanski continued to make films, including "Tess" (1979), starring Nastassja Kinski. It won France's César Awards for Best Picture and Best Director, and received three Oscars. He later produced and directed "The Pianist" (2002), a drama about a Jewish-Polish musician escaping Nazi persecution, starring Adrien Brody and Emilia Fox. The film won three Academy Awards including Best Director, along with numerous international awards. He also directed "Oliver Twist" (2005), a story which parallels his own life as a "young boy attempting to triumph over adversity". He was awarded Best Director for "The Ghost Writer" (2010) at the 23rd European Film Awards. He also received Best Screenwriter nomination at the aforementioned awards for "Carnage" (2011). In 2018, the Academy of Motion Picture Arts and Sciences voted to expel Polanski from its membership.

Polanski was born in Paris; he was the son of Bella (Bula) née Katz-Przedborska and Mojżesz Liebling, a painter and manufacturer of sculptures, who after World War II was known as Ryszard Polański. Polański's father was Jewish and originally from Poland; Polański's mother, born in Russia, had been raised Roman Catholic and was of half Jewish ancestry. His mother had a daughter, Annette, by her previous husband. Annette managed to survive Auschwitz, where her mother died, and left Poland forever for France. Polański's parents were both agnostics. Polański stated "I'm an atheist" in an interview about his film, "Rosemary's Baby".

The Polański family moved back to the Polish city of Kraków in early 1937, and were living there when World War II began with the invasion of Poland. Kraków was soon occupied by the German forces, and the racist and anti-Semitic Nuremberg Laws made the Polańskis targets of persecution, forcing them into the Kraków Ghetto, along with thousands of the city's Jews. Around the age of six, he attended primary school for only a few weeks, until "all the Jewish children were abruptly expelled," writes biographer Christopher Sandford. That initiative was soon followed by the requirement that all Jewish children over the age of twelve wear white armbands with a blue Star of David imprinted for visual identification. After he was expelled, he would not be allowed to enter another classroom for the next six years. Polanski then witnessed both the ghettoization of Kraków's Jews into a compact area of the city, and the subsequent deportation of all the ghetto's Jews to German death camps. He watched as his father was taken away. He remembers from age six, one of his first experiences of the terrors to follow:
His father was transferred, along with thousands of other Jews, to Mauthausen, a group of 49 German concentration camps in Austria. His mother was taken to Auschwitz, and was killed soon after arriving. The forced exodus took place immediately after the German liquidation of the Warsaw Ghetto, a true-life backdrop to Polanski's film "The Pianist" (2002). Polanski, who was then hiding from the Germans, remembered seeing his father being marched off with a long line of people. Polanski tried getting closer to his father to ask him what was happening, and managed to get within a few yards. His father saw him, but afraid his son might be spotted by the German soldiers, whispered (in Polish), "Get lost!"

Polański escaped the Kraków Ghetto in 1943 and survived with the help of some Polish Roman Catholics, including a woman who had promised Polański's father that she would shelter Polanski. Polański attended church, learned to recite Catholic prayers by heart, and behaved outwardly as a Roman Catholic, although he was never baptized. His efforts to blend into a Catholic household failed miserably at least once, when the parish priest visiting the family posed questions to him one-on-one about the catechism, and ultimately said, "You aren't one of us". The punishment for helping a Jew in German-occupied Poland was death.

As he roamed the countryside trying to survive in a Poland now occupied by German troops, he witnessed many horrors, such as being "forced to take part in a cruel and sadistic game in which German soldiers took shots at him for target practice." Author Ian Freer concludes that Polanski's constant childhood fears and dread of violence have contributed to the "tangible atmospheres he conjures up on film."

By the time the war ended in 1945, a fifth of the Polish population had been killed, with the vast majority of the victims being civilians. Of those deaths, 3 million were Polish Jews, which accounted for 90% of the country's Jewish population. According to Sandford, Polanski would use the memory of his mother, her dress and makeup style, as a physical model for Faye Dunaway's character in his film "Chinatown" (1974).

After the war, he was reunited with his father and moved back to Kraków. His father remarried 21 December 1946 to Wanda Zajączkowska (a woman Polanski had never liked) and died of cancer in 1984. Time repaired the family contacts; Polanski visited them in Kraków, and relatives visited him in Hollywood and Paris. Polanski recalls the villages and families he lived with as relatively primitive by European standards:
He stated that "you must live in a Communist country to really understand how bad it can be. Then you will appreciate capitalism." He also remembered events at the war's end and his reintroduction to mainstream society when he was 12, forming friendships with other children, such as Roma Ligocka, Ryszard Horowitz and his family.

Polanski's fascination with cinema began very early, when he was around age four or five. He recalls this period in an interview:
After the war, he watched films, either at school or at a local cinema, using whatever pocket money he had. Polanski writes, "Most of this went on the movies, but movie seats were dirt cheap, so a little went a long way. I lapped up every kind of film." As time went on, movies became more than an escape into entertainment, as he explains:

Polanski attended the National Film School in Łódź, the third-largest city in Poland. In the 1950s, Polanski took up acting, appearing in Andrzej Wajda's "Pokolenie" ("A Generation", 1954) and in the same year in Silik Sternfeld's "Zaczarowany rower" ("Enchanted Bicycle" or "Magical Bicycle"). Polanski's directorial debut was also in 1955 with a short film "Rower" ("Bicycle"). "Rower" is a semi-autobiographical feature film, believed to be lost, which also starred Polanski. It refers to his real-life violent altercation with a notorious Kraków felon, Janusz Dziuba, who arranged to sell Polanski a bicycle, but instead beat him badly and stole his money. In real life, the offender was arrested while fleeing after fracturing Polanski's skull, and executed for three murders, out of eight prior such assaults which he had committed. Several other short films made during his study at Łódź gained him considerable recognition, particularly "Two Men and a Wardrobe" (1958) and "When Angels Fall" (1959). He graduated in 1959.

Polanski's first feature-length film, "Knife in the Water", was also one of the first significant Polish films after the Second World War that did not have a war theme. Scripted by Jerzy Skolimowski, Jakub Goldberg, and Polanski, "Knife in the Water" is about a wealthy, unhappily married couple who decide to take a mysterious hitchhiker with them on a weekend boating excursion. "Knife in the Water" was a major commercial success in the West and gave Polanski an international reputation. The film also earned its director his first Academy Award nomination (Best Foreign Language Film) in 1963. Leon Niemczyk, who played Andrzej, was the only professional actor in the film. Jolanta Umecka, who played Krystyna, was discovered by Polanski at a swimming pool.

Polanski left then-communist Poland and moved to France, where he had already made two notable short films in 1961: "The Fat and the Lean" and "Mammals". While in France, Polanski contributed one segment ("La rivière de diamants") to the French-produced omnibus film, "Les plus belles escroqueries du monde" (English title: "The Beautiful Swindlers") in 1964. (He has since had the segment removed from all releases of the film.) However, Polanski found that in the early 1960s, the French film industry was xenophobic and generally unwilling to support a rising filmmaker of foreign origin.

Polanski made three feature films in England, based on original scripts written by himself and Gérard Brach, a frequent collaborator. "Repulsion" (1965) is a psychological horror film focusing on a young Belgian woman named Carol (Catherine Deneuve) who suffers from a physical and emotional dread of sexual intercourse. The film's themes, situations, visual motifs, and effects clearly reflect the influence of early surrealist cinema as well as horror movies of the 1950s—particularly Luis Buñuel's "Un chien Andalou", Jean Cocteau's "The Blood of a Poet", Henri-Georges Clouzot's "Diabolique" and Alfred Hitchcock's "Psycho".

"Cul-de-sac" (1966) is a bleak nihilist tragicomedy filmed on location in Northumberland. The tone and premise of the film owe a great deal to Samuel Beckett's "Waiting for Godot", along with aspects of Harold Pinter's "The Birthday Party".

"The Fearless Vampire Killers" (1967) (known by its original title, "Dance of the Vampires" in most countries outside the United States) is a parody of vampire films. The plot concerns a buffoonish professor and his clumsy assistant, Alfred (played by Polanski), who are traveling through Transylvania in search of vampires. "The Fearless Vampire Killers" was Polanski's first feature to be photographed in color with the use of Panavision lenses, and included a striking visual style with snow-covered, fairy-tale landscapes, similar to the work of Soviet fantasy filmmakers. In addition, the richly textured color schemes of the settings evoke the paintings of the Belarusian-Jewish artist Marc Chagall, who provides the namesake for the innkeeper in the film. The film was written for Jack MacGowran, who played the lead role of Professor Abronsius.

Polanski met Sharon Tate while making the film; she played the role of the local innkeeper's daughter. They were married in London on 1968. Shortly after they married, Polanski, with Tate at his side during a documentary film, described the demands of young movie viewers who he said always wanted to see something "new" and "different".

Paramount studio head Robert Evans brought Polanski to America ostensibly to direct the film "Downhill Racer", but told Polanski that he really wanted to him to read the horror novel "Rosemary's Baby" by Ira Levin to see if a film could be made out of it. Polanski read it non-stop through the night and the following morning decided he wanted to write as well as direct it. He wrote the 272-page screenplay for the film in slightly longer than three weeks. The film, "Rosemary's Baby" (1968), was a box-office success and became his first Hollywood production, thereby establishing his reputation as a major commercial filmmaker. The film, a horror-thriller set in trendy Manhattan, is about Rosemary Woodhouse (Mia Farrow), a young housewife who is impregnated by the devil. Polanski's screenplay adaptation earned him a second Academy Award nomination.

On 9 August 1969, while Polanski was working in London, his wife, Sharon Tate, and four other people were murdered at the Polanskis' residence in Los Angeles by cult leader Charles Manson's followers.

Polanski adapted "Macbeth" into a screenplay with the Shakespeare expert Kenneth Tynan. Jon Finch and Francesca Annis played the main characters. Hugh Hefner and Playboy Productions funded the 1971 film, which opened in New York and was screened in Playboy Theater. Hefner was credited as executive producer, and the film was listed as a "Playboy Production". It was controversial because of Lady Macbeth's being nude in a scene, and received an X rating because of its graphic violence and nudity. In his autobiography, Polanski wrote that he wanted to be true to the violent nature of the work, and that he had been aware that his first project following Tate's murder would be subject to scrutiny and probable criticism regardless of the subject matter; if he had made a comedy he would have been perceived as callous.

Written by Polanski and previous collaborator Gérard Brach, "What?" (1973) is a mordant absurdist comedy loosely based on the themes of "Alice in Wonderland" and Henry James. The film is a rambling shaggy dog story about the sexual indignities that befall a winsome young American hippie woman hitchhiking through Europe.

Polanski returned to Hollywood in 1973 to direct "Chinatown" (1974) for Paramount Pictures. The film is widely considered to be one of the finest American mystery crime movies, inspired by the real-life California Water Wars, a series of disputes over southern California water at the beginning of the 20th century.

It was nominated for 11 Academy Awards, including those for actors Jack Nicholson and Faye Dunaway. Robert Towne won for Best Original Screenplay. It also had actor-director John Huston in a supporting role, and was the last film Polanski directed in the United States. In 1991, the film was selected by the Library of Congress for preservation in the United States National Film Registry as being "culturally, historically or aesthetically significant" and it is frequently listed as among the best in world cinema.

Polanski returned to Paris for his next film, "The Tenant" (1976), which was based on a 1964 novel by Roland Topor, a French writer of Polish-Jewish origin. In addition to directing the film, Polanski also played a leading role of a timid Polish immigrant living in Paris. Together with "Repulsion" and "Rosemary's Baby", "The Tenant" can be seen as the third installment in a loose trilogy of films called the "Apartment Trilogy" that explore the themes of social alienation and psychic and emotional breakdown.

In 1978, Polanski became a fugitive from American justice and could no longer work in countries where he might face arrest or extradition.

He dedicated his next film, "Tess" (1979), to the memory of his late wife, Sharon Tate. It was Tate who first suggested he read "Tess of the d'Urbervilles", which she thought would make a good film; he subsequently expected her to star in it. Nearly a decade after Tate's death, he met Nastassja Kinski, a model and aspiring young actress who had already been in a number of European films. He offered her the starring role, which she accepted. Her father was Klaus Kinski, a leading German actor, who had introduced her to films.

Because the role required having a local dialect, Polanski sent her to London for five months of study and to spend time in the Dorset countryside to get a flavor of the region. In the film, Kinski starred opposite Peter Firth and Leigh Lawson.
"Tess" was shot in the north of France instead of Hardy's England and became the most expensive film made in France up to that time. Ultimately, it proved a financial success and was well received by both critics and the public. Polanski won France's César Awards for Best Picture and Best Director and received his fourth Academy Award nomination (and his second nomination for Best Director). The film received three Oscars: best cinematography, best art direction, best costume design, and was nominated for best picture.

At the time, there were rumors that Polanski and Kinski became romantically involved, but she says the rumors are untrue; they were never lovers or had an affair. She admits that "there was a flirtation. There "could" have been a seduction, but there was not. He had respect for me." She also recalls his influence on her while filming: "He was really a gentleman, not at all like the things I had heard. He introduced me to beautiful books, plays, movies. He educated me." On an emotional level, she said years later that "he was one of the people in my life who cared, ... who took me seriously and gave me a lot of strength." She told David Letterman more about her experience working with Polanski during an interview.

In 1981, Polanski directed and co-starred (as Mozart) in a stage production of Peter Shaffer's play "Amadeus", first in Warsaw, then in Paris. The play was again directed by Polanski, in Milan, in 1999.

Nearly seven years passed before Polanski's next film, "Pirates", a lavish period piece starring Walter Matthau as Captain Red, which the director intended as an homage to the beloved Errol Flynn swashbucklers of his childhood. Captain Red's henchman, Jean Baptiste, was played by Cris Campion. The film is about a rebellion the two led on a ship called the "Neptune", in the seventeenth century. The screenplay was written by Polanski, Gérard Brach, and John Brownjohn. The film was shot on location in Tunisia, using a full-sized pirate vessel constructed for the production. It was a financial and critical failure, recovering a small fraction of its production budget and garnering a single Academy Award nomination.

"Frantic" (1988) was a Hitchcockian suspense-thriller starring Harrison Ford and the actress/model Emmanuelle Seigner, who later became Polanski's wife. The film follows an ordinary tourist in Paris whose wife is kidnapped. He attempts, hopelessly, to go through the Byzantine bureaucratic channels to deal with her disappearance, but finally takes matters into his own hands.

Polanski followed this with the dark psycho-sexual film "Bitter Moon" (1992), followed by a film of the acclaimed play "Death and the Maiden" (1994) starring Sigourney Weaver.

In 1997, Polanski directed a stage version of his 1967 film "The Fearless Vampire Killers", which debuted in Vienna followed by successful runs in Stuttgart, Hamburg, Berlin, and Budapest. On 1998, Polanski was elected a member of the Académie des Beaux-Arts.

"The Ninth Gate" is a thriller based on the novel "El Club Dumas" by Arturo Perez-Reverte and starring Johnny Depp. The movie's plot is based on the idea that an ancient text called "The Nine Gates of the Kingdom of Shadows", authored by Aristide Torchia along with Lucifer, is the key to raising Satan.

In 2001, Polanski filmed "The Pianist", an adaptation of the World War II autobiography of the same name by Polish-Jewish musician Władysław Szpilman. Szpilman's experiences as a persecuted Jew in Poland during World War II were reminiscent of those of Polanski and his family. While Szpilman and Polanski escaped the concentration camps, their families did not, eventually perishing.

When Warsaw, Poland, was chosen for the 2002 premiere of "The Pianist", "the country exploded with pride." According to reports, numerous former communists came to the screening and "agreed that it was a fantastic film."

In May 2002, the film won the "Palme d'Or" (Golden Palm) award at the Cannes Film Festival, as well as Césars for Best Film and Best Director, and later the 2002 Academy Award for Best Director. Because Polanski would have been arrested in the United States, he did not attend the Academy Awards ceremony in Hollywood. After the announcement of the Best Director Award, Polanski received a standing ovation from most of those present in the theater. Actor Harrison Ford accepted the award for Polanski, and then presented the Oscar to him at the Deauville Film Festival five months later in a public ceremony. Polanski later received the Crystal Globe award for outstanding artistic contribution to world cinema at the Karlovy Vary International Film Festival in 2004.

"Oliver Twist" is an adaptation of Charles Dickens' novel, written by "The Pianist"s Ronald Harwood and shot in Prague. Polanski said in interviews that he made the film as something he could show his children, and that the life of the young scavenger mirrored his own life, fending for himself in World War II Poland.

"The Ghost Writer", a thriller focusing on a ghostwriter working on the memoirs of a character based loosely on former British prime minister Tony Blair, swept the European Film Awards in 2010, winning six awards, including best movie, director, actor and screenplay. When it premiered at the 60th Berlinale in February 2010, Polanski won a Silver Bear for Best Director, and in February 2011, it won four César Awards, France's version of the Academy Awards.

The film is based on a novel by British writer Robert Harris. Harris and Polanski had previously worked for many months on a film of Harris's earlier novel "Pompeii", a novel that was actually inspired by Polanski's "Chinatown". They had completed a script for "Pompeii" and were nearing production when the film was cancelled due to a looming actors' strike in September 2007. After that film fell apart, they moved on to Harris's novel, The Ghost, and adapted it for the screen together.

The cast includes Ewan McGregor as the writer and Pierce Brosnan as former British Prime Minister Adam Lang. The film was shot on locations in Germany.

In the United States, film critic Roger Ebert included it in his top 10 pick for 2010, and states that "this movie is the work of a man who knows how to direct a thriller. Smooth, calm, confident, it builds suspense instead of depending on shock and action." Co-star Ewan McGregor agrees, saying about Polanski that "he's a legend ... I've never examined a director and the way that they work, so much before. He's brilliant, just brilliant, and absolutely warrants his reputation as a great director."
Polanski shot "Carnage" in February/March 2011. The film is a screen version of Yasmina Reza's play "God of Carnage", a comedy about two couples who meet after their children get in a fight at school, and how their initially civilized conversation devolves into chaos. It stars Kate Winslet, Jodie Foster, Christoph Waltz and John C. Reilly. Though set in New York, it was shot in Paris. The film had its world premiere on 9 September 2011 at the Venice Film Festival and was released in the United States by Sony Pictures Classics on 16 December 2011.

Co-stars Jodie Foster and Kate Winslet commented about Polanski's directing style. According to Foster, "He has a very, very definitive style about how he likes it done. He decides everything. He decided every lens. Every prop. Everything. It's all him." Winslet adds that "Roman is one of the most extraordinary men I've ever met. The guy is 77 years old. He has an effervescent quality to him. He's very joyful about his work, which is infectious. He likes to have a small crew, to the point that, when I walked on the set, my thought was, 'My God, this is it?'" Also noting that style of directing, New York Film Festival director Richard Pena, during the American premiere of the film, called Polanski "a poet of small spaces ... in just a couple of rooms he can conjure up an entire world, an entire society."

Polanski makes an uncredited cameo appearance as a neighbor.

Polanski's French-language adaptation of the award-winning play "Venus in Fur", stars his wife Emmanuelle Seigner and Mathieu Amalric. Polanski worked with the play's author, David Ives, on the screenplay. The film was shot from December 2012 to February 2013 in French and is Polanski's first non-English-language feature film in forty years. The film premiered in competition at the 2013 Cannes Film Festival on 25 May 2013.

Polanski's "Based on a True Story" is an adaptation of the French novel by bestselling author Delphine de Vignan. The film follows a writer (Emmanuelle Seigner) struggling to complete a new novel, while followed by an obsessed fan (Eva Green). It started production in November 2016 from a script adapted by Polanski and Olivier Assayas. It premiered out of competition at the 2017 Cannes Film Festival on 27 May 2017 and opened in France on 1 November 2017.

Polanski's next film, "An Officer and a Spy", centers on the notorious 19th century Dreyfus affair, The film stars Jean Dujardin as French officer Georges Picquart and follows his struggle from 1896-1906 to expose the truth about the doctored evidence that led to Alfred Dreyfus, one of the few Jewish members of the French Army's general staff, being wrongly convicted of passing military secrets to the German Empire and sent to Devil's Island. The film is written by Robert Harris, who is working with Polanski for the third time. It co-stars Louis Garrel as Dreyfus, Mathieu Amalric, Olivier Gourmet and Polanski's wife Emmanuelle Seigner. It is being produced by Alain Goldman's Legende Films and distributed by Gaumont. Filming began on 26 November 2018 and was completed on 28 April 2019.

Although set in Paris, the film was first scheduled to shoot in Warsaw in 2014, for economic reasons. However, production was postponed after Polanski moved to Poland for filming and the U.S. Government filed extradition papers. The Polish government eventually rejected them, by which time new French film tax credits had been introduced, allowing the film to shoot on location in Paris. It was budgeted at €60m and was again set to start production in July 2016, however its production was postponed as Polanski waited on the availability of a star, whose name was not announced. In a 2017 interview Polanski discussed the difficulty of the project:

It had its world premiere at the Venice Film Festival on 30 August 2019. It received a standing ovation and won the Grand Jury Prize. It is scheduled to be released in France on 13 November 2019, by Gaumont. The film has received backlash due to the plot of the film relating to Polanski's sexual abuse case and further accusations of harassment and assault.

Polanski's first wife, Barbara Lass (née Kwiatkowska), was a Polish actress who also starred in Polanski's 1959 "When Angels Fall". The couple were married in 1959 and divorced in 1961.

Polanski met rising actress Sharon Tate while filming "The Fearless Vampire Killers", and during the production, the two of them began dating. On 1968, Polanski married Tate in London.

In August 1969, while Polanski was in Europe working on a film, Tate was murdered along with four of their friends at their home in Los Angeles by members of Charles Manson's "family", a group of young, mostly female followers. Tate was pregnant at the time of her murder.

Manson, along with members of his "family", was arrested in late 1969, and eventually tried and found guilty in 1971 of 27 counts, including first-degree murder, an event now called the Manson murders. Because at the time it was one of the most "horrific crimes in modern history," the crime and trial of Manson and his followers became a media sensation, leading to movies, documentaries and bestselling books.

Polanski has said that his absence on the night of the murders is the greatest regret of his life. In his autobiography, he wrote, "Sharon's death is the only watershed in my life that really matters", and commented that her murder changed his personality from a "boundless, untroubled sea of expectations and optimism" to one of "ingrained pessimism ... eternal dissatisfaction with life". In his autobiography, Polanski described his brief time with Tate as the best years of his life.

Polanski was also left with a very negative impression of the press, which he felt was interested in sensationalizing the lives of the victims, and indirectly himself, to attract readers. He was shocked by the lack of sympathy expressed in various news stories:

Among the media-generated sensationalism were rumors that claimed Tate and her visitors were taking drugs, despite the coroner's announcing that no traces of drugs or nicotine were found after Tate's autopsy. For years afterward, notes Sandford, "reporters openly speculated about the Polanskis' home life" and their personalities in order to create more media gossip about the private lives of Hollywood celebrities.

In 1989, Polanski married French actress Emmanuelle Seigner, who is 33 years younger than he. They have two children, daughter Morgane and son Elvis. Polanski and his children speak Polish at home.

On 11 March 1977, three years after making "Chinatown", Polanski was arrested at the Beverly Wilshire Hotel for the sexual assault of 13-year-old Samantha Gailey. Gailey had modeled for Polanski during a "Vogue" photoshoot the previous day around the swimming pool at the Bel Air home of Polanski was indicted on six counts of criminal behavior, including rape. At his arraignment, he pleaded not guilty to all charges. Many executives in Hollywood came to his defense. Gailey's attorney arranged a plea bargain in which five of the six charges would be dismissed, and Polanski accepted.
As a result of the plea bargain, Polanski pleaded guilty to the charge of "unlawful sexual intercourse with a minor", and was ordered to undergo 90 days of psychiatric evaluation at California Institution for Men at Chino. Upon release from prison after 42 days, Polanski agreed to the plea bargain, his penalty to be time served along with probation. However, he learned afterward that the judge, Laurence J. Rittenband, had told some friends that he was going to disregard the plea bargain and sentence Polanski to 50 years in prison: "I'll see this man never gets out of jail," he told Polanski's friend, screenwriter Howard E. Koch. Gailey's attorney confirmed the judge changed his mind after he met the judge in his chambers:
Polanski was told by his attorney that "the judge could no longer be trusted" and that the judge's representations were "worthless". Polanski decided not to appear at his sentencing. He told his friend, director Dino De Laurentis, "I've made up my mind. I'm getting out of here." the day before sentencing, Polanski left the country on a flight to where he had a home. One day later, he left for As a French citizen, he has been protected from extradition and has lived mostly in France since then. However, since he fled the United States before final sentencing, the charges are still pending.

In 1988, Gailey sued Polanski. Among other things, the suit alleged sexual assault, false imprisonment, seduction of a minor, and intentional infliction of emotional distress. In 1993, Polanski agreed to settle with his victim. In August 1996, Polanski still owed her $604,416; court filings confirm that the settlement was completed by 1997 via a confidential financial arrangement. The victim, now married and going by the name Samantha Geimer, stated in a 2003 interview with Larry King that the police and media had been slow at the time of the assault to believe her account, which she attributed to the social climate of the era. In 2008, she stated, "I don't wish for him to be held to further punishment or consequences."

On 26 September 2009, Polanski was arrested while in Switzerland at the request of United States authorities. The arrest brought renewed attention to the case and stirred controversy, particularly in the United States and Europe. Polanski was defended by many prominent individuals, including Hollywood celebrities and European artists and politicians, who called for his release. American public opinion was reported to run against him, however, and polls in France and Poland showed that strong majorities favored his extradition to the United States.

Polanski was jailed near Zürich for two months, then put under house arrest at his home in Gstaad while awaiting decision of appeals fighting extradition. On 12 July 2010, the Swiss rejected the United States' request, declared Polanski a "free man" and released him from custody. He remains the subject of an Interpol red notice issued in 2005 at the request of the United States.

During a television interview on 10 March 2011, Geimer blamed the media, reporters, the court, and the judge for having caused "way more damage to me and my family than anything Roman Polanski has ever done", and opined that the judge was using her and Polanski for the media exposure.

In January 2014, newly uncovered emails from 2008 by a Los Angeles County Superior Court Judge, Larry P. Fidler, indicated that if Polanski returned to the United States for a hearing, the conduct of the judge who had originally presided over the case, Laurence A. Rittenband, might require that Polanski be freed. These emails were related to a 2008 documentary film by Marina Zenovich. In late October 2014, Polanski was questioned by prosecutors in Kraków.

On 30 October 2015, Polish judge Dariusz Mazur denied a request by the United States to extradite Polanski (a dual French-Polish citizen) for a full trial, claiming that it would be "obviously unlawful". The Kraków prosecutor's office declined to challenge the court's ruling, agreeing that Polanski had served his punishment and did not need to face a U.S. court again. However, Poland's national justice ministry took up the appeal, arguing that sexual abuse of minors should be prosecuted regardless of the suspect's accomplishments or the length of time since the suspected crime took place. In a December 2016 decision, the Supreme Court of Poland dismissed the government's appeal, holding that the prosecutor general had failed to prove misconduct or flagrant legal error on the part of the lower court.

Preparations for a movie he was working on about the Dreyfus affair had been stalled by the extradition request.

On 3 May 2018, Polanski was removed from the Academy of Motion Picture Arts and Sciences, with the decision referencing the case.

In 2008, the documentary film by Marina Zenovich, "", was released in Europe and the United States where it won numerous awards. The film focuses on the judge in the case and the possible reasons why he changed his mind. It includes interviews with people involved in the case, including the victim, Geimer, and the prosecutor, Roger Gunson. Geimer said that the judge "didn't care what happened" to her or Polanski, but "was orchestrating some little show", while Gunson added, "I'm not surprised that Polanski left under those circumstances, ... it was going to be a real circus."

Former Los Angeles County Deputy District Attorney David Wells, whose statements were the most damning against Polanski, and who said he advised the judge to imprison Polanski, admitted that he lied about those statements, and said that to the press to "play up" his own role.

In December 2009, a California appellate court discussed the film's allegations as it denied Polanski's request to have the case dismissed. While saying it was "deeply concerned" by the allegations, and that the allegations were "in many cases supported by considerable evidence", it also found that "Even in light of our fundamental concern about the misconduct ... flight was not Polanski's only option. It was not even his best option." It said dismissal of the case, which would erase Polanski's guilty plea, would not be an "appropriate result", and that he still had other legal options.

In September 2011, the documentary film "" had its world premiere in Zürich, Switzerland. During an interview in the film, he offers his apology to Geimer: "She is a double victim: My victim, and a victim of the press." On this occasion, he collected the lifetime achievement award he was to have received at the time of his arrest two years earlier.

In 2004, Polanski sued "Vanity Fair" magazine in London for libel. A 2002 article in the magazine claimed that Polanski promised he would "make another Sharon Tate out of you" in an attempt to seduce a Scandinavian model while he was travelling to Tate's funeral. He received supporting testimony from Mia Farrow, and "Vanity Fair" "was unable to prove that the incident occurred". Polanski was awarded £50,000 in damages plus some of his legal costs.

In December 2017, Polanski filed a ₪1.5 million suit in Herzliya Magistrates' Court against Israeli journalist and filmmaker Matan Uziel. Polanski maintained that Uziel, through his website, www.imetpolanski.com, falsely reported that five women had come forward to accuse him of raping them. Polanski was suing for libel and defamation of character. Herzliya Magistrates' Court rejected Polanski's request to be exempt from appearing in court after filing the libel suit. While Polanski gave various reasons for his inability to appear, the presiding judge, Gilad Hess, dismissed these one by one and ordered Polanski to pay Uziel ₪10,000 in costs. In November 2018, it was published that Polanski decided to drop the lawsuit, and was ordered by the court to pay Uziel ₪30,000 (US$8,000) for court costs. The court accepted Uziel's request that the suit not be dropped, but rather that it be rejected, making Polanski unable to sue Uziel again over the same issue in the future.

In late December 2019, in Polanski's interviews with Paris Match and Gazeta Wyborcza, the latter accused Matan Uziel of carefully orchestrating the attacks on his character and for playing a major role in designing an international campaign to besmirch his name and reputation in order to make his career fall from grace.

In 2010, British actress Charlotte Lewis said that Polanski had "forced himself" on her while she was auditioning for a role in Paris in 1983, when she was 16. In 1999, however, Lewis had given a very different account of events in an interview with the UK's "News of the World", which was unearthed by the French daily "Libération". In that interview, Lewis asserted that she actually had a six-month tryst with Polanski when she was 17: "I knew that Roman had done something bad in the United States, but I wanted to be his mistress," Lewis said, according to "Liberation". "I wanted him probably more than he wanted me." In addition, Lewis never mentioned any sexual abuse, and she said that their relationship ended when Polanski introduced her to Warren Beatty, and she claimed that they soon began an affair. Furthermore, she was cast in Polanski's 1986 film "Pirates", appeared at the Cannes film festival on his arm years after the alleged incident, and in an interview the year of the film's release, Lewis stated, "I'd love to have had a romantic relationship with [Polanski], and a physical one. You can't help falling in love with him. But he didn't want me that way."<ref name="Pape_5/17/2010"></ref>

In October 2017, a woman named Renate Langer interviewed by Swiss police said Polanski raped her in Gstaad when she was 15, in 1972. That same month, Marianne Barnard accused Polanski of having assaulted her in 1975, when she was 10 years old.

In November 2019, a French actress named Valentine Monnier said Polanski violently raped her at a ski chalet in Gstaad in 1975.


New York Film Critics Circle Awards

Venice Film Festival




</doc>
<doc id="25431" url="https://en.wikipedia.org/wiki?curid=25431" title="Russian language">
Russian language

Russian (, "rússky yazýk") is an East Slavic language, which is an official language in Russia, Belarus, Kazakhstan and Kyrgyzstan, as well as being widely used throughout Eastern Europe, the Baltic states, the Caucasus and Central Asia. It was the "de facto" language of the Soviet Union until its dissolution on 25 December 1991. Although nearly three decades have passed since the breakup of the Soviet Union, Russian is used in official capacity or in public life in all the post-Soviet nation-states, as well as in Israel and Mongolia.

Russian belongs to the family of Indo-European languages, one of the four living members of the East Slavic languages, and part of the larger Balto-Slavic branch. Written examples of Old East Slavonic are attested from the 10th century onward.

Russian is the largest native language in Europe and the most geographically widespread language in Eurasia. It is the most widely spoken of the Slavic languages, with 144 million speakers in Russia, Ukraine and Belarus. Russian is the eighth most spoken language in the world by number of native speakers and the seventh by total number of speakers. The language is one of the six official languages of the United Nations. Russian is also the second most widespread language on the Internet, after English.

Russian distinguishes between consonant phonemes with palatal secondary articulation and those without, the so-called "soft" and "hard" sounds. Almost every consonant has a hard or a soft counterpart, and the distinction is a prominent feature of the language. Another important aspect is the reduction of unstressed vowels. Stress, which is unpredictable, is not normally indicated orthographically though an optional acute accent may be used to mark stress, such as to distinguish between homographic words, for example ("zamók" - a lock) and ("zámok" - a castle), or to indicate the proper pronunciation of uncommon words or names.

Russian is an East Slavic language of the wider Indo-European family. It is a descendant of the language used in Kievan Rus', a loose conglomerate of East Slavic tribes from the late 9th to the mid 13th centuries. From the point of view of spoken language, its closest relatives are Ukrainian, Belarusian, and Rusyn, the other three languages in the East Slavic branch. In many places in eastern and southern Ukraine and throughout Belarus, these languages are spoken interchangeably, and in certain areas traditional bilingualism resulted in language mixtures such as Surzhyk in eastern Ukraine and Trasianka in Belarus. An East Slavic Old Novgorod dialect, although it vanished during the 15th or 16th century, is sometimes considered to have played a significant role in the formation of modern Russian. Also Russian has notable lexical similarities with Bulgarian due to a common Church Slavonic influence on both languages, as well as because of later interaction in the 19th and 20th centuries, Bulgarian grammar differs markedly from Russian. In the 19th century (in Russia until 1917), the language was often called "Great Russian" to distinguish it from Belarusian, then called "White Russian" and Ukrainian, then called "Little Russian".

The vocabulary (mainly abstract and literary words), principles of word formations, and, to some extent, inflections and literary style of Russian have been also influenced by Church Slavonic, a developed and partly russified form of the South Slavic Old Church Slavonic language used by the Russian Orthodox Church. However, the East Slavic forms have tended to be used exclusively in the various dialects that are experiencing a rapid decline. In some cases, both the East Slavic and the Church Slavonic forms are in use, with many different meanings. "For details, see Russian phonology and History of the Russian language."

Over the course of centuries, the vocabulary and literary style of Russian have also been influenced by Western and Central European languages such as Greek, Latin, Polish, Dutch, German, French, Italian, and English, and to a lesser extent the languages to the south and the east: Uralic, Turkic, Persian, and Arabic, as well as Hebrew.

According to the Defense Language Institute in Monterey, California, Russian is classified as a level III language in terms of learning difficulty for native English speakers, requiring approximately 1,100 hours of immersion instruction to achieve intermediate fluency. It is also regarded by the United States Intelligence Community as a "hard target" language, due to both its difficulty to master for English speakers and its critical role in U.S. world policy.

Feudal divisions and conflicts as well as other obstacles to the exchange of goods and ideas that ancient Russian principalities have suffered from before and especially during the Mongol yoke strengthened dialectical differences and for a while prevented the emergence of the standardized national language. The formation of the unified and centralized Russian state in 15th and 16th centuries and the gradual (re)emergence of a common political, economic, and cultural space have created the need for a common standard language. The initial impulse for the standardization came from the government bureaucracy for the lack of a reliable tool of communication in administrative, legal, and judicial affairs became an obvious practical problem. The earliest attempts at standardizing Russian were made based on the so-called Moscow official or chancery language. Since then the underlying logic of language reforms in Russia reflected primarily the considerations of standardizing and streamlining language norms and rules in order to ensure the Russian language's role as a practical tool of communication and administration.

The current standard form of Russian is generally regarded as the "modern Russian literary language" ( - "sovremenny russky literaturny yazyk"). It arose in the beginning of the 18th century with the modernization reforms of the Russian state under the rule of Peter the Great, and developed from the Moscow (Middle or Central Russian) dialect substratum under the influence of some of the previous century's Russian chancery language.

Mikhail Lomonosov first compiled a normalizing grammar book in 1755; in 1783 the Russian Academy's first explanatory Russian dictionary appeared. During the end of the 18th and 19th centuries, a period known as the "Golden Age", the grammar, vocabulary, and pronunciation of the Russian language was stabilized and standardized, and it became the nationwide literary language; meanwhile, Russia's world-famous literature flourished.

Until the 20th century, the language's spoken form was the language of only the upper noble classes and urban population, as Russian peasants from the countryside continued to speak in their own dialects. By the mid-20th century, such dialects were forced out with the introduction of the compulsory education system that was established by the Soviet government. Despite the formalization of Standard Russian, some nonstandard dialectal features (such as fricative in Southern Russian dialects) are still observed in colloquial speech.

In 2010, there were 259.8 million speakers of Russian in the world: in Russia – 137.5 million, in the CIS and Baltic countries – 93.7 million, in Eastern Europe – 12.9 million, Western Europe – 7.3 million, Asia – 2.7 million, Middle East and North Africa – 1.3 million, Sub-Saharan Africa – 0.1 million, Latin America – 0.2 million, U.S., Canada, Australia and New Zealand – 4.1 million speakers. Therefore, the Russian language is the 8th largest in the world by number of speakers, after English, Mandarin, Hindi-Urdu, Spanish, French, Arabic and Portuguese.

Russian is one of the six official languages of the United Nations. Education in Russian is still a popular choice for both Russian as a second language (RSL) and native speakers in Russia as well as many of the former Soviet republics. Russian is still seen as an important language for children to learn in most of the former Soviet republics.

In Belarus, Russian is a second state language alongside Belarusian per the Constitution of Belarus. 77% of the population was fluent in Russian in 2006, and 67% used it as the main language with family, friends, or at work.

In Estonia, Russian is spoken by 29.6% of the population according to a 2011 estimate from the World Factbook. and is officially considered a foreign language.

In Latvia, Russian is officially considered a foreign language. 55% of the population was fluent in Russian in 2006, and 26% used it as the main language with family, friends, or at work. On 18 February 2012, Latvia held a constitutional referendum on whether to adopt Russian as a second official language. According to the Central Election Commission, 74.8% voted against, 24.9% voted for and the voter turnout was 71.1%. Beginning in 2019, instruction in Russian language will be gradually discontinued in private colleges and universities in Latvia, as well as general instruction in Latvian public high schools.

In Lithuania, Russian is not official, but it still retains the function of a "lingua franca". In contrast to the other two Baltic states, Lithuania has a relatively small Russian-speaking minority (5.0% as of 2008).

In Moldova, Russian is considered to be the language of inter-ethnic communication under a Soviet-era law. 50% of the population was fluent in Russian in 2006, and 19% used it as the main language with family, friends, or at work.

According to the 2010 census in Russia, Russian language skills were indicated by 138 million people (99.4% of the respondents), while according to the 2002 census – 142.6 million people (99.2% of the respondents).

In Ukraine, Russian is seen as a language of inter-ethnic communication, and a minority language, under the 1996 Constitution of Ukraine. According to estimates from Demoskop Weekly, in 2004 there were 14,400,000 native speakers of Russian in the country, and 29 million active speakers. 65% of the population was fluent in Russian in 2006, and 38% used it as the main language with family, friends, or at work. On 5 September 2017, Ukraine's Parliament passed a new education law which bars primary education to all students in any language but Ukrainian. The law faced criticism from officials in Russia. 

In the 20th century, Russian was a mandatory language taught in the schools of the members of the old Warsaw Pact and in other countries that used to be satellites of the USSR. According to the Eurobarometer 2005 survey, fluency in Russian remains fairly high (20–40%) in some countries, in particular those where the people speak a Slavic language and thereby have an edge in learning Russian (namely, Poland, Czech Republic, Slovakia, and Bulgaria).

Significant Russian-speaking groups also exist in Western Europe. These have been fed by several waves of immigrants since the beginning of the 20th century, each with its own flavor of language. The United Kingdom, Germany, Finland, Spain, Portugal, France, Italy, Belgium, Greece, Norway, and Austria have significant Russian-speaking communities.

In Armenia, Russian has no official status, but it is recognized as a minority language under the Framework Convention for the Protection of National Minorities. 30% of the population was fluent in Russian in 2006, and 2% used it as the main language with family, friends, or at work.

In Azerbaijan, Russian has no official status, but is a "lingua franca" of the country. 26% of the population was fluent in Russian in 2006, and 5% used it as the main language with family, friends, or at work.

In China, Russian has no official status, but it is spoken by the small Russian communities in the northeastern Heilongjiang province.

In Georgia, Russian has no official status, but it is recognized as a minority language under the Framework Convention for the Protection of National Minorities. Russian is the language of 9% of the population according to the World Factbook. Ethnologue cites Russian as the country's de facto working language.

In Kazakhstan, Russian is not a state language, but according to article 7 of the Constitution of Kazakhstan its usage enjoys equal status to that of the Kazakh language in state and local administration. The 2009 census reported that 10,309,500 people, or 84.8% of the population aged 15 and above, could read and write well in Russian, as well as understand the spoken language.

In Kyrgyzstan, Russian is a co-official language per article 5 of the Constitution of Kyrgyzstan. The 2009 census states that 482,200 people speak Russian as a native language, or 8.99% of the population. Additionally, 1,854,700 residents of Kyrgyzstan aged 15 and above fluently speak Russian as a second language, or 49.6% of the population in the age group.

In Tajikistan, Russian is the language of inter-ethnic communication under the Constitution of Tajikistan and is permitted in official documentation. 28% of the population was fluent in Russian in 2006, and 7% used it as the main language with family, friends or at work. The World Factbook notes that Russian is widely used in government and business.

In Turkmenistan, Russian lost its status as the official "lingua franca" in 1996. Russian is spoken by 12% of the population according to an undated estimate from the World Factbook. The Turkmen state press and websites regularly publish material in Russian and there is the Russian-language newspaper Neytralny Turkmenistan, the television channel TV4, and there are schools like Joint Turkmen-Russian Secondary School.

In Uzbekistan, Russian is the language of inter-ethnic communication. Has some official roles, being permitted in official documentation and is the lingua franca of the country and the language of the élite. Russian is spoken by 14.2% of the population according to an undated estimate from the World Factbook.

In 2005, Russian was the most widely taught foreign language in Mongolia, and was compulsory in Year 7 onward as a second foreign language in 2006.

Russian is also spoken in Israel. The number of native Russian-speaking Israelis numbers around 1.5 million Israelis, 15% of the population. The Israeli press and websites regularly publish material in Russian and there are Russian newspapers, television stations, schools, and social media outlets based in the country. There is an Israeli TV channel mainly broadcasting in Russian with Israel Plus. See also Russian language in Israel.

Russian is also spoken as a second language by a small number of people in Afghanistan.

In Vietnam, Russian has been added in the elementary curriculum along with Chinese and Japanese and were named as "first foreign languages" for Vietnamese students to learn, on equal footing with English.

The language was first introduced in North America when Russian explorers voyaged into Alaska and claimed it for Russia during the 18th century. Although most Russian colonists left after the United States bought the land in 1867, a handful stayed and preserved the Russian language in this region to this day, although only a few elderly speakers of this unique dialect are left. In Nikolaevsk, Alaska Russian is more spoken than English. Sizable Russian-speaking communities also exist in North America, especially in large urban centers of the U.S. and Canada, such as New York City, Philadelphia, Boston, Los Angeles, Nashville, San Francisco, Seattle, Spokane, Toronto, Baltimore, Miami, Chicago, Denver, and Cleveland. In a number of locations they issue their own newspapers, and live in ethnic enclaves (especially the generation of immigrants who started arriving in the early 1960s). Only about 25% of them are ethnic Russians, however. Before the dissolution of the Soviet Union, the overwhelming majority of Russophones in Brighton Beach, Brooklyn in New York City were Russian-speaking Jews. Afterward, the influx from the countries of the former Soviet Union changed the statistics somewhat, with ethnic Russians and Ukrainians immigrating along with some more Russian Jews and Central Asians. According to the United States Census, in 2007 Russian was the primary language spoken in the homes of over 850,000 individuals living in the United States.

In the second half of the 20th century, Russian was the most popular foreign language in Cuba. Besides being taught at universities and schools, there were also educational programs on the radio and TV. However, starting January, 2019 the Cuban television opens an educational program devoted to the Russian language. This project is fully entitled to be called an anticipated one, because the Russian – Cuban collaboration is a strategic direction actively developed as more and more young people are interested in the Russian language, the Education navigator informs. The Havana State University has started a bachelor's specialization called the Russian Language and the Second Foreign Language. There is also the Russian language department, where students can scrutinize e-books without internet connection. Additional courses on the Russian language are open at two schools of the Cuban capital city.

Russian is one of the official languages (or has similar status and interpretation must be provided into Russian) of the following:


The Russian language is also one of two official languages aboard the International Space Station – NASA astronauts who serve alongside Russian cosmonauts usually take Russian language courses. This practice goes back to the Apollo-Soyuz mission, which first flew in 1975.

In March 2013 it was announced that Russian is now the second-most used language on the Internet after English. People use the Russian language on 5.9% of all websites, slightly ahead of German and far behind English (54.7%). Russian is used not only on 89.8% of .ru sites, but also on 88.7% of sites with the former Soviet Union domain .su. The websites of former Soviet Union nations also use high levels of Russian: 79.0% in Ukraine, 86.9% in Belarus, 84.0% in Kazakhstan, 79.6% in Uzbekistan, 75.9% in Kyrgyzstan and 81.8% in Tajikistan. However, Russian is the sixth-most used language on the top 1,000 sites, behind English, Chinese, French, German, and Japanese.

Russian is a rather homogeneous language, in terms of dialectal variation, due to the early political centralization under Moscow's rule, compulsory education, mass migration from rural to urban areas in the 20th century, as well as other factors. The standard language is used in written and spoken form almost everywhere in the country, from Kaliningrad and Saint Petersburg in the West to Vladivostok and Petropavlovsk-Kamchatsky in the East, the enormous distance between notwithstanding.

Despite leveling after 1900, especially in matters of vocabulary and phonetics, a number of dialects still exist in Russia. Some linguists divide the dialects of Russian into two primary regional groupings, "Northern" and "Southern", with Moscow lying on the zone of transition between the two. Others divide the language into three groupings, Northern, Central (or Middle), and Southern, with Moscow lying in the Central region. All dialects also divided in two main chronological categories: the dialects of "primary formation" (the territory of Muscovy, roughly consists of the modern Central and Northwestern Federal districts); and "secondary formation" (other territories where Russian was brought by migrants from primary formation territories or adopted by the local population). Dialectology within Russia recognizes dozens of smaller-scale variants. The dialects often show distinct and non-standard features of pronunciation and intonation, vocabulary, and grammar. Some of these are relics of ancient usage now completely discarded by the standard language.

The Northern Russian dialects and those spoken along the Volga River typically pronounce unstressed clearly, a phenomenon called okanye (). Besides the absence of vowel reduction, some dialects have high or diphthongal in the place of and in stressed closed syllables (as in Ukrainian) instead of Standard Russian and . Another Northern dialectal morphological feature is a post-posed definite article "-to", "-ta", "-te" similarly to that existing in Bulgarian and Macedonian.

In the Southern Russian dialects, instances of unstressed and following palatalized consonants and preceding a stressed syllable are not reduced to (as occurs in the Moscow dialect), being instead pronounced in such positions (e.g. is pronounced , not ) – this is called yakanye ().
Consonants include a fricative, a semivowel and , whereas the Standard and Northern dialects have the consonants , , and final and , respectively.
The morphology features a palatalized final in 3rd person forms of verbs (this is unpalatalized in the Standard and Northern dialects). Some of these features such as akanye and yakanye, a debuccalized or lenited , a semivowel and palatalized final in 3rd person forms of verbs are also present in modern Belarusian and some dialects of Ukrainian (Eastern Polesian), indicating a linguistic continuum.

The city of Veliky Novgorod has historically displayed a feature called chokanye or tsokanye ( or ), in which and were switched or merged. So, ('heron') has been recorded as . Also, the second palatalization of velars did not occur there, so the so-called ě² (from the Proto-Slavic diphthong *ai) did not cause to shift to ; therefore, where Standard Russian has ('chain'), the form is attested in earlier texts.

Among the first to study Russian dialects was Lomonosov in the 18th century. In the 19th, Vladimir Dal compiled the first dictionary that included dialectal vocabulary. Detailed mapping of Russian dialects began at the turn of the 20th century. In modern times, the monumental "Dialectological Atlas of the Russian Language" ( - "Dialektologichesky atlas russkogo yazyka"), was published in three folio volumes 1986–1989, after four decades of preparatory work.


Russian is written using a Cyrillic alphabet. The Russian alphabet consists of 33 letters. The following table gives their upper case forms, along with values for each letter's typical sound:
Older letters of the Russian alphabet include , which merged to ( or ); and , which both merged to (); , which merged to (); , which merged to (); , which merged to ( or ); and and , which later were graphically reshaped into and merged phonetically to or . While these older letters have been abandoned at one time or another, they may be used in this and related articles. The yers and originally indicated the pronunciation of "ultra-short" or "reduced" , .

Because of many technical restrictions in computing and also because of the unavailability of Cyrillic keyboards abroad, Russian is often transliterated using the Latin alphabet. For example, ('frost') is transliterated "moroz", and ('mouse'), "mysh" or "myš"'. Once commonly used by the majority of those living outside Russia, transliteration is being used less frequently by Russian-speaking typists in favor of the extension of Unicode character encoding, which fully incorporates the Russian alphabet. Free programs leveraging this Unicode extension are available which allow users to type Russian characters, even on Western 'QWERTY' keyboards.

The Russian alphabet has many systems of character encoding. KOI8-R was designed by the Soviet government and was intended to serve as the standard encoding. This encoding was and still is widely used in UNIX-like operating systems. Nevertheless, the spread of MS-DOS and OS/2 (IBM866), traditional Macintosh (ISO/IEC 8859-5) and Microsoft Windows (CP1251) meant the proliferation of many different encodings as de facto standards, with Windows-1251 becoming a de facto standard in Russian Internet and e-mail communication during the period of roughly 1995–2005.

All the obsolete 8-bit encodings are rarely used in the communication protocols and text-exchange data formats, having been mostly replaced with UTF-8. A number of encoding conversion applications were developed. "iconv" is an example that is supported by most versions of Linux, Macintosh and some other operating systems; but converters are rarely needed unless accessing texts created more than a few years ago.

In addition to the modern Russian alphabet, Unicode (and thus UTF-8) encodes the Early Cyrillic alphabet (which is very similar to the Greek alphabet), as well as all other Slavic and non-Slavic but Cyrillic-based alphabets.

Russian spelling is reasonably phonemic in practice. It is in fact a balance among phonemics, morphology, etymology, and grammar; and, like that of most living languages, has its share of inconsistencies and controversial points. A number of rigid spelling rules introduced between the 1880s and 1910s have been responsible for the former whilst trying to eliminate the latter.

The current spelling follows the major reform of 1918, and the final codification of 1956. An update proposed in the late 1990s has met a hostile reception, and has not been formally adopted. The punctuation, originally based on Byzantine Greek, was in the 17th and 18th centuries reformulated on the French and German models.

According to the Institute of Russian Language of the Russian Academy of Sciences, an optional acute accent () may, and sometimes should, be used to mark stress. For example, it is used to distinguish between otherwise identical words, especially when context does not make it obvious: ("zamók" - "lock") – ("zámok" - "castle"), ("stóyashchy" - "worthwhile") – ("stoyáshchy" - "standing"), ("chudnó" - "this is odd") – ("chúdno" - "this is marvellous"), ("molodéts" - "well done!") – ("mólodets" - "fine young man"), ("uznáyu" - "I shall learn it") – ("uznayú" - "I recognize it"), ("otrezát" - "to be cutting") – ("otrézat" - "to have cut"); to indicate the proper pronunciation of uncommon words, especially personal and family names, like ("aféra", "scandal, affair"), ("gúru", "guru"), ("García"), ("Olésha"), ("Fermi"), and to show which is the stressed word in a sentence, for example ("Tý syel pechenye?" - "Was it "you" who ate the cookie?") – ("Ty syél pechenye?" - "Did you "eat" the cookie?) – ("Ty syel pechénye?" "Was it the "cookie" you ate?"). Stress marks are mandatory in lexical dictionaries and books for children or Russian learners.

The phonological system of Russian is inherited from Common Slavonic; it underwent considerable modification in the early historical period before being largely settled around the year 1400.

The language possesses five vowels (or six, under the St.Petersburg Phonological School), which are written with different letters depending on whether the preceding consonant is palatalized. The consonants typically come in plain vs. palatalized pairs, which are traditionally called "hard" and "soft." The hard consonants are often velarized, especially before front vowels, as in Irish and Marshallese. The standard language, based on the Moscow dialect, possesses heavy stress and moderate variation in pitch. Stressed vowels are somewhat lengthened, while unstressed vowels tend to be reduced to near-close vowels or an unclear schwa. (See also: vowel reduction in Russian.)

The Russian syllable structure can be quite complex, with both initial and final consonant clusters of up to four consecutive sounds. Using a formula with V standing for the nucleus (vowel) and C for each consonant, the maximal structure can be described as follows:

However, Russian has a constraint on syllabification such that syllables cannot span multiple morphemes.

Clusters of four consonants are not very common however, especially within a morpheme. Some examples are: ( "vzglyad", 'glance'), ( "gosudarstv", 'of the states'), ( "stroitelstv", 'of the constructions').

Russian is notable for its distinction based on palatalization of most of its consonants. While do have palatalized allophones , only might be considered a phoneme, though it is marginal and generally not considered distinctive. The only native minimal pair that argues for being a separate phoneme is ( "eto tkyot" - "it weaves") (, "etot kot" - "this cat"). Palatalization means that the center of the tongue is raised during and after the articulation of the consonant. In the case of and , the tongue is raised enough to produce slight frication (affricate sounds; cf. Belarusian ць, дзь, or Polish ć, dź). The sounds are dental, that is, pronounced with the tip of the tongue against the teeth rather than against the alveolar ridge.

Russian has five or six vowels in stressed syllables, and in some analyses , but in most cases these vowels have merged to only two to four vowels when unstressed: (or ) after hard consonants and after soft ones.

Russian has preserved an Indo-European synthetic-inflectional structure, although considerable levelling has taken place.
Russian grammar encompasses:

The spoken language has been influenced by the literary one but continues to preserve characteristic forms. The dialects show various non-standard grammatical features, some of which are archaisms or descendants of old forms since discarded by the literary language.

The Church Slavonic language (not to be confused with Old Church Slavonic which was introduced during the Christianization of the Kievan Rus' in the 10th century) was introduced to Moskovy in the late 15th century and was adopted as official language for correspondence for convenience. Firstly with the newly conquered southwestern regions of former Kyivan Rus and Grand Duchy of Lithuania, later, when Moskovy cut its ties with the Golden Horde, for communication between all newly consolidated regions of Moskovy.

In terms of actual grammar, there are three tenses in Russian - past, present, and future - and each verb has two aspects (perfective and imperfective). Russian nouns each have a gender - either feminine, masculine, or neuter, indicated by spelling at the end of the word. Words change depending on both their gender and function in the sentence. Russian has six cases: Nominative (for the subject of the sentence), Accusative (for direct objects), Dative (for indirect objects), Genitive (to indicate possession), Instrumental (to indicate 'with' or 'by means of'), and Prepositional (used after a preposition). Verbs of motion in Russian - such as 'go', 'walk', 'run', 'swim', and 'fly' - use the imperfective or perfective form to indicate a single or return trip, and also use a multitude of prefixes to add more meaning to the verb.

See History of the Russian language for an account of the successive foreign influences on Russian.

The number of listed words or entries in some of the major dictionaries published during the past two centuries, and the total vocabulary of Alexander Pushkin (who is credited with greatly augmenting and codifying literary Russian), are as follows:

The history of the Russian language may be divided into the following periods:

Judging by the historical records, by approximately 1000 AD the predominant ethnic group over much of modern European Russia, Ukraine, and Belarus was the Eastern branch of the Slavs, speaking a closely related group of dialects. The political unification of this region into Kievan Rus' in about 880, from which modern Russia, Ukraine, and Belarus trace their origins, established Old East Slavic as a literary and commercial language. It was soon followed by the adoption of Christianity in 988 and the introduction of the South Slavic Old Church Slavonic as the liturgical and official language. Borrowings and calques from Byzantine Greek began to enter the Old East Slavic and spoken dialects at this time, which in their turn modified the Old Church Slavonic as well.
Dialectal differentiation accelerated after the breakup of Kievan Rus' in approximately 1100. On the territories of modern Belarus and Ukraine emerged Ruthenian and in modern Russia medieval Russian. They became distinct since the 13th century, i.e. following the division of the land between the Grand Duchy of Lithuania and the Poland in the west and independent Novgorod and Pskov feudal republics plus numerous small duchies (which came to be vassals of the Tatars) in the east.

The official language in Moscow and Novgorod, and later, in the growing Muscovy, was Church Slavonic, which evolved from Old Church Slavonic and remained the literary language for centuries, until the Petrine age, when its usage became limited to biblical and liturgical texts. Russian developed under a strong influence of Church Slavonic until the close of the 17th century; afterward the influence reversed, leading to corruption of liturgical texts.

The political reforms of Peter the Great (Пётр Вели́кий, "Pyótr Velíky") were accompanied by a reform of the alphabet, and achieved their goal of secularization and Westernization. Blocks of specialized vocabulary were adopted from the languages of Western Europe. By 1800, a significant portion of the gentry spoke French daily, and German sometimes. Many Russian novels of the 19th century, e.g. Leo Tolstoy's (Лев Толсто́й) "War and Peace", contain entire paragraphs and even pages in French with no translation given, with an assumption that educated readers would not need one.

The modern literary language is usually considered to date from the time of Alexander Pushkin () in the first third of the 19th century. Pushkin revolutionized Russian literature by rejecting archaic grammar and vocabulary (so-called — "high style") in favor of grammar and vocabulary found in the spoken language of the time. Even modern readers of younger age may only experience slight difficulties understanding some words in Pushkin's texts, since relatively few words used by Pushkin have become archaic or changed meaning. In fact, many expressions used by Russian writers of the early 19th century, in particular Pushkin, Mikhail Lermontov (), Nikolai Gogol (), Aleksander Griboyedov (), became proverbs or sayings which can be frequently found even in modern Russian colloquial speech.

<poem> ("Zímny vécher,")

The political upheavals of the early 20th century and the wholesale changes of political ideology gave written Russian its modern appearance after the spelling reform of 1918. Political circumstances and Soviet accomplishments in military, scientific, and technological matters (especially cosmonautics), gave Russian a worldwide prestige, especially during the mid-20th century.

During the Soviet period, the policy toward the languages of the various other ethnic groups fluctuated in practice. Though each of the constituent republics had its own official language, the unifying role and superior status was reserved for Russian, although it was declared the official language only in 1990. Following the break-up of the USSR in 1991, several of the newly independent states have encouraged their native languages, which has partly reversed the privileged status of Russian, though its role as the language of post-Soviet national discourse throughout the region has continued.

The Russian language in the world declined after 1991 due to the collapse of the Soviet Union and decrease in the number of Russians in the world and diminution of the total population in Russia (where Russian is an official language), however this has since been reversed.

According to figures published in 2006 in the journal "" research deputy director of Research Center for Sociological Research of the Ministry of Education and Science (Russia) Arefyev A. L., the Russian language is gradually losing its position in the world in general, and in Russia in particular. In 2012, A. L. Arefyev published a new study "Russian language at the turn of the 20th-21st centuries", in which he confirmed his conclusion about the trend of weakening of the Russian language after the Soviet Union's collapse in various regions of the world (findings published in 2013 in the journal ""). In the countries of the former Soviet Union the Russian language was being replaced or used in conjunction with local languages. Currently, the number of speakers of Russian in the world depends on the number of Russians in the world and total population in Russia.








</doc>
<doc id="25432" url="https://en.wikipedia.org/wiki?curid=25432" title="Rush (band)">
Rush (band)

Rush was a Canadian rock band consisting of Geddy Lee (bass, vocals, keyboards), Alex Lifeson (guitars), and Neil Peart (drums, percussion). Formed in 1968, the band went through several configurations until arriving at its longest and classic line-up when Peart replaced original drummer John Rutsey in July 1974, two weeks before the group's first tour of the United States.

Rush is known for its musicianship, complex compositions, and eclectic lyrical motifs drawing heavily on science fiction, fantasy, and philosophy. The band's musical style has changed several times over the years, from a blues-inspired hard rock beginning, later moving into progressive rock, and including a period marked by heavy use of synthesizers. In the early 1990s, Rush returned to a guitar-driven hard rock sound, which continued for the rest of their career. Rush announced plans to cease large-scale touring at the end of 2015, following the conclusion of their R40 Live Tour. After nearly three years of an uncertain future, Lifeson reluctantly announced in January 2018 that the band was not going to continue. Peart died of brain cancer at the age of 67 on January 7, 2020.

According to the RIAA, Rush ranks 88th with sales of 25 million units in the U.S. Although total worldwide album sales are not calculated by any single entity, several industry sources estimated Rush's total worldwide album sales at over 40 million units as of 2017. The group has been awarded 24 gold, 14 platinum, and 3 multi-platinum albums.

Rush has been nominated for seven Grammy Awards, has won several Juno Awards, and won an International Achievement Award at the 2009 SOCAN Awards. The band was inducted into the Canadian Music Hall of Fame in 1994 and the Rock and Roll Hall of Fame in 2013. Over their careers, the members of Rush have been acknowledged as some of the most proficient players on their respective instruments, with each band member winning numerous awards in magazine readers' polls.

The original line-up formed in the neighbourhood of Willowdale in Toronto, Ontario, by guitarist Alex Lifeson, bassist and front man Jeff Jones, and drummer John Rutsey on September 18, 1968. Within a couple of weeks of forming, and before their second performance, bassist and lead vocalist Jones left the band and was replaced by Geddy Lee, a schoolmate of Lifeson. Their first gigs took place at the Coff-Inn, a youth centre in the basement of St. Theodore of Canterbury Anglican Church in North York. After several line-up reformations, Rush's official incarnation formed in May 1971 consisting of Lee, Lifeson, and Rutsey. The name "Rush" was suggested by Rutsey's brother, Bill. The band was managed by local Toronto resident Ray Danniels, a frequent attendee of Rush's early shows.

After gaining stability in the line-up and honing their skills on the local bar and high school dance circuit, the band members released their first single "Not Fade Away", a cover of the Buddy Holly song, in 1973. The B-side contained an original composition, "You Can't Fight It", credited to Lee and Rutsey. The single generated little reaction (#99 on the RPM charts) and, because of record company indifference, the band formed their own independent label, Moon Records.
With the assistance of Danniels and the newly enlisted engineer Terry Brown, the band released its self-titled debut album in 1974, which was considered highly derivative of Led Zeppelin. "Rush" had limited local popularity until the album was picked up by WMMS, a radio station in Cleveland, Ohio. Donna Halper, a music director and DJ working at the station, selected "Working Man" for her regular playlist. The song's blue-collar theme resonated with hard rock fans, and this newfound popularity led to the album being re-released by Mercury Records in the U.S. Immediately after the release of the debut album, Rutsey left the band due to health difficulties stemming from diabetes and his distaste for touring. His last performance with the band was on July 25, 1974, at Centennial Hall in London, Ontario.

Rush held auditions for a new drummer and selected Neil Peart as Rutsey's replacement. Peart officially joined the band on July 29, 1974, sixteen days before the group's first US tour. They performed their first concert together, opening for Uriah Heep and Manfred Mann with an attendance of over 11,000 people at the Civic Arena in Pittsburgh, Pennsylvania on August 14. In addition to becoming the band's drummer, Peart assumed the role of principal lyricist from Lee, who had very little interest in writing, despite having penned the lyrics of the band's first album. Lee and Lifeson focused primarily on the instrumental aspects of Rush, and pushed the band in an increasingly progressive rock-oriented direction. "Fly by Night" (1975), Rush's first album after recruiting Peart, saw the inclusion of the band's first epic mini-tale "By-Tor and the Snow Dog", replete with complex arrangements and a multi-section format. Lyrical themes also underwent dramatic changes because of Peart's love for fantasy and science-fiction literature. Despite these new styles, some other songs on the album mirrored the simplistic blues style found on Rush's debut.
The band followed "Fly by Night" quickly with "Caress of Steel" (1975), a five-track album featuring two extended multi-chapter songs, "The Necromancer" and "The Fountain of Lamneth". Some critics said "Caress of Steel" was unfocused and an audacious move for the band because of the placement of two back-to-back protracted songs, as well as a heavier reliance on atmospherics and story-telling, a large deviation from "Fly by Night". Intended to be the band's break-through album, "Caress of Steel" sold below expectations and the promotional tour consisted of smaller venues, which led to the moniker the "Down the Tubes Tour".

In light of these events, Rush's record label tried to pressure the members into moulding their next album in a more commercially friendly and accessible fashion; the band ignored the requests and developed their next album "2112" with a 20-minute title track divided into seven sections. Despite this, the album was the band's first taste of commercial success and their first platinum album in Canada. The supporting tour culminated in a three-night stand at Massey Hall in Toronto, which the band recorded for the release of their first live album, "All the World's a Stage". AllMusic critic Greg Prato notes the album demarcates the boundary between the band's early years and the next era of their music.

After "2112", Rush went to the United Kingdom to record "A Farewell to Kings" (1977) and "Hemispheres" (1978) at Rockfield Studios in Wales. These albums saw the band members expanding the progressive elements in their music. "As our tastes got more obscure," Lee said in an interview, "we discovered more progressive rock-based bands like Yes, Van der Graaf Generator and King Crimson, and we were very inspired by those bands. They made us want to make our music more interesting and more complex and we tried to blend that with our own personalities to see what we could come up with that was indisputably us." Increased synthesizer use, lengthy songs, and highly dynamic playing featuring complex time signature changes became a staple of Rush's compositions. To achieve a broader, more progressive sound, Lifeson began to experiment with classical and twelve-string guitars, and Lee added bass-pedal synthesizers and Minimoog. Likewise, Peart's percussion became diversified in the form of triangles, glockenspiel, wood blocks, cowbells, timpani, gong, and chimes. Beyond instrument additions, the band kept in stride with the progressive rock trends by continuing to compose long, conceptual songs with science fiction and fantasy overtones. As the new decade approached, Rush gradually began to dispose of its older styles of music in favour of shorter and sometimes softer arrangements. The lyrics up to this point were heavily influenced by classical poetry, fantasy literature, science fiction, and the writings of novelist Ayn Rand, as exhibited most prominently by their 1975 song "Anthem" from "Fly By Night" and a specifically acknowledged derivation in "2112" (1976).

"Permanent Waves" (1980) shifted Rush's style of music with the introduction of reggae and new wave elements. Although a hard rock style was still evident, more synthesizers were introduced. Moreover, because of the limited airplay Rush's previous extended-length songs received, "Permanent Waves" included shorter, more radio-friendly songs such as "The Spirit of Radio" and "Freewill", two songs that helped "Permanent Waves" become Rush's first US Top 5 album. Meanwhile, Peart's lyrics shifted toward an expository tone with subject matter that dwelled less on fantastical or allegorical story-telling and more heavily on topics that explored humanistic, social, and emotional elements. Rush joined with fellow Toronto-based rock band Max Webster on July 28, 1980, to record "Battle Scar" for their 1980 release, "Universal Juveniles". Max Webster lyricist Pye Dubois offered the band lyrics to a song he had written. The band accepted; the song went on, after reworking by Peart, to become "Tom Sawyer".
Rush's popularity reached its pinnacle with the release of "Moving Pictures" in 1981. "Moving Pictures" essentially continued where "Permanent Waves" left off, extending the trend of accessible and commercially friendly progressive rock that helped thrust them into the spotlight. The lead track, "Tom Sawyer", is probably the band's best-known song with "Limelight" also receiving satisfactory responses from listeners and radio stations. "Moving Pictures" was Rush's last album to feature an extended song, the eleven-minute "The Camera Eye". The song also contained the band's heaviest usage of synthesizers yet, hinting Rush's music was shifting direction once more. "Moving Pictures" reached No. 3 on the "Billboard" 200 album chart and has been certified quadruple platinum by the Recording Industry Association of America.

Following the success of "Moving Pictures" and having completed another four studio albums, Rush released a second live recording, "Exit... Stage Left", in 1981.

The band underwent another stylistic change with the recording of "Signals" in 1982. While Lee's synthesizers had been featured instruments ever since the late 1970s, keyboards were suddenly shifted from the contrapuntal background to the melodic front-lines in songs like "Countdown" and the lead-off track "Subdivisions". Both feature prominent lead synthesizer lines with minimalistic guitar chords and solos. Other previously unused instrument additions were seen in the song "Losing It", featuring collaborator Ben Mink on electric violin.
"Signals" also represented a drastic stylistic transformation apart from instrumental changes. The album contained Rush's only US top-40 pop hit, "New World Man", while other more experimental songs such as "Digital Man", "The Weapon", and "Chemistry" expanded the band's use of ska, reggae, and funk. Although the band members consciously decided to move in this overall direction, creative differences between the band and long-time producer Terry Brown began to emerge. The band felt dissatisfied with Brown's studio treatment of "Signals", while Brown was becoming more uncomfortable with the increased use of synthesizers in the music. Ultimately, Rush and Brown parted ways in 1983, and the experimentation with new electronic instruments and varying musical styles would come into further play on their next studio album.

The style and production of "Signals" were augmented and taken to new heights on "Grace Under Pressure" (1984). It was Peart who named the album, as he borrowed the words of Ernest Hemingway to describe what the band had to go through after making the decision to leave Terry Brown. Producer Steve Lillywhite, who gained fame with successful productions of Simple Minds and U2, was enlisted to produce "Grace Under Pressure". He backed out at the last moment, however, much to the ire of Lee, Lifeson and Peart. Lee said "Steve Lillywhite is really not a man of his word ... after agreeing to do our record, he got an offer from Simple Minds, changed his mind, blew us off ... so it put us in a horrible position." Rush eventually hired Peter Henderson to co-produce and engineer the album instead.
Musically, although Lee's use of sequencers and synthesizers remained the band's cornerstone, his focus on new technology was complemented by Peart's adaptation of Simmons electronic drums and percussion. Lifeson's contributions on the album were decidedly enhanced, in response to the minimalistic role he played on "Signals". Still, many of his trademark guitar textures remained intact in the form of open reggae chords and funk and new-wave rhythms.

With new producer Peter Collins, the band released "Power Windows" (1985) and "Hold Your Fire" (1987). The music on these two albums gives far more emphasis and prominence to Lee's multi-layered synthesizer work. While fans and critics took notice of Lifeson's diminished guitar work, his presence was still palpable. Lifeson, like many guitarists in the mid-to-late 1980s, experimented with processors that reduced his instrument to echoey chord bursts and razor-thin leads. "Hold Your Fire" represents both an extension of the guitar stylings found on "Power Windows", and, according to Allmusic critic Eduardo Rivadavia, the culmination of this era of Rush. Whereas the previous five Rush albums sold platinum or better, "Hold Your Fire" only went gold in November 1987, although it managed to peak at number 13 on the "Billboard" 200.

A third live album and video, "A Show of Hands" (1989), was also released by Anthem and Mercury following the "Power Windows" and "Hold Your Fire" tours, demonstrating the aspects of Rush in the '80s. "A Show of Hands" met with strong fan approval, but "Rolling Stone" critic Michael Azerrad dismissed it as "musical muscle" with 1.5 stars, claiming Rush fans viewed their favourite power trio as "the holy trinity". Nevertheless, "A Show of Hands" managed to surpass the gold album mark, reaching number 21 on the "Billboard" 200. At this point, the group decided to change international record labels from Mercury to Atlantic. After Rush's departure in 1989, Mercury released a double platinum two-volume compilation of their Rush catalogue, "Chronicles" (1990).

Rush started to deviate from its 1980s style with the albums "Presto" and "Roll the Bones". Produced by record engineer and musician Rupert Hine, these two albums saw Rush shedding much of its keyboard-saturated sound. Beginning with "Presto" (1989), the band opted for arrangements notably more guitar-centric than the previous two studio albums. Although synthesizers were still used in many songs, the instrument was no longer featured as the centrepiece of Rush's compositions. Continuing this trend, "Roll the Bones" (1991) extended the use of the standard three-instrument approach with even less focus on synthesizers than its predecessor. While musically these albums do not deviate significantly from a general pop-rock sound, Rush incorporated traces of other musical styles. "Roll the Bones", for example exhibits funk and hip hop elements, and the instrumental track "Where's My Thing?" features several jazz components. This return to three-piece instrumentation helped pave the way for future albums, which would adopt a more streamlined rock formula.

The transition from synthesizers to more guitar-oriented and organic instrumentation continued with "Counterparts" (1993) and its follow-up, "Test for Echo" (1996), again both produced in collaboration with Peter Collins. Up to this point, "Counterparts" and "Test For Echo" were two of Rush's most guitar-driven albums. The latter album also includes elements of jazz and swing-style drumming by Peart, which he had learned from Freddie Gruber during the interim between "Counterparts" and "Test For Echo". In October 1996, in support of "Test For Echo", the band embarked on a North American tour, the band's first without an opening act and dubbed "An Evening with Rush". The tour was broken up into two segments spanning October through December 1996 and May through July 1997.

After the conclusion of the "Test for Echo" tour in 1997, the band entered a five-year hiatus primarily due to personal tragedies in Peart's life. Peart's daughter Selena died in a car crash in August 1997, followed by the death of his wife Jacqueline from cancer in June 1998. Peart took a hiatus to mourn and reflect, during which he travelled extensively throughout North America on his BMW motorcycle, covering . At some point in his journey, Peart decided to return to the band. Peart's book "" is a chronicle of his journey. In the book, he writes of how he had told his bandmates at Selena's funeral, "consider me retired." On November 10, 1998, a three-disc live album entitled "Different Stages" was released, dedicated to the memory of Selena and Jacqueline. Mixed by producer Paul Northfield and engineered by Terry Brown, it features recorded performances from the band's "Counterparts", "Test For Echo", and "A Farewell to Kings" tours, marking the band's fourth live album.

After a time of grief and recovery, and while visiting long-time Rush photographer Andrew MacNaughtan in Los Angeles, Peart was introduced to his future wife, photographer Carrie Nuttall. Peart married Nuttall on September 9, 2000. In early 2001, he announced to his bandmates he was ready to once again enter the studio and get back into the business of making music.

With the help of producer Paul Northfield, the band returned in May 2002 with "Vapor Trails", written and recorded in Toronto. To herald the band's comeback, the single and lead track from the album, "One Little Victory", was designed to grab the attention of listeners with its rapid guitar and drum tempos. "Vapor Trails" marked the first Rush studio recording to not include any keyboards or synthesizers since "Caress of Steel", released 27 years earlier. While the album is almost completely guitar-driven, it is mostly devoid of any traditional guitar solos, a conscious decision by Lifeson. According to the band, the entire developmental process for "Vapor Trails" was extremely taxing and took approximately 14 months to finish, by far the longest the band had ever spent writing and recording a studio album. The album was supported by the band's first tour in six years, including first-ever concerts in Brazil and Mexico City, where they played to some of the largest crowds of their career. The largest of these was a capacity of 60,000 in São Paulo. The show at Maracanã Stadium represented the second largest attendance for a headlining Rush concert with 40,000 people.

A live album and DVD, "Rush in Rio", was released in late October 2003 featuring an entire concert performance recorded on November 23, 2002, at Maracanã Stadium in Rio de Janeiro, Brazil. The show was the last of the Vapor Trails Tour. To celebrate the band's 30th anniversary, June 2004 saw the release of "Feedback", an extended play recorded in suburban Toronto featuring eight covers of such artists as Cream, The Who and The Yardbirds, bands the members of Rush cite as inspiration around the time of their inception. To help support "Feedback" and continue celebrating their 30-year anniversary as a band, Rush hit the road again for their in the summer of 2004, playing dates in the United States, Canada, the United Kingdom, Germany, Italy, Sweden, the Czech Republic, and the Netherlands. On September 24, 2004, the concert at The Festhalle in Frankfurt, Germany was filmed for a DVD titled "", which was released on November 22, 2005. This release omitted eight songs also included on "Rush in Rio"; the complete concert was released on Blu-ray on December 8, 2009.

During promotional interviews for the "R30" DVD, the band members revealed their intention to begin writing new material in early 2006. While in Toronto, Lifeson and Lee began the songwriting process in January 2006. During this time, Peart simultaneously assumed his role of lyric writing while residing in Southern California. The following September, Rush chose to hire American producer Nick Raskulinecz to co-produce the album. The band officially entered Allaire Studios in Shokan, New York in November 2006 to record the bulk of the material. Taking the band five weeks, the sessions ended in December. On February 14, 2007, an announcement was made on the official Rush web site that the title of the new album would be "Snakes & Arrows". The first single, entitled "Far Cry", was released to North American radio stations on March 12, 2007 and reached No. 2 on the Mediabase Mainstream and Radio and Records Charts.
The Rush website, newly redesigned on March 12, 2007, to support the new album, also announced the band would embark on a tour to begin in the summer. "Snakes & Arrows" was released on May 1, 2007, in North America, where it debuted at No. 3 on the "Billboard" 200 with approximately 93,000 units sold in its first week. It sold an estimated 611,000 copies worldwide. To coincide with the beginning of Atlantic Ocean hurricane season, "Spindrift" was released as the official second radio single on June 1, 2007, while "The Larger Bowl (A Pantoum)" saw single status on June 25, 2007. "The Larger Bowl" peaked within the top 20 of both the "Billboard" Mainstream Rock and Media Base Mainstream charts, but "Spindrift" failed to appear on any commercial chart. The planned intercontinental tour in support of "Snakes & Arrows" began on June 13, 2007 in Atlanta, Georgia, coming to a close on October 29, 2007, at Hartwall Arena in Helsinki, Finland.

The 2008 portion of the "Snakes & Arrows" tour began on April 11, 2008, in San Juan, Puerto Rico, at José Miguel Agrelot Coliseum, and concluded on July 24, 2008 in Noblesville, Indiana at the Verizon Wireless Music Center. On April 15, 2008, the band released "Snakes & Arrows Live", a double live album documenting the first leg of the tour, recorded at the Ahoy arena in Rotterdam, Netherlands on October 16 and 17, 2007. A DVD and Blu-ray recording of the same concerts was released on November 24, 2008. The video also includes four songs added to the 2008 portion of the tour, recorded at Verizon Wireless Amphitheater in Atlanta, Georgia.

As Rush neared the conclusion of the "Snakes & Arrows" tour, they announced their first appearance on American television in over 30 years. They appeared on "The Colbert Report" on July 16, 2008, where they were interviewed by Stephen Colbert and performed "Tom Sawyer". Continuing to ride what film critic Manohla Dargis called a "pop cultural wave", the band appeared as themselves in the 2009 comedy film "I Love You, Man", starring Paul Rudd and Jason Segel.

On February 16, 2009, Lifeson remarked the band may begin working on a new album in the Fall of 2009 with American producer Nick Raskulinecz once again producing. In November 2009, Lee, Lifeson and Peart were awarded the International Achievement Award at the annual SOCAN Awards in Toronto. On March 19, 2010, the CBC posted a video interview with Lee and Lifeson where they discussed Rush's induction into the Canadian Songwriters Hall of Fame on March 28, 2010, at the Toronto Centre for the Arts' George Weston Recital Hall. The band was recognized for the songs "Limelight", "Closer to the Heart", "The Spirit of Radio", "Tom Sawyer" and "Subdivisions". In addition to discussing their induction, Lee and Lifeson touched on future material, and Lee said, "Just about a month and a half ago we had no songs. And now we've been writing and now we've got about 6 songs that we just love ..." On March 26, 2010, in an interview with The Globe and Mail, Lifeson reconfirmed the band had already written a half-dozen songs and there was the potential for two supporting tours, one planned for Summer 2010 and a more extensive tour planned for Summer 2011. While still uncertain of exactly how and when the new material would be released, at the time he projected a tentative Spring 2011 release date. Soon after, Peart confirmed Nick Raskulinecz had returned as co-producer.

In April 2010, Rush entered Blackbird Studios in Nashville, Tennessee with Raskulinecz to record "Caravan" and "BU2B", two new songs to be featured on the band's studio album "Clockwork Angels". Mixing was done by record engineer Richard Chycki at the Sound Kitchen in Franklin, Tennessee. "Caravan" was released on June 1, 2010 to radio stations and made available for digital download at this time along with "BU2B". Lifeson's predictions from March were confirmed, and the Time Machine Tour's first leg began on June 29 in Albuquerque, New Mexico, and finished on October 17 in Santiago, Chile, at the National Stadium. It featured the album "Moving Pictures" played in its entirety, as well as "Caravan" and "BU2B". It was suggested Rush would return to the studio after the completion of the Time Machine Tour with plans to release "Clockwork Angels" in 2011. Nonetheless, Rush announced on November 19, 2010, they would extend the Time Machine Tour. The second leg began on March 30, 2011, in Fort Lauderdale, Florida, and came to an end on July 2, 2011, in George, Washington. On November 8, 2011, the band released "", a concert DVD, Blu-ray and double CD documenting the April 15, 2011, concert at the Quicken Loans Arena in Cleveland, Ohio. After the tour's second leg was finished, Rush entered Revolution Recording studios in Toronto, Ontario, to finalize the recording of "Clockwork Angels." The second single, "Headlong Flight", was released on April 19, 2012. Peart and author Kevin J. Anderson collaborated on a novelization of "Clockwork Angels" that was released in September 2012.

"Clockwork Angels" was released in the United States and Canada on June 12, 2012, and its supporting Clockwork Angels Tour began on September 7, 2012. As of August 31, 2011, Rush switched their American distribution from Atlantic Records over to the Warner Brothers majority-owned metal label, Roadrunner Records. Roadrunner handled American distribution of "Time Machine 2011: Live in Cleveland" and "Clockwork Angels". Anthem/Universal Music would continue to release their music in Canada. On April 18, 2013, Rush was inducted into the Rock and Roll Hall of Fame.

During Rush's European leg of the "Clockwork Angels Tour", the June 8, 2013, show at the Sweden Rock Festival was the group's first festival appearance in 30 years. The band's performances on November 25, 2012, in Phoenix, Arizona and November 28, 2012, in Dallas, Texas were recorded to make a live CD/DVD/Blu-ray that was released on November 19, 2013.

On November 18, 2013, Lifeson said the band had committed to taking a year off, following the completion of the world tour in support of "Clockwork Angels". "We've committed to taking about a year off", Lifeson said. "We all agreed when we finished this ["Clockwork Angels"] tour [in early August] we were going to take this time off and we weren't going to talk about band stuff or make any plans. We committed to a year, so that's going to take us through to the end of next summer, for sure. That's the minimum. We haven't stopped or quit. Right now we're just relaxing. We're taking it easy and just enjoying our current employment."

In September 2014, the "Rush R40" box set was announced to commemorate the fortieth anniversary of the release of the band's self-titled debut album. It included five previously released live video albums, as well as various previously unreleased footage from across the band's career. On January 22, 2015, the band officially announced the Rush R40 Tour, celebrating the fortieth anniversary of Peart's membership in the band. The tour started on May 8 at the BOK Center in Tulsa, Oklahoma, and wrapped up on August 1 at The Forum in Los Angeles.

On April 29, 2015, Lifeson stated in an interview that R40 might be the final large-scale Rush tour due to his psoriatic arthritis and Peart's chronic tendinitis. He noted that it didn't necessarily mean an end to the band, suggesting the possibility of smaller tours and limited performances. He also said he would like to work on soundtracks with Lee. On December 7, 2015, Peart stated in an interview he was retiring. The following day, Lee insisted that Peart's remarks had been taken out of context, and suggested he was "simply taking a break". Lifeson confirmed in 2016 that the R40 tour was the band's last large-scale tour. The band's latest documentary, "Time Stand Still", was announced in November 2016.

On January 16, 2018, Lifeson told "The Globe and Mail" that it was unlikely that Rush would play any more shows or record new material. He was quoted as saying, "We have no plans to tour or record anymore. We're basically done. After 41 years, we felt it was enough." In October 2018, "Rolling Stone" published an interview with Lee, who stated, "I'd say I can't really tell you much other than that there are zero plans to tour again. As I said earlier, we're very close and talk all the time, but we don't talk about work. We're friends, and we talk about life as friends. I can't really tell you more than that, I'm afraid. I would say there's no chance of seeing Rush on tour again as Alex, Geddy, Neil. But would you see one of us or two of us or three of us? That's possible."

On January 7, 2020, Peart died at the age of 67 following a -year battle with glioblastoma, a type of brain cancer.

Rush's musical style changed substantially over the years. Its debut album was strongly influenced by British blues-based hard rock: an amalgam of sounds and styles from such rock bands as Black Sabbath, the Who, Cream, and Led Zeppelin. Rush became increasingly influenced by bands of the British progressive rock movement of the mid-1970s, especially Genesis, Yes, and Jethro Tull. In the tradition of progressive rock, Rush wrote extended songs with irregular and shifting time signatures, combined with fantasy and science fiction-themed lyrics. In the 1980s, Rush merged their sound with the trends of this period, experimenting with new wave, reggae, and pop rock. This period included the band's most extensive use of instruments such as synthesizers, sequencers, and electronic percussion. In the early 1990s, the band transformed their style once again to return to a more grounded hard rock style and simultaneously harmonize with the alternative rock movement.



More than 40 years of activity has provided Rush with the opportunity for musical diversity across their discography. As with many bands known for experimentation, changes have inevitably resulted in dissent among critics and fans. The bulk of the band's music has always included synthetic instruments, and this has been a source of contention among fans and critics, especially the band's heavy usage of synthesizers and keyboards during the 1980s, particularly on albums "Grace Under Pressure", "Power Windows", and "Hold Your Fire".

The members of Rush have noted people "either love Rush or hate Rush", resulting in strong detractors and an intensely loyal fan base. In 1979, "The Rolling Stone Record Guide" called it "the power boogie band for the "16" magazine graduating class". A July 2008 "Rolling Stone" article stated "Rush fans are the Trekkies/trekkers of rock". They have been cited as an influence by various musical artists, including Alice in Chains, Anthrax, Fishbone, Foo Fighters, Jane's Addiction, Manic Street Preachers, Metallica, No Doubt, Pixies, Porcupine Tree, Primus, Queensrÿche, Rage Against the Machine, Red Hot Chili Peppers, The Smashing Pumpkins, Elliott Smith and Soundgarden as well as progressive metal bands such as Meshuggah, Prototype, Dream Theater, Puya, Tool, Cynic, and Symphony X. Trent Reznor considers Rush to be one of his favourite bands in the 2010 documentary "" and has particularly cited the album "Signals" as a major influence on how to incorporate keyboards and synthesizers into hard rock.

Rush was eligible for nomination into the Rock and Roll Hall of Fame beginning in 1998; the band was nominated for entry in 2012 and their induction was announced on December 11, 2012. A reason for their previous exclusion may have been their genre. "USA Today" writer Edna Gunderson criticized the Hall of Fame for excluding some genres, including progressive rock. Supporters cited the band's accomplishments including longevity, proficiency, and influence, as well as commercial sales figures and RIAA certifications. In the years before induction, Lifeson expressed his indifference toward the perceived slight saying, "I couldn't care less. Look who's up for induction; it's a joke".

On April 24, 2010, the documentary "", directed by Scot McFadyen and Sam Dunn, premiered at the Tribeca Film Festival. It went on to receive the Tribeca Film Festival Audience Award. The film explores the band's influence on popular music and the reasons why that influence has been under-represented over the years. This is done via interviews with popular musicians, music industry professionals, and the band members themselves.

On June 25, 2010, Rush received a star on the Hollywood Walk of Fame at 6752 Hollywood Boulevard. Critical acclaim continued to mount for Rush in 2010 when, on September 28, "Classic Rock" announced Rush would be that year's Living Legends awarded at the Marshall Classic Rock Roll of Honour Awards in the UK. The award was presented November 10, 2010. On September 29, Billboard.com announced Rush would also receive the 2010 Legends of Live award for significant and lasting contributions to live music and the art of performing live and reaching fans through the concert experience. The award was presented at the Billboard Touring Awards on November 4, 2010.

In 2013, the Canadian government honored Rush with a first class "permanent" postage stamp featuring the iconic "Starman" Rush logo. It is the equivalent of a "forever" stamp in the US.

The band members were made Officers of the Order of Canada in 1996. In May 2012, the band received the Governor General's Performing Arts Award for Lifetime Artistic Achievement at a ceremony at Rideau Hall followed by a gala at the National Arts Centre celebrating the award recipients the following day. In 2017, the band members had three new microbe species named in their honour.

Geddy Lee's high-register vocal style has always been a signature of the band – and sometimes a focal point for criticism, especially during the early years of Rush's career when Lee's vocals were high-pitched, with a strong likeness to other singers like Robert Plant of Led Zeppelin. A review in "The New York Times" opined Lee's voice "suggests a munchkin giving a sermon". Although his voice has softened, it is often described as a "wail". His instrumental abilities, on the other hand, are rarely criticized. He has cited Jeff Berlin, Jack Casady, John Entwistle, Jack Bruce and Chris Squire as the bassists who had the biggest impact on his playing style. Lee's style, technique, and ability on the bass guitar have been influential to rock and heavy metal musicians, inspiring players including Steve Harris, John Myung, Les Claypool, and Cliff Burton. Lee is able to operate various pieces of instrumentation simultaneously during live concert, most evidently when Lee plays bass and keyboards, sings, and triggers foot pedals as in the song "Tom Sawyer".

Lifeson as a guitarist is best known for his signature riffing, electronic effects and processing, unorthodox chord structures, and a copious arsenal of equipment used over the years.

During his adolescent years, he was influenced by Jimi Hendrix, Pete Townshend, Jeff Beck, Eric Clapton and Jimmy Page. Lifeson incorporated touches of Spanish and classical music into Rush's sound during the 1970s, reflecting his interest in progressive rock guitarists like Steve Hackett and Steve Howe. To adapt to Lee's expanding use of synthesizers in the 1980s, Lifeson took inspiration from guitarists like Allan Holdsworth, Andy Summers of The Police and The Edge of U2, who gave him models for rethinking the guitar's role in Rush's music. Lifeson's guitar returned to the forefront in the 1990s, and especially on "Vapor Trails" (2002). During live performances, he was responsible for cuing various guitar effects, the use of bass-pedal synthesizers and backing vocals.
Peart has been voted the greatest rock drummer by music fans, critics and fellow musicians, according to Drummerworld. He was also regarded as one of the finest practitioners of the in-concert drum solo. Initially inspired by Keith Moon, Peart absorbed the influence of other rock drummers from the 1960s and 1970s such as Ginger Baker, Carmine Appice, and John Bonham. Incorporation of unusual instruments (for rock drummers of the time) such as the glockenspiel and tubular bells, along with several standard kit elements, helped create a highly varied setup. Continually modified, Peart's drumkit offered an enormous array of percussion instruments for sonic diversity. For two decades Peart honed his technique; each new Rush album introduced an expanded percussive vocabulary. In the 1990s, he reinvented his style with the help of drum coach Freddie Gruber.

Peart also served as Rush's primary lyricist, attracting much attention over the years for his eclectic style. During the band's early years, Peart's lyrics were largely fantasy/science fiction-focused, though after 1980 he focused more on social, emotional, and humanitarian issues. In 2007, he was placed second on "Blender" magazine's list of the "40 Worst Lyricists In Rock". In contrast, Allmusic has called Peart "one of rock's most accomplished lyricists", Gibson.com describes Rush's lyrics as "great", and others believe the lyrics are "brilliant".

Rush has released 24 gold records and 14 platinum records (including 3 multi-platinum), placing them fifth behind the Beatles, the Rolling Stones, Kiss and Aerosmith for the most consecutive gold or platinum studio albums by a rock band in the United States. As of 2005, Rush had sold about 25 million copies of their albums in the U.S. (ranking them 79th among recording acts) and 40 million worldwide. As of 2012, "Moving Pictures" was the band's highest-selling album (4.4 million units).

Despite dropping out of the public eye for five years after the gold-selling "Test for Echo" (which peaked at No. 5 on the "Billboard" 200 chart) and the band being relegated almost solely to classic rock stations in the U.S., "Vapor Trails" reached No. 6 on the "Billboard" 200 in its first week of release in 2002 with 108,000 copies sold. It has sold about 343,000 units to date. The subsequent "Vapor Trails" tour grossed over $24 million and included the largest audience ever to see a headlining Rush show: 60,000 fans in São Paulo, Brazil. 

Rush's triple-CD live album, "Rush in Rio" (2003), was certified gold, marking the fourth decade in which a Rush album had been released and certified at least gold. In 2004, "Feedback" cracked the top 20 on the "Billboard" 200 and received radio airplay. The band's 2007 album, "Snakes & Arrows", debuted at number 3 (just one position shy of Rush's highest peaking albums, "Counterparts" (1993) and "Clockwork Angels" (2012), which both debuted at number 2) on the "Billboard" 200, selling about 93,000 its first week of release. This marks the 13th studio album to appear in the Top 20 and the band's 27th album to appear on the chart. The album also debuted at number 1 on the Billboard's Top Rock Albums chart, and, when the album was released on the MVI format a month later, peaked at number 1 on the Top Internet Albums chart.

The tours in support of "Snakes & Arrows" in 2007 and 2008 accrued $21 million and $18.3 million, respectively, earning Rush the number 6 and 8 spots among the summers' rock concerts.

The members of Rush shared a strong work ethic, desiring to accurately recreate songs from their albums when playing live performances. To achieve this goal, beginning in the late 1980s, Rush included a capacious rack of digital samplers in their concert equipment to recreate the sounds of non-traditional instruments, accompaniments, vocal harmonies, and other sound "events" in real-time to match the sounds on the studio versions of the songs. In live performances, the band members shared duties throughout most songs. Each member had one or more MIDI controllers, which were loaded with different sounds for each song, and used available limbs to trigger the sounds while simultaneously playing their primary instrument(s). It was with this technology that the group was able to present their arrangements in a live setting with the level of complexity and fidelity fans had come to expect, and without the need to resort to the use of backing tracks or employing an additional band member. The band members' coordinated use of pedal keyboards and other electronic triggers to "play" sampled instruments and audio events was subtly visible in their live performances, especially so on , their 2005 concert DVD.

A staple of Rush's concerts was a Neil Peart drum solo. Peart's drum solos included a basic framework of routines connected by sections of improvisation, making each performance unique. Each successive tour saw the solo more advanced, with some routines dropped in favour of newer, more complex ones. Since the mid-1980s, Peart had used MIDI trigger pads to trigger sounds sampled from various pieces of acoustic percussion that would otherwise consume far too much stage area, such as a marimba, harp, temple blocks, triangles, glockenspiel, orchestra bells, tubular bells, and vibraslap as well as other, more esoteric percussion.

One prominent feature of Rush's concerts were props on stage, at one point called "diversions". These props have included washing machines, vintage popcorn poppers, or animations and inflatable rabbits emerging from giant hats behind the band. Starting in the mid-90s, the props often took up Lee's side of the stage (stage left) as a way to balance out the amp stacks on Lifeson's side (stage right) when Lee opted to use a venue's house system instead of amps.

Rush actively participates in philanthropic causes. The band was one of several hometown favourites to play Molson Canadian Rocks for Toronto, also dubbed SARStock, at Downsview Park in Toronto on July 30, 2003, with an attendance of over half a million people. The concert was intended to benefit the Toronto economy after the SARS outbreaks earlier in the year. The band has also sustained an interest in promoting human rights. They donated $100,000 to the Canadian Museum for Human Rights after a concert they held in Winnipeg on May 24, 2008. Rush continues to sell T-shirts and donate the proceeds to the museum.

On July 24, 2013, Rush performed a benefit concert in Red Deer, Alberta, at the ENMAX Centrium with all proceeds going to the Canadian Red Cross to help victims of the 2013 flooding that devastated many regions of southern Alberta. The original venue for the show, the Scotiabank Saddledome, was heavily damaged from the flooding and was unavailable for the concert date as originally planned.

The individual members of Rush have also been a part of philanthropic causes. Hughes & Kettner zenTera and TriAmp electronics have been endorsed and used by Lifeson for many years. A custom signature amplifier was engineered by Lifeson and released in April 2005 with the stipulation UNICEF will receive a donation in the amount of $50 for every Alex Lifeson Signature TriAmp sold. Lee, a longtime fan of baseball, donated 200 baseballs signed by famous Negro League players, including Willie Mays, Hank Aaron, and Josh Gibson, to the Negro Leagues Baseball Museum in June 2008. In late 2009, Geddy Lee and Alex Lifeson launched an auction for their initiative "Grapes Under Pressure", in support of the cause "Grapes for Humanity". The auction consisted of items from the band such as signed guitars, cymbals and basses, as well as autographs on all items by the band members. There were also autographs by band members from Depeche Mode, Tool, the Fray, Judas Priest, Pearl Jam and more, as well as signatures from Ricky, Julian and Bubbles from "Trailer Park Boys" on a rare Epiphone guitar.

The band is featured on the album "Songs for Tibet", appearing with other celebrities as an initiative to support Tibet and the current Dalai Lama Tenzin Gyatso. The album was made downloadable on August 5, 2008, via iTunes and was released commercially August 12, 2008.

Rush has also been a big supporter of Little Kids Rock, a national nonprofit that works to restore and revitalize music education programs in disadvantaged U.S. public schools. They teamed up with Musician's Friend and Sabian to help Little Kids Rock provide percussion to public schools nationwide. They donated $500 of the proceeds from every Neil Peart Paragon Cymbal Pack sold, each of which came with a free splash cymbal personalized, autographed, and dated by Peart. The cause-based marketing initiative raised over $50,000 for Little Kids Rock.

Studio albums








</doc>
<doc id="25433" url="https://en.wikipedia.org/wiki?curid=25433" title="Ronald Reagan">
Ronald Reagan

Ronald Wilson Reagan (; February 6, 1911 – June 5, 2004) was an American politician who served as the 40th president of the United States from 1981 to 1989 and became a highly influential voice of modern conservatism. Prior to his presidency, he was a Hollywood actor and union leader before serving as the 33rd governor of California from 1967 to 1975.

Reagan was raised in a low-income family in small towns of northern Illinois. He graduated from Eureka College in 1932 and worked as a sports commentator on several regional radio stations. After moving to California in 1937, he found work as an actor and starred in a few major productions. Reagan was twice elected president of the Screen Actors Guild—the labor union for actors—where he worked to root out Communist influence. In the 1950s, he moved into television and was a motivational speaker at General Electric factories, during which time he became a conservative. Reagan was a Democrat until 1962 when he switched to the Republican Party. In 1964, Reagan's speech "A Time for Choosing" supported Barry Goldwater's foundering presidential campaign and earned him national attention as a new conservative spokesman. Building a network of supporters, Reagan was elected governor of California in 1966. As governor, Reagan raised taxes, turned a state budget deficit to a surplus, challenged the protesters at the University of California, ordered in National Guard troops during a period of protest movements in 1969, and was re-elected in 1970. He twice ran unsuccessfully for the Republican presidential nomination, in 1968 and 1976. 

In 1980, Reagan won the Republican presidential nomination and defeated the incumbent president, Jimmy Carter. At of age at the time of his first inauguration, Reagan was the oldest person to assume the U.S. presidency, a distinction he held until 2017, when Donald Trump was inaugurated at age . Reagan faced former vice president Walter Mondale when he ran for re-election in 1984, and defeated him, winning the most electoral votes of any U.S. president, 525, or 97.6% of the 538 votes in the Electoral College. It was the second-most lopsided presidential election in modern U.S. history after Franklin D. Roosevelt's 1936 victory over Alfred M. Landon, in which he won 98.5% or 523 of the (then-total) 531 electoral votes.
Soon after taking office as president, Reagan began implementing sweeping new political and economic initiatives. His supply-side economic policies, dubbed "Reaganomics", advocated tax rate reduction to spur economic growth, economic deregulation, and reduction in government spending. In his first term, he survived an assassination attempt, spurred the War on Drugs, and fought public sector labor. Over his two terms, the economy saw a reduction of inflation from 12.5% to 4.4% and an average real GDP annual growth of 3.4%. Reagan enacted cuts in domestic discretionary spending, cut taxes, and increased military spending, which contributed to increased federal outlays overall, even after adjustment for inflation. Foreign affairs dominated his second term, including ending the Cold War, the bombing of Libya, the Iran–Iraq War, and the Iran–Contra affair. In June 1987, four years after he publicly described the Soviet Union as an "evil empire", Reagan challenged Soviet General Secretary Mikhail Gorbachev to "tear down this wall!", during a speech at the Brandenburg Gate. He transitioned Cold War policy from détente to rollback by escalating an arms race with the USSR while engaging in talks with Gorbachev. The talks culminated in the INF Treaty, which shrank both countries' nuclear arsenals. Reagan began his presidency during the decline of the Soviet Union, and the Berlin Wall fell just ten months after the end of his term. Germany reunified the following year, and on December 26, 1991 (nearly three years after he left office), the Soviet Union collapsed.

When Reagan left office in 1989, he held an approval rating of 68%, matching those of Franklin D. Roosevelt, and later Bill Clinton, as the highest ratings for departing presidents in the modern era. He was the first president since Dwight D. Eisenhower to serve two full terms after the five prior presidents did not. Although he had planned an active post-presidency, Reagan disclosed in November 1994 that he had been diagnosed with Alzheimer's disease earlier that year. Afterward, his informal public appearances became more infrequent as the disease progressed. He died at home on June 5, 2004. His tenure constituted a realignment toward conservative policies in the United States, and he is an icon among conservatives. Evaluations of his presidency among historians and the general public place him among the upper tier of American presidents.

Ronald Wilson Reagan was born on February 6, 1911, in an apartment on the second floor of a commercial building in Tampico, Illinois. He was the younger son of Nelle Clyde (; 1883–1962) and Jack Reagan (1883–1941). Jack was a salesman and storyteller whose grandparents were Irish Catholic emigrants from County Tipperary, while Nelle was of English, Irish and Scottish descent. Reagan's older brother, Neil Reagan (1908–1996), became an advertising executive.

Reagan's father nicknamed his son "Dutch", due to his "fat little Dutchman"–like appearance and "Dutchboy" haircut; the nickname stuck with him throughout his youth. Reagan's family briefly lived in several towns and cities in Illinois, including Monmouth, Galesburg, and Chicago. In 1919, they returned to Tampico and lived above the H. C. Pitney Variety Store until finally settling in Dixon, Illinois. After his election as president, Reagan resided in the upstairs White House private quarters, and he would quip that he was "living above the store again".

Ronald Reagan wrote that his mother "always expected to find the best in people and often did." She attended the Disciples of Christ church regularly and was active, and very influential, within it; she frequently led Sunday school services and gave the Bible readings to the congregation during the services. A firm believer in the power of prayer, she led prayer meetings at church and was in charge of mid-week prayers when the pastor was out of town. She was also an adherent of the Social Gospel movement. Her strong commitment to the church is what induced her son Ronald to become a Protestant Christian rather than a Roman Catholic like his father. He also stated that she strongly influenced his own beliefs: "I know that she planted that faith very deeply in me." Reagan identified himself as a born-again Christian.

Reagan had a particularly strong faith in the goodness of people; this faith stemmed from the optimistic faith of his mother and the Disciples of Christ faith, into which he was baptized in 1922. For that period, which was long before the civil rights movement, Reagan's opposition to racial discrimination was unusual. He recalled the time when his college football team was staying at a local hotel which would not allow two black teammates to stay there, and he invited them to his parents' home away in Dixon. His mother invited them to stay overnight and have breakfast the next morning.

Reagan attended Dixon High School, where he developed interests in acting, sports, and storytelling. His first job involved working as a lifeguard at the Rock River in Lowell Park in 1927. Over six years, Reagan performed 77 rescues. He attended Eureka College, a Disciples-oriented liberal arts school, where he became a member of the Tau Kappa Epsilon fraternity, a cheerleader. He was an indifferent student, majored in economics and sociology and graduated with a C grade. He developed a reputation as a "jack of all trades", excelling in campus politics, sports, and theater. He was a member of the football team and captain of the swim team. He was elected student body president and participated in student protests against the college president.

After graduating from Eureka in 1932, Reagan took jobs in Iowa as a radio announcer at several stations. He moved to WHO radio in Des Moines as an announcer for Chicago Cubs baseball games. His specialty was creating play-by-play accounts of games using only basic descriptions that the station received by wire as the games were in progress.

While traveling with the Cubs in California in 1937, Reagan took a screen test that led to a seven-year contract with Warner Brothers studios. He spent the first few years of his Hollywood career in the "B film" unit, where, Reagan joked, the producers "didn't want them good; they wanted them Thursday".

He earned his first screen credit with a starring role in the 1937 movie "Love Is on the Air", and by the end of 1939, he had already appeared in 19 films, including "Dark Victory" with Bette Davis and Humphrey Bogart. Before the film "Santa Fe Trail" with Errol Flynn in 1940, he played the role of George "The Gipper" Gipp in the film "Knute Rockne, All American"; from it, he acquired the lifelong nickname "the Gipper." In 1941, exhibitors voted him the fifth most popular star from the younger generation in Hollywood.

Reagan played his favorite acting role in 1942's "Kings Row", where he plays a double amputee who recites the line "Where's the rest of me?"—later used as the title of his 1965 autobiography. Many film critics considered "Kings Row" to be his best movie, though the film was condemned by "The New York Times" critic Bosley Crowther.

"Kings Row" made Reagan a star—Warner immediately tripled his salary to $3,000 a week. Early in 1942, he was ordered to military active duty in San Francisco and never became a true film star. After his wartime military service he co-starred in such films as "The Voice of the Turtle", "John Loves Mary", "The Hasty Heart", "Bedtime for Bonzo", "Cattle Queen of Montana", "Tennessee's Partner", "Hellcats of the Navy" (the only film in which he appears with Nancy Reagan), and the 1964 remake "The Killers" (his final film). Throughout his film career, Reagan's mother answered much of his fan mail.

After completing 14 home-study Army Extension Courses, Reagan enlisted in the Army Enlisted Reserve and was commissioned a second lieutenant in the Officers' Reserve Corps of the Cavalry on May 25, 1937.

On April 18, 1942, Reagan was ordered to active duty for the first time. Due to his poor eyesight, he was classified for limited service only, which excluded him from serving overseas. His first assignment was at the San Francisco Port of Embarkation at Fort Mason, California, as a liaison officer of the Port and Transportation Office. Upon the approval of the Army Air Forces (AAF), he applied for a transfer from the cavalry to the AAF on May 15, 1942, and was assigned to AAF Public Relations and subsequently to the First Motion Picture Unit (officially, the 18th AAF Base Unit) in Culver City, California. On January 14, 1943, he was promoted to first lieutenant and was sent to the Provisional Task Force Show Unit of "This Is the Army" at Burbank, California. He returned to the First Motion Picture Unit after completing this duty and was promoted to captain on July 22, 1943.

In January 1944, Reagan was ordered to temporary duty in New York City to participate in the opening of the Sixth War Loan Drive, which campaigned for the purchase of war bonds. He was reassigned to the First Motion Picture Unit on November 14, 1944, where he remained until the end of World War II. By the end of the war, his units had produced some 400 training films for the Air Force, including cockpit simulations for B-29 crews scheduled to bomb Japan. He was separated from active duty on December 9, 1945, as an Army captain. While he was in the service, Reagan obtained a film reel depicting the liberation of the Auschwitz concentration camp; he held on to it, believing that doubts would someday arise as to whether the Holocaust had occurred.

Reagan was first elected to the Board of Directors of the Screen Actors Guild (SAG) in 1941, serving as an alternate member. After World War II, he resumed service and became third vice-president in 1946. When the SAG president and six board members resigned in March 1947 due to the union's new bylaws on conflict of interest, Reagan was elected president in a special election. He was subsequently re-elected six times, in 1947, 1948, 1949, 1950, 1951 and 1959. He led the SAG through implementing the 1947 Taft–Hartley Act, various labor-management disputes, and the Hollywood blacklist era. First instituted in 1947 by Studio executives who agreed that they would not employ anyone believed to be or to have been Communists or sympathetic with radical politics, the blacklist grew steadily larger during the early 1950s as the U.S. Congress continued to investigate domestic political subversion.

A fervent anti-communist, Reagan testified before the House Un-American Activities Committee in 1947 that a small clique within the SAG was using "communist-like tactics" in attempting to steer union policy, but that he did not know if those (unnamed) members were communists or not, and that regardless, he believed the union had them under control. Additionally, he reaffirmed his personal commitment to democratic principles, stating, "I never as a citizen want to see our country become urged, by either fear or resentment of this group, that we ever compromise with any of our democratic principles through that fear or resentment." Also during his tenure, Reagan was instrumental in securing residuals for television actors when their episodes were re-run, and later, for motion picture actors when their studio films aired on TV.

In 1946, Reagan served on the national board of directors for the Independent Citizens Committee of the Arts, Sciences and Professions (ICCASP) and had been a member of its Hollywood chapter. His attendance at a July 10, 1946, meeting of HICCASP brought him to the attention of the FBI, which interviewed him on April 10, 1947, in connection with its investigation into HICCASP. Four decades later it was revealed that, during the late 1940s, Reagan (under the code name T-10) and his then-wife, Jane Wyman, provided the FBI with the names of actors within the motion picture industry whom they believed to be communist sympathizers. Even so, he was uncomfortable with the way the SAG was being used by the government, asking during one FBI interview, "Do they (ie. the House Un-American Activities Committee) expect us to constitute ourselves as a little FBI of our own and determine just who is a Commie and who isn't?"

Reagan landed fewer film roles in the late 1950s and moved into television. He was hired as the host of "General Electric Theater", a series of weekly dramas that became very popular. His contract required him to tour General Electric (GE) plants 16 weeks out of the year, which often demanded that he give 14 talks per day. He earned approximately $125,000 (equivalent to $ million in ) in this role. The show ran for ten seasons from 1953 to 1962, which increased Reagan's national profile. On January 1, 1959, Reagan was the host and announcer for ABC's coverage of the Tournament of Roses Parade. In his final work as a professional actor, Reagan was a host and performer from 1964 to 1965 on the television series "Death Valley Days". Following their marriage in 1952, Ronald and Nancy Reagan, who continued to use the stage name Nancy Davis, acted together in three TV series episodes, including a 1958 installment of "General Electric Theater" titled "A Turkey for the President".

In 1938, Reagan co-starred in the film "Brother Rat" with actress Jane Wyman (1917–2007). They announced their engagement at the Chicago Theatre and married on January 26, 1940, at the Wee Kirk o' the Heather church in Glendale, California. Together they had two biological children, Maureen (1941–2001) and Christine (b. in 1947 but lived only one day), and adopted a third, Michael (b. 1945). After the couple had arguments about Reagan's political ambitions, Wyman filed for divorce in 1948, citing a distraction due to her husband's Screen Actors Guild union duties; the divorce was finalized in 1949. Wyman, who was a registered Republican, also stated that their break-up was due to a difference in politics (Reagan was still a Democrat at the time).
When Reagan became President 32 years later, he had the distinction of being the only divorced person to assume the nation's highest office; Donald Trump (2 divorces) would follow him in that respect 36 years later. Reagan and Wyman continued to be friends until his death, with Wyman voting for Reagan in both his runs and, upon his death, saying "America has lost a great president and a great, kind, and gentle man."

Reagan met actress Nancy Davis (1921–2016) in 1949 after she contacted him in his capacity as president of the Screen Actors Guild. He helped her with issues regarding her name appearing on a Communist blacklist in Hollywood. She had been mistaken for another Nancy Davis. She described their meeting by saying, "I don't know if it was exactly love at first sight, but it was pretty close." They were engaged at Chasen's restaurant in Los Angeles and were married on March 4, 1952, at the Little Brown Church in the Valley (North Hollywood, now Studio City) San Fernando Valley. Actor William Holden served as best man at the ceremony. They had two children: Patti (b. 1952) and Ronald "Ron" (b. 1958).

The couple's relationship was close, authentic and intimate. During his presidency, they often displayed affection for one another; one press secretary said, "They never took each other for granted. They never stopped courting." He often called her "Mommy", and she called him "Ronnie." He once wrote to her, "Whatever I treasure and enjoy... all would be without meaning if I didn't have you." In 1998, while he was stricken by Alzheimer's, Nancy told "Vanity Fair", "Our relationship is very special. We were very much in love and still are. When I say my life began with Ronnie, well, it's true. It did. I can't imagine life without him." Nancy Reagan died on March 6, 2016, at the age of 94.

Reagan began as a Hollywood Democrat, and Franklin D. Roosevelt was "a true hero" to him. He moved to the right-wing in the 1950s, became a Republican in 1962, and emerged as a leading conservative spokesman in the Goldwater campaign of 1964.

In his early political career, he joined numerous political committees with a left-wing orientation, such as the American Veterans Committee. He fought against Republican-sponsored right-to-work legislation and supported Helen Gahagan Douglas in 1950 when she was defeated for the Senate by Richard Nixon. It was his realization that Communists were a powerful backstage influence in those groups that led him to rally his friends against them.

At rallies, Reagan frequently spoke with a strong ideological dimension. In December 1945, he was stopped from leading an anti-nuclear rally in Hollywood by pressure from the Warner Bros. studio. He would later make nuclear weapons a key point of his presidency when he specifically stated his opposition to mutual assured destruction. Reagan also built on previous efforts to limit the spread of nuclear weapons. In the 1948 presidential election, Reagan strongly supported Harry S. Truman and appeared on stage with him during a campaign speech in Los Angeles. In the early 1950s, his relationship with actress Nancy Davis grew, and he shifted to the right when he endorsed the presidential candidacies of Dwight D. Eisenhower (1952 and 1956) and Richard Nixon (1960).

Reagan was hired by General Electric (GE) in 1954 to host the "General Electric Theater", a weekly TV drama series. He also traveled across the country to give motivational speeches to over 200,000 GE employees. His many speeches—which he wrote himself—were non-partisan but carried a conservative, pro-business message; he was influenced by Lemuel Boulware, a senior GE executive. Boulware, known for his tough stance against unions and his innovative strategies to win over workers, championed the core tenets of modern American conservatism: free markets, anticommunism, lower taxes, and limited government. Eager for a larger stage, but not allowed to enter politics by GE, he quit and formally registered as a Republican. He often said, "I didn't leave the Democratic Party. The party left me."

When the legislation that would become Medicare was introduced in 1961, he created a recording for the American Medical Association (AMA) warning that such legislation would mean the end of freedom in America. Reagan said that if his listeners did not write letters to prevent it, "we will awake to find that we have socialism. And if you don't do this, and if I don't do it, one of these days, you and I are going to spend our sunset years telling our children, and our children's children, what it once was like in America when men were free." Other Democratic initiatives he opposed in the 1960s included the Food Stamp Program, raising the minimum wage, and the establishment of the Peace Corps. He also joined the National Rifle Association (NRA) and would become a lifetime member.

Reagan gained national attention in his speeches for conservative presidential contender Barry Goldwater in 1964. Speaking for Goldwater, Reagan stressed his belief in the importance of smaller government. He consolidated themes that he had developed in his talks for GE to deliver his famous speech, "A Time for Choosing":

This "A Time for Choosing" speech was not enough to turn around the faltering Goldwater campaign, but it was the crucial event that established Reagan's national political visibility.

California Republicans were impressed with Reagan's political views and charisma after his "Time for Choosing" speech, and in late 1965 he announced his campaign for Governor in the 1966 election. He defeated former San Francisco mayor George Christopher in the GOP primary. In Reagan's campaign, he emphasized two main themes: "to send the welfare bums back to work," and, in reference to burgeoning anti-war and anti-establishment student protests at the University of California at Berkeley, "to clean up the mess at Berkeley." In 1966, Reagan accomplished what both U.S. Senator William F. Knowland in 1958 and former Vice President Richard Nixon in 1962 had attempted to do: he was elected, defeating two-term governor Pat Brown, and was sworn in on January 2, 1967. In his first term, he froze government hiring and approved tax hikes to balance the budget.

Shortly after assuming office, Reagan tested the 1968 presidential waters as part of a "Stop Nixon" movement, hoping to cut into Nixon's southern support and become a compromise candidate if neither Nixon nor second-place candidate Nelson Rockefeller received enough delegates to win on the first ballot at the Republican convention. However, by the time of the convention, Nixon had 692 delegate votes, 25 more than he needed to secure the nomination, followed by Rockefeller with Reagan in third place.

Reagan was involved in several high-profile conflicts with the protest movements of the era, including his public criticism of university administrators for tolerating student demonstrations at the University of California, Berkeley campus. On May 15, 1969, during the at the university's campus (the original purpose of which was to discuss the Arab–Israeli conflict), Reagan sent the California Highway Patrol and other officers to quell the protests. This led to an incident that became known as "Bloody Thursday," resulting in the death of student James Rector and the blinding of carpenter Alan Blanchard. In addition, 111 police officers were injured in the conflict, including one who was knifed in the chest. Reagan then called out 2,200 state National Guard troops to occupy the city of Berkeley for two weeks to crack down on the protesters. The Guard remained in Berkeley for 17 days, camping in People's Park, and demonstrations subsided as the university removed cordoned-off fencing and placed all development plans for People's Park on hold. One year after "Bloody Thursday," Reagan responded to questions about campus protest movements saying, "If it takes a bloodbath, let's get it over with. No more appeasement." When the Symbionese Liberation Army kidnapped Patty Hearst in Berkeley and demanded the distribution of food to the poor, Reagan joked to a group of political aides about a botulism outbreak contaminating the food.

Early in 1967, the national debate on abortion was starting to gain traction. In the early stages of the debate, Democratic California state senator Anthony C. Beilenson introduced the "Therapeutic Abortion Act" in an effort to reduce the number of "back-room abortions" performed in California. The state legislature sent the bill to Reagan's desk where, after many days of indecision, he reluctantly signed it on June 14, 1967. About two million abortions would be performed as a result, mostly because of a provision in the bill allowing abortions for the well-being of the mother. Reagan had been in office for only four months when he signed the bill and later stated that had he been more experienced as governor, he would not have signed it. After he recognized what he called the "consequences" of the bill, he announced that he was pro-life. He maintained that position later in his political career, writing extensively about abortion.

In 1967, Reagan signed the Mulford Act, which repealed a law allowing the public carrying of loaded firearms (becoming California Penal Code 12031 and 171(c)). The bill, which was named after Republican assemblyman Don Mulford, garnered national attention after the Black Panthers marched bearing arms upon the California State Capitol to protest it.

Despite an unsuccessful attempt to force a recall election on Reagan in 1968, he was re-elected governor in 1970, defeating "Big Daddy" Jesse M. Unruh. He chose not to seek a third term in the following election cycle. One of Reagan's greatest frustrations in office was the controversy of capital punishment, which he strongly supported. His efforts to enforce the state's laws in this area were thwarted when the Supreme Court of California issued its "People v. Anderson" decision, which invalidated all death sentences issued in California before 1972, though the decision was later overturned by a constitutional amendment. The only execution during Reagan's governorship was on April 12, 1967, when Aaron Mitchell's sentence was carried out by the state in San Quentin's gas chamber.

In 1969, Reagan signed the Family Law Act, which was an amalgam of two bills that had been written and revised by the California State Legislature over more than two years. It became the first no-fault divorce legislation in the United States. Years later, no-fault divorce became Reagan's greatest regret.

Reagan's terms as governor helped to shape the policies he would pursue in his later political career as president. By campaigning on a platform of sending "the welfare bums back to work," he spoke out against the idea of the welfare state. He also strongly advocated the Republican ideal of less government regulation of the economy, including that of undue federal taxation.

Reagan did not seek re-election to a third term as governor in 1974. He was succeeded by the Secretary of State, Democrat Jerry Brown (son of former governor Pat Brown), who took office on January 6, 1975.

In 1976, Reagan challenged incumbent President Gerald Ford in a bid to become the Republican Party's candidate for president. Reagan soon established himself as the conservative candidate with the support of like-minded organizations such as the American Conservative Union, which became fundamental components of his political base, while Ford was considered a more moderate Republican.

Reagan's campaign relied on a strategy crafted by campaign manager John Sears of winning a few primaries early to damage the inevitability of Ford's likely nomination. Reagan won North Carolina, Texas, and California, but the strategy failed, as he ended up losing New Hampshire, Florida, and his native Illinois. The Texas campaign lent renewed hope to Reagan when he swept all 96 delegates chosen in the May 1 primary, with four more awaiting at the state convention. Much of the credit for that victory came from the work of three co-chairmen, including Ernest Angelo, the mayor of Midland, and Ray Barnhart of Houston, whom Reagan as President would appoint in 1981 as director of the Federal Highway Administration.

However, as the GOP convention neared, Ford appeared close to victory. Acknowledging his party's moderate wing, Reagan chose moderate Senator Richard Schweiker of Pennsylvania as his running mate if nominated. Nonetheless, Ford prevailed with 1,187 delegates to Reagan's 1,070. Ford would go on to lose the 1976 presidential election to the Democratic nominee, Jimmy Carter.

Reagan's concession speech emphasized the dangers of nuclear war and the threat posed by the Soviet Union. Though he lost the nomination, he received 307 write-in votes in New Hampshire, 388 votes as an Independent on Wyoming's ballot, and a single electoral vote from a faithless elector in the November election from the state of Washington, which Ford had won over Democratic challenger Jimmy Carter.

After the campaign, Reagan remained in the public debate with the Ronald Reagan Radio Commentary series and his political action committee, Citizens for the Republic, which was later revived in Alexandria, Virginia, in 2009 by the Reagan biographer Craig Shirley.

The 1980 presidential election featured Reagan against incumbent President Jimmy Carter and was conducted amid a multitude of domestic concerns as well as the ongoing Iran hostage crisis. Reagan's campaign stressed some of his fundamental principles: lower taxes to stimulate the economy, less government interference in people's lives, states' rights, and a strong national defense.

Reagan launched his campaign with an indictment of a Federal Government which he believed had "overspent, overstimulated, and overregulated." After receiving the Republican nomination, Reagan selected one of his opponents in the primaries, George H. W. Bush, to be his running mate. His relaxed and confident appearance during the televised Reagan-Carter debate on October 28, boosted his popularity and helped to widen his lead in the polls.

On November 4, Reagan won a decisive victory over Carter, carrying 44 states and receiving 489 electoral votes to Carter's 49 in six states plus D.C. He also won the popular vote, receiving 50.7% to Carter's 41.0%, with independent John B. Anderson garnering 6.6%. Republicans also won a majority of seats in the Senate for the first time since 1952, though Democrats retained a majority in the House of Representatives.

During his presidency, Reagan pursued policies that reflected his personal belief in individual freedom; brought changes domestically, both to the U.S. economy and expanded military; and contributed to the end of the Cold War. Termed the "Reagan Revolution," his presidency would reinvigorate American morale, reinvigorate the U.S. economy and reduce reliance upon government. As president, Reagan kept a diary in which he commented on daily occurrences of his presidency and his views on the issues of the day. The diaries were published in May 2007 in the bestselling book, "The Reagan Diaries".

Ronald Reagan was 69 years old when he was sworn into office for his first term on January 20, 1981. In his inaugural address (which Reagan himself wrote), he addressed the country's economic malaise, arguing: "In this present crisis, government is not the solution to our problems; government is the problem."

During his term in office, Reagan campaigned vigorously to restore organized prayer to the schools, first as a moment of prayer and later as a moment of silence. In 1981, Reagan became the first president to propose a constitutional amendment on school prayer. Reagan's election reflected an opposition to the 1962 Supreme Court case "Engel v. Vitale", prohibiting state officials from composing an official state prayer and requiring that it be recited in the public schools. Reagan's 1981 proposed amendment stated: "Nothing in this Constitution shall be construed to prohibit individual or group prayer in public schools or other public institutions. No person shall be required by the United States or by any state to participate in prayer." In 1984, Reagan again raised the issue, asking Congress "why can't freedom to acknowledge God be enjoyed again by children in every schoolroom across this land?" In 1985, Reagan expressed his disappointment that the Supreme Court ruling still banned a moment of silence for public schools, and said that efforts to reinstitute prayer in public schools were "an uphill battle". In 1987 Reagan renewed his call for Congress to support voluntary prayer in schools and end "the expulsion of God from America's classrooms."

On March 30, 1981 (shortly into his new administration), Reagan, his press secretary James Brady, Washington police officer Thomas Delahanty, and Secret Service agent Tim McCarthy were struck by gunfire from would-be assassin John Hinckley Jr. outside the Washington Hilton hotel. Although "close to death" upon arrival at George Washington University Hospital, Reagan was stabilized in the emergency room, then underwent emergency exploratory surgery. He recovered and was released from the hospital on April 11, becoming the first serving U.S. president to survive being shot in an assassination attempt. The attempt had a significant influence on Reagan's popularity; polls indicated his approval rating to be around 73%. Reagan believed that God had spared his life so that he might go on to fulfill a higher purpose.

In August 1981, PATCO, the union of federal air traffic controllers, went on strike, violating a federal law prohibiting government unions from striking. Declaring the situation an emergency as described in the 1947 Taft–Hartley Act, Reagan stated that if the air traffic controllers "do not report for work within 48 hours, they have forfeited their jobs and will be terminated." They did not return, and on August 5, Reagan fired 11,345 striking air traffic controllers who had ignored his order and used supervisors and military controllers to handle the nation's commercial air traffic until new controllers could be hired and trained. A leading reference work on public administration concluded, "The firing of PATCO employees not only demonstrated a clear resolve by the president to take control of the bureaucracy, but it also sent a clear message to the private sector that unions no longer needed to be feared."

During Jimmy Carter's last year in office (1980), inflation averaged 12.5%, compared with 4.4% during Reagan's last year in office (1988). During Reagan's administration, the unemployment rate declined from 7.5% to 5.4%, with the rate reaching highs of 10.8% in 1982 and 10.4% in 1983, averaging 7.5% over the eight years, and real GDP growth averaged 3.4% with a high of 8.6% in 1983, while nominal GDP growth averaged 7.4%, and peaked at 12.2% in 1982.

Reagan implemented policies based on supply-side economics, advocating a "laissez-faire" philosophy and free-market fiscal policy, seeking to stimulate the economy with large, across-the-board tax cuts. He also supported returning the United States to some sort of gold standard and successfully urged Congress to establish the U.S. Gold Commission to study how one could be implemented. Citing the economic theories of Arthur Laffer, Reagan promoted the proposed tax cuts as potentially stimulating the economy enough to expand the tax base, offsetting the revenue loss due to reduced rates of taxation, a theory that entered political discussion as the Laffer curve. Reaganomics was the subject of debate with supporters pointing to improvements in certain key economic indicators as evidence of success, and critics pointing to large increases in federal budget deficits and the national debt. His policy of "peace through strength" resulted in a record peacetime defense buildup including a 40% real increase in defense spending between 1981 and 1985.

During Reagan's presidency, federal income tax rates were lowered significantly with the signing of the Economic Recovery Tax Act of 1981, which lowered the top marginal tax bracket from 70% to 50% over three years (as part of a "5-10-10" plan), and the lowest bracket from 14% to 11%. Other tax increases passed by Congress and signed by Reagan ensured however that tax revenues over his two terms were 18.2% of GDP as compared to 18.1% over the 40 years of 1970–2010. The 1981 tax act also required that exemptions and brackets be indexed for inflation starting in 1985.

In 1982, the Job Training Partnership Act of 1982 was signed into law, initiating one of the United States' first public–private partnerships and a substantial part of the president's job creation program. Reagan's Assistant Secretary of Labor and Chief of Staff, Al Angrisani, was a primary architect of the bill.

Conversely, Congress passed and Reagan signed into law tax increases of some nature in every year from 1981 to 1987 to continue funding such government programs as Tax Equity and Fiscal Responsibility Act of 1982 (TEFRA), Social Security, and the Deficit Reduction Act of 1984 (DEFRA). TEFRA was the "largest peacetime tax increase in American history." Gross domestic product (GDP) growth recovered strongly after the early 1980s recession ended in 1982, and grew during his eight years in office at an annual rate of 7.9% per year, with a high of 12.2% growth in 1981. Unemployment peaked at 10.8% monthly rate in December 1982—higher than any time since the Great Depression—then dropped during the rest of Reagan's presidency. Sixteen million new jobs were created, while inflation significantly decreased. The Tax Reform Act of 1986, another bipartisan effort championed by Reagan, simplified the tax code by reducing the number of tax brackets to four and slashing several tax breaks. The top rate was dropped to 28%, but capital gains taxes were increased on those with the highest incomes from 20% to 28%. The increase of the lowest tax bracket from 11% to 15% was more than offset by the expansion of personal exemption, standard deduction, and earned income tax credit. The net result was the removal of six million poor Americans from the income tax roll and a reduction of income tax liability at all income levels.

The net effect of all Reagan-era tax bills was a 1% decrease in government revenues when compared to Treasury Department revenue estimates from the Administration's first post-enactment January budgets. However, federal income tax receipts increased from 1980 to 1989, rising from $308.7 billion to $549 billion or an average annual rate of 8.2% (2.5% attributed to higher Social Security receipts), and federal outlays grew at an annual rate of 7.1%.

Reagan's policies proposed that economic growth would occur when marginal tax rates were low enough to spur investment, which would then lead to higher employment and wages. Critics labeled this "trickle-down economics"—the belief that tax policies that benefit the wealthy will create a "trickle-down" effect reaching the poor. Questions arose whether Reagan's policies benefited the wealthy more than those living in poverty, and many poor and minority citizens viewed Reagan as indifferent to their struggles. These views were exacerbated by the fact that Reagan's economic regimen included freezing the minimum wage at $3.35 an hour, slashing federal assistance to local governments by 60%, cutting the budget for public housing and Section 8 rent subsidies in half, and eliminating the antipoverty Community Development Block Grant program. The widening gap between the rich and poor had already begun during the 1970s before Reagan's economic policies took effect. Along with Reagan's 1981 cut in the top regular tax rate on unearned income, he reduced the maximum capital gains rate to 20%. Reagan later set tax rates on capital gains at the same level as the rates on ordinary income like salaries and wages, with both topping out at 28%. Reagan is viewed as an antitax hero despite raising taxes eleven times throughout his presidency, all in the name of fiscal responsibility. According to Paul Krugman, "Over all, the 1982 tax increase undid about a third of the 1981 cut; as a share of GDP, the increase was substantially larger than Mr. Clinton's 1993 tax increase." According to historian and domestic policy adviser Bruce Bartlett, Reagan's tax increases throughout his presidency took back half of the 1981 tax cut.

Reagan was opposed to government intervention, and he cut the budgets of non-military programs including Medicaid, food stamps, federal education programs and the EPA. He protected entitlement programs such as Social Security and Medicare, but his administration attempted to purge many people with disabilities from the Social Security disability rolls.

The administration's stance toward the savings and loan industry contributed to the savings and loan crisis. A minority of the critics of Reaganomics also suggested that the policies partially influenced the stock market crash of 1987, but there is no consensus regarding a single source for the crash. To cover newly spawned federal budget deficits, the United States borrowed heavily both domestically and abroad, raising the national debt from $997 billion to $2.85 trillion. Reagan described the new debt as the "greatest disappointment" of his presidency.

He reappointed Paul Volcker as Chairman of the Federal Reserve, and in 1987 he appointed monetarist Alan Greenspan to succeed him. Reagan ended the price controls on domestic oil that had contributed to the energy crises of 1973–74 and the summer of 1979. The price of oil subsequently dropped, and there were no fuel shortages like those in the 1970s. Reagan also fulfilled a 1980 campaign promise to repeal the windfall profits tax in 1988, which had previously increased dependence on foreign oil. Some economists, such as Nobel Prize winners Milton Friedman and Robert Mundell, argue that Reagan's tax policies invigorated America's economy and contributed to the economic boom of the 1990s. Other economists, such as Nobel Prize winner Robert Solow, argue that Reagan's deficits were a major reason his successor, George H. W. Bush, reneged on his and resorted to raising taxes.

During Reagan's presidency, a program was initiated within the United States Intelligence Community to ensure America's economic strength. The program, Project Socrates, developed and demonstrated the means required for the United States to generate and lead the next evolutionary leap in technology acquisition and utilization for a competitive advantage—automated innovation. To ensure that the United States acquired the maximum benefit from automated innovation, Reagan, during his second term, had an executive order drafted to create a new federal agency to implement the Project Socrates results on a nationwide basis. However, Reagan's term came to an end before the executive order could be coordinated and signed, and the incoming Bush administration, labeling Project Socrates as "industrial policy," had it terminated.

The Reagan administration was often criticized for inadequately enforcing, if not actively undermining, civil rights legislation. In 1982, he signed a bill extending the Voting Rights Act for 25 years after a grass-roots lobbying and legislative campaign forced him to abandon his plan to ease that law's restrictions. He also signed legislation establishing a federal Martin Luther King holiday, though he did so with reservations. In 1988, he vetoed the Civil Rights Restoration Act, but his veto was overridden by Congress. Reagan had argued that the legislation infringed on states' rights and the rights of churches and business owners.

Reagan escalated the Cold War, accelerating a reversal from the policy of détente that began during the Carter administration, following the Afghan Saur Revolution and subsequent Soviet invasion. He ordered a massive buildup of the United States Armed Forces and implemented new policies that were directed towards the Soviet Union; he revived the B-1 Lancer program that had been canceled by the Carter administration, and he produced the MX missile. In response to Soviet deployment of the SS-20, Reagan oversaw NATO's deployment of the Pershing missile in West Germany. In 1982 Reagan tried to cut off Moscow's access to hard currency by impeding its proposed gas line to Western Europe. It hurt the Soviet economy, but it also caused ill will among American allies in Europe who counted on that revenue. Reagan retreated on this issue.

In 1984, journalist Nicholas Lemann interviewed Secretary of Defense Caspar Weinberger and summarized the strategy of the Reagan administration to roll back the Soviet Union:

Lemann noted that when he wrote that in 1984, he thought the Reaganites were living in a fantasy world. But by 2016, Lemann stated that the passage represents "a fairly uncontroversial description of what Reagan actually did."

Reagan and the United Kingdom's prime minister Margaret Thatcher both denounced the Soviet Union in ideological terms. In a famous address on June 8, 1982, to the Parliament of the United Kingdom in the Royal Gallery of the Palace of Westminster, Reagan said, "the forward march of freedom and democracy will leave Marxism–Leninism on the ash heap of history." On March 3, 1983, he predicted that communism would collapse, stating, "Communism is another sad, bizarre chapter in human history whose last pages even now are being written." In a speech to the National Association of Evangelicals on March 8, 1983, Reagan called the Soviet Union "an evil empire."

After Soviet fighters downed Korean Air Lines Flight 007 near Moneron Island on September 1, 1983, carrying 269 people, including Georgia congressman Larry McDonald, Reagan labeled the act a "massacre" and declared that the Soviets had turned "against the world and the moral precepts which guide human relations among people everywhere." The Reagan administration responded to the incident by suspending all Soviet passenger air service to the United States and dropped several agreements being negotiated with the Soviets, wounding them financially. As a result of the shootdown, and the cause of KAL 007's going astray thought to be inadequacies related to its navigational system, Reagan announced on September 16, 1983, that the Global Positioning System would be made available for civilian use, free of charge, once completed in order to avert similar navigational errors in future.

Under a policy that came to be known as the Reagan Doctrine, Reagan and his administration also provided overt and covert aid to anti-communist resistance movements in an effort to "rollback" Soviet-backed communist governments in Africa, Asia, and Latin America. However, in a break from the Carter administration's policy of arming Taiwan under the Taiwan Relations Act, Reagan also agreed with the communist government in China to reduce the sale of arms to Taiwan.

Reagan deployed the CIA's Special Activities Division to Afghanistan and Pakistan. They were instrumental in training, equipping and leading Mujahideen forces against the Soviet Army. President Reagan's Covert Action program has been given credit for assisting in ending the Soviet occupation of Afghanistan, though some of the United States funded armaments introduced then would later pose a threat to U.S. troops in the 2001 War in Afghanistan. The CIA also began sharing information with the Iranian government which it was secretly courting. In one instance, in 1982, this practice enabled the government to identify and purge communists from its ministries and to virtually eliminate the pro-Soviet infrastructure in Iran.

In March 1983, Reagan introduced the Strategic Defense Initiative, a defense project that would have used ground- and space-based systems to protect the United States from attack by strategic nuclear ballistic missiles. Reagan believed that this defense shield could make nuclear war impossible. There was much disbelief surrounding the program's scientific feasibility, leading opponents to dub SDI "Star Wars" and argue that its technological objective was unattainable. The Soviets became concerned about the possible effects SDI would have; leader Yuri Andropov said it would put "the entire world in jeopardy." For those reasons, David Gergen, a former aide to President Reagan, believes that in retrospect, SDI hastened the end of the Cold War.

Though supported by leading American conservatives who argued that Reagan's foreign policy strategy was essential to protecting U.S. security interests, critics labeled the administration's foreign policy initiatives as aggressive and imperialistic, and chided them as "warmongering." The administration was also heavily criticized for backing anti-communist leaders accused of severe human rights violations, such as Hissène Habré of Chad and Efraín Ríos Montt of Guatemala. During the 16 months (1982–1983) Montt was President of Guatemala, the Guatemalan military was accused of genocide for massacres of members of the Ixil people and other indigenous groups. Reagan had said that Montt was getting a "bum rap," and described him as "a man of great personal integrity." Previous human rights violations had prompted the United States to cut off aid to the Guatemalan government, but the Reagan administration appealed to Congress to restart military aid. Although unsuccessful with that, the administration was successful in providing nonmilitary assistance such as USAID.

With the approval of Congress, Reagan sent forces to Lebanon in 1983 to reduce the threat of the Lebanese Civil War. The American peacekeeping forces in Beirut, a part of a multinational force during the Lebanese Civil War, were attacked on October 23, 1983. The Beirut barracks bombing killed 241 American servicemen and wounded more than 60 others by a suicide truck bomber. Reagan sent in the battleship to shell Syrian positions in Lebanon. He then withdrew all the Marines from Lebanon.

On October 25, 1983, Reagan ordered U.S. forces to invade Grenada (codenamed "Operation Urgent Fury") where a 1979 coup d'état had established an independent non-aligned Marxist–Leninist government. A formal appeal from the Organisation of Eastern Caribbean States (OECS) led to the intervention of U.S. forces; President Reagan also cited an allegedly regional threat posed by a Soviet-Cuban military build-up in the Caribbean and concern for the safety of several hundred American medical students at St. George's University as adequate reasons to invade. "Operation Urgent Fury" was the first major military operation conducted by U.S. forces since the Vietnam War, several days of fighting commenced, resulting in a U.S. victory, with 19 American fatalities and 116 wounded American soldiers. In mid-December, after a new government was appointed by the governor-general, U.S. forces withdrew.

Reagan accepted the Republican nomination in the Republican convention in Dallas, Texas. He proclaimed that it was "morning again in America," regarding the recovering economy and the dominating performance by the American athletes at the 1984 Summer Olympics on home soil, among other things. He became the first U.S. president to open an Olympic Games. Previous Olympics taking place in the United States had been opened by either the Vice President (three times) or another person in charge (twice).

Reagan's opponent in the 1984 presidential election was former Vice President Walter Mondale. Following a weak performance in the first presidential debate, Reagan's ability to perform the duties of president for another term was questioned. His confused and forgetful behavior was evident to his supporters; they had previously known him to be clever and witty. Rumors began to circulate that Reagan had Alzheimer's disease. Reagan rebounded in the second debate; confronting questions about his age, he quipped: "I will not make age an issue of this campaign. I am not going to exploit, for political purposes, my opponent's youth and inexperience". This remark generated applause and laughter, even from Mondale himself.

That November, Reagan won a landslide re-election victory, carrying 49 of the 50 states. Mondale won only his home state of Minnesota and the District of Columbia. Reagan won 525 of the 538 electoral votes, the most of any presidential candidate in U.S. history. In terms of electoral votes, this was the second-most lopsided presidential election in modern U.S. history (Franklin D. Roosevelt's 1936 victory over Alfred M. Landon, in which he won 98.5% or 523 of the (then-total) 531 electoral votes, ranks first). Reagan won 58.8% of the popular vote to Mondale's 40.6%. His popular-vote margin of victory—nearly 16.9 million votes (54.4 million for Reagan to 37.5 million for Mondale)—was exceeded only by Richard Nixon in his 1972 victory over George McGovern.

Reagan was sworn in as president for the second time on January 20, 1985, in a private ceremony at the White House. To date, at 73 years of age, he is the oldest person to take the presidential oath of office. Because January 20 fell on a Sunday, a public celebration was not held but took place in the Capitol rotunda the following day. January 21 was one of the coldest days on record in Washington, D.C.; due to poor weather, inaugural celebrations were held inside the Capitol. In the weeks that followed, he shook up his staff somewhat, moving White House Chief of Staff James Baker to Secretary of the Treasury and naming Treasury Secretary Donald Regan, a former Merrill Lynch officer, Chief of Staff.

In response to concerns about the increasing crack epidemic, Reagan began the War on Drugs campaign in 1982, a policy led by the federal government to reduce the illegal drug trade. Though Nixon had previously declared war on drugs, Reagan advocated more aggressive policies. He said that "drugs were menacing our society" and promised to fight for drug-free schools and workplaces, expanded drug treatment, stronger law enforcement and drug interdiction efforts, and greater public awareness.

In 1986, Reagan signed a drug enforcement bill that budgeted $1.7 billion (equivalent to $ billion in ) to fund the War on Drugs and specified a mandatory minimum penalty for drug offenses. The bill was criticized for promoting significant racial disparities in the prison population, and critics also charged that the policies did little to reduce the availability of drugs on the street while resulting in a tremendous financial burden for America. Defenders of the effort point to success in reducing rates of adolescent drug use: marijuana use among high-school seniors declined from 33% in 1980 to 12% in 1991. First Lady Nancy Reagan made the War on Drugs her main priority by founding the "Just Say No" drug awareness campaign, which aimed to discourage children and teenagers from engaging in recreational drug use by offering various ways of saying "no." Nancy Reagan traveled to 65 cities in 33 states, raising awareness about the dangers of drugs, including alcohol.

According to AIDS activist organizations such as ACT UP, the Reagan administration largely ignored the AIDS crisis, which began to unfold in the United States in 1981, the same year Reagan took office. They also claim AIDS research was chronically underfunded during Reagan's administration, and requests for more funding by doctors at the Centers for Disease Control (CDC) were routinely denied.

By the time President Reagan had given his first prepared speech on the epidemic, some six years into his presidency, 36,058 Americans had been diagnosed with AIDS, and 20,849 had died of it. By the end of 1989, the year Reagan left office, 115,786 people had been diagnosed with AIDS in the United States, and more than 70,000 of them had died of it.

Others, however, point out that federal funding for AIDS-related programs was $2.3 billion in 1989 and nearly $6 billion total over his presidency. In a September 1985 press conference, Reagan said: "this is a top priority with us... there's no question about the seriousness of this and the need to find an answer." Gary Bauer, who was Reagan's domestic policy adviser near the end of his second term, similarly argued that Reagan's belief in cabinet government led him to assign the job of speaking out against AIDS to his Surgeon General and Secretary of Health and Human Services.

From the late 1960s onward, the American public grew increasingly vocal in its opposition to the apartheid policy of the white-minority government of South Africa, and in its insistence that the U.S. impose economic and diplomatic sanctions on South Africa. The strength of the anti-apartheid opposition surged during Reagan's first term in office as its component disinvestment from South Africa movement, which had been in existence for quite some years, gained critical mass following in the United States, particularly on college campuses and among mainline Protestant denominations. President Reagan was opposed to divestiture because, as he wrote in a letter to Sammy Davis Jr., it "would hurt the very people we are trying to help and would leave us no contact within South Africa to try and bring influence to bear on the government". He also noted the fact that the "American-owned industries there employ more than 80,000 blacks" and that their employment practices were "very different from the normal South African customs".

As an alternative strategy for opposing apartheid, the Reagan Administration developed a policy of constructive engagement with the South African government as a means of encouraging it to move away from apartheid gradually. It was part of a larger initiative designed to foster peaceful economic development and political change throughout southern Africa. This policy, however, engendered much public criticism and renewed calls for the imposition of stringent sanctions. In response, Reagan announced the imposition of new sanctions on the South African government, including an arms embargo in late 1985. These sanctions were, however, seen as weak by anti-apartheid activists, and as insufficient by the president's opponents in Congress. In August 1986, Congress approved the Comprehensive Anti-Apartheid Act, which included tougher sanctions. Reagan vetoed the act, but the veto was overridden by Congress. Afterward, Reagan reiterated that his administration and "all America" opposed apartheid, and said, "the debate... was not whether or not to oppose apartheid but, instead, how best to oppose it and how best to bring freedom to that troubled country." Several European countries as well as Japan soon followed the U.S. lead and imposed their sanctions on South Africa.

Relations between Libya and the United States under President Reagan were continually contentious, beginning with the Gulf of Sidra incident in 1981; by 1982, Libyan leader Muammar Gaddafi was considered by the CIA to be, along with USSR leader Leonid Brezhnev and Cuban leader Fidel Castro, part of a group known as the "unholy trinity" and was also labeled as "our international public enemy number one" by a CIA official. These tensions were later revived in early April 1986, when a bomb exploded in a Berlin discothèque, resulting in the injury of 63 American military personnel and death of one serviceman. Stating that there was "irrefutable proof" that Libya had directed the "terrorist bombing," Reagan authorized the use of force against the country. In the late evening of April 15, 1986, the United States launched a series of airstrikes on ground targets in Libya.

Britain's prime minister, Margaret Thatcher, allowed the U.S. Air Force to use Britain's air bases to launch the attack, on the justification that the UK was supporting America's right to self-defense under Article 51 of the United Nations Charter. The attack was designed to halt Gaddafi's "ability to export terrorism," offering him "incentives and reasons to alter his criminal behavior." The president addressed the nation from the Oval Office after the attacks had commenced, stating, "When our citizens are attacked or abused anywhere in the world on the direct orders of hostile regimes, we will respond so long as I'm in this office." The attack was condemned by many countries. By a vote of 79 in favor to 28 against with 33 abstentions, the United Nations General Assembly adopted resolution 41/38 which "condemns the military attack perpetrated against the Socialist People's Libyan Arab Jamahiriya on April 15, 1986, which constitutes a violation of the Charter of the United Nations and of international law."

Reagan signed the Immigration Reform and Control Act in 1986. The act made it illegal to knowingly hire or recruit illegal immigrants, required employers to attest to their employees' immigration status, and granted amnesty to approximately three million illegal immigrants who entered the United States before January 1, 1982, and had lived in the country continuously. Upon signing the act at a ceremony held beside the newly refurbished Statue of Liberty, Reagan said, "The legalization provisions in this act will go far to improve the lives of a class of individuals who now must hide in the shadows, without access to many of the benefits of a free and open society. Very soon, many of these men and women will be able to step into the sunlight and, ultimately, if they choose, they may become Americans." Reagan also said, "The employer sanctions program is the keystone and major element. It will remove the incentive for illegal immigration by eliminating the job opportunities which draw illegal aliens here."

In 1986, the Iran–Contra affair became a problem for the administration stemming from the use of proceeds from covert arms sales to Iran during the Iran–Iraq War to fund the Contra rebels fighting against the government in Nicaragua, which had been specifically outlawed by an act of Congress. The affair became a political scandal in the United States during the 1980s. The International Court of Justice, whose jurisdiction to decide the case was disputed by the United States, ruled that the United States had violated international law and breached treaties in Nicaragua in various ways.

President Reagan professed that he was unaware of the plot's existence. He opened his own investigation and appointed two Republicans and one Democrat, John Tower, Brent Scowcroft and Edmund Muskie, respectively, to investigate the scandal. The commission could not find direct evidence that Reagan had prior knowledge of the program, but criticized him heavily for his disengagement from managing his staff, making the diversion of funds possible. A separate report by Congress concluded that "If the president did not know what his national security advisers were doing, he should have." Reagan's popularity declined from 67% to 46% in less than a week, the most significant and quickest decline ever for a president. The scandal resulted in eleven convictions and fourteen indictments within Reagan's staff.

Many Central Americans criticize Reagan for his support of the Contras, calling him an anti-communist zealot, blinded to human rights abuses, while others say he "saved Central America." Daniel Ortega, Sandinistan and president of Nicaragua, said that he hoped God would forgive Reagan for his "dirty war against Nicaragua."

In 1988, near the end of the Iran–Iraq War, the U.S. Navy guided-missile cruiser accidentally shot down Iran Air Flight 655 killing 290 civilian passengers. The incident further worsened already tense Iran–United States relations.

Until the early 1980s, the United States had relied on the qualitative superiority of its weapons to essentially frighten the Soviets, but the gap had been narrowed. Although the Soviet Union did not accelerate military spending after President Reagan's military buildup, their enormous military expenses, in combination with collectivized agriculture and inefficient planned manufacturing, were a heavy burden for the Soviet economy. At the same time, oil prices in 1985 fell to one-third of the previous level; oil was the primary source of Soviet export revenues. These factors contributed to a stagnant Soviet economy during Gorbachev's tenure.

Reagan was deeply committed first to the abolition of nuclear weapons worldwide. Second, he was committed (thanks to his California friend Edward Teller, the father of the hydrogen bomb) to building a defense against nuclear weapons, called the Strategic Defense Initiative (SDI, nicknamed "Star Wars"). American scientists were not sure that SDI would work, but they were sure its total cost would reach in the trillions of dollars. Reagan encouraged Congress to think of it as billions of new dollars spent in individual districts. If SDI worked, thousands of Soviet nuclear-armed missiles would be worthless — if launched, they could all be shot down. Gorbachev made it his highest priority to get Reagan to abandon SDI.

Meanwhile, Reagan escalated the rhetoric. In his famous 1983 speech to religious fundamentalists, he outlined his strategy for victory. First, he labeled the Soviet system an "Evil Empire" and a failure—its demise would be a godsend for the world. Second, Reagan explained his strategy was an arms buildup that would leave the Soviets far behind, with no choice but to negotiate arms reduction. Finally, displaying his characteristic optimism, he praised liberal democracy and promised that such a system eventually would triumph over Soviet communism.

Reagan appreciated the revolutionary change in the direction of the Soviet policy with Mikhail Gorbachev, and shifted to diplomacy, intending to encourage the Soviet leader to pursue substantial arms agreements. He and Gorbachev held four summit conferences between 1985 and 1988: the first in Geneva, Switzerland, the second in Reykjavík, Iceland, the third in Washington, D.C., and the fourth in Moscow. Reagan believed that if he could persuade the Soviets to allow for more democracy and free speech, this would lead to reform and the end of Communism. The critical summit was at Reykjavík in October 1986, where they met alone, with translators but with no aides. To the astonishment of the world, and the chagrin of Reagan's most conservative supporters, they agreed to abolish all nuclear weapons. Gorbachev then asked the end of SDI. Reagan said no, claiming that it was defensive only, and that he would share the secrets with the Soviets. No deal was achieved.

Speaking at the Berlin Wall on June 12, 1987, Reagan challenged Gorbachev to go further, saying "General Secretary Gorbachev, if you seek peace, if you seek prosperity for the Soviet Union and Eastern Europe, if you seek liberalization, come here to this gate! Mr. Gorbachev, open this gate! Mr. Gorbachev, tear down this wall!" Later, in November 1989, East German authorities began allowing citizens to pass freely through border checkpoints, and began dismantling the Wall the following June; its demolition was completed in 1992.

At Gorbachev's visit to Washington in December 1987, he and Reagan signed the Intermediate-Range Nuclear Forces Treaty (INF Treaty) at the White House, which eliminated an entire class of nuclear weapons. The two leaders laid the framework for the Strategic Arms Reduction Treaty, or START I; Reagan insisted that the name of the treaty be changed from Strategic Arms Limitation Talks to Strategic Arms Reduction Talks.

When Reagan visited Moscow for the fourth summit in 1988, he was viewed as a celebrity by the Soviets. A journalist asked the president if he still considered the Soviet Union the evil empire. "No," he replied, "I was talking about another time, another era." At Gorbachev's request, Reagan gave a speech on free markets at the Moscow State University.

Reagan came under much criticism in 1985 when he was accused of honoring Nazi war criminals at a cemetery in West Germany. In February 1985, the administration accepted an invitation for Reagan to visit a German military cemetery in Bitburg and to place a wreath alongside West German Chancellor Helmut Kohl. Deaver was given assurances by a German head of protocol that no war criminals were buried there. It was later determined that the cemetery held the graves of 49 members of the Waffen-SS. What neither Deaver nor other administration officials initially realized was that many Germans distinguished the regular SS, who typically were composed of Nazi true believers, and the Waffen-SS which were attached to military units and composed of conscripted soldiers.

As the controversy brewed in April 1985, Reagan issued a statement that called the Nazi soldiers buried in that cemetery as themselves "victims," a designation which ignited a stir over whether Reagan had equated the SS men to victims of the Holocaust. Pat Buchanan, Reagan's Director of Communications, argued that the president did not equate the SS members with the actual Holocaust, but as victims of the ideology of Nazism. Now strongly urged to cancel the visit, the president responded that it would be wrong to back down on a promise he had made to Chancellor Kohl. On May 5, 1985, President Reagan and Chancellor Kohl first visited the site of the former Nazi Bergen-Belsen concentration camp and then the Bitburg cemetery where, along with two military generals, they did place a wreath.

"See also: "

Early in his presidency, Reagan started wearing a custom-made, technologically advanced hearing aid, first in his right ear and later in his left ear as well. His decision to go public in 1983 regarding his wearing the small, audio-amplifying device boosted their sales.

On July 13, 1985, Reagan underwent surgery at Bethesda Naval Hospital to remove cancerous polyps from his colon. He relinquished presidential power to the Vice President for eight hours in a similar procedure as outlined in the 25th Amendment, which he specifically avoided invoking. The surgery lasted just under three hours and was successful. Reagan resumed the powers of the presidency later that day. In August of that year, he underwent an operation to remove skin cancer cells from his nose. In October, more skin cancer cells were detected on his nose and removed.

In January 1987, Reagan underwent surgery for an enlarged prostate that caused further worries about his health. No cancerous growths were found, and he was not sedated during the operation. In July of that year, aged 76, he underwent a third skin cancer operation on his nose.

On January 7, 1989, Reagan underwent surgery at Walter Reed Army Medical Center to repair a Dupuytren's contracture of the ring finger of his left hand. The surgery lasted for more than three hours and was performed under regional anesthesia. This procedure was done just 13 days before he left office. For this reason, he had a hand and finger bandage the day of his farewell speech and during President Bush's inauguration.

During the 1980 presidential campaign, Reagan pledged that he would appoint the first female Supreme Court Justice if given the opportunity. That opportunity came during his first year in office when Associate Justice Potter Stewart retired; Reagan selected Sandra Day O'Connor, who was confirmed unanimously by the Senate. In his second term, Reagan had three opportunities to fill a Supreme Court vacancy. When Chief Justice Warren E. Burger retired in September 1986, Reagan nominated incumbent Associate Justice William Rehnquist to succeed Burger as Chief Justice (the appointment of an incumbent associate justice as chief justice is subject to a separate confirmation process). Then, following Rehnquist's confirmation, the president named Antonin Scalia to fill the consequent associate justice vacancy. Reagan's final opportunity to fill a vacancy arose in mid-1987 when Associate Justice Lewis F. Powell Jr. announced his intention to retire. Reagan initially chose Conservative jurist Robert Bork to succeed Powell. Bork's nomination was strongly opposed by civil and women's rights groups, and by Senate Democrats. That October, after a contentious Senate debate, the nomination was rejected by a roll call vote of 42–58. Soon afterward, Reagan announced his intention to nominate Douglas Ginsburg to the Court. However, before his name was submitted to the Senate, Ginsburg withdrew himself from consideration. Anthony Kennedy was subsequently nominated and confirmed as Powell's successor.

Along with his four Supreme Court appointments, Reagan appointed 83 judges to the United States courts of appeals, and 290 judges to the United States district courts. Early in his presidency, Reagan appointed Clarence M. Pendleton Jr., of San Diego as the first African American to chair the United States Commission on Civil Rights. Pendleton tried to steer the commission into a conservative direction in line with Reagan's views on social and civil rights policy during his tenure from 1981 until his sudden death in 1988. Pendleton soon aroused the ire of many civil rights advocates and feminists when he ridiculed the comparable worth proposal as being "Looney Tunes."

On April 13, 1992, Reagan was assaulted by an anti-nuclear protester during a luncheon speech while accepting an award from the National Association of Broadcasters in Las Vegas. The protester, Richard Springer, smashed a 2-foot-high (60 cm) 30-pound (13.5 kg) crystal statue of an eagle that the broadcasters had given the former president. Flying shards of glass hit Reagan, but he was not injured. Using media credentials, Springer intended to announce government plans for an underground nuclear weapons test in the Nevada desert the following day. Springer was the founder of an anti-nuclear group called the "100th Monkey". Following his arrest on assault charges, a Secret Service spokesman could not explain how Springer got past the federal agents who guarded Reagan's life at all times. Later, Springer pled guilty to reduced charges and said he had not meant to hurt Reagan through his actions. He pled guilty to a misdemeanor federal charge of interfering with the Secret Service, but other felony charges of assault and resisting officers were dropped.

After leaving office in 1989, the Reagans purchased a home in Bel Air, Los Angeles, in addition to the Reagan Ranch in Santa Barbara. They regularly attended Bel Air Church and occasionally made appearances on behalf of the Republican Party; Reagan delivered a well-received speech at the 1992 Republican National Convention. Previously, on November 4, 1991, the Ronald Reagan Presidential Library was dedicated and opened to the public. Five presidents and six first ladies attended the dedication ceremonies, marking the first time that five presidents were gathered in the same location. Reagan continued to speak publicly in favor of a line-item veto; the Brady Bill; a constitutional amendment requiring a balanced budget; and the repeal of the 22nd Amendment, which prohibits anyone from serving more than two terms as president. In 1992 Reagan established the Ronald Reagan Freedom Award with the newly formed Ronald Reagan Presidential Foundation. His final public speech occurred on February 3, 1994, during a tribute to him in Washington, D.C.; his last major public appearance was at the funeral of Richard Nixon on April 27, 1994.

In August 1994, at the age of 83, Reagan was diagnosed with Alzheimer's disease, an incurable neurological disorder which destroys brain cells and ultimately causes death. In November of that year, he informed the nation of the diagnosis through a handwritten letter, writing in part:

After his diagnosis, letters of support from well-wishers poured into his California home. But there was also speculation over how long Reagan had demonstrated symptoms of mental degeneration. At a June 1981 reception for mayors, not long after the assassination attempt, Reagan greeted his Secretary of Housing and Urban Development Samuel Pierce by saying "How are you, Mr. Mayor? How are things in your city?", although he later realized his mistake. In a 2011 book, Reagan's son Ron said he had suspected early signs of his father's dementia as early as 1984. Former CBS White House correspondent Lesley Stahl recounted that in her final meeting with the president in 1986, Reagan did not seem to know who Stahl was. Stahl came close to reporting that Reagan was senile, but at the end of the meeting, he had regained his alertness.

Dr. Lawrence Altman of "The New York Times" noted that "the line between mere forgetfulness and the beginning of Alzheimer's can be fuzzy", and all four of Reagan's White House doctors said that they saw no evidence of Alzheimer's while he was president. Daniel Ruge, a neurosurgeon who served as Physician to the President from 1981 to 1985, said that he never detected signs of the disease while speaking almost every day with Reagan. John E. Hutton, who served from 1985 to 1989, said the president "absolutely" did not "show any signs of dementia or Alzheimer's". Other staff members, former aides, and friends said they saw no indication of Alzheimer's while he was president. Reagan did experience occasional memory lapses, though, especially with names. Reagan's doctors said that he began exhibiting overt symptoms of the illness in late 1992<ref name="NYT_2004/06/15"></ref> or 1993, several years after he had left office. For example, Reagan repeated a toast to Margaret Thatcher, with identical words and gestures, at his 82nd-birthday party on February 6, 1993.

Reagan suffered an episode of head trauma in July 1989, five years before his diagnosis. After being thrown from a horse in Mexico, a subdural hematoma was found and surgically treated later in the year. Nancy Reagan, citing what doctors told her, asserted that her husband's 1989 fall hastened the onset of Alzheimer's disease, although acute brain injury has not been conclusively proven to accelerate Alzheimer's or dementia. Ruge said it is possible that the horse accident affected Reagan's memory.

As the years went on, the disease slowly destroyed Reagan's mental capacity. He was able to recognize only a few people, including his wife, Nancy. He remained active, however; he took walks through parks near his home and on beaches, played golf regularly, and until 1999 he often went to his office in nearby Century City.

Reagan suffered a fall at his Bel Air home on January 13, 2001, resulting in a broken hip. The fracture was repaired the following day, and the 89-year-old Reagan returned home later that week, although he faced difficult physical therapy at home. On February 6, 2001, Reagan reached the age of 90, only the third U.S. president (after John Adams and Herbert Hoover) to do so. Reagan's public appearances became much less frequent with the progression of the disease, and as a result, his family decided that he would live in quiet semi-isolation with his wife Nancy. She told CNN's Larry King in 2001 that very few visitors were allowed to see her husband because she felt that "Ronnie would want people to remember him as he was." After her husband's diagnosis and death, Nancy Reagan became a stem-cell research advocate, asserting that it could lead to a cure for Alzheimer's.

Reagan died of pneumonia, complicated by Alzheimer's disease, at his home in the Bel Air district of Los Angeles, California, on the afternoon of June 5, 2004. A short time after his death, Nancy Reagan released a statement saying, "My family and I would like the world to know that President Ronald Reagan has died after 10 years of Alzheimer's disease at 93 years of age. We appreciate everyone's prayers." Speaking in Paris, France, President George W. Bush called Reagan's death "a sad hour in the life of America". He also declared June 11 a National Day of Mourning.

Reagan's body was taken to the Kingsley and Gates Funeral Home in Santa Monica, California, where well-wishers paid tribute by laying flowers and American flags in the grass. On June 7, his body was transferred to the Ronald Reagan Presidential Library, where a brief family funeral, conducted by Pastor Michael Wenning, was held. Reagan's body lay in repose in the Library lobby until June 9; over 100,000 people viewed the coffin. On June 9, Reagan's body was flown to Washington, D.C., where he became the tenth U.S. president to lie in state in the Rotunda of the U.S. Capitol; in thirty-four hours, 104,684 people filed past the coffin.

On June 11, a state funeral was conducted in the Washington National Cathedral, presided over by President George W. Bush. Eulogies were given by former British Prime Minister Margaret Thatcher, former Canadian Prime Minister Brian Mulroney, and both former President George H. W. Bush and President George W. Bush. Also in attendance were Mikhail Gorbachev and many world leaders, including British Prime Minister Tony Blair; Prince Charles, representing his mother Queen Elizabeth II; German Chancellor Gerhard Schröder; Italian Prime Minister Silvio Berlusconi; and interim presidents Hamid Karzai of Afghanistan and Ghazi al-Yawer of Iraq.

After the funeral, the Reagan entourage was flown back to the Ronald W. Reagan Presidential Library in Simi Valley, California, where another service was held, and President Reagan was interred. At the time of his death, Reagan was the longest-lived president in U.S. history, having lived 93 years and 120 days (2 years, 8 months, and 23 days longer than John Adams, whose record he surpassed). He was also the first U.S. president to die in the 21st century.

Reagan's burial site is inscribed with the words he delivered at the opening of the Ronald Reagan Presidential Library: "I know in my heart that man is good, that what is right will always eventually triumph and that there is purpose and worth to each and every life."

Since Reagan left office in 1989, substantial debate has occurred among scholars, historians, and the general public surrounding his legacy. Supporters have pointed to a more efficient and prosperous economy as a result of Reagan's economic policies, foreign policy triumphs including a peaceful end to the Cold War, and a restoration of American pride and morale. Proponents say that he had an unabated and passionate love for the United States which restored faith in the American Dream, after a decline in American confidence and self-respect under Jimmy Carter's perceived weak leadership, particularly during the Iran hostage crisis, as well as his gloomy, dreary outlook for the future of the United States during the 1980 election. Critics point out that Reagan's economic policies resulted in rising budget deficits, a wider gap in wealth, and an increase in homelessness and that the Iran–Contra affair lowered American credibility.

Opinions of Reagan's legacy among the country's leading policymakers and journalists differ as well. Edwin Feulner, president of The Heritage Foundation, said that Reagan "helped create a safer, freer world" and said of his economic policies: "He took an America suffering from 'malaise'... and made its citizens believe again in their destiny." However, Mark Weisbrot, co-Director of the Center for Economic and Policy Research, contended that Reagan's "economic policies were mostly a failure" while Howard Kurtz of "The Washington Post" opined that Reagan was "a far more controversial figure in his time than the largely gushing obits on television would suggest."

Despite the continuing debate surrounding his legacy, many conservative and liberal scholars agree that Reagan has been the most influential president since Franklin D. Roosevelt, leaving his imprint on American politics, diplomacy, culture, and economics through his effective communication and pragmatic compromising. Since he left office, historians have reached a consensus, as summarized by British historian M. J. Heale, who finds that scholars now concur that Reagan rehabilitated conservatism, turned the nation to the right, practiced a considerably pragmatic conservatism that balanced ideology and the constraints of politics, revived faith in the presidency and American exceptionalism, and contributed to victory in the Cold War.

In 2017 a C-SPAN survey of scholars ranked Reagan in terms of leadership in comparison with all 42 presidents. He ranked number nine in international relations.

Reagan's major achievement was that the USSR and Communism collapsed, causing the U.S. to become the world's only superpower. His admirers say he won the Cold War. After 40 years of high tension, the USSR pulled back in the last years of Reagan's second term. In 1989, the Kremlin lost control of all its East European satellites. In 1991, Communism was overthrown in the USSR, and on December 26, 1991, the Soviet Union ceased to exist. The resulting states were no threat to the United States. Reagan's exact role is debated, with many believing that Reagan's defense policies, economic policies, military policies and hard-line rhetoric against the Soviet Union and Communism—together with his summits with General Secretary Gorbachev—played a significant part in ending the Cold War.

He was the first president to reject containment and détente and to put into practice the concept that the Soviet Union could be defeated rather than simply negotiated with, a post-Détente strategy, a conviction that was vindicated by Gennadi Gerasimov, the Foreign Ministry spokesman under Gorbachev, who said that the Strategic Defense Initiative was "very successful blackmail...The Soviet economy couldn't endure such competition." Reagan's aggressive rhetoric toward the USSR had mixed effects; Jeffery W. Knopf observes that being labeled "evil" probably made no difference to the Soviets but gave encouragement to the East-European citizens opposed to communism.

General Secretary Gorbachev said of his former rival's Cold War role: "[He was] a man who was instrumental in bringing about the end of the Cold War," and deemed him "a great president." Gorbachev does not acknowledge a win or loss in the war, but rather a peaceful end; he said he was not intimidated by Reagan's harsh rhetoric. Margaret Thatcher, former Prime Minister of the United Kingdom, said of Reagan, "he warned that the Soviet Union had an insatiable drive for military power... but he also sensed it was being eaten away by systemic failures impossible to reform." She later said, "Ronald Reagan had a higher claim than any other leader to have won the Cold War for liberty and he did it without a shot being fired." Said Brian Mulroney, former Prime Minister of Canada: "He enters history as a strong and dramatic player ." Former President Lech Wałęsa of Poland acknowledged, "Reagan was one of the world leaders who made a major contribution to communism's collapse." Professor Jeffrey Knopf has argued that Reagan's leadership was only one of several causes of the end of the Cold War. President Harry S. Truman's policy of containment is also regarded as a force behind the fall of the USSR, and the Soviet invasion of Afghanistan undermined the Soviet system itself.

Reagan reshaped the Republican party, led the modern conservative movement, and altered the political dynamic of the United States. More men voted Republican under Reagan, and Reagan tapped into religious voters. The so-called "Reagan Democrats" were a result of his presidency. When Reagan left office in 1989, he held an approval rating of 68%. This figure equaled the approval rating of Franklin D. Roosevelt (and was later matched by Bill Clinton), as the highest rating for a departing president in the modern era.

After leaving office, Reagan became an iconic influence within the Republican Party. His policies and beliefs have been frequently invoked by Republican presidential candidates since 1988. The 2008 Republican presidential candidates were no exception, for they aimed to liken themselves to him during the primary debates, even imitating his campaign strategies. Republican nominee John McCain frequently said that he came to office as "a foot soldier in the Reagan Revolution." Reagan's most famous statement regarding the role of smaller government was that "Government is not a solution to our problem, government is the problem." Praise for Reagan's accomplishments was part of standard GOP rhetoric a quarter-century after his retirement. "Washington Post" reporter Carlos Lozada noted how the main Republican contenders in the 2016 presidential race adopted "standard GOP Gipper worship".

The period of American history most dominated by Reagan and his policies that concerned taxes, welfare, defense, the federal judiciary and the Cold War is known today as the Reagan Era. This time period emphasized that the conservative "Reagan Revolution," led by Reagan, had a permanent impact on the United States in domestic and foreign policy. The Bill Clinton administration is often treated as an extension of the Reagan Era, as is the George W. Bush administration. Historian Eric Foner noted that the Obama candidacy in 2008 "aroused a great deal of wishful thinking among those yearning for a change after nearly thirty years of Reaganism."

According to columnist Chuck Raasch, "Reagan transformed the American presidency in ways that only a few have been able to." He redefined the political agenda of the times, advocating lower taxes, a conservative economic philosophy, and a stronger military. His role in the Cold War further enhanced his image as a different kind of leader. Reagan's "avuncular style, optimism, and plain-folks demeanor" also helped him turn "government-bashing into an art form."

Reagan's popularity has increased since 1989. Gallup polls in 2001 and 2007 ranked him number one or number two when correspondents were asked for the greatest president in history. Reagan ranked third of post–World War II presidents in a 2007 Rasmussen Reports poll, fifth in an ABC 2000 poll, ninth in another 2007 Rasmussen poll, and eighth in a late 2008 poll by British newspaper "The Times". In a Siena College survey of over 200 historians, however, Reagan ranked sixteenth out of 42. While the debate about Reagan's legacy is ongoing, the 2009 Annual "C-SPAN Survey of Presidential Leaders" ranked Reagan the 10th greatest president. The survey of leading historians rated Reagan number 11 in 2000.

In 2011, the Institute for the Study of the Americas released the first-ever British academic survey to rate U.S. presidents. This poll of British specialists in U.S. history and politics placed Reagan as the eighth greatest U.S. president.

Reagan's ability to talk about substantive issues with understandable terms and to focus on mainstream American concerns earned him the laudatory moniker "The Great Communicator." Of it, Reagan said, "I won the nickname the great communicator. But I never thought it was my style that made a difference—it was the content. I wasn't a great communicator, but I communicated great things." His age and soft-spoken speech gave him a warm grandfatherly image.

Reagan also earned the nickname "the Teflon President," in that public perceptions of him were not tarnished by the controversies that arose during his administration. According to Colorado congresswoman Patricia Schroeder, who coined the phrase, and reporter Howard Kurtz, the epithet referred to Reagan's ability to "do almost anything wrong and not get blamed for it."

Public reaction to Reagan was always mixed. He was the oldest president up to that time and was supported by young voters, who began an alliance that shifted many of them to the Republican party. Reagan did not fare well with minority groups, especially African-Americans. However, his support of Israel throughout his presidency earned him support from many Jews. He emphasized family values in his campaigns and during his presidency, although he was the first president to have been divorced. The combination of Reagan's speaking style, unabashed patriotism, negotiation skills, as well as his savvy use of the media, played an important role in defining the 1980s and his future legacy.

Reagan was known to joke frequently during his lifetime, displayed humor throughout his presidency, and was famous for his storytelling. His numerous jokes and one-liners have been labeled "classic quips" and "legendary." Among the most notable of his jokes was one regarding the Cold War. As a microphone test in preparation for his weekly radio address in August 1984, Reagan made the following joke: "My fellow Americans, I'm pleased to tell you today that I've signed legislation that will outlaw Russia forever. We begin bombing in five minutes." Former aide David Gergen commented, "It was that humor... that I think endeared people to Reagan."

He also had the ability to offer comfort and hope to the nation as a whole at times of tragedy. Following the disintegration of the Space Shuttle "Challenger" on January 28, 1986. On the evening of the disaster, Reagan addressed the nation saying,

Reagan received several awards in his pre- and post-presidential years. After his election as president, Reagan received a lifetime gold membership in the Screen Actors Guild, was inducted into the National Speakers Association Speaker Hall of Fame, and received the United States Military Academy's Sylvanus Thayer Award.

In 1981, Reagan was inducted as a Laureate of The Lincoln Academy of Illinois and awarded the Order of Lincoln (the state's highest honor) by the Governor of Illinois in the area of Government. In 1982 he was given the "Distinguished Service Medal" by the American Legion because his highest priority was the national defense. In 1983, he received the highest distinction of the Scout Association of Japan, the Golden Pheasant Award. In 1989, Reagan was made an Honorary Knight Grand Cross of the Order of the Bath, one of the highest British orders (this entitled him to the use of the post-nominal letters "GCB" but, as a foreign national, not to be known as "Sir Ronald Reagan"); only two U.S. presidents have received this honor since attaining office, Reagan and George H. W. Bush, while Dwight D. Eisenhower received his before becoming President in his capacity as a general after World War II. Reagan was also named an honorary Fellow of Keble College, Oxford. Japan awarded him the Grand Cordon of the Order of the Chrysanthemum in 1989; he was the second U.S. president to receive the order and the first to have it given to him for personal reasons (Dwight D. Eisenhower received it as a commemoration of U.S.-Japanese relations). In 1990, Reagan was awarded the WPPAC's Top Honor Prize because he signed the Intermediate-Range Nuclear Forces Treaty with H.E. Mikhail Sergeyevich Gorbachev (then President of Russia), ending the cold war.

On January 18, 1993, Reagan received the Presidential Medal of Freedom (awarded with distinction), the highest honor that the United States can bestow, from President George H. W. Bush, his Vice President and successor. Reagan was also awarded the Republican Senatorial Medal of Freedom, the highest honor bestowed by Republican members of the Senate.

On Reagan's 87th birthday in 1998, Washington National Airport was renamed Ronald Reagan Washington National Airport by a bill signed into law by President Bill Clinton. That year, the Ronald Reagan Building and International Trade Center was dedicated in Washington, D.C. He was among 18 included in Gallup's most admired man and woman poll of the 20th century, from a poll conducted in the U.S. in 1999; two years later, was christened by Nancy Reagan and the United States Navy. It is one of few Navy ships christened in honor of a living person and the first aircraft carrier to be named in honor of a living former president.

In 1998 the U.S. Navy Memorial Foundation awarded Reagan its Naval Heritage award for his support of the U.S. Navy and military in both his film career and while he served as president.

Congress authorized the creation of the Ronald Reagan Boyhood Home in Dixon, Illinois in 2002, pending federal purchase of the property. On May 16 of that year, Nancy Reagan accepted the Congressional Gold Medal, the highest civilian honor bestowed by Congress, on behalf of the president and herself.

After Reagan's death, the United States Postal Service issued a President Ronald Reagan commemorative postage stamp in 2005. Later in the year, CNN, along with the editors of "Time" magazine, named him the "most fascinating person" of the network's first 25 years; "Time" listed Reagan one of the 100 Most Important People of the 20th century as well. The Discovery Channel asked its viewers to vote for The Greatest American in June 2005; Reagan placed in first place, ahead of Lincoln and Martin Luther King Jr.

In 2006, Reagan was inducted into the California Hall of Fame, located at The California Museum. Every year from 2002, California governors Gray Davis and Arnold Schwarzenegger proclaimed February 6 "Ronald Reagan Day" in the state of California in honor of their most famous predecessor. In 2010, Schwarzenegger signed Senate Bill 944, authored by Senator George Runner, to make every February 6 Ronald Reagan Day in California.

In 2007, Polish President Lech Kaczyński posthumously conferred on Reagan the highest Polish distinction, the Order of the White Eagle, saying that Reagan had inspired the Polish people to work for change and helped to unseat the repressive communist regime; Kaczyński said it "would not have been possible if it was not for the tough-mindedness, determination, and feeling of mission of President Ronald Reagan." Reagan backed the nation of Poland throughout his presidency, supporting the anti-communist Solidarity movement, along with Pope John Paul II; the Ronald Reagan Park, a public facility in Gdańsk, was named in his honor.

On June 3, 2009, Nancy Reagan unveiled a statue of her late husband in the United States Capitol rotunda. The statue represents the state of California in the National Statuary Hall Collection. After Reagan's death, both major American political parties agreed to erect a statue of Reagan in the place of that of Thomas Starr King. The day before, President Obama signed the Ronald Reagan Centennial Commission Act into law, establishing a commission to plan activities to mark the upcoming centenary of Reagan's birth.

On Independence Day 2011 a statue to Reagan was unveiled in London, England, outside the U.S. embassy in Grosvenor Square. The unveiling was supposed to be attended by Reagan's wife Nancy, but she did not attend; former Secretary of State Condoleezza Rice took her place and read a statement on her behalf. President Reagan's friend and British prime minister during his presidency, Margaret Thatcher, was also unable to attend due to frail health.









</doc>
<doc id="25440" url="https://en.wikipedia.org/wiki?curid=25440" title="Robert J. Flaherty">
Robert J. Flaherty

Robert Joseph Flaherty, (; February 16, 1884 – July 23, 1951) was an American filmmaker who directed and produced the first commercially successful feature-length documentary film, "Nanook of the North" (1922). The film made his reputation and nothing in his later life fully equaled its success, although he continued the development of this new genre of narrative documentary with "Moana" (1926), set in the South Seas, and "Man of Aran" (1934), filmed in Ireland's Aran Islands. Flaherty is considered the "father" of both the documentary and the ethnographic film.

Flaherty was married to writer Frances H. Flaherty from 1914 until his death in 1951. Frances worked on several of her husband's films, and received an Academy Award nomination for Best Original Story for "Louisiana Story" (1948).
Flaherty was one of seven children born to prospector Robert Henry Flaherty (an Irish Protestant) and Susan Klockner (a German Catholic).

Due to exposure from his father's work as an iron ore explorer, he developed a natural curiosity for people of other cultures. Flaherty was an acclaimed still-photographer in Toronto. His portraits of American Indians and wild life during his travels are what led to the creation of his critically acclaimed "Nanook of the North". It was his enthusiasm and interests in these people that sparked his need to create a new genre of film.

Flaherty couldn't do it alone, however. In 1914, he married his fiancée Frances Hubbard. Hubbard came from a highly educated family, her father being a distinguished geologist. A graduate from Bryn Mawr University in Pennsylvania, Hubbard studied music and poetry in Paris and was also secretary of the local Suffragette Society. Following their marriage, Frances Flaherty became a crucial part of Robert's success in film. Francis took on the role of director at times helped to edit and distribute her husband's films, even landing governmental film contracts for England.

In 1909 he shared stories about information he was told by an Inuk man named Wetallok. Flaherty said he met Wetallok while visiting the Hudson Bay in search of iron ore. In his story, Flaherty published a detailed map of the Inuit region and shared information about the bay that Wetallok had told him. His writing about Wetallok would go on to be published in his book, "My Eskimo Friends: "Nanook of the North"."

In 1913, on Flaherty's expedition to prospect the Belcher Islands, his boss, Sir William Mackenzie, suggested that he take a motion picture camera along. He brought a Bell and Howell hand cranked motion picture camera. He was particularly intrigued by the life of the Inuit people, and spent so much time filming them that he had begun to neglect his real work. When Flaherty returned to Toronto with 30,000 feet of film, the nitrate film stock was ignited in a fire started from his cigarette in his editing room. His film was destroyed and his hands were burned. Although his editing print was saved and shown several times, Flaherty was not satisfied with the results. "It was utterly inept, simply a scene of this or that, no relation, no thread of story or continuity whatever, and it must have bored the audience to distraction. Certainly it bored me."

Flaherty was determined to make a new film, one following a life of a typical Inuk and his family. In 1920, he secured funds from Revillon Frères, a French fur trade company to shoot what was to become "Nanook of the North". On 15 August 1920, Flaherty arrived in Port Harrison, Quebec to shoot his film. He brought two Akeley motion-picture cameras which the Inuit referred to as "the aggie". He also brought full developing, printing, and projection equipment to show the Inuit his film, while he was still in the process of filming. He lived in an attached cabin to the Revillon Frères trading post.

In making "Nanook", Flaherty cast various locals in parts in the film, in the way that one would cast actors in a work of fiction. With the aim of showing traditional Inuit life, he also staged some scenes, including the ending, where Allakariallak (who acts the part of Nanook) and his screen family are supposedly at risk of dying if they could not find or build shelter quickly enough. The half-igloo had been built beforehand, with a side cut away for light so that Flaherty's camera could get a good shot. Flaherty insisted that the Inuit not use rifles to hunt, though their use had by that time become common. He also pretended at one point that he could not hear the hunters' pleas for help, instead continuing to film their struggle and putting them in greater danger.
Melanie McGrath writes that, while living in Northern Quebec for the year of filming "Nanook", Flaherty had an affair with his lead actress, the young Inuk woman who played Nanook's wife. A few months after he left, she gave birth to his son, Josephie (December 25, 1921 – 1984), whom he never acknowledged. Josephie was one of the Inuit who were relocated in the 1950s to very difficult living conditions in Resolute and Grise Fiord, in the extreme north (see High Arctic relocation). Corroboration of McGrath's account is not readily available and Flaherty never discussed the matter.

"Nanook" began a series of films that Flaherty was to make on the same theme of humanity against the elements. Others included "Moana: A Romance of the Golden Age", set in Samoa, and "Man of Aran", set in the Aran Islands of Ireland. All these films employ the same rhetorical devices: the dangers of nature and the struggle of the communities to eke out an existence.

"Nanook of the North" (1922) was a successful film, and Flaherty was in great demand afterwards. On a contract with Paramount to produce another film on the order of "Nanook", he went to Samoa to film "Moana" (1926). He shot the film in Safune on the island of Savai'i where he lived with his wife and family for more than a year. The studio heads repeatedly asked for daily rushes but Flaherty had nothing to show because he had not filmed anything yet — his approach was to try to live with the community, becoming familiar with their way of life before writing a story about it to film. He was also concerned that there was no inherent conflict in the islanders' way of life, providing further incentive not to shoot anything. Eventually he decided to build the film around the ritual of a boy's entry to manhood. Flaherty was in Samoa from April 1923 until December 1924, with the film completed in December 1925 and released the following month. The film, on its release, was not as successful as "Nanook of the North" domestically, but it did very well in Europe, inspiring John Grierson to coin the word "documentary."

Before the release of "Moana", Flaherty made two short films in New York City with private backing, "The Pottery Maker" (1925) and "The Twenty-Four Dollar Island" (1927). Irving Thalberg of Metro-Goldwyn-Mayer invited Flaherty to film "White Shadows in the South Seas" (1928) in collaboration with W. S. Van Dyke, but their talents proved an uncomfortable fit, and Flaherty resigned from the production. Moving to Fox Film Corporation, Flaherty spent eight months working on the Native American documentary "Acoma the Sky City" (1929), but the production was shut down, and subsequently Flaherty's footage was lost in a studio vault fire. He then agreed to collaborate with F. W. Murnau on another South Seas picture, "Tabu" (1931). However, this combination proved even more volatile, and while Flaherty did contribute significantly to the story, the finished film, originally released by Paramount Pictures, is essentially Murnau's.

After "Tabu", Flaherty was considered finished in Hollywood, and Frances Flaherty contacted John Grierson of the Empire Marketing Board Film Unit in London, who assigned Flaherty to the documentary "Industrial Britain" (1931). By comparison to Grierson and his unit, Flaherty's habitual working methods involved shooting relatively large amounts of film in relation to the planned length of the eventual finished movie, and the ensuing cost overruns obliged Grierson to take Flaherty off the project, which was edited by other hands into three shorter films.

Flaherty's career in Britain ended when producer Alexander Korda removed him from the production "Elephant Boy" (1937), re-editing it into a commercial entertainment picture.

Producer Michael Balcon took Flaherty on to direct "Man of Aran" (1934), which portrayed the harsh traditional lifestyle of the occupants of the isolated Aran Islands off the west coast of Ireland. The film was a major critical success, and for decades was considered in some circles an even greater achievement than "Nanook". As with "Nanook", "Man of Aran" showed human beings' efforts to survive under extreme conditions: in this case, an island whose soils were so thin that the inhabitants carried seaweed up from the sea to construct fields for cultivation. Flaherty again cast locals in the various fictionalized roles, and made use of dramatic recreation of anachronistic behaviors: in this case, a sequence showing the hunting of sharks from small boats with harpoons, which the islanders had by then not practiced for several decades. He also staged the film's climactic sequence, in which three men in a small boat strive to row back to shore through perilously high, rock-infested seas.

Back in the United States, Pare Lorentz of the United States Film Service hired Flaherty to film a documentary about US agriculture, a project which became "The Land". Flaherty and his wife covered some 100,000 miles, shooting 25,000 feet of film, and captured a series of striking images of rural America. Among the themes raised by Flaherty's footage were the challenge of the erosion of agricultural land and the Dust Bowl (as well as the beginning of effective responses via improved soil conservation practices), mechanization and rural unemployment, and large-scale migration from the Great Plains to California. In the latter context, Flaherty highlighted competition for agricultural jobs between native-born Americans and migrants from Mexico and the Philippines.

The film encountered a series of obstacles. After production had begun, Congress abolished the United States Film Service, and the project was shunted to the U.S. Department of Agriculture (USDA). With America's entry to World War II approaching, USDA officials (and the film's editor Helen van Dongen) attempted to reconcile Flaherty's footage with rapidly changing official messages (including a reversal of concern from pre-war rural unemployment to wartime labor shortages). Following the attack on Pearl Harbor, officials grew apprehensive that the film could project an unduly negative image of the US internationally, and although a prestige opening was held at the Museum of Modern Art in 1942, the film was never authorized for general release.

"Louisiana Story" (1948) was a Flaherty documentary shot by himself and Richard Leacock, about the installation of an oil rig in a Louisiana swamp. The film stresses the rig's peaceful and unproblematic coexistence with the surrounding environment, and it was in fact funded by Standard Oil, a petroleum company. The main character of the film is a Cajun boy. The poetry of childhood and nature, some critics argue, is used to make the exploration of oil look beautiful. Virgil Thomson composed the music for the film.

Flaherty was one of the makers of "" (1950), which won the Academy Award for Best Documentary Feature. The film was a re-edited version of the German/Swiss film originally titled "Michelangelo: Life of a Titan" (1938), directed by Curt Oertel. The re-edited version put a new English narration by Fredric March and musical score onto a shorter edit of the existing film. The new credits include Richard Lyford as director and Robert Snyder as producer. The film was edited by Richard Lyford.

Flaherty is considered a pioneer of documentary film. He was one of the first to combine documentary subjects with a fiction-film-like narrative and poetic treatment.

A self-proclaimed explorer, Flaherty was inducted into the Royal Geographic Society of England for his (re)discovery of the main island of the Belcher group in Hudson Bay in 1914.

Flaherty Island, one of the Belcher Islands in Hudson Bay, is named in his honor.

The Flaherty Seminar is an annual international forum for independent filmmakers and film-lovers, held in rural upstate New York at Colgate University in mid June . The festival was founded in Flaherty's honor by his widow in 1955.

Flaherty's contribution to the advent of the documentary is scrutinized in the 2010 British Universities Film & Video Council award-winning and FOCAL International award-nominated documentary "A Boatload of Wild Irishmen", written by Professor Brian Winston of University of Lincoln, UK, and directed by Mac Dara Ó Curraidhín. The film explores the nature of "controlled actuality" and sheds new light on thinking about Flaherty. The argument is made that the impact of Flaherty's films on the indigenous peoples portrayed changes over time, as the films become valuable records for subsequent generations of now-lost ways of life. The film's title derives from Flaherty's own statement that he had been accused, in the staged climactic sequence of "Man of Aran", of "trying to drown a boatload of wild Irishmen".

In 1994, Flaherty was portrayed by Charles Dance in the Canadian drama film "Kabloonak", a dramatization of the making of "Nanook of the North" from an Inuit perspective.







</doc>
<doc id="25441" url="https://en.wikipedia.org/wiki?curid=25441" title="Rolling Stone">
Rolling Stone

Rolling Stone is an American monthly magazine that focuses on popular culture. It was founded in San Francisco, California, in 1967 by Jann Wenner, who is still the magazine's publisher, and the music critic Ralph J. Gleason. It was first known for its musical coverage of rock music and for political reporting by Hunter S. Thompson. In the 1990s, the magazine broadened and shifted its focus to a younger readership interested in youth-oriented television shows, film actors, and popular music. It has returned to its traditional mix of content, including music, entertainment, and politics.

The first magazine was released in 1967 and featured John Lennon on the cover. It is known for provocative photography and its cover photos, featuring musicians, politicians, athletes, and actors. In addition to its print version in the United States, it publishes content through Rollingstone.com and numerous international editions.

Penske Media Corporation is the current owner of "Rolling Stone", purchasing 51 percent of the magazine in 2017 and the remaining 49 percent in 2019.

"Rolling Stone" was founded in San Francisco in 1967 by Jann Wenner and Ralph Gleason. To get it off the ground, Wenner borrowed $7,500 from his own family and from the parents of his soon-to-be wife, Jane Schindelheim. The first issue was released on November 9, 1967 and featured John Lennon on the cover. It was in newspaper format with a lead article on the Monterey Pop Festival. The cover price was 25¢ (equivalent to $ in 2016).

In the first issue, Wenner explained that the title of the magazine referred to the 1950 blues song "Rollin' Stone", recorded by Muddy Waters, and Bob Dylan's hit single "Like a Rolling Stone":

In the 1970s, "Rolling Stone" began to make a mark with its political coverage, with the likes of gonzo journalist Hunter S. Thompson writing for the magazine's political section. Thompson first published his most famous work "Fear and Loathing in Las Vegas" within the pages of "Rolling Stone", where he remained a contributing editor until his death in 2005. In the 1970s, the magazine also helped launch the careers of many prominent authors, including Cameron Crowe, Lester Bangs, Joe Klein, Joe Eszterhas, Ben Fong-Torres, Patti Smith and P. J. O'Rourke. It was at this point that the magazine ran some of its most famous stories, including that of the Patty Hearst abduction odyssey. One interviewer, speaking for many his peers, said that he bought his first copy of the magazine upon initial arrival on his college campus, describing it as a "rite of passage".

In 1977, the magazine moved its headquarters from San Francisco to New York City. Editor Jann Wenner said San Francisco had become "a cultural backwater".

"Rolling Stone" shifted to more of an entertainment magazine in the 1980s. It still had music as the main topic but began to increase its coverage of celebrities, films, and pop culture. It also began releasing its annual "Hot Issue."

"Rolling Stone" was initially known for its musical coverage and for Thompson's political reporting. In the 1990s, the magazine changed its format to appeal to a younger readership interested in youth-oriented television shows, film actors, and popular music. This led to criticism that the magazine was emphasizing style over substance. In recent years, the magazine has resumed its traditional mix of content, including in-depth political stories. It has also expanded content to include coverage of financial and banking issues. As a result, the magazine has seen its circulation increase and its reporters invited as experts to network television programs of note.

After years of declining readership, the magazine experienced a major resurgence of interest and relevance with the work of two young journalists in the late 2000s, Michael Hastings and Matt Taibbi.

In 2005, Dana Leslie Fields, former publisher of "Rolling Stone", who had worked at the magazine for 17 years, was an inaugural inductee into the Magazine Hall of Fame.

In 2009, Taibbi unleashed an acclaimed series of scathing reports on the financial meltdown of the time. He famously described Goldman Sachs as "a great vampire squid".

Bigger headlines came at the end of June 2010. "Rolling Stone" caused a controversy in the White House by publishing in the July issue an article by journalist Michael Hastings entitled, "The Runaway General", quoting criticism by General Stanley A. McChrystal, commander of the International Security Assistance Force and U.S. Forces-Afghanistan commander, about Vice President Joe Biden and other Administration members of the White House. McChrystal resigned from his position shortly after his statements went public.

In 2010, Taibbi documented illegal and fraudulent actions by banks in the foreclosure courts, after traveling to Jacksonville, Florida and sitting in on hearings in the courtroom. His article, "Invasion of the Home Snatchers" also documented attempts by the judge to intimidate a homeowner fighting foreclosure and the attorney Taibbi accompanied into the court.

In January 2012, the magazine ran exclusive excerpts from Hastings' book just prior to publication. The book, "The Operators: The Wild and Terrifying Inside Story of America's War in Afghanistan", provided a much more expansive look at McChrystal and the culture of senior American military and how they become embroiled in such wars. The book reached Amazon's bestseller list in the first 48 hours of release, and it received generally favorable reviews. Salon's Glenn Greenwald described it as "superb," "brave" and "eye-opening".

In 2012, Taibbi, through his coverage of the Libor scandal, emerged as an expert on that topic, which led to media appearances outside "Rolling Stone".

On November 9, 2012, the magazine published its first Spanish-language section on Latino music and culture, in the issue dated November 22.

In September 2016, Advertising Age reported that Wenner is in the process of selling a 49% stake of the magazine to a company from Singapore called BandLab. The new investor had no direct involvement in the editorial content of the magazine.

In September 2017, Wenner Media announced that the remaining 51% of Rolling Stone magazine is up for sale. In December 2017, Penske Media acquired the remaining stake from Wenner Media. On January 31, 2019, Penske acquired BandLab's 49% stake in "Rolling Stone", gaining full ownership of the magazine.

Some artists have been featured on the cover many times, and some of these pictures went on to become iconic. The Beatles, for example, have appeared on the cover more than 30 times, either individually or as a band. The magazine is known for provocative photography and has featured musicians and celebrities on the cover throughout its history. "Vanity Fair" called the January 22, 1981 cover featuring John Lennon and Yoko Ono the "Greatest Rolling Stone Cover Ever".

The first 10 issues featured, in order of appearance, the following:


The printed format has gone through several changes. The first publications, in 1967–72, were in folded tabloid newspaper format, with no staples, black ink text, and a single color highlight that changed each edition. From 1973 onwards, editions were produced on a four-color press with a different newsprint paper size. In 1979, the bar code appeared. In 1980, it became a gloss-paper, large format (10"×12") magazine. Editions switched to the standard 8"×11" magazine size starting with the issue dated October 30, 2008. Starting with the July 2018 issue, it returned to the previous 10"×12" large format.

"Rolling Stone"s maintains a website where it shares similar content to its print publication.

The site at one time had an extensive message-board forum. By the late 1990s, this had developed into a thriving community, with many regular members and contributors worldwide. However, the site was also plagued with numerous Internet trolls and malicious code-hackers, who vandalized the forum substantially. The magazine abruptly deleted the forum in May 2004, then began a new, much more limited message board community on their site in late 2005, only to remove it again in 2006. In March 2008, the website started a new message board section once again, then deleted it in April 2010.

"Rolling Stone" devotes one of its table of contents pages to promoting material currently appearing on its website, listing detailed links to the items.

On April 19, 2010, the website underwent a redesign and began featuring the complete archives of "Rolling Stone". The archive was first launched under a for-pay model, but has since transitioned to a free-with-print-subscription model. In the spring of 2012, "Rolling Stone" launched a federated search feature which searches both the website and the archive.

The website has become an interactive source of biographical information on music artists in addition to historical rankings from the magazine. Users can cross-reference lists and they are also provided with historical insights. For example, one group that is listed on both Rolling Stone's 500 Greatest Albums of All Time and Rolling Stone's 500 Greatest Songs of All Time is Toots and the Maytals, with biographical details from Rolling Stone that explain how Toots and the Maytals are responsible for coining the term "reggae" in their song "Do the Reggay". For biographical information on all artists, the website contains a directory listed alphabetically.

In May 2016, Wenner Media announced plans to create a separate online publication dedicated to the coverage of video games and video game culture. Gus Wenner, Jann Wenner's son and head of digital for the publication at the time, told "The New York Times" that "gaming is today what rock 'n' roll was when "Rolling Stone" was founded". "Glixel" was originally hosted on "Rolling Stone"s website and transitioned to its own domain by October 2016. Stories from "Glixel" are included on the "Rolling Stone" website, while writers for "Rolling Stone" were also able to contribute to "Glixel". The site was headed by John Davison, and its offices were located in San Francisco. "Rolling Stone" closed down the offices in June 2017 and fired the entire staff, citing the difficulties of working with the remote site from their main New York office. Brian Crecente, founder of Kotaku and co-founder of bigger Polygon, was hired as editorial director and runs the site from the main New York office. Following the sale of "Rolling Stone"s assets to Penske Media Corporation, the "Glixel" content was merged into the routine publishing of "Variety", with Crecente remaining as editorial director.

"Rolling Stone" endorsed Democratic candidate Hillary Clinton in the run-up for the 2016 U.S. presidential election.

In December 2009, the "Los Angeles Times" reported that the owners of "Rolling Stone" magazine planned to open a "Rolling Stone" restaurant in the Hollywood & Highland Center in Hollywood in the spring of 2010. The expectation was that the restaurant could become the first of a national chain if it was successful. As of November 2010, the "soft opening" of the restaurant was planned for December 2010. In 2011, the restaurant was open for lunch and dinner as well as a full night club downstairs on the weekends. The restaurant closed in February 2013.

One major criticism of "Rolling Stone" involves its generational bias toward the 1960s and 1970s. One critic referred to the "Rolling Stone" list of the "500 Greatest Songs" as an example of "unrepentant rockist fogeyism". In further response to this issue, rock critic Jim DeRogatis, a former "Rolling Stone" editor, published a thorough critique of the magazine's lists in a book called "Kill Your Idols: A New Generation of Rock Writers Reconsiders the Classics", which featured differing opinions from many younger critics.

"Rolling Stone" magazine has been criticized for reconsidering many classic albums that it had previously dismissed, and for frequent use of the 3.5-star rating. For example, Led Zeppelin was largely written off by "Rolling Stone" magazine critics during the band's most active years in the 1970s, but by 2006, a cover story on the band honored them as "the Heaviest Band of All Time". A critic for "Slate" magazine described a conference at which 1984's "The Rolling Stone Record Guide" was scrutinized. As he described it, "The guide virtually ignored hip-hop and ruthlessly panned heavy metal, the two genres that within a few years would dominate the pop charts. In an auditorium packed with music journalists, you could detect more than a few anxious titters: How many of us will want our record reviews read back to us 20 years hence?"

The hiring of former "FHM" editor Ed Needham further enraged critics who alleged that "Rolling Stone" had lost its credibility.

The 2003 "Rolling Stone's 100 Greatest Guitarists of all Time" article, which named only two female musicians, resulted in "Venus Zine" answering with their own list, entitled "The Greatest Female Guitarists of All Time".

Conservative columnist Jonah Goldberg stated that "Rolling Stone" had "essentially become the house organ of the Democratic National Committee". "Rolling Stone" editor Jann Wenner has made all of his political donations to Democrats.

"Rolling Stone"s film critic, Peter Travers, has been criticized for his high number of repetitively used blurbs.

The August 2013 "Rolling Stone" cover, featuring then-accused (later convicted) Boston Marathon bomber Dzhokhar Tsarnaev, drew widespread criticism that the magazine was "glamorizing terrorism" and that the cover was a "slap in the face to the great city of Boston". The online edition of the article was accompanied by a short editorial stating that the story "falls within the traditions of journalism and "Rolling Stone"'s long-standing commitment to serious and thoughtful coverage of the most important political and cultural issues of our day". The controversial cover photograph that was used by "Rolling Stone" had previously featured on the front page of "The New York Times" on May 5, 2013.

In response to the outcry, New England-based CVS Pharmacy and Tedeschi Food Shops banned their stores from carrying the issue. Also refusing to sell the issue were 
Walgreens; 
Rite-Aid and Kmart; Roche Bros. and Stop & Shop; 
H-E-B and Walmart; 
7-Eleven; 
Hy-Vee, Rutter's Farm, and United Supermarkets; 
Cumberland Farms and Market Basket; 
and Shaw's.
Boston mayor Thomas Menino sent a letter to "Rolling Stone" publisher Jann Wenner, calling the cover "ill-conceived, at best ... [it] reaffirms a message that destruction gains fame for killers and their 'causes'." Menino also wrote, "To respond to you in anger is to feed into your obvious market strategy", and that Wenner could have written about the survivors or the people who came to help after the bombings instead. In conclusion he wrote, "The survivors of the Boston Marathon deserve "Rolling Stone" cover stories, though I no longer feel that "Rolling Stone "deserves them."

In the issue dated November 19, 2014, the story "A Rape on Campus" was run about an alleged gang rape on the campus of the University of Virginia. Separate inquiries by Phi Kappa Psi, the fraternity accused by "Rolling Stone" of facilitating the alleged rape, and "The Washington Post" revealed major errors, omissions and discrepancies in the story. Reporter Sabrina Erdely's story was subject to intense media criticism. "The Washington Post" and "Boston Herald" issued calls for magazine staff involved in the report to be fired. "Rolling Stone" subsequently issued three apologies for the story. Some suggested that legal action against the magazine by persons accused of the rape might result.

On December 5, 2014, "Rolling Stone"s managing editor, Will Dana, apologized for not fact-checking the story. "Rolling Stone" commissioned an outside investigation of the story and its problems by the dean of the Columbia School of Journalism. The report uncovered journalistic failure in the UVA story and institutional problems with reporting at "Rolling Stone". "Rolling Stone" retracted the story on April 5, 2015. On April 6, 2015, following the investigation and retraction of the story, Phi Kappa Psi announced plans to pursue all available legal action against "Rolling Stone", including claims of defamation.

On May 12, 2015, UVA associate dean Nicole Eramo, chief administrator for handling sexual assault issues at the school, filed a $7.5 million defamation lawsuit in Charlottesville Circuit Court against "Rolling Stone" and Erdely, claiming damage to her reputation and emotional distress. Said the filing, ""Rolling Stone" and Erdely's highly defamatory and false statements about Dean Eramo were not the result of an innocent mistake. They were the result of a wanton journalist who was more concerned with writing an article that fulfilled her preconceived narrative about the victimization of women on American college campuses, and a malicious publisher who was more concerned about selling magazines to boost the economic bottom line for its faltering magazine, than they were about discovering the truth or actual facts." On November 4, 2016, after 20 hours of deliberation, a jury consisting of eight women and two men found "Rolling Stone", the magazine's publisher and Erdely liable for defaming Eramo.

On July 29, 2015, three graduates of the fraternity Phi Kappa Psi filed a lawsuit against "Rolling Stone", its publisher Wenner Media, and a journalist for defamation and infliction of emotional distress. The same day, and just months after the controversy began, "The New York Times" reported that managing editor Will Dana was departing the magazine with his last date recorded as August 7, 2015. On November 9, 2015, the Phi Kappa Psi Fraternity filed suit for $25 million for damages to its reputation caused by the magazine's publication of this story, "with reckless disregard for the truth".

George Harrison's song "This Guitar" (1975), a lyrical sequel to his Beatles track "While My Guitar Gently Weeps" (1968), references the magazine in its second verse: "Learned to get up when I fall / Can even climb" Rolling Stone "walls". The song was written in response to some highly unfavorable reviews from "Rolling Stone" and other publications for Harrison's 1974 North American tour and the "Dark Horse" album.

The 2000 film "Almost Famous" centers on a teenage journalist writing for the magazine in the early 1970s while covering the fictional band Stillwater. The film was directed by Cameron Crowe and based on his own experiences as a young journalist for the magazine in the same time period.

"The Cover of Rolling Stone" is a song written by Shel Silverstein and first recorded by American rock group Dr. Hook & the Medicine Show. The song satirizes success in the music business; the song's narrator laments that his band, despite having the superficial attributes of a successful rock star (including drug usage, "teenage groupies, who'll do anything we say", and a frenetic guitar solo), has been unable to "get their pictures on the cover of the "Rolling Stone"".






</doc>
<doc id="25445" url="https://en.wikipedia.org/wiki?curid=25445" title="Romania">
Romania

Romania ( ; ) is a country located at the crossroads of Central, Eastern, and Southeastern Europe. It has borders with the Black Sea to the southeast, Bulgaria to the south, Ukraine to the north, Hungary to the west, Serbia to the southwest, and Moldova to the east. It has a predominantly temperate-continental climate. With a total area of , Romania is the 12th largest country and also the 7th most populous member state of the European Union, having almost 20 million inhabitants. Its capital and largest city is Bucharest, and other major urban areas include Cluj-Napoca, Timișoara, Iași, Constanța, Craiova, Brașov, and Galați.

The River Danube, Europe's second-longest river, rises in Germany's Black Forest and flows in a general southeast direction for , coursing through ten countries before emptying into Romania's Danube Delta. The Carpathian Mountains, which cross Romania from the north to the southwest, include Moldoveanu Peak, at an altitude of .

Modern Romania was formed in 1859 through a personal union of the Danubian Principalities of Moldavia and Wallachia. The new state, officially named Romania since 1866, gained independence from the Ottoman Empire in 1877. Following World War I after declaring its neutrality in 1914, when Romania fought on the side of the Allied powers starting with 1916, Bukovina, Bessarabia, Transylvania as well as parts of Banat, Crișana, and Maramureș became part of the sovereign Kingdom of Romania. In June–August 1940, as a consequence of the Molotov–Ribbentrop Pact and Second Vienna Award, Romania was compelled to cede Bessarabia and Northern Bukovina to the Soviet Union, and Northern Transylvania to Hungary. In November 1940, Romania signed the Tripartite Pact and, consequently, in June 1941 entered World War II on the Axis side, fighting against the Soviet Union until August 1944, when it joined the Allies and recovered Northern Transylvania. Following the war, under the occupation of the Red Army's forces, Romania became a socialist republic and member of the Warsaw Pact. After the 1989 Revolution, Romania began a transition towards democracy and a market economy.

Romania ranks 52nd in the Human Development Index, and is a developing country. It has the world's 47th largest economy by nominal GDP and an annual economic growth rate of 7% (2017), the highest in the EU at the time. Following rapid economic growth in the early 2000s, Romania has an economy predominantly based on services, and is a producer and net exporter of machines and electric energy, featuring companies like Automobile Dacia and OMV Petrom. It has been a member of the United Nations since 1955, part of NATO since 2004, and part of the European Union since 2007. An overwhelming majority of the population identifies themselves as Eastern Orthodox Christians and are native speakers of Romanian, a Romance language.

"Romania" derives from the Latin , meaning "citizen of Rome". The first known use of the appellation was attested to in the 16th century by Italian humanists travelling in Transylvania, Moldavia, and Wallachia.

The oldest known surviving document written in Romanian, a 1521 letter known as the "Letter of Neacșu from Câmpulung", is also notable for including the first documented occurrence of the country's name: Wallachia is mentioned as (old spelling for "The Romanian Land"; from the Latin , "land"; current spelling: ).

Two spelling forms: and were used interchangeably until sociolinguistic developments in the late 17th century led to semantic differentiation of the two forms: came to mean "bondsman", while retained the original ethnolinguistic meaning. After the abolition of serfdom in 1746, the word "rumân" gradually fell out of use and the spelling stabilised to the form . , a revolutionary leader of the early 19th century, used the term to refer exclusively to the principality of Wallachia."

The use of the name "Romania" to refer to the common homeland of all Romanians—its modern-day meaning—was first documented in the early 19th century. The name has been officially in use since 11 December 1861.

In English, the name of the country was formerly spelt "Rumania" or "Roumania". "Romania" became the predominant spelling around 1975. "Romania" is also the official English-language spelling used by the Romanian government. A handful of other languages (including Italian, Hungarian, Portuguese, and Norwegian) have also switched to "o" like English, but most languages continue to prefer forms with "u", e.g. French , German and Swedish , Spanish (the archaic form is still in use in Spain), Polish , Russian (), and Japanese ().


Human remains found in Peștera cu Oase ("Cave with Bones"), radiocarbon dated as being from circa 40,000 years ago, represent the oldest known "Homo sapiens" in Europe. Neolithic techniques and agriculture spread after the arrival of a mixed group of people from Thessaly in the 6th millennium BC. Excavations near a salt spring at Lunca yielded the earliest evidence for salt exploitation in Europe; here the production of salt started between 6050 and 5900 BC. The first permanent settlements also appeared in the Neolithic. Some of them developed into "proto-cities", which were larger than . The Cucuteni–Trypillia culturethe best known archaeological culture of Old Europeflourished in Muntenia, southeastern Transylvania and northeastern Moldavia in the 3rd millennium BC. The first fortified settlements appeared around 1800 BC, showing the militant character of Bronze Age societies.

Greek colonies established on the Black Sea coast in the 7th century BC became important centres of commerce with the local tribes. Among the native peoples, Herodotus listed the Getae of the Lower Danube region, the Agathyrsi of Transylvania and the Syginnae of the plains along the river Tisza at the beginning of the 5th century BC. Centuries later, Strabo associated the Getae with the Dacians who dominated the lands along the southern Carpathian Mountains in the 1st century BC. Burebista was the first Dacian ruler to unite the local tribes. He also conquered the Greek colonies in Dobruja and the neighboring peoples as far as the Middle Danube and the Balkan Mountains between around 55 and 44 BC. After Burebista was murdered in 44 BC, his empire collapsed.

The Romans reached Dacia during Burebista's reign and conquered Dobruja in 46 AD. Dacia was again united under Decebalus around 85. He resisted the Romans for decades, but the Roman army annihilated his troops in 106. Emperor Trajan transformed Banat, Oltenia and the greater part of Transylvania into the new Roman province of Dacia, but Dacian, Germanic and Sarmatian tribes continued to dominate the lands along the Roman frontiers. The Romans pursued an organised colonisation policy and the provincials enjoyed a long period of peace and prosperity in the 2nd century. Scholars accepting the Daco-Roman continuity theoryone of the main theories about the origin of the Romanianssay that the cohabitation of the native Dacians and the Roman colonists in Roman Dacia was the first phase of the Romanians' ethnogenesis.

The Carpians, Goths and other neighboring tribes made regular raids against Dacia from the 210s. The Romans could not resist and Emperor Aurelian ordered the evacuation of the province Dacia Trajana in 271. Scholars supporting the continuity theory are convinced that most Latin-speaking commoners stayed behind when the army and civil administration was withdrawn. The Romans did not abandon their fortresses along the northern banks of the Lower Danube for decades, and Dobruja (known as Scythia Minor) remained an integral part of the Roman Empire until the early 7th century.

The Goths were expanding towards the Lower Danube from the 230s, forcing the native peoples to flee to the Roman Empire or to accept their suzerainty. The Goths' rule came to an abrupt end when the Huns invaded their territory in 376, causing new waves of migrations. The Huns forced the remnants of the local population into submission, but their empire collapsed in 454. The Gepids took possession of the former Dacia province. The nomadic Avars defeated the Gepids and established a powerful empire around 570. The Bulgars, who also came from the Eurasian steppes, occupied the Lower Danube region in 680. According to scholars who accept the Daco-Roman continuity theory, the Romanians' ancestors, known by the exonym Vlachs in the Middle Ages, lived in densely forested areas, separated from the Goths, Huns, Gepids and Avars during these centuries.

Place names of Slavic origin abound in Romania, indicating that a numerous Slavic-speaking population used to live in the territory. The first Slavic groups settled in Moldavia and Wallachia in the 6th century, in Transylvania around 600. After the Avar Khaganate collapsed in the 790s, Bulgaria became the dominant power of the region, occupying lands as far as the river Tisa. The Council of Preslav declared Old Church Slavonic the language of liturgy in the First Bulgarian Empire in 893. The Romanians also adopted Old Church Slavonic as their liturgical language.

The Magyars (or Hungarians) took control of the steppes north of the Lower Danube in the 830s, but the Bulgarians and the Pechenegs jointly forced them to abandon this region for the lowlands along the Middle Danube around 894. Centuries later, the "Gesta Hungarorum" wrote of the invading Magyars' wars against three dukesGlad, Menumorut and the Vlach Geloufor Banat, Crișana and Transylvania. The "Gesta" also listed many peoplesSlavs, Bulgarians, Vlachs, Khazars and Székelysinhabiting the same regions. The reliability of the "Gesta" is debated, with some scholars regarding it as a basically accurate account, others describing it as a literary work filled with invented details. The lowlands abandoned by the Hungarians to east of the Carpathians were seized by the Pechenegs.

Byzantine missionaries proselytised in the lands east of the Tisa from the 940s and Byzantine troops occupied Dobruja in the 970s. The first king of Hungary, Stephen I, who supported Western European missionaries, defeated the local chieftains and established Roman Catholic bishoprics in Transylvania and Banat in the early 11th century. Significant Pecheneg groups fled to the Byzantine Empire in the 1040s; the Oghuz Turks followed them, and the nomadic Cumans became the dominant power of the steppes in the 1060s. Cooperation between the Cumans and the Vlachs against the Byzantine Empire is well documented from the end of the 11th century. Scholars who reject the Daco-Roman continuity theory say that the first Vlach groups left their Balkan homeland for the mountain pastures of the eastern and southern Carpathians in the 11th century, establishing the Romanians' presence in the lands to the north of the Lower Danube.

Exposed to nomadic incursions, Transylvania developed into an important border province of the Kingdom of Hungary. The Székelysa community of free warriorssettled in central Transylvania around 1100, and moved to the easternmost regions around 1200. Colonists from the Holy Roman Empirethe Transylvanian Saxons' ancestorscame to the province in the 1150s. A high-ranking royal official, styled voivode, ruled the Transylvanian counties from the 1170s, but the Székely and Saxon seats (or districts) were not subject to the voivodes' authority. Royal charters wrote of the "Vlachs' land" in southern Transylvania in the early 13th century, indicating the existence of autonomous Romanian communities. Papal correspondence mentioned the activities of Orthodox prelates among the Romanians in Muntenia in the 1230s.

The Mongols destroyed large territories during their invasion of Eastern and Central Europe in 1241 and 1242. The Mongols' Golden Horde emerged as the dominant power of Eastern Europe, but Béla IV of Hungary's land grant to the Knights Hospitallers in Oltenia and Muntenia shows that the local Vlach rulers were subject to the king's authority in 1247. Basarab I of Wallachia united the Romanian polities between the southern Carpathians and the Lower Danube in the 1310s. He defeated the Hungarian royal army in the Battle of Posada and secured the independence of Wallachia in 1330. The second Romanian principality, Moldavia, achieved full autonomy during the reign of Bogdan I around 1360. A local dynasty ruled the Despotate of Dobruja in the second half of the 14th century, but the Ottoman Empire took possession of the territory after 1388.

Princes Mircea I and Vlad III of Wallachia, and Stephen III of Moldavia defended their countries independence against the Ottomans, but most Wallachian and Moldavian princes paid a regular tribute to the Ottoman sultans from 1417 and 1456, respectively. A military commander of Romanian origin, John Hunyadi, organised the defence of the Kingdom of Hungary until his death in 1456. Increasing taxes outraged the Transylvanian peasants and they rose up in an open rebellion in 1437, but the Hungarian nobles and the heads of the Saxon and Székely communities jointly suppressed their revolt. The formal alliance of the Hungarian, Saxon and Székely leaders, known as the Union of the Three Nations, became an important element of the self-government of Transylvania. The Orthodox Romanian "knezes" (or chiefs) were excluded from the Union.

The Kingdom of Hungary collapsed and the Ottomans occupied parts of Banat and Crișana in 1541. Transylvania and Maramureș, along with the rest of Banat and Crișana developed into a new state under Ottoman suzerainty, the Principality of Transylvania. Reformation spread and four denominationsCalvinism, Lutheranism, Unitarianism and Roman Catholicismwere officially acknowledged in 1568. The Romanians' Orthodox faith remained only tolerated, although they made up more than one-third of the population, according to 17th-century estimations.

The princes of Transylvania, Wallachia and Moldavia joined the Holy League against the Ottoman Empire in 1594. The Wallachian prince, Michael the Brave, united the three principalities under his rule in May 1600. The neighboring powers forced him to abdicate in September, but he became a symbol of the unification of the Romanian lands in the 19th century. Although the rulers of the three principalities continued to pay tribute to the Ottomans, the most talented princesGabriel Bethlen of Transylvania, Matei Basarab of Wallachia, and Vasile Lupu of Moldaviastrengthened their autonomy.

The united armies of the Holy League expelled the Ottoman troops from Central Europe between 1684 and 1699 and the Principality of Transylvania was integrated into the Habsburg Monarchy. The Habsburgs supported the Catholic clergy and persuaded the Orthodox Romanian prelates to accept the union with the Roman Catholic Church in 1699. The Church Union strengthened the Romanian intellectuals' devotion to their Roman heritage. The Orthodox Church was restored in Transylvania only after Orthodox monks stirred up revolts in 1744 and 1759. The organization of the Transylvanian Military Frontier caused further disturbances, especially among the Székelys in 1764.

Princes Dimitrie Cantemir of Moldavia and Constantin Brâncoveanu of Wallachia concluded alliances with the Habsburg Monarchy and Russia against the Ottomans, but they were dethroned in 1711 and 1714, respectively. The sultans lost confidence in the native princes and appointed Orthodox merchants from the Phanar district of Istanbul to rule Moldova and Wallachia. The Phanariot princes pursued oppressive fiscal policies and dissolved the army. The neighboring powers take advantage of the situation: the Habsburg Monarchy annexed northwestern part of Moldavia, or Bucovina, in 1775, and the Russian Empire seized the eastern half of Moldavia, or Bessarabia, in 1812.

A census revealed that the Romanians were more numerous than any of the other ethnic groups in Transylvania in 1733, but legislation continued to use contemptuous adjectives (such as "tolerated" and "admitted") when referring to them. The Uniate bishop, Inocențiu Micu-Klein who demanded the recognition of the Romanians as the fourth privileged nation was forced into exile. Uniate and Orthodox clerics and laymen jointly signed a plea for the Transylvanian Romanians' emancipation in 1791, but the monarch and the local authorities denied to grant their requests.

The Treaty of Küçük Kaynarca authorised the Russian ambassador in Istanbul to defend the autonomy of Moldavia and Wallachia (known as the Danubian Principalities) in 1774. Taking advantage of the Greek War of Independence, a Wallachian lesser nobleman, Tudor Vladimirescu, stirred up a revolt against the Ottomans in January 1821, but he was murdered in June by Phanariot Greeks. After a new Russo-Turkish War, the Treaty of Adrianople strengthened the autonomy of the Danubian Principalities in 1829, although it also acknowledged the sultan's right to confirm the election of the princes.

Mihail Kogălniceanu, Nicolae Bălcescu and other leaders of the 1848 revolutions in Moldavia and Wallachia demanded the emancipation of the peasants and the union of the two principalities, but Russian and Ottoman troops crushed their revolt. The Wallachian revolutionists were the first to adopt the blue, yellow and red tricolour as national flag. In Transylvania, most Romanians supported the imperial government against the Hungarian revolutioners after the Diet passed a law about the union of Transylvania and Hungary. Bishop Andrei Șaguna proposed the unification of the Romanians of the Habsburg Monarchy in a separate duchy, but the central government refused to change the internal frontiers.

The Treaty of Paris put the Danubian Principalities under the collective guardianship of the Great Powers in 1856. After special assemblies convoked in Moldavia and Wallachia urged the unification of the two principalities, the Great Powers did not prevent the election of Alexandru Ioan Cuza as their collective "domnitor" (or ruling prince) in January 1859. The united principalities officially adopted the name Romania on 21 February 1862. Cuza's government carried out a series of reforms, including the secularization of the property of monasteries and agrarian reform, but a coalition of conservative and radical politicians forced him to abdicate in February 1866.

Cuza's successor, a German prince, Karl of Hohenzollern-Sigmaringen (or Carol I), was elected in May. The parliament adopted the first constitution of Romania in the same year. The Great Powers acknowledged Romania's full independence at the Congress of Berlin and Carol I was crowned king in 1881. The Congress also granted the Danube Delta and Dobruja to Romania. Although Romanian scholars strove for the unification of all Romanians into a Greater Romania, the government did not openly support their irredentist projects.

The Transylvanian Romanians and Saxons wanted to maintain the separate status of Transylvania in the Habsburg Monarchy, but the Austro-Hungarian Compromise brought about the union of the province with Hungary in 1867. Ethnic Romanian politicians sharply opposed the Hungarian government's attempts to transform Hungary into a national state, especially the laws prescribing the obligatory teaching of Hungarian. Leaders of the Romanian National Party proposed the federalization of Austria-Hungary and the Romanian intellectuals established cultural association to promote the use of Romanian.

Fearing of Russian expansionism, Romania secretly joined the Triple Alliance of Germany, Austria-Hungary and Italy in 1883, but public opinion remained hostile to Austria-Hungary. Romania seized Southern Dobruja from Bulgaria in the Second Balkan War in 1913. For German and Austrian-Hungarian diplomacy supported Bulgaria during the war, it brought about a rapprochement between Romania and the Triple Entente of France, Russia and the United Kingdom. The country remained neutral when World War I broke out in 1914, but Prime Minister Ion I. C. Brătianu started negotiations with the Entente Powers. After they promised Austrian-Hungarian territories with a majority of ethnic Romanian population to Romania in the Treaty of Bucharest, Romania entered the war against the Central Powers in 1916. The German and Austrian-Hungarian troops defeated the Romanian army and occupied three-quarters of the country by early 1917. After the October Revolution turned Russia from ally into enemy, Romania was forced to sign a harsh peace treaty with the Central Powers in May 1918, but the collapse of Russia also enabled the union of Bessarabia with Romania. King Ferdinand again mobilised the Romanian army on behalf of the Entente Powers a day before Germany capitulated on 11 November 1918.

Austria-Hungary quickly disintegrated after the war. The General Congress of Bukovina proclaimed the union of the province with Romania on 28 November 1918, and the Grand National Assembly decided the union of Transylvania, Banat, Crișana and Maramureș with the kingdom on 1 December. Peace treaties with Austria, Bulgaria and Hungary delineated the new borders in 1919 and 1920, but the Soviet Union did not acknowledge the loss of Bessarabia. Romania achieved its greatest territorial extend, expanding from the pre-war . A new electoral system granted voting rights to all adult male citizens, and a series of radical agrarian reforms transformed the country into a "nation of small landowners" between 1918 and 1921. Gender equality as a principle was enacted, but women could not vote or be candidates. Calypso Botez established the National Council of Romanian Women to promote feminist ideas. Romania was a multiethnic country, with ethnic minorities making up about 30% of the population, but the new constitution declared it a unitary national state in 1923. Although minorities could establish their own schools, Romanian language, history and geography could only be taught in Romanian.

Agriculture remained the principal sector of economy, but several branches of industryespecially the production of coal, oil, metals, synthetic rubber, explosives and cosmeticsdeveloped during the interwar period. With oil production of 5.8 million tons in 1930, Romania ranked sixth in the world. Two parties, the National Liberal Party and the National Peasants' Party, dominated the political life, but the Great Depression brought about significant changes in the 1930s. The democratic parties were squeezed between conflicts with the fascist and Anti-Semitic Iron Guard and the authoritarian tendencies of King Carol II. The King promulgated a new constitution and dissolved the political parties in 1938, replacing the parliamentary system with a royal dictatorship.

The 1938 Munich Agreement convinced King Carol II that France and the United Kingdom could no more defend Romanian interests. German preparations for a new war required the regular supply of Romanian oil and agricultural products. The two countries concluded a treaty about the coordination of their economic policies in 1939, but the King could not persuade Adolf Hitler to guarantee Romania's frontiers. Romania was forced to cede Bessarabia and northern Bukovina to the Soviet Union on 26 June 1940, Northern Transylvania to Hungary on 30 August, and Southern Dobruja to Bulgaria in September. After the territorial losses, the King was forced to abdicate in favour of his minor son, Michael I, on 6 September, and Romania was transformed into a national-legionary state under the leadership of General Ion Antonescu. Antonescu signed the Tripartite Pact of Germany, Italy and Japan on 23 November. The Iron Guard staged a coup against Antonescu, but he crushed the riot with German support and introduced a military dictatorship in early 1941.

Romania entered World War II soon after the German invasion of the Soviet Union in June 1941. The country regained Bessarabia and northern Bucovina, and the Germans placed Transnistria (the territory between the rivers Dniester and Dnieper) under Romanian administration. The Romanian and German troops massacred at least 160,000 local Jews in these territories; more than 105,000 Jews and about 11,000 Gypsies died during their deportation from Bessarabia to Transnistria. The vast majority of the Jewish population of Moldavia, Wallachia, Banat and Southern Transylvania survived, but their fundamental rights were limited. After the German occupation of Hungary in March 1944, about 132,000 (mainly Hungarian-speaking) Jews were deported to extermination camps from Northern Transylvania with the Hungarian authorities' support.

After the Soviet victory in the Battle of Stalingrad in 1943, Iuliu Maniu, a leader of the opposition to Antonescu, entered into secret negotiations with British diplomats who made it clear that Romania had to seek reconciliation with the Soviet Union. To facilitate the coordination of their activities against Antonescu's regime, the National Liberal and National Peasants' parties established the National Democratic Bloc which also included the Social Democratic and Communist parties. After a successful Soviet offensive, the young King Michael I ordered the arrest of Antonescu and appointed politicians from the National Democratic Bloc to form a new government on 23 August 1944. Romania switched sides in the war, and nearly 250,000 Romanian troops joined the Red Army's military campaign against Hungary and Germany, but Joseph Stalin regarded the country as an occupied territory within the Soviet sphere of influence. Stalin's deputy instructed the King to make the Communists' candidate, Petru Groza, the prime minister in March 1945. The Romanian administration in Northern Transylvania was soon restored, and Groza's government carried out an agrarian reform. In February 1947, the Paris Peace Treaties confirmed the return of Northern Transylvania to Romania, but they also legalised the presence of units of the Red Army in the country.

During the Soviet occupation of Romania, the Communist-dominated government called for new elections in 1946, which were fraudulently won, with a fabricated 70% majority of the vote. Thus, they rapidly established themselves as the dominant political force. Gheorghe Gheorghiu-Dej, a Communist party leader imprisoned in 1933, escaped in 1944 to become Romania's first Communist leader. In 1947 he and others forced King Michael I to abdicate and leave the country, and proclaimed Romania a people's republic. Romania remained under the direct military occupation and economic control of the USSR until the late 1950s. During this period, Romania's vast natural resources were continuously drained by mixed Soviet-Romanian companies (SovRoms) set up for unilateral exploitative purposes.

In 1948, the state began to nationalise private firms and to collectivise agriculture. Until the early 1960s, the government severely curtailed political liberties and vigorously suppressed any dissent with the help of the Securitate (the Romanian secret police). During this period the regime launched several campaigns of purges in which numerous "enemies of the state" and "parasite elements" were targeted for different forms of punishment, such as deportation, internal exile, and internment in forced labour camps and prisons, sometimes for life, as well as extrajudicial killing. Nevertheless, anti-Communist resistance was one of the most long-lasting in the Eastern Bloc. A 2006 Commission estimated the number of direct victims of the Communist repression at two million people.

In 1965, Nicolae Ceaușescu came to power and started to conduct the foreign policy more independently from the Soviet Union. Thus, Communist Romania was the only Warsaw Pact country which refused to participate in the Soviet-led 1968 invasion of Czechoslovakia (with Ceaușescu at the time even publicly condemning the action as "a big mistake, [and] a serious danger to peace in Europe and to the fate of Communism in the world"); it was also the only Communist state to maintain diplomatic relations with Israel after 1967's Six-Day War; and established diplomatic relations with West Germany the same year. At the same time, close ties with the Arab countries (and the PLO) allowed Romania to play a key role in the Israel–Egypt and Israel–PLO peace talks.

As Romania's foreign debt sharply increased between 1977 and 1981 (from US$3 billion to $10 billion), the influence of international financial organizations (such as the IMF and the World Bank) grew, gradually conflicting with Ceaușescu's autocratic rule. The latter eventually initiated a policy of total reimbursement of the foreign debt by imposing austerity steps that impoverished the population and exhausted the economy. The process succeeded in repaying all foreign government debt of Romania in 1989. At the same time, Ceaușescu greatly extended the authority of the Securitate secret police and imposed a severe cult of personality, which led to a dramatic decrease in the dictator's popularity and culminated in his overthrow and eventual execution, together with his wife, in the violent Romanian Revolution of December 1989 in which thousands were killed or injured. The charges for which they were executed were, among others, genocide by starvation.

After the 1989 revolution, the National Salvation Front (NSF), led by Ion Iliescu, took partial multi-party democratic and free market measures. In April 1990, a sit-in protest contesting the results of that year's legislative elections and accusing the NSF, including Iliescu, of being made up of former Communists and members of the Securitate — rapidly grew to become what was called the Golaniad. The peaceful demonstrations degenerated into violence, prompting the intervention of coal miners summoned by Iliescu. This episode has been documented widely by both local and foreign media, and is remembered as the June 1990 Mineriad.

The subsequent disintegration of the Front produced several political parties, including most notably the Social Democratic Party and the Democratic Party. The former governed Romania from 1990 until 1996 through several coalitions and governments with Ion Iliescu as head of state. Since then, there have been several other democratic changes of government: in 1996 Emil Constantinescu was elected president, in 2000 Iliescu returned to power, while Traian Băsescu was elected in 2004 and narrowly re-elected in 2009.

In November 2014, Sibiu () mayor Klaus Johannis was elected president, unexpectedly defeating former Prime Minister Victor Ponta, who had been in the lead in the opinion polls. This surprise victory is attributed by many to the Romanian diaspora, of which almost 50 percent voted for Iohannis in the first tour, compared to 16 percent for Ponta.

The post-1989 period is also characterised by the fact that most of the former industrial and economic enterprises which were built and operated during the Communist period have been closed, mainly as a result of the policies of privatization of the post-1989 regimes.

Corruption has also been a major issue in contemporary Romanian politics. In November 2015, massive anti-corruption protests which developed in the wake of the Colectiv nightclub fire led to the resignation of Romania's Prime Minister Victor Ponta. During 2017–2018, in response to measures which were perceived to weaken the fight against corruption, some of the biggest protests since 1989 took place in Romania, with over 500,000 people protesting across the country.

Nevertheless, in recent years, many efforts have been made to tackle corruption. A National Anticorruption Directorate was formed in the country in 2002, and according to Transparency International's annual Corruption Perceptions Index, Romania has a corruption index similar to Croatia and neighbouring Hungary.

After the end of the Cold War, Romania developed closer ties with Western Europe and the United States, eventually joining NATO in 2004, and hosting the 2008 summit in Bucharest.

The country applied in June 1993 for membership in the European Union and became an Associated State of the EU in 1995, an Acceding Country in 2004, and a full member on 1 January 2007.

During the 2000s, Romania enjoyed one of the highest economic growth rates in Europe and has been referred at times as "the Tiger of Eastern Europe". This has been accompanied by a significant improvement in living standards as the country successfully reduced internal poverty and established a functional democratic state. However, Romania's development suffered a major setback during the late-2000s recession leading to a large gross domestic product contraction and budget deficit in 2009. This led to Romania borrowing from the International Monetary Fund. The worsening economic conditions led to unrest and triggered a political crisis in 2012.

Romania still faces problems related to infrastructure, medical services, education, and corruption. Near the end of 2013, The Economist reported Romania again enjoying 'booming' economic growth at 4.1% that year, with wages rising fast and a lower unemployment than in Britain. Economic growth accelerated in the midst of government liberalisations in opening up new sectors to competition and investment—most notably, energy and telecoms. In 2016 the Human Development Index ranked Romania as a nation of "Very High Human Development".

Following the experience of economic instability throughout the 1990s, and the implementation of a free travel agreement with the EU, a great number of Romanians emigrated to Western Europe and North America, with particularly large communities in Italy, Germany and Spain. In 2016, the Romanian diaspora was estimated to be at over 3.6 million people, the fifth-highest emigrant population in the world.

With an area of , Romania is the largest country in Southeastern Europe and the twelfth-largest in Europe. It lies between latitudes 43° and 49° N and longitudes 20° and 30° E.

The terrain is distributed roughly equally between mountains, hills, and plains.

The Carpathian Mountains dominate the centre of Romania, with 14 mountain ranges reaching above , the highest of which is Moldoveanu Peak at . They are surrounded by the Moldavian and Transylvanian plateaus and Carpathian Basin and Wallachian plains.

About 47% of the country's land area is covered with natural and semi-natural ecosystems. There are almost (about 5% of the total area) of protected areas in Romania covering 13 national parks and three biosphere reserves.

The Danube river forms a large part of the border with Serbia and Bulgaria, and flows into the Black Sea, forming the Danube Delta, which is the second-largest and best-preserved delta in Europe, and also a biosphere reserve and a biodiversity World Heritage Site. At , the Danube Delta is the largest continuous marshland in Europe, and supports 1,688 different plant species alone.

Romania has one of the largest areas of undisturbed forest in Europe, covering almost 27% of the territory. Some 3,700 plant species have been identified in the country, from which to date 23 have been declared natural monuments, 74 missing, 39 endangered, 171 vulnerable, and 1,253 rare.

The fauna of Romania consists of 33,792 species of animals, 33,085 invertebrate and 707 vertebrate, with almost 400 unique species of mammals, birds, reptiles, and amphibians, including about 50% of Europe's (excluding Russia) brown bears and 20% of its wolves.

Owing to its distance from open sea and position on the southeastern portion of the European continent, Romania has a climate that is temperate and continental, with four distinct seasons. The average annual temperature is in the south and in the north. In summer, average maximum temperatures in Bucharest rise to , and temperatures over are fairly common in the lower-lying areas of the country. In winter, the average maximum temperature is below . Precipitation is average, with over per year only on the highest western mountains, while around Bucharest it drops to approximately .
There are some regional differences: in western sections, such as Banat, the climate is milder and has some Mediterranean influences; the eastern part of the country has a more pronounced continental climate. In Dobruja, the Black Sea also exerts an influence over the region's climate.

The Constitution of Romania is based on the Constitution of France's Fifth Republic and was approved in a national referendum on 8 December 1991, and amended in October 2003 to bring it into conformity with the EU legislation. The country is governed on the basis of a multi-party democratic system and the separation of powers between the legislative, executive and judicial branches. It is a semi-presidential republic where executive functions are held by both the government and the president. The latter is elected by popular vote for a maximum of two terms of five years and appoints the prime minister, who in turn appoints the Council of Ministers. The legislative branch of the government, collectively known as the Parliament (residing at the Palace of the Parliament), consists of two chambers (Senate and Chamber of Deputies) whose members are elected every four years by simple plurality.

The justice system is independent of the other branches of government, and is made up of a hierarchical system of courts culminating in the High Court of Cassation and Justice, which is the supreme court of Romania. There are also courts of appeal, county courts and local courts. The Romanian judicial system is strongly influenced by the French model, considering that it is based on civil law and is inquisitorial in nature. The Constitutional Court ("Curtea Constituțională") is responsible for judging the compliance of laws and other state regulations to the constitution, which is the fundamental law of the country and can only be amended through a public referendum. The 2007 entry into the EU has been a significant influence on its domestic policy, and including judicial reforms, increased judicial cooperation with other member states, and measures to combat corruption.

Since December 1989, Romania has pursued a policy of strengthening relations with the West in general, more specifically with the United States and the European Union, albeit with limited relations involving the Russian Federation. It joined the North Atlantic Treaty Organization (NATO) on 29 March 2004, the European Union (EU) on 1 January 2007, while it had joined the International Monetary Fund and the World Bank in 1972, and is a founding member of the World Trade Organization.

Past recent governments states that one of their goals is to strengthen ties with and helping other countries (in particular Moldova, Ukraine, and Georgia) with the process of integration with the rest of the West. Romania has also made clear since the late 1990s that it supports NATO and EU membership for the democratic former Soviet republics in Eastern Europe and the Caucasus. Romania also declared its public support for Turkey, and Croatia joining the European Union. Because it has a large Hungarian minority, Romania has also developed strong relations with Hungary.

Romania opted on 1 January 2007, to accede to the Schengen Area, and its bid to join was approved by the European Parliament in June 2011, but was rejected by the EU Council in September 2011. As of August 2019, its acceptance into the Schengen Area is hampered because the European Council has misgivings about Romania's adherence to the rule of law, a fundamental principle of EU membership.

In December 2005, President Traian Băsescu and United States Secretary of State Condoleezza Rice signed an agreement that would allow a U.S. military presence at several Romanian facilities primarily in the eastern part of the country. In May 2009, Hillary Clinton, US Secretary of State, declared that "Romania is one of the most trustworthy and respectable partners of the USA."

Relations with Moldova are a special case, considering that the two countries share the same language and a common history. A movement for unification of Romania and Moldova appeared in the early 1990s after both countries achieved emancipation from communist rule, but lost ground in the mid-1990s when a new Moldovan government pursued an agenda towards preserving a Moldovan republic independent of Romania. After the 2009 protests in Moldova and subsequent removal of Communists from power, relations between the two countries have improved considerably.

The Romanian Armed Forces consist of Land, Air, and Naval Forces, and are led by a Commander-in-chief under the supervision of the Ministry of National Defence, and by the president as the Supreme Commander during wartime. The Armed Forces consist of approximately 15,000 civilians and 75,000 are military personnel—45,800 for land, 13,250 for air, 6,800 for naval forces, and 8,800 in other fields. The total defence spending in 2007 accounted for 2.05% of total national GDP, or approximately US$2.9 billion, with a total of $11 billion spent between 2006 and 2011 for modernization and acquisition of new equipment.

The Air Force currently operates modernised Soviet MiG-21 Lancer fighters which are due to be replaced by twelve F-16s, recently purchased. The Air Force purchased seven new C-27J Spartan tactical airlifters, while the Naval Forces acquired two modernised Type 22 frigates from the British Royal Navy.

Romania has contributed troops to the international coalition in Afghanistan since 2002, with a peak deployment of 1,600 troops in 2010 (which was the 4th largest contributor according to the US). Its combat mission in the country concluded in 2014. Romanian troops participated in the occupation of Iraq, reaching a peak of 730 soldiers before being slowly drawn down to 350 soldiers. Romania terminated its mission in Iraq and withdrew its last troops on 24 July 2009, among the last countries to do so. The Regele Ferdinand frigate participated in the 2011 military intervention in Libya.

In December 2011, the Romanian Senate unanimously adopted the draft law ratifying the Romania-United States agreement signed in September of the same year that would allow the establishment and operation of a US land-based ballistic missile defence system in Romania as part of NATO's efforts to build a continental missile shield.

Romania is divided into 41 counties (județe, pronounced judetse) and the municipality of Bucharest. Each county is administered by a county council, responsible for local affairs, as well as a prefect responsible for the administration of national affairs at the county level. The prefect is appointed by the central government but cannot be a member of any political party. Each county is further subdivided into cities and communes, which have their own mayor and local council. There are a total of 320 cities and 2,861 communes in Romania. A total of 103 of the larger cities have municipality statuses, which gives them greater administrative power over local affairs. The municipality of Bucharest is a special case as it enjoys a status on par to that of a county. It is further divided into six sectors and has a prefect, a general mayor (primar), and a general city council.

The NUTS-3 (Nomenclature of Territorial Units for Statistics) level divisions of European Union reflect Romania's administrative-territorial structure, and correspond to the 41 counties plus Bucharest. The cities and communes correspond to the NUTS-5 level divisions, but there are no current NUTS-4 level divisions. The NUTS-1 (four macroregions) and NUTS-2 (eight development regions) divisions exist but have no administrative capacity, and are instead used for coordinating regional development projects and statistical purposes.

In 2019, Romania has a GDP (PPP) of around $547 billion and a GDP per capita (PPP) of $28,189. According to the World Bank, Romania is an upper-middle income country with a mixed economy. According to Eurostat, Romania's GDP per capita (PPS) was at 65% of the EU average in 2018, an increase from 41% in 2007 (the year of Romania's accession to the EU), making Romania one of the fastest growing economies in the EU.

After 1989 the country experienced a decade of economic instability and decline, led in part by an obsolete industrial base and a lack of structural reform. From 2000 onward, however, the Romanian economy was transformed into one of relative macroeconomic stability, characterised by high growth, low unemployment and declining inflation. In 2006, according to the Romanian Statistics Office, GDP growth in real terms was recorded at 7.7%, one of the highest rates in Europe. However, a recession following the global financial crisis of 2008–2009 forced the government to borrow externally, including an IMF €20bn bailout program. GDP has been growing by over 2% each year since. According to The World Bank, GDP per capita in purchasing power parity grew from $13,687 in 2007 to $28,206 in 2018. Romania still has one of the lowest net average monthly wages in the EU of €540 in 2016, and an inflation rate of −1.1% in 2016. Unemployment in Romania is at 4.3% in August 2018, which is very low compared to other EU countries.

Industrial output growth reached 6.5% year-on-year in February 2013, the highest in the EU-27. The largest local companies include car maker Automobile Dacia, Petrom, Rompetrol, Ford Romania, Electrica, Romgaz, RCS & RDS and Banca Transilvania. Exports have increased substantially in the past few years, with a 13% annual rise in exports in 2010. Romania's main exports are cars, software, clothing and textiles, industrial machinery, electrical and electronic equipment, metallurgic products, raw materials, military equipment, pharmaceuticals, fine chemicals, and agricultural products (fruits, vegetables, and flowers). Trade is mostly centred on the member states of the European Union, with Germany and Italy being the country's single largest trading partners. The account balance in 2012 was estimated to be −4.52% of the GDP.

After a series of privatizations and reforms in the late 1990s and 2000s, government intervention in the Romanian economy is somewhat lower than in other European economies. In 2005, the government replaced Romania's progressive tax system with a flat tax of 16% for both personal income and corporate profit, among the lowest rates in the European Union. The economy is predominantly based on services, which account for 56.2% of the country's total GDP as of 2017, with industry and agriculture accounting for 30% and 4.4% respectively.
Approximately 25.8% of the Romanian workforce is employed in agriculture, one of the highest rates in Europe.

Romania has attracted increasing amounts of foreign investment following the end of Communism, with the stock of foreign direct investment (FDI) in Romania rising to €83.8 billion in June 2019. Romania's FDI outward stock amounted to $745 million in December 2018, the lowest value among the 28 EU member states.

According to a 2019 World Bank report, Romania ranks 52nd out of 190 economies in the ease of doing business, one place higher than neighbouring Hungary and one place lower than Italy. The report praised the consistent enforcement of contracts and access to credit in the country, while noting difficulties in access to electricity and dealing with construction permits.

Since 1867 the official currency has been the Romanian "leu" ("lion") and following a denomination in 2005, it has been valued at €0.2–0.3. After joining the EU in 2007, Romania is expected to adopt the Euro sometime around 2020.

On 1 July 2015, Romania's external debt was reported to be situated at the sum of €90.59 billion. In 2018, the external debt of Romania was reported to be situated at the sum of €96 billion according to the National Bank of Romania.

According to the "INSSE", Romania's total road network was estimated in 2015 at . The World Bank estimates the railway network at of track, the fourth-largest railroad network in Europe. Rail transport experienced a dramatic decline after 1989, and was estimated at 99 million passenger journeys in 2004; but has experienced a recent (2013) revival due to infrastructure improvements and partial privatization of lines, accounting for 45% of all passenger and freight movements in the country. Bucharest Metro, the only underground railway system, was opened in 1979 and measures with an average ridership in 2007 of 600,000 passengers during the workweek. There are sixteen international commercial airports in service today. Over 12.8 million passengers flew through Bucharest's Henri Coandă International Airport in 2017.

Romania is a net exporter of electrical energy and is 48th worldwide in terms of consumption of electric energy. Around a third of the produced energy comes from renewable sources, mostly as hydroelectric power. In 2015, the main sources were coal (28%), hydroelectric (30%), nuclear (18%), and hydrocarbons (14%). It has one of the largest refining capacities in Eastern Europe, even though oil and natural gas production has been decreasing for more than a decade. With one of the largest reserves of crude oil and shale gas in Europe, it is among the most energy-independent countries in the European Union, and is looking to further expand its nuclear power plant at Cernavodă.

There were almost 18.3 million connections to the Internet in June, 2014. According to Bloomberg, in 2013 Romania ranked 5th in the world, and according to "The Independent", it ranks number one in Europe at Internet speeds, with Timișoara ranked among the highest in the world.

Tourism is a significant contributor to the Romanian economy, generating around 5% of GDP. According to the World Travel and Tourism Council, Romania was estimated to have the fourth-fastest-growing travel and tourism total demand in the world, with an estimated potential growth of 8% per year from 2007 to 2016. The number of tourists has been steadily rising, reaching 9.33 million foreign tourists in 2016, according to the Worldbank. Tourism in Romania attracted €400 million in investments in 2005.

More than 60% of the foreign visitors in 2007 were from other EU countries. The popular summer attractions of Mamaia and other Black Sea Resorts attracted 1.3 million tourists in 2009.

Most popular skiing resorts are along the Valea Prahovei and in Poiana Brașov. Castles, fortifications, or strongholds as well as well preserved medieval Transylvanian cities or towns such as Cluj-Napoca, Sibiu, Brașov, Bistrița, Mediaș, Cisnădie, or Sighișoara also attract a large number of tourists. Bran Castle, near Brașov, is one of the most famous attractions in Romania, drawing hundreds of thousands of tourists every year as it is often advertised as being Dracula's Castle. Hunedoara Castle is another famous structure.

Rural tourism, focusing on folklore and traditions, has become an important alternative, and is targeted to promote such sites as Bran and its Dracula's Castle, the painted churches of northern Moldavia, and the wooden churches of Maramureș, or the villages with fortified churches in Transylvania. Other attractions include the Danube Delta or the Sculptural Ensemble of Constantin Brâncuși at Târgu Jiu.

In 2014, Romania had 32,500 companies which were active in the hotel and restaurant industry, with a total turnover of EUR 2.6 billion. More than 1.9 million foreign tourists visited Romania in 2014, 12% more than in 2013. According to the country's National Statistics Institute, some 77% came from Europe (particularly from Germany, Italy, and France), 12% from Asia, and less than 7% from North America.

Historically, Romanian researchers and inventors have made notable contributions to several fields. In the history of flight, Traian Vuia made the first airplane to take off under its own power and Aurel Vlaicu built and flew some of the earliest successful aircraft, while Henri Coandă discovered the Coandă effect of fluidics. Victor Babeș discovered more than 50 types of bacteria; biologist Nicolae Paulescu discovered insulin, while Emil Palade, received the Nobel Prize for his contributions to cell biology. Lazăr Edeleanu was the first chemist to synthesise amphetamine and he also invented the procedure of separating valuable petroleum components with selective solvents, while Costin Nenițescu developed numerous new classes of compounds in organic chemistry. Notable include Spiru Haret, Grigore Moisil, and Ștefan Odobleja; physicists and inventors: Șerban Țițeica, Alexandru Proca, and Ștefan Procopiu.

During the 1990s and 2000s, the development of research was hampered by several factors, including corruption, low funding and a considerable brain drain. In recent years, Romania has ranked the lowest or second-lowest in the European Union by research and development spending as a percentage of GDP, standing at roughly 0.5% in 2016 and 2017, substantially below the EU average of just over 2%. The country joined the European Space Agency (ESA) in 2011, and CERN in 2016. In 2018, however, Romania lost its voting rights in the ESA due to a failure to pay 56.8 million EUR in membership contributions to the agency.

In the early 2010s, situation for science in Romania was characterised as "rapidly improving" albeit from a low base. In January 2011, the Parliament also passed a law that enforces "strict quality control on universities and introduces tough rules for funding evaluation and peer review".

The nuclear physics facility of the European Union's proposed Extreme Light Infrastructure (ELI) laser will be built in Romania. In early 2012, Romania launched its first satellite from the Centre Spatial Guyanais in French Guyana. Starting December 2014, Romania is a co-owner of the International Space Station.

According to the 2011 census, Romania's population is 20,121,641. Like other countries in the region, its population is expected to gradually decline in the coming years as a result of sub-replacement fertility rates and negative net migration rate. In October 2011, Romanians made up 88.9% of the population. The largest ethnic minorities are the Hungarians, 6.1% of the population, and the Roma, 3.0% of the population. The Roma minority is usually underestimated in census data and may represent up to 10% of the population. Hungarians constitute a majority in the counties of Harghita and Covasna. Other minorities include Ukrainians, Germans, Turks, Lipovans, Aromanians, Tatars, and Serbs. In 1930, there were 745,421 Germans in Romania, but only about 36,000 remain today. , there were also approximately 133,000 immigrants living in Romania, primarily from Moldova and China.

The total fertility rate (TFR) in 2018 was estimated at 1.36 children born per woman, which is below the replacement rate of 2.1, and one of the lowest in the world, it remains considerably below the high of 5.82 children born per woman in 1912. In 2014, 31.2% of births were to unmarried women.
The birth rate (9.49‰, 2012) is much lower than the mortality rate (11.84‰, 2012), resulting in a shrinking (−0.26% per year, 2012) and aging population (median age: 41.6 years, 2018), one of the oldest populations in the world, with approximately 16.8% of total population aged 65 years and over. The life expectancy in 2015 was estimated at 74.92 years (71.46 years male, 78.59 years female).

The number of Romanians and individuals with ancestors born in Romania living abroad is estimated at around 12 million. After the Romanian Revolution of 1989, a significant number of Romanians emigrated to other European countries, North America or Australia. For example, in 1990, 96,919 Romanians permanently settled abroad.

The official language is Romanian, a Romance language (the most widely spoken of the Eastern Romance branch), which presents a consistent degree of similarity to Aromanian, Megleno-Romanian, and Istro-Romanian, but equally shares many features with the rest of the Western Romance languages, specifically Italian, French, Spanish, Portuguese, and Catalan. The Romanian alphabet contains the same 26 letters of the standard Latin alphabet, as well as five additional ones (namely 'ă','â','î','ț', and 'ș'), totaling 31.

Romanian is spoken as a first language by approximately 90% of the entire population, while Hungarian and Vlax Romani are spoken by 6.2% and 1.2% of the population, respectively. There are also approximately 50,000 native speakers of Ukrainian (concentrated in some compact regions, near the border, where they form local majorities), 25,000 native speakers of German, and 32,000 native speakers of Turkish living in Romania.

According to the Constitution, local councils ensure linguistic rights to all minorities, with localities with ethnic minorities of over 20%, that minority's language can be used in the public administration, justice system, and education. Foreign citizens and stateless persons that live in Romania have access to justice and education in their own language. English and French are the main foreign languages taught in schools. In 2010, the Organisation internationale de la Francophonie identifies French speakers in the country. According to the 2012 Eurobarometer, English is spoken by 31% of Romanians, French is spoken by 17%, as well as Italian and German, each by 7%.

Romania is a secular state and has no state religion. An overwhelming majority of the population identify themselves as Christians. At the country's 2011 census, 81.0% of respondents identified as Orthodox Christians belonging to the Romanian Orthodox Church. Other denominations include Protestantism (6.2%), Roman Catholicism (4.3%), and Greek Catholicism (0.8%). From the remaining population, 195,569 people belong to other Christian denominations or have another religion, which includes 64,337 Muslims (mostly of Turkish and Tatar ethnicity) and 3,519 Jewish (Jews once constituted 4% of the Romanian population, 728,115 persons in the 1930 census). Moreover, 39,660 people have no religion or are atheist, whilst the religion of the rest is unknown.

The Romanian Orthodox Church is an autocephalous Eastern Orthodox Church in full communion with other Orthodox churches, with a Patriarch as its leader. It is the second-largest Orthodox Church in the world, and unlike other Orthodox churches, it functions within a Latin culture and utilises a Romance liturgical language. Its canonical jurisdiction covers the territories of Romania and Moldova, with dioceses for Romanians living in nearby Serbia and Hungary, as well as diaspora communities in Central and Western Europe, North America and Oceania.

Although 54.0% of the population lived in urban areas in 2011, this percentage has been declining since 1996. Counties with over ⅔ urban population are Hunedoara, Brașov and Constanța, while with less than a third are Dâmbovița (30.06%) and Giurgiu and Teleorman. Bucharest is the capital and the largest city in Romania, with a population of over 1.8 million in 2011. Its larger urban zone has a population of almost 2.2 million, which are planned to be included into a metropolitan area up to 20 times the area of the city proper. Another 19 cities have a population of over 100,000, with Cluj-Napoca and Timișoara of slightly more than 300,000 inhabitants, Iași, Constanța, Craiova, and Brașov with over 250,000 inhabitants, and Galați and Ploiești with over 200,000 inhabitants. Metropolitan areas have been constituted for most of these cities.

Since the Romanian Revolution of 1989, the Romanian educational system has been in a continuous process of reform that has received mixed criticism. In 2004, some 4.4 million of the population were enrolled in school. Out of these, 650,000 in kindergarten (3–6 years), 3.11 million in primary and secondary level, and 650,000 in tertiary level (universities). In 2018, the adult literacy rate was 98.8%. Kindergarten is optional between 3 and 6 years. Since 2012, compulsory schooling starts at age 6 with the "preparatory school year" ("clasa pregătitoare") and is compulsory until tenth grade. Primary and secondary education is divided into 12 or 13 grades. There also exists a semi-legal, informal private tutoring system used mostly during secondary school, which has prospered during the Communist regime.

Alexandru Ioan Cuza University of Iași, Babeș-Bolyai University of Cluj-Napoca, University of Bucharest, and West University of Timișoara have been included in the QS World University Rankings' top 800.

Romania ranks 5th in the all-time medal count at the International Mathematical Olympiad with 316 total medals, dating back to 1959. Ciprian Manolescu managed to write a perfect paper (42 points) for gold medal more times than anybody else in the history of the competition, doing it all three times he participated in the IMO (1995, 1996, 1997). Romania has achieved the highest team score in the competition, after China and Russia, and right after the United States and Hungary. Romania also ranks 6th in the all-time medal count at the International Olympiad in Informatics with 107 total medals, dating back to 1989.

Romania has a universal health care system, and total health expenditures by the government are roughly 5% of the GDP. It covers medical examinations, any surgical interventions, and any post-operator medical care, and provides free or subsidised medicine for a range of diseases. The state is obliged to fund public hospitals and clinics. The most common causes of death are cardiovascular diseases and cancer. Transmissible diseases are quite common by European standards. In 2010, Romania had 428 state and 25 private hospitals, with 6.2 hospital beds per 1,000 people, and over 200,000 medical staff, including over 52,000 doctors. , the emigration rate of doctors was 9%, higher than the European average of 2.5%.

The topic of the origin of the Romanians began to be discussed by the end of the 18th century among the Transylvanian School scholars.
Several writers rose to prominence in the 19th century, including George Coșbuc, Ioan Slavici, Mihail Kogălniceanu, Vasile Alecsandri, Nicolae Bălcescu, Ion Luca Caragiale, Ion Creangă, and Mihai Eminescu, the later being considered the greatest and most influential Romanian poet, particularly for the poem "Luceafărul".

In the 20th century, Romanian artists reached international acclaim, including Tristan Tzara, Marcel Janco, Mircea Eliade, Nicolae Grigorescu, Marin Preda, Liviu Rebreanu, Eugène Ionesco, Emil Cioran, and Constantin Brâncuși. The last has a sculptural ensemble in Târgu Jiu, while his sculpture "Bird in Space", was auctioned in 2005 for $27.5 million. Romanian-born Holocaust survivor Elie Wiesel received the Nobel Peace Prize in 1986, while Banat Swabian writer Herta Müller received the Nobel Prize in Literature in 2009.

Prominent Romanian painters include Nicolae Grigorescu, Ștefan Luchian, Ion Andreescu Nicolae Tonitza and Theodor Aman. Notable Romanian classical composers of the 19th and 20th centuries include Ciprian Porumbescu, Anton Pann, Eduard Caudella, Mihail Jora, Dinu Lipatti and especially George Enescu. The annual George Enescu Festival is held in Bucharest in honor of the 20th-century eponymous composer.

Contemporary musicians like Angela Gheorghiu, Gheorghe Zamfir, Inna, Alexandra Stan and many others have achieved various levels of international acclaim. At the Eurovision Song Contest Romanian singers have achieved third place in 2005 and 2010.

In cinema, several movies of the Romanian New Wave have achieved international acclaim. At the Cannes Film Festival, "The Death of Mr. Lazarescu" by Cristi Puiu won the "Prix Un Certain Regard" in 2005, while "4 Months, 3 Weeks and 2 Days" by Cristian Mungiu won the festival's top prize, the "Palme d'Or", in 2007. At the Berlin International Film Festival, "Child's Pose" by Călin Peter Netzer won the Golden Bear in 2013.

The list of World Heritage Sites includes six cultural sites located within Romania, including eight Painted churches of northern Moldavia, eight Wooden Churches of Maramureș, seven Villages with fortified churches in Transylvania, the Horezu Monastery, and the Historic Centre of Sighișoara.

There are 12 non-working public holidays, including the Great Union Day, celebrated on 1 December in commemoration of the 1918 union of Transylvania with Romania. Winter holidays include the Christmas festivities and the New Year during which, various unique folklore dances and games are common: "plugușorul", "sorcova", "ursul", and "capra". The traditional Romanian dress that otherwise has largely fallen out of use during the 20th century, is a popular ceremonial vestment worn on these festivities, especially in the rural areas. Sacrifices of live pigs during Christmas and lambs during Easter has required a special derogation from EU law after 2007. During Easter, painted eggs are very common, while on 1 March features "mărțișor" gifting, a tradition likely of Thracian origin.

Romanian cuisine has been influenced by Austrian and German cuisine (especially in the historical regions that had been formerly administered by the Habsburg Monarchy), but also shares some similarities with other cuisines in the Balkan region such as the Greek, Bulgarian, or Serbian cuisine. "Ciorbă" includes a wide range of sour soups, while "mititei", "mămăligă" (similar to polenta), and "sarmale" are featured commonly in main courses.

Pork, chicken, and beef are the preferred types of meat, but lamb and fish are also quite popular. Certain traditional recipes are made in direct connection with the holidays: "chiftele", "tobă" and "tochitura" at Christmas; "drob", "pască" and "cozonac" at Easter and other Romanian holidays. "Țuică" is a strong plum brandy reaching a 70% alcohol content which is the country's traditional alcoholic beverage, taking as much as 75% of the national crop (Romania is one of the largest plum producers in the world). Traditional alcoholic beverages also include wine, "rachiu", "palincă" and "vișinată", but beer consumption has increased dramatically over the recent years.

Football is the most popular sport in Romania with over 219,000 registered players . The market for professional football in Romania is roughly €740 million according to UEFA.

The governing body is the Romanian Football Federation, which belongs to UEFA. The Romania national football team played its first match in 1922 and is one of only four national teams to have taken part in the first three FIFA World Cups, the other three being Brazil, France, and Belgium. Overall, it has played in seven World Cups and had its most successful period during the 1990s, when it reached the quarterfinals of the 1994 FIFA World Cup, being eventually ranked third by FIFA in 1997.

The core player of this golden generation was Gheorghe Hagi, who was nicknamed "Maradona of the Carpathians." Other successful players include the European Golden Shoe winners Dudu Georgescu, Dorin Mateuț and Rodion Cămătaru, Nicolae Dobrin, Ilie Balaci, Florea Dumitrache, Mihai Mocanu, Michael Klein, Mircea Rednic, Cornel Dinu, Mircea Lucescu, Costică Ștefănescu, Liță Dumitru, Lajos Sătmăreanu, Ștefan Sameș, Ladislau Bölöni, Anghel Iordănescu, Miodrag Belodedici, Helmuth Duckadam, Marius Lăcătuș, Victor Pițurcă and many others, and most recently Gheorghe Popescu, Florin Răducioiu, Dorinel Munteanu, Dan Petrescu, Adrian Mutu, Cristian Chivu, or Cosmin Contra. The Romanian national team also reached the quarterfinals of the UEFA European Championship three times. Romania's home ground is the Arena Națională in Bucharest.

The most successful club is Steaua București, who were the first Eastern European team to win the Champions League in 1986, and were runners-up in 1989. They were also Europa League semi-finalists in 2006. Dinamo București reached the Champions League semi-final in 1984 and the Cup Winners' Cup semi-final in 1990. Other important Romanian football clubs are Rapid București, UTA Arad, Universitatea Craiova, Petrolul Ploiești, CFR Cluj, Astra Giurgiu, and Viitorul Constanța.

Tennis is the second most popular sport. Romania reached the Davis Cup finals three times (1969, 1971, 1972). In singles, Ilie Năstase was the first year-end World No. 1 in the ATP Rankings in 1973, winning several Grand Slam titles. Also Virginia Ruzici won the French Open in 1978, and was runner-up in 1980, Simona Halep won the French Open in 2018 and Wimbledon in 2019 after losing her first three Grand Slam finals. She has ended 2017 and 2018 as WTA's World No. 1. And Horia Tecău won in doubles three Grand Slams and the ATP Finals final. He was World No. 2 in 2015.

Other popular team sport is handball. Both the men's and women's handball national teams are multiple world champions. Cristina Neagu has a record four IHF World Player of the Year awards.

Popular individual sports include combat sports, martial arts and swimming. In professional boxing, Romania has produced many world champions across the weight divisions internationally recognised by the governing bodies. World champions include Lucian Bute, Leonard Dorin Doroftei, Adrian Diaconu and Michael Loewe. Another popular combat sport is professional kickboxing, and produced prominent names, including Daniel Ghiță, and Benjamin Adegbuyi.

Romania participated in the Olympic Games for the first time in 1900 and has taken part in 21 of the 28 summer games. It has been one of the more successful countries at the Summer Olympic Games, with a total of 307 medals won throughout the years, of which 89 gold ones, ranking 15th overall, and second of the nations that have never hosted the game. It participated at the 1984 Summer Olympics in Los Angeles and finished second in gold medals (20) and third in total medal count (53).

Almost a quarter of all the medals and 25 of the gold ones were won in gymnastics, with Olympic and sport icon Nadia Comăneci becoming the first gymnast ever to score a perfect ten in an Olympic event at the 1976 Summer Olympics. 
Romanian competitors have won gold medals in other Olympic sports: rowing, athletics, canoeing, wrestling, shooting, fencing, swimming, weightlifting, boxing, and judo.






</doc>
