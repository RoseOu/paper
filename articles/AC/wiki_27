<doc id="26976" url="https://en.wikipedia.org/wiki?curid=26976" title="Silicon Valley">
Silicon Valley

Silicon Valley is a region in the southern part of the San Francisco Bay Area in Northern California that serves as a global center for high technology, innovation, and social media. It corresponds roughly to the geographical Santa Clara Valley, although its boundaries have increased in recent decades. San Jose is the Valley's largest city, the third-largest in California, and the tenth-largest in the United States. Other major Silicon Valley cities include Palo Alto, Menlo Park, Redwood City, Cupertino, Santa Clara, Mountain View, and Sunnyvale. The San Jose Metropolitan Area has the third-highest GDP per capita in the world (after Zurich, Switzerland and Oslo, Norway), according to the Brookings Institution.

The word "silicon" in the name originally referred to the large number of innovators and manufacturers in the region specializing in silicon-based MOS transistors and integrated circuit chips. The area is now home to many of the world's largest high-tech corporations, including the headquarters of more than 30 businesses in the Fortune 1000, and thousands of startup companies. Silicon Valley also accounts for one-third of all of the venture capital investment in the United States, which has helped it to become a leading hub and startup ecosystem for high-tech innovation and scientific development. It was in Silicon Valley that the silicon-based integrated circuit, the microprocessor, and the microcomputer, among other technologies, were developed. As of 2013, the region employed about a quarter of a million information technology workers.

As more high-tech companies were established across San Jose and the Santa Clara Valley, and then north towards the Bay Area's two other major cities, San Francisco and Oakland, the term "Silicon Valley" has come to have two definitions: a geographic one, referring to Santa Clara County, and a metonymical one, referring to all high-tech businesses in the Bay Area. The term is often used as a synecdoche for the American high-technology economic sector. The name also became a global synonym for leading high-tech research and enterprises, and thus inspired similar named locations, as well as research parks and technology centers with a comparable structure all around the world.

Due to the personal connection between people and technology, many headquarters of companies in Silicon Valley are a hotspot for tourism.

The popularization of the name is credited to Don Hoefler, who first used it in the article "Silicon Valley USA", appearing in the January 11, 1971 issue of the weekly trade newspaper "Electronic News". The term gained widespread use in the early 1980s, at the time of the introduction of the IBM PC and numerous related hardware and software products to the consumer market.

Silicon Valley was born through several contributing factors intersecting, including a skilled STEM research base housed in area universities, plentiful venture capital, and steady U.S. Department of Defense spending. Stanford University leadership was especially important in the valley's early development. Together these elements formed the basis of its growth and success.

On August 23, 1899, the first ship-to-shore wireless telegraph message to be received in the US was from the San Francisco lightship outside the Golden Gate, signaling the return of the American fleet from the Philippines after their victory in the Spanish–American War. The ship had been outfitted with a wireless telegraph transmitter by a local newspaper, so that they could prepare a celebration on the return of the American sailors. Local historian Clyde Arbuckle states in "Clyde Arbuckle's History of San Jose" that "California first heard the click of a telegraph key on September 11, 1853. It marked completion of an enterprise begun by a couple of San Francisco Merchants' Exchange members named George Sweeney and Theodore E. Baugh…" He says, "In 1849, the gentleman established a wigwag telegraph station a top a high hill overlooking Portsmouth Squares for signaling arriving ships… The operator at the first station caught these signals by telescope and relayed them to the Merchant's Exchange for the waiting business community." Arbuckle points to the historic significance the Merchants Exchange Building (San Francisco) and Telegraph Hill, San Francisco when he goes on to say "The first station gave the name "Telegraph" to the hill on which it was located. It was known as the Inner Station; the second, as the Outer Station. Both used their primitive mode of communication until Messrs. Sweeney and Baugh connected the Outer Station directly with the Merchants's Exchange by electric telegraph Wire."

According to Arbuckle (p. 380–381) Sweeney and Baugh's line was strictly an intra-city, San Francisco-based service; that is until California State Telegraph Company enfranchised on May 3, 1852; whereas, O.E. Allen and C. Burnham led the way to "build a line from San Francisco to Marysville via San Jose, Stockton, and Sacramento". Delays to construction occurred until September 1853; but, "…San Jose became the first station on the line when the wire arrived here on October 15. The line was completed when [James] Gamble's northbound crew met a similar crew working southward from Marysville on October 24."

The Bay Area had long been a major site of United States Navy research and technology. In 1909, Charles Herrold started the first radio station in the United States with regularly scheduled programming in San Jose. Later that year, Stanford University graduate Cyril Elwell purchased the U.S. patents for Poulsen arc radio transmission technology and founded the Federal Telegraph Corporation (FTC) in Palo Alto. Over the next decade, the FTC created the world's first global radio communication system, and signed a contract with the Navy in 1912.

In 1933, Air Base Sunnyvale, California, was commissioned by the United States Government for use as a Naval Air Station (NAS) to house the airship USS Macon in Hangar One. The station was renamed NAS Moffett Field, and between 1933 and 1947, U.S. Navy blimps were based there. A number of technology firms had set up shop in the area around Moffett Field to serve the Navy. When the Navy gave up its airship ambitions and moved most of its west coast operations to San Diego, the National Advisory Committee for Aeronautics (NACA, forerunner of NASA) took over portions of Moffett Field for aeronautics research. Many of the original companies stayed, while new ones moved in. The immediate area was soon filled with aerospace firms, such as Lockheed, which was Silicon Valley's largest employer from the 1950s into 1980s.

The Bay Area was an early center of ham radio with about 10% of the operators in the United States. William Eitel, Jack McCullough, and Charles Litton, who together pioneered vacuum tube manufacturing in the Bay Area, were hobbyists with training in technology gained locally who participated in the development of shortwave radio by the ham radio hobby. High frequency, and especially, Very high frequency, VHF, transmission in the 10-meter band, required higher quality power tubes than were manufactured by the consortium of RCA, Western Electric, General Electric, Westinghouse which controlled vacuum tube manufacture. Litton, founder of Litton Industries, pioneered manufacturing techniques which resulted in the award of wartime contracts to manufacture transmitting tubes for radar to Eitel-McCullough, a San Bruno firm, which manufactured power-grid tubes for radio amateurs and aircraft radio equipment.

A union organizing drive in 1939–40 at Eitel-McCullough by the strong Bay Area labor movement was fought off by adoption of a strategy of welfare capitalism which included pensions and other generous benefits, profit sharing, and such extras as a medical clinic and a cafeteria. An atmosphere of cooperation and collaboration was established. Successes have been few and far between for union organizing drives by UE and others in subsequent years.

On October 4, 1957 the Soviet Union launched the first space satellite, Sputnik, which sparked fear that the Soviet Union was pulling ahead technologically. After President Eisenhower signed the National Aeronautics and Space Act (NASA), he turned to Fairchild Semiconductor, then the only company in the world that was able to make transistors. The president funded Fairchild's project, which was highly successful.

Stanford University, its affiliates, and graduates have played a major role in the development of this area. Some examples include the work of Lee De Forest with his invention of a pioneering vacuum tube called the Audion and the oscilloscopes of Hewlett-Packard.

A very powerful sense of regional solidarity accompanied the rise of Silicon Valley. From the 1890s, Stanford University's leaders saw its mission as service to the West and shaped the school accordingly. At the same time, the perceived exploitation of the West at the hands of eastern interests fueled booster-like attempts to build self-sufficient local industry. Thus, regionalism helped align Stanford's interests with those of the area's high-tech firms for the first fifty years of Silicon Valley's development.

After World War II, Frederick Terman, as Stanford University's dean of the school of engineering, encouraged faculty and graduates to start their own companies. In 1951, Terman spearheaded the creation of Stanford Industrial Park (now Stanford Research Park, an area surrounding Page Mill Road, south west of El Camino Real and extending beyond Foothill Expressway to Arastradero Road), whereby the University leased portions of its land to high-tech firms. He is credited with nurturing companies like Hewlett-Packard, Varian Associates, Eastman Kodak, General Electric, Lockheed Corporation, and other high-tech firms, until what would become Silicon Valley grew up around the Stanford University campus.

In 1956, William Shockley, the co-inventor of the first working transistor (with John Bardeen and Walter Houser Brattain), moved from New Jersey to Mountain View, California, to start Shockley Semiconductor Laboratory to live closer to his ailing mother in Palo Alto. Shockley's work served as the basis for many electronic developments for decades. Both Fredrick Terman and William Shockley are often called "the father of Silicon Valley".

During 1955–85, solid state technology research and development at Stanford University followed three waves of industrial innovation made possible by support from private corporations, mainly Bell Telephone Laboratories, Shockley Semiconductor, Fairchild Semiconductor, and Xerox PARC. In 1969, the Stanford Research Institute (now SRI International), operated one of the four original nodes that comprised ARPANET, predecessor to the Internet.

After World War II, universities were experiencing enormous demand due to returning students. To address the financial demands of Stanford's growth requirements, and to provide local employment opportunities for graduating students, Frederick Terman proposed the leasing of Stanford's lands for use as an office park, named the Stanford Industrial Park (later Stanford Research Park) in the year 1951. Leases were limited to high technology companies. Its first tenant was Varian Associates, founded by Stanford alumni in the 1930s to build military radar components. However, Terman also found venture capital for civilian technology start-ups. One of the major success stories was Hewlett-Packard. Founded in Packard's garage by Stanford graduates William Hewlett and David Packard, Hewlett-Packard moved its offices into the Stanford Research Park shortly after 1953. In 1954, Stanford created the Honors Cooperative Program to allow full-time employees of the companies to pursue graduate degrees from the University on a part-time basis. The initial companies signed five-year agreements in which they would pay double the tuition for each student in order to cover the costs. Hewlett-Packard has become the largest personal computer manufacturer in the world, and transformed the home printing market when it released the first thermal drop-on-demand ink jet printer in 1984. Other early tenants included Eastman Kodak, General Electric, and Lockheed.

Up until the late 1950s, germanium was the dominant semiconductor material for transistors and other semiconductor devices. Germanium was initially considered the more effective semiconductor material, as it was able to demonstrate better performance due to higher carrier mobility. The relative lack of performance in early silicon semiconductors was due to electrical conductivity being limited by unstable quantum surface states, preventing electricity from reliably penetrating the surface to reach the semiconducting silicon layer.

In 1953, William Shockley left Bell Labs in a disagreement over the handling of the invention of the bipolar transistor. After returning to California Institute of Technology for a short while, Shockley moved to Mountain View, California, in 1956, and founded Shockley Semiconductor Laboratory. Unlike many other researchers who used germanium as the semiconductor material, Shockley believed that silicon was the better material for making transistors. Shockley intended to replace the current transistor with a new three-element design (today known as the Shockley diode), but the design was considerably more difficult to build than the "simple" transistor. In 1957, Shockley decided to end research on the silicon transistor. As a result of Shockley's abusive management style, eight engineers left the company to form Fairchild Semiconductor; Shockley referred to them as the "traitorous eight". Two of the original employees of Fairchild Semiconductor, Robert Noyce and Gordon Moore, would go on to found Intel.

In 1957, Mohamed Atalla at Bell Labs developed the process of silicon surface passivation by thermal oxidation, which electrically stabilized silicon surfaces and reduced the concentration of electronic states at the surface. This enabled silicon to surpass the conductivity and performance of germanium, leading to silicon replacing germanium as the dominant semiconductor material, and paving the way for the mass-production of silicon semiconductor devices. This led to Atalla inventing the MOSFET (metal-oxide-silicon field-effect transistor), also known as the MOS transistor, with his colleague Dawon Kahng in 1959. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses, and is credited with starting the silicon revolution.

The MOSFET was initially overlooked and ignored by Bell Labs in favour of bipolar transistors, which led to Atalla resigning from Bell Labs and joining Hewlett-Packard in 1961. However, the MOSFET generated significant interest at RCA and Fairchild Semiconductor. In late 1960, Karl Zaininger and Charles Meuller fabricated a MOSFET at RCA, and Chih-Tang Sah built an MOS-controlled tetrode at Fairchild. MOS devices were later commercialized by General Microelectronics and Fairchild in 1964. The development of MOS technology became the focus of startup companies in California, such as Fairchild and Intel, fuelling the technological and economic growth of what would later be called Silicon Valley.

On April 23, 1963, J.C.R. Licklider, the first director of the Information Processing Techniques Office (IPTO) at The Pentagon's ARPA issued an office memorandum addressed to Members and Affiliates of the Intergalactic Computer Network. It rescheduled a meeting in Palo Alto regarding his vision of a computer network which he imagined as an electronic commons open to all the main and essential medium of informational interaction for governments, institutions, corporations, and individuals. As head of IPTO from 1962 to 1964, "Licklider initiated three of the most important developments in information technology: the creation of computer science departments at several major universities, time-sharing, and networking." By the late 1960s, his promotion of the concept had inspired a primitive version of his vision called ARPANET, which expanded into a network of networks in the 1970s that became the Internet.

The Immigration and Nationality Act of 1965 and other factors such as the mass exodus by Vietnamese boat people resulted in significant immigration, particularly by Asians, Latinos, and Portuguese, to Silicon Valley where they contributed to both the high-tech and production workforce. The Asian-American population in Santa Clara County rose from 43,000 in 1970 to 430,000 in 2000. During the same period the Latino population grew to 24% in the county and 30% in San Jose. The African-American population in the county remained steady but grew slightly to about 5%. Expansion of the H-1B visa in 1990 also played a role.

Following the 1959 inventions of the monolithic integrated circuit (IC) chip by Robert Noyce at Fairchild, and the MOSFET (MOS transistor) by Mohamed Atalla and Dawon Kahng at Bell Labs, Atalla first proposed the concept of the MOS integrated circuit (MOS IC) chip in 1960, and then the first commercial MOS IC was introduced by General Microelectronics in 1964. The development of the MOS IC led to the invention of the microprocessor, incorporating the functions of a computer's central processing unit (CPU) on a single integrated circuit. The first single-chip microprocessor was the Intel 4004, designed and realized by Federico Faggin along with Ted Hoff, Masatoshi Shima and Stanley Mazor at Intel in 1971. In April 1974, Intel released the Intel 8080, a "computer on a chip", "the first truly usable microprocessor".

The Homebrew Computer Club was an informal group of electronic enthusiasts and technically minded hobbyists who gathered to trade parts, circuits, and information pertaining to DIY construction of computing devices. It was started by Gordon French and Fred Moore who met at the Community Computer Center in Menlo Park. They both were interested in maintaining a regular, open forum for people to get together to work on making computers more accessible to everyone.

The first meeting was held as of March 1975 at French's garage in Menlo Park, San Mateo County, California; which was on occasion of the arrival of the MITS Altair microcomputer, the first unit sent to the area for review by People's Computer Company. Steve Wozniak and Steve Jobs credit that first meeting with inspiring them to design the original Apple I and (successor) Apple II computers. As a result, the first preview of the Apple I was given at the Homebrew Computer Club. Subsequent meetings were held at an auditorium at the Stanford Linear Accelerator Center.

By the early 1970s, there were many semiconductor companies in the area, computer firms using their devices, and programming and service companies serving both. Industrial space was plentiful and housing was still inexpensive. Growth during this era was fueled by the emergence of venture capital on Sand Hill Road, beginning with Kleiner Perkins and Sequoia Capital in 1972; the availability of venture capital exploded after the successful $1.3 billion IPO of Apple Computer in December 1980. Since the 1980s, Silicon Valley has been home to the largest concentration of venture capital firms in the world.

In 1971 Don Hoefler traced the origins of Silicon Valley firms, including via investments from Fairchild's eight co-founders. The key investors in Kleiner Perkins and Sequoia Capital were from the same group, directly leading to "Tech Crunch" 2014 estimate of 92 public firms of 130 related listed firms then worth over US$2.1 Trillion with over 2,000 firms traced back to them.
Prior to 1970, most Northern California lawyers were based in San Francisco, especially the experienced patent attorneys whom the high-tech industry needed to protect its intellectual property. During the 1970s, lawyers began to follow venture capitalists down the Peninsula to serve the booming high-tech industry in Silicon Valley. One sign of the rapid expansion of Silicon Valley legal services was that Palo Alto law firm Wilson Sonsini Goodrich & Rosati "expanded from a dozen attorneys in 1975 to more than 700 by 2000". During this era, law firms evolved from their "conventional role" as protectors of intellectual property into business advisers, intermediaries, and dealmakers, and thereby acquired "unusual prominence" in Silicon Valley.

Although semiconductors are still a major component of the area's economy, Silicon Valley has been most famous in recent years for innovations in software and Internet services. Silicon Valley has significantly influenced computer operating systems, software, and user interfaces.

Using money from NASA, the US Air Force, and ARPA, Doug Engelbart invented the mouse and hypertext-based collaboration tools in the mid-1960s and 1970s while at Stanford Research Institute (now SRI International), first publicly demonstrated in 1968 in what is now known as The Mother of All Demos. Engelbart's Augmentation Research Center at SRI was also involved in launching the ARPANET (precursor to the Internet) and starting the Network Information Center (now InterNIC). Xerox hired some of Engelbart's best researchers beginning in the early 1970s. In turn, in the 1970s and 1980s, Xerox's Palo Alto Research Center (PARC) played a pivotal role in object-oriented programming, graphical user interfaces (GUIs), Ethernet, PostScript, and laser printers.

While Xerox marketed equipment using its technologies, for the most part its technologies flourished elsewhere. The diaspora of Xerox inventions led directly to 3Com and Adobe Systems, and indirectly to Cisco, Apple Computer, and Microsoft. Apple's Macintosh GUI was largely a result of Steve Jobs' visit to PARC and the subsequent hiring of key personnel. Cisco's impetus stemmed from the need to route a variety of protocols over Stanford University's Ethernet campus network.

Commercial use of the Internet became practical and grew slowly throughout the early 1990s.

In 1995, commercial use of the Internet grew substantially and the initial wave of internet startups, Amazon.com, eBay, and the predecessor to Craigslist began operations.

Silicon Valley is generally considered to have been the center of the dot-com bubble, which started in the mid-1990s and collapsed after the NASDAQ stock market began to decline dramatically in April 2000. During the bubble era, real estate prices reached unprecedented levels. For a brief time, Sand Hill Road was home to the most expensive commercial real estate in the world, and the booming economy resulted in severe traffic congestion.

After the dot-com crash, Silicon Valley continues to maintain its status as one of the top research and development centers in the world. A 2006 "The Wall Street Journal" story found that 12 of the 20 most inventive towns in America were in California, and 10 of those were in Silicon Valley. San Jose led the list with 3,867 utility patents filed in 2005, and number two was Sunnyvale, at 1,881 utility patents. Silicon Valley is also home to a significant number of "Unicorn" ventures, referring to startup companies whose valuation has exceeded $1 billion dollars. However, taxes and the cost of living in Silicon Valley have prompted some corporations to gradually transfer their operations to the Midwest or Sun Belt states.

The San Francisco Bay Area has the largest concentration of high-tech companies in the United States, at 387,000 high-tech jobs, of which Silicon Valley accounts for 225,300 high-tech jobs. Silicon Valley has the highest concentration of high-tech workers of any metropolitan area, with 285.9 out of every 1,000 private-sector workers. Silicon Valley has the highest average high-tech salary in the United States at $144,800. Largely a result of the high technology sector, the San Jose-Sunnyvale-Santa Clara, CA Metropolitan Statistical Area has the most millionaires and the most billionaires in the United States per capita.

The region is the biggest high-tech manufacturing centre in the United States. The unemployment rate of the region was 9.4% in January 2009 and has decreased to a record low of 2.7% as of August 2019. Silicon Valley received 41% of all U.S. venture investment in 2011, and 46% in 2012. More traditional industries also recognize the potential of high-tech development, and several car manufacturers have opened offices in Silicon Valley to capitalize on its entrepreneurial ecosystem.

Manufacture of transistors is, or was, the core industry in Silicon Valley. The production workforce was for the most part composed of Asian and Latino immigrants who were paid low wages and worked in hazardous conditions due to the chemicals used in the manufacture of integrated circuits. Technical, engineering, design, and administrative staffs were in large part well compensated.

Silicon Valley has a severe housing shortage, caused by the market imbalance between jobs created and housing units built: from 2010 to 2015, many more jobs have been created than housing units built. (400,000 jobs, 60,000 housing units) This shortage has driven home prices extremely high, far out of the range of production workers. As of 2016 a two-bedroom apartment rented for about $2,500 while the median home price was about $1 million. The Financial Post called Silicon Valley the most expensive U.S. housing region. Homelessness is a problem with housing beyond the reach of middle-income residents; there is little shelter space other than in San Jose which, as of 2015, was making an effort to develop shelters by renovating old hotels.

"The Economist" also attributes the high cost of living to the success of the industries in this region. Although, this rift between high and low salaries is driving many residents out who can no longer afford to live there. In the Bay Area, the number of residents planning to leave within the next several years has had an increase of 12% since 2016, from 34% to 46%.

Thousands of high technology companies are headquartered in Silicon Valley. Among those, the following are in the Fortune 1000:

Additional notable companies headquartered (or with a significant presence) in Silicon Valley include (some defunct or subsumed):
Silicon Valley is also home to the high-tech superstore retail chain Fry's Electronics.


Depending on what geographic regions are included in the meaning of the term, the population of Silicon Valley is between 3.5 and 4 million. A 1999 study by AnnaLee Saxenian for the Public Policy Institute of California reported that a third of Silicon Valley scientists and engineers were immigrants and that nearly a quarter of Silicon Valley's high-technology firms since 1980 were run by Chinese (17 percent) or Indian CEOs (7 percent). There is a stratum of well-compensated technical employees and managers, including 10s of thousands of "single-digit millionaires". This income and range of assets will support a middle-class lifestyle in Silicon Valley.

In November 2006, the University of California, Davis released a report analyzing business leadership by women within the state. The report showed that although 103 of the 400 largest public companies headquartered in California were located in Santa Clara County (the most of all counties), only 8.8% of Silicon Valley companies had women CEOs. This was the lowest percentage in the state. (San Francisco County had 19.2% and Marin County had 18.5%.)

Silicon Valley tech leadership positions are occupied almost exclusively by men. This is also represented in the number of new companies founded by women as well as the number of women-lead startups that receive venture capital funding. Wadhwa said he believes that a contributing factor is a lack of parental encouragement to study science and engineering. He also cited a lack of women role models and noted that most famous tech leaders—like Bill Gates, Steve Jobs, and Mark Zuckerberg—are men.

In 2014, tech companies Google, Yahoo!, Facebook, Apple, and others, released corporate transparency reports that offered detailed employee breakdowns. In May, Google said 17% of its tech employees worldwide were women, and, in the U.S., 1% of its tech workers were black and 2% were Hispanic. June 2014 brought reports from Yahoo! and Facebook. Yahoo! said that 15% of its tech jobs were held by women, 2% of its tech employees were black and 4% Hispanic. Facebook reported that 15% of its tech workforce was female, and 3% was Hispanic and 1% was black. In August, Apple reported that 80% of its global tech staff was male and that, in the U.S., 54% of its tech jobs were staffed by Caucasians and 23% by Asians. Soon after, "USA Today" published an article about Silicon Valley's lack of tech-industry diversity, pointing out that it is largely white or Asian, and male. "Blacks and Hispanics are largely absent," it reported, "and women are underrepresented in Silicon Valley—from giant companies to start-ups to venture capital firms." Civil rights activist Jesse Jackson said of improving diversity in the tech industry, "This is the next step in the civil rights movement" while T.J. Rodgers has argued against Jackson's assertions.

As of October 2014, some high-profile Silicon Valley firms were working actively to prepare and recruit women. "Bloomberg" reported that Apple, Facebook, Google, and Microsoft attended the 20th annual Grace Hopper Celebration of Women in Computing conference to actively recruit and potentially hire female engineers and technology experts. The same month, the second annual Platform Summit was held to discuss increasing racial and gender diversity in tech. As of April 2015 experienced women were engaged in creation of venture capital firms which leveraged women's perspectives in funding of startups.

After UC Davis published its "Study of California Women Business Leaders" in November 2006, some "San Jose Mercury News" readers dismissed the possibility that sexism contributed in making Silicon Valley's leadership gender gap the highest in the state. A January 2015 issue of "Newsweek" magazine featured an article detailing reports of sexism and misogyny in Silicon Valley. The article's author, Nina Burleigh, asked, "Where were all these offended people when women like Heidi Roizen published accounts of having a venture capitalist stick her hand in his pants under a table while a deal was being discussed?"

Silicon Valley firms' board of directors are composed of 15.7% women compared with 20.9% in the S&P 100.

The 2012 lawsuit Pao v. Kleiner Perkins was filed in San Francisco County Superior Court by executive Ellen Pao for gender discrimination against her employer, Kleiner Perkins. The case went to trial in February 2015. On March 27, 2015 the jury found in favor of Kleiner Perkins on all counts. Nevertheless, the case, which had wide press coverage, resulted in major advances in consciousness of gender discrimination on the part of venture capital and technology firms and their women employees. Two other cases have been filed against Facebook and Twitter.

Funding for public schools in upscale Silicon Valley communities such as Woodside is often supplemented by grants from private foundations set up for that purpose and funded by local residents. Schools in less affluent areas such as East Palo Alto must depend on state funding.

The following Santa Clara County cities are traditionally considered to be in Silicon Valley (in alphabetical order):

The geographical boundaries of Silicon Valley have changed over the years, traditionally Silicon Valley is known as Santa Clara County, southern San Mateo County and southern Alameda county. However, over the years this geographical area has been expanded to include San Francisco County, Contra Costa County, and the northern parts of Alameda County and San Mateo County, this shift has occurred due to the expansion in the local economy and the development of new technologies.

The United States Department of Labor's Quarterly Census of Employment and Wages (QCEW) program defined Silicon Valley as the counties of Alameda, Contra Costa, San Francisco, San Mateo, Santa Clara, and Santa Cruz.

In 2015, MIT researchers developed a novel method for measuring which towns are home to startups with higher growth potential and this defines Silicon Valley to center on the municipalities of Menlo Park, Mountain View, Palo Alto, and Sunnyvale.

Silicon Valley's art gallery, Pace Art and Technology Gallery in Menlo Park, opened on February 6, 2016.

In 1928, the Allied Arts Guild was formed in Menlo Park and is a complex of artist studios, shops, restaurant, and gardens.




In 1980, "Intelligent Machines Journal" changed its name to "InfoWorld", and, with offices in Palo Alto, began covering the emergence of the microcomputer industry in the valley.

Local and national media cover Silicon Valley and its companies. CNN, "The Wall Street Journal", and Bloomberg News operate Silicon Valley bureaus out of Palo Alto. Public broadcaster KQED (TV) and KQED-FM, as well as the Bay Area's local ABC station KGO-TV, operate bureaus in San Jose. KNTV, NBC's local Bay Area affiliate "NBC Bay Area", is located in San Jose. Produced from this location is the nationally distributed TV Show "Tech Now" as well as the CNBC Silicon Valley bureau. San Jose-based media serving Silicon Valley include the "San Jose Mercury News" daily and the "Metro Silicon Valley" weekly.

Specialty media include "El Observador" and the "San Jose / Silicon Valley Business Journal". Most of the Bay Area's other major TV stations, newspapers, and media operate in San Francisco or Oakland. Patch.com operates various web portals, providing local news, discussion and events for residents of Silicon Valley. Mountain View has a public nonprofit station, KMVT-15. KMVT-15's shows include Silicon Valley Education News (EdNews)-Edward Tico Producer.

Some appearances in media, in order by release date:







</doc>
<doc id="26977" url="https://en.wikipedia.org/wiki?curid=26977" title="Stanford University">
Stanford University

Stanford University, officially Leland Stanford Junior University, is a private research university in Stanford, California. Stanford is known for its academic achievements, wealth, close proximity to Silicon Valley, and selectivity; it ranks as one of the world's top universities.

The university was founded in 1885 by Leland and Jane Stanford in memory of their only child, Leland Stanford Jr., who had died of typhoid fever at age 15 the previous year. Stanford was a U.S. Senator and former Governor of California who made his fortune as a railroad tycoon. The school admitted its first students on October 1, 1891, as a coeducational and non-denominational institution.

Stanford University struggled financially after the death of Leland Stanford in 1893 and again after much of the campus was damaged by the 1906 San Francisco earthquake. Following World War II, Provost Frederick Terman supported faculty and graduates' entrepreneurialism to build self-sufficient local industry in what would later be known as Silicon Valley. The university is also one of the top fundraising institutions in the country, becoming the first school to raise more than a billion dollars in a year.

The university is organized around three traditional schools consisting of 40 academic departments at the undergraduate and graduate level and four professional schools that focus on graduate programs in law, medicine, education, and business. Students compete in 36 varsity sports, and the university is one of two private institutions in the Division I FBS Pac-12 Conference. It has gained 126 NCAA team championships, and Stanford has won the NACDA Directors' Cup for 24 consecutive years, beginning in 1994–1995. In addition, Stanford students and alumni have won 270 Olympic medals including 139 gold medals.

As of January 2020, 83 Nobel laureates, 27 Turing Award laureates, and 8 Fields Medalists have been affiliated with Stanford as students, alumni, faculty or staff. In addition, Stanford University is particularly noted for its entrepreneurship and is one of the most successful universities in attracting funding for start-ups. Stanford alumni have founded numerous companies, which combined produce more than $2.7 trillion in annual revenue and have created 5.4 million jobs as of 2011, roughly equivalent to the 10th largest economy in the world (). Stanford is the alma mater of one president of the United States (Herbert Hoover), 30 living billionaires, and 17 astronauts. It is also one of the leading producers of members of the United States Congress.

Stanford University was founded in 1885 by Leland and Jane Stanford, dedicated to Leland Stanford Jr, their only child. The institution opened in 1891 on Stanford's previous Palo Alto farm. Despite being impacted by earthquakes in both 1906 and 1989, the campus was rebuilt each time. In 1919, The Hoover Institution on War, Revolution and Peace was started by Herbert Hoover to preserve artifacts related to World War I. The Stanford Medical Center, completed in 1959, is a teaching hospital with over 800 beds. The SLAC National Accelerator Laboratory (originally named the Stanford Linear Accelerator Center), which was established in 1962, performs research in particle physics.

Jane and Leland Stanford modeled their university after the great eastern universities, most specifically Cornell University. Stanford opened being called the "Cornell of the West" in 1891 due to faculty being former Cornell affiliates (either professors, alumni, or both) including its first president, David Starr Jordan. Both Cornell and Stanford were among the first to have higher education be accessible, nonsectarian, and open to women as well as to men. Cornell is credited as one of the first American universities to adopt this radical departure from traditional education, and Stanford became an early adopter as well.

Most of Stanford University is on an campus, one of the largest in the United States. It is located on the San Francisco Peninsula, in the northwest part of the Santa Clara Valley (Silicon Valley) approximately southeast of San Francisco and approximately northwest of San Jose. In 2008, 60% of this land remained undeveloped.

Stanford's main campus includes a census-designated place within unincorporated Santa Clara County, although some of the university land (such as the Stanford Shopping Center and the Stanford Research Park) is within the city limits of Palo Alto. The campus also includes much land in unincorporated San Mateo County (including the SLAC National Accelerator Laboratory and the Jasper Ridge Biological Preserve), as well as in the city limits of Menlo Park (Stanford Hills neighborhood), Woodside, and Portola Valley.

The academic central campus is adjacent to Palo Alto, bounded by El Camino Real, Stanford Avenue, Junipero Serra Boulevard, and Sand Hill Road. The United States Postal Service has assigned it two ZIP Codes: 94305 for campus mail and 94309 for P.O. box mail. It lies within area code 650.

Stanford currently operates or intends to operate in various locations outside of its central campus.

On the founding grant:

Off the founding grant:

Locations in development:

Many Stanford faculty members live in the "Faculty Ghetto", within walking or biking distance of campus. The Faculty Ghetto is composed of land owned entirely by Stanford. Similar to a condominium, the houses can be bought and sold but the land under the houses is rented on a 99-year lease. Houses in the "Ghetto" appreciate and depreciate, but not as rapidly as overall Silicon Valley values. However, it remains an expensive area in which to own property, and the average price of single-family homes on campus is actually higher than in Palo Alto.

Some of the land is managed to provide revenue for the university such as the Stanford Shopping Center and the Stanford Research Park. Stanford land is also leased for a token rent by the Palo Alto Unified School District for several schools including Palo Alto High School and Gunn High School. El Camino Park, the oldest Palo Alto city park (established 1914), is also on Stanford land.

Contemporary campus landmarks include the Main Quad and Memorial Church, the Cantor Center for Visual Arts and the Bing Concert Hall, the Stanford Mausoleum with the nearby Angel of Grief, Hoover Tower, the Rodin sculpture garden, the Papua New Guinea Sculpture Garden, the Arizona Cactus Garden, the Stanford University Arboretum, Green Library and the Dish. Frank Lloyd Wright's 1937 Hanna–Honeycomb House and the 1919 Lou Henry Hoover House are both listed on the National Historic Register. White Memorial Fountain (also known as "The Claw") between the Stanford Bookstore and the Old Union is a popular place to meet and to engage in the Stanford custom of "fountain hopping"; it was installed in 1964 and designed by Aristides Demetrios after a national competition as a memorial for two brothers in the class of 1949, William N. White and John B. White II, one of whom died before graduating and one shortly after in 1952.

Stanford University is a private, non-profit university that is administered as a corporate trust governed by a privately appointed board of trustees with a maximum membership of 38. Trustees serve five-year terms (not more than two consecutive terms) and meet five times annually. A new trustee is chosen by the current trustees by ballot. The Stanford trustees also oversee the Stanford Research Park, the Stanford Shopping Center, the Cantor Center for Visual Arts, Stanford University Medical Center, and many associated medical facilities (including the Lucile Packard Children's Hospital).

The Board appoints a President to serve as the chief executive officer of the university and prescribe the duties of professors and course of study, manage financial and business affairs, and appoint nine vice presidents. The Provost is the chief academic and budget officer, to whom the deans of each of the seven schools report. Persis Drell became the 13th Provost in February 2017.

As of 2018, the university was organized into seven academic schools. The schools of Humanities and Sciences (27 departments), Engineering (9 departments), and Earth, Energy & Environmental Sciences (4 departments) have both graduate and undergraduate programs while the Schools of Law, Medicine, Education and Business have graduate programs only. The powers and authority of the faculty are vested in the Academic Council, which is made up of tenure and non-tenure line faculty, research faculty, senior fellows in some policy centers and institutes, the president of the university, and some other academic administrators, but most matters are handled by the Faculty Senate, made up of 55 elected representatives of the faculty.

The Associated Students of Stanford University (ASSU) is the student government for Stanford University and all registered students are members. Its elected leadership consists of the Undergraduate Senate elected by the undergraduate students, the Graduate Student Council elected by the graduate students, and the President and Vice President elected as a ticket by the entire student body.

Stanford is the beneficiary of a special clause in the California Constitution, which explicitly exempts Stanford property from taxation so long as the property is used for educational purposes.

The university's endowment, managed by the Stanford Management Company, was valued at $27.7 billion as of August 31, 2019. Payouts from the Stanford endowment covered approximately 21.8% of university expenses in the 2019 fiscal year. In the 2018 NACUBO-TIAA survey of colleges and universities in the United States and Canada, only Harvard University, the University of Texas System, and Yale University had larger endowments than Stanford.

In 2006, President John L. Hennessy launched a five-year campaign called the Stanford Challenge, which reached its $4.3 billion fundraising goal in 2009, two years ahead of time, but continued fundraising for the duration of the campaign. It concluded on December 31, 2011, having raised a total of $6.23 billion and breaking the previous campaign fundraising record of $3.88 billion held by Yale. Specifically, the campaign raised $253.7 million for undergraduate financial aid, as well as $2.33 billion for its initiative in "Seeking Solutions" to global problems, $1.61 billion for "Educating Leaders" by improving K-12 education, and $2.11 billion for "Foundation of Excellence" aimed at providing academic support for Stanford students and faculty. Funds supported 366 new fellowships for graduate students, 139 new endowed chairs for faculty, and 38 new or renovated buildings. The new funding also enabled the construction of a facility for stem cell research; a new campus for the business school; an expansion of the law school; a new Engineering Quad; a new art and art history building; an on-campus concert hall; a new art museum; and a planned expansion of the medical school, among other things. In 2012, the university raised $1.035 billion, becoming the first school to raise more than a billion dollars in a year.

Stanford follows a quarter system with the Autumn quarter usually starting in late September and the Spring Quarter ending in early June. The full-time, four-year undergraduate program has an arts and sciences focus with high graduate student coexistence. Stanford is accredited by the Western Association of Schools and Colleges.

Full-time undergraduate tuition was $42,690 for 2013–2014. Stanford's admission process is need-blind for US citizens and permanent residents; while it is not need-blind for international students, 64% are on need-based aid, with an average aid package of $31,411. In 2012–13, the university awarded $126 million in need-based financial aid to 3,485 students, with an average aid package of $40,460. Eighty percent of students receive some form of financial aid. Stanford has a no-loan policy. For undergraduates admitted in 2015, Stanford waives tuition, room, and board for most families with incomes below $65,000, and most families with incomes below $125,000 are not required to pay tuition; those with incomes up to $150,000 may have tuition significantly reduced. 17% of students receive Pell Grants, a common measure of low-income students at a college.

As of 2016 the Office of the Vice Provost and Dean of Research oversaw eighteen independent laboratories, centers, and institutes.

Other Stanford-affiliated institutions include the SLAC National Accelerator Laboratory (originally the Stanford Linear Accelerator Center), the Stanford Research Institute (an independent institution which originated at the university), the Hoover Institution on War, Revolution and Peace (a major public policy think tank that attracts visiting scholars from around the world) and the Hasso Plattner Institute of Design (a multidisciplinary design school in cooperation with the Hasso Plattner Institute of University of Potsdam that integrates product design, engineering, and business management education).

Stanford is home to the Martin Luther King Jr. Research and Education Institute which grew out of and still contains the Martin Luther King Jr. Papers Project, a collaboration with the King Center to publish the King papers held by the King Center. It also runs the John S. Knight Fellowship for Professional Journalists and the Center for Ocean Solutions, which brings together marine science and policy to address challenges facing the ocean.

Together with UC Berkeley and UC San Francisco, Stanford is part of the Biohub, a new medical science research center founded in 2016 by a $600 million commitment from Facebook CEO and founder Mark Zuckerberg and pediatrician Priscilla Chan.

As of 2014, Stanford University Libraries (SUL) held a collection of more than 9.3 million volumes, nearly 300,000 rare or special books, 1.5 million e-books, 2.5 million audiovisual materials, 77,000 serials, nearly 6 million microform holdings, and thousands of other digital resources.

The main library in the SU library system is Green Library, which also contains various meeting and conference rooms, study spaces, and reading rooms. Lathrop Library (previously Meyer Library, demolished in 2015), holds various student-accessible media resources and houses one of the largest East Asia collections with 540,000 volumes.

Stanford is home to the Cantor Center for Visual Arts, a museum with 24 galleries, sculpture gardens, terraces, and a courtyard first established in 1891 by Jane and Leland Stanford as a memorial to their only child. The Center's collection of works by Rodin is among the largest in the world The Thomas Welton Stanford Gallery, built in 1917, serves as a teaching resource for the Department of Art & Art History as well as an exhibition venue. There are outdoor art installations throughout the campus, primarily sculptures, but some murals as well. The Papua New Guinea Sculpture Garden near Roble Hall features includes wood carvings and "totem poles."

The Stanford music department sponsors many ensembles including five choirs, the Stanford Symphony Orchestra, Stanford Taiko, and the Stanford Wind Ensemble. Extracurricular activities include theater groups such as Ram's Head Theatrical Society, the Stanford Improvisors, the Stanford Shakespeare Society, and the Stanford Savoyards, a group dedicated to performing the works of Gilbert and Sullivan. Stanford is also host to ten a cappella groups, including the Mendicants (Stanford's first), Counterpoint (the first all-female group on the West Coast), the Stanford Fleet Street Singers, Harmonics, Mixed Company, Testimony, Talisman, Everyday People, Raagapella and O-Tone.

Notably, Stanford ranks high and often first in many domestic college ranking measures, leading "Slate" to dub Stanford in 2014 as "the Harvard of the 21st century," and "The New York Times" in the same year to conclude that "Stanford University has become America's 'it' school, by measures that Harvard once dominated." From polls done by The Princeton Review in 2013, 2014, 2015, 2016, 2017, 2018 and 2019, the most commonly named "dream college" for students was Stanford; separately, parents, too, most frequently named Stanford their "dream college." The inaugural 2017 Wall Street Journal/Times Higher Education College Rankings picked Stanford as the No. 1 school in the United States.
Globally, the "Academic Ranking of World Universities (ARWU)" ranked Stanford second in the world most years from 2003 to 2016. In 2019, it ranked 4th among the universities around the world by SCImago Institutions Rankings. Additionally, "Times Higher Education" recognized Stanford as one of the world's "six super brands" on its "World Reputation Rankings", along with Berkeley, Cambridge, Harvard, MIT, and Oxford. Stanford was ranked fifth in the 2016, 2017, and 2018 Nature Index Annual Tables, which measure the largest contributors to papers published in 82 leading journals.



Stanford is one of the most successful universities in creating companies and licensing its inventions to existing companies; it is often held up as a model for technology transfer. Stanford's Office of Technology Licensing is responsible for commercializing university research, intellectual property, and university-developed projects.

The university is described as having a strong venture culture in which students are encouraged, and often funded, to launch their own companies.

Companies founded by Stanford alumni generate more than $2.7 trillion in annual revenue, equivalent to the 10th-largest economy in the world.

Some companies closely associated with Stanford and their connections include:

Stanford enrolled 7,061 undergraduate and 11,075 graduate students as of October 2013, and women comprised 47% of undergraduates and 41% of professional and graduate students. In the same academic year, the freshman retention rate was 99%.

Stanford awarded 1,715 undergraduate degrees, 2,278 master's degrees, 764 doctoral degrees, and 366 professional degrees in the 2011–2012 school year. The four-year graduation rate in the class of 2011 was 76%, and the six-year rate was 96%. The relatively low four-year graduation rate is a function of the university's coterminal degree (or "coterm") program, which allows students to earn a master's degree as an extension of their undergraduate program.

As of 2010, fifteen percent of undergraduates were first-generation students.

As of 2013, 89% of undergraduate students lived in on-campus university housing. First-year undergraduates are required to live on campus, and all undergraduates are guaranteed housing for all four undergraduate years. Undergraduates live in 80 different houses, including dormitories, co-ops, row houses, and fraternities and sororities. At Manzanita Park, 118 mobile homes were installed as "temporary" housing from 1969 to 1991, but as of 2015 was the site of newer dorms Castano, Kimball, Lantana, and the Humanities House, completed in 2015. Most student residences are just outside the campus core, within ten minutes (on foot or bike) of most classrooms and libraries. Some are reserved for freshman, sophomores, or upperclass students and some are open to all four classes. Most residences are co-ed; seven are all-male fraternities, three are all-female sororities, and there is also one all-female non-sorority house, Roth House. In most residences, men and women live on the same floor, but a few dorms are configured for men and women to live on separate floors (single-gender floors).
Several residences are considered theme houses. The Academic, Language and Culture Houses include EAST (Education And Society Themed House), Hammarskjöld (International Themed House), Haus Mitteleuropa (Central European Themed House), La Casa Italiana (Italian Language and Culture), La Maison Française (French Language and Culture House), Slavianskii Dom (Slavic/East European Themed House), Storey (Human Biology Themed House), and Yost (Spanish Language and Culture). Cross-Cultural Themed Houses include Casa Zapata (Chicano/Latino Theme in Stern Hall), Muwekma-tah-ruk (American Indian/Alaska Native, and Native Hawaiian Themed House), Okada (Asian-American Themed House in Wilbur Hall), and Ujamaa (Black/African-American Themed House in Lagunita Court). Focus Houses include Freshman-Sophomore College (Academic Focus), Branner Hall (Community Service), Kimball (Arts & Performing Arts), Crothers (Global Citizenship), and Toyon (Sophomore Priority). Theme houses predating the current "theme" classification system are Columbae (Social Change Through Nonviolence, since 1970), and Synergy (Exploring Alternatives, since 1972).

Co-ops or "Self-Ops" are another housing option. These houses feature cooperative living, where residents and eating associates each contribute work to keep the house running, such as cooking meals or cleaning shared spaces. These houses have unique themes around which their community is centered. Many co-ops are hubs of music, art and philosophy. The co-ops on campus are 576 Alvarado Row (formerly Chi Theta Chi), Columbae, Enchanted Broccoli Forest (EBF), Hammarskjöld, Kairos, Terra (the unofficial LGBT house), and Synergy. Phi Sigma, at 1018 Campus Drive was formerly Phi Sigma Kappa fraternity, but in 1973 became a Self-Op.

As of 2015 around 55 percent of the graduate student population lived on campus. First-year graduate students are guaranteed on-campus housing. Stanford also subsidizes off-campus apartments in nearby Palo Alto, Menlo Park, and Mountain View for graduate students who are guaranteed on-campus housing but are unable to live on campus due to a lack of space.

As of 2016 Stanford had 16 male varsity sports and 20 female varsity sports, 19 club sports and about 27 intramural sports In 1930, following a unanimous vote by the Executive Committee for the Associated Students, the athletic department adopted the mascot "Indian." The Indian symbol and name were later dropped by President Richard Lyman in 1972, after objections from Native American students and a vote by the student senate. The sports teams are now officially referred to as the "Stanford Cardinal", referring to the deep red color, not the cardinal bird. Stanford is a member of the Pac-12 Conference in most sports, the Mountain Pacific Sports Federation in several other sports, and the America East Conference in field hockey with the participation in the inter-collegiate NCAA's Division I FBS.

Its traditional sports rival is Berkeley, the neighbor to the north in the East Bay. The winner of the annual "Big Game" between the Cal and Cardinal football teams gains custody of the Stanford Axe.

Stanford has had at least one NCAA team champion every year since the 1976–77 school year and has earned 126 NCAA national team titles since its establishment, the most among universities, and Stanford has won 522 individual national championships, the most by any university. Stanford has won the award for the top-ranked Division 1 athletic program — the NACDA Directors' Cup, formerly known as the Sears Cup – annually for the past twenty-four straight years. Stanford athletes have won medals in every Olympic Games since 1912, winning 270 Olympic medals total, 139 of them gold. In the 2008 Summer Olympics, and 2016 Summer Olympics, Stanford won more Olympic medals than any other university in the United States. Stanford athletes won 16 medals at the 2012 Summer Olympics (12 gold, 2 silver and 2 bronze), and 27 medals at the 2016 Summer Olympics.


Students and staff at Stanford are of many different religions. The Stanford Office for Religious Life's mission is "to guide, nurture and enhance spiritual, religious and ethical life within the Stanford University community" by promoting enriching dialogue, meaningful ritual, and enduring friendships among people of all religious backgrounds. It is headed by a dean with the assistance of a senior associate dean and an associate dean.
Stanford Memorial Church, in the center of campus, has a Sunday University Public Worship service (UPW) usually in the "Protestant Ecumenical Christian" tradition where the Memorial Church Choir sings and a sermon is preached usually by one of the Stanford deans for Religious Life. UPW sometimes has multifaith services. In addition, the church is used by the Catholic community and by some of the other Christian denominations at Stanford. Weddings happen most Saturdays and the university has for over 20 years allowed blessings of same-gender relationships and now legal weddings.

In addition to the church, the Office for Religious Life has a Center for Inter-Religious Community, Learning and Experiences (CIRCLE) on the third floor of Old Union. It offers a common room, an interfaith sanctuary, a seminar room, a student lounge area, and a reading room, as well as offices housing a number of Stanford Associated Religions (SAR) member groups and the Senior Associate Dean and Associate Dean for Religious Life. Most though not all religious student groups belong to SAR. The SAR directory includes organizations that serve atheist, Baha'i, Buddhist, Christian, Hindu, Islam, Jewish, and Sikh groups, though these groups vary year by year.

The Windhover Contemplation Center was dedicated in October 2014, and was intended to provide spiritual sanctuary for students and staff in the midst of their course and work schedules; the center displays the "Windhover" paintings by Nathan Oliveira, the late Stanford professor and artist.

Some religions have a larger and more formal presence on campus in addition to the student groups; these include the Catholic Community at Stanford and Hillel at Stanford.

Fraternities and sororities have been active on the Stanford campus since 1891, when the university first opened. In 1944, University President Donald Tresidder banned all Stanford sororities due to extreme competition. However, following Title IX, the Board of Trustees lifted the 33-year ban on sororities in 1977. Students are not permitted to join a fraternity or sorority until Spring quarter of their freshman year.

As of 2016 Stanford had 31 Greek organizations, including 14 sororities and 16 fraternities. Nine of the Greek organizations were housed (eight in University-owned houses and one, Sigma Chi, in their own house [although the land is owned by the University]). Six chapters were members of the African American Fraternal and Sororal Association, 11 chapters were members of the Interfraternity Council, 7 chapters belonged to the Intersorority Council, and 6 chapters belonged to the Multicultural Greek Council.

As of 2014, Stanford had 650 student organizations. Groups are often, though not always, partially funded by the University via allocations directed by the student government organization, the ASSU. These funds include "special fees", which are decided by a Spring Quarter vote by the student body. Groups span from Athletic/Recreational (see section on Athletics), Careers/Pre-professional, Community Service, Ethnic/Cultural, Fraternities/Sororities, Health/Counseling, Media/Publications, Music/Dance/Creative Arts (see section on Arts), Political/Social Awareness to Religious/Philosophical.

The "Stanford Daily" is the daily newspaper and has been published since the University was founded in 1892. The Stanford Review is a conservative student newspaper founded in 1987. The student-run radio station, KZSU Stanford 90.1 FM, features freeform music programming, sports commentary, and news segments; it started in 1947 as an AM radio station.

Students run "SUpost.com", an online marketplace for Stanford students and alumni, in partnership with Stanford Student Enterprises (SSE) and the Stanford Pre-Business Association. The latter is intended to build connections among industry, alumni, and student communities. Stanford Marketing is a student group that provides students hands-on training through research and strategy consulting projects with Fortune 500 clients, as well as workshops led by people from industry and professors in the Stanford Graduate School of Business. Stanford Finance provides mentoring and internships for students who want to enter a career in finance. The Business Association of Stanford Entrepreneurial Students (BASES), is one of the largest professional organizations in Silicon Valley, with over 5,000 members. Its goal is to support the next generation of entrepreneurs. Stanford Women In Business (SWIB) is an on-campus business organization consisting of over a board of 40 and 100 active members. Each year, SWIB organizes over 25 events and workshops, hosts a winter and spring conference, and provides mentorship and spring quarter internships. StartX is a non-profit startup accelerator for student and faculty-led startups that over 12% of the study body has applied to. It is staffed primarily by students.

Other groups include:

Stanford's Department of Public Safety is responsible for law enforcement and safety on the main campus. Its deputy sheriffs are peace officers by arrangement with the Santa Clara County Sheriff's Office. The department is also responsible for publishing an annual crime report covering the previous three years as required by the Clery Act. Fire protection has been provided by contract with the Palo Alto Fire Department since 1976.

Murder is rare on the campus though a few of the cases have been notorious including Theodore Streleski's murder of his professor in 1978 and the unsolved 1974 murder of Arlis Perry in Stanford Memorial Church.

In 2014, Stanford University was the tenth highest in the nation in "total of reports of rape" on their main campus, with 26 reports of rape.

In Stanford University's 2015 Campus Climate Survey, 4.7 percent of female undergraduates reported experiencing sexual assault as defined by the university and 32.9 percent reported experiencing sexual misconduct. According to the survey, 85% of perpetrators of misconduct were Stanford students and 80% were men. Perpetrators of sexual misconduct were frequently aided by alcohol or drugs, according to the survey: "Nearly three-fourths of the students whose responses were categorized as sexual assault indicated that the act was accomplished by a person or person taking advantage of them when they were drunk or high, according to the survey. Close to 70 percent of students who reported an experience of sexual misconduct involving nonconsensual penetration and/or oral sex indicated the same." Associated Students of Stanford University and student and alumni activists with the anti-rape group Stand with Leah criticized the survey methodology for downgrading incidents involving alcohol if students did not check two separate boxes indicating they were both intoxicated and incapacity while sexually assaulted. Reporting on the Brock Turner rape case, a reporter from "The Washington Post" analyzed campus rape reports submitted by universities to the U.S. Department of Education, and found that Stanford was one of the top ten universities in campus rapes in 2014, with 26 reported that year, but when analyzed by rapes per 1000 students, Stanford was not among the top ten.

On the night of January 17–18, 2015, 22-year-old Chanel Miller, who had visited campus to attend a party at the Kappa Alpha fraternity, was sexually assaulted by Brock Turner, a freshman who had a swimming scholarship. Two graduate students witnessed the attack and intervened, catching Turner when he tried to flee and holding him down on the ground until police arrived. Stanford immediately referred the case to prosecutors and offered Miller counseling, and within two weeks had barred Turner from campus after conducting an investigation. Turner was convicted on three felony charges in March 2016 and in June 2016 he received a jail sentence of six months and was declared a sex offender, requiring him to register as such for the rest of his life; prosecutors had sought a six-year prison sentence out of the maximum 14 years that was possible. The case and the relatively lenient sentence drew nationwide attention. Two years later the judge in the case, Stanford graduate Aaron Persky, was recalled by the voters.

In February 2015, Elise Clougherty filed a sexual assault and harassment lawsuit against venture capitalist Joe Lonsdale. Lonsdale and Clougherty entered into a relationship in the spring of 2012 when she was a junior and he was her mentor in a Stanford entrepreneurship course. By the spring of 2013 Clougherty had broken off the relationship and filed charges at Stanford that Lonsdale had broken the Stanford policy against consensual relationships between students and faculty and that he had sexually assaulted and harassed her, which resulted in Lonsdale being banned from Stanford for 10 years. Lonsdale challenged Stanford's finding that he had had sexually assaulted and harassed her and Stanford rescinded that finding and the campus ban in the fall of 2015. Clougherty withdrew her suit that fall as well.

As of late 2016, Stanford had 2,153 tenure-line faculty, senior fellows, center fellows, and medical center faculty.

Stanford's current community of scholars includes:

Stanford's faculty and former faculty includes 46 Nobel laureates, 5 Fields Medalists as well as 14 winners of the Turing Award, the so-called "Nobel Prize in computer science", comprising one third of the awards given in its 44-year history. The university has 27 ACM fellows. It is also affiliated with 4 Gödel Prize winners, 4 Knuth Prize recipients, 10 IJCAI Computers and Thought Award winners, and about 15 Grace Murray Hopper Award winners for their work in the foundations of computer science. Stanford alumni have started many companies and, according to "Forbes", has produced the second highest number of billionaires of all universities.

13 Stanford alumni have won the Nobel Prize. As of 2019, 122 Stanford students or alumni have been named Rhodes Scholars.





</doc>
<doc id="26979" url="https://en.wikipedia.org/wiki?curid=26979" title="Substance">
Substance

Substance may refer to:






</doc>
<doc id="26980" url="https://en.wikipedia.org/wiki?curid=26980" title="Sun Microsystems">
Sun Microsystems

Sun Microsystems, Inc. (Sun for short) was an American company that sold computers, computer components, software, and information technology services and created the Java programming language, the Solaris operating system, ZFS, the Network File System (NFS), and SPARC. Sun contributed significantly to the evolution of several key computing technologies, among them Unix, RISC processors, thin client computing, and virtualized computing. Sun was founded on February 24, 1982. At its height, the Sun headquarters were in Santa Clara, California (part of Silicon Valley), on the former west campus of the Agnews Developmental Center.

On April 20, 2009, it was announced that Oracle Corporation would acquire Sun for 7.4 billion. The deal was completed on January 27, 2010.

Sun products included computer servers and workstations built on its own RISC-based SPARC processor architecture, as well as on x86-based AMD Opteron and Intel Xeon processors. Sun also developed its own storage systems and a suite of software products, including the Solaris operating system, developer tools, Web infrastructure software, and identity management applications. technologies included the Java platform and NFS. In general, Sun was a proponent of open systems, particularly Unix. It was also a major contributor to open-source software, as evidenced by its $1 billion purchase, in 2008, of MySQL, an open-source relational database management system. At various times, Sun had manufacturing facilities in several locations worldwide, including Newark, California; Hillsboro, Oregon; and Linlithgow, Scotland. However, by the time the company was acquired by Oracle, it had outsourced most manufacturing responsibilities.

The initial design for what became Sun's first Unix workstation, the Sun-1, was conceived by Andy Bechtolsheim when he was a graduate student at Stanford University in Palo Alto, California. Bechtolsheim originally designed the SUN workstation for the Stanford University Network communications project as a personal CAD workstation. It was designed around the Motorola 68000 processor with an advanced memory management unit (MMU) to support the Unix operating system with virtual memory support. He built the first ones from spare parts obtained from Stanford's Department of Computer Science and Silicon Valley supply houses.

On February 24, 1982, Scott McNealy, Andy Bechtolsheim, and Vinod Khosla, all Stanford graduate students, founded "Sun Microsystems". Bill Joy of Berkeley, a primary developer of the Berkeley Software Distribution (BSD), joined soon after and is counted as one of the original founders. The Sun name is derived from the initials of the Stanford University Network. Sun was profitable from its first quarter in July 1982.

By 1983 Sun was known for producing 68k-based systems with high-quality graphics that were the only computers other than DEC's VAX to run 4.2BSD. It licensed the computer design to other manufacturers, which typically used it to build Multibus-based systems running Unix from UniSoft. Sun's initial public offering was in 1986 under the stock symbol "SUNW", for "Sun Workstations" (later "Sun Worldwide"). The symbol was changed in 2007 to "JAVA"; Sun stated that the brand awareness associated with its Java platform better represented the company's current strategy.

Sun's logo, which features four interleaved copies of the word "sun" in the form of a rotationally symmetric ambigram, was designed by professor Vaughan Pratt, also of Stanford. The initial version of the logo was orange and had the sides oriented horizontally and vertically, but it was subsequently rotated to stand on one corner and re-colored purple, and later blue.

In the dot-com bubble, Sun began making much more money, and its shares rose dramatically. It also began spending much more, hiring workers and building itself out. Some of this was because of genuine demand, but much was from web start-up companies anticipating business that would never happen. In 2000, the bubble burst. Sales in Sun's important hardware division went into free-fall as customers closed shop and auctioned high-end servers.

Several quarters of steep losses led to executive departures, rounds of layoffs, and other cost cutting. In December 2001, the stock fell to the 1998, pre-bubble level of about $100. But it kept falling, faster than many other tech companies. A year later it had dipped below $10 (a tenth of what it was even in 1990) but bounced back to $20. In mid-2004, Sun closed their Newark, California, factory and consolidated all manufacturing to Hillsboro, Oregon and Linlithgow, Scotland. In 2006, the rest of the Newark campus was put on the market.

In 2004, Sun canceled two major processor projects which emphasized high instruction-level parallelism and operating frequency. Instead, the company chose to concentrate on processors optimized for multi-threading and multiprocessing, such as the UltraSPARC T1 processor (codenamed "Niagara"). The company also announced a collaboration with Fujitsu to use the Japanese company's processor chips in mid-range and high-end Sun servers. These servers were announced on April 17, 2007, as the M-Series, part of the SPARC Enterprise series.

In February 2005, Sun announced the Sun Grid, a grid computing deployment on which it offered utility computing services priced at US$1 per CPU/hour for processing and per GB/month for storage. This offering built upon an existing 3,000-CPU server farm used for internal R&D for over 10 years, which Sun marketed as being able to achieve 97% utilization. In August 2005, the first commercial use of this grid was announced for financial risk simulations which was later launched as its first software as a service product.

In January 2005, Sun reported a net profit of $19 million for fiscal 2005 second quarter, for the first time in three years. This was followed by net loss of $9 million on GAAP basis for the third quarter 2005, as reported on April 14, 2005. In January 2007, Sun reported a net GAAP profit of $126 million on revenue of $3.337 billion for its fiscal second quarter. Shortly following that news, it was announced that Kohlberg Kravis Roberts (KKR) would invest $700 million in the company.

Sun had engineering groups in Bangalore, Beijing, Dublin, Grenoble, Hamburg, Prague, St. Petersburg, Tel Aviv, Tokyo, Canberra and Trondheim.

In 2007–2008, Sun posted revenue of $13.8 billion and had $2 billion in cash. First-quarter 2008 losses were $1.68 billion; revenue fell 7% to $12.99 billion. Sun's stock lost 80% of its value November 2007 to November 2008, reducing the company's market value to $3 billion. With falling sales to large corporate clients, Sun announced plans to lay off 5,000 to 6,000 workers, or 15–18% of its work force. It expected to save $700 million to $800 million a year as a result of the moves, while also taking up to $600 million in charges.



As of May 11, 2009, the following shareholders held over 100,000 common shares of Sun and at $9.50 per share offered by Oracle, they received the amounts indicated when the acquisition closed.
For the first decade of Sun's history, the company positioned its products as technical workstations, competing successfully as a low-cost vendor during the Workstation Wars of the 1980s. It then shifted its hardware product line to emphasize servers and storage. High-level telecom control systems such as Operational Support Systems service predominantly used Sun equipment.

Sun originally used Motorola 68000 family central processing units for the Sun-1 through Sun-3 computer series. The Sun-1 employed a 68000 CPU, the Sun-2 series, a 68010. The Sun-3 series was based on the 68020, with the later Sun-3x using the 68030.

In 1987, the company began using "SPARC", a RISC processor architecture of its own design, in its computer systems, starting with the Sun-4 line. SPARC was initially a 32-bit architecture (SPARC V7) until the introduction of the SPARC V9 architecture in 1995, which added 64-bit extensions.

Sun has developed several generations of SPARC-based computer systems, including the SPARCstation, Ultra and Sun Blade series of workstations, and the SPARCserver, Netra, Enterprise and Sun Fire line of servers.

In the early 1990s the company began to extend its product line to include large-scale symmetric multiprocessing servers, starting with the four-processor SPARCserver 600MP. This was followed by the 8-processor SPARCserver 1000 and 20-processor SPARCcenter 2000, which were based on work done in conjunction with Xerox PARC. In 1995 the company introduced Sun Ultra series machines that were equipped with the first 64-bit implementation of SPARC processors (UltraSPARC). In the late 1990s the transformation of product line in favor of large 64-bit SMP systems was accelerated by the acquisition of Cray Business Systems Division from Silicon Graphics. Their 32-bit, 64-processor Cray Superserver 6400, related to the SPARCcenter, led to the 64-bit Sun Enterprise 10000 high-end server (otherwise known as "Starfire").

In September 2004 Sun made available systems with UltraSPARC IV which was the first multi-core SPARC processor. It was followed by UltraSPARC IV+ in September 2005 and its revisions with higher clock speeds in 2007. These CPUs were used in the most powerful, enterprise class high-end CC-NUMA servers developed by Sun, such as Sun Fire E25K.

In November 2005 Sun launched the UltraSPARC T1, notable for its ability to concurrently run 32 threads of execution on 8 processor cores. Its intent was to drive more efficient use of CPU resources, which is of particular importance in data centers, where there is an increasing need to reduce power and air conditioning demands, much of which comes from the heat generated by CPUs. The T1 was followed in 2007 by the UltraSPARC T2, which extended the number of threads per core from 4 to 8. Sun has open sourced the design specifications of both the T1 and T2 processors via the OpenSPARC project.

In 2006, Sun ventured into the "blade server" (high density rack-mounted systems) market with the Sun Blade (distinct from the Sun Blade workstation).

In April 2007 Sun released the SPARC Enterprise server products, jointly designed by Sun and Fujitsu and based on Fujitsu SPARC64 VI and later processors. The "M-class" SPARC Enterprise systems include high-end reliability and availability features. Later T-series servers have also been badged SPARC Enterprise rather than Sun Fire.

In April 2008 Sun released servers with UltraSPARC T2 Plus, which is an SMP capable version of UltraSPARC T2, available in 2 or 4 processor configurations. It was the first CoolThreads CPU with multi-processor capability and it made possible to build standard rack-mounted servers that could simultaneously process up to massive 256 CPU threads in hardware (Sun SPARC Enterprise T5440), which is considered a record in the industry.

Since 2010, all further development of Sun machines based on SPARC architecture (including new SPARC T-Series servers, SPARC T3 and T4 chips) is done as a part of Oracle Corporation hardware division.

In the late 1980s, Sun also marketed an Intel 80386-based machine, the Sun386i; this was designed to be a hybrid system, running SunOS but at the same time supporting DOS applications. This only remained on the market for a brief time. A follow-up "486i" upgrade was announced but only a few prototype units were ever manufactured.

Sun's brief first foray into x86 systems ended in the early 1990s, as it decided to concentrate on SPARC and retire the last Motorola systems and 386i products, a move dubbed by McNealy as "all the wood behind one arrowhead". Even so, Sun kept its hand in the x86 world, as a release of Solaris for PC compatibles began shipping in 1993.

In 1997 Sun acquired Diba, Inc., followed later by the acquisition of Cobalt Networks in 2000, with the aim of building "network appliances" (single function computers meant for consumers). Sun also marketed a Network Computer (a term popularized and eventually trademarked by Oracle); the JavaStation was a diskless system designed to run Java applications.

Although none of these business initiatives were particularly successful, the Cobalt purchase gave Sun a toehold for its return to the x86 hardware market. In 2002, Sun introduced its first general purpose x86 system, the LX50, based in part on previous Cobalt system expertise. This was also Sun's first system announced to support Linux as well as Solaris.

In 2003, Sun announced a strategic alliance with AMD to produce x86/x64 servers based on AMD's Opteron processor; this was followed shortly by Sun's acquisition of Kealia, a startup founded by original Sun founder Andy Bechtolsheim, which had been focusing on high-performance AMD-based servers.

The following year, Sun launched the Opteron-based Sun Fire V20z and V40z servers, and the Java Workstation W1100z and W2100z workstations.

On September 12, 2005, Sun unveiled a new range of Opteron-based servers: the Sun Fire X2100, X4100 and X4200 servers. These were designed from scratch by a team led by Bechtolsheim to address heat and power consumption issues commonly faced in data centers. In July 2006, the Sun Fire X4500 and X4600 systems were introduced, extending a line of x64 systems that support not only Solaris, but also Linux and Microsoft Windows.

On January 22, 2007, Sun announced a broad strategic alliance with Intel. Intel endorsed Solaris as a mainstream operating system and as its mission critical Unix for its Xeon processor-based systems, and contributed engineering resources to OpenSolaris. Sun began using the Intel Xeon processor in its x64 server line, starting with the Sun Blade X6250 server module introduced in June 2007.

On May 5, 2008, AMD announced its Operating System Research Center (OSRC) expanded its focus to include optimization to Sun's OpenSolaris and xVM virtualization products for AMD based processors.

Although Sun was initially known as a hardware company, its software history began with its founding in 1982; co-founder Bill Joy was one of the leading Unix developers of the time, having contributed the vi editor, the C shell, and significant work developing TCP/IP and the BSD Unix OS. Sun later developed software such as the Java programming language and acquired software such as StarOffice, VirtualBox and MySQL.

Sun used community-based and open-source licensing of its major technologies, and for its support of its products with other open source technologies. GNOME-based desktop software called Java Desktop System (originally code-named "Madhatter") was distributed for the Solaris operating system, and at one point for Linux. Sun supported its Java Enterprise System (a middleware stack) on Linux. It released the source code for Solaris under the open-source Common Development and Distribution License, via the OpenSolaris community. Sun's positioning includes a commitment to indemnify users of some software from intellectual property disputes concerning that software. It offers support services on a variety of pricing bases, including per-employee and per-socket.

A 2006 report prepared for the EU by UNU-MERIT stated that Sun was the largest corporate contributor to open source movements in the world. According to this report, Sun's open source contributions exceed the combined total of the next five largest commercial contributors.

Sun is best known for its Unix systems, which have a reputation for system stability and a consistent design philosophy.

Sun's first workstation shipped with UniSoft V7 Unix. Later in 1982 Sun began providing SunOS, a customized 4.1BSD Unix, as the operating system for its workstations.

In the late 1980s, AT&T tapped Sun to help them develop the next release of their branded UNIX, and in 1988 announced they would purchase up to a 20% stake in Sun. UNIX System V Release 4 (SVR4) was jointly developed by AT&T and Sun; Sun used SVR4 as the foundation for Solaris 2.x, which became the successor to SunOS 4.1.x (later retrospectively named Solaris 1.x). By the mid-1990s, the ensuing Unix wars had largely subsided, AT&T had sold off their Unix interests, and the relationship between the two companies was significantly reduced.

From 1992 Sun also sold Interactive Unix, an operating system it acquired when it bought Interactive Systems Corporation from Eastman Kodak Company. This was a popular Unix variant for the PC platform and a major competitor to market leader SCO UNIX. Sun's focus on Interactive Unix diminished in favor of Solaris on both SPARC and x86 systems; it was dropped as a product in 2001.

Sun dropped the Solaris 2.x version numbering scheme after the Solaris 2.6 release (1997); the following version was branded Solaris 7. This was the first 64-bit release, intended for the new UltraSPARC CPUs based on the SPARC V9 architecture. Within the next four years, the successors Solaris 8 and Solaris 9 were released in 2000 and 2002 respectively.

Following several years of difficult competition and loss of server market share to competitors' Linux-based systems, Sun began to include Linux as part of its strategy in 2002. Sun supported both Red Hat Enterprise Linux and SUSE Linux Enterprise Server on its x64 systems; companies such as Canonical Ltd., Wind River Systems and MontaVista also supported their versions of Linux on Sun's SPARC-based systems.

In 2004, after having cultivated a reputation as one of Microsoft's most vocal antagonists, Sun entered into a joint relationship with them, resolving various legal entanglements between the two companies and receiving US$1.95 billion in settlement payments from them. Sun supported Microsoft Windows on its x64 systems, and announced other collaborative agreements with Microsoft, including plans to support each other's virtualization environments.

In 2005, the company released Solaris 10. The new version included a large number of enhancements to the operating system, as well as very novel features, previously unseen in the industry. Solaris 10 update releases continued through the next 8 years, the last release from Sun Microsystems being Solaris 10 10/09. The following updates were released by Oracle under the new license agreement; the final release is Solaris 10 1/13.

Previously, Sun offered a separate variant of Solaris called Trusted Solaris, which included augmented security features such as multilevel security and a least privilege access model. Solaris 10 included many of the same capabilities as Trusted Solaris at the time of its initial release; Solaris 10 11/06 included Solaris Trusted Extensions, which give it the remaining capabilities needed to make it the functional successor to Trusted Solaris.

After releasing Solaris 10, its source code was opened under CDDL free software license and developed in open with contributing Opensolaris community through SXCE that used SVR4 .pkg packaging and supported Opensolaris releases that used IPS.
Following acquisition of Sun by Oracle , Opensolaris continued to develop in open under illumos with illumos distributions.

Oracle Corporation continued to develop OpenSolaris into next Solaris release, changing back the license to proprietary, and released it as Oracle Solaris 11 in November 2011.

The Java platform was developed at Sun by James Gosling in the early 1990s with the objective of allowing programs to function regardless of the device they were used on, sparking the slogan "Write once, run anywhere" (WORA). While this objective was not entirely achieved (prompting the riposte "Write once, debug everywhere"), Java is regarded as being largely hardware- and operating system-independent.

Java was initially promoted as a platform for client-side "applets" running inside web browsers. Early examples of Java applications were the HotJava web browser and the HotJava Views suite. However, since then Java has been more successful on the server side of the Internet.

The platform consists of three major parts: the Java programming language, the Java Virtual Machine (JVM), and several Java Application Programming Interfaces (APIs). The design of the Java platform is controlled by the vendor and user community through the Java Community Process (JCP).

Java is an object-oriented programming language. Since its introduction in late 1995, it became one of the world's most popular programming languages.

Java programs are compiled to byte code, which can be executed by any JVM, regardless of the environment.

The Java APIs provide an extensive set of library routines. These APIs evolved into the "Standard Edition" (Java SE), which provides basic infrastructure and GUI functionality; the "Enterprise Edition" (Java EE), aimed at large software companies implementing enterprise-class application servers; and the "Micro Edition" (Java ME), used to build software for devices with limited resources, such as mobile devices.

On November 13, 2006, Sun announced it would be licensing its Java implementation under the GNU General Public License; it released its Java compiler and JVM at that time.

In February 2009 Sun entered a battle with Microsoft and Adobe Systems, which promoted rival platforms to build software applications for the Internet. JavaFX was a development platform for music, video and other applications that builds on the Java programming language.

In 1999, Sun acquired the German software company Star Division and with it the office suite StarOffice, which Sun later released as OpenOffice.org under both GNU LGPL and the SISSL (Sun Industry Standards Source License). OpenOffice.org supported Microsoft Office file formats (though not perfectly), was available on many platforms (primarily Linux, Microsoft Windows, Mac OS X, and Solaris) and was used in the open source community.

The principal differences between StarOffice and OpenOffice.org were that StarOffice was supported by Sun, was available as either a single-user retail box kit or as per-user blocks of licensing for the enterprise, and included a wider range of fonts and document templates and a commercial quality spellchecker. StarOffice also contained commercially licensed functions and add-ons; in OpenOffice.org these were either replaced by open-source or free variants, or are not present at all. Both packages had native support for the OpenDocument format.

In 2007, Sun announced the Sun xVM virtualization and datacenter automation product suite for commodity hardware. Sun also acquired VirtualBox in 2008. Earlier virtualization technologies from Sun like "Dynamic System Domains" and "Dynamic Reconfiguration" were specifically designed for high-end SPARC servers, and Logical Domains only supports the UltraSPARC T1/T2/T2 Plus server platforms. Sun marketed "Sun Ops Center" provisioning software for datacenter automation.

On the client side, Sun offered virtual desktop solutions. Desktop environments and applications could be hosted in a datacenter, with users accessing these environments from a wide range of client devices, including Microsoft Windows PCs, Sun Ray virtual display clients, Apple Macintoshes, PDAs or any combination of supported devices. A variety of networks were supported, from LAN to WAN or the public Internet. Virtual desktop products included Sun Ray Server Software, Sun Secure Global Desktop and Sun Virtual Desktop Infrastructure.

Sun acquired MySQL AB, the developer of the MySQL database in 2008 for US$1 billion. CEO Jonathan Schwartz mentioned in his blog that optimizing the performance of MySQL was one of the priorities of the acquisition. In February 2008, Sun began to publish results of the MySQL performance optimization work. Sun contributed to the PostgreSQL project. On the Java platform, Sun contributed to and supported Java DB.

Sun offered other software products for software development and infrastructure services. Many were developed in house; others came from acquisitions, including Tarantella, Waveset Technologies, SeeBeyond, and Vaau. Sun acquired many of the Netscape non-browser software products as part a deal involving Netscape's merger with AOL. These software products were initially offered under the "iPlanet" brand; once the Sun-Netscape alliance ended, they were re-branded as "Sun ONE" (Sun Open Network Environment), and then the "Sun Java System".

Sun's middleware product was branded as the "Java Enterprise System" (or JES), and marketed for web and application serving, communication, calendaring, directory, identity management and service-oriented architecture. Sun's Open ESB and other software suites were available free of charge on systems running Solaris, Red Hat Enterprise Linux, HP-UX, and Windows, with support available optionally.

Sun developed data center management software products, which included the "Solaris Cluster" high availability software, and a grid management package called "Sun Grid Engine" and firewall software such as SunScreen.
For Network Equipment Providers and telecommunications customers, Sun developed the Sun Netra High-Availability Suite.

Sun produced compilers and development tools under the "Sun Studio" brand, for building and developing Solaris and Linux applications.
Sun entered the software as a service (SaaS) market with zembly, a social cloud-based computing platform and Project Kenai, an open-source project hosting service.

Sun sold its own storage systems to complement its system offerings; it has also made several storage-related acquisitions.
On June 2, 2005, Sun announced it would purchase Storage Technology Corporation (StorageTek) for US$4.1 billion in cash, or $37.00 per share, a deal completed in August 2005.

In 2006, Sun introduced the Sun StorageTek 5800 System, the first application-aware programmable storage solution. In 2008, Sun contributed the source code of the StorageTek 5800 System under the BSD license.

Sun announced the Sun Open Storage platform in 2008 built with open source technologies.
In late 2008 Sun announced the Sun Storage 7000 Unified Storage systems (codenamed Amber Road). Transparent placement of data in the systems' solid-state drives (SSD) and conventional hard drives was managed by ZFS to take advantage of the speed of SSDs and the economy of conventional hard disks.

Other storage products included Sun Fire X4500 storage server and SAM-QFS filesystem and storage management software.

Sun marketed the Sun Constellation System for high-performance computing (HPC). Even before the introduction of the Sun Constellation System in 2007, Sun's products were in use in many of the TOP500 systems and supercomputing centers:

The "Sun HPC ClusterTools" product was a set of Message Passing Interface (MPI) libraries and tools for running parallel jobs on Solaris HPC clusters. Beginning with version 7.0, Sun switched from its own implementation of MPI to Open MPI, and donated engineering resources to the Open MPI project.

Sun was a participant in the OpenMP language committee. Sun Studio compilers and tools implemented the OpenMP specification for shared memory parallelization.

In 2006, Sun built the "TSUBAME supercomputer", which was until June 2008 the fastest supercomputer in Asia. Sun built "Ranger" at the Texas Advanced Computing Center (TACC) in 2007. Ranger had a peak performance of over 500 TFLOPS, and was the 6th most powerful supercomputer on the TOP500 list in November 2008.
Sun announced an OpenSolaris distribution that integrated Sun's HPC products with others.

Notable Sun employees included John Gilmore, Whitfield Diffie, Radia Perlman, and Marc Tremblay. Sun was an early advocate of Unix-based networked computing, promoting TCP/IP and especially NFS, as reflected in the company's motto "The Network Is The Computer", coined by John Gage. James Gosling led the team which developed the Java programming language. Jon Bosak led the creation of the XML specification at W3C.

Sun staff published articles on the company's blog site. Staff were encouraged to use the site to blog on any aspect of their work or personal life, with few restrictions placed on staff, other than commercially confidential material. Jonathan I. Schwartz was one of the first CEOs of large companies to regularly blog; his postings were frequently quoted and analyzed in the press. In 2005, Sun Microsystems was one of the first Fortune 500 companies that instituted a formal Social Media program.

Sun was sold to Oracle Corporation in 2009 for $5.6B.
Sun's staff were asked to share anecdotes about their experiences at Sun. A web site containing videos, stories, and photographs from 27 years at Sun was made available on September 2, 2009.
In October, Sun announced a second round of thousands of employees to be laid off, blamed partially on delays in approval of the merger.
The transaction was completed in early 2010.
In January 2011, Oracle agreed to pay $46 million to settle charges that it submitted false claims to US federal government agencies and paid "kickbacks" to systems integrators.
In February 2011, Sun's former Menlo Park, California, campus of about was sold, and it was announced that it would become headquarters for Facebook.
The sprawling facility built around an enclosed courtyard had been nicknamed "Sun Quentin". On September 1, 2011, Sun India legally became part of Oracle. It had been delayed due to legal issues in Indian court.





</doc>
<doc id="26981" url="https://en.wikipedia.org/wiki?curid=26981" title="Solaris">
Solaris

Solaris, a Latin word meaning "pertaining to the sun", may refer to:








</doc>
<doc id="26983" url="https://en.wikipedia.org/wiki?curid=26983" title="Saladin">
Saladin

An-Nasir Salah ad-Din Yusuf ibn Ayyub ( / ALA-LC: "Ṣalāḥ ad-Dīn Yūsuf ibn Ayyūb"; / ALA-LC: "Selahedînê Eyûbî"), known as Salah ad-Din or Saladin (; 11374 March 1193), was the first sultan of Egypt and Syria and the founder of the Ayyubid dynasty. A Sunni Muslim of Kurdish ethnicity, Saladin led the Muslim military campaign against the Crusader states in the Levant. At the height of his power, his sultanate included Egypt, Syria, Upper Mesopotamia, the Hejaz, Yemen and other parts of North Africa.

He was originally sent to Fatimid Egypt in 1164 alongside his uncle Shirkuh, a general of the Zengid army, on the orders of their lord Nur ad-Din to help restore Shawar as vizier of the teenage Fatimid caliph . A power struggle ensued between Shirkuh and Shawar after the latter was reinstated. Saladin, meanwhile, climbed the ranks of the Fatimid government by virtue of his military successes against Crusader assaults against its territory and his personal closeness to al-Adid. After Shawar was assassinated and Shirkuh died in 1169, al-Adid appointed Saladin vizier, a rare nomination of a Sunni Muslim to such an important position in the Isma'ili Shia caliphate. During his tenure as vizier, Saladin began to undermine the Fatimid establishment and, following al-Adid's death in 1171, he abolished the and realigned the country's allegiance with the Sunni, Baghdad-based Abbasid Caliphate.

In the following years, he led forays against the Crusaders in Palestine, commissioned the successful conquest of Yemen, and staved off pro-Fatimid rebellions in Upper Egypt. Not long after Nur ad-Din's death in 1174, Saladin launched his conquest of Syria, peacefully entering Damascus at the request of its governor. By mid-1175, Saladin had conquered Hama and Homs, inviting the animosity of other Zengid lords, the official rulers of Syria's various regions. Soon after, he defeated the Zengid army at the Battle of the Horns of Hama and was thereafter proclaimed the "Sultan of Egypt and Syria" by the Abbasid caliph al-Mustadi. Saladin made further conquests in northern Syria and Jazira, escaping two attempts on his life by Assassins, before returning to Egypt in 1177 to address issues there. By 1182, Saladin had completed the conquest of Muslim Syria after capturing Aleppo, but ultimately failed to take over the Zengid stronghold of Mosul.

Under Saladin's command, the Ayyubid army defeated the Crusaders at the decisive Battle of Hattin in 1187, and thereafter wrested control of Palestine—including the city of Jerusalem—from the Crusaders, who had conquered the area 88 years earlier. Although the Crusader Kingdom of Jerusalem continued to exist until the late 13th century, its defeat at Hattin marked a turning point in its conflict with the Muslim powers of the region. Saladin died in Damascus in 1193, having given away much of his personal wealth to his subjects. He is buried in a mausoleum adjacent to the Umayyad Mosque. Saladin has become a prominent figure in Muslim, Arab, Turkish and Kurdish culture, and he has often been described as being the most famous Kurd in history.

Saladin was born in Tikrit in modern-day Iraq. His personal name was "Yusuf"; "Salah ad-Din" is a "laqab", an honorific epithet, meaning "Righteousness of the Faith." His family was most likely of Kurdish ancestry, and had originated from the village of Ajdanakan near the city of Dvin in central Armenia. The Rawadiya tribe he hailed from had been partially assimilated into the Arabic-speaking world by this time. In 1132, the defeated army of Imad ad-Din Zengi, the ruler of Mosul, found their retreat blocked by the Tigris River opposite the fortress of Tikrit, where Saladin's father, Najm ad-Din Ayyub served as the warden. Ayyub provided ferries for the army and gave them refuge in Tikrit. Mujahed al-Din Bihruz, a former Greek slave who had been appointed as the military governor of northern Mesopotamia for his service to the Seljuks, reprimanded Ayyub for giving Zengi refuge and in 1137 banished Ayyub from Tikrit after his brother Asad al-Din Shirkuh killed a friend of Bihruz. According to Baha ad-Din ibn Shaddad, Saladin was born on the same night that his family left Tikrit. In 1139, Ayyub and his family moved to Mosul, where Imad ad-Din Zengi acknowledged his debt and appointed Ayyub commander of his fortress in Baalbek. After the death of Zengi in 1146, his son, Nur ad-Din, became the regent of Aleppo and the leader of the Zengids.

Saladin, who now lived in Damascus, was reported to have a particular fondness for the city, but information on his early childhood is scarce. About education, Saladin wrote "children are brought up in the way in which their elders were brought up." According to his biographers, Anne-Marie Eddé and al-Wahrani, Saladin was able to answer questions on Euclid, the Almagest, arithmetic, and law, but this was an academic ideal and it was study of the Qur'an and the "sciences of religion" that linked him to his contemporaries. Several sources claim that during his studies he was more interested in religion than joining the military.<ref name="Who2 Biography: Saladin, Sultan/Military Leader"></ref> Another factor which may have affected his interest in religion was that, during the First Crusade, Jerusalem was taken by the Christians. In addition to Islam, Saladin had a knowledge of the genealogies, biographies, and histories of the Arabs, as well as the bloodlines of Arabian horses. More significantly, he knew the "Hamasah" of Abu Tammam by heart. He spoke Kurdish and Arabic.

Saladin's military career began under the tutelage of his uncle Asad al-Din Shirkuh, a prominent military commander under Nur ad-Din, the Zengid emir of Damascus and Aleppo and the most influential teacher of Saladin. In 1163, the vizier to the Fatimid caliph al-Adid, Shawar, had been driven out of Egypt by his rival Dirgham, a member of the powerful Banu Ruzzaik tribe. He asked for military backing from Nur ad-Din, who complied and, in 1164, sent Shirkuh to aid Shawar in his expedition against Dirgham. Saladin, at age 26, went along with them. After Shawar was successfully reinstated as vizier, he demanded that Shirkuh withdraw his army from Egypt for a sum of 30,000 gold dinars, but he refused, insisting it was Nur ad-Din's will that he remain. Saladin's role in this expedition was minor, and it is known that he was ordered by Shirkuh to collect stores from Bilbais prior to its siege by a combined force of Crusaders and Shawar's troops.

After the sacking of Bilbais, the Crusader-Egyptian force and Shirkuh's army were to engage in a battle on the desert border of the River Nile, just west of Giza. Saladin played a major role, commanding the right wing of the Zengid army, while a force of Kurds commanded the left, and Shirkuh was stationed in the center. Muslim sources at the time, however, put Saladin in the "baggage of the centre" with orders to lure the enemy into a trap by staging a feigned retreat. The Crusader force enjoyed early success against Shirkuh's troops, but the terrain was too steep and sandy for their horses, and commander Hugh of Caesarea was captured while attacking Saladin's unit. After scattered fighting in little valleys to the south of the main position, the Zengid central force returned to the offensive; Saladin joined in from the rear.

The battle ended in a Zengid victory, and Saladin is credited with having helped Shirkuh in one of the "most remarkable victories in recorded history", according to Ibn al-Athir, although more of Shirkuh's men were killed and the battle is considered by most sources as not a total victory. Saladin and Shirkuh moved towards Alexandria where they were welcomed, given money, arms and provided a base. Faced by a superior Crusader-Egyptian force attempting to besiege the city, Shirkuh split his army. He and the bulk of his force withdrew from Alexandria, while Saladin was left with the task of guarding the city.

Shirkuh was in a power struggle over Egypt with Shawar and Amalric I of the Kingdom of Jerusalem, in which Shawar requested Amalric's assistance. In 1169, Shawar was reportedly assassinated by Saladin, and Shirkuh died later that year. Nur ad-Din chose a successor for Shirkuh, but al-Adid appointed Saladin to replace Shawar as vizier.

The reasoning behind the Shia caliph al-Adid's selection of Saladin, a Sunni, varies. Ibn al-Athir claims that the caliph chose him after being told by his advisers that "there is no one weaker or younger" than Saladin, and "not one of the emirs [commanders] obeyed him or served him". However, according to this version, after some bargaining, he was eventually accepted by the majority of the emirs. Al-Adid's advisers were also suspected of promoting Saladin in an attempt to split the Syria-based Zengids. Al-Wahrani wrote that Saladin was selected because of the reputation of his family in their "generosity and military prowess". Imad ad-Din wrote that after the brief mourning period for Shirkuh, during which "opinions differed", the Zengid emirs decided upon Saladin and forced the caliph to "invest him as vizier". Although positions were complicated by rival Muslim leaders, the bulk of the Syrian commanders supported Saladin because of his role in the Egyptian expedition, in which he gained a record of military qualifications.

Inaugurated as vizier on 26 March, Saladin repented "wine-drinking and turned from frivolity to assume the dress of religion", according to Arabic sources of the time. Having gained more power and independence than ever before in his career, he still faced the issue of ultimate loyalty between al-Adid and Nur ad-Din. Later in the year, a group of Egyptian soldiers and emirs attempted to assassinate Saladin, but having already known of their intentions thanks to his intelligence chief Ali ibn Safyan, he had the chief conspirator, Naji, Mu'tamin al-Khilafa—the civilian controller of the Fatimid Palace—arrested and killed. The day after, 50,000 Black African soldiers from the regiments of the Fatimid army opposed to Saladin's rule, along with a number of Egyptian emirs and commoners, staged a revolt. By 23 August, Saladin had decisively quelled the uprising, and never again had to face a military challenge from Cairo.

Towards the end of 1169, Saladin, with reinforcements from Nur ad-Din, defeated a massive Crusader-Byzantine force near Damietta. Afterward, in the spring of 1170, Nur ad-Din sent Saladin's father to Egypt in compliance with Saladin's request, as well as encouragement from the Baghdad-based Abbasid caliph, al-Mustanjid, who aimed to pressure Saladin in deposing his rival caliph, al-Adid. Saladin himself had been strengthening his hold on Egypt and widening his support base there. He began granting his family members high-ranking positions in the region; he ordered the construction of a college for the Maliki branch of Sunni Islam in the city, as well as one for the Shafi'i denomination to which he belonged in al-Fustat.

After establishing himself in Egypt, Saladin launched a campaign against the Crusaders, besieging Darum in 1170. Amalric withdrew his Templar garrison from Gaza to assist him in defending Darum, but Saladin evaded their force and captured Gaza in 1187. In 1191 Saladin destroyed the fortifications in Gaza build by King Baldwin III for the Knights Templar. It is unclear exactly when, but during that same year, he attacked and captured the Crusader castle of Eilat, built on an island off the head of the Gulf of Aqaba. It did not pose a threat to the passage of the Muslim navy, but could harass smaller parties of Muslim ships and Saladin decided to clear it from his path.

According to Imad ad-Din, Nur ad-Din wrote to Saladin in June 1171, telling him to reestablish the Abbasid caliphate in Egypt, which Saladin coordinated two months later after additional encouragement by Najm ad-Din al-Khabushani, the Shafi'i "faqih", who vehemently opposed Shia rule in the country. Several Egyptian emirs were thus killed, but al-Adid was told that they were killed for rebelling against him. He then fell ill, or was poisoned according to one account. While ill, he asked Saladin to pay him a visit to request that he take care of his young children, but Saladin refused, fearing treachery against the Abbasids, and is said to have regretted his action after realizing what al-Adid had wanted. He died on 13 September, and five days later, the Abbasid "khutba" was pronounced in Cairo and al-Fustat, proclaiming al-Mustadi as caliph.

On 25 September, Saladin left Cairo to take part in a joint attack on Kerak and Montreal, the desert castles of the Kingdom of Jerusalem, with Nur ad-Din who would attack from Syria. Prior to arriving at Montreal, Saladin however withdrew back to Cairo as he received the reports that in his absence the Crusader leaders had increased their support to the traitors inside Egypt to attack Saladin from within and lessen his power especially the Fatimid who started plotting to restore their past glory. Because of this, Nur ad-Din went on alone.

During the summer of 1173, a Nubian army along with a contingent of Armenian refugees were reported on the Egyptian border, preparing for a siege against Aswan. The emir of the city had requested Saladin's assistance and was given reinforcements under Turan-Shah, Saladin's brother. Consequently, the Nubians departed; but returned in 1173 and were again driven off. This time, Egyptian forces advanced from Aswan and captured the Nubian town of Ibrim. Saladin sent a gift to Nur ad-Din, who had been his friend and teacher, 60,000 dinars, "wonderful manufactured goods", some jewels, and an elephant. While transporting these goods to Damascus, Saladin took the opportunity to ravage the Crusader countryside. He did not press an attack against the desert castles, but attempted to drive out the Muslim Bedouins who lived in Crusader territory with the aim of depriving the Franks of guides.

On 31 July 1173, Saladin's father Ayyub was wounded in a horse-riding accident, ultimately causing his death on 9 August. In 1174, Saladin sent Turan-Shah to conquer Yemen to allocate it and its port Aden to the territories of the Ayyubid Dynasty.

In the early summer of 1174, Nur ad-Din was mustering an army, sending summons to Mosul, Diyar Bakr, and the Jazira in an apparent preparation of attack against Saladin's Egypt. The Ayyubids held a council upon the revelation of these preparations to discuss the possible threat and Saladin collected his own troops outside Cairo. On 15 May, Nur ad-Din died after falling ill the previous week and his power was handed to his eleven-year-old son as-Salih Ismail al-Malik. His death left Saladin with political independence and in a letter to as-Salih, he promised to "act as a sword" against his enemies and referred to the death of his father as an "earthquake shock".

In the wake of Nur ad-Din's death, Saladin faced a difficult decision; he could move his army against the Crusaders from Egypt or wait until invited by as-Salih in Syria to come to his aid and launch a war from there. He could also take it upon himself to annex Syria before it could possibly fall into the hands of a rival, but he feared that attacking a land that formerly belonged to his master—forbidden in the Islamic principles in which he believed—could portray him as hypocritical, thus making him unsuitable for leading the war against the Crusaders. Saladin saw that in order to acquire Syria, he either needed an invitation from as-Salih, or to warn him that potential anarchy could give rise to danger from the Crusaders.

When as-Salih was removed to Aleppo in August, Gumushtigin, the emir of the city and a captain of Nur ad-Din's veterans, assumed the guardianship over him. The emir prepared to unseat all his rivals in Syria and the Jazira, beginning with Damascus. In this emergency, the emir of Damascus appealed to Saif al-Din of Mosul (a cousin of Gumushtigin) for assistance against Aleppo, but he refused, forcing the Syrians to request the aid of Saladin, who complied. Saladin rode across the desert with 700 picked horsemen, passing through al-Kerak then reaching Bosra. According to his own account, was joined by "emirs, soldiers, and Bedouins—the emotions of their hearts to be seen on their faces." On 23 November, he arrived in Damascus amid general acclamation and rested at his father's old home there, until the gates of the Citadel of Damascus, whose commander Raihan initially refused to surrender, were opened to Saladin four days later, after a brief siege by his brother Tughtakin ibn Ayyub. He installed himself in the castle and received the homage and salutations of the inhabitants.

Leaving his brother Tughtakin ibn Ayyub as Governor of Damascus, Saladin proceeded to reduce other cities that had belonged to Nur al-Din, but were now practically independent. His army conquered Hama with relative ease, but avoided attacking Homs because of the strength of its citadel. Saladin moved north towards Aleppo, besieging it on 30 December after Gumushtigin refused to abdicate his throne. As-Salih, fearing capture by Saladin, came out of his palace and appealed to the inhabitants not to surrender him and the city to the invading force. One of Saladin's chroniclers claimed "the people came under his spell".

Gumushtigin requested Rashid ad-Din Sinan, chief "da'i" of the Assassins of Syria, who were already at odds with Saladin since he replaced the Fatimids of Egypt, to assassinate Saladin in his camp. On 11 May 1175, a group of thirteen Assassins easily gained admission into Saladin's camp, but were detected immediately before they carried out their attack by Nasih al-Din Khumartekin of Abu Qubays. One was killed by one of Saladin's generals and the others were slain while trying to escape. To deter Saladin's progress, Raymond of Tripoli gathered his forces by Nahr al-Kabir, where they were well placed for an attack on Muslim territory. Saladin later moved toward Homs instead, but retreated after being told a relief force was being sent to the city by Saif al-Din.

Meanwhile, Saladin's rivals in Syria and Jazira waged a propaganda war against him, claiming he had "forgotten his own condition [servant of Nur ad-Din]" and showed no gratitude for his old master by besieging his son, rising "in rebellion against his Lord". Saladin aimed to counter this propaganda by ending the siege, claiming that he was defending Islam from the Crusaders; his army returned to Hama to engage a Crusader force there. The Crusaders withdrew beforehand and Saladin proclaimed it "a victory opening the gates of men's hearts". Soon after, Saladin entered Homs and captured its citadel in March 1175, after stubborn resistance from its defenders.

Saladin's successes alarmed Saif al-Din. As head of the Zengids, including Gumushtigin, he regarded Syria and Mesopotamia as his family estate and was angered when Saladin attempted to usurp his dynasty's holdings. Saif al-Din mustered a large army and dispatched it to Aleppo, whose defenders anxiously had awaited them. The combined forces of Mosul and Aleppo marched against Saladin in Hama. Heavily outnumbered, Saladin initially attempted to make terms with the Zengids by abandoning all conquests north of the Damascus province, but they refused, insisting he return to Egypt. Seeing that confrontation was unavoidable, Saladin prepared for battle, taking up a superior position at the Horns of Hama, hills by the gorge of the Orontes River. On 13 April 1175, the Zengid troops marched to attack his forces, but soon found themselves surrounded by Saladin's Ayyubid veterans, who crushed them. The battle ended in a decisive victory for Saladin, who pursued the Zengid fugitives to the gates of Aleppo, forcing as-Salih's advisers to recognize Saladin's control of the provinces of Damascus, Homs and Hama, as well as a number of towns outside Aleppo such as Ma'arat al-Numan.

After his victory against the Zengids, Saladin proclaimed himself king and suppressed the name of as-Salih in Friday prayers and Islamic coinage. From then on, he ordered prayers in all the mosques of Syria and Egypt as the sovereign king and he issued at the Cairo mint gold coins bearing his official title—"al-Malik an-Nasir Yusuf Ayyub, ala ghaya" "the King Strong to Aid, Joseph son of Job; exalted be the standard." The Abbasid caliph in Baghdad graciously welcomed Saladin's assumption of power and declared him "Sultan of Egypt and Syria". The Battle of Hama did not end the contest for power between the Ayyubids and the Zengids, with the final confrontation occurring in the spring of 1176. Saladin had gathered massive reinforcements from Egypt while Saif al-Din was levying troops among the minor states of Diyarbakir and al-Jazira. When Saladin crossed the Orontes, leaving Hama, the sun was eclipsed. He viewed this as an omen, but he continued his march north. He reached the Sultan's Mound, roughly from Aleppo, where his forces encountered Saif al-Din's army. A hand-to-hand fight ensued and the Zengids managed to plow Saladin's left wing, driving it before him, when Saladin himself charged at the head of the Zengid guard. The Zengid forces panicked and most of Saif al-Din's officers ended up being killed or captured—Saif al-Din narrowly escaped. The Zengid army's camp, horses, baggage, tents and stores were seized by the Ayyubids. The Zengid prisoners of war, however, were given gifts and freed. All of the booty from the Ayyubid victory was accorded to the army, Saladin not keeping anything himself.

He continued towards Aleppo, which still closed its gates to him, halting before the city. On the way, his army took Buza'a, then captured Manbij. From there, they headed west to besiege the fortress of A'zaz on 15 May. Several days later, while Saladin was resting in one of his captain's tents, an Assassin rushed forward at him and struck at his head with a knife. The cap of his head armour was not penetrated and he managed to grip the Assassin's hand—the dagger only slashing his gambeson—and the assailant was soon killed. Saladin was unnerved at the attempt on his life, which he accused Gumushtugin and the Assassins of plotting, and so increased his efforts in the siege.

A'zaz capitulated on 21 June, and Saladin then hurried his forces to Aleppo to punish Gumushtigin. His assaults were again resisted, but he managed to secure not only a truce, but a mutual alliance with Aleppo, in which Gumushtigin and as-Salih were allowed to continue their hold on the city and in return, they recognized Saladin as the sovereign over all of the dominions he conquered. The "emirs" of Mardin and Keyfa, the Muslim allies of Aleppo, also recognised Saladin as the King of Syria. When the treaty was concluded, the younger sister of as-Salih came to Saladin and requested the return of the Fortress of A'zaz; he complied and escorted her back to the gates of Aleppo with numerous presents.

Saladin had by now agreed truces with his Zengid rivals and the Kingdom of Jerusalem (the latter occurred in the summer of 1175), but faced a threat from the Isma'ili sect known as the Assassins, led by Rashid ad-Din Sinan. Based in the an-Nusayriyah Mountains, they commanded nine fortresses, all built on high elevations. As soon as he dispatched the bulk of his troops to Egypt, Saladin led his army into the an-Nusayriyah range in August 1176. He retreated the same month, after laying waste to the countryside, but failing to conquer any of the forts. Most Muslim historians claim that Saladin's uncle, the governor of Hama, mediated a peace agreement between him and Sinan.

Saladin had his guards supplied with link lights and had chalk and cinders strewed around his tent outside Masyaf—which he was besieging—to detect any footsteps by the Assassins. According to this version, one night Saladin's guards noticed a spark glowing down the hill of Masyaf and then vanishing among the Ayyubid tents. Presently, Saladin awoke to find a figure leaving the tent. He saw that the lamps were displaced and beside his bed laid hot scones of the shape peculiar to the Assassins with a note at the top pinned by a poisoned dagger. The note threatened that he would be killed if he didn't withdraw from his assault. Saladin gave a loud cry, exclaiming that Sinan himself was the figure that had left the tent.

Another version claims that Saladin hastily withdrew his troops from Masyaf because they were urgently needed to fend off a Crusader force in the vicinity of Mount Lebanon. In reality, Saladin sought to form an alliance with Sinan and his Assassins, consequently depriving the Crusaders of a potent ally against him. Viewing the expulsion of the Crusaders as a mutual benefit and priority, Saladin and Sinan maintained cooperative relations afterwards, the latter dispatching contingents of his forces to bolster Saladin's army in a number of decisive subsequent battlefronts.

After leaving the an-Nusayriyah Mountains, Saladin returned to Damascus and had his Syrian soldiers return home. He left Turan Shah in command of Syria and left for Egypt with only his personal followers, reaching Cairo on 22 September. Having been absent roughly two years, he had much to organize and supervise in Egypt, namely fortifying and reconstructing Cairo. The city walls were repaired and their extensions laid out, while the construction of the Cairo Citadel was commenced. The deep Bir Yusuf ("Joseph's Well") was built on Saladin's orders. The chief public work he commissioned outside of Cairo was the large bridge at Giza, which was intended to form an outwork of defense against a potential Moorish invasion.

Saladin remained in Cairo supervising its improvements, building colleges such as the Madrasa of the Sword Makers and ordering the internal administration of the country. In November 1177, he set out upon a raid into Palestine; the Crusaders had recently forayed into the territory of Damascus, so Saladin saw the truce as no longer worth preserving. The Christians sent a large portion of their army to besiege the fortress of Harim north of Aleppo, so southern Palestine bore few defenders. Saladin found the situation ripe and marched to Ascalon, which he referred to as the "Bride of Syria." William of Tyre recorded that the Ayyubid army consisted of soldiers, of which 8,000 were elite forces and were black soldiers from Sudan. This army proceeded to raid the countryside, sack Ramla and Lod, and dispersed themselves as far as the Gates of Jerusalem.

The Ayyubids allowed Baldwin IV of Jerusalem to enter Ascalon with his Gaza-based Knights Templar without taking any precautions against a sudden attack. Although the Crusader force consisted of only 375 knights, Saladin hesitated to ambush them because of the presence of highly skilled generals. On 25 November, while the greater part of the Ayyubid army was absent, Saladin and his men were surprised near Ramla in the battle of Montgisard. Before they could form up, the Templar force hacked the Ayyubid army down. Initially, Saladin attempted to organize his men into battle order, but as his bodyguards were being killed, he saw that defeat was inevitable and so with a small remnant of his troops mounted a swift camel, riding all the way to the territories of Egypt.

Not discouraged by his defeat at Tell Jezer, Saladin was prepared to fight the Crusaders once again. In the spring of 1178, he was encamped under the walls of Homs, and a few skirmishes occurred between his generals and the Crusader army. His forces in Hama won a victory over their enemy and brought the spoils, together with many prisoners of war, to Saladin who ordered the captives to be beheaded for "plundering and laying waste the lands of the Faithful". He spent the rest of the year in Syria without a confrontation with his enemies.

Saladin's intelligence services reported to him that the Crusaders were planning a raid into Syria. He ordered one of his generals, Farrukh-Shah, to guard the Damascus frontier with a thousand of his men to watch for an attack, then to retire, avoiding battle, and to light warning beacons on the hills, after which Saladin would march out. In April 1179, the Crusaders led by King Baldwin expected no resistance and waited to launch a surprise attack on Muslim herders grazing their herds and flocks east of the Golan Heights. Baldwin advanced too rashly in pursuit of Farrukh-Shah's force, which was concentrated southeast of Quneitra and was subsequently defeated by the Ayyubids. With this victory, Saladin decided to call in more troops from Egypt; he requested al-Adil to dispatch 1,500 horsemen.

In the summer of 1179, King Baldwin had set up an outpost on the road to Damascus and aimed to fortify a passage over the Jordan River, known as Jacob's Ford, that commanded the approach to the Banias plain (the plain was divided by the Muslims and the Christians). Saladin had offered 100,000 gold pieces to Baldwin to abandon the project, which was particularly offensive to the Muslims, but to no avail. He then resolved to destroy the fortress, called Chastellet and manned by the Templars, moving his headquarters to Banias. As the Crusaders hurried down to attack the Muslim forces, they fell into disorder, with the infantry falling behind. Despite early success, they pursued the Muslims far enough to become scattered, and Saladin took advantage by rallying his troops and charged at the Crusaders. The engagement ended in a decisive Ayyubid victory, and many high-ranking knights were captured. Saladin then moved to besiege the fortress, which fell on 30 August 1179.

In the spring of 1180, while Saladin was in the area of Safad, anxious to commence a vigorous campaign against the Kingdom of Jerusalem, King Baldwin sent messengers to him with proposals of peace. Because droughts and bad harvests hampered his commissariat, Saladin agreed to a truce. Raymond of Tripoli denounced the truce but was compelled to accept after an Ayyubid raid on his territory in May and upon the appearance of Saladin's naval fleet off the port of Tartus.

In June 1180, Saladin hosted a reception for Nur al-Din Muhammad, the Artuqid "emir" of Keyfa, at Geuk Su, in which he presented him and his brother Abu Bakr with gifts, valued at over 100,000 dinars according to Imad al-Din. This was intended to cement an alliance with the Artuqids and to impress other "emirs" in Mesopotamia and Anatolia. Previously, Saladin offered to mediate relations between Nur al-Din and Kilij Arslan II—the Seljuk sultan of Rûm—after the two came into conflict. The latter demanded that Nur al-Din return the lands given to him as a dowry for marrying his daughter when he received reports that she was being abused and used to gain Seljuk territory. Nur al-Din asked Saladin to mediate the issue, but Arslan refused.

After Nur al-Din and Saladin met at Geuk Su, the top Seljuk "emir", Ikhtiyar al-Din al-Hasan, confirmed Arslan's submission, after which an agreement was drawn up. Saladin was later enraged when he received a message from Arslan accusing Nur al-Din of more abuses against his daughter. He threatened to attack the city of Malatya, saying, "it is two days march for me and I shall not dismount [my horse] until I am in the city." Alarmed at the threat, the Seljuks pushed for negotiations. Saladin felt that Arslan was correct to care for his daughter, but Nur al-Din had taken refuge with him, and therefore he could not betray his trust. It was finally agreed that Arslan's daughter would be sent away for a year and if Nur al-Din failed to comply, Saladin would move to abandon his support for him.

Leaving Farrukh-Shah in charge of Syria, Saladin returned to Cairo at the beginning of 1181. According to Abu Shama, he intended to spend the fast of Ramadan in Egypt and then make the "hajj" pilgrimage to Mecca in the summer. For an unknown reason he apparently changed his plans regarding the pilgrimage and was seen inspecting the Nile River banks in June. He was again embroiled with the Bedouin; he removed two-thirds of their fiefs to use as compensation for the fief-holders at Fayyum. The Bedouin were also accused of trading with the Crusaders and, consequently, their grain was confiscated and they were forced to migrate westward. Later, Ayyubid warships were waged against Bedouin river pirates, who were plundering the shores of Lake Tanis.

In the summer of 1181, Saladin's former palace administrator Qara-Qush led a force to arrest Majd al-Din—a former deputy of Turan-Shah in the Yemeni town of Zabid—while he was entertaining Imad ad-Din at his estate in Cairo. Saladin's intimates accused Majd al-Din of misappropriating the revenues of Zabid, but Saladin himself believed there was no evidence to back the allegations. He had Majd al-Din released in return for a payment of 80,000 dinars. In addition, other sums were to be paid to Saladin's brothers al-Adil and Taj al-Muluk Buri. The controversial detainment of Majd al-Din was a part of the larger discontent associated with the aftermath of Turan-Shah's departure from Yemen. Although his deputies continued to send him revenues from the province, centralized authority was lacking and internal quarrel arose between Izz al-Din Uthman of Aden and Hittan of Zabid. Saladin wrote in a letter to al-Adil: "this Yemen is a treasure house ... We conquered it, but up to this day we have had no return and no advantage from it. There have been only innumerable expenses, the sending out of troops ... and expectations which did not produce what was hoped for in the end."

Saif al-Din had died earlier in June 1181 and his brother Izz al-Din inherited leadership of Mosul. On 4 December, the crown-prince of the Zengids, as-Salih, died in Aleppo. Prior to his death, he had his chief officers swear an oath of loyalty to Izz al-Din, as he was the only Zengid ruler strong enough to oppose Saladin. Izz al-Din was welcomed in Aleppo, but possessing it and Mosul put too great of a strain on his abilities. He thus, handed Aleppo to his brother Imad al-Din Zangi, in exchange for Sinjar. Saladin offered no opposition to these transactions in order to respect the treaty he previously made with the Zengids.

On 11 May 1182, Saladin, along with half of the Egyptian Ayyubid army and numerous non-combatants, left Cairo for Syria. On the evening before he departed, he sat with his companions and the tutor of one of his sons quoted a line of poetry: "enjoy the scent of the ox-eye plant of Najd, for after this evening it will come no more." Saladin took this as an evil omen and he never saw Egypt again. Knowing that Crusader forces were massed upon the frontier to intercept him, he took the desert route across the Sinai Peninsula to Ailah at the head of the Gulf of Aqaba. Meeting no opposition, Saladin ravaged the countryside of Montreal, whilst Baldwin's forces watched on, refusing to intervene. He arrived in Damascus in June to learn that Farrukh-Shah had attacked the Galilee, sacking Daburiyya and capturing Habis Jaldek, a fortress of great importance to the Crusaders. In July, Saladin dispatched Farrukh-Shah to attack Kawkab al-Hawa. Later, in August, the Ayyubids launched a naval and ground assault to capture Beirut; Saladin led his army in the Bekaa Valley. The assault was leaning towards failure and Saladin abandoned the operation to focus on issues in Mesopotamia.

Kukbary (Muzaffar ad-Din Gökböri), the "emir" of Harran, invited Saladin to occupy the Jazira region, making up northern Mesopotamia. He complied and the truce between him and the Zengids officially ended in September 1182. Prior to his march to Jazira, tensions had grown between the Zengid rulers of the region, primarily concerning their unwillingness to pay deference to Mosul. Before he crossed the Euphrates, Saladin besieged Aleppo for three days, signaling that the truce was over.

Once he reached Bira, near the river, he was joined by Kukbary and Nur al-Din of Hisn Kayfa and the combined forces captured the cities of Jazira, one after the other. First, Edessa fell, followed by Saruj, then Raqqa, Qirqesiya and Nusaybin. Raqqa was an important crossing point and held by Qutb al-Din Inal, who had lost Manbij to Saladin in 1176. Upon seeing the large size of Saladin's army, he made little effort to resist and surrendered on the condition that he would retain his property. Saladin promptly impressed the inhabitants of the town by publishing a decree that ordered a number of taxes to be canceled and erased all mention of them from treasury records, stating "the most miserable rulers are those whose purses are fat and their people thin". From Raqqa, he moved to conquer al-Fudain, al-Husain, Maksim, Durain, 'Araban, and Khabur—all of which swore allegiance to him.

Saladin proceeded to take Nusaybin which offered no resistance. A medium-sized town, Nusaybin was not of great importance, but it was located in a strategic position between Mardin and Mosul and within easy reach of Diyarbakir. In the midst of these victories, Saladin received word that the Crusaders were raiding the villages of Damascus. He replied "Let them... whilst they knock down villages, we are taking cities; when we come back, we shall have all the more strength to fight them." Meanwhile, in Aleppo, the "emir" of the city Zangi raided Saladin's cities to the north and east, such as Balis, Manbij, Saruj, Buza'a, al-Karzain. He also destroyed his own citadel at A'zaz to prevent it from being used by the Ayyubids if they were to conquer it.

Saladin turned his attention from Mosul to Aleppo, sending his brother Taj al-Muluk Buri to capture Tell Khalid, 130 km northeast of the city. A siege was set, but the governor of Tell Khalid surrendered upon the arrival of Saladin himself on 17 May before a siege could take place. According to Imad ad-Din, after Tell Khalid, Saladin took a detour northwards to Ain Tab, but he gained possession of it when his army turned towards it, allowing to quickly move backward another c. 100 km towards Aleppo. On 21 May, he camped outside the city, positioning himself east of the Citadel of Aleppo, while his forces encircles the suburb of Banaqusa to the northeast and Bab Janan to the west. He stationed his men dangerously close to the city, hoping for an early success.

Zangi did not offer long resistance. He was unpopular with his subjects and wished to return to his Sinjar, the city he governed previously. An exchange was negotiated where Zangi would hand over Aleppo to Saladin in return for the restoration of his control of Sinjar, Nusaybin, and Raqqa. Zangi would hold these territories as Saladin's vassals on terms of military service. On 12 June, Aleppo was formally placed in Ayyubid hands. The people of Aleppo had not known about these negotiations and were taken by surprise when Saladin's standard was hoisted over the citadel. Two "emir"s, including an old friend of Saladin, Izz al-Din Jurduk, welcomed and pledged their service to him. Saladin replaced the Hanafi courts with Shafi'i administration, despite a promise he would not interfere in the religious leadership of the city. Although he was short of money, Saladin also allowed the departing Zangi to take all the stores of the citadel that he could travel with and to sell the remainder—which Saladin purchased himself. In spite of his earlier hesitation to go through with the exchange, he had no doubts about his success, stating that Aleppo was "the key to the lands" and "this city is the eye of Syria and the citadel is its pupil." For Saladin, the capture of the city marked the end of over eight years of waiting since he told Farrukh-Shah that ""we have only to do the milking and Aleppo will be ours"".

After spending one night in Aleppo's citadel, Saladin marched to Harim, near the Crusader-held Antioch. The city was held by Surhak, a "minor "mamluk"." Saladin offered him the city of Busra and property in Damascus in exchange for Harim, but when Surhak asked for more, his own garrison in Harim forced him out. He was arrested by Saladin's deputy Taqi al-Din on allegations that he was planning to cede Harim to Bohemond III of Antioch. When Saladin received its surrender, he proceeded to arrange the defense of Harim from the Crusaders. He reported to the caliph and his own subordinates in Yemen and Baalbek that was going to attack the Armenians. Before he could move, however, there were a number of administrative details to be settled. Saladin agreed to a truce with Bohemond in return for Muslim prisoners being held by him and then he gave A'zaz to Alam ad-Din Suleiman and Aleppo to Saif al-Din al-Yazkuj—the former was an "emir" of Aleppo who joined Saladin and the latter was a former "mamluk" of Shirkuh who helped rescue him from the assassination attempt at A'zaz.

As Saladin approached Mosul, he faced the issue of taking over a large city and justifying the action. The Zengids of Mosul appealed to an-Nasir, the Abbasid caliph at Baghdad whose vizier favored them. An-Nasir sent Badr al-Badr (a high-ranking religious figure) to mediate between the two sides. Saladin arrived at the city on 10 November 1182. Izz al-Din would not accept his terms because he considered them disingenuous and extensive, and Saladin immediately laid siege to the heavily fortified city.

After several minor skirmishes and a stalemate in the siege that was initiated by the caliph, Saladin intended to find a way to withdraw without damage to his reputation while still keeping up some military pressure. He decided to attack Sinjar, which was held by Izz al-Din's brother Sharaf al-Din. It fell after a 15-day siege on 30 December. Saladin's soldiers broke their discipline, plundering the city; Saladin only managed to protect the governor and his officers by sending them to Mosul. After establishing a garrison at Sinjar, he awaited a coalition assembled by Izz al-Din consisting of his forces, those from Aleppo, Mardin, and Armenia. Saladin and his army met the coalition at Harran in February 1183, but on hearing of his approach, the latter sent messengers to Saladin asking for peace. Each force returned to their cities and al-Fadil wrote: ""They [Izz al-Din's coalition] advanced like men, like women they vanished.""

On 2 March, al-Adil from Egypt wrote to Saladin that the Crusaders had struck the "heart of Islam". Raynald de Châtillon had sent ships to the Gulf of Aqaba to raid towns and villages off the coast of the Red Sea. It was not an attempt to extend the Crusader influence into that sea or to capture its trade routes, but merely a piratical move. Nonetheless, Imad al-Din writes the raid was alarming to the Muslims because they were not accustomed to attacks on that sea, and Ibn al-Athir adds that the inhabitants had no experience with the Crusaders either as fighters or traders.

Ibn Jubair was told that sixteen Muslim ships were burnt by the Crusaders, who then captured a pilgrim ship and caravan at Aidab. He also reported that they intended to attack Medina and remove Muhammad's body. Al-Maqrizi added to the rumor by claiming Muhammad's tomb was going to be relocated to Crusader territory so Muslims would make pilgrimages there. Al-Adil had his warships moved from Fustat and Alexandria to the Red Sea under the command of an Armenian mercenary Lu'lu. They broke the Crusader blockade, destroyed most of their ships, and pursued and captured those who anchored and fled into the desert. The surviving Crusaders, numbered at 170, were ordered to be killed by Saladin in various Muslim cities.

From the point of view of Saladin, in terms of territory, the war against Mosul was going well, but he still failed to achieve his objectives and his army was shrinking; Taqi al-Din took his men back to Hama, while Nasir al-Din Muhammad and his forces had left. This encouraged Izz al-Din and his allies to take the offensive. The previous coalition regrouped at Harzam some 140 km from Harran. In early April, without waiting for Nasir al-Din, Saladin and Taqi al-Din commenced their advance against the coalition, marching eastward to Ras al-Ein unhindered. By late April, after three days of "actual fighting", according to Saladin, the Ayyubids had captured Amid. He handed the city to Nur al-Din Muhammad together with its stores, which consisted of 80,000 candles, a tower full of arrowheads, and 1,040,000 books. In return for a diploma granting him the city, Nur al-Din swore allegiance to Saladin, promising to follow him in every expedition in the war against the Crusaders, and repairing damage done to the city. The fall of Amid, in addition to territory, convinced Il-Ghazi of Mardin to enter the service of Saladin, weakening Izz al-Din's coalition.

Saladin attempted to gain the Caliph an-Nasir's support against Izz al-Din by sending him a letter requesting a document that would give him legal justification for taking over Mosul and its territories. Saladin aimed to persuade the caliph claiming that while he conquered Egypt and Yemen under the flag of the Abbasids, the Zengids of Mosul openly supported the Seljuks (rivals of the caliphate) and only came to the caliph when in need. He also accused Izz al-Din's forces of disrupting the Muslim "Holy War" against the Crusaders, stating "they are not content not to fight, but they prevent those who can." Saladin defended his own conduct claiming that he had come to Syria to fight the Crusaders, end the heresy of the Assassins, and stop the wrong-doing of the Muslims. He also promised that if Mosul was given to him, it would lead to the capture of Jerusalem, Constantinople, Georgia, and the lands of the Almohads in the Maghreb, "until the word of God is supreme and the Abbasid caliphate has wiped the world clean, turning the churches into mosques". Saladin stressed that all this would happen by the will of God, and instead of asking for financial or military support from the caliph, he would capture and give the caliph the territories of Tikrit, Daquq, Khuzestan, Kish Island, and Oman.

On 29 September 1182, Saladin crossed the Jordan River to attack Beisan, which was found to be empty. The next day his forces sacked and burned the town and moved westwards. They intercepted Crusader reinforcements from Karak and Shaubak along the Nablus road and took a number of prisoners. Meanwhile, the main Crusader force under Guy of Lusignan moved from Sepphoris to al-Fula. Saladin sent out 500 skirmishers to harass their forces, and he himself marched to Ain Jalut. When the Crusader force—reckoned to be the largest the kingdom ever produced from its own resources, but still outmatched by the Muslims—advanced, the Ayyubids unexpectedly moved down the stream of Ain Jalut. After a few Ayyubid raids—including attacks on Zir'in, Forbelet, and Mount Tabor—the Crusaders still were not tempted to attack their main force, and Saladin led his men back across the river once provisions and supplies ran low.

Crusader attacks provoked further responses by Saladin. Raynald of Châtillon, in particular, harassed Muslim trading and pilgrimage routes with a fleet on the Red Sea, a water route that Saladin needed to keep open. In response, Saladin built a fleet of 30 galleys to attack Beirut in 1182. Raynald threatened to attack the holy cities of Mecca and Medina. In retaliation, Saladin twice besieged Kerak, Raynald's fortress in Oultrejordain, in 1183 and 1184. Raynald responded by looting a caravan of pilgrims on the Hajj in 1185. According to the later 13th-century "Old French Continuation of William of Tyre", Raynald captured Saladin's sister in a raid on a caravan; this claim is not attested in contemporary sources, Muslim or Frankish, however, instead stating that Raynald had attacked a preceding caravan, and Saladin set guards to ensure the safety of his sister and her son, who came to no harm.

Following the failure of his Kerak sieges, Saladin temporarily turned his attention back to another long-term project and resumed attacks on the territory of ʻIzz ad-Dīn (Masʻūd ibn Mawdūd ibn Zangi), around Mosul, which he had begun with some success in 1182. However, since then, Masʻūd had allied himself with the powerful governor of Azerbaijan and Jibal, who in 1185 began moving his troops across the Zagros Mountains, causing Saladin to hesitate in his attacks. The defenders of Mosul, when they became aware that help was on the way, increased their efforts, and Saladin subsequently fell ill, so in March 1186 a peace treaty was signed.

In July 1187, Saladin captured most of the Kingdom of Jerusalem. On 4 July 1187, at the Battle of Hattin, he faced the combined forces of Guy of Lusignan, King Consort of Jerusalem, and Raymond III of Tripoli. In this battle alone the Crusader force was largely annihilated by Saladin's determined army. It was a major disaster for the Crusaders and a turning point in the history of the Crusades. Saladin captured Raynald and was personally responsible for his execution in retaliation for his attacks against Muslim caravans. The members of these caravans had, in vain, besought his mercy by reciting the truce between the Muslims and the Crusaders, but Raynald ignored this and insulted the Islamic prophet, Muhammad, before murdering and torturing a number of them. Upon hearing this, Saladin swore an oath to personally execute Raynald. Guy of Lusignan was also captured. Seeing the execution of Raynald, he feared he would be next. However, his life was spared by Saladin, who said of Raynald, "[i]t is not the wont of kings, to kill kings; but that man had transgressed all bounds, and therefore did I treat him thus."

Saladin had captured almost every Crusader city. Saladin preferred to take Jerusalem without bloodshed and offered generous terms, but those inside refused to leave their holy city, vowing to destroy it in a fight to the death rather than see it handed over peacefully. Jerusalem capitulated to his forces on Friday, 2 October 1187, after a siege. When the siege had started, Saladin was unwilling to promise terms of quarter to the Frankish inhabitants of Jerusalem. Balian of Ibelin threatened to kill every Muslim hostage, estimated at 5,000, and to destroy Islam's holy shrines of the Dome of the Rock and the al-Aqsa Mosque if such quarter were not provided. Saladin consulted his council and the terms were accepted. The agreement was read out through the streets of Jerusalem so that everyone might within forty days provide for himself and pay to Saladin the agreed tribute for his freedom. An unusually low ransom for the times (around US$50 today) was to be paid for each Frank in the city, whether man, woman, or child, but Saladin, against the wishes of his treasurers, allowed many families who could not afford the ransom to leave. Patriarch Heraclius of Jerusalem organised and contributed to a collection that paid the ransoms for about 18,000 of the poorer citizens, leaving another 15,000 to be enslaved. Saladin's brother al-Adil "asked Saladin for a thousand of them for his own use and then released them on the spot." Most of the foot soldiers were sold into slavery. Upon the capture of Jerusalem, Saladin summoned the Jews and permitted them to resettle in the city. In particular, the residents of Ashkelon, a large Jewish settlement, responded to his request. The subject ordered the churches repurposed as horse stables and the church towers destroyed.

Tyre, on the coast of modern-day Lebanon, was the last major Crusader city that was not captured by Muslim forces. Strategically, it would have made more sense for Saladin to capture Tyre before Jerusalem; Saladin, however, chose to pursue Jerusalem first because of the importance of the city to Islam. Tyre was commanded by Conrad of Montferrat, who strengthened its defences and withstood two sieges by Saladin. In 1188, at Tortosa, Saladin released Guy of Lusignan and returned him to his wife, Queen Sibylla of Jerusalem. They went first to Tripoli, then to Antioch. In 1189, they sought to reclaim Tyre for their kingdom but were refused admission by Conrad, who did not recognize Guy as king. Guy then set about besieging Acre.

Saladin was on friendly terms with Queen Tamar of Georgia. Saladin's biographer Bahā' ad-Dīn ibn Šaddād reports that, after Saladin's conquest of Jerusalem, the Georgian Queen sent envoys to the sultan to request the return of confiscated possessions of the Georgian monasteries in Jerusalem. Saladin's response is not recorded, but the queen's efforts seem to have been successful as Jacques de Vitry, the Bishop of Acre, reports the Georgians were, in contrast to the other Christian pilgrims, allowed a free passage into the city with their banners unfurled. Ibn Šaddād furthermore claims that Queen Tamar outbid the Byzantine emperor in her efforts to obtain the relics of the True Cross, offering 200,000 gold pieces to Saladin who had taken the relics as booty at the battle of Hattin, but to no avail.

Hattin and the fall of Jerusalem prompted the Third Crusade (1189–1192), financed in England by a special "Saladin tithe". Richard the Lionheart, King of England led Guy's siege of Acre, conquered the city and executed 3,000 Muslim prisoners, including women and children. Bahā' ad-Dīn wrote:

The armies of Saladin engaged in combat with the army of King Richard at the Battle of Arsuf on 7 September 1191, at which Saladin's forces suffered heavy losses and were forced to withdraw. After the battle of Arsuf, Richard occupied Jaffa, restoring the city's fortifications. Meanwhile, Saladin moved south, where he dismantled the fortifications of Ascalon to prevent this strategically important city, which lay at the junction between Egypt and Palestine, from falling into Crusader hands.

In October 1191, Richard began restoring the inland castles on the coastal plain beyond Jaffa in preparation for an advance on Jerusalem. During this period, Richard and Saladin passed envoys back and forth, negotiating the possibility of a truce. Richard proposed that his sister, Joan of England, Queen of Sicily, should marry Saladin's brother and that Jerusalem could be their wedding gift. However, Saladin rejected this idea when Richard insisted that Saladin's brother convert to Christianity. Richard suggested that his niece Eleanor, Fair Maid of Brittany be the bride instead, an idea that Saladin also rejected.

In January 1192, Richard's army occupied Beit Nuba, just twelve miles from Jerusalem, but withdrew without attacking the Holy City. Instead, Richard advanced south on Ascalon, where he restored the fortifications. In July 1192, Saladin tried to threaten Richard's command of the coast by attacking Jaffa. The city was besieged, and Saladin very nearly captured it; however, Richard arrived a few days later and defeated Saladin's army in a battle outside the city.

The Battle of Jaffa (1192) proved to be the last military engagement of the Third Crusade. After Richard reoccupied Jaffa and restored its fortifications, he and Saladin again discussed terms. At last Richard agreed to demolish the fortifications of Ascalon, while Saladin agreed to recognize Crusader control of the Palestinian coast from Tyre to Jaffa. The Christians would be allowed to travel as unarmed pilgrims to Jerusalem, and Saladin's kingdom would be at peace with the Crusader states for the following three years.

Saladin died of a fever on 4 March 1193, at Damascus, not long after King Richard's departure. In Saladin’s possession at the time of his death were one piece of gold and forty pieces of silver. He had given away his great wealth to his poor subjects, leaving nothing to pay for his funeral. He was buried in a mausoleum in the garden outside the Umayyad Mosque in Damascus, Syria. Originally the tomb was part of a complex which also included a school, Madrassah al-Aziziah, of which little remains except a few columns and an internal arch. Seven centuries later, Emperor Wilhelm II of Germany donated a new marble sarcophagus to the mausoleum. However, the original sarcophagus was not replaced; instead, the mausoleum, which is open to visitors, now has two sarcophagi: the marble one placed on the side and the original wooden one, which covers Saladin's tomb. (Muslims are buried in a simple shroud, so if there are any sarcophagi present, they are usually used for covering the top of the Islamic burials.)

Imad ad-Din al-Isfahani compiled a list of Saladin's sons along with their dates of birth, according to information provided by Saladin late in his reign. They were:

The sons who were full brothers were:

The sons listed by Imad number fifteen, but elsewhere he writes that Saladin was survived by seventeen sons and one daughter. According to Abu Hamah, Imad missed two sons who were born to slave-women: Imad al-Din Shadhi and Nusrat al-Din Marwan. As for Saladin's daughter, she was Mu'nisah Khatun; she married her cousin al-Kamil Muhammad ibn Adil. Saladin also had other children who died before him, such as al-Mansur Hasan and Ahmad. Al-Zahir Dawud, whom Imad listed eighth, is recorded as being his twelfth son in a letter written by Saladin's minister.

Not much is known of Saladin's wives or slave-women. He married Ismat al-Din Khatun, the widow of Nur al-Din Zengi, in 1176. She did not have children. One of his wives, Shamsah, is buried with her son al-Aziz in the tomb of al-Shafi'i.

Saladin eventually achieved a great reputation in Europe as a chivalrous knight, due to his fierce struggle against the crusaders and his generosity. In "The Divine Comedy" he is mentioned as one of the virtuous non-Christians in limbo. Although Saladin faded into history after the Middle Ages, he appears in a sympathetic light in Gotthold Ephraim Lessing's play "Nathan the Wise" (1779) and in Sir Walter Scott's novel "The Talisman" (1825). The modern view of Saladin originates mainly from these texts. According to Jonathan Riley-Smith, Scott's portrayal of Saladin was that of a "modern [19th-century] liberal European gentlemen, beside whom medieval Westerners would always have made a poor showing". Despite the Crusaders' slaughter when they originally conquered Jerusalem in 1099, Saladin granted amnesty and free passage to all common Catholics and even to the defeated Christian army, as long as they were able to pay the aforementioned ransom (the Greek Orthodox Christians were treated even better, because they often opposed the western Crusaders).

Notwithstanding the differences in beliefs, the Muslim Saladin was respected by Christian lords, Richard especially. Richard once praised Saladin as a great prince, saying that he was without doubt the greatest and most powerful leader in the Islamic world. Saladin in turn stated that there was not a more honorable Christian lord than Richard. After the treaty, Saladin and Richard sent each other many gifts as tokens of respect but never met face to face. In April 1191, a Frankish woman's three-month-old baby had been stolen from her camp and sold on the market. The Franks urged her to approach Saladin herself with her grievance. According to Bahā' al-Dīn, Saladin used his own money to buy the child back:

In 1898, German Emperor Wilhelm II visited Saladin's tomb to pay his respects. The visit, coupled with anti-imperialist sentiments, led nationalist Arabs to reinvent the image of Saladin and portray him as a hero of the struggle against the West. The image of Saladin they used was the romantic one created by Walter Scott and other Europeans in the West at the time. It replaced Saladin's reputation as a figure who had been largely forgotten in the Muslim world, eclipsed by more successful figures, such as Baybars of Egypt.

Modern Arab states have sought to commemorate Saladin through various measures, often based on the image created of him in the 19th-century west. A governorate centered around Tikrit and Samarra in modern-day Iraq, Saladin Governorate, is named after him, as is Salahaddin University in Erbil, the largest city of Iraqi Kurdistan. A suburban community of Erbil, Masif Salahaddin, is also named after him.

Few structures associated with Saladin survive within modern cities. Saladin first fortified the Citadel of Cairo (1175–1183), which had been a domed pleasure pavilion with a fine view in more peaceful times. In Syria, even the smallest city is centred on a defensible citadel, and Saladin introduced this essential feature to Egypt.

Although the Ayyubid dynasty that he founded would only outlive him by 57 years, the legacy of Saladin within the Arab World continues to this day. With the rise of Arab nationalism in the 20th Century, particularly with regard to the Arab–Israeli conflict, Saladin's heroism and leadership gained a new significance. Saladin's recapture of Palestine from the European Crusaders is considered an inspiration for modern-day Arabs' opposition to Zionism. Moreover, the glory and comparative unity of the Arab World under Saladin was seen as the perfect symbol for the new unity sought by Arab nationalists, such as Gamal Abdel Nasser. For this reason, the Eagle of Saladin became the symbol of revolutionary Egypt, and was subsequently adopted by several other Arab states (the United Arab Republic, Iraq, Libya, the State of Palestine, and Yemen).

Among Egyptian Shias, Saladin is dubbed as "Kharab al-Din", the destroyer of religion—a derisive play on the name "Saladin."

Statue of Saladin is an oversize equestrian bronze statue depicting the Kurdish Ayyubid Sultan Saladin located in front of the 11th century Citadel of Damascus in the Ancient City of Damascus in Damascus, Syria.






</doc>
<doc id="26984" url="https://en.wikipedia.org/wiki?curid=26984" title="Sophocles">
Sophocles

Sophocles (; "Sophoklēs", ; 497/6 – winter 406/5 BC) is one of three ancient Greek tragedians whose plays have survived. His first plays were written later than or contemporary with those of Aeschylus, and earlier than or contemporary with those of Euripides. Sophocles wrote over 120 plays during the course of his life, but only seven have survived in a complete form: "Ajax", "Antigone", "Women of Trachis", "Oedipus Rex", "Electra", "Philoctetes" and "Oedipus at Colonus". For almost 50 years, Sophocles was the most celebrated playwright in the dramatic competitions of the city-state of Athens that took place during the religious festivals of the Lenaea and the Dionysia. He competed in 30 competitions, won 24, and was never judged lower than second place. Aeschylus won 13 competitions, and was sometimes defeated by Sophocles, while Euripides won four competitions.

The most famous tragedies of Sophocles feature Oedipus and also Antigone: they are generally known as the Theban plays, although each play was actually a part of a different tetralogy, the other members of which are now lost. Sophocles influenced the development of drama, most importantly by adding a third actor, thereby reducing the importance of the chorus in the presentation of the plot. He also developed his characters to a greater extent than earlier playwrights such as Aeschylus.

Sophocles, the son of Sophilus, was a wealthy member of the rural "deme" (small community) of Hippeios Colonus in Attica, which was to become a setting for one of his plays, and he was probably born there. Sophocles was born a few years before the Battle of Marathon in 490 BC: the exact year is unclear, although 497/6 is the most likely. Sophocles was born into a wealthy family (his father was an armour manufacturer) and was highly educated. Sophocles' first artistic triumph was in 468 BC, when he took first prize in the Dionysia theatre competition over the reigning master of Athenian drama, Aeschylus. According to Plutarch, the victory came under unusual circumstances. Instead of following the usual custom of choosing judges by lot, the archon asked Cimon and the other "strategoi" present to decide the victor of the contest. Plutarch further contends that following this loss Aeschylus soon left for Sicily. Although Plutarch says that this was Sophocles' first production, it is now thought that his first production was probably in 470 BC. "Triptolemus" was probably one of the plays that Sophocles presented at this festival.

In 480 BC Sophocles was chosen to lead the paean (a choral chant to a god), celebrating the Greek victory over the Persians at the Battle of Salamis. Early in his career, the politician Cimon might have been one of his patrons, although if he was, there was no ill will borne by Pericles, Cimon's rival, when Cimon was ostracized in 461 BC. In 443/2 he served as one of the "Hellenotamiai", or treasurers of Athena, helping to manage the finances of the city during the political ascendancy of Pericles. According to the "Vita Sophoclis", in 441 BC he was elected one of the ten generals, executive officials at Athens, as a junior colleague of Pericles, and he served in the Athenian campaign against Samos; he was supposed to have been elected to this position as the result of his production of "Antigone".

In 420 BC, he welcomed and set up an altar for the image of Asclepius at his house, when the deity was introduced to Athens. For this, he was given the posthumous epithet "Dexion" (receiver) by the Athenians. He was also elected, in 413 BC, one of the commissioners ("probouloi") who responded to the catastrophic destruction of the Athenian expeditionary force in Sicily during the Peloponnesian War.

Sophocles died at the age of ninety or ninety-one in the winter of 406/5 BC, having seen within his lifetime both the Greek triumph in the Persian Wars and the bloodletting of the Peloponnesian War. As with many famous men in classical antiquity, his death inspired a number of apocryphal stories. The most famous is the suggestion that he died from the strain of trying to recite a long sentence from his "Antigone" without pausing to take a breath. Another account suggests he choked while eating grapes at the Anthesteria festival in Athens. A third holds that he died of happiness after winning his final victory at the City Dionysia. A few months later, a comic poet, in a play titled "The Muses", wrote this eulogy: "Blessed is Sophocles, who had a long life, was a man both happy and talented, and the writer of many good tragedies; and he ended his life well without suffering any misfortune." According to some accounts, however, his own sons tried to have him declared incompetent near the end of his life; he is said to have refuted their charge in court by reading from his as yet unproduced "Oedipus at Colonus". One of his sons, Iophon, and a grandson, also called Sophocles, also became playwrights.

An ancient source, Athenaeus’s work "Sophists at Dinner", contains references to Sophocles' homosexuality or bisexuality. In that work, a character named Myrtilus, in a lengthy banquet speech claims that Ion of Chios writes in his book "Encounters", that Sophocles loved boys as much as Euripides loved women. Myrtilus also repeats an anecdote reportedly told by Ion of Chios that involves Sophocles flirting with a serving boy at a symposium. Myrtilus also claims that in a work by Hieronymus of Rhodes entitled "Historical Notes" it is said that Sophocles once lured a boy outside to have sex, and afterwards the boy left with Sophocles' cape, while the boy's own cape was left with Sophocles.

Among Sophocles' earliest innovations was the addition of a third actor, which further reduced the role of the chorus and created greater opportunity for character development and conflict between characters. Aeschylus, who dominated Athenian playwriting during Sophocles' early career, followed suit and adopted the third character into his own work towards the end of his life. Aristotle credits Sophocles with the introduction of "skenographia", or scenery-painting. It was not until after the death of the old master Aeschylus in 456 BC that Sophocles became the pre-eminent playwright in Athens.

Thereafter, Sophocles emerged victorious in dramatic competitions at 18 Dionysia and 6 Lenaia festivals. In addition to innovations in dramatic structure, Sophocles' work is also known for its deeper development of characters than earlier playwrights. His reputation was such that foreign rulers invited him to attend their courts, although unlike Aeschylus who died in Sicily, or Euripides who spent time in Macedon, Sophocles never accepted any of these invitations. Aristotle used Sophocles' "Oedipus Rex" in his "Poetics" (c. 335 BC) as an example of the highest achievement in tragedy, which suggests the high esteem in which his work was held by later Greeks.

Only two of the seven surviving plays can be dated securely: "Philoctetes" (409 BC) and "Oedipus at Colonus" (401 BC, staged after Sophocles' death by his grandson). Of the others, "Electra" shows stylistic similarities to these two plays, which suggests that it was probably written in the latter part of his career. "Ajax", "Antigone" and "The Trachiniae" are generally thought to be among his early works, again based on stylistic elements, with "Oedipus Rex" coming in Sophocles' middle period. Most of Sophocles' plays show an undercurrent of early fatalism and the beginnings of Socratic logic as a mainstay for the long tradition of Greek tragedy.

The Theban plays consist of three plays: "Oedipus Rex" (also called "Oedipus Tyrannus" or "Oedipus the King"), "Oedipus at Colonus" and "Antigone". All three plays concern the fate of Thebes during and after the reign of King Oedipus. They have often been published under a single cover. Sophocles, however, wrote the three plays for separate festival competitions, many years apart. Not only are the Theban plays not a true trilogy (three plays presented as a continuous narrative) but they are not even an intentional series and contain some inconsistencies among them. He also wrote other plays having to do with Thebes, such as the "Epigoni", of which only fragments have survived.

Each of the plays relates to the tale of the mythological Oedipus, who killed his father and married his mother without knowledge that they were his parents. His family is fated to be doomed for three generations.

In "Oedipus Rex", Oedipus is the protagonist. Oedipus' infanticide is planned by his parents, Laius and Jocasta, to avert him from fulfilling a prophecy; in truth, the servant entrusted with the infanticide passes the infant on through a series of intermediaries to a childless couple, who adopt him not knowing his history. Oedipus eventually learns of the Delphic Oracle's prophecy of him, that he would kill his father and marry his mother; Oedipus attempts to flee his fate without harming those he knows as his parents (at this point, he does not know that he is adopted). Oedipus meets a man at a crossroads accompanied by servants; Oedipus and the man fight, and Oedipus kills the man (who was his father, Laius, although neither knew at the time). He becomes the ruler of Thebes after solving the riddle of the Sphinx and in the process, marries the widowed queen, his mother Jocasta. Thus the stage is set for horror. When the truth comes out, following from another true but confusing prophecy from Delphi, Jocasta commits suicide, Oedipus blinds himself and leaves Thebes. At the end of the play, order is restored. This restoration is seen when Creon, brother of Jocasta, becomes king, and also when Oedipus, before going off to exile, asks Creon to take care of his children. Oedipus's children will always bear the weight of shame and humiliation because of their father's actions.

In "Oedipus at Colonus", the banished Oedipus and his daughter Antigone arrive at the town of Colonus where they encounter Theseus, King of Athens. Oedipus dies and strife begins between his sons Polyneices and Eteocles.

In "Antigone", the protagonist is Oedipus' daughter, Antigone. She is faced with the choice of allowing her brother Polyneices' body to remain unburied, outside the city walls, exposed to the ravages of wild animals, or to bury him and face death. The king of the land, Creon, has forbidden the burial of Polyneices for he was a traitor to the city. Antigone decides to bury his body and face the consequences of her actions. Creon sentences her to death. Eventually, Creon is convinced to free Antigone from her punishment, but his decision comes too late and Antigone commits suicide. Her suicide triggers the suicide of two others close to King Creon: his son, Haemon, who was to wed Antigone, and his wife, Eurydice, who commits suicide after losing her only surviving son.

The plays were written across thirty-six years of Sophocles' career and were not composed in chronological order, but instead were written in the order "Antigone", "Oedipus Rex", and "Oedipus at Colonus". Nor were they composed as a "trilogy" – a group of plays to be performed together, but are the remaining parts of three different groups of plays. As a result, there are some inconsistencies: notably, Creon is the undisputed king at the end of "Oedipus Rex" and, in consultation with Apollo, single-handedly makes the decision to expel Oedipus from Thebes. Creon is also instructed to look after Oedipus' daughters Antigone and Ismene at the end of "Oedipus Rex". By contrast, in the other plays there is some struggle with Oedipus' sons Eteocles and Polynices in regard to the succession. In "Oedipus at Colonus", Sophocles attempts to work these inconsistencies into a coherent whole: Ismene explains that, in light of their tainted family lineage, her brothers were at first willing to cede the throne to Creon. Nevertheless, they eventually decided to take charge of the monarchy, with each brother disputing the other's right to succeed. In addition to being in a clearly more powerful position in "Oedipus at Colonus", Eteocles and Polynices are also culpable: they consent (l. 429, Theodoridis, tr.) to their father's going to exile, which is one of his bitterest charges against them.

In addition to the three Theban plays, there are four surviving plays by Sophocles: "Ajax", "Women of Trachis", "Electra", and "Philoctetes", the last of which won first prize in 409 BC in which it competed.

"Ajax" focuses on the proud hero of the Trojan War, Telamonian Ajax, who is driven to treachery and eventually suicide. Ajax becomes gravely upset when Achilles’ armor is presented to Odysseus instead of himself. Despite their enmity toward him, Odysseus persuades the kings Menelaus and Agamemnon to grant Ajax a proper burial.

"The Women of Trachis" (named for the Trachinian women who make up the chorus) dramatizes Deianeira's accidentally killing Heracles after he had completed his famous twelve labors. Tricked into thinking it is a love charm, Deianeira applies poison to an article of Heracles' clothing; this poisoned robe causes Heracles to die an excruciating death. Upon learning the truth, Deianeira commits suicide.

"Electra" corresponds roughly to the plot of Aeschylus' "Libation Bearers". It details how Electra and Orestes' avenge their father Agamemnon's murder by Clytemnestra and Aegisthus.

"Philoctetes" retells the story of Philoctetes, an archer who had been abandoned on Lemnos by the rest of the Greek fleet while on the way to Troy. After learning that they cannot win the Trojan War without Philoctetes' bow, the Greeks send Odysseus and Neoptolemus to retrieve him; due to the Greeks' earlier treachery, however, Philoctetes refuses to rejoin the army. It is only Heracles' deus ex machina appearance that persuades Philoctetes to go to Troy.

Although the list of over 120 titles of plays associated with Sophocles are known and presented below, little is known of the precise dating of most of them. "Philoctetes" is known to have been written in 409 BC, and "Oedipus at Colonus" is known to have only been performed in 401 BC, posthumously, at the initiation of Sophocles' grandson. The convention on writing plays for the Greek festivals was to submit them in tetralogies of three tragedies along with one satyr play. Along with the unknown dating of the vast majority of over 120 play titles, it is also largely unknown how the plays were grouped. It is, however, known that the three plays referred to in the modern era as the "Theban plays" were never performed together in Sophocles' own lifetime, and are therefore not a trilogy (which they are sometimes erroneously seen as).

Fragments of "Ichneutae" ("Tracking Satyrs") were discovered in Egypt in 1907. These amount to about half of the play, making it the best preserved satyr play after Euripides' "Cyclops", which survives in its entirety. Fragments of the "Epigoni" were discovered in April 2005 by classicists at Oxford University with the help of infrared technology previously used for satellite imaging. The tragedy tells the story of the second siege of Thebes. A number of other Sophoclean works have survived only in fragments, including:
There is a passage of Plutarch's tract "De Profectibus in Virtute 7 " in which Sophocles discusses his own growth as a writer. A likely source of this material for Plutarch was the "Epidemiae" of Ion of Chios, a book that recorded many conversations of Sophocles. This book is a likely candidate to have contained Sophocles' discourse on his own development because Ion was a friend of Sophocles, and the book is known to have been used by Plutarch. Though some interpretations of Plutarch's words suggest that Sophocles says that he imitated Aeschylus, the translation does not fit grammatically, nor does the interpretation that Sophocles said that he was making fun of Aeschylus' works. C. M. Bowra argues for the following translation of the line:
"After practising to the full the bigness of Aeschylus, then the painful ingenuity of my own invention, now in the third stage I am changing to the kind of diction which is most expressive of character and best."

Here Sophocles says that he has completed a stage of Aeschylus' work, meaning that he went through a phase of imitating Aeschylus' style but is finished with that. Sophocles' opinion of Aeschylus was mixed. He certainly respected him enough to imitate his work early on in his career, but he had reservations about Aeschylus' style, and thus did not keep his imitation up. Sophocles' first stage, in which he imitated Aeschylus, is marked by "Aeschylean pomp in the language". Sophocles' second stage was entirely his own. He introduced new ways of evoking feeling out of an audience, as in his "Ajax", when Ajax is mocked by Athene, then the stage is emptied so that he may commit suicide alone. Sophocles mentions a third stage, distinct from the other two, in his discussion of his development. The third stage pays more heed to diction. His characters spoke in a way that was more natural to them and more expressive of their individual character feelings.





</doc>
<doc id="26985" url="https://en.wikipedia.org/wiki?curid=26985" title="Salinity">
Salinity

Salinity () is the saltiness or amount of salt dissolved in a body of water, called saline water (see also soil salinity). This is usually measured in formula_1 (note that this is technically dimensionless). Salinity is an important factor in determining many aspects of the chemistry
of natural waters and of biological processes within it, and is a thermodynamic state variable that, along with temperature and pressure, governs physical characteristics like the density and heat capacity of the water.

A contour line of constant salinity is called an "isohaline", or sometimes "isohale".

Salinity in rivers, lakes, and the ocean is conceptually simple, but technically challenging to define and measure precisely. Conceptually the salinity is the quantity of dissolved salt content of the water. Salts are compounds like sodium chloride, magnesium sulfate, potassium nitrate, and sodium bicarbonate which dissolve into ions. The concentration of dissolved chloride ions is sometimes referred to as chlorinity. Operationally, dissolved matter is defined as that which can pass through a very fine filter (historically a filter with a pore size of 0.45 μm, but nowadays usually 0.2 μm). Salinity can be expressed in the form of a mass fraction, i.e. the mass of the dissolved material in a unit mass of solution.

Seawater typically has a mass salinity of around 35 g/kg, although lower values are typical near coasts where rivers enter the ocean. Rivers and lakes can have a wide range of salinities, from less than 0.01 g/kg to a few g/kg, although there are many places where higher salinities are found. The Dead Sea has a salinity of more than 200 g/kg. Rainwater before touching the ground typically has a TDS of 20 mg/L or less.

Whatever pore size is used in the definition, the resulting salinity value of a given sample of natural water will not vary by more than a few percent (%). Physical oceanographers working in the abyssal ocean, however, are often concerned with precision and intercomparability of measurements by different researchers, at different times, to almost five significant digits. A bottled seawater product known as IAPSO Standard Seawater is used by oceanographers to standardize their measurements with enough precision to meet this requirement.

Measurement and definition difficulties arise because natural waters contain a complex mixture of many different elements from different sources (not all from dissolved salts) in different molecular forms. The chemical properties of some of these forms depend on temperature and pressure. Many of these forms are difficult to measure with high accuracy, and in any case complete chemical analysis is not practical when analyzing multiple samples. Different practical definitions of salinity result from different attempts to account for these problems, to different levels of precision, while still remaining reasonably easy to use.

For practical reasons salinity is usually related to the sum of masses of a subset of these dissolved chemical constituents (so-called "solution salinity"), rather than to the unknown mass of salts that gave rise to this composition (an exception is when artificial seawater is created). For many purposes this sum can be limited to a set of eight major ions in natural waters, although for seawater at highest precision an additional seven minor ions are also included. The major ions dominate the inorganic composition of most (but by no means all) natural waters. Exceptions include some pit lakes and waters from some hydrothermal springs.

The concentrations of dissolved gases like oxygen and nitrogen are not usually included in descriptions of salinity. However, carbon dioxide gas, which when dissolved is partially converted into carbonates and bicarbonates, is often included. Silicon in the form of silicic acid, which usually appears as a neutral molecule in the pH range of most natural waters, may also be included for some purposes (e.g., when salinity/density relationships are being investigated).

The term 'salinity' is, for oceanographers, usually associated with one of a set of specific measurement techniques. As the dominant techniques evolve, so do different descriptions of salinity. Salinities were largely measured using titration-based techniques before the 1980s. Titration with silver nitrate could be used to determine the concentration of halide ions (mainly chlorine and bromine) to give a chlorinity. The chlorinity was then multiplied by a factor to account for all other constituents. The resulting 'Knudsen salinities' are expressed in units of parts per thousand (ppt or ‰).

The use of electrical conductivity measurements to estimate the ionic content of seawater led to the development of the scale called the "practical salinity scale 1978" (PSS-78). Salinities measured using PSS-78 do not have units. The suffix psu or PSU (denoting "practical salinity unit") is sometimes added to PSS-78 measurement values.

In 2010 a new standard for the properties of seawater called the "thermodynamic equation of seawater 2010" (TEOS-10) was introduced, advocating absolute salinity as a replacement for practical salinity, and conservative temperature as a replacement for potential temperature. This standard includes a new scale called the "reference composition salinity scale". Absolute salinities on this scale are expressed as a mass fraction, in grams per kilogram of solution. Salinities on this scale are determined by combining electrical conductivity measurements with other information that can account for regional changes in the composition of seawater. They can also be determined by making direct density measurements.

A sample of seawater from most locations with a chlorinity of 19.37 ppt will have a Knudsen salinity of 35.00 ppt, a PSS-78 practical salinity of about 35.0, and a TEOS-10 absolute salinity of about 35.2 g/kg. The electrical conductivity of this water at a temperature of 15 °C is 42.9 mS/cm.

Limnologists and chemists often define salinity in terms of mass of salt per unit volume, expressed in units of mg per litre or g per litre. It is implied, although often not stated, that this value applies accurately only at some reference temperature. Values presented in this way are typically accurate to the order of 1%. Limnologists also use electrical conductivity, or "reference conductivity", as a proxy for salinity. This measurement may be corrected for temperature effects, and is usually expressed in units of μS/cm.

A river or lake water with a salinity of around 70 mg/L will typically have a specific conductivity at 25 °C of between 80 and 130 μS/cm. The actual ratio depends on the ions present. The actual conductivity usually changes by about 2% per degree Celsius, so the measured conductivity at 5 °C might only be in the range of 50–80 μS/cm.

Direct density measurements are also used to estimate salinities, particularly in highly saline lakes. Sometimes density at a specific temperature is used as a proxy for salinity. At other times an empirical salinity/density relationship developed for a particular body of water is used to estimate the salinity of samples from a measured density.
Marine waters are those of the ocean, another term for which is "euhaline seas". The salinity of euhaline seas is 30 to 35. "Brackish seas" or waters have salinity in the range of 0.5 to 29 and "metahaline seas" from 36 to 40. These waters are all regarded as "thalassic" because their salinity is derived from the ocean and defined as "homoiohaline" if salinity does not vary much over time (essentially constant). The table on the right, modified from Por (1972), follows the "Venice system" (1959).

In contrast to homoiohaline environments are certain "poikilohaline" environments (which may also be "thalassic") in which the salinity variation is biologically significant. "Poikilohaline" water salinities may range anywhere from 0.5 to greater than 300. The important characteristic is that these waters tend to vary in salinity over some biologically meaningful range seasonally or on some other roughly comparable time scale. Put simply, these are bodies of water with quite variable salinity.

Highly saline water, from which salts crystallize (or are about to), is referred to as brine.

Salinity is an ecological factor of considerable importance, influencing the types of organisms that live in a body of water. As well, salinity influences the kinds of plants that will grow either in a water body, or on land fed by a water (or by a groundwater). A plant adapted to saline conditions is called a halophyte. A halophyte which is tolerant to residual sodium carbonate salinity are called glasswort or saltwort or barilla plants. Organisms (mostly bacteria) that can live in very salty conditions are classified as extremophiles, or halophiles specifically. An organism that can withstand a wide range of salinities is euryhaline.

Salt is expensive to remove from water, and salt content is an important factor in water use (such as potability). Increases in salinity have been observed in lakes and rivers in the United States, due to common road salt and other salt de-icers in runoff.

The degree of salinity in oceans is a driver of the world's ocean circulation, where density changes due to both salinity changes and temperature changes at the surface of the ocean produce changes in buoyancy, which cause the sinking and rising of water masses. Changes in the salinity of the oceans are thought to contribute to global changes in carbon dioxide as more saline waters are less soluble to carbon dioxide. In addition, during glacial periods, the hydrography is such that a possible cause of reduced circulation is the production of stratified oceans. In such cases, it is more difficult to subduct water through the thermohaline circulation.




</doc>
<doc id="26987" url="https://en.wikipedia.org/wiki?curid=26987" title="Saxifragales">
Saxifragales

The Saxifragales (saxifrages) are an order of flowering plants (Angiosperms). They are an extremely diverse group of plants which include trees, shrubs, perennial herbs, succulent and aquatic plants. The degree of diversity in terms of vegetative and floral features makes it difficult to define common features that unify the order.

In the Angiosperm Phylogeny Group classification system, the Saxifragales are placed within the major division of flowering plants referred to as eudicots, specifically the core eudicots. This subgroup consist of the Dilleniaceae, superasterids and superrosids. The superrosids in turn have two components, rosids and Saxifragales. The Saxifragales order has undergone considerable revision since its original classification based purely on plant characteristics. The modern classification is based on genetic studies using molecular phylogenetics. There is an extensive fossil record from the Turonian-Campanian (late Cretaceous) time, about 90 million years ago (Myr). However, but molecular studies suggest an earlier origin in the early Cretaceous (102–108 Myr) with rapid early diversification to more modern forms.

The order Saxifragales consists of 15 families, about 100 genera and 2,470 species. Of the 15 families, many are small, with 8 having only a single genus, the largest family being the Crassulaceae (stonecrops) with about 35 genera. Saxifragales are found worldwide, though rarely in the tropics, and in a wide variety of habitats from desert to aquatic. They also have a wide variety of uses, from timber to foodstuffs and ornamental plants. Apart from ornamentals, the major economically important group is the Grossulariaceae (currants and gooseberries), particularly blackcurrant.

The order Saxifragales is extremely morphologically diverse (hyper-diverse). It includes trees (e.g. witch hazel, witch alder in Hamamelidaceae), fruit bearing shrubs (e.g. currants, gooseberries in Grossulariaceae), lianas, annual and perennial herbs, rock garden plants (e.g. saxifrage in Saxifragaceae), ornamental garden plants (e.g. peonies in Paeoniaceae), succulents (e.g. stonecrop in Crassulaceae) and aquatics (e.g. watermilfoil in Haloragaceae). The flowers demonstrate major variations in sepal, petal, stamen, and carpel number, as well as ovary position ("see" Biogeography and evolution).
This degree of diversity makes defining synapomorphy (derived common characteristics) for the group extremely difficult, the order being defined on the basis of molecular affinity rather than morphology. However, some characteristics that are prevalent (common traits) represent potential or putative synapomorphies based on ancestral states. These include flowers that are usually radially symmetric and petals that are free. The gynoecium (female reproductive part) generally consists of two carpels (ovary, style and stigma) that are free, at least toward the apex (partially fused bicarpellate gynoecium) and possess a hypanthium (cup shaped basal floral tube). In the androecium (male reproductive part), the stamen anthers are generally basifixed (attached at its base to the filament), sometimes dorsifixed (attached at centre) ("see Figure 2"). Other commonly occurring features are fruit that is generally follicular (formed from a single carpel), seeds with abundant endosperm surrounding the embryo and leaves with glandular teeth at their margins (glandular dentate, "see image"). Within the Saxifragales, while the families of the woody clade are primarily woody, the primarily herbaceous families of Crassulaceae and Saxifragaceae exhibit woody features as a secondary transition.

With 15 families, about 100 genera and about 2,470 species, Saxifragales is a relatively small angiosperm order.

Saxifragales was first described in 1820 by Berchtold and Presl in 1820 as a group of plants, Saxifrageae, with five genera, including "Saxifraga", and therefore bear their names as the botanical authority (Bercht. & J.Presl). At times, that authority has also been given to Dumortier, due to a later publication (1829). Dumortier first used the word Saxifragaceae. By the time of John Lindley's "The Vegetable Kingdom" (1853), the term Saxifragales was in use, which Lindley called an Alliance, containing five families. Later, the Saxifragales were placed in the angiosperm class Dicotyledons, also called Magnoliopsida.

The order Saxifragales has undergone considerable revision in both placement and composition, since the use of molecular phylogenetics, and the use of the modern Angiosperm Phylogeny Group (APG) classification. They are identified as a strongly monophyletic group.

In the initial APG publication (1998), the Saxifragales were identified within the core eudicots clade but its relationship to other clades was uncertain. The core eudicots consist of the order Gunnerales and a large clade of Pentapetalae (so named for having a synapomorphy of pentamerous (5 part) perianths), the latter representing about 70% of all angiosperms, with eight major lineages. Later (2003), the order was described as "one of the major surprises of molecular phylogenetic analyses of the angiosperms", having elements previously placed in three or four separate subclasses based on morphology. This was eventually resolved in the third APG system (2009) placing Saxifragales as a sister group to the rosids (Rosidae), within the Pentapetalae clade. This large combination has subsequently been given the name superrosids (Superrosidae), representing part of an early diversification of the angiosperms. Among the rosids, they share a number of similarities with the Rosales, particularly Rosaceae, including a hypanthium, five part flowers and free floral parts. As circumscribed, Saxifragales account for 1.3% of eudicot diversity.

Diversification among Saxifragales was rapid, with the extensive fossil record indicating that the order was more diverse and more widespread than an examination of the extant members suggests, with considerable phenotypic diversity occurring early. The earliest fossil evidence is found in the Turonian-Campanian (late Cretaceous), suggesting a minimum age of 89.5 Myr. However, molecular divergence time estimation suggest an earlier time of 102–108 Myr, into the early Cretaceous, for the crown and stem groups respectively. Within the order Saxifragales, the molecular data imply a very rapid initial diversification time of about 6–8 Myr, between 112–120 Myr, with major lineages appearing within 3–6 Myr.

The ancestral state appears to be woody, as in Peridiscaceae and the woody clade, but is also ancestral to Grossulariaceae. A number of independent transitions to a herbaceous habit occurred in the ancestors of Crassulaceae, Saxifragaceae and the base of the Haloragaceae-Penthoraceae clade (the other two families in Haloragaceae "s.l." remaining woody}, while other taxa reverted to a woody habit, especially Crassulaceae. Most of Saxifragales have a superior ovary, but some families show frequent transition with inferior or subinferior position, particularly Saxifragaceae and to a lesser extent Hamamelidaceae. Almost all Grossulariaceae have an inferior ovary. The ancestral carpel number is two, with transition to higher numbers, such as four in Haloragaceae "s.l." and Peridiscaceae with five in Penthoraceae. The ancestral carpel number for Crassulaceae is five, decreasing to four in "Kalanchoe", where it is synapomorphic for the genus, though the most frequent transition in this family is 6–10, but only where stamen number is increased above five. Some Macaronesian taxa (Aeonieae) have 8–12, with up to 32 carpels for "Aeonium".

The ancestral petal number is five, with three major transitions; 5 to 0, 5 to 4, 5 to 6–10. Increased petal number is seen in Paeoniaceae and Crassulaceae, particularly where stamen number is also increased. Cercidiphyllum + Daphniphyllum, Chrysosplenium and "Altingia" are examples of the complete loss of petals. The ancestral stamen:petal ratio is 1, with transitions characterising several clades, e.g. Paeonicaceae+woody clade >2, Crassulaceae 2 (but "Crassula" 1). Overall there has been a decrease over evolution, but independent of a decrease in petal number, so that it is the stamen number that has decreased. The ancestral habitat appears to be forests, followed by early diversification into desert and aquatic habitats, with shrubland the most recent colonization.

Species diversification was rapid following a transition from a warmer, wetter Earth in the Eocene (56–40 Myr) to early Miocene (23–16 Myr), to the cooler drier conditions of the mid-Miocene (16–12 Myr). However, this appears to not have coincided with ecological and phenotypic evolution, which are themselves correlated. There is a clear lag, whereby increase in species diversification was followed later by increases in niche and phenotypic lability.

The first APG classification (1998) placed 13 families with the order Saxifragales: 


This was subsequently revised to 15, in the fourth version (2016). The Saxifragales families have been grouped into a number of informally named suprafamilial subclades, with the exception of the basal split of Peridiscaceae, which thus forms a sister group with the rest of Saxifragales. The two major ones are (Paeoniaceae + the woody clade of primarily woody families) and the "core" Saxifragales (i.e. the primarily herbaceous families), with the latter subdivided into two further subclades, (Haloragaceae "sensu lato" + Crassulaceae) and the Saxifragaceae alliance.

In the clade Haloragaceae "sensu lato" "(s.l.)" + Crassulaceae the genera constituting Haloragaceae "s.l." are all small, and APG II (2003) proposed merging them into a single larger Haloragaceae "s.l.", but transferred "Aphanopetalum" from Cunoniaceae to this group. The Saxifragaceae alliance represents Saxifragaceae together with a number of woody members of the traditional Saxifragaceae "sensu" Engler (1930). Within this, APG II (2003) proposed placing the two species of "Pterostemon" that constitute Pterostemonaceae within Iteaceae, and all subsequent versions have maintained this practice. Thus Saxifragales "sensu" APG II consisted of only 10 families. The third version (2009) added Peridiscaceae (from Malpighiales), as sister to all other families, but re-expanded Haloragaceae to provide for a narrower circumscription, Haloragaceae "sensu strictu" ("s.s."), to give a total of 14 families. APG IV (2016) added the parasitic family Cynomoriaceae to provide a total of 15 families, although its placement within the order remained unclear.

Of the 15 families included in APG IV, the basal divergence Peridiscaceae underwent radical shifting and recircumscription from 2003 to 2009. Originally, it consisted of two closely related genera, "Peridiscus" and "Whittonia". The APG II system placed the family in Malpighiales, based on a DNA sequence for the "rbcL" gene from "Whittonia". This sequence turned out to be not from "Whittonia", but from other plants whose DNA had contaminated the sample. After placement in Saxifragales, it was expanded to include "Soyauxia" in 2007, and "Medusandra" in 2009.

In the first of the subclades of the remaining Saxifragales, Paeoniaceae possesses many unique features and its taxonomic position was controversial for a long time, and "Paeonia" was placed in Ranunculales, close to "Glaucidium", prior to transfer to Saxifragales as sister to the woody clade.

In the woody clade, the genus "Liquidamber" was included in Hamamelidaceae until molecular phylogenetic studies showed that its inclusion might make Hamamelidaceae paraphyletic, and was segregated as a separate monotypic family, Altingiaceae in 2008. Cercidiphyllaceae was for a long time associated with Hamamelidaceae and Trochodendraceae and was often thought to be closer to the latter, which is now in the basal eudicot order Trochodendrales. "Daphniphyllum" was always thought to have an anomalous combination of characters and was placed in several different orders before molecular phylogenetic analysis showed it to belong to Saxifragales.

In the core Saxifragales, Crassulaceae and Tetracarpaeaceae have been associated with Saxifragaceae, while "Penthorum" has been associated both with Crassulaceae and Saxifragaceae, before being placed here. "Aphanopetalum" was often placed in Cunoniaceae, a family in Oxalidales, even though there were good reasons to put it in Saxifragales, and it was subsequently transferred. Haloragaceae was included in Myrtales, before being placed in Saxifragales.

The other "core" group, the Saxifragaceae alliance comprises four families: Pterostemonaceae, Iteaceae, Grossulariaceae, and Saxifragaceae, which have long been known to be related to each other, but the circumscription of Saxifragaceae has been much reduced and Pterostemonaceae submerged as "Pterostemon" in Iteaceae.

Most of the families are monogeneric. "Choristylis" is now considered a synonym of Itea, but the addition of "Pterostemon", gives Iteaceae two genera. "Liquidambar" and "Semiliquidambar" are also submerged into "Altingia", making Altingiaceae monogeneric. About 95% of the species are in five families: Crassulaceae (1400), Saxifragaceae (500), Grossulariaceae (150–200), Haloragaceae (150), and Hamamelidaceae (100).

The relationships of the Saxifragales families to each other is shown in the following cladogram. The phylogeny in this cladogram still has some uncertainty as to the exact relationships, and the phylogenetic tree is subject to further revision. Cynomoriaceae, previously placed in Santales or Rosales is included in Saxifragales, but unplaced within it. Li et al. (2019) have slightly different relationships, and also place Cynomoriaceae as the first branch in the Crassulaceae+Haloragaceae "s.l." tree, i.e. as sister to those two families. The number of genera in each family is shown in parentheses:

Saxifragales are found worldwide, though primarily in temperate zones and rarely in the tropics. They occupy a wide variety of habitats from arid desert (Crassulaceae) to aquatic conditions (Haloragaceae), with 6 families, including North American species, that are obligate aquatic (fully dependant on an aquatic environment), and including forests, grasslands and tundra. Saxifragales exceeds all other comparably sized clades in terms of diversity of habitats. Most of the diversity occurs in temperate (including montane and arid) conditions that expanded globally during cooling and drying trends in the last 15 My.

The most common habitats are forests and cliffs, with about 300 species occupying each, but with forests being the most diverse phenotypically, where nearly all families are represented. In contrast desert and tundra, with only two families each, contain only about 10% of species. About 90% of species can be assigned to a single habitat.

"Whittonia" (Peridiscaceae) is thought to be extinct. the IUCN lists 9 critically endangered, 12 endangered, 19 vulnerable and 7 near threatened species. Among the most threatened Saxifragales are "Aichryson dumosum" and "Monanthes wildpretii" (Crassulaceae), "Haloragis stokesii" and "Myriophyllum axilliflorum" (Haloragaceae), "Ribes malvifolium" and "R. sardoum" (Grossulariaceae), "Saxifraga artvinensis" (Saxifragaceae) and "Molinadendron hondurense" (Hamamelidaceae).

Plants in the order Saxifragales have found a wide variety of uses, including traditional medicines, ornamental, household, aquarium, pond and garden plants, spices, foodstuffs (fruit and greens), dyestuffs, smoking, resin, timber and roof coverings (see Families).

A number of Saxifragales genera are commercially cultivated. "Paeonia" are cultivated both as ornamental shrubs (generally sold as root stock) and for cut flowers, with the Netherlands representing the largest production, other more minor producers are Israel, New Zealand, Chile and the United States. "Liquidamber" is used for hardwood, with the American Sweetgum ("Liquidambar styraciflua") being among the most important sources of commercial hardwood in the Southeast United States, with one of its uses being veneer for plywood. "Hamamelis" is cultivated in New England for distilleries extracting witch-hazel, widely used in skincare, and is the largest source of this medicament in the world. Among the Crassulaceae, economic importance is limited to horticulture, with many species and cultivars important as ornamentals, including "Crassula ovata" (jade plant) and "Jovibarba" (hen and chicken). "Hylotelphium", "Phedimus", "Sedum" and "Sempervivum" are cultivated for rock gardens and for "green roofs". In particular, cultivars of the Madagascan "Kalanchoe blossfeldiana", e.g. 'Florists kalanchoe' have achieved commercial success throughout the world, being popular Christmas decorative plants. The Haloragaceae aquatic genus "Myriophyllum" and the closely related "Proserpinaca" are cultivated for the commercial aquarium trade. "Myriophyllum" is also economically important for purification of water and as feed for pigs, ducks, and fish, and polishing wood. 

A number of "Ribes" (Grossulariaceae) are in commercial production, concentrated in Europe and the USSR from species native to those areas. "R. nigrum" (blackcurrant) was first cultivated in monastery gardens in Russia in the 11th century, and currant cultivation more generally later in Western Europe, "R. uva-crispa" (gooseberry) production began around 1700. The first colonists in N America began cultivating currants in the late 1700s. "R. nigrum" is the most important commercial currant crop, being produced in more than 23 countries, with the major centres being Russia (more than 63 thousand hectares), Poland, Germany, Scandinavia and the UK. An important source of Vitamin C, black currants are used in the manufacture of jam, fruit jelly, compote, syrup, juice and other drinks, including the cordial Ribena and the liqueur Cassis. Other commercial crops include "R. rubrum" (red currant). World "Ribes" crop production was over 750,000 tons in 2002, of which about 150,000 tons were gooseberries, and the largest group blackcurrants.














</doc>
<doc id="26988" url="https://en.wikipedia.org/wiki?curid=26988" title="CLIÉ">
CLIÉ

The Sony CLIÉ is a series of personal digital assistants running the Palm Operating System developed and marketed by Sony from 2000 to 2005. The devices introduced many new features to the PDA market, such as a jog-wheel interface, high-resolution displays, and Sony technologies like Memory Stick slots and ATRAC3 audio playback. Most models were designed and manufactured in Japan. The name is an acronym for "creativity, lifestyle, innovation, emotion" though formerly "communication, link, information and entertainment". It was initially an attempt at a new coinage term, though it means "tool" in the Jèrriais language.

The CLIÉ handhelds were distinguished from other Palm OS models by their emphasis on multimedia capabilities, including photo, video, and audio playback, long before any other Palm OS PDAs had such capabilities. Later models have been credited with spurring competition in the previously stagnant Palm market, closing many of the gaps that existed between Palm OS PDAs and those powered by Microsoft's Windows Mobile operating system, particularly on the multimedia front, but also with Sony's proprietary application launcher interface.

In the summer of 2004, Sony announced that new CLIÉs would, from then on, be manufactured and available only in Japan, and in the spring of 2005, Sony announced the total termination of its CLIÉ line of products. The last models to be released worldwide were the PEG-TJ27, PEG-TJ37, and PEG-TH55. The last model released in Japan was the PEG-VZ90. Soon after the closure of the CLIÉ line, Sony stopped providing original installation drivers, including Sony's version of Palm Desktop for the CLIÉ, which are necessary for Hotsyncing with the PC and otherwise taking advantage of the handhelds' many features for which a PC may be required. Several CLIÉ fans took it upon themselves to offer these drivers freely for download at www.sonyclie.org.

CLIÉ handhelds were released in series, usually with a few models released in each series. In later years, multiple series would be in production at the same time.



Officially, the CLIÉ line did not support the Mac OS, and Sony never provided any software with the handhelds for Mac OS. However, as a Palm OS device, every CLIÉ handheld was inherently capable of HotSync operations with a Mac OS computer. This allowed for the synchronization of the basic PIM functions, as well as for the installation of new software, though this inherent capability was unusable because the Mac HotSync software would not recognize the handheld. PalmSource, however, silently added the capability to recognize older CLIÉ devices when providing new versions of its Palm Desktop software for Mac. This was necessary for those who could synchronize only via USB.

The CLIÉ user community soon discovered that these "updates" were simply a matter of adding a few lines to the USB-detection property-list file. Since then, detailed instructions have been posted online for those who want to synchronize their CLIÉ handhelds. No modifications are required for Bluetooth synchronization, but Wi-Fi synchronization is impossible because the Mac OS HotSync software does not support network synchronization. Some workarounds for the multimedia features also exist. For those who desire stronger Mac OS/CLIÉ integration, the product Missing Sync made by the company Mark/Space is also available. This does make unencrypted Wi-Fi synchronization possible but a bug in the CLIÉ network stack reverses IP addresses which means that the Macintosh involved needs a palindromic IP address such as 10.0.0.10.




</doc>
<doc id="26989" url="https://en.wikipedia.org/wiki?curid=26989" title="Sony">
Sony

Sony Corporation is the electronics business unit and the parent company of the , which is engaged in business through its four operating components: electronics (AV, IT & communication products, semiconductors, video games, network services and medical business), motion pictures (movies and TV shows), music (record labels and music publishing) and financial services (banking and insurance). These make Sony one of the most comprehensive entertainment companies in the world. The group consists of Sony Corporation, Sony Pictures, Sony Mobile, Sony Interactive Entertainment, Sony Music, Sony Financial Holdings, and others.

Sony is among the semiconductor sales leaders and since 2015, the fifth-largest television manufacturer in the world after Samsung Electronics, LG Electronics, TCL and Hisense.

The company's current slogan is "Be Moved". Their former slogans were "The One and Only" (1979–1982), "It's a Sony" (1982–2005), "like.no.other" (2005–2009) and "make.believe" (2009–2013).

Sony has a weak tie to the Sumitomo Mitsui Financial Group (SMFG) corporate group, the successor to the Mitsui group.

Sony began in the wake of World War II. In 1946, Masaru Ibuka started an electronics shop in a department store building in Tokyo. The company started with a capital of ¥190,000 and a total of eight employees. On 7 May 1946, Ibuka was joined by Akio Morita to establish a company called (Tokyo Telecommunications Engineering Corporation). The company built Japan's first tape recorder, called the Type-G. In 1958, the company changed its name to "Sony".

When "Tokyo Tsushin Kogyo" was looking for a romanized name to use to market themselves, they strongly considered using their initials, TTK. The primary reason they did not is that the railway company Tokyo Kyuko was known as TTK. The company occasionally used the acronym "Totsuko" in Japan, but during his visit to the United States, Morita discovered that Americans had trouble pronouncing that name. Another early name that was tried out for a while was "Tokyo Teletech" until Akio Morita discovered that there was an American company already using Teletech as a brand name.

The name "Sony" was chosen for the brand as a mix of two words: one was the Latin word ""sonus"", which is the root of sonic and sound, and the other was ""sonny"", a common slang term used in 1950s America to call a young boy. In 1950s Japan, "sonny boys" was a loan word in Japanese, which connoted smart and presentable young men, which Sony founders Akio Morita and Masaru Ibuka considered themselves to be.

The first Sony-branded product, the TR-55 transistor radio, appeared in 1955 but the company name did not change to Sony until January 1958.

At the time of the change, it was extremely unusual for a Japanese company to use Roman letters to spell its name instead of writing it in kanji. The move was not without opposition: TTK's principal bank at the time, Mitsui, had strong feelings about the name. They pushed for a name such as Sony Electronic Industries, or Sony Teletech. Akio Morita was firm, however, as he did not want the company name tied to any particular industry. Eventually, both Ibuka and Mitsui Bank's chairman gave their approval.

According to Schiffer, Sony's TR-63 radio "cracked open the U.S. market and launched the new industry of consumer microelectronics." By the mid-1950s, American teens had begun buying portable transistor radios in huge numbers, helping to propel the fledgling industry from an estimated 100,000 units in 1955 to 5 million units by the end of 1968.

Sony co-founder Akio Morita founded Sony Corporation of America in 1960. In the process, he was struck by the mobility of employees between American companies, which was unheard of in Japan at that time. When he returned to Japan, he encouraged experienced, middle-aged employees of other companies to reevaluate their careers and consider joining Sony. The company filled many positions in this manner, and inspired other Japanese companies to do the same. Moreover, Sony played a major role in the development of Japan as a powerful exporter during the 1960s, 1970s and 1980s. It also helped to significantly improve American perceptions of "made in Japan" products. Known for its production quality, Sony was able to charge above-market prices for its consumer electronics and resisted lowering prices.

In 1971, Masaru Ibuka handed the position of president over to his co-founder Akio Morita. Sony began a life insurance company in 1979, one of its many peripheral businesses. Amid a global recession in the early 1980s, electronics sales dropped and the company was forced to cut prices. Sony's profits fell sharply. "It's over for Sony," one analyst concluded. "The company's best days are behind it." Around that time, Norio Ohga took up the role of president. He encouraged the development of the Compact Disc in the 1970s and 1980s, and of the PlayStation in the early 1990s. Ohga went on to purchase CBS Records in 1988 and Columbia Pictures in 1989, greatly expanding Sony's media presence. Ohga would succeed Morita as chief executive officer in 1989.
Under the vision of co-founder Akio Morita and his successors, the company had aggressively expanded into new businesses. Part of its motivation for doing so was the pursuit of "convergence," linking film, music and digital electronics via the Internet. This expansion proved unrewarding and unprofitable, threatening Sony's ability to charge a premium on its products as well as its brand name. In 2005, Howard Stringer replaced Nobuyuki Idei as chief executive officer, marking the first time that a foreigner had run a major Japanese electronics firm. Stringer helped to reinvigorate the company's struggling media businesses, encouraging blockbusters such as "Spider-Man" while cutting 9,000 jobs. He hoped to sell off peripheral business and focus the company again on electronics. Furthermore, he aimed to increase cooperation between business units, which he described as "silos" operating in isolation from one another. In a bid to provide a unified brand for its global operations, Sony introduced a slogan known as "make.believe" in 2009.
Despite some successes, the company faced continued struggles in the mid- to late-2000s. In 2012, Kazuo Hirai was promoted to president and CEO, replacing Stringer. Shortly thereafter, Hirai outlined his company-wide initiative, named "One Sony" to revive Sony from years of financial losses and bureaucratic management structure, which proved difficult for former CEO Stringer to accomplish, partly due to differences in business culture and native languages between Stringer and some of Sony's Japanese divisions and subsidiaries. Hirai outlined three major areas of focus for Sony's electronics business, which include imaging technology, gaming and mobile technology, as well as a focus on reducing the major losses from the television business.
In February 2014, Sony announced the sale of its Vaio PC division to a new corporation owned by investment fund Japan Industrial Partners and spinning its TV division into its own corporation as to make it more nimble to turn the unit around from past losses totaling $7.8 billion over a decade. Later that month, they announced that they would be closing 20 stores. In April, the company announced that they would be selling 9.5 million shares in Square Enix (roughly 8.2 percent of the game company's total shares) in a deal worth approximately $48 million. In May 2014 the company announced it was forming two joint ventures with Shanghai Oriental Pearl Group to manufacture and market Sony's PlayStation games consoles and associated software in China.

It was reported in December 2016 by multiple news outlets that Sony was considering restructuring its U.S. operations by merging its TV & film business, Sony Pictures Entertainment, with its gaming business, Sony Interactive Entertainment. According to the reports, such a restructuring would have placed Sony Pictures under Sony Interactive's CEO, Andrew House, though House wouldn't have taken over day-to-day operations of the film studio. According to one report, Sony was set to make a final decision on the possibility of the merger of the TV, film, & gaming businesses by the end of its fiscal year in March of the following year (2017).

Sony has historically been notable for creating its own in-house standards for new recording and storage technologies, instead of adopting those of other manufacturers and standards bodies. Sony (either alone or with partners) has introduced several of the most popular recording formats, including the floppy disk, Compact Disc and Blu-ray Disc.

The company launched the Betamax videocassette recording format in 1975. Sony was involved in the videotape format war of the early 1980s, when they were marketing the Betamax system for video cassette recorders against the VHS format developed by JVC. In the end, VHS gained critical mass in the marketbase and became the worldwide standard for consumer VCRs.

Betamax is, for all practical purposes, an obsolete format. Sony's professional-oriented component video format called Betacam, which was derived from Betamax, was used until 2016 when Sony announced it was stopping production of all remaining 1/2-inch video tape recorders and players, including the Digital Betacam format.

In 1985, Sony launched their Handycam products and the Video8 format. Video8 and the follow-on hi-band Hi8 format became popular in the consumer camcorder market. In 1987 Sony launched the 4 mm DAT or Digital Audio Tape as a new digital audio tape standard.

In 1979, the Walkman brand was introduced, in the form of the world's first portable music player using the compact cassette format. Sony introduced the MiniDisc format in 1992 as an alternative to Philips DCC or Digital Compact Cassette and as a successor to the compact cassette. Since the introduction of MiniDisc, Sony has attempted to promote its own audio compression technologies under the ATRAC brand, against the more widely used MP3. Until late 2004, Sony's Network Walkman line of digital portable music players did not support the MP3 standard natively.

In 2004, Sony built upon the MiniDisc format by releasing Hi-MD. Hi-MD allows the playback and recording of audio on newly introduced 1 GB Hi-MD discs in addition to playback and recording on regular MiniDiscs. In addition to saving audio on the discs, Hi-MD allows the storage of computer files such as documents, videos and photos.

In 1993, Sony challenged the industry standard Dolby Digital 5.1 surround sound format with a newer and more advanced proprietary motion picture digital audio format called SDDS (Sony Dynamic Digital Sound). This format employed eight channels (7.1) of audio opposed to just six used in Dolby Digital 5.1 at the time. Ultimately, SDDS has been vastly overshadowed by the preferred DTS (Digital Theatre System) and Dolby Digital standards in the motion picture industry. SDDS was solely developed for use in the theatre circuit; Sony never intended to develop a home theatre version of SDDS.

Sony and Philips jointly developed the Sony-Philips digital interface format (S/PDIF) and the high-fidelity audio system SACD. The latter became entrenched in a format war with DVD-Audio. Neither gained a major foothold with the general public. CDs were preferred by consumers because of the ubiquitous presence of CD drives in consumer devices until the early 2000s when the iPod and streaming services became available.

In 1983, Sony followed their counterpart Philips to the compact disc (CD). In addition to developing consumer-based recording media, after the launch of the CD Sony began development of commercially based recording media. In 1986 they launched Write-Once optical discs (WO) and in 1988 launched Magneto-optical discs which were around 125MB size for the specific use of archival data storage. In 1984, Sony launched the Discman series which extended their Walkman brand to portable CD products.

In the early 1990s, two high-density optical storage standards were being developed: one was the MultiMedia Compact Disc (MMCD), backed by Philips and Sony, and the other was the Super Density disc (SD), supported by Toshiba and many others. Philips and Sony abandoned their MMCD format and agreed upon Toshiba's SD format with only one modification. The unified disc format was called DVD and was introduced in 1997.

Sony was one of the leading developers of the Blu-ray optical disc format, the newest standard for disc-based content delivery. The first Blu-ray players became commercially available in 2006. The format emerged as the standard for HD media over the competing format, Toshiba's HD DVD, after a two-year-long high-definition optical disc format war.

In 1983, Sony introduced 90 mm micro diskettes (better known as floppy disks), which it had developed at a time when there were 4" floppy disks, and a lot of variations from different companies, to replace the then on-going 5.25" floppy disks. Sony had great success and the format became dominant. 3.5" floppy disks gradually became obsolete as they were replaced by current media formats.

Sony launched in 1998, their Memory Stick format, flash memory cards for use in Sony lines of digital cameras and portable music players. It has seen little support outside of Sony's own products, with Secure Digital cards (SD) commanding considerably greater popularity. Sony has made updates to the Memory Stick format with Memory Stick Duo and Memory Stick Micro.

Sony offers products in a variety of product lines around the world. Sony has developed a music playing robot called Rolly, dog-shaped robots called AIBO and a humanoid robot called QRIO.

As of 1 April 2016, Sony is organized into the following business segments: Mobile Communications (MC), Game & Network Services (G&NS), Imaging Products & Solutions (IP&S), Home Entertainment & Sound (HE&S), Semiconductors, Components, Pictures, Music, Financial Services and All Other. The network and medical businesses are included in the G&NS and IP&S, respectively.

Sony Corporation is the electronics business unit and the parent company of the Sony Group. It primarily conducts strategic business planning of the group, research and development (R&D), planning, designing and marketing for electronics products. Its subsidiaries such as Sony Global Manufacturing & Operations Corporation (SGMO; 4 plants in Japan), Sony Semiconductor Manufacturing Corporation (7 plants in Japan), Sony Storage Media and Devices Corporation, Sony Energy Devices Corporation and its subsidiaries outside Japan (Brazil, China, UK (Wales), India, Malaysia, Singapore, South Korea, Thailand, Ireland and United States) are responsible for manufacturing as well as product engineering (SGMO is also responsible for customer service operations). In 2012, Sony rolled most of its consumer content services (including video, music and gaming) into the Sony Entertainment Network.

Sony produced the world's first portable music player, the Walkman in 1979. This line fostered a fundamental change in music listening habits by allowing people to carry music with them and listen to music through lightweight headphones. Walkman originally referred to portable audio cassette players. The company now uses the Walkman brand to market its portable audio and video players as well as a line of former Sony Ericsson mobile phones.

Sony utilized a related brand, Discman, to refer to its CD players. It dropped this name in the late 1990s.

Sony produced computers (MSX home computers and NEWS workstations) during the 1980s. The company withdrew from the computer business around 1990. Sony entered again into the global computer market under the new VAIO brand, began in 1996. Short for "Video Audio Integrated Operation", the line was the first computer brand to highlight visual-audio features.

Sony faced considerable controversy when some of its laptop batteries exploded and caught fire in 2006, resulting in the largest computer-related recall to that point in history.

In a bid to join the tablet computer market, the company launched its Sony Tablet line of Android tablets in 2011. Since 2012, Sony's Android products have been marketed under the Xperia brand used for its smartphones.

On 4 February 2014, Sony announced that it would sell its VAIO PC business due to poor sales and Japanese company Japan Industrial Partners (JIP) will purchase the VAIO brand, with the deal finalized by the end of March 2014. Sony maintains a minority stake in the new, independent company.

Sony offers a wide range of digital cameras. Point-and-shoot models adopt the Cyber-shot name, while digital single-lens reflex models are branded using Alpha.

The first Cyber-shot was introduced in 1996. At the time, digital cameras were a relative novelty. Sony's market share of the digital camera market fell from a high of 20% to 9% by 2005.

Sony entered the market for digital single-lens reflex cameras in 2006 when it acquired the camera business of Konica Minolta. Sony rebranded the company's line of cameras as its Alpha line. Sony is the world's third largest manufacturer of the cameras, behind Canon and Nikon respectively.

There are also a variety of Camcorders which are manufactured by Sony.

In 1968, Sony introduced the Trinitron brand name for its lines of aperture grille cathode ray tube televisions and (later) computer monitors. Sony stopped production of Trinitron for most markets, but continued producing sets for markets such as Pakistan, Bangladesh and China. Sony discontinued its series of Trinitron computer monitors in 2005. The company discontinued the last Trinitron-based television set in the US in early 2007. The end of Trinitron marked the end of Sony's analog television sets and monitors.

Sony used the LCD WEGA name for its LCD TVs until summer 2005. The company then introduced the BRAVIA name. BRAVIA is an in house brand owned by Sony which produces high-definition LCD televisions, projection TVs and front projectors, home cinemas and the BRAVIA home theatre range. All Sony high-definition flat-panel LCD televisions in North America have carried the logo for BRAVIA since 2005. Sony is the third-largest maker of televisions in the world. , Sony's television business has been unprofitable for eight years.
In December 2011, Sony agreed to sell all stake in an LCD joint venture with Samsung Electronics for about $940 million. On 28 March 2012, Sony Corporation and Sharp Corporation announced that they have agreed to further amend the joint venture agreement originally executed by the parties in July 2009, as amended in April 2011, for the establishment and operation of Sharp Display Products Corporation ("SDP"), a joint venture to produce and sell large-sized LCD panels and modules.

On 9 November 2015, Sony announced that they are going to stop producing Betamax Tapes in March 2016.

Sony also sells a range of DVD players. It has shifted its focus in recent years to promoting the Blu-ray format, including discs and players.

Sony produces a wide range of semiconductors and electronic components including image sensors (Exmor), image processor (BIONZ), laser diodes, system LSIs, mixed-signal LSIs, OLED panels, etc. The company has a strong presence in the image sensor market. Sony-manufactured CMOS image sensors are widely used in digital cameras, tablet computers and smartphones.

In April 2018, Sony announced to join the market for satellite communications and develop laser communication products for small satellites. Sony wants to use its heritage with optical disc technology used in products like CD players and plans to start initial tests in collaboration with JAXA in 2018.

Sony has targeted medical, healthcare and biotechnology business as a growth sector in the future. The company acquired iCyt Mission Technology, Inc. (renamed Sony Biotechnology Inc. in 2012), a manufacturer of flow cytometers, in 2010 and Micronics, Inc., a developer of microfluidics-based diagnostic tools, in 2011.

In 2012, Sony announced that it will acquire all shares of So-net Entertainment Corporation, which is the majority shareholder of M3, Inc., an operator of portal sites (m3.com, MR-kun, MDLinx and MEDI:GATE) for healthcare professionals.

On 28 September 2012, Olympus and Sony announced that the two companies will establish a joint venture to develop new surgical endoscopes with 4K resolution (or higher) and 3D capability. Sony Olympus Medical Solutions Inc. (Sony 51%, Olympus 49%) was established on 16 April 2013.

On 28 February 2014, Sony, M3 and Illumina established a joint venture called P5, Inc. to provide a genome analysis service for research institutions and enterprises in Japan.

Sony Mobile Communications Inc. (formerly Sony Ericsson) is a multinational mobile phone manufacturing company headquartered in Tokyo, Japan and a wholly owned subsidiary of Sony Corporation.

In 2001, Sony entered into a joint venture with Swedish telecommunications company Ericsson, forming Sony Ericsson. Initial sales were rocky, and the company posted losses in 2001 and 2002. However, SMC reached a profit in 2003. Sony Ericsson distinguished itself with multimedia-capable mobile phones, which included features such as cameras. These were unusual for the time. Despite their innovations, SMC faced intense competition from Apple's iPhone which released in 2007. From 2008 to 2010, amid a global recession, SMC slashed its workforce by several thousand. Sony acquired Ericsson's share of the venture in 2012 for over US$1 billion. In 2009, SMC was the fourth-largest mobile phone manufacturer in the world (after Nokia, Samsung and LG). By 2010, its market share had fallen to sixth place. Sony Mobile Communications now focuses exclusively on the smartphone market under the Xperia name. In 2015, Sony released Xperia Z5 Premium in Canada following US and Europe.

In the year 2013, Sony contributed to around two percent of the mobile phone market with 37 million mobile phones sold.

Sony Interactive Entertainment (formerly Sony Computer Entertainment) is best known for producing the popular line of PlayStation consoles. The line grew out of a failed partnership with Nintendo. Originally, Nintendo requested Sony to develop an add-on for its console that would play Compact Discs. In 1991 Sony announced the add-on, as well as a dedicated console known as the "Play Station". However, a disagreement over software licensing for the console caused the partnership to fall through. Sony then continued the project independently.

Launched in 1994, the first PlayStation gained 61% of global console sales and broke Nintendo's long-standing lead in the market. Sony followed up with the PlayStation 2 in 2000, which was even more successful. The console has become the most successful of all time, selling over 150 million units . Sony released the PlayStation 3, a high-definition console, in 2006. It was the first console to use the Blu-ray format, and was considerably more expensive than competitors Xbox 360 and Wii due to a Cell processor. Early on, poor sales performance resulted in significant losses for the company, pushing it to sell the console at a loss. The PlayStation 3 sold generally more poorly than its competitors in the early years of its release but managed to overtake the Xbox 360 in global sales later on. It later introduced the PlayStation Move, an accessory that allows players to control video games using motion gestures.
Sony extended the brand to the portable games market in 2004 with the PlayStation Portable (PSP). The console has sold reasonably, but has taken a second place to a rival handheld, the Nintendo DS. Sony developed the Universal Media Disc (UMD) optical disc medium for use on the PlayStation Portable. Early on, the format was used for movies, but it has since lost major studio support. Sony released a disc-less version of its PlayStation Portable, the PSP Go, in 2009. The company went on to release its second portable video game system, PlayStation Vita, in 2011 and 2012. Sony launched its fourth console, the PlayStation 4, on 15 November 2013, which as of 31 December 2017 has sold 73.6 million units globally.

On 18 March 2014, at GDC, president of Sony Computer Entertainment Worldwide Studios Shuhei Yoshida announced their new virtual reality technology dubbed Project Morpheus, and later named PlayStation VR, for PlayStation 4. The headset brought VR gaming and non-gaming software to the company's console. According to a report released by Houston-based patent consulting firm LexInnova in May 2015, Sony is leading the virtual reality patent race. According to the firm's analysis of nearly 12,000 patents or patent applications, Sony has 366 virtual reality patents or patent applications. PlayStation VR was released worldwide on 13 October 2016.

In 2014, Sony participated within NRG Energy eVgo Ready for Electric Vehicle (REV) program, for EV charging parking lots. Sony is in the business of electric vehicle lithium-ion batteries; on 28 July 2016, Sony announced that the company will sell its battery business to Murata Manufacturing.

IT giants such as Google (driverless car) and Apple (iCar/Project Titan) are working on electric vehicles and self driving cars, competing with Tesla; Sony is entering into this field by investing $842,000 in the ZMP company. On 6 January 2020 at the Consumer Electronics Show in Las Vegas, Sony announced a concept Electric Vehicle called the Vision-S.

In January 2020 Sony unveiled a concept electric car at the Consumer Electronics Show, named Vision-S, designed in collaboration with components manufacturer Magna International. At the occasion Sony also stated its goal of developing technology for the automotive sector, especially concerning autonomous driving and entertainment.

Sony Entertainment has two divisions: Sony Pictures Entertainment, Sony Music Group (Sony Music Entertainment, Sony/ATV Music Publishing).

Sony USA also previously owned and operated Sony Trans Com: a technology business that provided in-flight entertainment programming as well as video and audio playback equipment for the airline industry. Sony had purchased the business from Sundstrand Corp. in 1989 and subsequently sold it to Rockwell Collins in 2000.

Sony Pictures Entertainment Inc. (SPE) is the television and film production/distribution unit of Sony. With 12.5% box office market share in 2011, the company was ranked third among movie studios. Its group sales in 2010 were US$7.2 billion. The company has produced many notable movie franchises, including "Spider-Man", "The Karate Kid" and "Men in Black". It has also produced the popular television game shows "Jeopardy!" and "Wheel of Fortune".

Sony entered the television and film production market when it acquired Columbia Pictures Entertainment in 1989 for $3.4 billion. Columbia lives on in the Sony Pictures Motion Picture Group, a division of SPE which in turn owns Columbia Pictures and TriStar Pictures among other film production and distribution companies such as Screen Gems, Sony Pictures Classics, Sony Pictures Home Entertainment. SPE's television division is known as Sony Pictures Television.
For the first several years of its existence, Sony Pictures Entertainment performed poorly, leading many to suspect the company would sell off the division. Sony Pictures Entertainment encountered controversy in the early 2000s. In July 2000, a marketing executive working for Sony Corporation created a fictitious film critic, David Manning, who gave consistently good reviews for releases from Sony subsidiary Columbia Pictures that generally received poor reviews amongst real critics. Sony later pulled the ads, suspended Manning's creator and his supervisor and paid fines to the state of Connecticut and to fans who saw the reviewed films in the US. In 2006 Sony started using ARccOS Protection on some of their film DVDs, but later issued a recall.

In late 2014, Sony Pictures became the target of a hack attack from a clandestine group called Guardians of Peace, weeks before releasing the anti-North Korean comedy film "The Interview".

Sony Music Entertainment (also known as SME or Sony Music) is the second-largest global recorded music company of the "big three" record companies and is controlled by Sony Corporation of America, the United States subsidiary of Japan's "Sony".

In one of its largest-ever acquisitions, Sony purchased CBS Record Group in 1988 for US$2 billion. In the process, Sony partnered and gained the rights to the ATV catalogue of Michael Jackson, considered by the "Guinness Book of World Records" to be the most successful entertainer of all time. The acquisition of CBS Records provided the foundation for the formation of Sony Music Entertainment, which Sony established in 1991.

In 2004, Sony entered into a joint venture with Bertelsmann AG, merging Sony Music Entertainment with Bertelsmann Music Group to create Sony BMG. In 2005, Sony BMG faced a copy protection scandal, because its music CDs had installed malware on users' computers that was posing a security risk to affected customers. In 2007, the company acquired Famous Music for US$370 million, gaining the rights to the catalogues of Eminem and Akon, among others.

Sony bought out Bertelsmann's share in the company and formed a new Sony Music Entertainment in 2008. Since then, the company has undergone management changes. In January 1988, Sony acquired CBS Records and the 50% of CBS/Sony Group. In March 1988, four wholly owned subsidiaries were folded into CBS/Sony Group and the company was renamed as Sony Music Entertainment Japan

Sony purchased digital music recognition company Gracenote for US$260 million in 2008. Tribune Media Company acquired Gracenote from Sony in 2014 for $170 million.

Besides its record label, Sony operates other music businesses. In 1995, Sony merged its publisher with Michael Jackson's ATV Music Publishing, forming Sony/ATV Music Publishing. At the time, the publishing company was the second largest of its kind in the world. The company owns the publishing rights to over 4 million compositions, including The Beatles' Lennon-McCartney catalogue, Bob Dylan, Eminem, Lady Gaga, Sam Smith, Ed Sheeran, and Taylor Swift.

In 2012, Sony/ATV then acquired a majority stake in EMI Music Publishing, making them the world's largest music publishing company. As of 2016, Sony owns all of Sony/ATV.<ref name="Sony Finalizes Acquisition of Michael Jackson Estate's Stake in Sony/ATV Publishing"></ref>

Sony Financial Holdings is a holding company for Sony's financial services business. It owns and oversees the operation of Sony Life (in Japan and the Philippines), Sony Assurance, Sony Bank and Sony Bank Securities. The company is headquartered in Tokyo, Japan. Sony Financial accounts for half of Sony's global earnings. The unit proved the most profitable of Sony's businesses in fiscal year 2006, earning $1.7 billion in profit. Sony Financial's low fees have aided the unit's popularity while threatening Sony's premium brand name.

Sony wants to contend with Apple and Samsung on mobile payments in Asia. Sony plans to use its contact-less payment technology to make ground in the public transportation industry across Asia. The system, known as FeliCa, relies on two forms of technologies to make it viable, either chips embedded in smartphones or plastic cards with chips embedded in them. Sony plans to implement this technology in train systems in Indonesia as early as Spring 2016.

Sony is a "kabushiki gaisha" registered to the Tokyo Stock Exchange in Japan and the New York Stock Exchange for overseas trading. As of 30 September 2017, there are 484,812 shareholders and 1,264,649,260 shares issued. Most of these shares are held by foreign institutions and investors.

Sony is one of Japan's largest corporations by revenue. It had revenues of ¥6.493 trillion in 2012. It also maintains large reserves of cash, with ¥895 billion on hand as of 2012. In May 2012, Sony shares were valued at about $15 billion.

The company was immensely profitable throughout the 1990s and early 2000s, in part because of the success of its new PlayStation line. The company encountered financial difficulty in the mid- to late-2000s due to a number of factors: the global financial crisis, increased competition for PlayStation, and the devastating Japanese earthquake of 2011. The company faced three consecutive years of losses leading up to 2011. While noting the negative effects of intervening circumstances such as natural disasters and fluctuating currency exchange rates, the "Financial Times" criticized the company for its "lack of resilience" and "inability to gauge the economy." The newspaper voiced skepticism about Sony's revitalization efforts, given a lack of tangible results.

In September 2000 Sony had a market capitalization of $100 billion; but by December 2011 it had plunged to $18 billion, reflecting falling prospects for Sony but also reflecting grossly inflated share prices of the 'dot.com' years. Net worth, as measured by stockholder equity, has steadily grown from $17.9 billion in March 2002 to $35.6 billion through December 2011. Earnings yield (inverse of the price to earnings ratio) has never been more than 5% and usually much less; thus Sony has always traded in over-priced ranges with the exception of the 2009 market bottom.

On 9 December 2008, Sony Corporation announced that it would be cutting 8,000 jobs, dropping 8,000 contractors and reducing its global manufacturing sites by 10% to save $1.1 billion per year.

In April 2012, Sony announced that it would reduce its workforce by 10,000 (6% of its employee base) as part of CEO Hirai's effort to get the company back into the black. This came after a loss of 520 billion yen (roughly US$6.36 billion) for fiscal 2012, the worst since the company was founded. Accumulation loss for the past four years was 919.32 billion-yen. Sony planned to increase its marketing expenses by 30% in 2012. 1,000 of the jobs cut come from the company's mobile phone unit's workforce. 700 jobs will be cut in the 2012–2013 fiscal year and the remaining 300 in the following fiscal year.

In January 2013, Sony announced it was selling its US headquarters building for $1.1 billion to a consortium led by real estate developer The Chetrit Group.

On 28 January 2014, Moody's Investors Services dropped Sony's credit rating to Ba1—"judged to have speculative elements and a significant credit risk"—saying that the company's "profitability is likely to remain weak and volatile."

On 6 February 2014, Sony announced it would trim as many as 5,000 jobs as it attempts to sell its PC business and focus on mobile and tablets.

In 2014, Sony South Africa closed its TV, Hi-Fi and camera divisions with the purpose of reconsidering its local distribution model and, in 2017, it returned facilitated by Premium Brand Distributors (Pty) Ltd.
In November 2011, Sony was ranked 9th (jointly with Panasonic) in Greenpeace's Guide to Greener Electronics. This chart grades major electronics companies on their environmental work. The company scored 3.6/10, incurring a penalty point for comments it has made in opposition to energy efficiency standards in California. It also risks a further penalty point in future editions for being a member of trade associations that have commented against energy efficiency standards. Together with Philips, Sony receives the highest score for energy policy advocacy after calling on the EU to adopt an unconditional 30% reduction target for greenhouse gas emissions by 2020. Meanwhile, it receives full marks for the efficiency of its products. In June 2007, Sony ranked 14th on the Greenpeace guide. Sony fell from its earlier 11th-place ranking due to Greenpeace's claims that Sony had double standards in their waste policies.

Since 1976, Sony has had an Environmental Conference. Sony's policies address their effects on global warming, the environment, and resources. They are taking steps to reduce the amount of greenhouse gases that they put out as well as regulating the products they get from their suppliers in a process that they call "green procurement". Sony has said that they have signed on to have about 75 percent of their Sony Building running on geothermal power. The "Sony Take Back Recycling Program" allow consumers to recycle the electronics products that they buy from Sony by taking them to eCycle (Recycling) drop-off points around the U.S. The company has also developed a biobattery that runs on sugars and carbohydrates that works similarly to the way living creatures work. This is the most powerful small biobattery to date.

In 2000, Sony faced criticism for a document entitled "NGO Strategy" that was leaked to the press. The document involved the company's surveillance of environmental activists in an attempt to plan how to counter their movements. It specifically mentioned environmental groups that were trying to pass laws that held electronics-producing companies responsible for the cleanup of the toxic chemicals contained in their merchandise.

Sony Corporation is actively involved in the EYE SEE project conducted by UNICEF. EYE SEE digital photography workshops have been run for children in Argentina, Tunisia, Mali, South Africa, Ethiopia, Madagascar, Rwanda, Liberia and Pakistan.

Sony assists The South Africa Primary Education Support Initiative (SAPESI) through financial donations and children book donations to the South Africa Mobile Library Project.

The Sony Canada Charitable Foundation (SCCF) is a non-profit organization which supports three key charities; the Make-A-Wish Canada, the United Way of Canada and the EarthDay and ECOKIDS program.

After the 2011 Queensland floods and Victorian bushfires, Sony Music released benefit albums with money raised going to the Sony Foundation. You Can is the youth cancer program of Sony Foundation.

Sony launched its Open Planet Ideas Crowdsourcing Project, in partnership with the World Wildlife Fund and the design group, IDEO.

On the occasion of the 2014 World Cup in Brazil, Sony partnered with and launched the Street Football Stadium Project to support football-based educational programmes in local communities across Latin America and Brazil. More than 25 Street Stadiums were developed since the project's inception.




</doc>
<doc id="26990" url="https://en.wikipedia.org/wiki?curid=26990" title="Social psychology">
Social psychology

Social psychology is the scientific study of how people's thoughts, feelings, and behaviors are influenced by the actual, imagined or implied presence of others. In this definition, "scientific" refers to the empirical investigation using the scientific method. The terms "thoughts", "feelings", and "behaviors" refer to psychological variables that can be measured in humans. The statement that others' presence may be "imagined" or "implied" suggests that humans are malleable to social influences even when alone, such as when watching videos, sitting on the toilet, or quietly appreciating art. In such situations, people can be influenced to follow internalized cultural norms. Social psychologists typically explain human behavior as a result of the interaction of mental states and social situations.

Social psychologists examine factors that cause behaviors to unfold in a given way in the presence of others. They study conditions under which certain behavior, actions, and feelings occur. Social psychology is concerned with the way these feelings, thoughts, beliefs, intentions, and goals are cognitively constructed and how these mental representations, in turn, influence our interactions with others.

Social psychology traditionally bridged the gap between psychology and sociology. During the years immediately following World War II there was frequent collaboration between psychologists and sociologists. The two disciplines, however, have become increasingly specialized and isolated from each other in recent years, with sociologists focusing on "macro variables" (e.g., social structure) to a much greater extent than psychologists. Nevertheless, sociological approaches to psychology remain an important counterpart to psychological research in this area.

In addition to the split between psychology and sociology, there has been a somewhat less pronounced difference in emphasis between American social psychologists and European social psychologists. As a generalization, American researchers traditionally have focused more on the individual, whereas Europeans have paid more attention to group level phenomena (see group dynamics).

Although there were some older writings about social psychology, such as those by Islamic philosopher Al-Farabi (Alpharabius), the discipline of social psychology, as its modern-day definition, began in the United States at the beginning of the 20th century. By that time, though, the discipline had already developed a significant foundation. Following the 18th century, those in the emerging field of social psychology were concerned with developing concrete explanations for different aspects of human nature. They attempted to discover concrete cause and effect relationships that explained the social interactions in the world around them. In order to do so, they believed that the scientific method, an empirically based scientific measure, could be applied to human behavior.

The first published study in this area was an experiment in 1898 by Norman Triplett, on the phenomenon of social facilitation. During the 1930s, many Gestalt psychologists, most notably Kurt Lewin, fled to the United States from Nazi Germany. They were instrumental in developing the field as something separate from the behavioral and psychoanalytic schools that were dominant during that time, and social psychology has always maintained the legacy of their interests in perception and cognition. Attitudes and small group phenomena were the most commonly studied topics in this era.

During World War II, social psychologists studied persuasion and propaganda for the U.S. military. After the war, researchers became interested in a variety of social problems, including gender issues and racial prejudice. Most notable, revealing, and contentious of these were the Stanley Milgram shock experiments on obedience to authority. In the sixties, there was growing interest in new topics, such as cognitive dissonance, bystander intervention, and aggression. By the 1970s, however, social psychology in America had reached a crisis. There was heated debate over the ethics of laboratory experimentation, whether or not attitudes really predicted behavior, and how much science could be done in a cultural context. This was also the time when a radical situationist approach challenged the relevance of self and personality in psychology.
Throughout the 1980s and 1990s social psychology reached a more mature level. Two of the areas social psychology matured in were theories and methods. Careful ethical standards now regulate research. Pluralistic and multicultural perspectives have emerged. Modern researchers are interested in many phenomena, but attribution, social cognition, and the self-concept are perhaps the greatest areas of growth in recent years. Social psychologists have also maintained their applied interests with contributions in the social psychology of health, education, law, and the workplace.

In social psychology, attitudes are defined as learned, global evaluations of a person, object, place, or issue that influence thought and action. Put more simply, attitudes are basic expressions of approval or disapproval, favorability or unfavorability, or as Bem put it, likes and dislikes. Examples would include liking chocolate ice cream, or endorsing the values of a particular political party.

Social psychologists have studied attitude formation, the structure of attitudes, attitude change, the function of attitudes, and the relationship between attitudes and behavior. Because people are influenced by the situation, general attitudes are not always good predictors of specific behavior. For example, for a variety of reasons, a person may value the environment but not recycle a can on a particular day.

In recent times, research on attitudes has examined the distinction between traditional, self-reported attitude measures and "implicit" or unconscious attitudes. For example, experiments using the Implicit Association Test have found that people often demonstrate implicit bias against other races, even when their explicit responses reveal equal mindedness. One study found that explicit attitudes correlate with verbal behavior in interracial interactions, whereas implicit attitudes correlate with nonverbal behavior.

One hypothesis on how attitudes are formed, first advanced by Abraham Tesser in 1983, is that strong likes and dislikes are ingrained in our genetic make-up. Tesser speculates that individuals are disposed to hold certain strong attitudes as a result of inborn physical, sensory, and cognitive skills, temperament, and personality traits. Whatever disposition nature elects to give us, our most treasured attitudes are often formed as a result of exposure to attitude objects; our history of rewards and punishments; the attitude that our parents, friends, and enemies express; the social and cultural context in which we live; and other types of experiences we have. Obviously, attitudes are formed through the basic process of learning. Numerous studies have shown that people can form strong positive and negative attitudes toward neutral objects that are in some way linked to emotionally charged stimuli.

Attitudes are also involved in several other areas of the discipline, such as conformity, interpersonal attraction, social perception, and prejudice.

The topic of persuasion has received a great deal of attention in recent years. Persuasion is an active method of influence that attempts to guide people toward the adoption of an attitude, idea, or behavior by rational or emotive means. Persuasion relies on "appeals" rather than strong pressure or coercion. Numerous variables have been found to influence the persuasion process; these are normally presented in five major categories: "who" said "what" to "whom" and "how".

Dual-process theories of persuasion (such as the elaboration likelihood model) maintain that the persuasive process is mediated by two separate routes; central and peripheral. The central route of persuasion is more fact-based and results in longer lasting change, but requires motivation to process. The peripheral route is more superficial and results in shorter lasting change, but does not require as much motivation to process. An example of a peripheral route of persuasion might be a politician using a flag lapel pin, smiling, and wearing a crisp, clean shirt. Notice that this does not require motivation to be persuasive, but should not last as long as persuasion based on the central route. If that politician were to outline exactly what they believed, and their previous voting record, this would be using the central route, and would result in longer lasting change, but would require a good deal of motivation to process.

Social cognition is a growing area of social psychology that studies how people perceive, think about, and remember information about others. Much research rests on the assertion that people think about (other) people differently from non-social targets. This assertion is supported by the social cognitive deficits exhibited by people with Williams syndrome and autism. Person perception is the study of how people form impressions of others. The study of how people form beliefs about each other while interacting is known as interpersonal perception.

A major research topic in social cognition is attribution. Attributions are the explanations we make for people's behavior, either our own behavior or the behavior of others. One element of attribution ascribes the locus of a behavior to either internal or external factors. An "internal", or dispositional, attribution assigns behavior to causes related to inner traits such as personality, disposition, character or ability. An "external", or situational, attribution involves situational elements, such as the weather. A second element of attribution ascribes the cause of behavior to either stable or unstable factors (whether the behavior will be repeated or changed under similar circumstances). Finally, we also attribute causes of behavior to either controllable or uncontrollable factors: how much control one has over the situation at hand.

Numerous biases in the attribution process have been discovered. For instance, the fundamental attribution error is the tendency to make dispositional attributions for behavior, overestimating the influence of personality and underestimating the influence of situations. The actor-observer difference is a refinement of this bias, the tendency to make dispositional attributions for other people's behavior and situational attributions for our own. The self-serving bias is the tendency to attribute dispositional causes for successes, and situational causes for failure, particularly when self-esteem is threatened. This leads to assuming one's successes are from innate traits, and one's failures are due to situations, including other people. Other ways people protect their self-esteem are by believing in a just world, blaming victims for their suffering, and making defensive attributions, which explain our behavior in ways which defend us from feelings of vulnerability and mortality. Researchers have found that mildly depressed individuals often lack this bias and actually have more realistic perceptions of reality (as measured by the opinions of others).

Heuristics are cognitive short cuts. Instead of weighing all the evidence when making a decision, people rely on heuristics to save time and energy. The availability heuristic occurs when people estimate the probability of an outcome based on how easy that outcome is to imagine. As such, vivid or highly memorable possibilities will be perceived as more likely than those that are harder to picture or are difficult to understand, resulting in a corresponding cognitive bias. The representativeness heuristic is a shortcut people use to categorize something based on how similar it is to a prototype they know of. Numerous other biases have been found by social cognition researchers. The hindsight bias is a false memory of having predicted events, or an exaggeration of actual predictions, after becoming aware of the outcome. The confirmation bias is a type of bias leading to the tendency to search for, or interpret information in a way that confirms one's preconceptions.

Another key concept in social cognition is the assumption that reality is too complex to easily discern. As a result, we tend to see the world according to simplified schemas or images of reality. Schemas are generalized mental representations that organize knowledge and guide information processing. Schemas often operate automatically and unintentionally, and can lead to biases in perception and memory. Expectations from schemas may lead us to see something that is not there. One experiment found that people are more likely to misperceive a weapon in the hands of a black man than a white man. This type of schema is actually a stereotype, a generalized set of beliefs about a particular group of people (when incorrect, an ultimate attribution error). Stereotypes are often related to negative or preferential attitudes (prejudice) and behavior (discrimination). Schemas for behaviors (e.g., going to a restaurant, doing laundry) are known as "scripts".

Self-concept is a term referring to the whole sum of beliefs that people have about themselves. However, what specifically does self-concept consist of? According to Hazel Markus (1977), the self-concept is made up of cognitive molecules called self-schemas – beliefs that people have about themselves that guide the processing of self-reliant information. For example, an athlete at a university would have multiple selves that would process different information pertinent to each self: the student would be one "self," who would process information pertinent to a student (taking notes in class, completing a homework assignment, etc.); the athlete would be the "self" who processes information about things related to being an athlete (recognizing an incoming pass, aiming a shot, etc.). These "selves" are part of one's identity and the self-reliant information is the information that relies on the proper "self" to process and react on it. If a "self" is not part of one's identity, then it is much more difficult for one to react. For example, a civilian may not know how to handle a hostile threat as a trained Marine would. The Marine contains a "self" that would enable him/her to process the information about the hostile threat and react accordingly, whereas a civilian may not contain that self, disabling them from properly processing the information from the hostile threat and, furthermore, debilitating them from acting accordingly. Self-schemas are to an individual's total self–concept as a hypothesis is to a theory, or a book is to a library. A good example is the body weight self-schema; people who regard themselves as over or underweight, or for those whom body image is a significant self-concept aspect, are considered "schematics" with respect to weight. For these people a range of otherwise mundane events – grocery shopping, new clothes, eating out, or going to the beach – can trigger thoughts about the self. In contrast, people who do not regard their weight as an important part of their lives are "a-schematic" on that attribute.

It is rather clear that the self is a special object of our attention. Whether one is mentally focused on a memory, a conversation, a foul smell, the song that is stuck in one's head, or this sentence, consciousness is like a spotlight. This spotlight can shine on only one object at a time, but it can switch rapidly from one object to another and process the information out of awareness. In this spotlight the self is front and center: things relating to the self have the spotlight more often.

The self's ABCs are affect, behavior, and cognition. An affective (or emotional) question: How do people evaluate themselves, enhance their self-image, and maintain a secure sense of identity? A behavioral question: How do people regulate their own actions and present themselves to others according to interpersonal demands? A cognitive question: How do individuals become themselves, build a self-concept, and uphold a stable sense of identity?

Affective forecasting is the process of predicting how one would feel in response to future emotional events. Studies done by Timothy Wilson and Daniel Gilbert in 2003 have shown that people overestimate the strength of reaction to anticipated positive and negative life events that they actually feel when the event does occur.

There are many theories on the perception of our own behavior. Daryl Bem's (1972) self-perception theory claims that when internal cues are difficult to interpret, people gain self-insight by observing their own behavior. Leon Festinger's 1954 social comparison theory is that people evaluate their own abilities and opinions by comparing themselves to others when they are uncertain of their own ability or opinions. There is also the facial feedback hypothesis: that changes in facial expression can lead to corresponding changes in emotion.

The fields of social psychology and personality have merged over the years, and social psychologists have developed an interest in self-related phenomena. In contrast with traditional personality theory, however, social psychologists place a greater emphasis on cognitions than on traits. Much research focuses on the self-concept, which is a person's understanding of their self. The self-concept is often divided into a cognitive component, known as the "self-schema", and an evaluative component, the "self-esteem". The need to maintain a healthy self-esteem is recognized as a central human motivation in the field of social psychology.

Self-efficacy beliefs are associated with the self-schema. These are expectations that performance on some task will be effective and successful. Social psychologists also study such self-related processes as self-control and self-presentation.

People develop their self-concepts by varied means, including introspection, feedback from others, self-perception, and social comparison. By comparing themselves to relevant others, people gain information about themselves, and they make inferences that are relevant to self-esteem. Social comparisons can be either "upward" or "downward," that is, comparisons to people who are either higher in status or ability, or lower in status or ability. Downward comparisons are often made in order to elevate self-esteem.

Self-perception is a specialized form of attribution that involves making inferences about oneself after observing one's own behavior. Psychologists have found that too many extrinsic rewards (e.g. money) tend to reduce intrinsic motivation through the self-perception process, a phenomenon known as overjustification. People's attention is directed to the reward and they lose interest in the task when the reward is no longer offered. This is an important exception to reinforcement theory.

Social influence is an overarching term given to describe the persuasive effects people have on each other. It is seen as a fundamental value in social psychology and overlaps considerably with research on attitudes and persuasion. The three main areas of social influence include: conformity, compliance, and obedience. Social influence is also closely related to the study of group dynamics, as most principles of influence are strongest when they take place in social groups.

The first major area of social influence is conformity. Conformity is defined as the tendency to act or think like other members of a group. The identity of members within a group, i.e. status, similarity, expertise, as well as cohesion, prior commitment, and accountability to the group help to determine the level of conformity of an individual. Individual variation among group members plays a key role in the dynamic of how willing people will be to conform. Conformity is usually viewed as a negative tendency in American culture, but a certain amount of conformity is adaptive in some situations, as is nonconformity in other situations.

The second major area of social influence research is compliance. Compliance refers to any change in behavior that is due to a request or suggestion from another person. The foot-in-the-door technique is a compliance method in which the persuader requests a small favor and then follows up with requesting a larger favor, e.g., asking for the time and then asking for ten dollars. A related trick is the bait and switch.

The third major form of social influence is obedience; this is a change in behavior that is the result of a direct order or command from another person. Obedience as a form of compliance was dramatically highlighted by the Milgram study, wherein people were ready to administer shocks to a person in distress on a researcher's command.

An unusual kind of social influence is the self-fulfilling prophecy. This is a prediction that, in being made, actually causes itself to become true. For example, in the stock market, if it is widely believed that a crash is imminent, investors may lose confidence, sell most of their stock, and thus actually cause the crash. Similarly, people may expect hostility in others and actually induce this hostility by their own behavior.

Psychologist have spent decades studying the power of social influence, and the way in which it manipulates people's opinions and behavior. Specifically, social influence refers to the way in which individuals change their ideas and actions to meet the demands of a social group, received authority, social role or a minority within a group wielding influence over the majority. No matter if you are student, teacher, doctor, lawyer or entrepreneur, you will encounter some type of social influence.

A group can be defined as two or more individuals that are connected to each another by social relationships. Groups tend to interact, influence each other, and share a common identity. They have a number of emergent qualities that distinguish them from aggregates:
Temporary groups and aggregates share few or none of these features, and do not qualify as true social groups. People waiting in line to get on a bus, for example, do not constitute a group.

Groups are important not only because they offer social support, resources, and a feeling of belonging, but because they supplement an individual's self-concept. To a large extent, humans define themselves by the group memberships which form their social identity. The shared social identity of individuals within a group influences intergroup behavior, the way in which groups behave towards and perceive each other. These perceptions and behaviors in turn define the social identity of individuals within the interacting groups. The tendency to define oneself by membership in a group may lead to intergroup discrimination, which involves favorable perceptions and behaviors directed towards the in-group, but negative perceptions and behaviors directed towards the out-group. On the other hand, such discrimination and segregation may sometimes exist partly to facilitate a diversity which strengthens society. Intergroup discrimination leads to prejudice and stereotyping, while the processes of social facilitation and group polarization encourage extreme behaviors towards the out-group.

Groups often moderate and improve decision making, and are frequently relied upon for these benefits, such as in committees and juries. A number of group biases, however, can interfere with effective decision making. For example, group polarization, formerly known as the "risky shift," occurs when people polarize their views in a more extreme direction after group discussion. More problematic is the phenomenon of groupthink. This is a collective thinking defect that is characterized by a premature consensus or an incorrect assumption of consensus, caused by members of a group failing to promote views which are not consistent with the views of other members. Groupthink occurs in a variety of situations, including isolation of a group and the presence of a highly directive leader. Janis offered the 1961 Bay of Pigs Invasion as a historical case of groupthink.

Groups also affect performance and productivity. Social facilitation, for example, is a tendency to work harder and faster in the presence of others. Social facilitation increases the "dominant response"s likelihood, which tends to improve performance on simple tasks and reduce it on complex tasks. In contrast, social loafing is the tendency of individuals to slack off when working in a group. Social loafing is common when the task is considered unimportant and individual contributions are not easy to see.

Social psychologists study group-related (collective) phenomena such as the behavior of crowds. An important concept in this area is deindividuation, a reduced state of self-awareness that can be caused by feelings of anonymity. Deindividuation is associated with uninhibited and sometimes dangerous behavior. It is common in crowds and mobs, but it can also be caused by a disguise, a uniform, alcohol, dark environments, or online anonymity.

A major area in the study of people's relations to each other is interpersonal attraction. This refers to all forces that lead people to like each other, establish relationships, and (in some cases) fall in love. Several general principles of attraction have been discovered by social psychologists, but many still continue to experiment and do research to find out more. One of the most important factors in interpersonal attraction is how similar two particular people are. The more similar two people are in general attitudes, backgrounds, environments, worldviews, and other traits, the more probable an attraction is possible.

Physical attractiveness is an important element of romantic relationships, particularly in the early stages characterized by high levels of passion. Later on, similarity and other compatibility factors become more important, and the type of love people experience shifts from "passionate" to "companionate". Robert Sternberg has suggested that there are actually three components of love: intimacy, passion, and commitment. When two (or more) people experience all three, they are said to be in a state of consummate love.

According to social exchange theory, relationships are based on rational choice and cost-benefit analysis. If one partner's costs begin to outweigh their benefits, that person may leave the relationship, especially if there are good alternatives available. This theory is similar to the minimax principle proposed by mathematicians and economists (despite the fact that human relationships are not zero-sum games). With time, long term relationships tend to become communal rather than simply based on exchange.

Social psychology is an empirical science that attempts to answer questions about human behavior by testing hypotheses, both in the laboratory and in the field. Careful attention to sampling, research design, and statistical analysis is important; results are published in peer reviewed journals such as the "Journal of Experimental Social Psychology", "Personality and Social Psychology Bulletin" and the "Journal of Personality and Social Psychology". Social psychology studies also appear in general science journals such as "Psychological Science" and "Science".

Experimental methods involve the researcher altering a variable in the environment and measuring the effect on another variable. An example would be allowing two groups of children to play violent or nonviolent videogames, and then observing their subsequent level of aggression during free-play period. A valid experiment is controlled and uses random assignment.

Correlational methods examine the statistical association between two naturally occurring variables. For example, one could correlate the amount of violent television children watch at home with the number of violent incidents the children participate in at school. Note that this study would "not" prove that violent TV causes aggression in children: it is quite possible that aggressive children choose to watch more violent TV.

Observational methods are purely descriptive and include naturalistic observation, "contrived" observation, participant observation, and archival analysis. These are less common in social psychology but are sometimes used when first investigating a phenomenon. An example would be to unobtrusively observe children on a playground (with a videocamera, perhaps) and record the number and types of aggressive actions displayed.

Whenever possible, social psychologists rely on controlled experimentation. Controlled experiments require the manipulation of one or more independent variables in order to examine the effect on a dependent variable. Experiments are useful in social psychology because they are high in internal validity, meaning that they are free from the influence of confounding or extraneous variables, and so are more likely to accurately indicate a causal relationship. However, the small samples used in controlled experiments are typically low in external validity, or the degree to which the results can be generalized to the larger population. There is usually a trade-off between experimental control (internal validity) and being able to generalize to the population (external validity).

Because it is usually impossible to test everyone, research tends to be conducted on a sample of persons from the wider population. Social psychologists frequently use survey research when they are interested in results that are high in external validity. Surveys use various forms of random sampling to obtain a sample of respondents that are representative of a population. This type of research is usually descriptive or correlational because there is no experimental control over variables. Some psychologists have raised concerns about social psychological research for relying too heavily on studies conducted on university undergraduates in academic settings, or participants from crowdsourcing labor markets such as Amazon Mechanical Turk. In David Sears' 1986 study, over 70% of experiments used North American undergraduates as subjects, a subset of the population that are unrepresentative of the population as a whole.

Regardless of which method has been chosen to be used, the results are of high importance. Results need to be used to evaluate the hypothesis of the research that is done. These results should either confirm or reject the original hypothesis that was predicted. There are two different types of testing social psychologists use in order to test their results. Statistics and probability testing define a significant finding that can be as low as 5% or less, likely to be due to chance. Replications are important, to ensure that the result is valid and not due to chance, or some feature of a particular sample. False positive conclusions, often resulting from the pressure to publish or the author's own confirmation bias, are a hazard in the field.

The Asch conformity experiments demonstrated the power of conformity in small groups with a line length estimation task that was designed to be extremely easy. In well over a third of the trials, participants conformed to the majority, who had been instructed to provide incorrect answers, even though the majority judgment was clearly wrong. Seventy-five percent of the participants conformed at least once during the experiment. Additional manipulations to the experiment showed participant conformity decreased when at least one other individual failed to conform, but increased when the individual began conforming or withdrew from the experiment. Also, participant conformity increased substantially as the number of incorrect individuals increased from one to three, and remained high as the incorrect majority grew. Participants with three incorrect opponents made mistakes 31.8% of the time, while those with one or two incorrect opponents made mistakes only 3.6% and 13.6% of the time, respectively.

Muzafer Sherif's Robbers' Cave Experiment divided boys into two competing groups to explore how much hostility and aggression would emerge. Sherif's explanation of the results became known as realistic group conflict theory, because the intergroup conflict was induced through competition over resources. Inducing cooperation and superordinate goals later reversed this effect.

In Leon Festinger's cognitive dissonance experiment, participants were asked to perform a boring task. They were divided into 2 groups and given two different pay scales. At the study's end, some participants were paid $1 to say that they enjoyed the task and another group of participants was paid $20 to say the same lie. The first group ($1) later reported liking the task better than the second group ($20). Festinger's explanation was that for people in the first group being paid only $1 is not sufficient incentive for lying and those who were paid $1 experienced dissonance. They could only overcome that dissonance by justifying their lies by changing their previously unfavorable attitudes about the task. Being paid $20 provides a reason for doing the boring task, therefore no dissonance.

One of the most notable experiments in social psychology was the Milgram experiment, which studied how far people would go to obey an authority figure. Following the events of The Holocaust in World War II, the experiment showed that (most) normal American citizens were capable of following orders from an authority even when they believed they were causing an innocent person to suffer.

Albert Bandura's Bobo doll experiment demonstrated how aggression is learned by imitation. This set of studies fueled debates regarding media violence which continue to be waged among scholars.

In the Stanford prison study, by Philip Zimbardo, a simulated exercise between student prisoners and guards showed how far people would follow an adopted role. In just a few days, the "guards" became brutal and cruel, and the prisoners became miserable and compliant. This was initially argued to be an important demonstration of the power of the immediate social situation and its capacity to overwhelm normal personality traits. However, to this day, it remains a matter of contention what conclusions may be drawn from this study. For example, it has been pointed out that participant self-selection may have affected the participants' behaviour, and that the participants' personality influenced their reactions in a variety of ways, including how long they chose to remain in the study. One of the most concerted empirical revisitations of the themes raised by Zimbardo came with the 2002 BBC prison study.

The goal of social psychology is to understand cognition and behavior as they naturally occur in a social context, but the very act of observing people can influence and alter their behavior. For this reason, many social psychology experiments utilize deception to conceal or distort certain aspects of the study. Deception may include false cover stories, false participants (known as confederates or stooges), false feedback given to the participants, and so on.

The practice of deception has been challenged by some psychologists who maintain that deception under any circumstances is unethical, and that other research strategies (e.g., role-playing) should be used instead. Unfortunately, research has shown that role-playing studies do not produce the same results as deception studies and this has cast doubt on their validity. In addition to deception, experimenters have at times put people into potentially uncomfortable or embarrassing situations (e.g., the Milgram experiment and Stanford prison experiment), and this has also been criticized for ethical reasons.

To protect the rights and well-being of research participants, and at the same time discover meaningful results and insights into human behavior, virtually all social psychology research must pass an ethical review process. At most colleges and universities, this is conducted by an ethics committee or Institutional Review Board. This group examines the proposed research to make sure that no harm is likely to be done to the participants, and that the study's benefits outweigh any possible risks or discomforts to people taking part in the study.

Furthermore, a process of informed consent is often used to make sure that volunteers know what will happen in the experiment and understand that they are allowed to quit the experiment at any time. A debriefing is typically done at the experiment's conclusion in order to reveal any deceptions used and generally make sure that the participants are unharmed by the procedures. Today, most research in social psychology involves no more risk of harm than can be expected from routine psychological testing or normal daily activities.

Social Psychology plays a key role in a child's development. During this time, teens are faced with many issues and decisions that can impact a teen's social development. They are faced with self esteem issues, peer pressure, drugs, alcohol, tobacco, sex, social media and more. Psychologists today are not fully aware of the effect of social media. Social media is worldwide, so one can be influenced by something they will never encounter in real life. In 2019, social media had become the single most important activity in adolescents and even some older adults lives.

Social psychology has recently found itself at the center of a "replication crisis" due to some research findings proving difficult to replicate. Replication failures are not unique to social psychology and are found in all fields of science. However, several factors have combined to put social psychology at the center of the current controversy.

Firstly, questionable research practices (QRP) have been identified as common in the field. Such practices, while not necessarily intentionally fraudulent, involve converting undesired statistical outcomes into desired outcomes via the manipulation of statistical analyses, sample size or data management, typically to convert non-significant findings into significant ones. Some studies have suggested that at least mild versions of QRP are highly prevalent. One of the critics of Daryl Bem in the feeling the future controversy has suggested that the evidence for precognition in this study could (at least in part) be attributed to QRP.

Secondly, social psychology has found itself at the center of several recent scandals involving outright fraudulent research. Most notably the admitted data fabrication by Diederik Stapel as well as allegations against others. However, most scholars acknowledge that fraud is, perhaps, the lesser contribution to replication crises.

Third, several effects in social psychology have been found to be difficult to replicate even before the current replication crisis. For example, the scientific journal "Judgment and Decision Making" has published several studies over the years that fail to provide support for the unconscious thought theory. Replications appear particularly difficult when research trials are pre-registered and conducted by research groups not highly invested in the theory under questioning.

These three elements together have resulted in renewed attention for replication supported by Daniel Kahneman. Scrutiny of many effects have shown that several core beliefs are hard to replicate. A recent special edition of the journal Social Psychology focused on replication studies and a number of previously held beliefs were found to be difficult to replicate. A 2012 special edition of the journal "Perspectives on Psychological Science" also focused on issues ranging from publication bias to null-aversion that contribute to the replication crises in psychology

It is important to note that this replication crisis does not mean that social psychology is unscientific. Rather this process is a healthy if sometimes acrimonious part of the scientific process in which old ideas or those that cannot withstand careful scrutiny are pruned. The consequence is that some areas of social psychology once considered solid, such as social priming, have come under increased scrutiny due to failed replications.




</doc>
<doc id="26992" url="https://en.wikipedia.org/wiki?curid=26992" title="Suleiman the Magnificent">
Suleiman the Magnificent

Suleiman I ( '; or '; 6 November 1494 – 6 September 1566), commonly known as Suleiman the Magnificent in the West and Kanunî Sultan Süleyman (; "Suleiman the Lawgiver") in his realm, was the tenth and longest-reigning Sultan of the Ottoman Empire from 1520 until his death in 1566. Under his administration, the Ottoman state ruled over at least 25 million people.

Suleiman succeeded his father as sultan in September 1520 and began his reign with campaigns against the Christian powers in central Europe and the Mediterranean. Belgrade fell to him in 1521 and Rhodes, long under the rule of the Knights of St. John, in 1522–23. At Mohács, in August 1526, Suleiman broke the military strength of Hungary, with the Hungarian king Louis II losing his life in the battle.

Suleiman became a prominent monarch of 16th-century Europe, presiding over the apex of the Ottoman Empire's economic, military and political power. Suleiman personally led Ottoman armies in conquering the Christian strongholds of Belgrade and Rhodes as well as most of Hungary before his conquests were checked at the Siege of Vienna in 1529. He annexed much of the Middle East in his conflict with the Safavids and large areas of North Africa as far west as Algeria. Under his rule, the Ottoman fleet dominated the seas from the Mediterranean to the Red Sea and through the Persian Gulf.

At the helm of an expanding empire, Suleiman personally instituted major legislative changes relating to society, education, taxation and criminal law. His reforms, carried out in conjunction with the empire's chief judicial official Ebussuud Efendi, harmonized the relationship between the two forms of Ottoman law: sultanic (Kanun) and religious (Sharia). He was a distinguished poet and goldsmith; he also became a great patron of culture, overseeing the "Golden" age of the Ottoman Empire in its artistic, literary and architectural development.

Breaking with Ottoman tradition, Suleiman married Hürrem Sultan, a woman from his harem, a Christian of Ruthenian origin who converted to Islam, and who became famous in the West by the name Roxelana, purportedly due to her red hair. Their son Selim II succeeded Suleiman following his death in 1566 after 46 years of rule. Suleiman's other potential heirs, Mehmed and Mustafa, had died; the former had died from smallpox, and the latter had been strangled to death 13 years earlier at the sultan's order. His other son Bayezid was executed in 1561 on Suleiman's orders, along with Bayezid's four sons, after a rebellion. Although scholars no longer believe that the empire declined after his death, the end of Suleiman's reign is still frequently characterized as a watershed in Ottoman history. In the decades after Suleiman, the empire began to experience significant political, institutional, and economic changes, a phenomenon often referred to as the Transformation of the Ottoman Empire.

Suleiman the Magnificent ( "Muḥteşem Süleymān"), as he was known in the West, was also called Suleiman the First ( "Sulṭān Süleymān-ı Evvel"), and Suleiman the Lawgiver ( "Ḳānūnī Sulṭān Süleymān") for his reform of the Ottoman legal system.

It is unclear when exactly the term "Kanunî" (the Lawgiver) first came to be used as an epithet for Suleiman. It is entirely absent from sixteenth and seventeenth-century Ottoman sources, and may date from the early eighteenth century.

Suleiman was born in Trabzon along the east coast of the Black Sea to Şehzade Selim (later Selim I), probably on 6 November 1494, although this date is not known with absolute certainty. His mother was Hafsa Sultan, a convert to Islam of unknown origins, who died in 1534. At the age of seven, Suleiman was sent to study science, history, literature, theology and military tactics in the schools of the imperial Topkapı Palace in Constantinople (modern Istanbul). As a young man, he befriended Pargalı Ibrahim, a slave who later became one of his most trusted advisers (but who was later executed on Suleiman's orders).
From the age of seventeen, he was appointed as the governor of first Kaffa (Theodosia), then Manisa, with a brief tenure at Edirne.

Upon the death of his father, Selim I (r. 1512–1520), Suleiman entered Constantinople and ascended to the throne as the tenth Ottoman Sultan. An early description of Suleiman, a few weeks following his accession, was provided by the Venetian envoy Bartolomeo Contarini: "The sultan is only twenty-five years [actually 26] old, tall and slender but tough, with a thin and bony face. Facial hair is evident but only barely. The sultan appears friendly and in good humor. Rumor has it that Suleiman is aptly named, enjoys reading, is knowledgeable and shows good judgment." Some historians claim that in his youth Suleiman had an admiration for Alexander the Great.

Upon succeeding his father, Suleiman began a series of military conquests, eventually suppressing a revolt led by the Ottoman-appointed governor of Damascus in 1521. Suleiman soon made preparations for the conquest of Belgrade from the Kingdom of Hungary—something his great-grandfather Mehmed II had failed to achieve because of John Hunyadi's strong defense in the region. Its capture was vital in removing the Hungarians and Croats who, following the defeats of the Albanians, Bosniaks, Bulgarians, Byzantines and the Serbs, remained the only formidable force who could block further Ottoman gains in Europe. Suleiman encircled Belgrade and began a series of heavy bombardments from an island in the Danube. Belgrade, with a garrison of only 700 men, and receiving no aid from Hungary, fell in August 1521.

The fall of Christendom's major strongholds spread fear across central Europe. As the ambassador of the Holy Roman Empire to Constantinople was to note, "The capture of Belgrade was at the origin of the dramatic events which engulfed Hungary. It led to the death of King Louis, the capture of Buda, the occupation of Transylvania, the ruin of a flourishing kingdom and the fear of neighboring nations that they would suffer the same fate ..."

The road to Hungary and Austria lay open, but Suleiman turned his attention instead to the Eastern Mediterranean island of Rhodes, the home base of the Knights Hospitaller. In the summer of 1522, taking advantage of the large navy he inherited from his father, Suleiman dispatched an armada of some 400 ships towards Rhodes, while personally leading an army of 180,000 across Asia Minor to a point opposite the island itself. Here Suleiman built a large fortification, Marmaris Castle, that served as a base for the Ottoman Navy. Following the five-month Siege of Rhodes (1522), Rhodes capitulated and Suleiman allowed the Knights of Rhodes to depart. The conquest of the island cost the Ottomans 50,000 to 60,000 dead from battle and sickness (Christian claims went as high as 64,000 Ottoman battle deaths and 50,000 disease deaths).

As relations between Hungary and the Ottoman Empire deteriorated, Suleiman resumed his campaign in Central Europe, and on 29 August 1526 he defeated Louis II of Hungary (1506–1526) at the Battle of Mohács. In its wake, Hungarian resistance collapsed, and the Ottoman Empire became the preeminent power in Central Europe. Upon encountering the lifeless body of King Louis, Suleiman is said to have lamented: "I came indeed in arms against him; but it was not my wish that he should be thus cut off before he scarcely tasted the sweets of life and royalty." While Suleiman was campaigning in Hungary, Turkmen tribes in central Anatolia (in Cilicia) revolted under the leadership of Kalender Çelebi.

Some Hungarian nobles proposed that Ferdinand, who was the ruler of neighboring Austria and tied to Louis II's family by marriage, be King of Hungary, citing previous agreements that the Habsburgs would take the Hungarian throne if Louis died without heirs. However, other nobles turned to the nobleman Ioan Zápolya, who was being supported by Suleiman. Under Charles V and his brother Ferdinand I, the Habsburgs reoccupied Buda and took possession of Hungary. Reacting in 1529, Suleiman marched through the valley of the Danube and regained control of Buda; in the following autumn, his forces laid siege to Vienna. This was to be the Ottoman Empire's most ambitious expedition and the apogee of its drive to the West. With a reinforced garrison of 16,000  men, the Austrians inflicted the first defeat on Suleiman, sowing the seeds of a bitter Ottoman–Habsburg rivalry that lasted until the 20th century. His second attempt to conquer Vienna failed in 1532, as Ottoman forces were delayed by the siege of Güns and failed to reach Vienna. In both cases, the Ottoman army was plagued by bad weather, forcing them to leave behind essential siege equipment, and was hobbled by overstretched supply lines.

By the 1540s a renewal of the conflict in Hungary presented Suleiman with the opportunity to avenge the defeat suffered at Vienna. In 1541 the Habsburgs attempted to lay siege to Buda but were repulsed, and more Habsburg fortresses were captured by the Ottomans in two consecutive campaigns in 1541 and 1544 as a result, Ferdinand and Charles were forced to conclude a humiliating five-year treaty with Suleiman. Ferdinand renounced his claim to the Kingdom of Hungary and was forced to pay a fixed yearly sum to the Sultan for the Hungarian lands he continued to control. Of more symbolic importance, the treaty referred to Charles V not as 'Emperor' but as the 'King of Spain', leading Suleiman to identify as the true 'Caesar'.

As Suleiman stabilized his European frontiers, he now turned his attention to the ever-present threat posed by the Shi'a Safavid dynasty of Persia. Two events in particular were to precipitate a recurrence of tensions. First, Shah Tahmasp had the Baghdad governor loyal to Suleiman killed and replaced with an adherent of the Shah, and second, the governor of Bitlis had defected and sworn allegiance to the Safavids. As a result, in 1533, Suleiman ordered his Grand Vizier Pargalı Ibrahim Pasha to lead an army into eastern Asia Minor where he retook Bitlis and occupied Tabriz without resistance. Having joined Ibrahim in 1534, Suleiman made a push towards Persia, only to find the Shah sacrificing territory instead of facing a pitched battle, resorting to harassment of the Ottoman army as it proceeded along the harsh interior. When in the following year Suleiman made a grand entrance into Baghdad, he greatly enhanced his prestige by restoring the tomb of Abu Hanifa, the founder of the Hanafi school of Islamic law to which the Ottomans adhered.

Attempting to defeat the Shah once and for all, Suleiman embarked upon a second campaign in 1548–1549. As in the previous attempt, Tahmasp avoided confrontation with the Ottoman army and instead chose to retreat, using scorched earth tactics in the process and exposing the Ottoman army to the harsh winter of the Caucasus. Suleiman abandoned the campaign with temporary Ottoman gains in Tabriz and the Urmia region, a lasting presence in the province of Van, control of the western half of Azerbaijan and some forts in Georgia.

In 1553 Suleiman began his third and final campaign against the Shah. Having initially lost territories in Erzurum to the Shah's son, Suleiman retaliated by recapturing Erzurum, crossing the Upper Euphrates and laying waste to parts of Persia. The Shah's army continued its strategy of avoiding the Ottomans, leading to a stalemate from which neither army made any significant gain. In 1554, a settlement was signed which was to conclude Suleiman's Asian campaigns. Part of the treaty included and confirmed the return of Tabriz, but secured Baghdad, lower Mesopotamia, the mouths of the river Euphrates and Tigris, as well as part of the Persian Gulf.

Ottoman ships had been sailing in the Indian Ocean since the year 1518. Ottoman Admirals such as Hadim Suleiman Pasha, Seydi Ali Reis and Kurtoğlu Hızır Reis are known to have voyaged to the Mughal imperial ports of Thatta, Surat and Janjira. The Mughal Emperor Akbar the Great himself is known to have exchanged six documents with Suleiman the Magnificent.

Suleiman led several naval campaigns against the Portuguese in an attempt to remove them and reestablish trade with the Mughal Empire. Aden in Yemen was captured by the Ottomans in 1538, in order to provide an Ottoman base for raids against Portuguese possessions on the western coast of the Mughal Empire. Sailing on, the Ottomans failed against the Portuguese at the Siege of Diu in September 1538, but then returned to Aden, where they fortified the city with 100 pieces of artillery. From this base, Sulayman Pasha managed to take control of the whole country of Yemen, also taking Sana'a.

With its strong control of the Red Sea, Suleiman successfully managed to dispute control of the trade routes to the Portuguese and maintained a significant level of trade with the Mughal Empire throughout the 16th century.

From 1526 till 1543, Suleiman stationed over 900 Turkish soldiers to fight alongside the Somali Adal Sultanate led by Ahmad ibn Ibrahim al-Ghazi during the Conquest of Abyssinia. After the first Ajuran-Portuguese war, the Ottoman Empire would in 1559 absorb the weakened Adal Sultanate into its domain. This expansion fathered Ottoman rule in Somalia and the Horn of Africa. This also increased its influence in the Indian Ocean to compete with the Portuguese Empire with its close ally, the Ajuran Empire.

In 1564, Suleiman received an embassy from Aceh (a sultanate on Sumatra, in modern Indonesia), requesting Ottoman support against the Portuguese. As a result, an Ottoman expedition to Aceh was launched, which was able to provide extensive military support to the Acehnese.

The discovery of new maritime trade routes by Western European states allowed them to avoid the Ottoman trade monopoly. The Portuguese discovery of the Cape of Good Hope in 1488 initiated a series of Ottoman-Portuguese naval wars in the Ocean throughout the 16th century. The Ajuran Sultanate allied with the Ottomans defied the Portuguese economic monopoly in the Indian Ocean by employing a new coinage which followed the Ottoman pattern, thus proclaiming an attitude of economic independence in regard to the Portuguese.

Having consolidated his conquests on land, Suleiman was greeted with the news that the fortress of Koroni in Morea (the modern Peloponnese, peninsular Greece) had been lost to Charles V's admiral, Andrea Doria. The presence of the Spanish in the Eastern Mediterranean concerned Suleiman, who saw it as an early indication of Charles V's intention to rival Ottoman dominance in the region. Recognizing the need to reassert naval preeminence in the Mediterranean, Suleiman appointed an exceptional naval commander in the form of Khair ad Din, known to Europeans as Barbarossa. Once appointed admiral-in-chief, Barbarossa was charged with rebuilding the Ottoman fleet, to such an extent that the Ottoman navy equaled in number those of all other Mediterranean countries put together.

In 1535, Charles V led a Holy League of 27,000 soldiers (10,000 Spaniards, 8,000 Italians, 8,000 Germans, and 700 Knights of St. John) to victory against the Ottomans at Tunis, which together with the war against Venice the following year, led Suleiman to accept proposals from Francis I of France to form an alliance against Charles.
Huge Muslim territories in North Africa were annexed. The Barbary States of Tripolitania, Tunisia and Algeria became autonomous provinces of the Empire, serving as the leading edge of Suleiman's conflict with Charles V, whose attempt to drive out the Turks failed in 1541. The piracy carried on thereafter by the Barbary pirates of North Africa can be seen in the context of the wars against Spain.
In 1542, facing a common Habsburg enemy, Francis I sought to renew the Franco-Ottoman alliance. As a result, Suleiman dispatched 100 galleys under Barbarossa to assist the French in the western Mediterranean.

Elsewhere in the Mediterranean, when the Knights Hospitallers were re-established as the Knights of Malta in 1530, their actions against Muslim navies quickly drew the ire of the Ottomans, who assembled another massive army in order to dislodge the Knights from Malta. The Ottomans invaded Malta in 1565, undertaking the Great Siege of Malta, which began on 18 May and lasted until 8 September, and is portrayed vividly in the frescoes of Matteo Perez d'Aleccio in the Hall of St. Michael and St. George. At first it seemed that this would be a repeat of the battle on Rhodes, with most of Malta's cities destroyed and half the Knights killed in battle; but a relief force from Spain entered the battle, resulting in the loss of 10,000 Ottoman troops and the victory of the local Maltese citizenry.

While Sultan Suleiman was known as "the Magnificent" in the West, he was always "Kanuni" Suleiman or "The Lawgiver" () to his own Ottoman subjects. The overriding law of the empire was the Shari'ah, or Sacred Law, which as the divine law of Islam was outside of the Sultan's powers to change. Yet an area of distinct law known as the "Kanuns" (, canonical legislation) was dependent on Suleiman's will alone, covering areas such as criminal law, land tenure and taxation. He collected all the judgments that had been issued by the nine Ottoman Sultans who preceded him. After eliminating duplications and choosing between contradictory statements, he issued a single legal code, all the while being careful not to violate the basic laws of Islam. It was within this framework that Suleiman, supported by his Grand Mufti Ebussuud, sought to reform the legislation to adapt to a rapidly changing empire. When the Kanun laws attained their final form, the code of laws became known as the "kanun‐i Osmani" (), or the "Ottoman laws". Suleiman's legal code was to last more than three hundred years.

Suleiman gave particular attention to the plight of the rayas, Christian subjects who worked the land of the Sipahis. His Kanune Raya, or "Code of the Rayas", reformed the law governing levies and taxes to be paid by the rayas, raising their status above serfdom to the extent that Christian serfs would migrate to Turkish territories to benefit from the reforms. The Sultan also played a role in protecting the Jewish subjects of his empire for centuries to come. In late 1553 or 1554, on the suggestion of his favorite doctor and dentist, the Spanish Jew Moses Hamon, the Sultan issued a "firman" () formally denouncing blood libels against the Jews. Furthermore, Suleiman enacted new criminal and police legislation, prescribing a set of fines for specific offenses, as well as reducing the instances requiring death or mutilation. In the area of taxation, taxes were levied on various goods and produce, including animals, mines, profits of trade, and import-export duties.

Education was another important area for the Sultan. Schools attached to mosques and funded by religious foundations provided a largely free education to Muslim boys in advance of the Christian countries of the time. In his capital, Suleiman increased the number of "mektebs" (, primary schools) to fourteen, teaching boys to read and write as well as the principles of Islam. Young men wishing further education could proceed to one of eight "medreses" (, colleges), whose studies included grammar, metaphysics, philosophy, astronomy and astrology. Higher "medreses" provided education of university status, whose graduates became "imams" () or teachers. Educational centers were often one of many buildings surrounding the courtyards of mosques, others included libraries, baths, soup kitchens, residences and hospitals for the benefit of the public.

Under Suleiman's patronage, the Ottoman Empire entered the golden age of its cultural development. Hundreds of imperial artistic societies (called the "Ehl-i Hiref", "Community of the Craftsmen") were administered at the Imperial seat, the Topkapı Palace. After an apprenticeship, artists and craftsmen could advance in rank within their field and were paid commensurate wages in quarterly annual installments. Payroll registers that survive testify to the breadth of Suleiman's patronage of the arts, the earliest of the documents dating from 1526 list 40 societies with over 600 members. The "Ehl-i Hiref" attracted the empire's most talented artisans to the Sultan's court, both from the Islamic world and from the recently conquered territories in Europe, resulting in a blend of Arabic, Turkish and European cultures. Artisans in service of the court included painters, book binders, furriers, jewellers and goldsmiths. Whereas previous rulers had been influenced by Persian culture (Suleiman's father, Selim I, wrote poetry in Persian), Suleiman's patronage of the arts saw the Ottoman Empire assert its own artistic legacy.

Suleiman himself was an accomplished poet, writing in Persian and Turkish under the takhallus (nom de plume) "Muhibbi" (, "Lover"). Some of Suleiman's verses have become Turkish proverbs, such as the well-known "Everyone aims at the same meaning, but many are the versions of the story". When his young son Mehmed died in 1543, he composed a moving chronogram to commemorate the year: "Peerless among princes, my Sultan Mehmed". In Turkish the chronogram reads ("Şehzadeler güzidesi Sultan Muhammed'üm"), in which the Arabic Abjad numerals total 955, the equivalent in the Islamic calendar of 1543 AD. In addition to Suleiman's own work, many great talents enlivened the literary world during Suleiman's rule, including Fuzûlî and Bâkî. The literary historian Elias John Wilkinson Gibb observed that "at no time, even in Turkey, was greater encouragement given to poetry than during the reign of this Sultan". Suleiman's most famous verse is:

<poem style="margin-left:2em">
The people think of wealth and power as the greatest fate,
But in this world a spell of health is the best state.
What men call sovereignty is a worldly strife and constant war;
Worship of God is the highest throne, the happiest of all estates.
</poem>

Suleiman also became renowned for sponsoring a series of monumental architectural developments within his empire. The Sultan sought to turn Constantinople into the center of Islamic civilization by a series of projects, including bridges, mosques, palaces and various charitable and social establishments. The greatest of these were built by the Sultan's chief architect, Mimar Sinan, under whom Ottoman architecture reached its zenith. Sinan became responsible for over three hundred monuments throughout the empire, including his two masterpieces, the Süleymaniye and Selimiye mosques—the latter built in Adrianople (now Edirne) in the reign of Suleiman's son Selim II. Suleiman also restored the Dome of the Rock in Jerusalem and the Walls of Jerusalem (which are the current walls of the Old City of Jerusalem), renovated the Kaaba in Mecca, and constructed a complex in Damascus.

Suleiman had two known consorts

Suleiman had several children with his consorts, including:


Sultanzade Osman Bey (born 1545 and died 1575, Istanbul, buried in Mihrimah Sultan Mosque Üskudar)

Suleiman was infatuated with Hürrem Sultan, a harem girl from Ruthenia, then part of Poland. Western diplomats, taking notice of the palace gossip about her, called her "Russelazie" or "Roxelana", referring to her Ruthenian origins. The daughter of an Orthodox priest, she was captured by Tatars from Crimea, sold as a slave in Constantinople, and eventually rose through the ranks of the Harem to become Suleiman's favorite. Breaking with two centuries of Ottoman tradition, a former concubine had thus become the legal wife of the Sultan, much to the astonishment of the observers in the palace and the city. He also allowed Hürrem Sultan to remain with him at court for the rest of her life, breaking another tradition—that when imperial heirs came of age, they would be sent along with the imperial concubine who bore them to govern remote provinces of the Empire, never to return unless their progeny succeeded to the throne.

Under his pen name, Muhibbi, Sultan Suleiman composed this poem for Hürrem Sultan:
<poem style="margin-left:2em">
Throne of my lonely niche, my wealth, my love, my moonlight.
My most sincere friend, my confidant, my very existence, my Sultan, my one and only love.
The most beautiful among the beautiful ...
My springtime, my merry faced love, my daytime, my sweetheart, laughing leaf ...
My plants, my sweet, my rose, the one only who does not distress me in this room ...
My Istanbul, my karaman, the earth of my Anatolia
My Badakhshan, my Baghdad and Khorasan
My woman of the beautiful hair, my love of the slanted brow, my love of eyes full of misery ...
I'll sing your praises always
I, lover of the tormented heart, Muhibbi of the eyes full of tears, I am happy.
</poem>

Suleiman could speak Ottoman Turkish, Arabic, Chagatai, Persian and Serbian.

Pargalı Ibrahim Pasha was a friend of Suleiman from before his accession. Ibrahim was originally a Christian from Parga (in Epirus), who was captured in a raid during the 1499–1503 Ottoman–Venetian War, and was given as a slave to Suleiman most likely in 1514. Ibrahim converted to Islam and Suleiman made him the royal falconer, then promoted him to first officer of the Royal Bedchamber. Ibrahim Pasha rose to Grand Vizier in 1523 and commander-in-chief of all the armies. Suleiman also conferred upon Ibrahim Pasha the honor of "beylerbey" of Rumelia (first-ranking military governor-general), granting Ibrahim authority over all Ottoman territories in Europe, as well as command of troops residing within them in times of war. According to a 17th-century chronicler, Ibrahim had asked Suleiman not to promote him to such high positions, fearing for his safety; to which Suleiman replied that under his reign, no matter what the circumstance, Ibrahim would never be put to death.

Yet Ibrahim eventually fell from grace with the Sultan. During his thirteen years as Grand Vizier, his rapid rise to power and vast accumulation of wealth had made Ibrahim many enemies at the Sultan's court. Reports had reached the Sultan of Ibrahim's impudence during a campaign against the Persian Safavid empire: in particular his adoption of the title "serasker sultan" () was seen as a grave affront to Suleiman.

Suleiman's suspicion of Ibrahim was worsened by a quarrel between the latter and the finance secretary ("defterdar") Iskender Çelebi. The dispute ended in the disgrace of Çelebi on charges of intrigue, with Ibrahim convincing Suleiman to sentence the "defterdar" to death. Before his death however, Çelebi's last words were to accuse Ibrahim of conspiracy against the Sultan. These dying words convinced Suleiman of Ibrahim's disloyalty, and on 15 March 1536 Ibrahim was executed.

Ibrahim Pasha supported Şehzade Mustafa as the successor of Suleiman. This brought him with disputes with Hürrem Sultan, who wanted her sons to succeed to the throne. For many a long time, these disputes went on and since Suleiman trusted Hurrem most of all, she convinced the Sultan that Ibrahim was thinking of a rebellion. Taking this into account and the last words of Iskander Çelebi convinced Suleiman that Ibrahim was soon going to revolt against him. He consulted his Qadi, who suggested that Ibrahim be put to death. So, Suleiman recruited assassins and ordered to strangle Ibrahim in his sleep, and Ibrahim Pasha was killed by Suleiman, who had promised that he would not kill him.

Sultan Suleiman's two known consorts (Hürrem and Mahidevran) had borne him six sons, four of whom survived past the 1550s. They were Mustafa, Selim, Bayezid, and Cihangir. Of these, the eldest was not Hürrem's son, but rather Mahidevran's. The Austrian ambassador Busbecq would note "Suleiman has among his children a son called Mustafa, marvelously well educated and prudent and of an age to rule, since he is 24 or 25 years old; may God never allow a Barbary of such strength to come near us", going on to talk of Mustafa's "remarkable natural gifts".
Hürrem is usually held at least partly responsible for the intrigues in nominating a successor, though there is no evidence to support this. Although she was Suleiman's wife, she exercised no official public role. This did not, however, prevent Hürrem from wielding powerful political influence. Since the Empire lacked, until the reign of Ahmed I, any formal means of nominating a successor, successions usually involved the death of competing princes in order to avert civil unrest and rebellions.

By 1552, when the campaign against Persia had begun with Rüstem appointed commander-in-chief of the expedition, intrigues against Mustafa began. Rüstem sent one of Suleiman's most trusted men to report that since Suleiman was not at the head of the army, the soldiers thought the time had come to put a younger prince on the throne; at the same time he spread rumors that Mustafa had proved receptive to the idea. Angered by what he came to believe were Mustafa's plans to claim the throne, the following summer upon return from his campaign in Persia, Suleiman summoned him to his tent in the Ereğli valley, stating he would "be able to clear himself of the crimes he was accused of and would have nothing to fear if he came".

Mustafa was confronted with a choice: either he appeared before his father at the risk of being killed; or, if he refused to attend, he would be accused of betrayal. In the end, Mustafa chose to enter his father's tent, confident that the support of the army would protect him. Busbecq, who claims to have received an account from an eyewitness, describes Mustafa's final moments. As Mustafa entered his father's tent, Suleiman's eunuchs attacked Mustafa, with the young prince putting up a brave defence. Suleiman, separated from the struggle only by the linen hangings of the tent, peered through the chamber of his tent and "directed fierce and threatening glances upon the mutes, and by menacing gestures sternly rebuked their hesitation. Thereupon, the mutes in their alarm, redoubling their efforts, hurled Mustafa to the ground and, throwing the bowstring round his neck, strangled him."

Cihangir is said to have died of grief a few months after the news of his half-brother's murder. The two surviving brothers, Selim and Bayezid, were given command in different parts of the empire. Within a few years, however, civil war broke out between the brothers, each supported by his loyal forces. With the aid of his father's army, Selim defeated Bayezid in Konya in 1559, leading the latter to seek refuge with the Safavids along with his four sons. Following diplomatic exchanges, the Sultan demanded from the Safavid Shah that Bayezid be either extradited or executed. In return for large amounts of gold, the Shah allowed a Turkish executioner to strangle Bayezid and his four sons in 1561, clearing the path for Selim's succession to the throne five years later.
On 6 September 1566, Suleiman, who had set out from Constantinople to command an expedition to Hungary, died before an Ottoman victory at the Battle of Szigetvár in Hungary and the Grand Vizier kept his death secret during the retreat for the enthronement of Selim II. Just the night before the sickly sultan died in his tent, two months before he would have turned 72. The sultan's body was taken back to Istanbul to be buried, while his heart, liver, and some other organs were buried in Turbék, outside Szigetvár. A mausoleum was constructed above the burial site, and came to be regarded as a holy place and pilgrimage site. Within a decade a mosque and Sufi hospice were built near it, and the site was protected by a salaried garrison of several dozen men.

The formation of Suleiman's legacy began even before his death. Throughout his reign literary works were commissioned praising Suleiman and constructing an image of him as an ideal ruler, most significantly by Celalzade Mustafa, chancellor of the empire from 1534–1557. Later Ottoman writers applied this idealized image of Suleiman to the Near Eastern literary genre of advice literature ("naṣīḥatnāme"), urging sultans to conform to his model of rulership and to maintain the empire's institutions in their sixteenth-century form. Such writers were pushing back against the political and institutional transformation of the empire after the middle of the sixteenth century, and portrayed deviation from the norm as it had existed under Suleiman as evidence of the decline of the empire. Western historians, failing to recognize that these 'decline writers' were working within an established literary genre and often had deeply personal reasons for criticizing the empire, long took their claims at face value and consequently adopted the idea that the empire entered a period of decline after the death of Suleiman. Since the 1980s this view has been thoroughly reexamined, and modern scholars have come to overwhelmingly reject the idea of decline, labeling it an "untrue myth".

At the time of Suleiman's death, the Ottoman Empire was one of the world's foremost powers. Suleiman's conquests had brought under the control of the Empire major Muslim cities (such as Baghdad), many Balkan provinces (reaching present day Croatia and Hungary), and most of North Africa. His expansion into Europe had given the Ottoman Turks a powerful presence in the European balance of power. Indeed, such was the perceived threat of the Ottoman Empire under the reign of Suleiman that Austria's ambassador Busbecq warned of Europe's imminent conquest: "On [the Turks'] side are the resources of a mighty empire, strength unimpaired, habituation to victory, endurance of toil, unity, discipline, frugality and watchfulness ... Can we doubt what the result will be? ... When the Turks have settled with Persia, they will fly at our throats supported by the might of the whole East; how unprepared we are I dare not say." Suleiman's legacy was not, however, merely in the military field. The French traveler Jean de Thévenot bears witness a century later to the "strong agricultural base of the country, the well being of the peasantry, the abundance of staple foods and the pre-eminence of organization in Suleiman's government".

Even thirty years after his death, "Sultan Solyman" was quoted by the English playwright William Shakespeare as a military prodigy in "The Merchant of Venice", where the Prince of Morocco boasts about his prowess by saying that he defeated Suleiman in three battles (Act 2, Scene 1).

Through the distribution of court patronage, Suleiman also presided over a Golden Age in Ottoman arts, witnessing immense achievement in the realms of architecture, literature, art, theology and philosophy. Today the skyline of the Bosphorus and of many cities in modern Turkey and the former Ottoman provinces, are still adorned with the architectural works of Mimar Sinan. One of these, the Süleymaniye Mosque, is the final resting place of Suleiman: he is buried in a domed mausoleum attached to the mosque.

Nevertheless, assessments of Suleiman's reign have frequently fallen into the trap of the Great Man theory of history. The administrative, cultural, and military achievements of the age were a product not of Suleiman alone, but also of the many talented figures who served him, such as grand viziers Ibrahim Pasha and Rüstem Pasha, the Grand Mufti Ebussuud Efendi, who played a major role in legal reform, and chancellor and chronicler Celalzade Mustafa, who played a major role in bureaucratic expansion and in constructing Suleiman's legacy.

He has also been portrayed as a main playable character in the hit game Age of Empires III which is developed by Ensemble Studios.


Printed sources

On-line sources




</doc>
<doc id="26994" url="https://en.wikipedia.org/wiki?curid=26994" title="Scotland">
Scotland

Scotland (, ) is a country that is part of the United Kingdom. It covers the northern third of the island of Great Britain, with a border with England to the southeast, and is surrounded by the Atlantic Ocean to the north and west, the North Sea to the northeast, the Irish Sea to the south, and more than 790 islands, including the Northern Isles and the Hebrides.

The Kingdom of Scotland emerged as an independent sovereign state in the European Early Middle Ages and continued to exist until 1707. By inheritance in 1603, King James VI of Scotland became king of England and Ireland, thus forming a personal union of the three kingdoms. Scotland subsequently entered into a political union with England on 1 May 1707 to create the new Kingdom of Great Britain. The union also created a new Parliament of Great Britain, which succeeded both the Parliament of Scotland and the Parliament of England. In 1801, Great Britain entered into a political union with Ireland to create the United Kingdom of Great Britain and Ireland (in 1922, the Irish Free State seceded from the United Kingdom, leading to the latter being renamed the United Kingdom of Great Britain and Northern Ireland in 1927).

Within Scotland, the monarchy of the United Kingdom has continued to use a variety of styles, titles and other royal symbols of statehood specific to the pre-union Kingdom of Scotland. The legal system within Scotland has also remained separate from those of England and Wales and Northern Ireland; Scotland constitutes a distinct jurisdiction in both public and private law. The continued existence of legal, educational, religious and other institutions distinct from those in the remainder of the UK have all contributed to the continuation of Scottish culture and national identity since the 1707 union with England.

In 1997, a Scottish Parliament was re-established, in the form of a devolved unicameral legislature comprising 129 members, having authority over many areas of domestic policy. The head of the Scottish Government is the first minister of Scotland, who is supported by the deputy first minister of Scotland. Scotland is represented in the United Kingdom Parliament by 59 MPs. Scotland is also a member of the British–Irish Council, and sends five members of the Scottish Parliament to the British–Irish Parliamentary Assembly.

Scotland is divided into 32 administrative subdivisions or local authorities, known as council areas. Glasgow City is the largest council area in terms of population, with Highland being the largest in terms of area. Limited self-governing power, covering matters such as education, social services and roads and transportation, is devolved from the Scottish Government to each subdivision.
"Scotland" comes from "Scoti", the Latin name for the Gaels. Philip Freeman has speculated on the likelihood of a group of raiders adopting a name from an Indo-European root, *"skot", citing the parallel in Greek "skotos" (σκότος), meaning "darkness, gloom". The Late Latin word "Scotia" ("land of the Gaels") was initially used to refer to Ireland. By the 11th century at the latest, "Scotia" was being used to refer to (Gaelic-speaking) Scotland north of the River Forth, alongside "Albania" or "Albany", both derived from the Gaelic "Alba". The use of the words "Scots" and "Scotland" to encompass all of what is now Scotland became common in the Late Middle Ages.

Repeated glaciations, which covered the entire land mass of modern Scotland, destroyed any traces of human habitation that may have existed before the Mesolithic period. It is believed the first post-glacial groups of hunter-gatherers arrived in Scotland around 12,800 years ago, as the ice sheet retreated after the last glaciation. At the time, Scotland was covered in forests, had more bog-land, and the main form of transport was by water. These settlers began building the first known permanent houses on Scottish soil around 9,500 years ago, and the first villages around 6,000 years ago. The well-preserved village of Skara Brae on the mainland of Orkney dates from this period. Neolithic habitation, burial, and ritual sites are particularly common and well preserved in the Northern Isles and Western Isles, where a lack of trees led to most structures being built of local stone. Evidence of sophisticated pre-Christian belief systems is demonstrated by sites such as the Callanish Stones on Lewis and the Maes Howe on Orkney, which were built in the third millennium BCE. 

The first written reference to Scotland was in 320 BC by Greek sailor Pytheas, who called the northern tip of Britain "Orcas", the source of the name of the Orkney islands. During the first millennium BCE, the society changed dramatically to a chiefdom model, as consolidation of settlement led to the concentration of wealth and underground stores of surplus food. The first Roman incursion into Scotland occurred in 79 AD, when Agricola invaded Scotland; he defeated a Caledonian army at the Battle of Mons Graupius in 83 AD. After the Roman victory, Roman forts were briefly set along the Gask Ridge close to the Highland line, but by three years after the battle, the Roman armies had withdrawn to the Southern Uplands. The Romans erected Hadrian's Wall in northern England and the "Limes Britannicus" became the northern border of the Roman Empire. The Roman influence on the southern part of the country was considerable, and they introduced Christianity to Scotland.

Beginning in the sixth century, the area that is now Scotland was divided into three areas: Pictland, a patchwork of small lordships in central Scotland; the Anglo-Saxon Kingdom of Northumbria, which had conquered southeastern Scotland; and Dál Riata, founded by settlers from Ireland, bringing Gaelic language and culture with them. These societies were based on the family unit and had sharp divisions in wealth, although the vast majority were poor and worked full-time in subsistence agriculture. The Picts kept slaves (mostly captured in war) through the ninth century. 

Gaelic influence over Pictland and Northumbria was facilitated by the large number of Gaelic-speaking clerics working as missionaries. Operating in the sixth century on the island of Iona, Saint Columba was one of the earliest and best-known missionaries. The Vikings began to raid Scotland in the eighth century. Although the raiders sought slaves and luxury items, their main motivation was to acquire land. The oldest Norse settlements were in northwest Scotland, but they eventually conquered many areas along the coast. Old Norse entirely displaced Gaelic in the Northern Isles. 

In the ninth century, the Norse threat allowed a Gael named Cináed mac Ailpín (Kenneth I) to seize power over Pictland, establishing a royal dynasty to which the modern monarchs trace their lineage, and marking the beginning of the end of Pictish culture. The kingdom of Cináed and his descendants, called Alba, was Gaelic in character but existed on the same area as Pictland. By the end of the tenth century, the Pictish language went extinct as its speakers shifted to Gaelic. From a base in eastern Scotland north of the River Forth and south of the River Spey, the kingdom expanded first southwards, into the former Northumbrian lands, and northwards into Moray. Around the turn of the millennium, there was a centralization in agricultural lands and the first towns began to be established. 

In the twelfth and thirteenth centuries, with much of Scotland under the control of a single ruler and united by the Gaelic language, a modern nation-state first emerged, as did Scottish national consciousness. The domination of Gaelic was diminished during the reign of David I (1124–53), during which many English-speaking colonists settled in Scotland. David I and his successors centralized royal power and united mainland Scotland, capturing regions such as Moray, Galloway, and Caithness, although he did not succeed at extending his power over the Hebrides, which had been ruled by various Scottish clans following the death of Somerled in 1164. The system of feudalism was consolidated, with both Anglo-Norman incomers and native Gaelic chieftains being granted land in exchange for serving the king. The Scottish kings rejected English demands to subjugate themselves; in fact, England invaded Scotland several times to prevent Scotland's expansion into northern England. 
The death of Alexander III in March 1286 broke the succession line of Scotland's kings. Edward I of England arbitrated between various claimants for the Scottish crown. In return for surrendering Scotland's nominal independence, John Balliol was pronounced king in 1292. In 1294, Balliol and other Scottish lords refused Edward's demands to serve in his army against the French. Scotland and France sealed a treaty on 23 October 1295, known as the Auld Alliance. War ensued, and John was deposed by Edward who took personal control of Scotland. Andrew Moray and William Wallace initially emerged as the principal leaders of the resistance to English rule in the Wars of Scottish Independence, until Robert the Bruce was crowned king of Scotland in 1306. Victory at the Battle of Bannockburn in 1314 proved the Scots had regained control of their kingdom. In 1320 the world's first documented declaration of independence, the Declaration of Arbroath, won the support of Pope John XXII, leading to the legal recognition of Scottish sovereignty by the English Crown.
A civil war between the Bruce dynasty and their long-term Comyn-Balliol rivals lasted until the middle of the 14th century. Although the Bruce faction was successful, David II's lack of an heir allowed his half-nephew Robert II to come to the throne and establish the House of Stewart. The Stewarts ruled Scotland for the remainder of the Middle Ages. The country they ruled experienced greater prosperity from the end of the 14th century through the Scottish Renaissance to the Reformation, despite the effects of the Black Death in 1349 and increasing division between Highlands and Lowlands. Multiple truces reduced warfare on the southern border.

The Treaty of Perpetual Peace was signed in 1502 by James IV of Scotland and Henry VII of England. James married Henry's daughter, Margaret Tudor. James invaded England in support of France under the terms of the Auld Alliance and became the last British monarch to die in battle, at Flodden in 1513. In 1560, the Treaty of Edinburgh brought an end to the Anglo-French conflict and recognized the Protestant Elizabeth I as Queen of England. The Parliament of Scotland met and immediately adopted the Scots Confession, which signaled the Scottish Reformation's sharp break from papal authority and Catholic teaching. The Catholic Mary, Queen of Scots was forced to abdicate in 1567.

In 1603, James VI, King of Scots inherited the thrones of the Kingdom of England and the Kingdom of Ireland in the Union of the Crowns, and moved to London. The military was strengthened, allowing the imposition of royal authority on the western Highland clans. The 1609 Statutes of Iona compelled the cultural integration of Hebridean clan leaders. With the exception of a short period under the Protectorate, Scotland remained a separate state, but there was considerable conflict between the crown and the Covenanters over the form of church government. The Glorious Revolution of 1688–89 saw the overthrow of King James VII of Scotland and II of England by the English Parliament in favour of William III and Mary II. 

The Battle of Altimarlach in 1680 was the last significant clan battle fought between highland clans. In common with countries such as France, Norway, Sweden and Finland, Scotland experienced famines during the 1690s. Mortality, reduced childbirths and increased emigration reduced the population of parts of the country about 10–15%.

In 1698, the Company of Scotland attempted a project to secure a trading colony on the Isthmus of Panama. Almost every Scottish landowner who had money to spare is said to have invested in the Darien scheme. Its failure bankrupted these landowners, but not the burghs. Nevertheless, the nobles' bankruptcy, along with the threat of an English invasion, played a leading role in convincing the Scots elite to back a union with England.

On 22 July 1706, the Treaty of Union was agreed between representatives of the Scots Parliament and the Parliament of England. The following year, twin Acts of Union were passed by both parliaments to create the united Kingdom of Great Britain with effect from 1 May 1707 with popular opposition and anti-union riots in Edinburgh, Glasgow, and elsewhere.

With trade tariffs with England abolished, trade blossomed, especially with Colonial America. The clippers belonging to the Glasgow Tobacco Lords were the fastest ships on the route to Virginia. Until the American War of Independence in 1776, Glasgow was the world's premier tobacco port, dominating world trade. The disparity between the wealth of the merchant classes of the Scottish Lowlands and the ancient clans of the Scottish Highlands grew, amplifying centuries of division.

The deposed Jacobite Stuart claimants had remained popular in the Highlands and north-east, particularly amongst non-Presbyterians, including Roman Catholics and Episcopalian Protestants. However, two major Jacobite risings launched in 1715 and 1745 failed to remove the House of Hanover from the British throne. The threat of the Jacobite movement to the United Kingdom and its monarchs effectively ended at the Battle of Culloden, Great Britain's last pitched battle.

The Scottish Enlightenment and the Industrial Revolution turned Scotland into an intellectual, commercial and industrial powerhouse–so much so Voltaire said "We look to Scotland for all our ideas of civilisation." With the demise of Jacobitism and the advent of the Union, thousands of Scots, mainly Lowlanders, took up numerous positions of power in politics, civil service, the army and navy, trade, economics, colonial enterprises and other areas across the nascent British Empire. Historian Neil Davidson notes "after 1746 there was an entirely new level of participation by Scots in political life, particularly outside Scotland." Davidson also states "far from being 'peripheral' to the British economy, Scotland – or more precisely, the Lowlands – lay at its core."

In the Highlands, clan chiefs gradually started to think of themselves more as commercial landlords than leaders of their people. These social and economic changes included the first phase of the Highland Clearances and, ultimately, the demise of clanship.

The Scottish Reform Act 1832 increased the number of Scottish MPs and widened the franchise to include more of the middle classes. From the mid-century, there were increasing calls for Home Rule for Scotland and the post of Secretary of State for Scotland was revived. Towards the end of the century Prime Ministers of Scottish descent included William Gladstone, and the Earl of Rosebery. In the late 19th century the growing importance of the working classes was marked by Keir Hardie's success in the Mid Lanarkshire by-election, 1888, leading to the foundation of the Scottish Labour Party, which was absorbed into the Independent Labour Party in 1895, with Hardie as its first leader.

Glasgow became one of the largest cities in the world and known as "the Second City of the Empire" after London. After 1860 the Clydeside shipyards specialised in steamships made of iron (after 1870, made of steel), which rapidly replaced the wooden sailing vessels of both the merchant fleets and the battle fleets of the world. It became the world's pre-eminent shipbuilding centre. The industrial developments, while they brought work and wealth, were so rapid that housing, town-planning, and provision for public health did not keep pace with them, and for a time living conditions in some of the towns and cities were notoriously bad, with overcrowding, high infant mortality, and growing rates of tuberculosis.
While the Scottish Enlightenment is traditionally considered to have concluded toward the end of the 18th century, disproportionately large Scottish contributions to British science and letters continued for another 50 years or more, thanks to such figures as the physicists James Clerk Maxwell and Lord Kelvin, and the engineers and inventors James Watt and William Murdoch, whose work was critical to the technological developments of the Industrial Revolution throughout Britain. In literature, the most successful figure of the mid-19th century was Walter Scott. His first prose work, "Waverley" in 1814, is often called the first historical novel. It launched a highly successful career that probably more than any other helped define and popularise Scottish cultural identity. In the late 19th century, a number of Scottish-born authors achieved international reputations, such as Robert Louis Stevenson, Arthur Conan Doyle, J. M. Barrie and George MacDonald. Scotland also played a major part in the development of art and architecture. The Glasgow School, which developed in the late 19th century, and flourished in the early 20th century, produced a distinctive blend of influences including the Celtic Revival the Arts and Crafts movement, and Japonism, which found favour throughout the modern art world of continental Europe and helped define the Art Nouveau style. Proponents included architect and artist Charles Rennie Mackintosh.

This period saw a process of rehabilitation for Highland culture. In the 1820s, as part of the Romantic revival, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe, prompted by the popularity of Macpherson's Ossian cycle and then Walter Scott's Waverley novels. However, the Highlands remained poor, the only part of mainland Britain to continue to experience recurrent famine, with a limited range of products exported out of the region, negligible industrial production, but a continued population growth that tested the subsistence agriculture. These problems, and the desire to improve agriculture and profits were the driving forces of the ongoing Highland Clearances, in which many of the population of the Highlands suffered eviction as lands were enclosed, principally so that they could be used for sheep farming. The first phase of the clearances followed patterns of agricultural change throughout Britain. The second phase was driven by overpopulation, the Highland Potato Famine and the collapse of industries that had relied on the wartime economy of the Napoleonic Wars. The population of Scotland grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901. Even with the development of industry, there were not enough good jobs. As a result, during the period 1841–1931, about 2 million Scots migrated to North America and Australia, and another 750,000 Scots relocated to England.
After prolonged years of struggle in the Kirk, in 1834 the Evangelicals gained control of the General Assembly and passed the Veto Act, which allowed congregations to reject unwanted "intrusive" presentations to livings by patrons. The following "Ten Years' Conflict" of legal and political wrangling ended in defeat for the non-intrusionists in the civil courts. The result was a schism from the church by some of the non-intrusionists led by Dr Thomas Chalmers, known as the Great Disruption of 1843. Roughly a third of the clergy, mainly from the North and Highlands, formed the separate Free Church of Scotland. In the late 19th century growing divisions between fundamentalist Calvinists and theological liberals resulted in a further split in the Free Church as the rigid Calvinists broke away to form the Free Presbyterian Church in 1893. Catholic emancipation in 1829 and the influx of large numbers of Irish immigrants, particularly after the famine years of the late 1840s, mainly to the growing lowland centres like Glasgow, led to a transformation in the fortunes of Catholicism. In 1878, despite opposition, a Roman Catholic ecclesiastical hierarchy was restored to the country, and Catholicism became a significant denomination within Scotland.

Industrialisation, urbanisation and the Disruption of 1843 all undermined the tradition of parish schools. From 1830 the state began to fund buildings with grants; then from 1846 it was funding schools by direct sponsorship; and in 1872 Scotland moved to a system like that in England of state-sponsored largely free schools, run by local school boards. The historic University of Glasgow became a leader in British higher education by providing the educational needs of youth from the urban and commercial classes, as opposed to the upper class. The University of St Andrews pioneered the admission of women to Scottish universities. From 1892 Scottish universities could admit and graduate women and the numbers of women at Scottish universities steadily increased until the early 20th century.

Caused by the advent of refrigeration and imports of lamb, mutton and wool from overseas, the 1870s brought with them a collapse of sheep prices and an abrupt halt in the previous sheep farming boom. Land prices subsequently plummeted, too, and accelerated the process of the so-called "Balmoralisation" of Scotland, an era in the second half of the 19th century that saw an increase in tourism and the establishment of large estates dedicated to field sports like deer stalking and grouse shooting, especially in the Scottish Highlands. The process was named after Balmoral Estate, purchased by Queen Victoria in 1848, that fueled the romanticisation of upland Scotland and initiated an influx of the newly wealthy acquiring similar estates in the following decades. In the late 19th century just 118 people owned half of Scotland, with nearly 60 per cent of the whole country being part of shooting estates. While their relative importance has somewhat declined due to changing recreational interests throughout the 20th century, deer stalking and grouse shooting remain of prime importance on many private estates in Scotland.

Scotland played a major role in the British effort in the First World War. It especially provided manpower, ships, machinery, fish and money. With a population of 4.8 million in 1911, Scotland sent over half a million men to the war, of whom over a quarter died in combat or from disease, and 150,000 were seriously wounded. Field Marshal Sir Douglas Haig was Britain's commander on the Western Front.

The war saw the emergence of a radical movement called "Red Clydeside" led by militant trades unionists. Formerly a Liberal stronghold, the industrial districts switched to Labour by 1922, with a base among the Irish Catholic working-class districts. Women were especially active in building neighbourhood solidarity on housing issues. However, the "Reds" operated within the Labour Party and had little influence in Parliament and the mood changed to passive despair by the late 1920s.

The shipbuilding industry expanded by a third and expected renewed prosperity, but instead, a serious depression hit the economy by 1922 and it did not fully recover until 1939. The interwar years were marked by economic stagnation in rural and urban areas, and high unemployment. Indeed, the war brought with it deep social, cultural, economic, and political dislocations. Thoughtful Scots pondered their declension, as the main social indicators such as poor health, bad housing, and long-term mass unemployment, pointed to terminal social and economic stagnation at best, or even a downward spiral. Service abroad on behalf of the Empire lost its allure to ambitious young people, who left Scotland permanently. The heavy dependence on obsolescent heavy industry and mining was a central problem, and no one offered workable solutions. The despair reflected what Finlay (1994) describes as a widespread sense of hopelessness that prepared local business and political leaders to accept a new orthodoxy of centralised government economic planning when it arrived during the Second World War.

During the Second World War, Scotland was targeted by Nazi Germany largely due to its factories, shipyards, and coal mines. Cities such as Glasgow and Edinburgh were targeted by German bombers, as were smaller towns mostly located in the central belt of the country. Perhaps the most significant air-raid in Scotland was the Clydebank Blitz of March 1941, which intended to destroy naval shipbuilding in the area. 528 people were killed and 4,000 homes totally destroyed.

Perhaps Scotland's most unusual wartime episode occurred in 1941 when Rudolf Hess flew to Renfrewshire, possibly intending to broker a peace deal through the Duke of Hamilton. Before his departure from Germany, Hess had given his adjutant, Karlheinz Pintsch, a letter addressed to Hitler that detailed his intentions to open peace negotiations with the British. Pintsch delivered the letter to Hitler at the Berghof around noon on 11 May. Albert Speer later said Hitler described Hess's departure as one of the worst personal blows of his life, as he considered it a personal betrayal. Hitler worried that his allies, Italy and Japan, would perceive Hess's act as an attempt by Hitler to secretly open peace negotiations with the British.

As in World War I, Scapa Flow in Orkney served as an important Royal Navy base. Attacks on Scapa Flow and Rosyth gave RAF fighters their first successes downing bombers in the Firth of Forth and East Lothian. The shipyards and heavy engineering factories in Glasgow and Clydeside played a key part in the war effort, and suffered attacks from the Luftwaffe, enduring great destruction and loss of life. As transatlantic voyages involved negotiating north-west Britain, Scotland played a key part in the battle of the North Atlantic. Shetland's relative proximity to occupied Norway resulted in the Shetland bus by which fishing boats helped Norwegians flee the Nazis, and expeditions across the North Sea to assist resistance.

Scottish industry came out of the depression slump by a dramatic expansion of its industrial activity, absorbing unemployed men and many women as well. The shipyards were the centre of more activity, but many smaller industries produced the machinery needed by the British bombers, tanks and warships. Agriculture prospered, as did all sectors except for coal mining, which was operating mines near exhaustion. Real wages, adjusted for inflation, rose 25% and unemployment temporarily vanished. Increased income, and the more equal distribution of food, obtained through a tight rationing system, dramatically improved the health and nutrition.

After 1945, Scotland's economic situation worsened due to overseas competition, inefficient industry, and industrial disputes. Only in recent decades has the country enjoyed something of a cultural and economic renaissance. Economic factors contributing to this recovery included a resurgent financial services industry, electronics manufacturing, (see Silicon Glen), and the North Sea oil and gas industry. The introduction in 1989 by Margaret Thatcher's government of the Community Charge (widely known as the Poll Tax) one year before the rest of Great Britain, contributed to a growing movement for Scottish control over domestic affairs. Following a referendum on devolution proposals in 1997, the Scotland Act 1998 was passed by the UK Parliament, which established a devolved Scottish Parliament and Scottish Government with responsibility for most laws specific to Scotland. The Scottish Parliament was reconvened in Edinburgh on 4 July 1999. The first to hold the office of first minister of Scotland was Donald Dewar, who served until his sudden death in 2000.

The Scottish Parliament Building at Holyrood itself did not open until October 2004, after lengthy construction delays and running over budget. The Scottish Parliament has a form of proportional representation (the additional member system), which normally results in no one party having an overall majority. The pro-independence Scottish National Party led by Alex Salmond achieved this in the 2011 election, winning 69 of the 129 seats available. The success of the SNP in achieving a majority in the Scottish Parliament paved the way for the September 2014 referendum on Scottish independence. The majority voted against the proposition, with 55% voting no to independence. More powers, particularly in relation to taxation, were devolved to the Scottish Parliament after the referendum, following cross-party talks in the Smith Commission.

The mainland of Scotland comprises the northern third of the land mass of the island of Great Britain, which lies off the north-west coast of Continental Europe. The total area is , comparable to the size of the Czech Republic. Scotland's only land border is with England, and runs for between the basin of the River Tweed on the east coast and the Solway Firth in the west. The Atlantic Ocean borders the west coast and the North Sea is to the east. The island of Ireland lies only from the south-western peninsula of Kintyre; Norway is to the east and the Faroe Islands, to the north.

The territorial extent of Scotland is generally that established by the 1237 Treaty of York between Scotland and the Kingdom of England and the 1266 Treaty of Perth between Scotland and Norway. Important exceptions include the Isle of Man, which having been lost to England in the 14th century is now a crown dependency outside of the United Kingdom; the island groups Orkney and Shetland, which were acquired from Norway in 1472; and Berwick-upon-Tweed, lost to England in 1482

The geographical centre of Scotland lies a few miles from the village of Newtonmore in Badenoch. Rising to above sea level, Scotland's highest point is the summit of Ben Nevis, in Lochaber, while Scotland's longest river, the River Tay, flows for a distance of .

The whole of Scotland was covered by ice sheets during the Pleistocene ice ages and the landscape is much affected by glaciation. From a geological perspective, the country has three main sub-divisions.

The Highlands and Islands lie to the north and west of the Highland Boundary Fault, which runs from Arran to Stonehaven. This part of Scotland largely comprises ancient rocks from the Cambrian and Precambrian, which were uplifted during the later Caledonian orogeny. It is interspersed with igneous intrusions of a more recent age, remnants of which formed mountain massifs such as the Cairngorms and Skye Cuillins.

A significant exception to the above are the fossil-bearing beds of Old Red Sandstones found principally along the Moray Firth coast. The Highlands are generally mountainous and the highest elevations in the British Isles are found here. Scotland has over 790 islands divided into four main groups: Shetland, Orkney, and the Inner Hebrides and Outer Hebrides. There are numerous bodies of freshwater including Loch Lomond and Loch Ness. Some parts of the coastline consist of machair, a low-lying dune pasture land.

The Central Lowlands is a rift valley mainly comprising Paleozoic formations. Many of these sediments have economic significance for it is here that the coal and iron bearing rocks that fuelled Scotland's industrial revolution are found. This area has also experienced intense volcanism, Arthur's Seat in Edinburgh being the remnant of a once much larger volcano. This area is relatively low-lying, although even here hills such as the Ochils and Campsie Fells are rarely far from view.

The Southern Uplands are a range of hills almost long, interspersed with broad valleys. They lie south of a second fault line (the Southern Uplands fault) that runs from Girvan to Dunbar. The geological foundations largely comprise Silurian deposits laid down some 400–500 million years ago. The high point of the Southern Uplands is Merrick with an elevation of . The Southern Uplands is home to Scotland's highest village, Wanlockhead ( above sea level).

The climate of most of Scotland is temperate and oceanic, and tends to be very changeable., As it is warmed by the Gulf Stream from the Atlantic, it has much milder winters (but cooler, wetter summers) than areas on similar latitudes, such as Labrador, southern Scandinavia, the Moscow region in Russia, and the Kamchatka Peninsula on the opposite side of Eurasia. However, temperatures are generally lower than in the rest of the UK, with the coldest ever UK temperature of recorded at Braemar in the Grampian Mountains, on 11 February 1895. Winter maxima average in the Lowlands, with summer maxima averaging . The highest temperature recorded was at Greycrook, Scottish Borders on 9 August 2003.

The west of Scotland is usually warmer than the east, owing to the influence of Atlantic ocean currents and the colder surface temperatures of the North Sea. Tiree, in the Inner Hebrides, is one of the sunniest places in the country: it had more than 300 hours of sunshine in May 1975. Rainfall varies widely across Scotland. The western highlands of Scotland are the wettest, with annual rainfall in a few places exceeding . In comparison, much of lowland Scotland receives less than annually. Heavy snowfall is not common in the lowlands, but becomes more common with altitude. Braemar has an average of 59 snow days per year, while many coastal areas average fewer than 10 days of lying snow per year.

Scotland's wildlife is typical of the north-west of Europe, although several of the larger mammals such as the lynx, brown bear, wolf, elk and walrus were hunted to extinction in historic times. There are important populations of seals and internationally significant nesting grounds for a variety of seabirds such as gannets. The golden eagle is something of a national icon.

On the high mountain tops, species including ptarmigan, mountain hare and stoat can be seen in their white colour phase during winter months. Remnants of the native Scots pine forest exist and within these areas the Scottish crossbill, the UK's only endemic bird species and vertebrate, can be found alongside capercaillie, Scottish wildcat, red squirrel and pine marten. Various animals have been re-introduced, including the white-tailed sea eagle in 1975, the red kite in the 1980s, and there have been experimental projects involving the beaver and wild boar. Today, much of the remaining native Caledonian Forest lies within the Cairngorms National Park and remnants of the forest remain at 84 locations across Scotland. On the west coast, remnants of ancient Celtic Rainforest still remain, particularly on the Taynish peninsula in Argyll, these forests are particularly rare due to high rates of deforestation throughout Scottish history.

The flora of the country is varied incorporating both deciduous and coniferous woodland as well as moorland and tundra species. However, large scale commercial tree planting and the management of upland moorland habitat for the grazing of sheep and field sport activities like deer stalking and driven grouse shooting impacts upon the distribution of indigenous plants and animals. The UK's tallest tree is a grand fir planted beside Loch Fyne, Argyll in the 1870s, and the Fortingall Yew may be 5,000 years old and is probably the oldest living thing in Europe. Although the number of native vascular plants is low by world standards, Scotland's substantial bryophyte flora is of global importance.

The population of Scotland at the 2001 Census was 5,062,011. This rose to 5,295,400, the highest ever, at the 2011 Census. The most recent ONS estimate, for mid-2017, was 5,424,800.

In the 2011 Census, 62% of Scotland's population stated their national identity as 'Scottish only', 18% as 'Scottish and British', 8% as 'British only', and 4% chose 'other identity only'.

Although Edinburgh is the capital of Scotland, the largest city is Glasgow, which has just over 584,000 inhabitants. The Greater Glasgow conurbation, with a population of almost 1.2 million, is home to nearly a quarter of Scotland's population. The Central Belt is where most of the main towns and cities are located, including Glasgow, Edinburgh, Dundee, and Perth. Scotland's only major city outside the Central Belt is Aberdeen. The Scottish Lowlands host 80% of the total population, where the Central Belt accounts for 3.5 million people.

In general, only the more accessible and larger islands remain inhabited. Currently, fewer than 90 remain inhabited. The Southern Uplands are essentially rural in nature and dominated by agriculture and forestry. Because of housing problems in Glasgow and Edinburgh, five new towns were designated between 1947 and 1966. They are East Kilbride, Glenrothes, Cumbernauld, Livingston, and Irvine.

Immigration since World War II has given Glasgow, Edinburgh, and Dundee small South Asian communities. In 2011, there were an estimated 49,000 ethnically Pakistani people living in Scotland, making them the largest non-White ethnic group. Since the Enlargement of the European Union more people from Central and Eastern Europe have moved to Scotland, and the 2011 census indicated that 61,000 Poles live there.
Scotland has three officially recognised languages: English, Scots, and Scottish Gaelic. Scottish Standard English, a variety of English as spoken in Scotland, is at one end of a bipolar linguistic continuum, with broad Scots at the other. Scottish Standard English may have been influenced to varying degrees by Scots. The 2011 census indicated that 63% of the population had "no skills in Scots". Others speak Highland English. Gaelic is mostly spoken in the Western Isles, where a large proportion of people still speak it; however, nationally its use is confined to just 1% of the population. The number of Gaelic speakers in Scotland dropped from 250,000 in 1881 to 60,000 in 2008.

There are many more people with Scottish ancestry living abroad than the total population of Scotland. In the 2000 Census, 9.2 million Americans self-reported some degree of Scottish descent. Ulster's Protestant population is mainly of lowland Scottish descent, and it is estimated that there are more than 27 million descendants of the Scots-Irish migration now living in the US. In Canada, the Scottish-Canadian community accounts for 4.7 million people. About 20% of the original European settler population of New Zealand came from Scotland.

In August 2012, the Scottish population reached an all-time high of 5.25 million people. The reasons given were that, in Scotland, births were outnumbering the number of deaths, and immigrants were moving to Scotland from overseas. In 2011, 43,700 people moved from Wales, Northern Ireland or England to live in Scotland.

The total fertility rate (TFR) in Scotland is below the replacement rate of 2.1 (the TFR was 1.73 in 2011). The majority of births are to unmarried women (51.3% of births were outside of marriage in 2012).
Life expectancy for those born in Scotland between 2012 and 2014 is 77.1 years for males and 81.1 years for females. This is the lowest of any of the four countries of the UK.

Just over half (54%) of the Scottish population reported being a Christian while nearly 37% reported not having a religion in a 2011 census.
Since the Scottish Reformation of 1560, the national church (the Church of Scotland, also known as The Kirk) has been Protestant in classification and Reformed in theology. Since 1689 it has had a Presbyterian system of church government and enjoys independence from the state. Its membership is 398,389, about 7.5% of the total population, though according to the 2014 Scottish Annual Household Survey, 27.8%, or 1.5 million adherents, identified the Church of Scotland as the church of their religion. The Church operates a territorial parish structure, with every community in Scotland having a local congregation.

Scotland also has a significant Roman Catholic population, 19% professing that faith, particularly in Greater Glasgow and the north-west. After the Reformation, Roman Catholicism in Scotland continued in the Highlands and some western islands like Uist and Barra, and it was strengthened during the 19th century by immigration from Ireland. Other Christian denominations in Scotland include the Free Church of Scotland, and various other Presbyterian offshoots. Scotland's third largest church is the Scottish Episcopal Church.

Islam is the largest non-Christian religion (estimated at around 75,000, which is about 1.4% of the population), and there are also significant Jewish, Hindu and Sikh communities, especially in Glasgow. The Samyé Ling monastery near Eskdalemuir, which celebrated its 40th anniversary in 2007, is the first Buddhist monastery in western Europe.

The head of state of the United Kingdom is the monarch, currently Queen Elizabeth II (since 1952). The regnal numbering ("Elizabeth II") caused controversy around the time of her coronation because there had never been an Elizabeth I in Scotland. The British government stated in April 1953 that future British monarchs would be numbered according to either their English or their Scottish predecessors, whichever number would be higher. For instance, any future King James would be styled James VIIIsince the last Scottish King James was James VII (also James II of England, etc.)while the next King Henry would be King Henry IX throughout the UK even though there have been no Scottish kings of that name. A legal action, MacCormick v Lord Advocate (1953 SC 396), was brought in Scotland to contest the right of the Queen to entitle herself "Elizabeth II" within Scotland, but the Crown won the case.

The monarchy of the United Kingdom continues to use a variety of styles, titles and other royal symbols of statehood specific to pre-union Scotland, including: the Royal Standard of Scotland, the Royal coat of arms used in Scotland together with its associated Royal Standard, royal titles including that of Duke of Rothesay, certain Great Officers of State, the chivalric Order of the Thistle and, since 1999, reinstating a ceremonial role for the Crown of Scotland after a 292-year hiatus.

Scotland has limited self-government within the United Kingdom, as well as representation in the UK Parliament. Executive and legislative powers respectively have been devolved to the Scottish Government and the Scottish Parliament at Holyrood in Edinburgh since 1999. The UK Parliament retains control over reserved matters specified in the Scotland Act 1998, including UK taxes, social security, defence, international relations and broadcasting. The Scottish Parliament has legislative authority for all other areas relating to Scotland. It initially had only a limited power to vary income tax, but powers over taxation and social security were significantly expanded by the Scotland Acts of 2012 and 2016.

The Scottish Parliament can give legislative consent over devolved matters back to the UK Parliament by passing a Legislative Consent Motion if United Kingdom-wide legislation is considered more appropriate for a certain issue. The programmes of legislation enacted by the Scottish Parliament have seen a divergence in the provision of public services compared to the rest of the UK. For instance, university education and care services for the elderly are free at point of use in Scotland, while fees are paid in the rest of the UK. Scotland was the first country in the UK to ban smoking in enclosed public places.

The Scottish Parliament is a unicameral legislature with 129 members (MSPs): 73 of them represent individual constituencies and are elected on a first-past-the-post system; the other 56 are elected in eight different electoral regions by the additional member system. MSPs serve for a four-year period (exceptionally five years from 2011–16). The Parliament nominates one of its Members, who is then appointed by the monarch to serve as first minister. Other ministers are appointed by the first minister and serve at his/her discretion. Together they make up the Scottish Government, the executive arm of the devolved government. The Scottish Government is headed by the first minister, who is accountable to the Scottish Parliament and is the minister of charge of the Scottish Government. The first minister is also the political leader of Scotland. The Scottish Government also comprises the deputy first minister, currently John Swinney MSP, who deputises for the first minister during a period of absence of overseas visits. Alongside the deputy first minister's requirements as Deputy, the minister also has a cabinet ministerial responsibility. Swinney is also currently Cabinet Secretary for Education and Skills. The Scottish Government's cabinet comprises nine cabinet secretaries, who form the Cabinet of Scotland. There are also twelve other ministers, who work alongside the cabinet secretaries in their appointed areas.

In the 2016 election, the Scottish National Party (SNP) won 63 of the 129 seats available. Nicola Sturgeon, the leader of the SNP, has been the first minister since November 2014. The Conservative Party became the largest opposition party in the 2016 elections, with the Labour Party, Liberal Democrats and the Green Party also represented in the Parliament. The next Scottish Parliament election is due to be held on 6 May 2021.

Scotland is represented in the British House of Commons by 59 MPs elected from territory-based Scottish constituencies. In the 2019 general election, the SNP won 48 of the 59 seats. This represented a significant increase from the 2017 general election, when the SNP won 35 seats. Conservative, Labour and Liberal Democrat parties also represent Scottish constituencies in the House of Commons. The next United Kingdom general election is scheduled for 2 May 2024. The Scotland Office represents the UK government in Scotland on reserved matters and represents Scottish interests within the UK government. The Scotland Office is led by the Secretary of State for Scotland, who sits in the Cabinet of the United Kingdom. Conservative MP Alister Jack has held the position since July 2019.

The relationships between the central UK Government and devolved governments of Scotland, Wales and Northern Ireland are based on the extra-statutory principles and agreements with the main elements are set out in a "Memorandum of Understanding" between the UK government and the devolved governments of Scotland, Wales and Northern Ireland. The MOU lays emphasis on the principles of good communication, consultation and co-operation.

Since devolution in 1999, Scotland has devolved stronger working relations across the two other devolved governments, the Welsh Government and Northern Ireland Executive. Whilst there are no formal concordats between the Scottish Government, Welsh Government and Northern Ireland Executive, ministers from each devolved government meet at various points throughout the year at various events such as the British-Irish Council and also meet to discuss matters and issues that are devolved to each government. Scotland, along with the Welsh Government, British Government as well as the Northern Ireland executive, participate in the Joint Ministerial Committee (JMC) which allows each government to discuss policy issues together and work together across each government to find solutions. The Scottish Government considers the successful re-establishment of the Plenary, and establishment of the Domestic fora to be important facets of the relationship with the UK Government and the other devolved administrations.

In the aftermath of the United Kingdom's decision to withdraw from the European Union in 2016, the Scottish Government has called for there to be a joint approach from each of the devolved governments. In early 2017, the devolved governments met to discuss Brexit and agree on Brexit strategies from each devolved government which lead for Theresa May to issue a statement that claims that the devolved governments will not have a central role or decision making process in the Brexit process, but that the UK Government plans to "fully engage" Scotland in talks alongside the governments of Wales and Northern Ireland.

Whilst foreign policy remains a reserved matter, the Scottish Government still has the power and ability to strengthen and develop Scotland, the economy and Scottish interests on the world stage and encourage foreign businesses, international devolved, regional and central governments to invest in Scotland. Whilst the first minister usually undertakes a number of foreign and international visits to promote Scotland, international relations, European and Commonwealth relations are also included within the portfolios of both the Cabinet Secretary for Culture, Tourism and External Affairs (responsible for international development) and the Minister for International Development and Europe (responsible for European Union relations and international relations).

During the G8 Summit in 2005, First Minister Jack McConnell welcomed each head of government of the G8 nations to the countries Glasgow Prestwick Airport on behalf of then UK Prime Minister Tony Blair. At the same time, McConnell and the then Scottish Executive pioneered the way forward to launch what would become the Scotland Malawi Partnership which co-ordinates Scottish activities to strengthen existing links with Malawi. During McConnell's time as first minister, several relations with Scotland, including Scottish and Russian relations strengthened following a visit by President of Russia Vladimir Putin to Edinburgh. McConnell, speaking at the end, highlighted that the visit by Putin was a "post-devolution" step towards "Scotland regaining its international identity".

Under the Salmond administration, Scotland's trade and investment deals with countries such as China and Canada, where Salmond established the Canada Plan 2010–2015 which aimed to strengthen "the important historical, cultural and economic links" between both Canada and Scotland. To promote Scotland's interests and Scottish businesses in North America, there is a Scottish Affairs Office located in Washington, D.C. with the aim to promoting Scotland in both the United States and Canada.

During a 2017 visit to the United States, First Minister Nicola Sturgeon met with Jerry Brown, Governor of California, where both signed an agreement committing both the Government of California and the Scottish Government to work together to tackle climate change, as well as Sturgeon signing a £6.3 million deal for Scottish investment from American businesses and firms promoting trade, tourism and innovation. During an official visit to the Republic of Ireland in 2016, Sturgeon claimed that is it "important for Ireland and Scotland and the whole of the British Isles that Ireland has a strong ally in Scotland". During the same engagement, Sturgeon became the first head of government to address the Seanad Éireann, the Upper House of the Irish Parliament.

A policy of devolution had been advocated by the three main UK parties with varying enthusiasm during recent history. A previous Labour leader. John Smith, described the revival of a Scottish parliament as the "settled will of the Scottish people". The devolved Scottish Parliament was created after a referendum in 1997 found majority support for both creating the Parliament and granting it limited powers to vary income tax.

The Scottish National Party (SNP), which supports Scottish independence, was first elected to form the Scottish Government in 2007. The new government established a "National Conversation" on constitutional issues, proposing a number of options such as increasing the powers of the Scottish Parliament, federalism, or a referendum on Scottish independence from the United Kingdom. In rejecting the last option, the three main opposition parties in the Scottish Parliament created a commission to investigate the distribution of powers between devolved Scottish and UK-wide bodies. The Scotland Act 2012, based on proposals by the commission, was subsequently enacted devolving additional powers to the Scottish Parliament.

In August 2009 the SNP proposed a bill to hold a referendum on independence in November 2010. Opposition from all other major parties led to an expected defeat. After the 2011 elections gave the SNP an overall majority in the Scottish Parliament, a referendum on independence for Scotland was held on 18 September 2014. The referendum resulted in a rejection of independence, by 55.3% to 44.7%. During the campaign, the three main parties in the UK Parliament pledged to extend the powers of the Scottish Parliament. An all-party commission chaired by Lord Smith of Kelvin was formed, which led to a further devolution of powers through the Scotland Act 2016.

Following a referendum on the UK's membership of the European Union on 23 June 2016, where a UK-wide majority voted to withdraw from the EU whilst a majority within Scotland voted to remain, Scotland's first minister, Nicola Sturgeon, announced that as a result a new independence referendum was "highly likely".

Historical subdivisions of Scotland included the mormaerdom, stewartry, earldom, burgh, parish, county and regions and districts. Some of these names are still sometimes used as geographical descriptors.

Modern Scotland is subdivided in various ways depending on the purpose. In local government, there have been 32 single-tier council areas since 1996, whose councils are responsible for the provision of all local government services. Decisions are made by councillors who are elected at local elections every five years. The head of each council is usually the Lord Provost alongside the Leader of the Council, with a Chief Executive being appointed as director of the council area. Community Councils are informal organisations that represent specific sub-divisions within each council area.

In the Scottish Parliament, there are 73 constituencies and eight regions. For the Parliament of the United Kingdom, there are 59 constituencies. Until 2013, the Scottish fire brigades and police forces were based on a system of regions introduced in 1975. For healthcare and postal districts, and a number of other governmental and non-governmental organisations such as the churches, there are other long-standing methods of subdividing Scotland for the purposes of administration.

City status in the United Kingdom is conferred by letters patent. There are seven cities in Scotland: Aberdeen, Dundee, Edinburgh, Glasgow, Inverness, Stirling and Perth.

Scots law has a basis derived from Roman law, combining features of both uncodified civil law, dating back to the "Corpus Juris Civilis", and common law with medieval sources. The terms of the Treaty of Union with England in 1707 guaranteed the continued existence of a separate legal system in Scotland from that of England and Wales. Prior to 1611, there were several regional law systems in Scotland, most notably Udal law in Orkney and Shetland, based on old Norse law. Various other systems derived from common Celtic or Brehon laws survived in the Highlands until the 1800s.

Scots law provides for three types of courts responsible for the administration of justice: civil, criminal and heraldic. The supreme civil court is the Court of Session, although civil appeals can be taken to the Supreme Court of the United Kingdom (or before 1 October 2009, the House of Lords). The High Court of Justiciary is the supreme criminal court in Scotland. The Court of Session is housed at Parliament House, in Edinburgh, which was the home of the pre-Union Parliament of Scotland with the High Court of Justiciary and the Supreme Court of Appeal currently located at the Lawnmarket. The sheriff court is the main criminal and civil court, hearing most cases. There are 49 sheriff courts throughout the country. District courts were introduced in 1975 for minor offences and small claims. These were gradually replaced by Justice of the Peace Courts from 2008 to 2010. The Court of the Lord Lyon regulates heraldry.

For three centuries the Scots legal system was unique for being the only national legal system without a parliament. This ended with the advent of the Scottish Parliament in 1999, which legislates for Scotland. Many features within the system have been preserved. Within criminal law, the Scots legal system is unique in having three possible verdicts: "guilty", "not guilty" and ""not proven"". Both "not guilty" and "not proven" result in an acquittal, typically with no possibility of retrial in accordance with the rule of double jeopardy. There is, however, the possibility of a retrial where new evidence emerges at a later date that might have proven conclusive in the earlier trial at first instance, where the person acquitted subsequently admits the offence or where it can be proved that the acquittal was tainted by an attempt to pervert the course of justice – see the provisions of the Double Jeopardy (Scotland) Act 2011. Many laws differ between Scotland and the other parts of the United Kingdom, and many terms differ for certain legal concepts. Manslaughter, in England and Wales, is broadly similar to culpable homicide in Scotland, and arson is called wilful fire raising. Indeed, some acts considered crimes in England and Wales, such as forgery, are not so in Scotland. Procedure also differs. Scots juries, sitting in criminal cases, consist of fifteen jurors, which is three more than is typical in many countries.

The Scottish Prison Service (SPS) manages the prisons in Scotland, which collectively house over 8,500 prisoners. The Cabinet Secretary for Justice is responsible for the Scottish Prison Service within the Scottish Government.

Health care in Scotland is mainly provided by NHS Scotland, Scotland's public health care system. This was founded by the National Health Service (Scotland) Act 1947 (later repealed by the National Health Service (Scotland) Act 1978) that took effect on 5 July 1948 to coincide with the launch of the NHS in England and Wales. However, even prior to 1948, half of Scotland's landmass was already covered by state-funded health care, provided by the Highlands and Islands Medical Service. Healthcare policy and funding is the responsibility of the Scottish Government's Health Directorates. The current Cabinet Secretary for Health and Sport is Jeane Freeman and the Director-General (DG) Health and chief executive, NHS Scotland is Paul Gray.

In 2008, the NHS in Scotland had around 158,000 staff including more than 47,500 nurses, midwives and health visitors and over 3,800 consultants. There are also more than 12,000 doctors, family practitioners and allied health professionals, including dentists, opticians and community pharmacists, who operate as independent contractors providing a range of services within the NHS in return for fees and allowances. These fees and allowances were removed in May 2010, and prescriptions are entirely free, although dentists and opticians may charge if the patient's household earns over a certain amount, about £30,000 per annum.

Scotland has a Western-style open mixed economy closely linked with the rest of the UK and the wider world. Traditionally, the Scottish economy was dominated by heavy industry underpinned by shipbuilding in Glasgow, coal mining and steel industries. Petroleum related industries associated with the extraction of North Sea oil have also been important employers from the 1970s, especially in the north-east of Scotland. De-industrialisation during the 1970s and 1980s saw a shift from a manufacturing focus towards a more service-oriented economy.

Scotland's gross domestic product (GDP), including oil and gas produced in Scottish waters, was estimated at £150 billion for the calendar year 2012. In 2014, Scotland's per capita GDP was one of the highest in the EU. As of April 2019 the Scottish unemployment rate was 3.3%, below the UK rate of 3.8%, and the Scottish employment rate was 75.9%.

Edinburgh is the financial services centre of Scotland, with many large finance firms based there, including: Lloyds Banking Group (owners of HBOS); the Government-owned Royal Bank of Scotland and Standard Life. Edinburgh was ranked 15th in the list of world financial centres in 2007, but fell to 37th in 2012, following damage to its reputation, and in 2016 was ranked 56th out of 86.
In 2014, total Scottish exports (excluding intra-UK trade) were estimated to be £27.5 billion. Scotland's primary exports include whisky, electronics and financial services. The United States, Netherlands, Germany, France, and Norway constitute the country's major export markets.

Whisky is one of Scotland's more known goods of economic activity. Exports increased by 87% in the decade to 2012 and were valued at £4.3 billion in 2013, which was 85% of Scotland's food and drink exports. It supports around 10,000 jobs directly and 25,000 indirectly. It may contribute £400–682 million to Scotland, rather than several billion pounds, as more than 80% of whisky produced is owned by non-Scottish companies.

A briefing published in 2002 by the Scottish Parliament Information Centre (SPICe) for the Scottish Parliament's Enterprise and Life Long Learning Committee stated that tourism accounted for up to 5% of GDP and 7.5% of employment.

Although the Bank of England is the central bank for the UK, three Scottish clearing banks issue Sterling banknotes: the Bank of Scotland, the Royal Bank of Scotland and the Clydesdale Bank. The value of the Scottish banknotes in circulation in 2013 was £3.8 billion, underwritten by the Bank of England using funds deposited by each clearing bank, under the Banking Act 2009, in order to cover the total value of such notes in circulation.

Of the money spent on UK defence, about £3.3 billion can be attributed to Scotland as of 2013. Although Scotland has a long military tradition predating the Treaty of Union with England, its armed forces now form part of the British Armed Forces, with the exception of the Atholl Highlanders, Europe's only legal private army. In 2006, the infantry regiments of the Scottish Division were amalgamated to form the Royal Regiment of Scotland. Other distinctively Scottish regiments in the British Army include the Scots Guards, the Royal Scots Dragoon Guards and the 154 (Scottish) Regiment RLC, an Army Reserve Regiment of the Royal Logistic Corps.

Because of their topography and perceived remoteness, parts of Scotland have housed many sensitive defence establishments. Between 1960 and 1991, the Holy Loch was a base for the US fleet of Polaris ballistic missile submarines. Today, Her Majesty's Naval Base Clyde, north-west of Glasgow, is the base for the four Trident-armed ballistic missile submarines that comprise the UK's nuclear deterrent. Scapa Flow was the major Fleet base for the Royal Navy until 1956.

A single front-line Royal Air Force base is located in Scotland. RAF Lossiemouth, located in Moray, is the most northerly air defence fighter base in the United Kingdom and is home to three fast-jet squadrons equipped with the Eurofighter Typhoon.

The Scottish education system has always been distinct from the rest of the United Kingdom, with a characteristic emphasis on a broad education. In the 15th century, the Humanist emphasis on education cumulated with the passing of the Education Act 1496, which decreed that all sons of barons and freeholders of substance should attend grammar schools to learn "perfyct Latyne", resulting in an increase in literacy among a male and wealthy elite. In the Reformation, the 1560 "First Book of Discipline" set out a plan for a school in every parish, but this proved financially impossible. In 1616 an act in Privy council commanded every parish to establish a school. By the late seventeenth century there was a largely complete network of parish schools in the lowlands, but in the Highlands basic education was still lacking in many areas. Education remained a matter for the church rather than the state until the Education (Scotland) Act 1872.

The "Curriculum for Excellence", Scotland's national school curriculum, presently provides the curricular framework for children and young people from age 3 to 18. All 3- and 4-year-old children in Scotland are entitled to a free nursery place. Formal primary education begins at approximately 5 years old and lasts for 7 years (P1–P7); children in Scotland study Standard Grades, or Intermediate qualifications between the ages of 14 and 16. These are being phased out and replaced by the National Qualifications of the Curriculum for Excellence. The school leaving age is 16, after which students may choose to remain at school and study for Access, Intermediate or Higher Grade and Advanced Higher qualifications. A small number of students at certain private, independent schools may follow the English system and study towards GCSEs and A and AS-Levels instead.

There are fifteen Scottish universities, some of which are amongst the oldest in the world. These include the University of St Andrews, the University of Glasgow, the University of Aberdeen and the University of Edinburgh—many of which are ranked amongst the best in the UK. Scotland had more universities per capita in QS' World University Rankings' top 100 in 2012 than any other nation. The country produces 1% of the world's published research with less than 0.1% of the world's population, and higher education institutions account for 9% of Scotland's service sector exports. Scotland's University Courts are the only bodies in Scotland authorised to award degrees.

Tuition is handled by the Student Awards Agency Scotland (SAAS), which does not charge fees to what it defines as "Young Students". Young Students are defined as those under 25, without children, marriage, civil partnership or cohabiting partner, who have not been outside of full-time education for more than three years. Fees exist for those outside the young student definition, typically from £1,200 to £1,800 for undergraduate courses, dependent on year of application and type of qualification. Postgraduate fees can be up to £3,400. The system has been in place since 2007 when graduate endowments were abolished.<ref name="www.scotland.gov.uk/News/Releases/2008/02/28172530"></ref> Labour's education spokesperson Rhona Brankin criticised the Scottish system for failing to address student poverty.

Scotland's universities are complemented in the provision of Further and Higher Education by 43 colleges. Colleges offer National Certificates, Higher National Certificates, and Higher National Diplomas. These Group Awards, alongside Scottish Vocational Qualifications, aim to ensure Scotland's population has the appropriate skills and knowledge to meet workplace needs. In 2014, research reported by the Office for National Statistics found that Scotland was the most highly educated country in Europe and among the most well-educated in the world in terms of tertiary education attainment, with roughly 40% of people in Scotland aged 16–64 educated to NVQ level 4 and above. Based on the original data for EU statistical regions, all four Scottish regions ranked significantly above the European average for completion of tertiary-level education by 25- to 64-year-olds.

Kilmarnock Academy in East Ayrshire is one of only two schools in the UK, and the only school in Scotland, to have educated two Nobel Prize Laureates – Alexander Fleming, discoverer of Penicillin, and John Boyd Orr, 1st Baron Boyd-Orr, for his scientific research into nutrition and his work as the first Director-General of the United Nations Food and Agriculture Organization (FAO).

Scottish music is a significant aspect of the nation's culture, with both traditional and modern influences. A famous traditional Scottish instrument is the Great Highland bagpipe, a wind instrument consisting of three drones and a melody pipe (called the chanter), which are fed continuously by a reservoir of air in a bag. Bagpipe bands, featuring bagpipes and various types of drums, and showcasing Scottish music styles while creating new ones, have spread throughout the world. The clàrsach (harp), fiddle and accordion are also traditional Scottish instruments, the latter two heavily featured in Scottish country dance bands. There are many successful Scottish bands and individual artists in varying styles including Annie Lennox, Amy Macdonald, Runrig, Belle and Sebastian, Boards of Canada, Camera Obscura, Cocteau Twins, Deacon Blue, Franz Ferdinand, Susan Boyle, Emeli Sandé, Texas, The View, The Fratellis, Twin Atlantic and Biffy Clyro. Other Scottish musicians include Shirley Manson, Paolo Nutini, Andy Stewart and Calvin Harris.

Scotland has a literary heritage dating back to the early Middle Ages. The earliest extant literature composed in what is now Scotland was in Brythonic speech in the 6th century, but is preserved as part of Welsh literature. Later medieval literature included works in Latin, Gaelic, Old English and French. The first surviving major text in Early Scots is the 14th-century poet John Barbour's epic "Brus", focusing on the life of Robert I, and was soon followed by a series of vernacular romances and prose works. In the 16th century, the crown's patronage helped the development of Scots drama and poetry, but the accession of James VI to the English throne removed a major centre of literary patronage and Scots was sidelined as a literary language. Interest in Scots literature was revived in the 18th century by figures including James Macpherson, whose Ossian Cycle made him the first Scottish poet to gain an international reputation and was a major influence on the European Enlightenment. It was also a major influence on Robert Burns, whom many consider the national poet, and Walter Scott, whose Waverley Novels did much to define Scottish identity in the 19th century. Towards the end of the Victorian era a number of Scottish-born authors achieved international reputations as writers in English, including Robert Louis Stevenson, Arthur Conan Doyle, J. M. Barrie and George MacDonald. In the 20th century the Scottish Renaissance saw a surge of literary activity and attempts to reclaim the Scots language as a medium for serious literature. Members of the movement were followed by a new generation of post-war poets including Edwin Morgan, who would be appointed the first Scots Makar by the inaugural Scottish government in 2004. From the 1980s Scottish literature enjoyed another major revival, particularly associated with a group of writers including Irvine Welsh. Scottish poets who emerged in the same period included Carol Ann Duffy, who, in May 2009, was the first Scot named UK Poet Laureate.

As one of the Celtic nations, Scotland and Scottish culture are represented at interceltic events at home and over the world. Scotland hosts several music festivals including Celtic Connections (Glasgow), and the Hebridean Celtic Festival (Stornoway). Festivals celebrating Celtic culture, such as Festival Interceltique de Lorient (Brittany), the Pan Celtic Festival (Ireland), and the National Celtic Festival (Portarlington, Australia), feature elements of Scottish culture such as language, music and dance.

The image of St. Andrew, martyred while bound to an X-shaped cross, first appeared in the Kingdom of Scotland during the reign of William I. Following the death of King Alexander III in 1286 an image of Andrew was used on the seal of the Guardians of Scotland who assumed control of the kingdom during the subsequent interregnum. Use of a simplified symbol associated with Saint Andrew, the saltire, has its origins in the late 14th century; the Parliament of Scotland decreeing in 1385 that Scottish soldiers should wear a white Saint Andrew's Cross on the front and back of their tunics. Use of a blue background for the Saint Andrew's Cross is said to date from at least the 15th century. Since 1606 the saltire has also formed part of the design of the Union Flag. There are numerous other symbols and symbolic artefacts, both official and unofficial, including the thistle, the nation's floral emblem (celebrated in the song, The Thistle o' Scotland), the Declaration of Arbroath, incorporating a statement of political independence made on 6 April 1320, the textile pattern tartan that often signifies a particular Scottish clan and the royal Lion Rampant flag. Highlanders can thank James Graham, 3rd Duke of Montrose, for the repeal in 1782 of the Act of 1747 prohibiting the wearing of tartans.
Although there is no official national anthem of Scotland, "Flower of Scotland" is played on special occasions and sporting events such as football and rugby matches involving the Scotland national teams and since 2010 is also played at the Commonwealth Games after it was voted the overwhelming favourite by participating Scottish athletes. Other currently less popular candidates for the National Anthem of Scotland include "Scotland the Brave", "Highland Cathedral", "Scots Wha Hae" and "A Man's A Man for A' That".

St Andrew's Day, 30 November, is the national day, although Burns' Night tends to be more widely observed, particularly outside Scotland. In 2006, the Scottish Parliament passed the St Andrew's Day Bank Holiday (Scotland) Act 2007, designating the day an official bank holiday. Tartan Day is a recent innovation from Canada.

The national animal of Scotland is the unicorn, which has been a Scottish heraldic symbol since the 12th century.

Scottish cuisine has distinctive attributes and recipes of its own but shares much with wider British and European cuisine as a result of local and foreign influences, both ancient and modern. Traditional Scottish dishes exist alongside international foodstuffs brought about by migration. Scotland's natural larder of game, dairy products, fish, fruit, and vegetables is the chief factor in traditional Scots cooking, with a high reliance on simplicity and a lack of spices from abroad, as these were historically rare and expensive. Irn-Bru is the most common Scottish carbonated soft drink, often described as "Scotland's other national drink" (after whisky). During the Late Middle Ages and early modern era, French cuisine played a role in Scottish cookery due to cultural exchanges brought about by the "Auld Alliance", especially during the reign of Mary, Queen of Scots. Mary, on her return to Scotland, brought an entourage of French staff who are considered responsible for revolutionising Scots cooking and for some of Scotland's unique food terminology.

National newspapers such as the "Daily Record", "The Herald", "The Scotsman" and "The National" are all produced in Scotland. Important regional dailies include the Evening News in Edinburgh, "The Courier" in Dundee in the east, and "The Press and Journal" serving Aberdeen and the north. Scotland is represented at the Celtic Media Festival, which showcases film and television from the Celtic countries. Scottish entrants have won many awards since the festival began in 1980.

Television in Scotland is largely the same as UK-wide broadcasts, however, the national broadcaster is BBC Scotland, a constituent part of the British Broadcasting Corporation, the publicly funded broadcaster of the United Kingdom. It runs three national television stations, and the national radio stations, "BBC Radio Scotland" and "BBC Radio nan Gàidheal", amongst others. Scotland also has some programming in the Gaelic language. BBC Alba is the national Gaelic-language channel. The main Scottish commercial television station is STV which broadcasts on two of the three ITV regions of Scotland.

Scotland has a number of production companies which produce films and television programmes for Scottish, UK and international audiences. Popular films associated with Scotland through Scottish production or being filmed in Scotland include "Braveheart" (1995), "Highlander" (1986), "Trainspotting" (1996), "Red Road" (2006), "Neds" (2010), "The Angel's Share" (2012), "Brave" (2012) and "Outlaw King" (2018). Popular television programmes associated with Scotland include the long running BBC Scotland soap opera "River City" which has been broadcast since 2002, "Still Game", a popular Scottish sitcom broadcast throughout the United Kingdom (2002–2007, revived in 2016), "Rab C. Nesbitt", "Two Doors Down" and "Take the High Road".

Wardpark Studios in Cumbernauld is one of Scotland's television and film production studios where the television programme "Outlander" is produced. Dumbarton Studios, located in Dumbarton is largely used for BBC Scotland programming, used for the filming and production of television programmes such as "Still Game", "River City", "Two Doors Down" "Shetland".

Scotland hosts its own national sporting competitions and has independent representation at several international sporting events, including the FIFA World Cup, the Rugby Union World Cup, the Rugby League World Cup, the Cricket World Cup, the Netball World Cup and the Commonwealth Games. Scotland has its own national governing bodies, such as the Scottish Football Association (the second oldest national football association in the world) and the Scottish Rugby Union. Variations of football have been played in Scotland for centuries, with the earliest reference dating back to 1424.

The world's first official international association football match was held in 1872 and was the idea of C. W. Alcock of the Football Association which was seeking to promote Association Football in Scotland. The match took place at the West of Scotland Cricket Club's Hamilton Crescent ground in the Partick area of Glasgow. The match was between Scotland and England and resulted in a 0–0 draw. Following this, the newly developed football became the most popular sport in Scotland. The Scottish Cup was first contested in 1873. Queen's Park F.C., in Glasgow, is probably the oldest association football club in the world outside England.

The Scottish Football Association (SFA), the second-oldest national football association in the world, is the main governing body for Scottish association football, and a founding member of the International Football Association Board (IFAB) which governs the Laws of the Game. As a result of this key role in the development of the sport Scotland is one of only four countries to have a permanent representative on the IFAB; the other four representatives being appointed for set periods by FIFA.

The SFA also has responsibility for the Scotland national football team, whose supporters are commonly known as the "Tartan Army". , Scotland are ranked as the 50th best national football team in the FIFA World Rankings. The national team last attended the World Cup in France in 1998, but finished last in their group stage. The Scotland women's team have achieved more recent success, qualifying for both Euro 2017 and the 2019 World Cup. , they were ranked as the 22nd best women's national team in the FIFA Rankings.

Scottish clubs have achieved some success in European competitions, with Celtic winning the European Cup in 1967, Rangers and Aberdeen winning the UEFA Cup Winners' Cup in 1972 and 1983 respectively, and Aberdeen also winning the UEFA Super Cup in 1983. Celtic, Rangers and Dundee United have also reached European finals, the most recent of these being Rangers in 2008.

With the modern game of golf originating in 15th-century Scotland, the country is promoted as the home of golf. To many golfers the Old Course in the Fife town of St Andrews, an ancient links course dating to before 1552, is considered a site of pilgrimage. In 1764, the standard 18-hole golf course was created at St Andrews when members modified the course from 22 to 18 holes. The world's oldest golf tournament, and golf's first major, is The Open Championship, which was first played on 17 October 1860 at Prestwick Golf Club, in Ayrshire, Scotland, with Scottish golfers winning the earliest majors. There are many other famous golf courses in Scotland, including Carnoustie, Gleneagles, Muirfield, and Royal Troon.

Other distinctive features of the national sporting culture include the Highland games, curling and shinty. In boxing, Scotland has had 13 world champions, including Ken Buchanan, Benny Lynch and Jim Watt.

Scotland has competed at every Commonwealth Games since 1930 and has won 356 medals in total—91 Gold, 104 Silver and 161 Bronze. Edinburgh played host to the Commonwealth Games in 1970 and 1986, and most recently Glasgow in 2014.

Scotland's primary sources for energy are provided though renewable energy (42%), nuclear (35%) and fossil fuel generation (22%).

The Scottish Government has a target to have the equivalent of 50% of the energy for Scotland's heat, transport and electricity consumption to be supplied from renewable sources by 2030.

Scotland has five international airports operating scheduled services to Europe, North America and Asia, as well domestic services to England, Northern Ireland and Wales.


Highlands and Islands Airports operates eleven airports across the Highlands, Orkney, Shetland and the Western Isles, which are primarily used for short distance, public service operations, although Inverness Airport has a number of scheduled flights to destinations across the UK and mainland Europe.

Edinburgh Airport is currently Scotland's busiest airport handling over 13 million passengers in 2017. It is also the UK's 6th busiest airport.

British Airways, easyJet, flybe, Jet2, and Ryanair operate the majority of flights between Scotland and other major UK and European airports.

Four airlines are based in Scotland:


Network Rail owns and operates the fixed infrastructure assets of the railway system in Scotland, while the Scottish Government retains overall responsibility for rail strategy and funding in Scotland. Scotland's rail network has around 350 railway stations and of track. Over 89.3million passenger journeys are made each year.

The East Coast and West Coast main railway lines connect the major cities and towns of Scotland with each other and with the rail network in England. London North Eastern Railway provides inter-city rail journeys between Glasgow, Edinburgh, Aberdeen and Inverness to London. Domestic rail services within Scotland are operated by Abellio ScotRail. During the time of British Rail, the West Coast Main Line from London Euston to Glasgow Central was electrified in the early 1970s, followed by the East Coast Main Line in the late 1980s. British Rail created the ScotRail brand. When British Rail existed, many railway lines in Strathclyde were electrified. Strathclyde Passenger Transport Executive was at the forefront with the acclaimed "largest electrified rail network outside London". Some parts of the network are electrified, but there are no electrified lines in the Highlands, Angus, Aberdeenshire, the cities of Dundee or Aberdeen, or Perth & Kinross, and none of the islands has a rail link (although the railheads at Kyle of Lochalsh and Mallaig principally serve the islands).

The East Coast Main Line crosses the Firth of Forth by the Forth Bridge. Completed in 1890, this cantilever bridge has been described as "the one internationally recognised Scottish landmark". Scotland's rail network is managed by Transport Scotland.

The Scottish motorways and major trunk roads are managed by Transport Scotland. The remainder of the road network is managed by the Scottish local authorities in each of their areas.

Regular ferry services operate between the Scottish mainland and outlying islands. Ferries serving both the inner and outer Hebrides are principally operated by the state-owned enterprise Caledonian MacBrayne.

Services to the Northern Isles are operated by Serco. Other routes, served by multiple companies, connect southwest Scotland to Northern Ireland. DFDS Seaways operated a freight-only Rosyth – Zeebrugge ferry service, until a fire damaged the vessel DFDS were using. A passenger service was also operated between 2002–2010.

Additional routes are operated by local authorities.




</doc>
<doc id="26995" url="https://en.wikipedia.org/wiki?curid=26995" title="Shire">
Shire

A shire is a traditional term for a division of land, found in Great Britain, Australia, New Zealand and some other English-speaking countries. It was first used in Wessex from the beginning of Anglo-Saxon settlement, and spread to most of the rest of England in the tenth century. In some rural parts of Australia, a shire is a local government area; however, in Australia it is not synonymous with a "county", which is a lands administrative division.

The word "shire" derives from the Old English "sćir", from the Proto-Germanic "skizo" (Old High German "sćira"), denoting an "official charge" a "district under a governor", and a "care". In UK usage, "shire" became synonymous with "county", an administrative term introduced to England through the Norman Conquest, in A.D. 1066. In contemporary British usage, the word "counties" also refers to "shires", mainly in places, such as Shire Hall. 

In regions with rhotic pronunciation, such as Scotland, the word "shire" is pronounced ; in areas of non-rhotic pronunciation, the final R is silent, unless the next word begins in a vowel sound. In England and Wales, when "shire" is a place-name suffix, the vowel is unstressed and usually shortened (monophthongised); the pronunciations include and , with the final R pronunciation depending on rhoticity. The vowel is normally reduced to a single schwa, as in "Leicestershire" and "Berkshire" .

The system was first used in the kingdom of Wessex from the beginning of Anglo-Saxon settlement, and spread to most of the rest of England in the tenth century, along with the West Saxon kingdom's political domination. In Domesday (1086) the city of York was divided into shires. The first shires of Scotland were created in English-settled areas such as Lothian and the Borders, in the ninth century. King David I more consistently created shires and appointed sheriffs across lowland "shores" of Scotland.

The shire in early days was governed by an "ealdorman" and in the later Anglo-Saxon period by royal official known as a "shire reeve" or sheriff. The shires were divided into hundreds or wapentakes, although other less common sub-divisions existed. An alternative name for a shire was a "sheriffdom" until sheriff court reforms separated the two concepts. The phrase "shire county" applies, unofficially, to non-metropolitan counties in England, specifically those that are not local unitary authority areas. In Scotland the word "county" was not adopted for the shires. Although "county" appears in some texts, "shire" was the normal name until counties for statutory purposes were created in the nineteenth century.

"Shire" also refers, in a narrower sense, to ancient counties with names that ended in "shire". These counties are typically (though not always) named after their county town. The suffix "-shire" is attached to most of the names of English, Scottish and Welsh counties. It tends not to be found in the names of shires that were pre-existing divisions. Essex, Kent, and Sussex, for example, have never borne a "-shire", as each represents a former Anglo-Saxon kingdom. Similarly Cornwall was a British kingdom before it became an English county. The term "shire" is not used in the names of the six traditional counties of Northern Ireland.

Counties in England bearing the "-shire" suffix include: Bedfordshire, Berkshire, Buckinghamshire, Cambridgeshire, Cheshire, Derbyshire, Gloucestershire, Hampshire, Herefordshire, Hertfordshire, Huntingdonshire, Lancashire, Lincolnshire, Leicestershire, Northamptonshire, Nottinghamshire, Oxfordshire, Shropshire, Staffordshire, Warwickshire, Wiltshire, Worcestershire and Yorkshire. These counties, on their historical boundaries, cover a little more than half the area of England. The counties that do not use "-shire" are mainly in three areas, in the south-east, south-west and far north of England. Several of these counties no longer exist as administrative units, or have had their administrative boundaries reduced by local government reforms. Several of the successor authorities retain the "-shire" county names, such as West Yorkshire and South Gloucestershire.

The county of Devon was historically known as Devonshire, although this is no longer the official name. Similarly, Dorset, Rutland and Somerset were formerly known as Dorsetshire, Rutlandshire and Somersetshire, but these terms are no longer official, and are rarely used outside the local populations.

Hexhamshire was a county in the north-east of England from the early 12th century until 1572, when it was incorporated into Northumberland.

In Scotland, barely affected by the Norman conquest of England, the word "shire" prevailed over "county" until the 19th century. Earliest sources have the same usage of the "-shire" suffix as in England (though in Scots this was oftenmost "schyr"). Later the "Shire" appears as a separate word.

"Shire" names in Scotland include Aberdeenshire, Ayrshire, Banffshire, Berwickshire, Clackmannanshire, Cromartyshire, Dumfriesshire, Dunbartonshire, East Dumbartonshire, Inverness-shire, Kincardineshire, Kinross-shire, Kirkcudbrightshire, Lanarkshire, Morayshire, Nairnshire, Peeblesshire, Perthshire, Renfrewshire, Ross-shire, Roxburghshire, Selkirkshire, Stirlingshire, and Wigtownshire. 

In Scotland four shires have alternative names with the "-shire" suffix: Angus (Forfarshire), East Lothian (Haddingtonshire), Midlothian (Edinburghshire) and West Lothian (Linlithgowshire).

Sutherland is occasionally still referred to as Sutherlandshire. Similarly, Argyllshire, Buteshire, Caithness-shire and Fifeshire are sometimes found. Also, Morayshire was previously called Elginshire. There is debate about whether Argyllshire was ever really used.

Shires in Wales bearing the "-shire" suffix include: Brecknockshire (or Breconshire), Caernarfonshire (historically Carnarvonshire), Cardiganshire (in Welsh- Ceredigion), Carmarthenshire, Denbighshire, Flintshire, Monmouthshire, Montgomeryshire, Pembrokeshire, and Radnorshire. In Wales, the counties of Merioneth and Glamorgan are occasionally referred to with the "shire" suffix. The only traditional Welsh county that never takes "shire" is Anglesey—in English: in Welsh it is referred to as 'Sir Fon'.

The suffix "-shire" could be a generalised term referring to a district. It did not acquire the strong association with county until later. Other than these, the term was used for several other districts. Bedlingtonshire, Craikshire, Norhamshire and Islandshire were exclaves of County Durham, which were incorporated into Northumberland or Yorkshire in 1844. The suffix was also used for many hundreds, wapentakes and liberties such as Allertonshire, Blackburnshire, Halfshire, Howdenshire, Leylandshire, Powdershire, Pydarshire, Richmondshire, Riponshire, Salfordshire, Triggshire, Tynemouthshire, West Derbyshire and Wivelshire, counties corporate such as Hullshire, and other districts such as Applebyshire, Bamburghshire, Bunkleshire, Carlisleshire, Coldinghamshire, Coxwoldshire, Cravenshire, Hallamshire, Mashamshire and Yetholmshire. Richmondshire is today the name of a local government district of North Yorkshire.

Non-county shires were very common in Scotland. Kinross-shire and Clackmannanshire are arguably survivals from such districts. Non-county "shires" in Scotland include Bunkleshire, Coldinghamshire and Yetholmshire.

"Shire" is the most common word in Australia for rural local government areas (LGAs). New South Wales, Victoria, Queensland, Western Australia and the Northern Territory use the term "shire" for this unit; the territories of Christmas Island and Cocos Island are also shires. In contrast, South Australia uses district and region for its rural LGA units, while Tasmania uses municipality. Shires are generally functionally indistinguishable from towns, borough, municipalities, or cities.

Three LGAs in outer metropolitan Sydney and four in outer metropolitan Melbourne have populations exceeding that of towns or municipalities, but retain significant bushlands and/or semi-rural areas, and most have continued to use "shire" in their titles whilst others have dropped it from theirs. These "city-shires" are:

Sydney:

Melbourne:

In 1634, eight "shires" were created in the Virginia Colony by order of Charles I, King of England. They were renamed as counties only a few years later. They were:


As of 2013, six of the original eight Shires of Virginia are considered to be still extant whilst two have consolidated with a neighbouring city. Most of their boundaries have changed in the intervening centuries.

Before the Province of New York was granted county subdivisions and a greater royal presence in 1683, the early ducal colony consisted of York Shire, as well as Albany and Ulster, after the three titles held by Prince James: Duke of York, Duke of Albany, Earl of Ulster. While these were basically renamed Dutch core settlements, they were quickly converted to English purposes, while the Dutch remained within the colony, as opposed to later practice of the Acadian Expulsion. Further Anglo-Dutch synthesis occurred when Prince James enacted the Dominion of New England and later when William III of England took over through the Glorious Revolution.

A few New England states and commonwealths; namely Vermont, Rhode Island, Massachusetts, and Maine; still use the term "shire town" for their county seats, although they use the term county, rather than shire.

The word also survives in the name of the state of New Hampshire, whose co-founder, John Mason, named his Province of New Hampshire after the English county of Hampshire.



</doc>
<doc id="26997" url="https://en.wikipedia.org/wiki?curid=26997" title="Scientist">
Scientist

A scientist is someone who conducts scientific research to advance knowledge in an area of interest.

In classical antiquity, there was no real ancient analog of a modern scientist. Instead, philosophers engaged in the philosophical study of nature called natural philosophy, a precursor of natural science. It was not until the 19th century that the term "scientist" came into regular use after it was coined by the theologian, philosopher, and historian of science William Whewell in 1833, to describe the noted polymath Mary Somerville.

In modern times, many scientists have advanced degrees in an area of science and pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit environments.""

The roles of "scientists", and their predecessors before the emergence of modern scientific disciplines, have evolved considerably over time. Scientists of different eras (and before them, natural philosophers, mathematicians, natural historians, natural theologians, engineers, and others who contributed to the development of science) have had widely different places in society, and the social norms, ethical values, and epistemic virtues associated with scientists—and expected of them—have changed over time as well. Accordingly, many different historical figures can be identified as early scientists, depending on which characteristics of modern science are taken to be essential.

Some historians point to the Scientific Revolution that began in 16th century as the period when science in a recognizably modern form developed. It wasn't until the 19th century that sufficient socioeconomic changes occurred for scientists to emerge as a major profession.

Knowledge about nature in classical antiquity was pursued by many kinds of scholars. Greek contributions to science—including works of geometry and mathematical astronomy, early accounts of biological processes and catalogs of plants and animals, and theories of knowledge and learning—were produced by philosophers and physicians, as well as practitioners of various trades. These roles, and their associations with scientific knowledge, spread with the Roman Empire and, with the spread of Christianity, became closely linked to religious institutions in most of European countries. Astrology and astronomy became an important area of knowledge, and the role of astronomer/astrologer developed with the support of political and religious patronage. By the time of the medieval university system, knowledge was divided into the "trivium"—philosophy, including natural philosophy—and the "quadrivium"—mathematics, including astronomy. Hence, the medieval analogs of scientists were often either philosophers or mathematicians. Knowledge of plants and animals was broadly the province of physicians.

Science in medieval Islam generated some new modes of developing natural knowledge, although still within the bounds of existing social roles such as philosopher and mathematician. Many proto-scientists from the Islamic Golden Age are considered polymaths, in part because of the lack of anything corresponding to modern scientific disciplines. Many of these early polymaths were also religious priests and theologians: for example, Alhazen and al-Biruni were mutakallimiin; the physician Avicenna was a hafiz; the physician Ibn al-Nafis was a hafiz, muhaddith and ulema; the botanist Otto Brunfels was a theologian and historian of Protestantism; the astronomer and physician Nicolaus Copernicus was a priest. During the Italian Renaissance scientists like Leonardo Da Vinci, Michelangelo, Galileo Galilei and Gerolamo Cardano have been considered as the most recognizable polymaths.

During the Renaissance, Italians made substantial contributions in science. Leonardo Da Vinci made significant discoveries in paleontology and anatomy. The Father of modern Science,
Galileo Galilei, made key improvements on the thermometer and telescope which allowed him to observe and clearly describe the solar system. Descartes was not only a pioneer of analytic geometry but formulated a theory of mechanics and advanced ideas about the origins of animal movement and perception. Vision interested the physicists Young and Helmholtz, who also studied optics, hearing and music. Newton extended Descartes' mathematics by inventing calculus (contemporaneously with Leibniz). He provided a comprehensive formulation of classical mechanics and investigated light and optics. Fourier founded a new branch of mathematics — infinite, periodic series — studied heat flow and infrared radiation, and discovered the greenhouse effect. Girolamo Cardano, Blaise Pascal Pierre de Fermat, Von Neumann, Turing, Khinchin, Markov and Wiener, all mathematicians, made major contributions to science and probability theory, including the ideas behind computers, and some of the foundations of statistical mechanics and quantum mechanics. Many mathematically inclined scientists, including Galileo, were also musicians.

There are many compelling stories in medicine and biology, such as the development of ideas about the circulation of blood from Galen to Harvey.

During the age of Enlightenment, Luigi Galvani, the pioneer of the bioelectromagnetics, discovered the animal electricity. He discovered that a charge applied to the spinal cord of a frog could generate muscular spasms throughout its body. Charges could make frog legs jump even if the legs were no longer attached to a frog. While cutting a frog leg, Galvani's steel scalpel touched a brass hook that was holding the leg in place. The leg twitched. Further experiments confirmed this effect, and Galvani was convinced that he was seeing the effects of what he called animal electricity, the life force within the muscles of the frog. At the University of Pavia, Galvani's colleague Alessandro Volta was able to reproduce the results, but was sceptical of Galvani's explanation.

Lazzaro Spallanzani is one of the most influential figures in experimental physiology and the natural sciences. His investigations have exerted a lasting influence on the medical sciences. He made important contributions to the experimental study of bodily functions and animal reproduction.

Francesco Redi discovered that microorganisms can cause disease. 

Until the late 19th or early 20th century, scientists were still referred to as "natural philosophers" or "men of science".

English philosopher and historian of science William Whewell coined the term "scientist" in 1833, and it first appeared in print in Whewell's anonymous 1834 review of Mary Somerville's "On the Connexion of the Physical Sciences" published in the "Quarterly Review". Whewell's suggestion of the term was partly satirical, a response to changing conceptions of science itself in which natural knowledge was increasingly seen as distinct from other forms of knowledge. Whewell wrote of "an increasing proclivity of separation and dismemberment" in the sciences; while highly specific terms proliferated—chemist, mathematician, naturalist—the broad term "philosopher" was no longer satisfactory to group together those who pursued science, without the caveats of "natural" or "experimental" philosopher. Members of the British Association for the Advancement of Science had been complaining about the lack of a good term at recent meetings, Whewell reported in his review; alluding to himself, he noted that "some ingenious gentleman proposed that, by analogy with "artist", they might form [the word] "scientist", and added that there could be no scruple in making free with this term since we already have such words as "economist", and "atheist"—but this was not generally palatable".

Whewell proposed the word again more seriously (and not anonymously) in his 1840 "The Philosophy of the Inductive Sciences":

He also proposed the term "physicist" at the same time, as a counterpart to the French word "physicien". Neither term gained wide acceptance until decades later; "scientist" became a common term in the late 19th century in the United States and around the turn of the 20th century in Great Britain. By the twentieth century, the modern notion of science as a special brand of information about the world, practiced by a distinct group and pursued through a unique method, was essentially in place.

Ramón y Cajal won the Nobel Prize in 1906 for his remarkable observations in neuroanatomy.

Marie Curie became the first female to win the Nobel Prize and the first person to win it twice. Her efforts led to the development of nuclear energy and Radiotherapy for the treatment of cancer. In 1922, she was appointed a member of the International Commission on Intellectual Co-operation by the Council of the League of Nations. She campaigned for scientist's right to patent their discoveries and inventions. She also campaigned for free access to international scientific literature and for internationally recognized scientific symbols.

As a profession, the scientist of today is widely recognized. 

In modern times, many professional scientists are trained in an academic setting (e.g., universities and research institutes), mostly at the level of graduate schools. Upon completion, they would normally attain an academic degree, with the highest degree being a doctorate such as a Doctor of Philosophy (PhD), Doctor of Medicine (MD), Doctor of Engineering (DEng), or even a dual doctoral degree (e.g., MD, PhD). Although graduate education for scientists varies among institutions and countries, some common training requirements include specializing in an area of interest, publishing research findings in peer-reviewed scientific journals and presenting them at scientific conferences, giving lectures or teaching, and defending a thesis (or dissertation) during an oral examination. To aid them in this endeavor, graduate students often work under the guidance of a mentor, usually a senior scientist, which may continue after the completion of their doctorates whereby they work as postdoctoral researchers.

After the completion of their training, many scientists pursue careers in a variety of work settings and conditions. In 2017, the British scientific journal "Nature" published the results of a large-scale survey of more than 5,700 doctoral students worldwide, asking them which sectors of the economy that would like to work in. A little over half of the respondents wanted to pursue a career in academia, with smaller proportions hoping to work in industry, government, and nonprofit environments.

Scientists are motivated to work in several ways. Many have a desire to understand why the world is as we see it and how it came to be. They exhibit a strong curiosity about reality. Other motivations are recognition by their peers and prestige. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, chemistry, and economics. 

Some scientists have a desire to apply scientific knowledge for the benefit of people's health, the nations, the world, nature, or industries (academic scientist and industrial scientist). Scientists tend to be less motivated by direct financial reward for their work than other careers. As a result, scientific researchers often accept lower average salaries when compared with many other professions which require a similar amount of training and qualification.

Although there have been exceptions, most scientists tend to do their best research when they are relatively young, in their 30s.

Scientists include experimentalists who mainly perform experiments to test hypotheses, and theoreticians who mainly develop models to explain existing data and predict new results. There is a continuum between two activities and the division between them is not clear-cut, with many scientists performing both tasks.

Those considering science as a career often look to the frontiers. These include cosmology and biology, especially molecular biology and the human genome project. Other areas of active research include the exploration of matter at the scale of elementary particles as described by high-energy physics, and materials science, which seeks to discover and design new materials. Although there have been remarkable discoveries with regard to brain function and neurotransmitters, the nature of the mind and human thought still remains unknown.


The number of scientists is vastly different from country to country. For instance, there are only four full-time scientists per 10,000 workers in India while this number is 79 for the United Kingdom and the United States.
According to the United States National Science Foundation 4.7 million people with science degrees worked in the United States in 2015, across all disciplines and employment sectors. The figure included twice as many men as women. Of that total, 17% worked in academia, that is, at universities and undergraduate institutions, and men held 53% of those positions. 5% of scientists worked for the federal government and about 3.5% were self-employed. Of the latter two groups, two-thirds were men. 59% of US scientists were employed in industry or business, and another 6% worked in non-profit positions.

Scientist and engineering statistics are usually intertwined, but they indicate that women enter the field far less than men, though this gap is narrowing. The number of science and engineering doctorates awarded to women rose from a mere 7 percent in 1970 to 34 percent in 1985 and in engineering alone the numbers of bachelor's degrees awarded to women rose from only 385 in 1975 to more than 11000 in 1985. 







</doc>
<doc id="27000" url="https://en.wikipedia.org/wiki?curid=27000" title="Smog">
Smog

Smog is a type of intense air pollution. The word "smog" was coined in the early 20th century, and is a contraction (portmanteau) of the words smoke and fog to refer to smoky fog; its opacity, and odor. The word was then intended to refer to what was sometimes known as pea soup fog, a familiar and serious problem in London from the 19th century to the mid-20th century. This kind of visible air pollution is composed of nitrogen oxides, sulphur oxides, ozone, smoke and other particulates. Man-made smog is derived from coal combustion emissions, vehicular emissions, industrial emissions, forest and agricultural fires and photochemical reactions of these emissions.

Smog is often categorized as being either summer smog or winter smog. Summer smog is primarily associated with the photochemical formation of ozone. During the summer season when the temperatures are warmer and there is more sunlight present, photochemical smog is the dominant type of smog formation. During the winter months when the temperatures are colder, and atmospheric inversions are common, there is an increase in coal and other fossil fuel usage to heat homes and buildings. These combustion emissions, together with the lack of pollutant dispersion under inversions, characterize winter smog formation. While photochemical smog is the main smog formation mechanism during summer months, winter smog episodes are still common. Smog formation in general relies on both primary and secondary pollutants. Primary pollutants are emitted directly from a source, such as emissions of sulfur dioxide from coal combustion. Secondary pollutants, such as ozone, are formed when primary pollutants undergo chemical reactions in the atmosphere.

Photochemical smog, as found for example in Los Angeles, is a type of air pollution derived from vehicular emission from internal combustion engines and industrial fumes. These pollutants react in the atmosphere with sunlight to form secondary pollutants that also combine with the primary emissions to form photochemical smog. In certain other cities, such as Delhi, smog severity is often aggravated by stubble burning in neighboring agricultural areas. The atmospheric pollution levels of Los Angeles, Beijing, Delhi, Lahore, Mexico City, Tehran and other cities are often increased by an inversion that traps pollution close to the ground. The developing smog is usually toxic to humans and can cause severe sickness, a shortened life span, or premature death.

Coinage of the term "smog" is often attributed to Dr. Henry Antoine Des Voeux in his 1905 paper, "Fog and Smoke" for a meeting of the Public Health Congress. The 26 July 1905 edition of the London newspaper "Daily Graphic" quoted Des Voeux, "He said it required no science to see that there was something produced in great cities which was not found in the country, and that was smoky fog, or what was known as 'smog'." The following day the newspaper stated that "Dr. Des Voeux did a public service in coining a new word for the London fog." However, the term appears fifteen years earlier than Dr. Voeux's paper, in a column in the July 3, 1880, Santa Cruz Weekly Sentinel.

Coal fires can emit significant clouds of smoke that contribute to the formation of winter smog. Coal fires can be used to heat individual buildings or to provide energy in a power-producing plant. Air pollution from this source has been reported in England since the Middle Ages.London, in particular, was notorious up through the mid-20th century for its coal-caused smogs, which were nicknamed 'pea-soupers.' Air pollution of this type is still a problem in areas that generate significant smoke from burning coal. The emissions from coal combustion are one of the main causes of air pollution in China. Especially during autumn and winter when coal-fired heating ramps up, the amount of produced smoke at times forces some Chinese cities to close down roads, schools or airports. One prominent example for this was China's Northeastern city of Harbin in 2013.

Traffic emissions – such as from trucks, buses, and automobiles– also contribute to the formation of smog. Airborne by-products from vehicle exhaust systems cause air pollution and are a major ingredient in the creation of smog in some large cities.

The major culprits from transportation sources are carbon monoxide (CO),nitrogen oxides (NO and NO),volatile organic compounds, and hydrocarbons (hydrocarbons are the main component of petroleum fuels such as gasoline and diesel fuel). Transportation emissions also include sulfur dioxides and particulate matter but in much smaller quantities than the pollutants mentioned previously. The nitrogen oxides and volatile organic compounds can undergo a series of chemical reactions with sunlight, heat, ammonia, moisture, and other compounds to form the noxious vapors, ground level ozone, and particles that comprise smog.

Photochemical smog, often referred to as "summer smog", is the chemical reaction of sunlight, nitrogen oxides and volatile organic compounds in the atmosphere, which leaves airborne particles and ground-level ozone. Photochemical smog depends on primary pollutants as well as the formation of secondary pollutants. These primary pollutants include nitrogen oxides, particularly nitric oxide (NO) and nitrogen dioxide (NO), and volatile organic compounds. The relevant secondary pollutants include peroxylacyl nitrates (PAN), tropospheric ozone, and aldehydes. An important secondary pollutant for photochemical smog is ozone, which is formed when hydrocarbons (HC) and nitrogen oxides (NO) combine in the presence of sunlight; nitrogen dioxide (NO), which is formed as nitric oxide (NO) combines with oxygen (O) in the air. In addition, when SO and NO are emitted they eventually are oxidized in the troposphere to nitric acid and sulfuric acid, which, when mixed with water, form the main components of acid rain. All of these harsh chemicals are usually highly reactive and oxidizing. Photochemical smog is therefore considered to be a problem of modern industrialization. It is present in all modern cities, but it is more common in cities with sunny, warm, dry climates and a large number of motor vehicles. Because it travels with the wind, it can affect sparsely populated areas as well.
The composition and chemical reactions involved in photochemical smog were not understood until the 1950s. In 1948, flavor chemist Arie Haagen-Smit adapted some of his equipment to collect chemicals from polluted air, and identified ozone as a component of Los Angeles smog. Haagen-Smit went on to discover that nitrogen oxides from automotive exhausts and gaseous hydrocarbons from cars and oil refineries, exposed to sunlight, were key ingredients in the formation of ozone and photochemical smog. Haagen-Smit worked with Arnold Beckman, who developed various equipment for detecting smog, ranging from an "Apparatus for recording gas concentrations in the atmosphere" patented on October 7, 1952, to "air quality monitoring vans" for use by government and industry.

During the morning rush hour, a high concentration of nitric oxide and hydrocarbons are emitted to the atmosphere, mostly via on-road traffic but also from industrial sources. Some hydrocarbons are rapidly oxidized by OH· and form peroxy radicals, which convert nitric oxide (NO) to nitrogen dioxide (NO).

(1) <chem>R{.} + O2 + M -> RO2{.} + M</chem>

(2) <chem>RO2{.} + NO -> NO2 + RO{.}</chem>

(3) <chem>HO2{.} + NO -> NO2 + OH{.}</chem>

Nitrogen dioxide (NO) and nitric oxide (NO) further react with ozone (O) in a series of chemical reactions:

(4) <chem>NO2 + hv -> O(^3P) + NO</chem>, formula_1

(5) <chem>O(^3P) + O2 + M-> O3 + M(heat)</chem>

(6) <chem>O3 + NO -> NO2 + O2</chem>

This series of equations is referred to as the photostationary state (PSS). However, because of the presence of Reaction 2 and 3, NO and ozone are not in a perfect steady state. By replacing Reaction 6 with Reaction 2 and Reaction 3, the O molecule is no longer destroyed. Therefore, the concentration of ozone keeps increasing throughout the day. This mechanism can escalate the formation of ozone in smog. Other reactions such as the photooxidation of formaldehyde (HCHO), a common secondary pollutant, can also contribute to the increased concentration of ozone and NO. Photochemical smog is more prevalent during summer days since incident solar radiation fluxes are high, which favors the formation of ozone (reactions 4 and 5). The presence of a temperature inversion layer is another important factor. That is because it prevents the vertical convective mixing of the air and thus allows the pollutants, including ozone, to accumulate near the ground level, which again favors the formation of photochemical smog.

There are certain reactions that can limit the formation of O in smog. The main limiting reaction in polluted areas is:

(7) <chem>NO2 + OH{.} + M -> HNO3 + M</chem>

This reaction removes NO which limits the amount of O that can be produced from its photolysis (reaction 4). HNO is a sticky compound that can easily be removed onto surfaces (dry deposition) or dissolved in water and be rained out (wet deposition). Both ways are common in the atmosphere and can efficiently remove the radicals and nitrogen dioxide.

An erupting volcano can emit high levels of sulphur dioxide along with a large quantity of particulates matter; two key components to the creation of smog. However, the smog created as a result of a volcanic eruption is often known as vog to distinguish it as a natural occurrence. The chemical reactions that form smog following a volcanic eruption are different than the reactions that form photochemical smog. The term smog encompasses the effect when a large amount of gas phase molecules and particulate matter are emitted to the atmosphere, creating a visible haze. The event causing a large amount of emissions can vary but still result in the formation of smog.

Plants are another natural source of hydrocarbons that could undergo reactions in the atmosphere and produce smog. Globally both plants and soil contribute a substantial amount to the production of hydrocarbons, mainly by producing isoprene and terpenes. Hydrocarbons released by plants can often be more reactive than man-made hydrocarbons. For example when plants release isoprene, the isoprene reacts very quickly in the atmosphere with hydroxyl radicals. These reactions produce hydroperoxides which increase ozone formation.

Smog is a serious problem in many cities and continues to harm human health. Ground-level ozone, sulphur dioxide, nitrogen dioxide and carbon monoxide are especially harmful for senior citizens, children, and people with heart and lung conditions such as emphysema, bronchitis, and asthma. It can inflame breathing passages, decrease the lungs' working capacity, cause shortness of breath, pain when inhaling deeply, wheezing, and coughing. It can cause eye and nose irritation and it dries out the protective membranes of the nose and throat and interferes with the body's ability to fight infection, increasing susceptibility to illness. Hospital admissions and respiratory deaths often increase during periods when ozone levels are high. There is a lack of knowledge on the long-term effects of air pollution exposure and the origin of asthma. An experiment was carried out using intense air pollution similar to that of the 1952 Great Smog of London. The results from this experiment concluded that there is a link between early-life pollution exposure that leads to the development of asthma, proposing the ongoing effect of the Great Smog.
Modern studies continue to find links between mortality and the presence of smog. One study, published in Nature magazine, found that smog episodes in the city of Jinan, a large city in eastern China, during 2011–15, were associated with a 5.87% (95% CI 0.16–11.58%) increase in the rate of overall mortality. This study highlights the effect of exposure to air pollution on the rate of mortality in China.

The U.S. EPA has developed an air quality index to help explain air pollution levels to the general public. 8 hour average ozone concentrations of 85 to 104 ppbv are described as "Unhealthy for Sensitive Groups", 105 ppbv to 124 ppbv as "unhealthy" and 125 ppb to 404 ppb as "very unhealthy". The "very unhealthy" range for some other pollutants are: 355 μg m – 424 μg m for PM10; 15.5 ppm – 30.4ppm for CO and 0.65 ppm – 1.24 ppm for NO.

In 2016, the Ontario Medical Association announced that smog is responsible for an estimated 9,500 premature deaths in the province each year.

A 20-year American Cancer Society study found that cumulative exposure also increases the likelihood of premature death from a respiratory disease, implying the 8-hour standard may be insufficient.

Tiny magnetic particles from air pollution have for the first time been discovered to be lodged in human brains– and researchers think they could be a possible cause of Alzheimer's disease.
Researchers at Lancaster University found abundant magnetite nanoparticles in the brain tissue from 37 individuals aged three to 92-years-old who lived in Mexico City and Manchester. This strongly magnetic mineral is toxic and has been implicated in the production of reactive oxygen species (free radicals) in the human brain, which are associated with neurodegenerative diseases including Alzheimer's disease.

A study examining 806 women who had babies with birth defects between 1997 and 2006, and 849 women who had healthy babies, found that smog in the San Joaquin Valley area of California was linked to two types of neural tube defects: spina bifida (a condition involving, among other manifestations, certain malformations of the spinal column), and anencephaly (the underdevelopment or absence of part or all of the brain, which if not fatal usually results in profound impairment).

According to a study published in The Lancet, even a very small (5 μg) change in PM2.5 exposure was associated with an increase (18%) in risk of a low birth weight at delivery, and this relationship held even below the current accepted safe levels.

Smog can form in almost any climate where industries or cities release large amounts of air pollution, such as smoke or gases. However, it is worse during periods of warmer, sunnier weather when the upper air is warm enough to inhibit vertical circulation. It is especially prevalent in geologic basins encircled by hills or mountains. It often stays for an extended period of time over densely populated cities or urban areas, and can build up to dangerous levels.

According to the Canadian Science Smog Assessment published in 2012, smog is responsible for detrimental effects on human and ecosystem health, as well as socioeconomic well-being across the country. It was estimated that the province of Ontario sustains $201 million in damages annually for selected crops, and an estimated tourism revenue degradation of $7.5 million in Vancouver and $1.32 million in The Fraser Valley due to decreased visibility. Air pollution in British Columbia is of particular concern, especially in the Fraser Valley, because of a meteorological effect called inversion which decreases air dispersion and leads to smog concentration.

For the past few years, cities in northern India have been covered in a thick layer of winter smog. The situation has turned quite drastic in the National Capital, Delhi. This smog is caused by the collection of Particulate Matter (a very fine type of dust and toxic gases) in the air due to stagnant movement of air during winters.

Delhi is the most polluted city in the world and according to one estimate, air pollution causes the death of about 10,500 people in Delhi every year. During 2013–14, peak levels of fine particulate matter (PM) in Delhi increased by about 44%, primarily due to high vehicular and industrial emissions, construction work and crop burning in adjoining states. Delhi has the highest level of the airborne particulate matter, PM2.5 considered most harmful to health, with 153 micrograms. Rising air pollution level has significantly increased lung-related ailments (especially asthma and lung cancer) among Delhi's children and women. The dense smog in Delhi during winter season results in major air and rail traffic disruptions every year. According to Indian meteorologists, the average maximum temperature in Delhi during winters has declined notably since 1998 due to rising air pollution.
Environmentalists have criticised the Delhi government for not doing enough to curb air pollution and to inform people about air quality issues. Most of Delhi's residents are unaware of alarming levels of air pollution in the city and the health risks associated with it. Since the mid-1990s, Delhi has undertaken some measures to curb air pollution – Delhi has the third highest quantity of trees among Indian cities and the Delhi Transport Corporation operates the world's largest fleet of environmentally friendly compressed natural gas (CNG) buses. In 1996, the Centre for Science and Environment (CSE) started a public interest litigation in the Supreme Court of India that ordered the conversion of Delhi's fleet of buses and taxis to run on CNG and banned the use of leaded petrol in 1998. In 2003, Delhi won the United States Department of Energy's first 'Clean Cities International Partner of the Year' award for its "bold efforts to curb air pollution and support alternative fuel initiatives". The Delhi Metro has also been credited for significantly reducing air pollutants in the city.

However, according to several authors, most of these gains have been lost, especially due to stubble burning, rise in market share of diesel cars and a considerable decline in bus ridership. According to CUE and System of Air Quality Weather Forecasting and Research (SAFER), burning of agricultural waste in nearby Punjab, Haryana and Uttar Pradesh regions results in severe intensification of smog over Delhi. The state government of adjoining Uttar Pradesh is considering imposing a ban on crop burning to reduce pollution in Delhi NCR and an environmental panel has appealed to India's Supreme Court to impose a 30% cess on diesel cars.

Joint research between American and Chinese researchers in 2006 concluded that much of the city's pollution comes from surrounding cities and provinces. On average 35–60% of the ozone can be traced to sources outside the city. Shandong Province and Tianjin Municipality have a "significant influence on Beijing's air quality", partly due to the prevailing south/southeasterly flow during the summer and the mountains to the north and northwest.

In 1306, concerns over air pollution were sufficient for Edward I to (briefly) ban coal fires in London. In 1661, John Evelyn's "Fumifugium" suggested burning fragrant wood instead of mineral coal, which he believed would reduce coughing. The "" the same year describes how the smoke "does our lungs and spirits choke, Our hanging spoil, and rust our iron."

Severe episodes of smog continued in the 19th and 20th centuries, mainly in the winter, and were nicknamed "pea-soupers," from the phrase "as thick as pea soup". The Great Smog of 1952 darkened the streets of London and killed approximately 4,000 people in the short time of four days (a further 8,000 died from its effects in the following weeks and months). Initially a flu epidemic was blamed for the loss of life.

In 1956 the Clean Air Act started legally enforcing smokeless zones in the capital. There were areas where no soft coal was allowed to be burned in homes or in businesses, only coke, which produces no smoke. Because of the smokeless zones, reduced levels of sooty particulates eliminated the intense and persistent London smog.

It was after this that the great clean-up of London began. One by one, historical buildings which, during the previous two centuries had gradually completely blackened externally, had their stone facades cleaned and restored to their original appearance. Victorian buildings whose appearance changed dramatically after cleaning included the British Museum of Natural History. A more recent example was the Palace of Westminster, which was cleaned in the 1980s. A notable exception to the restoration trend was 10 Downing Street, whose bricks upon cleaning in the late 1950s proved to be naturally "yellow"; the smog-derived black colour of the façade was considered so iconic that the bricks were painted black to preserve the image. Smog caused by traffic pollution, however, does still occur in modern London.

Other areas of the United Kingdom were affected by smog, especially heavily industrialised areas.

The cities of Glasgow and Edinburgh, in Scotland, suffered smoke-laden fogs in 1909. Des Voeux, commonly credited with creating the "smog" moniker, presented a paper in 1911 to the Manchester Conference of the Smoke Abatement League of Great Britain about the fogs and resulting deaths.

One Birmingham resident described near black-out conditions in the 1900s before the Clean Air Act, with visibility so poor that cyclists had to dismount and walk in order to stay on the road.

On 29 April 2015, the UK Supreme Court ruled that the government must take immediate action to cut air pollution, following a case brought by environmental lawyers at ClientEarth.

Due to its location in a highland "bowl", cold air sinks down onto the urban area of Mexico City, trapping industrial and vehicle pollution underneath, and turning it into the most infamously smog-plagued city of Latin America. Within one generation, the city has changed from being known for some of the cleanest air of the world into one with some of the worst pollution, with pollutants like nitrogen dioxide being double or even triple international standards.
Similar to Mexico City, the air pollution of Santiago valley, located between the Andes and the Chilean Coast Range, turn it into the most infamously smog-plagued city of South America. Other aggravates of the situation reside in its high latitude (31 degrees South) and dry weather during most of the year.

In December 2005, schools and public offices had to close in Tehran and 1600 people were taken to hospital, in a severe smog blamed largely on unfiltered car exhaust.

Smog was brought to the attention of the general U.S. public in 1933 with the publication of the book "Stop That Smoke", by Henry Obermeyer, a New York public utility official, in which he pointed out the effect on human life and even the destruction of of a farmer's spinach crop. Since then, the United States Environmental Protection Agency has designated over 300 U.S. counties to be non-attainment areas for one or more pollutants tracked as part of the National Ambient Air Quality Standards. These areas are largely clustered around large metropolitan areas, with the largest contiguous non-attainment zones in California and the Northeast. Various U.S. and Canadian government agencies collaborate to produce real-time air quality maps and forecasts. To combat smog conditions, localities may declare "smog alert" days, such as in the Spare the Air program in the San Francisco Bay Area.

In the United States, smog pollution kills 24,000 Americans every year. The U.S. is among the dirtier countries in terms of smog, ranked 123 out of 195 countries measured, where 1 is cleanest and 195 is most smog polluted.

Because of their locations in low basins surrounded by mountains, Los Angeles and the San Joaquin Valley are notorious for their smog. The over-reliance on vehicles for transportation in these regions, combined with the additional effects of the San Francisco Bay and Los Angeles/Long Beach port complexes, frequently contribute to further air pollution.

Los Angeles in particular is strongly predisposed to accumulation of smog, because of peculiarities of its geography and weather patterns. Los Angeles is situated in a flat basin with ocean on one side and mountain ranges on three sides. A nearby cold ocean current depresses surface air temperatures in the area, resulting in an inversion layer: a phenomenon where air temperature increases, instead of decreasing, with altitude, suppressing thermals and restricting vertical convection. All taken together, this results in a relatively thin, enclosed layer of air above the city that cannot easily escape out of the basin and tends to accumulate pollution.

Los Angeles was one of the best known cities suffering from transportation smog for much of the 20th century, so much so that it was sometimes said that "Los Angeles" was a synonym for "smog." In 1970, when the Clean Air Act was passed, Los Angeles was the most polluted basin in the country, and California was unable to create a State Implementation Plan that would enable it to meet the new air quality standards. However, ensuing strict regulations by state and federal government agencies overseeing this problem (such as the California Air Resources Board and the United States Environmental Protection Agency), including tight restrictions on allowed emissions levels for all new cars sold in California and mandatory regular emission tests of older vehicles, resulted in significant improvements in air quality. For example, air concentrations of volatile organic compounds declined by a factor of 50 between 1962 and 2012. Concentrations of air pollutants such as nitrous oxides and ozone declined by 70% to 80% over the same period of time.


In the late 1990s, massive immigration to Ulaanbaatar from the countryside began. An estimated 150,000 households, mainly living in traditional Mongolian gers on the outskirts of Ulaanbaatar, burn wood and coal (some poor families burn even car tires and trash) to heat themselves during the harsh winter, which lasts from October to April, since these outskirts are not connected to the city's central heating system. A temporary solution to decrease smog was proposed in the form of stoves with improved efficiency, although with no visible results. 
Coal-fired ger stoves release high levels of ash and other particulate matter (PM). When inhaled, these particles can settle in the lungs and respiratory tract and cause health problems. At two to 10 times above Mongolian and international air quality standards, Ulaanbaatar's PM rates are among the worst in the world, according to a December 2009 World Bank report. The Asian Development Bank (ADB) estimates that health costs related to this air pollution account for as much as 4 percent of Mongolia's GDP.

Smog is a regular problem in Southeast Asia caused by land and forest fires in Indonesia, especially Sumatra and Kalimantan, although the term haze is preferred in describing the problem. Farmers and plantation owners are usually responsible for the fires, which they use to clear tracts of land for further plantings. Those fires mainly affect Brunei, Indonesia, Philippines, Malaysia, Singapore and Thailand, and occasionally Guam and Saipan. The economic losses of the fires in 1997 have been estimated at more than US$9 billion. This includes damages in agriculture production, destruction of forest lands, health, transportation, tourism, and other economic endeavours. Not included are social, environmental, and psychological problems and long-term health effects. The second-latest bout of haze to occur in Malaysia, Singapore and the Malacca Straits is in October 2006, and was caused by smoke from fires in Indonesia being blown across the Straits of Malacca by south-westerly winds. A similar haze has occurred in June 2013, with the PSI setting a new record in Singapore on June 21 at 12pm with a reading of 401, which is in the "Hazardous" range.

The Association of Southeast Asian Nations (ASEAN) reacted. In 2002, the Agreement on Transboundary Haze Pollution was signed between all ASEAN nations. ASEAN formed a Regional Haze Action Plan (RHAP) and established a co-ordination and support unit (CSU). RHAP, with the help of Canada, established a monitoring and warning system for forest/vegetation fires and implemented a Fire Danger Rating System (FDRS). The Malaysian Meteorological Department (MMD) has issued a daily rating of fire danger since September 2003. Indonesia has been ineffective at enforcing legal policies on errant farmers.

Since start of winter season heavy smog loaded with pollutants covered major part of Punjab especially the city of Lahore, causing breathing problems and disrupting normal traffic.

Doctors advised residents to stay indoors and wear facemasks outside.

The severity of smog is often measured using automated optical instruments such as Nephelometers, as haze is associated with visibility and traffic control in ports. Haze however can also be an indication of poor air quality though this is often better reflected using accurate purpose built air indexes such as the American Air Quality Index, the Malaysian API (Air Pollution Index) and the Singaporean Pollutant Standards Index.

In hazy conditions, it is likely that the index will report the suspended particulate level. The disclosure of the responsible pollutant is mandated in some jurisdictions.

The Malaysian API does not have a capped value; hence its most hazardous readings can go above 500. Above 500, a state of emergency is declared in the affected area. Usually, this means that non-essential government services are suspended, and all ports in the affected area are closed. There may also be prohibitions on private sector commercial and industrial activities in the affected area excluding the food sector. So far, state of emergency rulings due to hazardous API levels were applied to the Malaysian towns of Port Klang, Kuala Selangor and the state of Sarawak during the 2005 Malaysian haze and the 1997 Southeast Asian haze.







</doc>
<doc id="27001" url="https://en.wikipedia.org/wiki?curid=27001" title="Smoke">
Smoke

Smoke is a collection of airborne particulates and gases emitted when a material undergoes combustion or pyrolysis, together with the quantity of air that is entrained or otherwise mixed into the mass. It is commonly an unwanted by-product of fires (including stoves, candles, internal combustion engines, oil lamps, and fireplaces), but may also be used for pest control (fumigation), communication (smoke signals), defensive and offensive capabilities in the military (smoke screen), cooking, or smoking (tobacco, cannabis, etc.). It is used in rituals where incense, sage, or resin is burned to produce a smell for spiritual or magical purposes. It can also be a flavoring agent and preservative for various foodstuffs.

Smoke inhalation is the primary cause of death in victims of indoor fires. The smoke kills by a combination of thermal damage, poisoning and pulmonary irritation caused by carbon monoxide, hydrogen cyanide and other combustion products.

Smoke is an aerosol (or mist) of solid particles and liquid droplets that are close to the ideal range of sizes for Mie scattering of visible light.

The composition of smoke depends on the nature of the burning fuel and the conditions of combustion. Fires with high availability of oxygen burn at a high temperature and with a small amount of smoke produced; the particles are mostly composed of ash, or with large temperature differences, of condensed aerosol of water. High temperature also leads to production of nitrogen oxides. Sulfur content yields sulfur dioxide, or in case of incomplete combustion, hydrogen sulfide. Carbon and hydrogen are almost completely oxidized to carbon dioxide and water. Fires burning with lack of oxygen produce a significantly wider palette of compounds, many of them toxic. Partial oxidation of carbon produces carbon monoxide, while nitrogen-containing materials can yield hydrogen cyanide, ammonia, and nitrogen oxides. Hydrogen gas can be produced instead of water. Contents of halogens such as chlorine (e.g. in polyvinyl chloride or brominated flame retardants) may lead to the production of hydrogen chloride, phosgene, dioxin, and chloromethane, bromomethane and other halocarbons. Hydrogen fluoride can be formed from fluorocarbons, whether fluoropolymers subjected to fire or halocarbon fire suppression agents. Phosphorus and antimony oxides and their reaction products can be formed from some fire retardant additives, increasing smoke toxicity and corrosivity. Pyrolysis of polychlorinated biphenyls (PCB), e.g. from burning older transformer oil, and to lower degree also of other chlorine-containing materials, can produce 2,3,7,8-tetrachlorodibenzodioxin, a potent carcinogen, and other polychlorinated dibenzodioxins. Pyrolysis of fluoropolymers, e.g. teflon, in presence of oxygen yields carbonyl fluoride (which hydrolyzes readily to HF and CO); other compounds may be formed as well, e.g. carbon tetrafluoride, hexafluoropropylene, and highly toxic perfluoroisobutene (PFIB).

Pyrolysis of burning material, especially incomplete combustion or smoldering without adequate oxygen supply, also results in production of a large amount of hydrocarbons, both aliphatic (methane, ethane, ethylene, acetylene) and aromatic (benzene and its derivates, polycyclic aromatic hydrocarbons; e.g. benzo[a]pyrene, studied as a carcinogen, or retene), terpenes. Heterocyclic compounds may be also present. Heavier hydrocarbons may condense as tar; smoke with significant tar content is yellow to brown. Presence of such smoke, soot, and/or brown oily deposits during a fire indicates a possible hazardous situation, as the atmosphere may be saturated with combustible pyrolysis products with concentration above the upper flammability limit, and sudden inrush of air can cause flashover or backdraft.

Presence of sulfur can lead to formation of e.g. hydrogen sulfide, carbonyl sulfide, sulfur dioxide, carbon disulfide, and thiols; especially thiols tend to get adsorbed on surfaces and produce a lingering odor even long after the fire. Partial oxidation of the released hydrocarbons yields in a wide palette of other compounds: aldehydes (e.g. formaldehyde, acrolein, and furfural), ketones, alcohols (often aromatic, e.g. phenol, guaiacol, syringol, catechol, and cresols), carboxylic acids (formic acid, acetic acid, etc.).

The visible particulate matter in such smokes is most commonly composed of carbon (soot). Other particulates may be composed of drops of condensed tar, or solid particles of ash. The presence of metals in the fuel yields particles of metal oxides. Particles of inorganic salts may also be formed, e.g. ammonium sulfate, ammonium nitrate, or sodium chloride. Inorganic salts present on the surface of the soot particles may make them hydrophilic. Many organic compounds, typically the aromatic hydrocarbons, may be also adsorbed on the surface of the solid particles. Metal oxides can be present when metal-containing fuels are burned, e.g. solid rocket fuels containing aluminium. Depleted uranium projectiles after impacting the target ignite, producing particles of uranium oxides. Magnetic particles, spherules of magnetite-like ferrous ferric oxide, are present in coal smoke; their increase in deposits after 1860 marks the beginning of the Industrial Revolution. (Magnetic iron oxide nanoparticles can be also produced in the smoke from meteorites burning in the atmosphere.) Magnetic remanence, recorded in the iron oxide particles, indicates the strength of Earth's magnetic field when they were cooled beyond their Curie temperature; this can be used to distinguish magnetic particles of terrestrial and meteoric origin. Fly ash is composed mainly of silica and calcium oxide. Cenospheres are present in smoke from liquid hydrocarbon fuels. Minute metal particles produced by abrasion can be present in engine smokes. Amorphous silica particles are present in smokes from burning silicones; small proportion of silicon nitride particles can be formed in fires with insufficient oxygen. The silica particles have about 10 nm size, clumped to 70-100 nm aggregates and further agglomerated to chains. Radioactive particles may be present due to traces of uranium, thorium, or other radionuclides in the fuel; hot particles can be present in case of fires during nuclear accidents (e.g. Chernobyl disaster) or nuclear war.

Smoke particulates, like other aerosols, are categorized into three modes based on particle size:
Most of the smoke material is primarily in coarse particles. Those undergo rapid dry precipitation, and the smoke damage in more distant areas outside of the room where the fire occurs is therefore primarily mediated by the smaller particles.

Aerosol of particles beyond visible size is an early indicator of materials in a preignition stage of a fire.

Burning of hydrogen-rich fuel produces water; this results in smoke containing droplets of water vapor. In absence of other color sources (nitrogen oxides, particulates...), such smoke is white and cloud-like.

Smoke emissions may contain characteristic trace elements. Vanadium is present in emissions from oil fired power plants and refineries; oil plants also emit some nickel. Coal combustion produces emissions containing aluminium, arsenic, chromium, cobalt, copper, iron, mercury, selenium, and uranium.

Traces of vanadium in high-temperature combustion products form droplets of molten vanadates. These attack the passivation layers on metals and cause high temperature corrosion, which is a concern especially for internal combustion engines. Molten sulfate and lead particulates also have such effect.

Some components of smoke are characteristic of the combustion source. Guaiacol and its derivatives are products of pyrolysis of lignin and are characteristic of wood smoke; other markers are syringol and derivates, and other methoxy phenols. Retene, a product of pyrolysis of conifer trees, is an indicator of forest fires. Levoglucosan is a pyrolysis product of cellulose. Hardwood vs softwood smokes differ in the ratio of guaiacols/syringols. Markers for vehicle exhaust include polycyclic aromatic hydrocarbons, hopanes, steranes, and specific nitroarenes (e.g. 1-nitropyrene). The ratio of hopanes and steranes to elemental carbon can be used to distinguish between emissions of gasoline and diesel engines.

Many compounds can be associated with particulates; whether by being adsorbed on their surfaces, or by being dissolved in liquid droplets. Hydrogen chloride is well absorbed in the soot particles.

Inert particulate matter can be disturbed and entrained into the smoke. Of particular concern are particles of asbestos.

Deposited hot particles of radioactive fallout and bioaccumulated radioisotopes can be reintroduced into the atmosphere by wildfires and forest fires; this is a concern in e.g. the Zone of alienation containing contaminants from the Chernobyl disaster.

Polymers are a significant source of smoke. Aromatic side groups, e.g. in polystyrene, enhance generation of smoke. Aromatic groups integrated in the polymer backbone produce less smoke, likely due to significant charring. Aliphatic polymers tend to generate the least smoke, and are non-self-extinguishing. However presence of additives can significantly increase smoke formation. Phosphorus-based and halogen-based flame retardants decrease production of smoke. Higher degree of cross-linking between the polymer chains has such effect too.

The naked eye detects particle sizes greater than 7 µm (micrometres). Visible particles emitted from a fire are referred to as smoke. Invisible particles are generally referred to as gas or fumes. This is best illustrated when toasting bread in a toaster. As the bread heats up, the products of combustion increase in size. The fumes initially produced are invisible but become visible if the toast is burnt.

An ionization chamber type smoke detector is technically a product of combustion detector, not a smoke detector. Ionization chamber type smoke detectors detect particles of combustion that are invisible to the naked eye. This explains why they may frequently false alarm from the fumes emitted from the red-hot heating elements of a toaster, before the presence of visible smoke, yet they may fail to activate in the early, low-heat smoldering stage of a fire.

Smoke from a typical house fire contains hundreds of different chemicals and fumes. As a result, the damage caused by the smoke can often exceed that caused by the actual heat of the fire. In addition to the physical damage caused by the smoke of a fire – which manifests itself in the form of stains – is the often even harder to eliminate problem of a smoky odor. Just as there are contractors that specialize in rebuilding/repairing homes that have been damaged by fire and smoke, fabric restoration companies specialize in restoring fabrics that have been damaged in a fire.

Smoke from oxygen-deprived fires contains a significant concentration of compounds that are flammable. A cloud of smoke, in contact with atmospheric oxygen, therefore has the potential of being ignited – either by another open flame in the area, or by its own temperature. This leads to effects like backdraft and flashover. Smoke inhalation is also a danger of smoke that can cause serious injury and death.

Many compounds of smoke from fires are highly toxic and/or irritating. The most dangerous is carbon monoxide leading to carbon monoxide poisoning, sometimes with the additive effects of hydrogen cyanide and phosgene. Smoke inhalation can therefore quickly lead to incapacitation and loss of consciousness. Sulfur oxides, hydrogen chloride and hydrogen fluoride in contact with moisture form sulfuric, hydrochloric and hydrofluoric acid, which are corrosive to both lungs and materials. When asleep the nose does not sense smoke nor does the brain, but the body will wake up if the lungs become enveloped in smoke and the brain will be stimulated and the person will be awoken. This does not work if the person is incapacitated or under the influence of drugs and/or alcohol.

Cigarette smoke is a major modifiable risk factor for lung disease, heart disease, and many cancers. Smoke can also be a component of ambient air pollution due to the burning of coal in power plants, forest fires or other sources, although the concentration of pollutants in ambient air is typically much less than that in cigarette smoke. One day of exposure to PM2.5 at a concentration of 880 μg/m3, such as occurs in Beijing, China, is the equivalent of smoking one or two cigarettes in terms of particulate inhalation by weight. The analysis is complicated, however, by the fact that the organic compounds present in various ambient particulates may have a higher carcinogenicity than the compounds in cigarette smoke particulates. Secondhand tobacco smoke is the combination of both sidestream and mainstream smoke emissions from a burning tobacco product. These emissions contain more than 50 carcinogenic chemicals. According to the Surgeon General's 2006 report on the subject, "Short exposures to secondhand [tobacco] smoke can cause blood platelets to become stickier, damage the lining of blood vessels, decrease coronary flow velocity reserves, and reduce heart variability, potentially increasing the risk of a heart attack". The American Cancer Society lists "heart disease, lung infections, increased asthma attacks, middle ear infections, and low birth weight" as ramifications of smoker's emission.
Smoke can obscure visibility, impeding occupant exiting from fire areas. In fact, the poor visibility due to the smoke that was in the Worcester Cold Storage Warehouse fire in Worcester, Massachusetts was the reason why the trapped rescue firefighters couldn't evacuate the building in time. Because of the striking similarity that each floor shared, the dense smoke caused the firefighters to become disoriented.

Smoke contains a wide variety of chemicals, many of them aggressive in nature. Examples are hydrochloric acid and hydrobromic acid, produced from halogen-containing plastics and fire retardants, hydrofluoric acid released by pyrolysis of fluorocarbon fire suppression agents, sulfuric acid from burning of sulfur-containing materials, nitric acid from high-temperature fires where nitrous oxide gets formed, phosphoric acid and antimony compounds from P and Sb based fire retardants, and many others. Such corrosion is not significant for structural materials, but delicate structures, especially microelectronics, are strongly affected. Corrosion of circuit board traces, penetration of aggressive chemicals through the casings of parts, and other effects can cause an immediate or gradual deterioration of parameters or even premature (and often delayed, as the corrosion can progress over long time) failure of equipment subjected to smoke. Many smoke components are also electrically conductive; deposition of a conductive layer on the circuits can cause crosstalks and other deteriorations of the operating parameters or even cause short circuits and total failures. Electrical contacts can be affected by corrosion of surfaces, and by deposition of soot and other conductive particles or nonconductive layers on or across the contacts. Deposited particles may adversely affect the performance of optoelectronics by absorbing or scattering the light beams.

Corrosivity of smoke produced by materials is characterized by the corrosion index (CI), defined as material loss rate (angstrom/minute) per amount of material gasified products (grams) per volume of air (m). It is measured by exposing strips of metal to flow of combustion products in a test tunnel. Polymers containing halogen and hydrogen (polyvinyl chloride, polyolefins with halogenated additives, etc.) have the highest CI as the corrosive acids are formed directly with water produced by the combustion, polymers containing halogen only (e.g. polytetrafluoroethylene) have lower CI as the formation of acid is limited to reactions with airborne humidity, and halogen-free materials (polyolefins, wood) have the lowest CI. However, some halogen-free materials can also release significant amount of corrosive products.

Smoke damage to electronic equipment can be significantly more extensive than the fire itself. Cable fires are of special concern; low smoke zero halogen materials are preferable for cable insulation.

When smoke comes into contact with the surface of any substance or structure, the chemicals contained in it are transferred to it. The corrosive properties of the chemicals cause the substance or structure to decompose at a rapid rate. Certain materials or structures absorb these chemicals, which is why clothing, unsealed surfaces, potable water, piping, wood, etc., are replaced in most cases of structural fires.

As early as the 15th century Leonardo da Vinci commented at length on the difficulty of assessing smoke, and distinguished between black smoke (carbonized particles) and white 'smoke' which is not a smoke at all but merely a suspension of harmless water particulates.

Smoke from heating appliances is commonly measured in one of the following ways:

In-line capture. A smoke sample is simply sucked through a filter which is weighed before and after the test and the mass of smoke found. This is the simplest and probably the most accurate method, but can only be used where the smoke concentration is slight, as the filter can quickly become blocked.

The ASTM smoke pump is a simple and widely used method of in-line capture where a measured volume of smoke is pulled through a filter paper and the dark spot so formed is compared with a standard.

Filter/dilution tunnel. A smoke sample is drawn through a tube where it is diluted with air, the resulting smoke/air mixture is then pulled through a filter and weighed. This is the internationally recognized method of measuring smoke from combustion.

Electrostatic precipitation. The smoke is passed through an array of metal tubes which contain suspended wires. A (huge) electrical potential is applied across the tubes and wires so that the smoke particles become charged and are attracted to the sides of the tubes. This method can over-read by capturing harmless condensates, or under-read due to the insulating effect of the smoke. However, it is the necessary method for assessing volumes of smoke too great to be forced through a filter, i.e., from bituminous coal.

Ringelmann scale. A measure of smoke color. Invented by Professor Maximilian Ringelmann in Paris in 1888, it is essentially a card with squares of black, white and shades of gray which is held up and the comparative grayness of the smoke judged. Highly dependent on light conditions and the skill of the observer it allocates a grayness number from 0 (white) to 5 (black) which has only a passing relationship to the actual quantity of smoke. Nonetheless, the simplicity of the Ringelmann scale means that it has been adopted as a standard in many countries.

Optical scattering. A light beam is passed through the smoke. A light detector is situated at an angle to the light source, typically at 90°, so that it receives only light reflected from passing particles. A measurement is made of the light received which will be higher as the concentration of smoke particles becomes higher.

Optical obscuration. A light beam is passed through the smoke and a detector opposite measures the light. The more smoke particles are present between the two, the less light will be measured.

Combined optical methods. There are various proprietary optical smoke measurement devices such as the 'nephelometer' or the 'aethalometer' which use several different optical methods, including more than one wavelength of light, inside a single instrument and apply an algorithm to give a good estimate of smoke. It has been claimed that these devices can differentiate types of smoke and so their probable source can be inferred, though this is disputed.

Inference from carbon monoxide. Smoke is incompletely burned fuel, carbon monoxide is incompletely burned carbon, therefore it has long been assumed that measurement of CO in flue gas (a cheap, simple and very accurate procedure) will provide a good indication of the levels of smoke. Indeed, several jurisdictions use CO measurement as the basis of smoke control. However it is far from clear how accurate the correspondence is.

Throughout recorded history, humans have used the smoke of medicinal plants to cure illness. A sculpture from Persepolis shows Darius the Great (522–486 BC), the king of Persia, with two censers in front of him for burning Peganum harmala and/or sandalwood Santalum album, which was believed to protect the king from evil and disease. More than 300 plant species in 5 continents are used in smoke form for different diseases. As a method of drug administration, smoking is important as it is a simple, inexpensive, but very effective method of extracting particles containing active agents. More importantly, generating smoke reduces the particle size to a microscopic scale thereby increasing the absorption of its active chemical principles.



</doc>
<doc id="27002" url="https://en.wikipedia.org/wiki?curid=27002" title="Tobacco pipe">
Tobacco pipe

A tobacco pipe, often called simply a pipe, is a device specifically made to smoke tobacco. It comprises a chamber (the bowl) for the tobacco from which a thin hollow stem (shank) emerges, ending in a mouthpiece (the bit). Pipes can range from very simple machine-made briar models to highly prized hand-made artisanal implements made by renowned pipemakers, which are often very expensive collector's items. Pipe smoking is the oldest known traditional form of tobacco smoking.

Some Native American cultures smoke tobacco in ceremonial pipes, and have done so since long before the arrival of Europeans. For instance the Lakota people use a ceremonial pipe called čhaŋnúŋpa. Other American Indian cultures smoke tobacco socially. The tobacco plant is native to South America but spread into North America long before Europeans arrived. Tobacco was introduced to Europe from the Americas in the 16th century and spread around the world rapidly.

As tobacco was not introduced to the Old World until the 16th century, the older pipes outside of the Americas were usually used to smoke various other substances, including hashish, a rare and expensive substance outside areas of the Middle East, Central Asia and India, where it was then produced. 

A pipe's fundamental function is to provide a relatively safe, manipulable volume in which to incompletely combust a smokable substance. Typically this is accomplished by connecting a refractory 'bowl' to some sort of 'stem' which extends and may also cool the smoke mixture drawn through the combusting organic mass (see below).

The broad anatomy of a pipe typically comprises mainly the bowl and the stem. The "bowl" (1) which is the cup-like outer shell, the part hand-held while packing, holding and smoking a pipe, is also the part "knocked" top-down to loosen and release impacted spent tobacco. On being sucked, the general stem delivers the smoke from the bowl to the user's mouth.

Inside the bowl is an inner "chamber" (2) space holding tobacco pressed into it. This "draught hole" (3), is for air flow where air has travelled through the tobacco in the chamber, taking the smoke with it, up the "shank" (4). At the end of the shank, the pipe's "mortise" (5) and "tenon" (6) join is an air-tight, simple connection of two detachable parts where the mortise is a hole met by the tenon, a tight-fitting "tongue" at the start of the "stem" (7). Known as the "bore" (10), the inner shaft of this second section stays uniform throughout while the outer stem tapers down to the mouthpiece or "bit" (8) held in the smoker's teeth, and finally ends in the "lip" (9), attenuated for comfort.

The bowls of tobacco pipes are commonly made of briar wood, meerschaum, corncob, pear-wood, rose-wood or clay. Less common are other dense-grained woods such as cherry, olive, maple, mesquite, oak, and bog-wood. Minerals such as catlinite and soapstone have also been used. Pipe bowls are sometimes decorated by carving, and moulded clay pipes often had simple decoration in the mould.

Unusual, but still noteworthy pipe materials include gourds, as in the famous calabash pipe, and pyrolytic graphite. Metal and glass are uncommon materials for tobacco pipes, but are common for pipes intended for other substances, such as cannabis.

The stem needs a long channel of constant position and diameter running through it for a proper draw, although filter pipes have varying diameters and can be successfully smoked even without filters or adapters. Because it is molded rather than carved, clay may make up the entire pipe or just the bowl, but most other materials have stems made separately and detachable. Stems and bits of tobacco pipes are usually made of moldable materials like Ebonite, Lucite, Bakelite, and soft plastic. Less common are stems made of reeds, bamboo, or hollowed out pieces of wood. Expensive pipes once had stems made of amber, though this is rare now.


Calabash gourds (usually with meerschaum or porcelain bowls set inside them) have long made prized pipes, but they are labour-intensive and, today, quite expensive. Because of this expense, pipes with bodies made of wood (usually mahogany) instead of gourd, but with the same classic shape, are sold as calabashes. Both wood and gourd pipes are functionally the same (with the important exception that the dried gourd, usually being noticeably lighter, sits more comfortably in the mouth). They consist of a downward curve that ends with an upcurve where the bowl sits. Beneath the bowl is an air chamber which serves to cool, dry, and mellow the smoke. There are also briar pipes being sold as calabashes. These typically do not have an air chamber and are so named only because of their external shape.

A calabash pipe is rather large and easy to recognize as a pipe when used on a stage in dramatic productions. Although a British newspaper cartoon of the early 1900s depicts the British actor H. A. Saintsbury as the Great Detective smoking what may be a calabash pipe, its now-stereotypical identification with Sherlock Holmes remains a mystery.

Some commentators have erroneously associated the calabash with William Gillette, the first actor to become universally recognized as the embodiment of the detective. Gillette actually introduced the curving or bent pipe for use by Holmes, but his pipe was an ornate briar. Gillette chose a bent pipe, more easily clenched in the teeth when delivering lines.

While there are promotional stills of Basil Rathbone smoking calabash pipes as Holmes for other projects, most notably his radio show, in his first two outings as Holmes produced by 20th Century-Fox as taking place in the Victorian era, Rathbone smoked an apple-bowled, black briar with a half bend, made by Dunhill, the company known for making the best pipes at that time. In the next dozen films, the series produced by Universal Studios, with Holmes and Watson updated to the 1940s, Rathbone smokes a much less expensive Peterson half bend with a billiard-shaped bowl. A calabash is introduced in "The Spider Woman" but Holmes does not smoke it.

In the original chronicles, such as "The Adventure of the Copper Beeches", Sherlock Holmes is described as smoking a long-stemmed cherrywood (but not a churchwarden pipe) which he favored "when in a disputatious, rather than a meditative mood." Holmes smokes an old briar-root pipe on occasion, "The Sign of the Four" for one, and an "unsavory" and "disreputable" black and oily clay pipe in several stories, notably in "The Red-Headed League". Dr Watson declares it to be the detective's preferred pipe: “It was to him as a counsellor” ("A Case of Identity"); the “companion of his deepest meditations" ("The Valley of Fear")..
Bowls are made of varying shapes and materials to allow the smoker to try different characteristics or to dedicate particular bowls for particular tobaccos. Bowls are not interchangeable between manufacturers.

A "hookah", "ghelyan", or "narghile", is a Middle Eastern water pipe that cools the smoke by filtering it through a water chamber. Often ice, cough-drops, milk, or fruit juice is added to the water. Traditionally, the tobacco is mixed with a sweetener, such as honey or molasses. Fruit flavors have also become popular. Modern hookah smokers, especially in the US, smoke "me'assel", "moassel", "molasses" or "shisha", all names for the same wet mixture of tobacco, molasses/honey, glycerine, and often, flavoring. This style of tobacco is smoked in a bowl with foil or a screen (metal or glass) on top of the bowl. More traditional tobaccos are "tombiek" (a dry unflavored tobacco, which the user moistens in water, squeezes out the extra liquid, and places coals directly on top) or "jarak" (more of a paste of tobacco with fruit to flavor the smoke).


The majority of pipes sold today, whether handmade or machine-made, are fashioned from briar (). Briar is a particularly well suited wood for pipe making for a number of reasons. The first and most important characteristic is its natural resistance to fire. The second is its inherent ability to absorb moisture. The burl absorbs water in nature to supply the tree in the dry times and likewise will absorb the moisture that is a byproduct of combustion. Briar is cut from the root burl of the tree heath ("Erica arborea"), which is native to the rocky and sandy soils of the Mediterranean region. Briar burls are cut into two types of blocks; ebauchon and plateaux. Ebauchon is taken from the heart of the burl while plateaux is taken from the outer part of the burl. While both types of blocks can produce pipes of the highest quality, most artisan pipemakers prefer to use plateaux because of their superior graining.

Ceramic pipes, made of moulded and then fired clay, were used almost universally by Europeans between the introduction of tobacco in the 16th century, and the introduction of cheap cigarettes at the end of the nineteenth.

The material is not very strong and the early varieties had long thin stems, so they frequently broke, but were cheap to replace. It has been claimed that this fragility was somewhat intentional as it was utilized by Colonial American tavern keepers, for example, in renting the clay pipes to patrons. When the patron was done smoking the pipe and returned it to the keeper, the end of the stem was simply broken off so as to be ready for the next patron. However, there is no documentary evidence for this practice; it is known that communal pipes used in taverns were cleansed by being heated in an oven on special iron racks.

Forming the pipe involved making them in moulds with the bore created by pushing an oiled wire inside the stem. The preferred material was pipeclay or "tobacco pipe clay", which fires to a white colour and is found in only certain locations. In North America, many clay pipes were historically made from more typical terracotta-coloured clays. According to one British writer in 1869, the French preferred old pipes and the English new, the middle class preferred long stems and the working class preferred short. Short stemmed pipes, sometimes called "cuttys" or "nose warmers" in England, were preferred by those doing manual work as they could be gripped between the teeth, leaving both of the smoker's hands free.

Later low-quality clay pipes were made by slip casting in a mould. Higher quality pipes are made in a labour-intensive hand shaping process. Traditionally, clay pipes are un-glazed. Clays burn "hot" in comparison to other types of pipes, so they are often difficult for most pipe-smokers to use. Their proponents claim that, unlike other materials, a well-made clay pipe gives a "pure" smoke with no flavour addition from the pipe bowl. In addition to aficionados, reproductions of historical clay styles are used by some historical re-enactors. Clay pipes were once very popular in Ireland, where they were called
"dudeen"s.
Broken fragments of clay pipe can be useful as dating evidence for archaeologists. In the 1950s, the American archaeologist J. C. Harrington noted that the bore of pipe stems decreased over time, so a late sixteenth or early seventeenth centuries pipe would have a stem bore diameter of around , but a late eighteenth century pipe would have a bore diameter of around . The size of bowls also increased over time as tobacco became a cheaper commodity, and later pipes tend to be more decorated.

The specifically American style of pipes made from corncobs are cheap and effective, even if some regard them as inelegant. The cobs are first dried for two years. Then they are hollowed out to make a bowl shape. The bowls are dipped in a plaster-based mixture and varnished or lacquered on the outside. Shanks made from birch wood are then inserted into the bowls. The first and largest manufacturer of corncob pipes is Missouri Meerschaum, located in Washington, Missouri, in the United States. Missouri Meerschaum has produced the pipes since 1869. General Douglas MacArthur and Mark Twain were perhaps the most famous smokers of this type of pipe, along with the cartoon characters Popeye and Frosty the Snowman.

Corncob pipes remain popular today because they are inexpensive and require no "break-in" period like briar pipes. For these two reasons, corncob pipes are often recommended as a "beginner's pipe." However, corncob pipes are equally valued by both learners and experienced smokers who simply desire a cool, clean smoke. Pipesmokers who wish to sample a wide variety of different tobaccos and blends also might keep a stock of corncobs on hand to permit them to try new flavors without "carryover" from an already-used pipe, or to keep a potentially bad-tasting tobacco from adding its flavor to a more expensive or favored pipe.

Meerschaum (hydrated magnesium silicate), a mineral found in small shallow deposits mainly around the city of Eskişehir in central Turkey, is prized for the properties which allow it to be carved into finely detailed decorative and figural shapes. It has been used since the 17th century and, with clay pipes, represented the most common medium for pipes before the introduction of briar as the material of choice in the 19th century. The word "meerschaum" means "sea foam" in German, alluding to its natural white color and its surprisingly low weight. Meerschaum is a very porous mineral that absorbs elements of the tobacco during the smoking process, and gradually changes color to a golden brown. Old, well-smoked meerschaum pipes are valued by collectors for their distinctive coloring.

Meerschaum pipes can either be carved from a block of meerschaum, or made from meerschaum dust collected after carving and mixed with a binder then pressed into a pipe shape. The latter are far less absorbent, color in blotches, and lack the smoking quality of the block carved pipe.

A variety of other materials may also be used for pipes. The Redmanol corporation manufactured pipes with translucent stems in the 1920s and a series of pipes were manufactured and distributed by the Tar Gard (later Venturi) Corporation of San Francisco from 1965-1975. Marketed under names such as "the pipe," "THE SMOKE" and "Venturi," they used materials such as pyrolytic graphite, phenolic resin, nylon, Bakelite and other synthetics, allowing for higher temperatures in the bowl, reduced tar, and aesthetic variations of color and style.
After Venturi stopped making pipes, several companies continue to make pipes from Brylon, a composite of nylon and wood flour, as a cheaper substitute for briar.











Used to absorb moisture, tar and nicotine. Made of:

Filters can be single- or double-sided. Double-sided filter has both ends ceramic that can withstand hot smoke. Single-sided filter has ceramic end to the bowl and plastic end to the stem.

Smoking a pipe requires more apparatus and technique than cigarette or even cigar smoking. In addition to the pipe itself and matches or a pipe lighter, smokers usually require a pipe tool for packing, adjusting, and emptying the tobacco in the bowl, and a regular supply of pipe cleaners.

Tobaccos for smoking in pipes are often carefully treated and blended to achieve flavour nuances not available in other tobacco products. Many of these are blends using staple ingredients of variously cured Burley and Virginia tobaccos which are enhanced by spice tobaccos, among them many Oriental or Balkan varietals, Latakia (a fire-cured spice tobacco of Syrian origin), Perique (uniquely grown in St. James Parish, Louisiana) which is also an old method of fermentation, or blends of Virginia and Burley tobaccos of African, Indian, or South American origins. Traditionally, many U.S. blends are made of American Burley with sweeteners and flavorings added to create an "aromatic" flavor, whereas "English" blends are based on natural Virginia tobaccos enhanced with Oriental and other natural tobaccos. There is a growing tendency towards "natural" tobaccos which derive their aromas from artful blending with selected spice tobaccos only and careful, often historically-based, curing processes.

Pipe tobacco can be purchased in several forms, which vary both in flavour (leading to many blends and opportunities for smokers to blend their own tobaccos) and in the physical shape and size to which the tobacco has been reduced. Most pipe tobaccos are less mild than cigarette tobacco, substantially more moist and cut much more coarsely. Too finely cut tobacco does not allow enough air to flow through the pipe, and overly dry tobacco burns too quickly with little flavour. Pipe tobacco must be kept in an airtight container, such as a canning jar or sealed tin, to keep from drying out.

Some pipe tobaccos are cut into long narrow ribbons. Some are pressed into flat plugs which are sliced into flakes. Others are tightly wound into long ropes, then sliced into discs. Plug tobacco is maintained in its pressed block form and sold in small blocks. The plug will be sliced into thin flakes by the smoker and then prepared in a similar fashion to flake tobacco. It is considered that plug tobacco holds its flavor better than rubbed or flake tobacco. Flake tobacco (sliced cakes or ropes) may be prepared in several ways. Generally it is rubbed out with the fingers and palms until it is loose enough to pack. It can also be crumbled or simply folded and stuffed into a pipe. Some people also prefer to dice up very coarse tobaccos before using them, making them easier to pack.

In the most common method of packing, tobacco is added to the bowl of the pipe in several batches, each one pressed down until the mixture has a uniform density that optimizes airflow (something that it is difficult to gauge without practice). This can be done with a finger or thumb, but if the tobacco needs to be repacked later, while it is burning, the tamper on a pipe tool is sometimes used. If it needs to be loosened, the reamer, or any similar long pin can be used. A traditional way of packing the pipe is to fill the bowl and then pack gently to about full, fill again and pack slightly more firmly to about full, and then pack more firmly still to the top.

An alternative packing technique called the Frank method involves lightly dropping tobacco in the pipe, after which a large plug is gingerly pushed into the bowl all at once.

Matches, or separately lit slivers of wood are often considered preferable to lighters because of lower burning temperature. Butane lighters made specifically for pipes 
emit flame sideways or at an angle to make it easier to direct flame into the bowl. Torch-style lighters should never be used to light a pipe because their flames are too hot and can char the rim of the pipe bowl. Matches should be allowed to burn for several seconds to allow the sulfur from the tip to burn away and the match to produce a full flame. A naphtha fueled lighter should also be allowed to burn a few seconds to get rid of stray naphtha vapors that could give a foul taste to the smoke. When a flame has been produced, it is then moved in circles above the rim of the bowl while the smoker puffs to draw the flame down and light the tobacco. Packing method and humidity can affect how often a pipe must be relit.

With care, a briar pipe can last a very long time without burning out. However, due to aggressive (hot) smoking, imperfections in the wood, a hole can be burned in the tobacco chamber of the pipe. There are several methods used to help prevent a wood pipe from burning out. These generally involve coating the chamber with any of a variety of substances, or by gently smoking a new pipe to build up a cake (a mixture of ash, unburned tobacco, oils, sugars, and other residue) on the walls.

These coatings may include honey and water; powdered sugar and water; cigar ash and water; and sour cream, buttermilk, and activated charcoal among many others.

Many modern briar pipes are pre-treated by the manufacturer to resist burning. If smoked correctly, the cake will build up properly on its own. Another technique is to alternate a half-bowl and a full-bowl the first several times the pipe is used to build an even cake. Burley is often recommended to help a new pipe build cake.

The effectiveness of these methods is by no means universally agreed upon.

The caked layer that helps prevent burning through the bottom or sides of a briar wood pipe may damage other pipes, such as meerschaum or clay. As the cake layer heats up, it expands and may cause cracks or breaks in non-briar pipes.

Pipe smoke, like cigar smoke, is usually not inhaled. It is merely brought into the mouth, pumped around oral and nasal cavities to permit absorption of nicotine toward the brain through the mucous membranes, and released. It is normal to have to relight a pipe periodically. If it is smoked too slowly, this will happen more often. If it is smoked too quickly, it can produce excess moisture causing a gurgling sound in the pipe and an uncomfortable sensation on the tongue (referred to as "pipe tongue", or more commonly, "tongue bite").

A pipe cleaner can be used to dry out the bowl and, wetted with alcohol, the inner channel. The bowl of the pipe can also become uncomfortably hot, depending on the material and the rate of smoking. For this reason, clay pipes in particular are often held by the stem. Meerschaum pipes are held in a square of chamois leather, with gloves, or else by the stem in order to prevent uneven coloring of the material.
The ash and the last bits of unburned tobacco, known as dottle, should be cleaned out with a suitable pipe tool. A soft or bristle pipe cleaner, which may be moistened with strong spirits is then run through the airways of the stem and shank to remove any moisture, ash, and other residue before the pipe is allowed to dry. A pipe should be allowed to cool before removing the stem to avoid the possibility of warping it.

A cake of ash eventually develops inside the bowl. This is generally considered desirable for controlling overall heat. However, if it becomes too thick, it may expand faster than the bowl of the pipe itself when heated, cracking the bowl. Before reaching this point, it needs to be scraped down with a reamer. It is generally recommended to keep the cake at approximately the thickness of a U.S. dime (about 1/20th of an inch or 1.5 mm), though sometimes the cake is removed entirely as part of efforts to eliminate flavors or aromas.

Cake is considered undesirable in meerschaum pipes because it can easily crack the bowl and/or interfere with the mineral's natural porosity. Meerschaum also softens when heated so it is recommended to allow meerschaum pipes to cool before cleaning as people have been known to push pipe cleaners through the walls of heated pipes.

Regardless if a pipe is cleaned after every smoke, over time there is a buildup of cake in the bowl and tars in the internals of a smoking pipe. The cake can be controlled by gentle reaming, but a buildup of tars in the shank and airway of a pipe is more difficult to deal with. This may require the services of a professional pipe restorer to properly clean and sanitize the pipe.

When tobacco is burned, oils from adjoining not yet ignited particles vaporize and condense into the existing cake on the walls of the bowl and shank. Over time, these oils can oxidize and turn rancid, causing the pipe to give a sour or bitter smoke. A purported countermeasure involves filling the bowl with kosher salt and carefully wetting it with strong spirits. It is important to not use iodized salt, as the iodine and other additives may impart an unpleasant flavor. Regularly wiping out the bowl with spirits such as vodka or rum is helpful in preventing souring. Commercial pipe-sweetening products are also available.





</doc>
<doc id="27003" url="https://en.wikipedia.org/wiki?curid=27003" title="Swiss cheese">
Swiss cheese

Swiss cheese is a generic name in North America for several related varieties of cheese, mainly of North American manufacture, which resemble Emmental cheese, a yellow, medium-hard cheese that originated in the area around Emmental, in Switzerland. Some types of Swiss cheese have a distinctive appearance, as the blocks of the cheese are riddled with holes known as "eyes". Swiss cheese without eyes is known as "blind".

Three types of bacteria are used in the production of Emmental cheese: "Streptococcus salivarius" subspecies "thermophilus" (also known as "Streptococcus thermophilus"), "Lactobacillus" ("Lactobacillus helveticus" or "Lactobacillus delbrueckii" subspecies "bulgaricus"), and "Propionibacterium" ("Propionibacterium freudenreichii" subspecies "shermani"). In a late stage of cheese production, the propionibacteria consume the lactic acid excreted by the other bacteria and release acetate, propionic acid, and carbon dioxide gas. The carbon dioxide slowly forms the bubbles that develop the "eyes". The acetate and propionic acid give Swiss its nutty and sweet flavor. A hypothesis proposed by Swiss researchers in 2015 notes that particulate matter may also play a role in the holes' development and that modern sanitation eliminated debris such as hay dust in the milk played a role in reduced hole size in Swiss cheeses, or even "blind cheese". Historically, the holes were seen as a sign of imperfection and cheese makers originally tried to avoid them by pressing during production. In modern times, the holes have become an identifier of the cheese.

In general, the larger the eyes in a Swiss cheese, the more pronounced its flavor because a longer fermentation period gives the bacteria more time to act. This poses a problem, however, because cheese with large eyes does not slice well and comes apart in mechanical slicers. As a result, industry regulators have limited the eye size by which Swiss cheese receives the Grade A stamp.

In 2014, 297.8 million pounds of Swiss cheese was reportedly produced in the United States.

Baby Swiss and Lacy Swiss are two varieties of American Swiss cheeses. Both have small holes and a mild flavor. Baby Swiss is made from whole milk, and Lacy Swiss is made from low fat milk. Baby Swiss was developed in the mid-1960s outside of Charm, Ohio, by the Guggisberg Cheese Company, owned by Alfred Guggisberg.


</doc>
<doc id="27004" url="https://en.wikipedia.org/wiki?curid=27004" title="Spontaneous combustion (disambiguation)">
Spontaneous combustion (disambiguation)

Spontaneous combustion is the self-ignition of a mass, for example, a pile of oily rags. Allegedly, humans can also ignite and burn without an obvious cause; this phenomenon is known as spontaneous human combustion.

Spontaneous Combustion is also the name of:


</doc>
<doc id="27005" url="https://en.wikipedia.org/wiki?curid=27005" title="Smoke signal">
Smoke signal

The smoke signal is one of the oldest forms of long-distance communication. It is a form of visual communication used over a long distance. In general smoke signals are used to transmit news, signal danger, or gather people to a common area.

In ancient China, soldiers stationed along the Great Wall would alert each other of impending enemy attack by signaling from tower to tower. In this way, they were able to transmit a message as far away as in just a few hours.

Misuse of the smoke signal is known to have contributed to the fall of the Western Zhou Dynasty in the 8th century BCE. King You of Zhou had a habit of fooling his warlords with false warning beacons in order to amuse Bao Si, his concubine.

Polybius, a Greek historian, devised a more complex system of alphabetical smoke signals around 150 BCE, which converted Greek alphabetic characters into numeric characters. It enabled messages to be easily signaled by holding sets of torches in pairs. This idea, known as the "Polybius square", also lends itself to cryptography and steganography. This cryptographic concept has been used with Japanese Hiragana and the Germans in the later years of the First World War.

North American indigenous peoples also communicated via smoke signal. Each tribe had its own signaling system and understanding. A signaler started a fire on an elevation typically using damp grass, which would cause a column of smoke to rise. The grass would be taken off as it dried and another bundle would be placed on the fire. Reputedly the location of the smoke along the incline conveyed a meaning. If it came from halfway up the hill, this would signify all was well, but from the top of the hill it would signify danger.

Smoke signals remain in use today. The College of Cardinals uses smoke signals to indicate the selection of a new Pope during a papal conclave. Eligible cardinals conduct a secret ballot until someone receives a vote of two-thirds plus one. The ballots are burned after each vote. Black smoke indicates a failed ballot, while white smoke means a new Pope has been elected.

Colored smoke grenades are commonly used by military forces to mark positions, especially during calls for artillery or air support.

Smoke signals may also refer to smoke-producing devices used to send distress signals.

Lewis and Clark's journals cite several occasions when they adopted the Native American method of setting the plains on fire to communicate the presence of their party or their desire to meet with local tribes.

Yámanas of South America used fire to send messages by smoke signals, for instance if a whale drifted ashore. The large amount of meat required notification of many people, so that it would not decay. They might also have used smoke signals on other occasions, thus it is possible that Magellan saw such fires (which inspired him to name the landscape Tierra del Fuego) but he may have seen the smoke or lights of natural phenomena.

The Cape Town Noon Gun, specifically the smoke its firing generates, was used to set marine chronometers in Table Bay.

Aboriginal Australians throughout Australia would send up smoke signals for various purposes. Sometimes to notify others of their presence, particularly when entering lands which were not their own. Sometimes used to describe visiting whites, smoke signals were the fastest way to send messages. Smoke signals were sometimes to notify of incursions by hostile tribes, or to arrange meetings between hunting parties of the same tribe. This signal could be from a fixed lookout on a ridge of from a mobile band of tribesman. "Putting up a smoke" would often result in nearby individuals or groups replying with their own signals. To carry information, the colour of the smoke was varied, sometimes black, white or blue depending on whether the material being burnt was wet grass, dry grass, reeds or other, and the shape of the smoke could be a column, ball or smoke ring. This message could include the names of individual tribesmen. Like other means of communication, signals could be misinterpreted. In one recorded instance, a smoke signal reply translated as "we are coming" was misinterpreted as joining a war party for protection of the tribe when it was actually hunting parties coming together after a successful hunt.

Modern aviation has made skywriting possible.



</doc>
<doc id="27006" url="https://en.wikipedia.org/wiki?curid=27006" title="Serendipity">
Serendipity

Serendipity is the occurrence of an unplanned fortunate discovery. Serendipity is a common occurrence throughout the history of product invention and scientific discovery. Serendipity is also seen as a potential design principle for online activities that would present a wide array of information and viewpoints, rather than just re-enforcing a user's opinion.

The first noted use of "serendipity" in the English language was by Horace Walpole on 28 January 1754. In a letter he wrote to his friend Horace Mann, Walpole explained an unexpected discovery he had made about a lost painting of Bianca Cappello by Giorgio Vasari by reference to a Persian fairy tale, "The Three Princes of Serendip". The princes, he told his correspondent, were "always making discoveries, by accidents and sagacity, of things which they were not in quest of." The name comes from "Serendip", an old name for Sri Lanka (Ceylon), hence "Sarandib" by Arab traders. It is derived from the Sanskrit "Siṃhaladvīpaḥ" (Siṃhalaḥ, Sri Lanka + dvīpaḥ, island).

The word has been exported into many other languages, with the general meaning of “unexpected discovery” or “fortunate chance”.

The term "serendipity" is often applied to inventions made by chance rather than intent. Andrew Smith, editor of "The" "Oxford Companion to American Food and Drink", has speculated that most everyday products had serendipitous roots, with many early ones related to animals. The origin of cheese, for example, possibly originated in the Nomad practice of storing milk in the stomach of a dead camel that was attached to the saddle of a live one, thereby mixing rennet from the stomach with the milk stored within.

Other examples of serendipity in inventions include:

Serendipity contributed to entomologist Shaun Winterton discovering "Semachrysa jade", a new species of lacewing, which he found not in its native Malaysia, but on the photo-sharing site Flickr. Winterton's discovery was aided by Flickr's ability to present images that are personalized to a user's interests, thereby increasing the odds he would chance upon the photo. Computer scientist Jaime Teevan has argued that serendipitous discovery is promoted by such personalization, writing that "people don’t know what to do with random new information. Instead, we want information that is at the fringe of what we already know, because that is when we have the cognitive structures to make sense of the new ideas."

Serendipity is a design principle for online activity that would present viewpoints that diverge from those participants already hold. Harvard Law professor Cass Sunstein argues that such an "architecture of serendipity" would promote a healthier democracy. Like a great city or university, "a well-functioning information market" provides exposure to new ideas, people, and ways of life, "Serendipity is crucial because it expands your horizons. You need that if you want to be free." The idea has potential application in the design of social media, information searches, and web browsing.

William Boyd coined the term zemblanity in the late twentieth century to mean somewhat the opposite of serendipity: "making unhappy, unlucky and expected discoveries occurring by design". A zemblanity is, effectively, an "unpleasant unsurprise". It derives from Novaya Zemlya (or Nova Zembla), a cold, barren land with many features opposite to the lush Sri Lanka (Serendip).

Bahramdipity is derived directly from Bahram Gur as characterized in "The Three Princes of Serendip". It describes the "suppression" of serendipitous discoveries or research results by powerful individuals.





</doc>
<doc id="27007" url="https://en.wikipedia.org/wiki?curid=27007" title="Samuel Morse">
Samuel Morse

Samuel Finley Breese Morse (April 27, 1791 – April 2, 1872) was an American painter and inventor. After having established his reputation as a portrait painter, in his middle age Morse contributed to the invention of a single-wire telegraph system based on European telegraphs. He was a co-developer of Morse code and helped to develop the commercial use of telegraphy.

Samuel F. B. Morse was born in Charlestown, Massachusetts, the first child of the pastor Jedidiah Morse (1761–1826), who was also a geographer, and his wife Elizabeth Ann Finley Breese (1766–1828). His father was a great preacher of the Calvinist faith and supporter of the American Federalist party. He thought it helped preserve Puritan traditions (strict observance of Sabbath, among other things), and believed in the Federalist support of an alliance with Britain and a strong central government. Morse strongly believed in education within a Federalist framework, alongside the instillation of Calvinist virtues, morals, and prayers for his first son. His first ancestor in America was Samuel Morse, who emigrated to Dedham, Massachusetts in 1635.

After attending Phillips Academy in Andover, Massachusetts, Samuel Morse went on to Yale College to receive instruction in the subjects of religious philosophy, mathematics, and science of horses. While at Yale, he attended lectures on electricity from Benjamin Silliman and Jeremiah Day and was a member of the Society of Brothers in Unity. He supported himself by painting. In 1810, he graduated from Yale with Phi Beta Kappa honors.

Morse married Lucretia Pickering Walker on September 29, 1818, in Concord, New Hampshire. She died on February 7, 1825, of a heart attack shortly after the birth of their third child. (Susan b. 1819, Charles b. 1823, James b. 1825). He married his second wife, Sarah Elizabeth Griswold on August 10, 1848, in Utica, New York and had four children (Samuel b. 1849, Cornelia b. 1851, William b. 1853, Edward b. 1857).

Morse expressed some of his Calvinist beliefs in his painting, "Landing of the Pilgrims", through the depiction of simple clothing as well as the people's austere facial features. His image captured the psychology of the Federalists; Calvinists from England brought to North America ideas of religion and government, thus linking the two countries. This work attracted the attention of the notable artist, Washington Allston. Allston wanted Morse to accompany him to England to meet the artist Benjamin West. Allston arranged—with Morse's father—a three-year stay for painting study in England. The two men set sail aboard the "Libya" on July 15, 1811.

In England, Morse perfected his painting techniques under Allston's watchful eye; by the end of 1811, he gained admittance to the Royal Academy. At the Academy, he was moved by the art of the Renaissance and paid close attention to the works of Michelangelo and Raphael. After observing and practicing life drawing and absorbing its anatomical demands, the young artist produced his masterpiece, the "Dying Hercules". (He first made a sculpture as a study for the painting.)

To some, the "Dying Hercules" seemed to represent a political statement against the British and also the American Federalists. The muscles symbolized the strength of the young and vibrant United States versus the British and British-American supporters. During Morse's time in Britain, the Americans and British were engaged in the War of 1812. Both societies were conflicted over loyalties. Anti-Federalist Americans aligned themselves with the French, abhorred the British, and believed a strong central government to be inherently dangerous to democracy.

As the war raged on, Morse's letters to his parents became more anti-Federalist in tone. In one such letter, Morse wrote:

I assert ... that the Federalists in the Northern States have done more injury to their country by their violent opposition measures than a French alliance could. Their proceedings are copied into the English papers, read before Parliament, and circulated through their country, and what do they say of them ... they call them [Federalists] cowards, a base set, say they are traitors to their country and ought to be hanged like traitors.

Although Jedidiah Morse did not change Samuel's political views, he continued as an influence. Critics believe that the elder Morse's Calvinist ideas are integral to Morse's "Judgment of Jupiter," another significant work completed in England. Jupiter is shown in a cloud, accompanied by his eagle, with his hand spread above the parties and he is pronouncing judgment. Marpessa, with an expression of compunction and shame, is throwing herself into the arms of her husband. Idas, who tenderly loved Marpessa, is eagerly rushing forward to receive her while Apollo stares with surprise.

Critics have suggested that Jupiter represents God's omnipotence—watching every move that is made. Some call the portrait a moral teaching by Morse on infidelity. Although Marpessa fell victim, she realized that her eternal salvation was important and desisted from her wicked ways. Apollo shows no remorse for what he did but stands with a puzzled look. Many American paintings throughout the early nineteenth century had religious themes, and Morse was an early exemplar of this. "Judgment of Jupiter" allowed Morse to express his support of Anti-Federalism while maintaining his strong spiritual convictions. Benjamin West sought to present the "Jupiter" at another Royal Academy exhibition, but Morse's time had run out. He left England on August 21, 1815, to return to the United States and begin his full-time career as a painter.

The decade 1815–1825 marked significant growth in Morse's work, as he sought to capture the essence of America's culture and life. He painted the Federalist former President John Adams (1816). The Federalists and Anti-Federalists clashed over Dartmouth College. Morse painted portraits of Francis Brown—the college's president—and Judge Woodward (1817), who was involved in bringing the Dartmouth case before the U.S. Supreme Court.

Morse also sought commissions among the elite of Charleston, South Carolina. Morse's 1818 painting of Mrs. Emma Quash symbolized the opulence of Charleston. The young artist was doing well for himself. Between 1819 and 1821, Morse went through great changes in his life, including a decline in commissions due to the Panic of 1819.

Morse was commissioned to paint President James Monroe in 1820. He embodied Jeffersonian democracy by favoring the common man over the aristocrat.

Morse had moved to New Haven. His commissions for "The House of Representatives" (1821) and a portrait of the Marquis de Lafayette (1825) engaged his sense of democratic nationalism. "The House of Representatives" was designed to capitalize on the success of François Marius Granet's "The Capuchin Chapel in Rome," which toured the United States extensively throughout the 1820s, attracting audiences willing to pay the 25-cent admission fee.

The artist chose to paint the House of Representatives, in a similar way, with careful attention to architecture and dramatic lighting. He also wished to select a uniquely American topic that would bring glory to the young nation. His subject did just that, showing American democracy in action. He traveled to Washington D.C. to draw the architecture of the new Capitol and placed eighty individuals within the painting. He chose to portray a night scene, balancing the architecture of the Rotunda with the figures, and using lamplight to highlight the work. Pairs of people, those who stood alone, individuals bent over their desks working, were each painted simply but with faces of character. Morse chose nighttime to convey that Congress' dedication to the principles of democracy transcended day.

"The House of Representatives" failed to draw a crowd when exhibited in New York City in 1823. By contrast, John Trumbull's "Declaration of Independence" had won popular acclaim a few years earlier. Viewers may have felt that the architecture of "The House of Representatives" overshadows the individuals, making it hard to appreciate the drama of what was happening.

Morse was honored to paint the Marquis de Lafayette, the leading French supporter of the American Revolution. He felt compelled to paint a grand portrait of the man who helped to establish a free and independent America. He features Lafayette against a magnificent sunset. He has positioned Lafayette to the right of three pedestals: one has a bust of Benjamin Franklin, another of George Washington, and the third seems reserved for Lafayette. A peaceful woodland landscape below him symbolized American tranquility and prosperity as it approached the age of fifty. The developing friendship between Morse and Lafayette and their discussions of the Revolutionary War affected the artist after his return to New York City.

In 1826, he helped found the National Academy of Design in New York City. He served as the Academy's President from 1826 to 1845 and again from 1861 to 1862.

From 1830 to 1832, Morse traveled and studied in Europe to improve his painting skills, visiting Italy, Switzerland, and France. During his time in Paris, he developed a friendship with the writer James Fennimore Cooper. As a project, he painted miniature copies of 38 of the Louvre's famous paintings on a single canvas (6 ft. x 9 ft), which he entitled "The Gallery of the Louvre." He completed the work upon his return to the United States.

On a subsequent visit to Paris in 1839, Morse met Louis Daguerre. He became interested in the latter's daguerreotype—the first practical means of photography. Morse wrote a letter to the "New York Observer" describing the invention, which was published widely in the American press and provided broad awareness of the new technology. Matthew Brady, one of the earliest photographers in American history, famous for his depictions of the Civil War, initially studied under Morse and later took photographs of him.

Some of Morse's paintings and sculptures are on display at his Locust Grove estate in Poughkeepsie, New York.

While returning by ship from Europe in 1832, Morse encountered Charles Thomas Jackson of Boston, a man who was well schooled in electromagnetism. Witnessing various experiments with Jackson's electromagnet, Morse developed the concept of a single-wire telegraph. He set aside his painting, "The Gallery of the Louvre". The original Morse telegraph, submitted with his patent application, is part of the collections of the National Museum of American History at the Smithsonian Institution. In time the Morse code, which he developed, would become the primary language of telegraphy in the world. It is still the standard for rhythmic transmission of data.

Meanwhile, William Cooke and Professor Charles Wheatstone had learned of the Wilhelm Weber and Carl Gauss electromagnetic telegraph in 1833. They had reached the stage of launching a commercial telegraph prior to Morse, despite starting later. In England, Cooke became fascinated by electrical telegraphy in 1836, four years after Morse. Aided by his greater financial resources, Cooke abandoned his primary subject of anatomy and built a small electrical telegraph within three weeks. Wheatstone also was experimenting with telegraphy and (most importantly) understood that a single large battery would not carry a telegraphic signal over long distances. He theorized that numerous small batteries were far more successful and efficient in this task. (Wheatstone was building on the primary research of Joseph Henry, an American physicist.) Cooke and Wheatstone formed a partnership and patented the electrical telegraph in May 1837, and within a short time had provided the Great Western Railway with a stretch of telegraph. However, within a few years, Cooke and Wheatstone's multiple-wire signaling method would be overtaken by Morse's cheaper method.

In an 1848 letter to a friend, Morse describes how vigorously he fought to be called the sole inventor of the electromagnetic telegraph despite the previous inventions.

Morse encountered the problem of getting a telegraphic signal to carry over more than a few hundred yards of wire. His breakthrough came from the insights of Professor Leonard Gale, who taught chemistry at New York University (he was a personal friend of Joseph Henry). With Gale's help, Morse introduced extra circuits or relays at frequent intervals and was soon able to send a message through of wire. This was the great breakthrough he had been seeking. Morse and Gale were soon joined by Alfred Vail, an enthusiastic young man with excellent skills, insights, and money.

At the Speedwell Ironworks in Morristown, New Jersey on January 11, 1838, Morse and Vail made the first public demonstration of the electric telegraph. Although Morse and Alfred Vail had done most of the research and development in the ironworks facilities, they chose a nearby factory house as the demonstration site. Without the repeater, the range of the telegraph was limited to , and the inventors had pulled of wires inside the factory house through an elaborate scheme. The first public transmission, with the message, "A patient waiter is no loser", was witnessed by a mostly local crowd.

Morse traveled to Washington, D.C. in 1838 seeking federal sponsorship for a telegraph line but was not successful. He went to Europe, seeking both sponsorship and patents, but in London discovered that Cooke and Wheatstone had already established priority. After his return to the US, Morse finally gained financial backing by Maine congressman Francis Ormand Jonathan Smith. This funding may be the first instance of government support to a private researcher, especially funding for applied (as opposed to basic or theoretical) research.

Morse made his last trip to Washington, D.C., in December 1842, stringing "wires between two committee rooms in the Capitol, and sent messages back and forth" to demonstrate his telegraph system. Congress appropriated $30,000 in 1843 for construction of an experimental telegraph line between Washington, D.C., and Baltimore along the right-of-way of the Baltimore and Ohio Railroad. An impressive demonstration occurred on May 1, 1844, when news of the Whig Party's nomination of Henry Clay for U.S. President was telegraphed from the party's convention in Baltimore to the Capitol Building in Washington.

On May 24, 1844, the line was officially opened as Morse sent the now-famous words, "What hath God wrought," from the Supreme Court chamber in the basement of the U.S. Capitol building in Washington, D.C., to the B&O's Mount Clare Station in Baltimore. Annie Ellsworth chose these words from the Bible (Numbers 23:23); her father, U.S. Patent Commissioner Henry Leavitt Ellsworth, had championed Morse's invention and secured early funding for it. His telegraph could transmit thirty characters per minute.

In May 1845, the Magnetic Telegraph Company was formed in order to build telegraph lines from New York City toward Philadelphia; Boston; Buffalo, New York; and the Mississippi. Telegraphic lines rapidly spread throughout the United States in the next few years, with 12,000 miles of wire laid by 1850.

Morse at one time adopted Wheatstone and Carl August von Steinheil's idea of broadcasting an electrical telegraph signal through a body of water or down steel railroad tracks or anything conductive. He went to great lengths to win a lawsuit for the right to be called "inventor of the telegraph" and promoted himself as being an inventor. But Alfred Vail also played an important role in the development of the Morse code, which was based on earlier codes for the electromagnetic telegraph.

Morse received a patent for the telegraph in 1847, at the old Beylerbeyi Palace (the present Beylerbeyi Palace was built in 1861–1865 on the same location) in Istanbul, which was issued by Sultan Abdülmecid, who personally tested the new invention. He was elected an Associate Fellow of the American Academy of Arts and Sciences in 1849. The original patent went to the Breese side of the family after the death of Samuel Morse.

In 1856, Morse went to Copenhagen and visited the Thorvaldsens Museum, where the sculptor's grave is in the inner courtyard. He was received by King Frederick VII, who decorated him with the Order of the Dannebrog for the telegraph. Morse expressed his wish to donate his Thorvaldsen portrait from 1831 in Rome to the king. The Thorvaldsen portrait today belongs to Margrethe II of Denmark.

The Morse telegraphic apparatus was officially adopted as the standard for European telegraphy in 1851. Only the United Kingdom (with its extensive overseas empire) kept the needle telegraph of Cooke and Wheatstone.

In 1858, Morse introduced wired communication to Latin America when he established a telegraph system in Puerto Rico, then a Spanish Colony. Morse's oldest daughter, Susan Walker Morse (1819–1885), would often visit her uncle Charles Pickering Walker, who owned the Hacienda Concordia in the town of Guayama. During one of her visits, she met Edward Lind, a Danish merchant who worked in his brother-in-law's Hacienda La Henriqueta in the town of Arroyo. They later married. Lind purchased the Hacienda from his sister when she became a widow. Morse, who often spent his winters at the Hacienda with his daughter and son-in-law, set a two-mile telegraph line connecting his son-in-law's Hacienda to their house in Arroyo. The line was inaugurated on March 1, 1859, in a ceremony flanked by the Spanish and American flags. The first words transmitted by Samuel Morse that day in Puerto Rico were:

Puerto Rico, beautiful jewel! When you are linked with the other jewels of the Antilles in the necklace of the world's telegraph, yours will not shine less brilliantly in the crown of your Queen!

There is an argument amongst historians that Morse may have received the idea of a plausible telegraph from Harrison Gray Dyar some eighteen years earlier than his patent.

Morse was a leader in the anti-Catholic and anti-immigration movement of the mid-19th century. In 1836, he ran unsuccessfully for mayor of New York under the anti-immigrant Nativist Party's banner, receiving only 1,496 votes. When Morse visited Rome, he allegedly refused to take his hat off in the presence of the Pope.

Morse worked to unite Protestants against Catholic institutions (including schools), wanted to forbid Catholics from holding public office, and promoted changing immigration laws to limit immigration from Catholic countries. On this topic, he wrote, "We must first stop the leak in the ship through which muddy waters from without threaten to sink us."

He wrote numerous letters to the New York "Observer" (his brother Sidney was the editor at the time) urging people to fight the perceived Catholic menace. These were widely reprinted in other newspapers. Among other claims, he believed that the Austrian government and Catholic aid organizations were subsidizing Catholic immigration to the United States in order to gain control of the country.

In his "Foreign Conspiracy Against the Liberties of the United States", Morse wrote:
Surely American Protestants, freemen, have discernment enough to discover beneath them the cloven foot of this subtle foreign heresy. They will see that Popery is now, what it has ever been, a system of the darkest political intrigue and despotism, cloaking itself to avoid attack under the sacred name of religion. They will be deeply impressed with the truth, that Popery is a political as well as a religious system; that in this respect it differs totally from all other sects, from all other forms of religion in the country.

In the 1850s, Morse became well known as a defender of slavery, considering it to be sanctioned by God. This was a position held by many Southerners and others. In his treatise "An Argument on the Ethical Position of Slavery," he wrote:

In the United States, Morse held his telegraph patent for many years, but it was both ignored and contested. In 1853, "The Telegraph Patent case – O'Reilly v. Morse" came before the U.S. Supreme Court where, after very lengthy investigation, Chief Justice Roger B. Taney ruled that Morse had been the first to combine the battery, electromagnetism, the electromagnet, and the correct battery configuration into a workable practical telegraph. However, in spite of this clear ruling, Morse still received no official recognition from the United States government.

The Supreme Court did not accept all of Morse's claims. The "O'Reilly v. Morse" case has become widely known among patent lawyers because the Supreme Court explicitly denied Morse's claim 8 for any and all use of the electromagnetic force for purposes of transmitting intelligible signals to any distance.
The Supreme Court sustained, however, Morse's claim to such telecommunication when effectuated by means of Morse's inventive "repeater" apparatus. This was an electrical circuit in which a cascade of many sets comprising a relay and a battery were connected in series, so that when each relay closed, it closed a circuit to cause the next battery to power the succeeding relay, as suggested in the accompanying figure. This caused Morse's signal to pass along the cascade without degrading into noise as its amplitude decreased with the distance traveled. (Each time the amplitude of the signal approaches the noise level, the repeater [in effect, a nonlinear amplifier] boosts the signal amplitude well above the noise level.) This use of "repeaters" permitted a message to be sent to great distances, which was previously not feasible.

The Supreme Court thus held that Morse could properly claim a patent monopoly on the system or process of transmitting signals at any distance by means of the repeater circuitry indicated above, but he could not properly claim a monopoly over any and all uses of electromagnetic force to transmit signals. The apparatus limitation in the former type of claim limited the patent monopoly to what Morse taught and gave the world. The lack of that limitation in the latter type of claim (i.e., claim 8) both gave Morse more than was commensurate with what he had contributed to society and discouraged the inventive efforts of others who might come up with different and/or better ways to send signals at a distance using the electromagnetic force.

The problem that Morse faced (the deterioration of the signal with distance) and how he solved it is discussed in more detail in the article "O'Reilly v. Morse". In summary, the solution, as the Supreme Court stated, was the repeater apparatus described in the preceding paragraphs.

The importance of this legal precedent in patent law cannot be overstated, as it became the foundation of the law governing the eligibility of computer program-implemented inventions (as well as inventions implementing natural laws) to be granted patents.

Assisted by the American ambassador in Paris, the governments of Europe were approached about their long neglect of Morse while their countries were using his invention. There was a widespread recognition that something must be done, and in 1858 Morse was awarded the sum of 400,000 French francs (equivalent to about $80,000 at the time) by the governments of France, Austria, Belgium, the Netherlands, Piedmont, Russia, Sweden, Tuscany, and Turkey, each of which contributed a share according to the number of Morse instruments in use in each country. In 1858, he was also elected a foreign member of the Royal Swedish Academy of Sciences.

Morse lent his support to Cyrus West Field's ambitious plan to construct the first transoceanic telegraph line. Morse had experimented with underwater telegraph circuits since 1842. He invested $10,000 in Field's Atlantic Telegraph Company, took a seat on its board of directors, and was appointed honorary "Electrician". In 1856, Morse traveled to London to help Charles Tilston Bright and Edward Whitehouse test a 2,000-mile-length of spooled cable.

After the first two cable-laying attempts failed, Field reorganized the project, removing Morse from direct involvement. Though the cable broke three times during the third attempt, it was successfully repaired, and the first transatlantic telegraph messages were sent in 1858. The cable failed after just three months of use. Though Field had to wait out the Civil War, the cable laid in 1866 proved more durable, and the era of reliable transatlantic telegraph service had begun.

In addition to the telegraph, Morse invented a marble-cutting machine that could carve three-dimensional sculptures in marble or stone. He could not patent it, however, because of an existing 1820 Thomas Blanchard design.

Samuel Morse gave large sums to charity. He also became interested in the relationship of science and religion and provided the funds to establish a lectureship on "the relation of the Bible to the Sciences". Though he was rarely awarded any royalties for the later uses and implementations of his inventions, he was able to live comfortably.

Morsemere in Ridgefield, New Jersey takes its name from Morse, who had bought property there to build a home, but died before its completion.

He died in New York City on April 2, 1872, and was interred at Green-Wood Cemetery in Brooklyn, New York. By the time of his death, his estate was valued at some $500,000 ($ today).

Morse was elected a member of the American Antiquarian Society in 1815.

Despite honors and financial awards received from foreign countries, there was no such recognition in the U.S. until he neared the end of his life when on June 10, 1871, a bronze statue of Samuel Morse was unveiled in Central Park, New York City. An engraved portrait of Morse appeared on the reverse side of the United States two-dollar bill silver certificate series of 1896. He was depicted along with Robert Fulton. An example can be seen on the website of the Federal Reserve Bank of San Francisco's website in their "American Currency Exhibit":

A blue plaque was erected to commemorate him at 141 Cleveland Street, London, where he lived from 1812 to 1815.

According to his "The New York Times" obituary published on April 3, 1872, Morse received respectively the decoration of the Atiq Nishan-i-Iftikhar (English: Order of Glory) [first medal on wearer's right depicted in photo of Morse with medals], set in diamonds, from Sultan Abdülmecid of Turkey (c.1847), a "golden snuff box containing the Prussian gold medal for scientific merit" from the King of Prussia (1851); the "Great Gold Medal of Arts and Sciences" from the King of Württemberg (1852); and the "Great Golden Medal of Science and Arts" from Emperor of Austria (1855); a cross of Chevalier in the Légion d'honneur from the Emperor of France; the "Cross of a Knight" of the Order of the Dannebrog from the King of Denmark (1856); the Cross of Knight Commander of the Order of Isabella the Catholic, from the Queen of Spain, besides being elected member of innumerable scientific and art societies in this [United States] and other countries. Other awards include Order of the Tower and Sword from the kingdom of Portugal (1860), and Italy conferred on him the insignia of chevalier of the Order of Saints Maurice and Lazarus in 1864. Morse's telegraph was recognized as an IEEE Milestone in 1988.

In 1975, Morse was inducted into the National Inventors Hall of Fame.

On April 1, 2012, Google announced the release of "Gmail Tap", an April Fools' Day joke that allowed users to use Morse Code to send text from their mobile phones. Morse's great-great-grandnephew Reed Morse—a Google engineer—was instrumental in the prank, which became a real product.








</doc>
<doc id="27008" url="https://en.wikipedia.org/wiki?curid=27008" title="Ship">
Ship

A ship is a large watercraft that travels the world's oceans and other sufficiently deep waterways, carrying passengers or goods, or in support of specialized missions, such as defense, research and fishing. Historically, a "ship" was a sailing vessel with at least three square-rigged masts and a full bowsprit. Ships are generally distinguished from boats, based on size, shape, load capacity, and tradition.

Ships have been important contributors to human migration and commerce. They have supported the spread of colonization and the slave trade, but have also served scientific, cultural, and humanitarian needs. After the 15th century, new crops that had come from and to the Americas via the European seafarers significantly contributed to the world population growth. Ship transport is responsible for the largest portion of world commerce.

As of 2016, there were more than 49,000 merchant ships, totaling almost 1.8 billion dead weight tons. Of these 28% were oil tankers, 43% were bulk carriers, and 13% were container ships.

Ships are generally larger than boats, but there is no universally accepted distinction between the two. Ships generally can remain at sea for longer periods of time than boats. A legal definition of ship from Indian case law is a vessel that carries goods by sea. A common notion is that a ship can carry a boat, but not "vice versa". A US Navy rule of thumb is that ships heel towards the "outside" of a sharp turn, whereas boats heel towards the "inside" because of the relative location of the center of mass versus the center of buoyancy. American and British 19th Century maritime law distinguished "vessels" from other craft; ships and boats fall in one legal category, whereas open boats and rafts are not considered vessels.

In the Age of Sail, a full-rigged ship was a sailing vessel with at least three square-rigged masts and a full bowsprit; other types of vessel were also defined by their sailplan, e.g. barque, brigantine, etc.

A number of large vessels are usually referred to as boats. Submarines are a prime example. Other types of large vessel which are traditionally called boats are Great Lakes freighters, riverboats, and ferryboats. Though large enough to carry their own boats and heavy cargoes, these vessels are designed for operation on inland or protected coastal waters.

In most maritime traditions ships have individual names, and modern ships may belong to a ship class often named after its first ship.

In the northern parts of Europe and America a ship is traditionally referred to with a female grammatical gender, represented in English with the pronoun "she", even if named after a man. This is not universal usage and some English language journalistic style guides advise using "it" as referring to ships with female pronouns can be seen as offensive and outdated. In many documents the ship name is introduced with a ship prefix being an abbreviation of the ship class, for example "MS" (motor ship) or "SV" (sailing vessel), making it easier to distinguish a ship name from other individual names in a text.

The first sea-going sailing ships were developed by the Austronesian peoples from what is now Southern China and Taiwan. Their invention of catamarans, outriggers, and crab claw sails enabled their ships to sail for vast distances in open ocean. It led to the Austronesian Expansion at around 3000 to 1500 BC. From Taiwan, they rapidly colonized the islands of Maritime Southeast Asia, then sailed further onwards to Micronesia, Island Melanesia, Polynesia, and Madagascar, eventually colonizing a territory spanning half the globe.

Austronesian rigs were distinctive in that they had spars supporting both the upper and lower edges of the sails (and sometimes in between), in contrast to western rigs which only had a spar on the upper edge. The sails were also made from woven leaves, usually from pandan plants. These were complemented by paddlers, who usually positioned themselves on platforms on the outriggers in the larger boats. Austronesian ships ranged in complexity from simple dugout canoes with outriggers or lashed together to large edge-pegged plank-built boats built around a keel made from a dugout canoe. Their designs were unique, evolving from ancient rafts to the characteristic double-hulled, single-outrigger, and double-outrigger designs of Austronesian ships.

Early Austronesian sailors influenced the development of sailing technologies in Sri Lanka and Southern India through the Austronesian maritime trade network of the Indian Ocean, the precursor to the spice trade route and the maritime silk road, which was established at around 1500 BC. Some scholars believe that the triangular Austronesian crab claw sail may have influenced the development of the lateen sail in western ships due to early contact. The junk rigs of Chinese ships is also believed to be originally Javanese in origin.

In the 1st century AD, the people from Nusantaran archipelago already made large ships over 50 m long and stood out 4–7 m out of the water. They could carry 700-1000 people and 260 ton cargo. These ships known as "kunlun bo" or "k'unlun po" (崑崙舶, lit. "ship of the Kunlun people") by the Chinese and kolandiaphonta by the Greeks. It has 4-7 masts and able to sail against the wind due to the usage of tanja sails. These ships reaching as far as Ghana.

In China, miniature models of ships that feature steering oars have been dated to the Warring States period (c. 475–221 BC). By the Han dynasty, a well kept naval fleet was an integral part of the military. Sternpost-mounted rudders started to appear on Chinese ship models starting in the 1st century AD. However, these early Chinese ships were fluvial (riverine), and were not seaworthy. The Chinese only acquired sea-going ship technologies in the 10th century AD Song Dynasty after contact with Southeast Asian djong trading ships, leading to the development of the junks.

In 3000 BC, Ancient Egyptians learned how to assemble wooden planks into a hull. They used woven straps to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. The Greek historian and geographer Agatharchides had documented ship-faring among the early Egyptians: ""During the prosperous period of the Old Kingdom, between the 30th and 25th centuries BC, the river-routes were kept in order, and Egyptian ships sailed the Red Sea as far as the myrrh-country."" Sneferu's ancient cedar wood ship Praise of the Two Lands is the first reference recorded (2613 BC) to a ship being referred to by name.

The ancient Egyptians were perfectly at ease building sailboats. A remarkable example of their shipbuilding skills was the Khufu ship, a vessel in length entombed at the foot of the Great Pyramid of Giza around 2500 BC and found intact in 1954.

The oldest discovered sea faring hulled boat is the Late Bronze Age Uluburun shipwreck off the coast of Turkey, dating back to 1300 BC.

The Phoenicians, the first to sail completely around Africa, and Greeks gradually mastered navigation at sea aboard triremes, exploring and colonizing the Mediterranean via ship.

At this time, ships were developing in Asia in much the same way as Europe. Japan used defensive naval techniques in the Mongol invasions of Japan in 1281. It is likely that the Mongols of the time took advantage of both European and Asian shipbuilding techniques. During the 15th century, China's Ming dynasty assembled one of the largest and most powerful naval fleets in the world for the diplomatic and power projection voyages of Zheng He. Elsewhere in Japan in the 15th century, one of the world's first iron-clads, "Tekkōsen" (), literally meaning "iron ships", was also developed. In Japan, during the Sengoku era from the fifteenth to 17th century, the great struggle for feudal supremacy was fought, in part, by coastal fleets of several hundred boats, including the atakebune. In Korea, in the early 15th century during the Joseon era, "Geobukseon"(거북선), was developed. The "turtle ship", as it was called is recognized as the first armored ship in the world.

Until the Renaissance, navigational technology remained comparatively primitive. This absence of technology did not prevent some civilizations from becoming sea powers. Examples include the maritime republics of Genoa and Venice, Hanseatic League, and the Byzantine navy. The Vikings used their knarrs to explore North America, trade in the Baltic Sea and plunder many of the coastal regions of Western Europe.

Towards the end of the 14th century, ships like the carrack began to develop towers on the bow and stern. These towers decreased the vessel's stability, and in the 15th century, the caravel, designed by the Portuguese, based on the Arabic "qarib" which could sail closer to the wind, became more widely used. The towers were gradually replaced by the forecastle and sterncastle, as in the carrack "Santa María" of Christopher Columbus. This increased freeboard allowed another innovation: the freeing port, and the artillery associated with it.

The carrack and then the caravel were developed in Portugal. After Columbus, European exploration rapidly accelerated, and many new trade routes were established. In 1498, by reaching India, Vasco da Gama proved that the access to the Indian Ocean from the Atlantic was possible. These explorations in the Atlantic and Indian Oceans were soon followed by France, England and the Netherlands, who explored the Portuguese and Spanish trade routes into the Pacific Ocean, reaching Australia in 1606 and New Zealand in 1642.

Parallel to the development of warships, ships in service of marine fishery and trade also developed in the period between antiquity and the Renaissance.

Maritime trade was driven by the development of shipping companies with significant financial resources. Canal barges, towed by draft animals on an adjacent towpath, contended with the railway up to and past the early days of the industrial revolution. Flat-bottomed and flexible scow boats also became widely used for transporting small cargoes. Mercantile trade went hand-in-hand with exploration, self-financed by the commercial benefits of exploration.

During the first half of the 18th century, the French Navy began to develop a new type of vessel known as a ship of the line, featuring seventy-four guns. This type of ship became the backbone of all European fighting fleets. These ships were long and their construction required 2,800 oak trees and of rope; they carried a crew of about 800 sailors and soldiers.

During the 19th century the Royal Navy enforced a ban on the slave trade, acted to suppress piracy, and continued to map the world. A clipper was a very fast sailing ship of the 19th century. The clipper routes fell into commercial disuse with the introduction of steam ships with better fuel efficiency, and the opening of the Suez and Panama Canals.

Ship designs stayed fairly unchanged until the late 19th century. The industrial revolution, new mechanical methods of propulsion, and the ability to construct ships from metal triggered an explosion in ship design. Factors including the quest for more efficient ships, the end of long running and wasteful maritime conflicts, and the increased financial capacity of industrial powers created an avalanche of more specialized boats and ships. Ships built for entirely new functions, such as firefighting, rescue, and research, also began to appear.

In 2007, the world's fleet included 34,882 commercial vessels with gross tonnage of more than 1,000 tons, totaling 1.04 billion tons. These ships carried 7.4 billion tons of cargo in 2006, a sum that grew by 8% over the previous year. In terms of tonnage, 39% of these ships are tankers, 26% are bulk carriers, 17% container ships and 15% were other types.

In 2002, there were 1,240 warships operating in the world, not counting small vessels such as patrol boats. The United States accounted for 3 million tons worth of these vessels, Russia 1.35 million tons, the United Kingdom 504,660 tons and China 402,830 tons. The 20th century saw many naval engagements during the two world wars, the Cold War, and the rise to power of naval forces of the two blocs. The world's major powers have recently used their naval power in cases such as the United Kingdom in the Falkland Islands and the United States in Iraq.

The size of the world's fishing fleet is more difficult to estimate. The largest of these are counted as commercial vessels, but the smallest are legion. Fishing vessels can be found in most seaside villages in the world. As of 2004, the United Nations Food and Agriculture Organization estimated 4 million fishing vessels were operating worldwide. The same study estimated that the world's 29 million fishermen caught of fish and shellfish that year.

Because ships are constructed using the principles of naval architecture that require same structural components, their classification is based on their function such as that suggested by Paulet and Presles, which requires modification of the components. The categories accepted in general by naval architects are:

Some of these are discussed in the following sections.

Freshwater shipping may occur on lakes, rivers and canals. Ships designed for those venues may be specially adapted to the widths and depths of specific waterways. Examples of freshwater waterways that are navigable in part by large vessels include the Danube, Mississippi, Rhine, Yangtze and Amazon Rivers, and the Great Lakes.

Lake freighters, also called lakers, are cargo vessels that ply the Great Lakes. The most well-known is , the latest major vessel to be wrecked on the Lakes. These vessels are traditionally called boats, not ships. Visiting ocean-going vessels are called "salties." Because of their additional beam, very large salties are never seen inland of the Saint Lawrence Seaway. Because the smallest of the Soo Locks is larger than any Seaway lock, salties that can pass through the Seaway may travel anywhere in the Great Lakes. Because of their deeper draft, salties may accept partial loads on the Great Lakes, "topping off" when they have exited the Seaway. Similarly, the largest lakers are confined to the Upper Lakes (Superior, Michigan, Huron, Erie) because they are too large to use the Seaway locks, beginning at the Welland Canal that bypasses the Niagara River.

Since the freshwater lakes are less corrosive to ships than the salt water of the oceans, lakers tend to last much longer than ocean freighters. Lakers older than 50 years are not unusual, and as of 2005, all were over 20 years of age.

, built in 1906 as "William P Snyder", was the oldest laker still working on the Lakes until its conversion into a barge starting in 2013. Similarly, "E.M. Ford", built in 1898 as "Presque Isle", was sailing the lakes 98 years later in 1996. As of 2007 "E.M. Ford" was still afloat as a stationary transfer vessel at a riverside cement silo in Saginaw, Michigan.

Merchant ships are ships used for commercial purposes and can be divided into four broad categories: fishing, cargo ships, passenger ships, and special-purpose ships. The UNCTAD review of maritime transport categorizes ships as: oil tankers, bulk (and combination) carriers, general cargo ships, container ships, and "other ships", which includes "liquefied petroleum gas carriers, liquefied natural gas carriers, parcel (chemical) tankers, specialized tankers, reefers, offshore supply, tugs, dredgers, cruise, ferries, other non-cargo". General cargo ships include "multi-purpose and project vessels and roll-on/roll-off cargo".

Modern commercial vessels are typically powered by a single propeller driven by a diesel or, less usually, gas turbine engine., but until the mid-19th century they were predominantly square sail rigged. The fastest vessels may use pump-jet engines. Most commercial vessels have full hull-forms to maximize cargo capacity. Hulls are usually made of steel, although aluminum can be used on faster craft, and fiberglass on the smallest service vessels. Commercial vessels generally have a crew headed by a sea captain, with deck officers and engine officers on larger vessels. Special-purpose vessels often have specialized crew if necessary, for example scientists aboard research vessels.

Fishing boats are generally small, often little more than but up to for a large tuna or whaling ship. Aboard a fish processing vessel, the catch can be made ready for market and sold more quickly once the ship makes port. Special purpose vessels have special gear. For example, trawlers have winches and arms, stern-trawlers have a rear ramp, and tuna seiners have skiffs. In 2004, of fish were caught in the marine capture fishery. Anchoveta represented the largest single catch at . That year, the top ten marine capture species also included Alaska pollock, Blue whiting, Skipjack tuna, Atlantic herring, Chub mackerel, Japanese anchovy, Chilean jack mackerel, Largehead hairtail, and Yellowfin tuna. Other species including salmon, shrimp, lobster, clams, squid and crab, are also commercially fished. Modern commercial fishermen use many methods. One is fishing by nets, such as purse seine, beach seine, lift nets, gillnets, or entangling nets. Another is trawling, including bottom trawl. Hooks and lines are used in methods like long-line fishing and hand-line fishing. Another method is the use of fishing trap.

Cargo ships transport dry and liquid cargo. Dry cargo can be transported in bulk by bulk carriers, packed directly onto a general cargo ship in break-bulk, packed in intermodal containers as aboard a container ship, or driven aboard as in roll-on roll-off ships. Liquid cargo is generally carried in bulk aboard tankers, such as oil tankers which may include both crude and finished products of oil, chemical tankers which may also carry vegetable oils other than chemicals and LPG/LNG tankers, although smaller shipments may be carried on container ships in tank containers.

Passenger ships range in size from small river ferries to very large cruise ships. This type of vessel includes ferries, which move passengers and vehicles on short trips; ocean liners, which carry passengers from one place to another; and cruise ships, which carry passengers on voyages undertaken for pleasure, visiting several places and with leisure activities on board, often returning them to the port of embarkation. Riverboats and inland ferries are specially designed to carry passengers, cargo, or both in the challenging river environment. Rivers present special hazards to vessels. They usually have varying water flows that alternately lead to high speed water flows or protruding rock hazards. Changing siltation patterns may cause the sudden appearance of shoal waters, and often floating or sunken logs and trees (called snags) can endanger the hulls and propulsion of riverboats. Riverboats are generally of shallow draft, being broad of beam and rather square in plan, with a low freeboard and high topsides. Riverboats can survive with this type of configuration as they do not have to withstand the high winds or large waves that are seen on large lakes, seas, or oceans.
Fishing vessels are a subset of commercial vessels, but generally small in size and often subject to different regulations and classification. They can be categorized by several criteria: architecture, the type of fish they catch, the fishing method used, geographical origin, and technical features such as rigging. As of 2004, the world's fishing fleet consisted of some 4 million vessels. Of these, 1.3 million were decked vessels with enclosed areas and the rest were open vessels. Most decked vessels were mechanized, but two-thirds of the open vessels were traditional craft propelled by sails and oars. More than 60% of all existing large fishing vessels were built in Japan, Peru, the Russian Federation, Spain or the United States of America.

A weather ship was a ship stationed in the ocean as a platform for surface and upper air meteorological observations for use in marine weather forecasting. Surface weather observations were taken hourly, and four radiosonde releases occurred daily. It was also meant to aid in search and rescue operations and to support transatlantic flights. Proposed as early as 1927 by the aviation community, the establishment of weather ships proved to be so useful during World War II that the International Civil Aviation Organization (ICAO) established a global network of weather ships in 1948, with 13 to be supplied by the United States. This number was eventually negotiated down to nine.

The weather ship crews were normally at sea for three weeks at a time, returning to port for 10-day stretches. Weather ship observations proved to be helpful in wind and wave studies, as they did not avoid weather systems like other ships tended to for safety reasons. They were also helpful in monitoring storms at sea, such as tropical cyclones. The removal of a weather ship became a negative factor in forecasts leading up to the Great Storm of 1987. Beginning in the 1970s, their role became largely superseded by weather buoys due to the ships' significant cost. The agreement of the use of weather ships by the international community ended in 1990. The last weather ship was "Polarfront", known as weather station M ("Mike"), which was put out of operation on 1 January 2010. Weather observations from ships continue from a fleet of voluntary merchant vessels in routine commercial operation.

Naval vessels are those used by a navy for military purposes.
There have been many types of naval vessel. Modern naval vessels can be broken down into three categories: surface warships, submarines, and auxiliary ships.

Modern warships are generally divided into seven main categories: aircraft carriers, cruisers, destroyers, frigates, corvettes, submarines and amphibious assault ships. The distinction between cruisers, destroyers, frigates, and corvettes is not rigorous; the same vessel may be described differently in different navies. Battleships were used during the Second World War and occasionally since then (the last battleships were removed from the U.S. Naval Vessel Register in March 2006), but were made obsolete by the use of carrier-borne aircraft and guided missiles.

Most military submarines are either attack submarines or ballistic missile submarines. Until the end of World War II the primary role of the diesel/electric submarine was anti-ship warfare, inserting and removing covert agents and military forces, and intelligence-gathering. With the development of the homing torpedo, better sonar systems, and nuclear propulsion, submarines also became able to effectively hunt each other. The development of submarine-launched nuclear and cruise missiles gave submarines a substantial and long-ranged ability to attack both land and sea targets with a variety of weapons ranging from cluster munitions to nuclear weapons.

Most navies also include many types of support and auxiliary vessel, such as minesweepers, patrol boats, offshore patrol vessels, replenishment ships, and hospital ships which are designated medical treatment facilities.

Fast combat vessels such as cruisers and destroyers usually have fine hulls to maximize speed and maneuverability. They also usually have advanced marine electronics and communication systems, as well as weapons.

Some components exist in vessels of any size and purpose. Every vessel has a hull of sorts. Every vessel has some sort of propulsion, whether it's a pole, an ox, or a nuclear reactor. Most vessels have some sort of steering system. Other characteristics are common, but not as universal, such as compartments, holds, a superstructure, and equipment such as anchors and winches.

For a ship to float, its weight must be less than that of the water displaced by the ship's hull. There are many types of hulls, from logs lashed together to form a raft to the advanced hulls of America's Cup sailboats. A vessel may have a single hull (called a monohull design), two in the case of catamarans, or three in the case of trimarans. Vessels with more than three hulls are rare, but some experiments have been conducted with designs such as pentamarans. Multiple hulls are generally parallel to each other and connected by rigid arms.

Hulls have several elements. The bow is the foremost part of the hull. Many ships feature a bulbous bow. The keel is at the very bottom of the hull, extending the entire length of the ship. The rear part of the hull is known as the stern, and many hulls have a flat back known as a transom. Common hull appendages include propellers for propulsion, rudders for steering, and stabilizers to quell a ship's rolling motion. Other hull features can be related to the vessel's work, such as fishing gear and sonar domes.

Hulls are subject to various hydrostatic and hydrodynamic constraints. The key hydrostatic constraint is that it must be able to support the entire weight of the boat, and maintain stability even with often unevenly distributed weight. Hydrodynamic constraints include the ability to withstand shock waves, weather collisions and groundings.

Older ships and pleasure craft often have or had wooden hulls. Steel is used for most commercial vessels. Aluminium is frequently used for fast vessels, and composite materials are often found in sailboats and pleasure craft. Some ships have been made with concrete hulls.

Propulsion systems for ships fall into three categories: human propulsion, sailing, and mechanical propulsion. Human propulsion includes rowing, which was used even on large galleys. Propulsion by sail generally consists of a sail hoisted on an erect mast, supported by stays and spars and controlled by ropes. Sail systems were the dominant form of propulsion until the 19th century. They are now generally used for recreation and competition, although experimental sail systems, such as the turbosails, rotorsails, and wingsails have been used on larger modern vessels for fuel savings.

Mechanical propulsion systems generally consist of a motor or engine turning a propeller, or less frequently, an impeller or wave propulsion fins. Steam engines were first used for this purpose, but have mostly been replaced by two-stroke or four-stroke diesel engines, outboard motors, and gas turbine engines on faster ships. Nuclear reactors producing steam are used to propel warships and icebreakers, and there have been attempts to utilize them to power commercial vessels (see NS "Savannah").

In addition to traditional fixed and controllable pitch propellers there are many specialized variations, such as contra-rotating and nozzle-style propellers. Most vessels have a single propeller, but some large vessels may have up to four propellers supplemented with transverse thrusters for maneuvring at ports. The propeller is connected to the main engine via a propeller shaft and, in case of medium- and high-speed engines, a reduction gearbox. Some modern vessels have a diesel-electric powertrain in which the propeller is turned by an electric motor powered by the ship's generators.

For ships with independent propulsion systems for each side, such as manual oars or some paddles, steering systems may not be necessary. In most designs, such as boats propelled by engines or sails, a steering system becomes necessary. The most common is a rudder, a submerged plane located at the rear of the hull. Rudders are rotated to generate a lateral force which turns the boat. Rudders can be rotated by a tiller, manual wheels, or electro-hydraulic systems. Autopilot systems combine mechanical rudders with navigation systems. Ducted propellers are sometimes used for steering.

Some propulsion systems are inherently steering systems. Examples include the outboard motor, the bow thruster, and the Z-drive.

Larger boats and ships generally have multiple decks and compartments. Separate berthings and heads are found on sailboats over about . Fishing boats and cargo ships typically have one or more cargo holds. Most larger vessels have an engine room, a galley, and various compartments for work. Tanks are used to store fuel, engine oil, and fresh water. Ballast tanks are equipped to change a ship's trim and modify its stability.

Superstructures are found above the main deck. On sailboats, these are usually very low. On modern cargo ships, they are almost always located near the ship's stern. On passenger ships and warships, the superstructure generally extends far forward.

Shipboard equipment varies from ship to ship depending on such factors as the ship's era, design, area of operation, and purpose. Some types of equipment that are widely found include:

Ships float in the water at a level where mass of the displaced water equals the mass of the vessel, such that the downwards force of gravity equals the upward force of buoyancy. As a vessel is lowered into the water its weight remains constant but the corresponding weight of water displaced by its hull increases. If the vessel's mass is evenly distributed throughout, it floats evenly along its length and across its beam (width). A vessel's stability is considered in both this hydrostatic sense as well as a hydrodynamic sense, when subjected to movement, rolling and pitching, and the action of waves and wind. Stability problems can lead to excessive pitching and rolling, and eventually capsizing and sinking.

The advance of a vessel through water is resisted by the water. This resistance can be broken down into several components, the main ones being the friction of the water on the hull and wave making resistance. To reduce resistance and therefore increase the speed for a given power, it is necessary to reduce the wetted surface and use submerged hull shapes that produce low amplitude waves. To do so, high-speed vessels are often more slender, with fewer or smaller appendages. The friction of the water is also reduced by regular maintenance of the hull to remove the sea creatures and algae that accumulate there. Antifouling paint is commonly used to assist in this. Advanced designs such as the bulbous bow assist in decreasing wave resistance.

A simple way of considering wave-making resistance is to look at the hull in relation to its wake. At speeds lower than the wave propagation speed, the wave rapidly dissipates to the sides. As the hull approaches the wave propagation speed, however, the wake at the bow begins to build up faster than it can dissipate, and so it grows in amplitude. Since the water is not able to "get out of the way of the hull fast enough", the hull, in essence, has to climb over or push through the bow wave. This results in an exponential increase in resistance with increasing speed.

This hull speed is found by the formula:

or, in metric units:

where "L" is the length of the waterline in feet or meters.

When the vessel exceeds a speed/length ratio of 0.94, it starts to outrun most of its bow wave, and the hull actually settles slightly in the water as it is now only supported by two wave peaks. As the vessel exceeds a speed/length ratio of 1.34, the hull speed, the wavelength is now longer than the hull, and the stern is no longer supported by the wake, causing the stern to squat, and the bow rise. The hull is now starting to climb its own bow wave, and resistance begins to increase at a very high rate. While it is possible to drive a displacement hull faster than a speed/length ratio of 1.34, it is prohibitively expensive to do so. Most large vessels operate at speed/length ratios well below that level, at speed/length ratios of under 1.0.

For large projects with adequate funding, hydrodynamic resistance can be tested experimentally in a hull testing pool or using tools of computational fluid dynamics.

Vessels are also subject to ocean surface waves and sea swell as well as effects of wind and weather. These movements can be stressful for passengers and equipment, and must be controlled if possible. The rolling movement can be controlled, to an extent, by ballasting or by devices such as fin stabilizers. Pitching movement is more difficult to limit and can be dangerous if the bow submerges in the waves, a phenomenon called pounding. Sometimes, ships must change course or speed to stop violent rolling or pitching.

How it has been convincingly shown in scientific studies of the 21st century, controllability of some vessels decreases dramatically in some cases that are conditioned by effects of the bifurcation memory. This class of vessels includes ships with high manoeuvring capabilities, aircraft and controlled underwater vehicles designed to be unstable in steady-state motion that are interesting in terms of applications. These features must be considered in designing ships and in their control in critical situations.

A ship will pass through several stages during its career. The first is usually an initial contract to build the ship, the details of which can vary widely based on relationships between the shipowners, operators, designers and the shipyard. Then, the design phase carried out by a naval architect. Then the ship is constructed in a shipyard. After construction, the vessel is launched and goes into service. Ships end their careers in a number of ways, ranging from shipwrecks to service as a museum ship to the scrapyard.

A vessel's design starts with a specification, which a naval architect uses to create a project outline, assess required dimensions, and create a basic layout of spaces and a rough displacement. After this initial rough draft, the architect can create an initial hull design, a general profile and an initial overview of the ship's propulsion. At this stage, the designer can iterate on the ship's design, adding detail and refining the design at each stage.

The designer will typically produce an overall plan, a general specification describing the peculiarities of the vessel, and construction blueprints to be used at the building site. Designs for larger or more complex vessels may also include sail plans, electrical schematics, and plumbing and ventilation plans.

As environmental laws are becoming more strict, ship designers need to create their design in such a way that the ship, when it nears its end-of-term, can be disassembled or disposed easily and that waste is reduced to a minimum.

Ship construction takes place in a shipyard, and can last from a few months for a unit produced in series, to several years to reconstruct a wooden boat like the frigate "Hermione", to more than 10 years for an aircraft carrier. During World War II, the need for cargo ships was so urgent that construction time for Liberty Ships went from initially eight months or longer, down to weeks or even days. Builders employed production line and prefabrication techniques such as those used in shipyards today.

Hull materials and vessel size play a large part in determining the method of construction. The hull of a mass-produced fiberglass sailboat is constructed from a mold, while the steel hull of a cargo ship is made from large sections welded together as they are built.

Generally, construction starts with the hull, and on vessels over about , by the laying of the keel. This is done in a drydock or on land. Once the hull is assembled and painted, it is launched. The last stages, such as raising the superstructure and adding equipment and accommodation, can be done after the vessel is afloat.

Once completed, the vessel is delivered to the customer. Ship launching is often a ceremony of some significance, and is usually when the vessel is formally named. A typical small rowboat can cost under US$100, $1,000 for a small speedboat, tens of thousands of dollars for a cruising sailboat, and about $2,000,000 for a Vendée Globe class sailboat. A trawler may cost $2.5 million, and a 1,000-person-capacity high-speed passenger ferry can cost in the neighborhood of $50 million. A ship's cost partly depends on its complexity: a small, general cargo ship will cost $20 million, a Panamax-sized bulk carrier around $35 million, a supertanker around $105 million and a large LNG carrier nearly $200 million. The most expensive ships generally are so because of the cost of embedded electronics: a costs around $2 billion, and an aircraft carrier goes for about $3.5 billion.

Ships undergo nearly constant maintenance during their career, whether they be underway, pierside, or in some cases, in periods of reduced operating status between charters or shipping seasons.

Most ships, however, require trips to special facilities such as a drydock at regular intervals. Tasks often done at drydock include removing biological growths on the hull, sandblasting and repainting the hull, and replacing sacrificial anodes used to protect submerged equipment from corrosion. Major repairs to the propulsion and steering systems as well as major electrical systems are also often performed at dry dock.

Some vessels that sustain major damage at sea may be repaired at a facility equipped for major repairs, such as a shipyard. Ships may also be converted for a new purpose: oil tankers are often converted into floating production storage and offloading units.

Most ocean-going cargo ships have a life expectancy of between 20 and 30 years. A sailboat made of plywood or fiberglass can last between 30 and 40 years. Solid wooden ships can last much longer but require regular maintenance. Carefully maintained steel-hulled yachts can have a lifespan of over 100 years.

As ships age, forces such as corrosion, osmosis, and rotting compromise hull strength, and a vessel becomes too dangerous to sail. At this point, it can be scuttled at sea or scrapped by shipbreakers. Ships can also be used as museum ships, or expended to construct breakwaters or artificial reefs.

Many ships do not make it to the scrapyard, and are lost in fires, collisions, grounding, or sinking at sea. The Allies lost some 5,150 ships during World War II.

One can measure ships in terms of overall length, length between perpendiculars, length of the ship at the waterline, beam (breadth), depth (distance between the crown of the weather deck and the top of the keelson), draft (distance between the highest waterline and the bottom of the ship) and tonnage. A number of different tonnage definitions exist and are used when describing merchant ships for the purpose of tolls, taxation, etc.

In Britain until Samuel Plimsoll's Merchant Shipping Act of 1876, ship-owners could load their vessels until their decks were almost awash, resulting in a dangerously unstable condition. Anyone who signed on to such a ship for a voyage and, upon realizing the danger, chose to leave the ship, could end up in jail. Plimsoll, a Member of Parliament, realised the problem and engaged some engineers to derive a fairly simple formula to determine the position of a line on the side of any specific ship's hull which, when it reached the surface of the water during loading of cargo, meant the ship had reached its maximum safe loading level. To this day, that mark, called the "Plimsoll Line", exists on ships' sides, and consists of a circle with a horizontal line through the centre. On the Great Lakes of North America the circle is replaced with a diamond. Because different types of water (summer, fresh, tropical fresh, winter north Atlantic) have different densities, subsequent regulations required painting a group of lines forward of the Plimsoll mark to indicate the safe depth (or freeboard above the surface) to which a specific ship could load in water of various densities. Hence the "ladder" of lines seen forward of the Plimsoll mark to this day. This is called the "freeboard mark" or "load line mark" in the marine industry.

Ship pollution is the pollution of air and water by shipping. It is a problem that has been accelerating as trade has become increasingly globalized, posing an increasing threat to the world’s oceans and waterways as globalization continues. It is expected that "shipping traffic to and from the United States is projected to double by 2020." Because of increased traffic in ocean ports, pollution from ships also directly affects coastal areas. The pollution produced affects biodiversity, climate, food, and human health. However, the degree to which humans are polluting and how it affects the world is highly debated and has been a hot international topic for the past 30 years.

Oil spills have devastating effects on the environment. Crude oil contains polycyclic aromatic hydrocarbons (PAHs) which are very difficult to clean up, and last for years in the sediment and marine environment. Marine species constantly exposed to PAHs can exhibit developmental problems, susceptibility to disease, and abnormal reproductive cycles.

By the sheer amount of oil carried, modern oil tankers must be considered something of a threat to the environment. An oil tanker can carry of crude oil, or . This is more than six times the amount spilled in the widely known "Exxon Valdez" incident. In this spill, the ship ran aground and dumped of oil into the ocean in March 1989. Despite efforts of scientists, managers, and volunteers, over 400,000 seabirds, about 1,000 sea otters, and immense numbers of fish were killed.

The International Tanker Owners Pollution Federation has researched 9,351 accidental spills since 1974. According to this study, most spills result from routine operations such as loading cargo, discharging cargo, and taking on fuel oil. 91% of the operational oil spills were small, resulting in less than 7 tons per spill. Spills resulting from accidents like collisions, groundings, hull failures, and explosions are much larger, with 84% of these involving losses of over 700 tons.

Following the "Exxon Valdez" spill, the United States passed the Oil Pollution Act of 1990 (OPA-90), which included a stipulation that all tankers entering its waters be double-hulled by 2015. Following the sinkings of "Erika" (1999) and "Prestige" (2002), the European Union passed its own stringent anti-pollution packages (known as Erika I, II, and III), which require all tankers entering its waters to be double-hulled by 2010. The Erika packages are controversial because they introduced the new legal concept of "serious negligence".

When a large vessel such as a container ship or an oil tanker unloads cargo, seawater is pumped into other compartments in the hull to help stabilize and balance the ship. During loading, this ballast water is pumped out from these compartments.

One of the problems with ballast water transfer is the transport of harmful organisms. Meinesz believes that one of the worst cases of a single invasive species causing harm to an ecosystem can be attributed to a seemingly harmless jellyfish. "Mnemiopsis leidyi", a species of comb jellyfish that inhabits estuaries from the United States to the Valdés peninsula in Argentina along the Atlantic coast, has caused notable damage in the Black Sea. It was first introduced in 1982, and thought to have been transported to the Black Sea in a ship's ballast water. The population of the jellyfish shot up exponentially and, by 1988, it was wreaking havoc upon the local fishing industry. "The anchovy catch fell from in 1984 to in 1993; sprat from in 1984 to in 1993; horse mackerel from in 1984 to zero in 1993." Now that the jellyfish have exhausted the zooplankton, including fish larvae, their numbers have fallen dramatically, yet they continue to maintain a stranglehold on the ecosystem. Recently the jellyfish have been discovered in the Caspian Sea. Invasive species can take over once occupied areas, facilitate the spread of new diseases, introduce new genetic material, alter landscapes and jeopardize the ability of native species to obtain food. "On land and in the sea, invasive species are responsible for about 137 billion dollars in lost revenue and management costs in the U.S. each year."

Ballast and bilge discharge from ships can also spread human pathogens and other harmful diseases and toxins potentially causing health issues for humans and marine life alike. Discharges into coastal waters, along with other sources of marine pollution, have the potential to be toxic to marine plants, animals, and microorganisms, causing alterations such as changes in growth, disruption of hormone cycles, birth defects, suppression of the immune system, and disorders resulting in cancer, tumors, and genetic abnormalities or even death.

Exhaust emissions from ships are considered to be a significant source of air pollution. "Seagoing vessels are responsible for an estimated 14 percent of emissions of nitrogen from fossil fuels and 16 percent of the emissions of sulfur from petroleum uses into the atmosphere." In Europe ships make up a large percentage of the sulfur introduced to the air, "as much sulfur as all the cars, lorries and factories in Europe put together". "By 2010, up to 40% of air pollution over land could come from ships." Sulfur in the air creates acid rain which damages crops and buildings. When inhaled, sulfur is known to cause respiratory problems and increase the risk of a heart attack.

Ship breaking or ship demolition is a type of ship disposal involving the breaking up of ships for scrap recycling, with the hulls being discarded in ship graveyards. Most ships have a lifespan of a few decades before there is so much wear that refitting and repair becomes uneconomical. Ship breaking allows materials from the ship, especially steel, to be reused.
In addition to steel and other useful materials, however, ships (particularly older vessels) can contain many substances that are banned or considered dangerous in developed countries. Asbestos and polychlorinated biphenyls (PCBs) are typical examples. Asbestos was used heavily in ship construction until it was finally banned in most of the developed world in the mid 1980s. Currently, the costs associated with removing asbestos, along with the potentially expensive insurance and health risks, have meant that ship-breaking in most developed countries is no longer economically viable. Removing the metal for scrap can potentially cost more than the scrap value of the metal itself. In most of the developing world, however, shipyards can operate without the risk of personal injury lawsuits or workers' health claims, meaning many of these shipyards may operate with high health risks. Furthermore, workers are paid very low rates with no overtime or other allowances. Protective equipment is sometimes absent or inadequate. Dangerous vapors and fumes from burning materials can be inhaled, and dusty asbestos-laden areas around such breakdown locations are commonplace.

Aside from the health of the yard workers, in recent years, ship breaking has also become an issue of major environmental concern. Many developing nations, in which ship breaking yards are located, have lax or no environmental law, enabling large quantities of highly toxic materials to escape into the environment and causing serious health problems among ship breakers, the local population and wildlife. Environmental campaign groups such as Greenpeace have made the issue a high priority for their campaigns.

<br>

Model ships

Lists

Ship sizes




</doc>
<doc id="27009" url="https://en.wikipedia.org/wiki?curid=27009" title="Soap opera">
Soap opera

A soap opera is a radio or television serial dealing especially with domestic situations and frequently characterized by melodrama and sentimentality. The term "soap opera" originated from radio dramas being sponsored by soap manufacturers.

BBC Radio's "The Archers", first broadcast in 1950, is the world's longest-running radio soap opera; the world's longest-running television soap opera is "Coronation Street", first broadcast on ITV in 1960.

A crucial element that defines the soap opera is the open-ended serial nature of the narrative, with stories spanning several episodes. One of the defining features that makes a television program a soap opera, according to Albert Moran, is "that form of television that works with a continuous open narrative. Each episode ends with a promise that the storyline is to be continued in another episode". In 2012, "Los Angeles Times" columnist Robert Lloyd wrote of daily dramas,

Although melodramatically eventful, soap operas such as this also have a luxury of space that makes them seem more naturalistic; indeed, the economics of the form demand long scenes, and conversations that a 22-episodes-per-season weekly series might dispense with in half a dozen lines of dialogue may be drawn out, as here, for pages. You spend more time even with the minor characters; the apparent villains grow less apparently villainous.

Soap opera storylines run concurrently, intersect and lead into further developments. An individual episode of a soap opera will generally switch between several different concurrent narrative threads that may at times interconnect and affect one another or may run entirely independent to each other. Episodes may feature some of the show's current storylines, but not always all of them. Especially in daytime serials and those that are broadcast each weekday, there is some rotation of both storyline and actors so any given storyline or actor will appear in some but usually not all of a week's worth of episodes. Soap operas rarely bring all the current storylines to a conclusion at the same time. When one storyline ends, there are several other story threads at differing stages of development. Soap opera episodes typically end on some sort of cliffhanger, and the season finale (if a soap incorporates a break between seasons) ends in the same way, only to be resolved when the show returns for the start of a new yearly broadcast.

Evening soap operas and those that air at a rate of one episode per week are more likely to feature the entire cast in each episode, and to represent all current storylines in each episode. Evening soap operas and serials that run for only part of the year tend to bring things to a dramatic end-of-season cliffhanger.

In 1976, "Time" magazine described American daytime television as "TV's richest market", noting the loyalty of the soap opera fan base and the expansion of several half-hour series into hour-long broadcasts in order to maximize ad revenues. The article explained that at that time, many prime time series lost money, while daytime serials earned profits several times more than their production costs. The issue's cover notably featured its first daytime soap stars, Bill Hayes and Susan Seaforth Hayes of "Days of Our Lives", a married couple whose onscreen and real-life romance was widely covered by both the soap opera magazines and the mainstream press at large.

The first serial considered to be a "soap opera" was "Painted Dreams", which debuted on October 20, 1930 on Chicago radio station WGN. Early radio series such as "Painted Dreams" were broadcast in weekday daytime slots, usually five days a week. Most of the listeners would be housewives, thus, the shows were aimed at and consumed by, a predominantly female audience. The first nationally broadcast radio soap opera was "Clara, Lu, and Em", which aired on the NBC Blue Network at 10:30 p.m. Eastern Time on January 27, 1931.

The main characteristics that define soap operas are "an emphasis on family life, personal relationships, sexual dramas, emotional and moral conflicts; some coverage of topical issues; set in familiar domestic interiors with only occasional excursions into new locations". Fitting in with these characteristics, most soap operas follow the lives of a group of characters who live or work in a particular place, or focus on a large extended family. The storylines follow the day-to-day activities and personal relationships of these characters. "Soap narratives, like those of film melodramas, are marked by what Steve Neale has described as 'chance happenings, coincidences, missed meetings, sudden conversions, last-minute rescues and revelations, deus ex machina endings.'" These elements may be found across the gamut of soap operas, from "EastEnders" to "Dallas".

In many soap operas, in particular daytime serials in the US, the characters are frequently attractive, seductive, glamorous and wealthy. Soap operas from the United Kingdom and Australia tend to focus on more everyday characters and situations, and are frequently set in working-class environments. Many of the soaps produced in those two countries explore social realist storylines such as family discord, marriage breakdown or financial problems. Both UK and Australian soap operas feature comedic elements, often affectionate comic stereotypes such as the gossip or the grumpy old man, presented as a comic foil to the emotional turmoil that surrounds them. This diverges from US soap operas where such comedy is rare. UK soap operas frequently make a claim to presenting "reality" or purport to have a "realistic" style. UK soap operas also frequently foreground their geographic location as a key defining feature of the show while depicting and capitalising on the exotic appeal of the stereotypes connected to the location. As examples, "EastEnders" focuses on the tough and grim life in the east end of London; "Coronation Street" and its characters exhibit the stereotypical characteristic of "northern straight talking".

Romance, secret relationships, extramarital affairs, and genuine hate have been the basis for many soap opera storylines. In US daytime serials, the most popular soap opera characters, and the most popular storylines, often involved a romance of the sort presented in paperback romance novels. Soap opera storylines weave intricate, convoluted and sometimes confusing tales of characters who have affairs, meet mysterious strangers and fall in love, and who commit adultery, all of which keeps audiences hooked on the unfolding story. Crimes such as kidnapping, rape, and even murder may go unpunished if the perpetrator is to be retained in the ongoing story.

Australian and UK soap operas also feature a significant proportion of romance storylines. In Russia, most popular serials explore the "romantic quality" of criminal and/or oligarch life.

In soap opera storylines, previously unknown children, siblings and twins (including the evil variety) of established characters often emerge to upset and reinvigorate the set of relationships examined by the series. Unexpected calamities disrupt weddings, childbirths, and other major life events with unusual frequency.

As in comic books – another popular form of linear storytelling pioneered in the US during the 20th century – a character's death is not guaranteed to be permanent. On "The Bold and the Beautiful", Taylor Forrester (Hunter Tylo) was shown to flatline and have a funeral. When Tylo reprised the character in 2005, a retcon explained that Taylor had actually gone into a coma.

Stunts and complex physical action are largely absent, especially from daytime serials. Such story events often take place off screen and are referred to in dialogue instead of being shown. This is because stunts or action scenes are difficult to adequately depict without complex movements, multiple takes, and post production editing. When episodes were broadcast live, post production work was impossible. Though all serials have long switched to being taped, extensive post production work and multiple takes, while possible, are not feasible due to the tight taping schedules and low budgets.

The first daytime TV soap opera in the United States was "These Are My Children" in 1949, though earlier melodramas had aired in the evenings as once-a-week programs. Soap operas quickly became a fixture of American daytime television in the early 1950s, joined by game shows, sitcom reruns and talk shows.

In 1988, H. Wesley Kenney, who at the time served as the executive producer of "General Hospital", said to "The New York Times":
Many long-running US soap operas established particular environments for their stories. "The Doctors" and "General Hospital", in the beginning, told stories almost exclusively from inside the confines of a hospital. "As the World Turns" dealt heavily with Chris Hughes' law practice and the travails of his wife Nancy who, tired of being "the loyal housewife" in the 1970s, became one of the first older women on the American serials to enter the workforce. "Guiding Light" dealt with Bert Bauer (Charita Bauer) and her alcoholic husband Bill, and their endless marital troubles. When Bert's status shifted to caring mother and town matriarch, her children's marital troubles were showcased. "Search for Tomorrow" mostly told its story through the eyes of Joanne Gardner (Mary Stuart). Even when stories revolved around other characters, Joanne was frequently a key player in their storylines. "Days of Our Lives" initially focused on Dr. Tom Horton and his steadfast wife Alice. The show later branched out to focus more on their five children. "The Edge of Night" featured as its central character Mike Karr, a police detective (later an attorney), and largely dealt with organized crime. "The Young and the Restless" first focused on two families, the prosperous Brooks family with four daughters, and the working class Foster family of a single working mother with three children. Its storylines explored realistic problems including cancer, mental illness, poverty, and infidelity.

In contrast, "Dark Shadows" (1966–1971), "Port Charles" (1997–2003) and "Passions" (1999-2008) featured supernatural characters and dealt with fantasy and horror storylines. Their characters included vampires, witches, ghosts, goblins, and angels.

The American soap opera "Guiding Light" (originally titled "The Guiding Light" until 1975) started as a radio drama in January 1937 and subsequently transferred to television in June 1952. With the exception of several years in the late 1940s, during which creator Irna Phillips was involved in a dispute with Procter & Gamble, "Guiding Light" was heard or seen nearly every weekday from 1937 until 2009, making it the longest story ever told in a broadcast medium.

Originally serials were broadcast as fifteen-minute installments each weekday in daytime slots. In 1956, "As the World Turns" and "The Edge of Night", both produced by Procter & Gamble Productions, debuted as the first half-hour soap operas on the CBS television network. All soap operas broadcast half-hour episodes by the end of the 1960s. With increased popularity in the 1970s, most soap operas had expanded to an hour in length by the end of the decade ("Another World" even expanded to 90 minutes for a short time). More than half of the serials had expanded to one-hour episodes by 1980. As of 2012, three of the four US serials air one-hour episodes each weekday; only "The Bold and the Beautiful" airs 30-minute episodes.

Soap operas were originally broadcast live from the studio, creating what many at the time regarded as a feeling similar to that of a stage play. As nearly all soap operas were originated at that time in New York City, a number of soap actors were also accomplished stage actors who performed live theatre during breaks from their soap roles. In the 1960s and 1970s, new serials such as "General Hospital", "Days of our Lives" and "The Young and the Restless" were produced in Los Angeles. Their success made the West Coast a viable alternative to New York-produced soap operas, which were becoming more costly to perform. By the early 1970s, nearly all soap operas had transitioned to being taped. "As the World Turns" and "The Edge of Night" were the last to make the switch, in 1975.

"Port Charles" used the practice of running 13-week "story arcs," in which the main events of the arc are played out and wrapped up over the 13 weeks, although some storylines did continue over more than one arc. According to the 2006 Preview issue of "Soap Opera Digest", it was briefly discussed that all ABC shows might do telenovela arcs, but this was rejected.

Though U.S. daytime soap operas are not generally rerun by their networks, occasionally they are rebroadcast elsewhere; CBS and ABC have made exceptions to this, airing older episodes (either those aired earlier in the current season or those aired years prior) on major holidays when special event programming is not scheduled. Early episodes of "Dark Shadows" were rerun on PBS member stations in the early 1970s after the show's cancellation, and the entire series (except for a single missing episode) was rerun on the Sci-Fi Channel in the 1990s. After "The Edge of Night"s 1984 cancellation, reruns of the show's final five years were shown late nights on USA Network from 1985 to 1989. On January 20, 2000, a digital cable and satellite network dedicated to the genre, SOAPnet, began re-airing soaps that originally aired on ABC, NBC and CBS.

Newer broadcast networks since the late 1980s, such as Fox and cable television networks, have largely eschewed soap operas in their daytime schedules, instead running syndicated programming and reruns. No cable television outlet has produced its own daytime serial, although DirecTV's The 101 Network took over existing serial "Passions", continuing production for one season; while TBS and CBN Cable Network respectively aired their own soap operas, "The Catlins" (a primetime soap that utilized the daily episode format of its daytime counterparts) and "Another Life" (a soap that combined standard serial drama with religious overtones), during the 1980s. Fox, the fourth "major network," carried a short lived daytime soap "Tribes" in 1990. Yet other than this and a couple of pilot attempts, Fox mainly stayed away from daytime soaps, and has not attempted them since their ascension to major-network status in 1994 (it did later attempt a series of daily prime time soaps, which aired on newly created sister network MyNetworkTV, but the experiment was largely a failure).

Due to the masses of episodes produced for a series, release of soap operas to DVD (a popular venue for distribution of current and vintage television series) is considered impractical. With the exception of occasional specials, daytime soap operas are notable by their absence from DVD release schedules (an exception being the supernatural soap opera, "Dark Shadows", which did receive an essentially complete release on both VHS and DVD; the single lost episode #1219 is reconstructed by means of an off-the-air audio recording, still images, and recap material from adjacent episodes).

Due to the longevity of these shows, it is not uncommon for a single character to be played by multiple actors. The key character of Mike Karr on "The Edge of Night" was played by three different actors.

Conversely, several actors have remained playing the same character for many years, or decades even. Helen Wagner played Hughes family matriarch Nancy Hughes on American soap "As the World Turns" from its April 2, 1956 debut through her death in May 2010. She is listed in the Guinness Book of World Records as the actor with the longest uninterrupted performance in a single role. A number of performers played roles for twenty years or longer, occasionally on more than one show. Rachel Ames played Audrey Hardy on both "General Hospital" and "Port Charles" from 1964 until 2007, and returned in 2009. Susan Lucci played Erica Kane in "All My Children" from the show's debut in January 1970 until it ended its network television run on ABC on September 23, 2011. Erika Slezak played Victoria Lord #3 on "One Life to Live" from 1971 until the show ended its network television run on ABC on January 13, 2012 and resumed the role in its short-lived online revival on April 29, 2013.

Other actors have played several characters on different shows. Millette Alexander, Bernard Barrow, Doris Belack, David Canary, Judith Chapman, Jordan Charney, Joan Copeland, Nicolas Coster, Jacqueline Courtney, Louis Edmonds, Dan Hamilton, Don Hastings, Vincent Irizarry, Lenore Kasdorf, Teri Keane, Lois Kibbee, John Loprieno, Maeve McGuire, James Mitchell, Christopher Pennock, Antony Ponzini, William Prince, Louise Shaffer, and Diana van der Vlis, among many others, have all played multiple soap roles.

For several decades, most daytime soap operas concentrated on family and marital discord, legal drama and romance. The action rarely left interior settings, and many shows were set in fictional, medium-sized Midwestern towns.

Exterior shots were slowly incorporated into the series "The Edge of Night" and "Dark Shadows". Unlike many earlier serials that were set in fictional towns, "The Best of Everything" and "Ryan's Hope" were set in a real-world location, New York City.

The first exotic location shoot was made by "All My Children", to St. Croix in 1978. Many other soap operas planned lavish storylines after the success of the "All My Children" shoot. Soap operas "Another World" and "Guiding Light" both went to St. Croix in 1980, the former show culminating a long-running storyline between popular characters Mac, Rachel and Janice, and the latter to serve as an exotic setting for Alan Spaulding and Rita Bauer's torrid affair. "Search for Tomorrow" taped for two weeks in Hong Kong in 1981. Later that year, some of the cast and crew ventured to Jamaica to tape a love consummation storyline between the characters of Garth and Kathy.

During the 1980s, perhaps as a reaction to the evening drama series that were gaining high ratings, daytime serials began to incorporate action and adventure storylines, more big-business intrigue, and an increased emphasis on youthful romance.

One of the first and most popular couples was Luke Spencer and Laura Webber on "General Hospital". Luke and Laura helped to attract both male and female fans. Even actress Elizabeth Taylor was a fan and at her own request was given a guest role in Luke and Laura's wedding episode. Luke and Laura's popularity led to other soap producers striving to reproduce this success by attempting to create supercouples of their own.

With increasingly bizarre action storylines coming into vogue, Luke and Laura saved the world from being frozen, brought a mobster down by finding his black book in a Left-Handed Boy Statue, and helped a Princess find her Aztec Treasure in Mexico. Other soap operas attempted similar adventure storylines, often featuring footage shot on location – frequently in exotic locales.

During the 1990s, the mob, action and adventure stories fell out of favor with producers, due to generally declining ratings for daytime soap operas at the time, and the resultant budget cuts. In addition, soap operas were no longer able to go on expensive location shoots overseas as they were able to do in the 1980s. During that decade, soap operas increasingly focused on younger characters and social issues, such as Erica Kane's drug addiction on "All My Children", the re-emergence of Viki Lord's multiple personality disorder on "One Life to Live", and Stuart Chandler dealing with his wife Cindy dying of AIDS on "All My Children". Other social issues included cancer, rape, abortion, homophobia, and racism.

Some shows during the 2000s incorporated supernatural and science fiction elements into their storylines. One of the main characters on the earlier soap opera "Dark Shadows" was Barnabas Collins, a vampire, and "One Life to Live" featured an angel named Virgil. Both shows featured characters who traveled to and from the past.

Modern U.S. daytime soap operas largely stay true to the original soap opera format. The duration and format of storylines and the visual grammar employed by U.S. daytime serials set them apart from soap operas in other countries and from evening soap operas. Stylistically, UK and Australian soap operas, which are usually produced for early evening timeslots, fall somewhere in between U.S. daytime and evening soap operas. Similar to U.S. daytime soap operas, UK and Australian serials are shot on videotape, and the cast and storylines are rotated across the week's episodes so that each cast member will appear in some but not all episodes. UK and Australian soap operas move through storylines at a faster rate than daytime serials, making them closer to U.S. evening soap operas in this regard.

American daytime soap operas feature stylistic elements that set them apart from other shows:

Soap opera ratings have significantly fallen in the U.S. since the 2000s. No new major daytime soap opera has been created since "Passions" in 1999, while many have been cancelled. Since January 2012, four daytime soap operas – "General Hospital", "Days of Our Lives", "The Young and the Restless" and "The Bold and the Beautiful" – continue to air on the three major networks, down from a total of 12 during the 1990–91 season and a high of 19 in the 1969–70 season. This marks the first time since 1953 that there have been only four soap operas airing on broadcast television. "The Young and the Restless", the highest-rated soap opera from 1988 to the present, had fewer than 5 million daily viewers as of February 2012, a number exceeded by several non-scripted programs such as "Judge Judy". Circulations of soap opera magazines have decreased and some have even ceased publication. SOAPnet, which largely aired soap opera reruns, began to be phased out in 2012 and fully ceased operations the following year. The Daytime Emmy Awards, which honor soap operas and other daytime shows, moved from primetime network television to smaller cable channels in 2012, then failed to get any TV broadcast at all in 2014, 2016, and 2017.

Several of the U.S.’s most established soaps ended between 2009 and 2012. The longest-running drama in television and radio history, "Guiding Light", barely reached 2.1 million daily viewers in 2009 and ended on September 18 of that year, after a 72-year run. "As the World Turns" aired its final episode on September 17, 2010 after a 54-year run. "As the World Turns" was the last of 20 soap operas produced by Procter & Gamble, the soap and consumer goods company from which the genre got its name. "As The World Turns" and "Guiding Light" were also among the last of the soaps that originated from New York City. "All My Children", another New York-based soap, moved its production out to Los Angeles in an effort to reduce costs and raise sagging ratings; however, both it and "One Life to Live", each with a four-decade-plus run, were cancelled in 2011. "All My Children" aired its network finale in September 2011 with "One Life to Live" following suit in January 2012. Both "All My Children" and "One Life to Live" were briefly revived online in 2013, before being canceled again that same year. In 2019, "Days of Our Lives" was put on "indefinite hiatus" and all of the cast's contracts were terminated.

As women increasingly worked outside of the home, daytime television viewing declined. New generations of potential viewers were not raised watching soap operas with their mothers, leaving the shows' long and complex storylines foreign to younger audiences. Now, as viewers age, ratings continue to drop among young adult women, the demographic group that soap opera advertisers pay the most for. Those who might watch in workplace breakrooms are not counted, as Nielsen does not track television viewing outside the home. The rise of cable and the internet has also provided new sources of entertainment during the day. The genre's decline has additionally been attributed to reality television displacing soap operas as TV's dominant form of melodrama. An early term for the reality TV genre was "docu-soap". A precursor to reality TV, the televised 1994–95 O. J. Simpson murder case, both preempted and competed with an entire season of soaps, transforming viewing habits and leaving soap operas with 10 percent fewer viewers after the trial ended.

Daytime programming alternatives such as talk shows, game shows, and court shows cost up to 50% less to produce than scripted dramas, making those formats more profitable and attractive to networks, even if they receive the same or slightly lower ratings than soap operas. A network may even prefer to return a timeslot to its local stations to keeping a soap opera with disappointing ratings on the air, as was the case with "Sunset Beach" and "Port Charles". Compounding the financial pressure on scripted programming in the 2007–2010 period was a decline in advertising during the Great Recession, which led shows to reduce their budgets and cast sizes. In addition to these external factors, a litany of production decisions has been cited by soap opera fans as contributing to the genre's decline, such as cliched plots, a lack of diversity that narrowed audience appeal, and the elimination of core families.

Serials produced for primetime slots have also found success. The first primetime soap opera was "Faraway Hill" (1946), which aired on October 2, 1946, on the now-defunct DuMont Television Network. "Faraway Hill" ran for 12 episodes and was primarily broadcast live, interspersed with short pre-recorded film clips and still photos to remind the audience of the previous week's episode.

The first long-running prime time soap opera was "Peyton Place" (1964–1969) on ABC. It was based in part on the eponymous 1957 film (which, in turn, was based on the 1956 novel).

The popularity of "Peyton Place" prompted the CBS network to spin-off popular "As the World Turns" character Lisa Miller into her own evening soap opera, "Our Private World" (originally titled "The Woman Lisa" in its planning stages). "Our Private World" was broadcast from May to September 1965. The character of Lisa (and her portrayer Eileen Fulton) returned to "As The World Turns" after the series ended.

The structure of "Peyton Place", with its episodic plots and long-running story arcs, set the mold for the primetime serials of the 1980s when the format reached its pinnacle.

The successful primetime serials of the 1980s included "Dallas", "Dynasty", "Knots Landing" and "Falcon Crest". These shows frequently dealt with wealthy families, and their personal and big-business travails. Common characteristics were sumptuous sets and costumes, complex storylines examining business schemes and intrigue, and spectacular disaster cliffhanger situations. Each of these series featured a wealthy, domineering, promiscuous, and passionate antagonist as a key character in the storyline – respectively, J. R. Ewing, Alexis Colby, Abby Cunningham and Angela Channing. These villainous schemers became immensely popular figures that audiences "loved to hate".

Unlike daytime serials, which are shot on video in a studio using the multi-camera setup, these evening series were shot on film using a single camera setup, and featured substantial location-shot footage, often in picturesque locales. "Dallas", its spin-off "Knots Landing", and "Falcon Crest" all initially featured episodes with self-contained stories and specific guest stars who appeared in just that episode. Each story was completely resolved by the end of the episode, and there were no end-of-episode cliffhangers. After the first couple of seasons, all three shows changed their story format to that of a pure soap opera, with interwoven ongoing narratives that ran over several episodes. "Dynasty" featured this format throughout its run.

The soap opera's distinctive open plot structure and complex continuity was increasingly incorporated into American primetime television programs of the period. The first significant drama series to do this was "Hill Street Blues". This series, produced by Steven Bochco, featured many elements borrowed from soap operas, such as an ensemble cast, multi-episode storylines, and extensive character development over the course of the series. It and the later "Cagney & Lacey" overlaid the police series formula with ongoing narratives exploring the personal lives and interpersonal relationships of the regular characters. The success of these series prompted other drama series, such as "St. Elsewhere" and situation comedy series, to incorporate serialized stories and story structure to varying degrees.

The primetime soap operas and drama series of the 1990s, such as "Beverly Hills, 90210", "Melrose Place", "Party of Five", "The OC", and "Dawson's Creek", focused more on younger characters. In the 2000s, ABC began to revitalize the primetime soap opera format with shows such as "Desperate Housewives", "Grey's Anatomy", "Brothers & Sisters", "Ugly Betty", "Private Practice", and more recently "Revenge", "Nashville", "Scandal", "Mistresses", and formerly "Ringer", which its sister production company ABC Studios co-produced with CBS Television Studios for The CW. While not soaps in the traditional sense, these shows managed to appeal to wide audiences with their high drama mixed with humor, and are soap operas by definition. These successes led to NBC's launching serials, including "Heroes" and "Friday Night Lights". The upstart MyNetworkTV, a sister network of Fox, launched a line of primetime telenovelas (a genre similar to soap operas in terms of content) upon its launch in September 2006, but discontinued its use of the format in 2007, after disappointing ratings.

On June 13, 2012, "Dallas", a continuation of the 1978 original series premiered on the cable network, TNT. The revived series, which was canceled after three seasons in 2014, delivered solid ratings for the channel, only losing viewership after the show's most established star, Larry Hagman, died midway through the series. In 2012, Nick at Nite debuted a primetime soap opera, "Hollywood Heights", which aired episodes five nights a week (on Monday through Fridays) in a manner similar to a daytime soap opera, instead of the once-a-week episode output common of other primetime soaps. The series, which was an adaptation of the Mexican telenovela "Alcanzar una estrella", suffered from low ratings (generally receiving less than one million viewers) and was later moved to sister cable channel TeenNick halfway through its run to burn off the remaining episodes.

In 2015, Fox debuted "Empire", a primetime musical serial centering on the power struggle between family members within the titular recording company. Created by Lee Daniels and Danny Strong and led by Oscar nominees Terrence Howard and Taraji P. Henson, the drama premiered to high ratings. The show is strongly influenced by other works such as William Shakespeare's "King Lear", James Goldman's "The Lion in Winter" and the 1980s soap opera "Dynasty". Also in 2015, E! introduced "The Royals", a series following the life and drama of a fictional English Royal family, which was also inspired by "Dynasty" (even featuring Joan Collins as the Queen's mother). In addition, ABC debuted a primetime soap opera "Blood & Oil", following a young couple looking to make money off the modern-day Williston oil boom, premiering on September 27, 2015 during the 2015-16 TV season.

The telenovela, a shorter-form format of serial melodrama, shares some thematic and especially stylistic similarity to the soap opera, enough that the colloquialism "Spanish soap opera" has arisen to describe the format. The chief difference between the two is length of series; while soap operas usually have indefinite runs, telenovelas typically have a central story arc with a prescribed ending within a year or two of the show's launch, requiring more concise storytelling.

Spanish-language networks, chiefly Univision and Telemundo, have found success airing telenovelas for the growing U.S. Hispanic market. Both originally produced and imported Latin American dramas are popular features of the networks' daytime and primetime lineups, sometimes beating English-language networks in the ratings.

Some web series are soap operas, such as "" or "". In 2013, production company Prospect Park revived "All My Children" and "One Life to Live" for the web, retaining original creator Agnes Nixon as a consultant and keeping many of the same actors (Prospect Park purchased the rights to both series months after their cancellations by ABC in 2011, although it initially suspended plans to relaunch the soaps later that same year due to issues receiving approval from acting and production unions). Each show initially produced four half-hour episodes a week, but quickly cut back to two half-hour episodes each. In the midst of (though not directly related to) a lawsuit between Prospect Park and ABC, the experiment ended that same year, with both shows being canceled again.

As of 2017, Turkey is the second largest exporter of television soap operas. In 2016, Turkish TV exports earned $350 million, making it the second largest drama exporter in the world behind the United States. Turkish soap operas have a large following across Asia, the Balkans, Eastern Europe, Latin America, the Middle East, and Africa.

Soap operas in the UK began on radio and consequently were associated with the BBC. It had resisted soaps as antithetical to its quality image, but began broadcasting "Front Line Family" in April 1941 on its North American shortwave service to encourage American intervention on Britain's behalf in World War II. The BBC continues to broadcast the world's longest-running radio soap, "The Archers", which first aired in May 1950, and has been running nationally since 1951. It is currently broadcast on BBC Radio 4 and continues to attract over five million listeners, or roughly 25% of the radio listening population of the UK at that time of the evening.

In the UK, soap operas are one of the most popular genres, with most being broadcast during prime time. Most UK soap operas focus on everyday, working-class communities, influenced by the conventions of the kitchen sink drama. The most popular British soap operas are "EastEnders", "Coronation Street", "Emmerdale", "Hollyoaks", "Doctors", and the Australian produced "Neighbours" and "Home and Away". The first three of these are consistently among the highest-rated shows on British television. Such is the magnitude of the popularity of the soap genre in the UK that all television serials in the country are reputedly enjoyed by members of the British Royal Family, including Elizabeth II herself. Major events in British culture are often mentioned in the storyline, such as England's participation at the World Cup, and the death of Princess Diana. Since 1999, The British Soap Awards has been televised on ITV.

The 1986 Christmas Day episode of "EastEnders" is often referred to as the highest-rated UK soap opera episode ever, with 30.15 million viewers (more than half the population at the time). The figure of 30.15 million was actually a combination of the original broadcast, which had just over 19 million viewers, and the Sunday omnibus edition with 10 million viewers. The combined 30.15 million audience figure makes the aforementioned Christmas Day 1986 episode of "EastEnders" the highest-rated single-channel broadcast in the history of UK television. Overall it ranks third behind the 1966 FIFA World Cup Final (32.3 million viewers) and Princess Diana's funeral in 1997 (32.1 million viewers) which were transmitted on both BBC One and ITV.

An early television serial was "The Grove Family" on the BBC, which produced 148 episodes from 1954 to 1957. The programme was broadcast live and only a handful of recordings were retained in the archives. The UK's first twice-weekly serial was ITV's "Emergency - Ward 10", running from 1957 until 1967.

In the 1960s, "Coronation Street" revolutionised UK television and quickly became a British institution. On 17 September 2010, it became the world's longest-running television soap opera and was listed in "Guinness World Records". The BBC also produced several serials: "Compact" was about the staff of a women's magazine; "The Newcomers" was about the upheaval caused by a large firm setting up a plant in a small town; "United!" contained 147 episodes and focused on a football team; "199 Park Lane" (1965) was an upper class serial, which ran for only 18 episodes. None of these serials came close to making the same impact as "Coronation Street". Indeed, most of the 1960s BBC serials were largely wiped.

During the 1960s, "Coronation Street"s main rival was "Crossroads", a daily serial that began in 1964 and aired on ITV in the early evening. "Crossroads" was set in a Birmingham motel and, although the program was popular, its purported low technical standard and bad acting were much mocked. By the 1980s, its ratings had begun to decline. Several attempts to revamp the program through cast changes and, later, expanding the focus from the motel to the surrounding community were unsuccessful. "Crossroads" was cancelled in 1988 (a new version of "Crossroads" was later produced, running from 2001 until 2003).

A later rival to "Coronation Street" was ITV's "Emmerdale Farm" (later renamed "Emmerdale"), which began in 1972 in a daytime slot and was set in rural Yorkshire. Increased viewership resulted in "Emmerdale" being moved to a prime-time slot in the 1980s.

"Pobol y Cwm" ("People of the Valley") is a Welsh language serial that has been produced by the BBC since October 1974, and is the longest-running television soap opera produced by the broadcaster. "Pobol y Cwm" was originally broadcast on BBC Wales television from 1974 to 1982; it was then moved to the Welsh-language television station S4C when it opened in November 1982. The program was occasionally shown on BBC1 in London during periods of regional optout in the mid- to late 1970s. "Pobol y Cwm" was briefly shown in the rest of the UK in 1994 on BBC2, with English subtitles; it is consistently the most watched programme each week on S4C.

Daytime soap operas were non-existent until the 1970s because there was virtually no daytime television in the UK. ITV introduced "General Hospital", which later moved to a prime time slot. In 1980, Scottish Television debuted "Take the High Road", which lasted for over twenty years. Later, daytime slots were filled with an influx of Australian soap operas such as "The Sullivans" (aired on ITV from 1977), "The Young Doctors" (from 1982), "Sons and Daughters" (from 1983), "A Country Practice" (from 1982), "Richmond Hill" (from 1988 to 1989) and eventually, "Neighbours" was acquired by the BBC in 1986, and "Home and Away" aired on ITV beginning in 1989. These achieved significant levels of popularity; "Neighbours" and "Home and Away" were moved to early-evening slots, helping begin the UK soap opera boom in the late 1980s.

The day Channel 4 began operations in 1982 it launched its own soap, the Liverpool-based "Brookside", which would redefine soaps over the next decade. The focus of "Brookside" was different from earlier soap operas in the UK; it was set in a middle-class new-build cul-de-sac, unlike "Coronation Street" and "Emmerdale Farm", which were set in established working-class communities. The characters in "Brookside" were generally either people who had advanced themselves from inner-city council estates, or the upper middle-class who had fallen on hard times. Though "Brookside" was still broadcast in a pre-watershed slot (8.00 p.m. and 8.30 p.m. on weekdays, around 5.00 p.m. for the omnibus on Saturdays), it was more liberal than other soaps of the time: the dialogue regularly included expletives. This stemmed from the overall more liberal policy of the channel during that period. The soap was also heavily politicised. Bobby Grant (Ricky Tomlinson), a militant trade-unionist anti-hero, was the most overtly political character. Storylines were often more sensationalist than on other soaps (throughout the soap's history, there were two armed sieges on the street) and were staged with more violence (particularly, rape) often being featured.

In 1985, the BBC's "EastEnders" debuted and became a near instant success with viewers and critics alike, with the first episode attracting over 17 million viewers. The Christmas Day 1986 episode was watched by 30.15 million viewers and contained a scene in which divorce papers were served to Angie Watts (Anita Dobson) by her husband, Queen Vic landlord Den (Leslie Grantham).

A notable success in pioneering late-night broadcasting, in October 1984, Yorkshire Television began airing the cult Australian soap opera "Prisoner", which originally ran from 1979 to 1986. It was eventually broadcast on all regions of the UK in differing slots, usually around 23:00 (but never before 22:30 in any region), under the title "Prisoner: Cell Block H". It was probably most popular in the Midlands where Central Television consistently broadcast the serial three times a week from 1987 to 1991. Its airing in the UK was staggered, so different regions of the country saw it at a different pace. The program was immensely successful, regularly achieving 10 million viewers when all regions' ratings per episode were added together. Central bowed to fan pressure to repeat the soap, of which the first 95 episodes aired. Then, rival station Channel 5 also acquired rights to repeat the entire rerun of the program, starting in 1997. All 692 episodes have since been released on DVD in the UK.

In 1992, the BBC made "Eldorado" to daily alternate with "EastEnders". The programme was heavily criticised and only lasted one year. Nevertheless, soap operas gained increasing prominence on UK television schedules. In 1995, Channel 4 premiered "Hollyoaks", a soap with a youth focus. When Channel 5 launched in March 1997, it debuted the soap opera "Family Affairs", which was formatted as a week-daily soap, airing Monday through Fridays.

"Brookside"s premise evolved during the 1990s, phasing out the politicised stories of the 1980s and shifting the emphasis to controversial and sensationalist stories such as child rape, sibling incest, religious cults and drug addiction, including the infamous 'body under the patio' storyline that ran from 1993 to 1995, and gave the serial its highest ratings ever with 9 million viewers.

"Coronation Street" and "Brookside" began releasing straight-to-video features. The "Coronation Street" releases generally kept the pace and style of conventional programs episodes with the action set in foreign locations. The "Brookside" releases were set in the usual locations, but featured stories with adult content not allowed on television pre-watershed, with these releases given '18' certificates.

"Emmerdale Farm" was renamed "Emmerdale" in 1989. The series was revamped in 1993 with many changes executed via the crash of a passenger jet that partially destroyed the village and killed several characters. This attracted criticism as it was broadcast near the fifth anniversary of the Lockerbie bombing. The storyline drew the soap its highest ever audience of 18 million viewers. The revamp was a success and "Emmerdale" grew in popularity.

Throughout the 1990s, "Brookside", "Coronation Street", "EastEnders" and "Emmerdale" continued to flourish. Each increased the number of episodes that aired on a weekly basis by at least one, further defining soap operas as the leading genre in British television.

Since 2000, new soap operas have continued to be developed. Daytime drama "Doctors" began in March 2000, preceding "Neighbours" on BBC One. In 2002, as ratings for the Scottish serial "High Road" (formerly "Take The High Road") continued to decline, BBC Scotland launched "River City", which proved popular and effectively replaced "High Road" when it was cancelled in 2003. The long-running serial "Brookside" ended in November 2003 after 21 years on the air, leaving "Hollyoaks" as Channel 4's flagship serial.

A new version of "Crossroads" featuring a mostly new cast was produced by Carlton Television for ITV in 2001. It did not achieve high ratings and was cancelled in 2003. In 2001, ITV also launched a new early-evening serial entitled "Night and Day". This program too attracted low viewership and, after being shifted to a late night time slot, was cancelled in 2003. "Family Affairs", which was broadcast opposite the racier "Hollyoaks", never achieved significantly high ratings leading to several dramatic casting revamps and marked changes in style and even location over its run. By 2004, "Family Affairs" had a larger fan base and won its first awards, but was cancelled in late 2005.

In 2008, ITV premiered "The Royal Today", a daily spin-off of popular 1960s-based drama "The Royal", which had been running in a primetime slot since 2002. Just days later, soap opera parody programs "Echo Beach" premiered alongside its sister show, the comedy "Moving Wallpaper". Both "Echo Beach" and "The Royal Today" ended after just one series due to low ratings. Radio soap opera "Silver Street" debuted on the BBC Asian Network in 2004. Poor ratings and criticism of the program led to its cancellation in 2010.

UK soap operas for many years usually only aired two nights a week. The exception was the original "Crossroads", which began as a week-daily soap opera in the 1960s, but later had its number of weekly broadcasts reduced.

In 1989 "Coronation Street" began airing three times a week. In 1996 it expanded to four episodes a week.

"Brookside" premiered in 1982 with two episodes a week. In 1990 it expanded to three episodes a week.

"EastEnders" increased its number of episode a week in 1994 and "Emmerdale" did so in 1997.

"Family Affairs" debuted as a weekdaily soap in 1997, producing five episodes a week its entire run.

In 2004, "Emmerdale" began airing six episodes a week.

In a January 2008 overhaul of the ITV network, the Sunday episodes of "Coronation Street" and "Emmerdale" were moved out of their slots. "Coronation Street" added a second episode on Friday evenings at 8:30 p.m. "Emmerdale"s Tuesday edition was extended to an hour, putting it in direct competition with "EastEnders". In July 2009, the schedules of these serials were changed again. On 23 July 2009, "Coronation Street" moved from the Wednesday slot it held for 49 years, to Thursday evenings. "Emmerdale" reverted to running just one 30-minute episode on Tuesday evenings and the other 30-minute installment was moved to Thursday evenings. "Coronation Street" later returned to a Wednesday slot, to air Mondays, Wednesdays and Fridays at 19:30 and 20:30. "Emmerdale" airs at 19:00 every weeknight, and 20:00 on Thursdays.

Later, "Coronation Street" (which began airing two episodes on Monday nights in 2002) produced five episodes a week.

It was announced in June 2016 that starting late 2017, Coronation Street would air six episodes a week.

"Doctors" airs five episodes a week, and is the only soap without a weekend omnibus repeat screening. "Hollyoaks" produces five episodes a week. The imported "Neighbours" screens as five new episodes a week. As of 2019, "EastEnders" produces four episode a week.

UK soap operas are shot on videotape in the studio using a multi-camera setup. In their early years "Coronation Street" and "Emmerdale" used 16 mm film for footage shot on location. Since the 1980s UK soap opera have routinely featured scenes shot outdoors in each episode. This footage is shot on videotape on a purpose-built outdoor set that represents the community that the soap focuses on.

"Hollyoaks" and "Family Affairs" were taped on high-definition video, and used the filmizing process.

Australia has had quite a number of well-known soap operas, some of which have gained cult followings in the United Kingdom, New Zealand and other countries. The majority of Australian television soap operas are produced for early evening or evening timeslots. They usually produce two or two-and-a-half hours of new material each week, either arranged as four or five half-hour episodes a week, or as two one-hour episodes.

Stylistically, these series most closely resemble UK soap operas in that they are nearly always shot on videotape, are mainly recorded in a studio and use a multi-camera setup. The original Australian serials were shot entirely in-studio. During the 1970s occasional filmed inserts were used to incorporate sequences shot outdoors. Outdoor shooting later became commonplace and starting in the late 1970s, it became standard practice for some on-location footage to be featured in each episode of any Australian soap opera, often to capitalise on the attractiveness and exotic nature of these locations for international audiences. Most Australian soap operas focus on a mixed age range of middle-class characters and will regularly feature a range of locations where the various, disparate characters can meet and interact, such as the café, the surf club, the wine bar or the school.

The genre began in Australia on radio, as it had in the United States and the United Kingdom. One such radio serial, "Big Sister", featured actress Thelma Scott in the cast and aired nationally for five years beginning in 1942. Probably the best known Australian radio serial was the long-running soap opera "Blue Hills", which was created by Gwen Meredith and ran from 1949 to 1976. With the advent of Australian television in 1956, daytime television serials followed. The first Australian television soap opera was "Autumn Affair" (1958) featuring radio personality and "Blue Hills" star Queenie Ashton making the transition to television. Each episode of this serial ran for 15 minutes and aired each weekday on the Seven Network. "Autumn Affair" failed to secure a sponsor and ended in 1959 after 156 episodes. It was followed by "The Story of Peter Grey" (1961), another Seven Network weekday series aired in a daytime slot in 15-minute installments. "The Story of Peter Grey" ran for 164 episodes.

The first successful wave of Australian evening television soap operas started in 1967 with "Bellbird", produced by the Australian Broadcasting Corporation. This rural-based serial screened in an early evening slot in 15-minute installments as a lead-in to the evening news. "Bellbird" was a moderate success but built-up a consistent and loyal viewer base, especially in rural areas, and enjoyed a ten-year run. "Motel" (1968) was Australia's first half-hour soap opera; the daytime soap had a short run of 132 episodes.

The first major soap opera hit in Australia was the sex-melodrama "Number 96", a nighttime series produced by Cash Harmon Television for Network Ten, which debuted March 1972. The program dealt with such topics as homosexuality, adultery, drug use, rape-within-marriage and racism, which had rarely been explored on Australian television programs before. The series became famous for its sex scenes and nudity and for its comedic characters, many of whom became cult heroes in Australia. By 1973, "Number 96" had become Australia's highest-rated show. In 1974, the sexed-up antics of "Number 96" prompted the creation of "The Box", which rivaled it in terms of nudity and sexual situations and was scheduled in a nighttime slot. Produced by Crawford Productions, many critics considered "The Box" to be a more slickly produced and better written show than "Number 96". "The Box" also aired on the Ten Network, programmed to run right after "Number 96". For 1974 "Number 96" was again the highest rating show on Australian television, and that year "The Box" occupied the number two spot.

Also in 1974, the Reg Grundy Organisation created its first soap opera, and significantly Australia's first "teen" soap opera, "Class of '74". With its attempts to hint at the sex and sin shown more openly on "Number 96" and "The Box", its high school setting and early evening timeslot, "Class of '74" came under intense scrutiny from the Broadcasting Control Board, who vetted scripts and altered entire storylines.

By 1975, both "Number 96" and "The Box", perhaps as a reaction to declining ratings for both shows, de-emphasised the sex and nudity shifting more towards comedic plots. "Class of '74" was renamed "Class of '75" and also added more slapstick comedy for its second year, but the revamped show's ratings declined, resulting in its cancellation in mid-1975. That year Cash Harmon's newly launched second soap "The Unisexers" failed in its early evening slot and was cancelled after three weeks; the Reg Grundy Organisation's second soap "Until Tomorrow" ran in a daytime slot for 180 episodes.

A feature film version of "Bellbird" entitled "Country Town" was produced in 1971 by two of the show's stars, Gary Gray and Terry McDermott, without production involvement by the Australian Broadcasting Corporation. "Number 96" and "The Box" also released feature film versions, both of which had the same title as the series, released in 1974 and 1975 respectively. As Australian television had broadcast in black and white until 1975, these theatrical releases all had the novelty of being in colour. The film versions of "Number 96" and "The Box" also allowed more explicit nudity than could be shown on television at that time.

In November 1976 "The Young Doctors" debuted on the Nine Network. This Grundy Organization series eschewed the adult drama of "Number 96" and "The Box", focusing more on relationship drama and romance. It became a popular success but received few critical accolades. A week later "The Sullivans", a carefully produced period serial chronicling the effects of World War II on a Melbourne family, also debuted on Nine. Produced by Crawford Productions, "The Sullivans" became a ratings success, attracted many positive reviews, and won television awards. During this period "Number 96" re-introduced nudity into its episodes, with several much-publicised full-frontal nude scenes, a cast revamp and a new range of shock storylines designed to boost the show's declining ratings. "Bellbird" experienced changes to its broadcast pattern with episodes screening in 60 minute blocks, and later in 30 minute installments.
"Bellbird", "Number 96" and "The Box", which had been experiencing declining ratings, were cancelled in 1977. Various attempts to revamp each of the shows with cast reshuffles or spectacular disaster storylines had proved only temporarily successful. "The Young Doctors" and "The Sullivans" continued to be popular. November 1977 saw the launch of successful soap opera/police procedural series "Cop Shop" (1977–1984) produced by Crawford Productions for Channel Seven. In early December 1977 Channel Ten debuted the Reg Grundy Organisation produced "The Restless Years" (1977–1981), a more standard soap drama focusing on several young school leavers.

The Seven Network, achieving success with "Cop Shop" produced by Crawford Productions, had Crawfords produce "Skyways", a series with a similar format but set in an airport, to compete with the Nine Network's popular talk show "The Don Lane Show". "Skyways", which debuted in July 1979, emphasised adult situations including homosexuality, marriage problems, adultery, prostitution, drug use and smuggling, crime, suicide, political intrigue, and murder, and featured some nudity. Despite this, the program achieved only moderate ratings and was cancelled in mid-1981.

The Reg Grundy Organisation found major success with the women's-prison drama "Prisoner" (1979–1986) on Network Ten, and melodramatic family saga "Sons and Daughters" (1982–1987) on the Seven Network. Both shows achieved high ratings in their original runs, and unusually, found success in repeats after the programs ended.

Grundy soap "The Young Doctors" and Crawford Productions' "The Sullivans" continued on the Nine Network until late 1982. Thereafter Nine attempted many new replacement soap operas produced by the Reg Grundy Organisation: "Taurus Rising" (1982), "Waterloo Station" (1983), "Starting Out" (1983) and "Possession" (1985), along with "Prime Time" (1986) produced by Crawford Productions. None of these programs were successful and most were cancelled after only a few months. The Reg Grundy Organisation also created "Neighbours", a suburban-based daily serial devised as a sedate family drama with some comedic and lightweight situations, for the Seven Network in 1985.

Produced in Melbourne at the studios of HSV-7, "Neighbours" achieved high ratings in Melbourne, Brisbane and Adelaide, but not in Sydney, where it aired at 5.30 p.m. placing it against the hit dating game show "Perfect Match" on Channel 10. The Seven Network's Sydney station ATN-7 quickly lost interest in "Neighbours" as a result of the low ratings in Sydney. HSV-7 in Melbourne lobbied heavily to keep "Neighbours" on the air, but ATN-7 managed to convince the rest of the network to cancel the show and instead keep ATN-7's own Sydney-based dramas "A Country Practice" and "Sons and Daughters".

After the network cancelled "Neighbours", it was immediately picked up by Channel Ten, which revamped the cast and scripts slightly and aired the series in the 7.00 p.m. slot starting 20 January 1986. It initially attracted low audiences, however after a concerted publicity drive, Ten managed to transform the series into a major success, turning several of its actors into major international stars. The show's popularity eventually declined and it was moved to the 6.30 p.m. slot in 1992. In January 2011 it moved to Eleven and is Australia's longest-running soap opera.

The success of "Neighbours" in the 1980s prompted the creation of somewhat similar suburban and family or teen-oriented soap operas such as "Home and Away" (1988–present) on Channel Seven and "Richmond Hill" (1988) on Channel Ten. Both proved popular, however "Richmond Hill" emerged as only a moderate success and was cancelled after one year to be replaced on Ten by "E Street" (1989–1993).

Nine continued trying to establish a successful new soap opera, without success. After the failure of family drama "Family and Friends" in 1990, it launched the raunchier and more extreme "Chances" in 1991, which resurrected the sex and melodrama of "Number 96" and "The Box" in an attempt to attract attention. "Chances" achieved only moderate ratings, and was moved to a late-night timeslot. It underwent several revamps that removed much of the original cast, and refocused the storylines to incorporate science-fiction and fantasy elements. The series continued in a late night slot until 1992, when it was cancelled due to low ratings despite the much-discussed fantasy storylines.

Several Australian soap operas have also found significant international success. In the UK, starting in the mid-1980s, daytime broadcasts of "The Young Doctors", "The Sullivans", "Sons and Daughters" and "Neighbours" (which itself was subsequently moved to an early-evening slot) achieved significant success. Grundy's "Prisoner" began airing in the United States in 1979 and achieved high ratings in many regions there, however the show ended its run in that country three years into its run. "Prisoner" also aired in late-night timeslots in the UK beginning in the late 1980s, achieving enduring cult success there. The show became so popular in that country that it prompted the creation of two stage plays and a stage musical based on the show, all of which toured the UK, among many other spin-offs. In the late 1990s, Channel 5 repeated "Prisoner" in the UK. Between 1998 and 2005, Channel 5 ran late-night repeats of "Sons and Daughters". During the 1980s, the Australian attempts to emulate big-budget U.S. soap operas such as "Dallas" and "Dynasty" had resulted in the debuts of "Taurus Rising" and "Return to Eden", two slick soap opera dramas with big budgets that were shot entirely on film. Though their middling Australian ratings resulted in the shows running only for a single season, both programs were successfully sold internationally.

Other shows to achieve varying levels of international success include "Richmond Hill", "E Street", "Paradise Beach" (1993–1994), and "Pacific Drive" (1995–1997). Indeed, these last two series were designed specifically for international distribution. Channel Seven's "Home and Away", a teen soap developed as a rival to "Neighbours", has also achieved significant and enduring success on UK television.

"Something in the Air", a serial examining a range of characters in a small country town ran on the ABC from 2000 to 2002.

Attempts to replicate the success of daily teen-oriented serials "Neighbours" and "Home and Away" saw the creation of "Echo Point" (1995) and "Breakers" (1999) on Network Ten. These programs foregrounded youthful attractive casts and appealing locations but the programs were not long-running successes and "Neighbours" and "Home and Away" remained the most visible and consistently successful Australian soap operas in production. In their home country, they both attracted respectable although not spectacular ratings in the early 2000s. By 2004, "Neighbours" was regularly attracting just under a million viewers per episode – considered at that time a low figure for Australian prime time television. By March 2007, the Australian audience for "Neighbours" had fallen to fewer than 700,000 a night. This prompted a revamp of the show's cast, its visual presentation, and a move away from the recently added action-oriented emphasis to refocus the show on the domestic storylines it is traditionally known for. During this period "Neighbours" and "Home and Away" continued to achieve significant ratings in the UK. This and other lucrative overseas markets, along with Australian broadcasting laws that enforce a minimum amount of domestic drama production on commercial television networks, help ensure that both programs remain in production. Both shows get higher total ratings in the UK than in Australia (the UK has three times the total population of Australia) and the UK channels make a major contribution to the production costs.

It has been suggested that with their emphasis on the younger, attractive and charismatic characters, "Neighbours" and "Home and Away" have found success in the middle ground between glamorous, fantastic U.S. soaps with their wealthy but tragic heroes and the more grim, naturalistic UK soap operas populated by older, unglamorous characters. The casts of "Neighbours" and "Home and Away" are predominantly younger and more attractive than the casts of UK soaps, and without excessive wealth and glamour of the U.S. daytime serial, a middle-ground in which they have found their lucrative niche.

"Neighbours" was carried in the United States on the Oxygen cable channel in March 2004; however it attracted few viewers, perhaps in part due to its scheduling opposite well-established and highly popular U.S. soap operas such as "All My Children" and "The Young and the Restless", and was dropped by the network shortly afterwards due to low ratings.

"headLand" made its debut on Channel Seven in November 2005, the series arose out of a proposed spinoff of "Home and Away" that was to have been produced in conjunction with "Home and Away"s UK broadcaster, Five. The idea for the spin-off was scuttled after Five pulled out of the deal, which meant that the show could potentially air on a rival channel in the UK; as such, Five requested that the new show be developed as a standalone series and not be spun off from a series that it owned a stake in. The series premiered in Australia on November 15, 2005, but was not a ratings success and was cancelled two months later on January 23, 2006. The series broadcast on E4 and Channel 4 in the UK. Nickelodeon's "" appeared on July 2006 on Network Ten. Since Connie considered this mention as a torrid soap opera, this was mentioned in the Steven Universe episode "Love Letters".

After losing the UK television rights to "Neighbours" to Five, the BBC commissioned a replacement serial "Out of the Blue", which was produced in Australia. It debuted as part of BBC One's weekday afternoon schedule on 28 April 2008 but low ratings prompted its move to BBC Two on 19 May 2008. The series was cancelled after its first season.

"Neighbours"' continued low ratings in Australia resulted in it being moved to Ten's new digital channel, Eleven on January 11, 2011. However, it continues to achieve reasonable ratings on Channel 5 in the United Kingdom, and as of March 2013 still reportedly achieved significant international sales.

Pioneering series "Pukemanu" aired over two years (1971–72) and was the NZBC's first continuing drama. It followed the goings-on of a North Island timber town.
"Close to Home" is a New Zealand television soap opera that ran on TVNZ 1 from 1975 to 1983. At its peak in 1977 nearly one million viewers tuned in twice weekly to watch the series co-created by Michael Noonan and Tony Isaac (who had initially only agreed to make the show on the condition they would get to make "The Governor"). "Gloss" is a television drama series that screened from 1987 to 1990. The series is about a fictional publishing empire run by the Redfern family. Gloss was NZ's answer to US soap "Dynasty", with the Carrington oil scions replaced by the wealthy Redferns and their Auckland magazine empire. It was a starting point for many actors who went on to many productions in New Zealand, Australia and around the world including Temuera Morrison, Miranda Harcourt, Peter Elliott, Lisa Chappell, Danielle Cormack and Kevin Smith. Many of them would go on to star in "Shortland Street", which has been New Zealand's most popular soap since its debut in 1992. It airs on TVNZ 2.

Radio New Zealand began airing its first radio soap "You Me Now" in September 2010. It is available for podcast on its website.

Relatively few daily soap operas have been produced on English Canadian television, with most Canadian stations and networks that carry soap operas airing those imported from the United States or the United Kingdom. Notable daily soaps that did exist included "Family Passions", "Scarlett Hill", "Strange Paradise", "Metropia", "Train 48" and the international co-production "Foreign Affairs". "Family Passions" was an hour-long program, as is typical of American daytime soaps; all of the others ran half-hour episodes. Unlike American or British soap operas, the most influential of which have run for years or even decades, even daily Canadian soap operas have run for a few seasons at most. Short-run soaps, including "49th & Main" and "North/South", have also aired. Many of these were produced in an effort to comply with Canadian content regulations, which require a percentage of programming on Canadian television to originate from Canada.

Notable prime time soap operas in Canada have included "Riverdale", "House of Pride", "Paradise Falls", "Lance et Compte" ("He Shoots, He Scores"), "Loving Friends and Perfect Couples" and "The City". The "Degrassi" franchise of youth dramas also incorporated some elements of the soap opera format.

On French-language television in Quebec, the "téléroman" has been a popular mainstay of network programming since the 1950s. Notable téléromans have included "Rue des Pignons", "Les Belles Histoires des pays d'en haut", "Diva", "La famille Plouffe", and the soap opera parody "Le Cœur a ses raisons".

Unlike the season based production in most countries, most of Indian television fiction tends to be regular-broadcasting soap opera. These started in the 1980s, as more and more people began to purchase television sets. At the beginning of the 21st century, soap operas became an integral part of Indian culture. Indian soap operas mostly concentrate on the conflict between love and arranged marriages occurring in India, and many includes family melodrama. Indian soap operas have multilingual production.

Many soap operas produced in India are also broadcast overseas in the UK, Canada, the United States, and some parts of Europe, South Africa, Australia and South East Asia. They are often mass-produced under large production banners, with companies like Balaji Telefilms running different language versions of the same serial on different television networks or channels.

The Australian serial "The Restless Years" was remade in the Netherlands as "Goede tijden, slechte tijden" (which debuted in 1990) and in Germany as "Gute Zeiten, schlechte Zeiten" (which has aired since 1992): both titles translate to "good times, bad times". These remakes are still airing, but have long since diverged from the original Australian storylines. The two shows are the highest-rated soap operas in their respective countries.

A later Australian serial, "Sons and Daughters", has inspired five remakes produced under license from the original producers and based, initially, on original story and character outlines. These are "Verbotene Liebe" (Germany, 1995–2015); "Skilda världar" (Sweden, 1996–2002); "Apagorevmeni agapi" (Greece, 1998); "Cuori Rubati" (Italy, 2002–2003) and "Zabranjena ljubav" (Croatia, 2004–2008). Both "The Restless Years" and "Sons and Daughters" were created and produced in Australia by the Reg Grundy Organisation.

The Norwegian soap opera "Hotel Cæsar" aired on TV 2 from 1998 to 2017, and is the longest-running television drama in Scandinavia. Popular foreign soaps in the country include "Days of Our Lives" (broadcast on TV6 (Norway), "The Bold and the Beautiful" (TNT (Norway) and "Home and Away" (TV 2), all of which are subtitled.

Serials have included "Goede tijden, slechte tijden" (1990–present), "Onderweg naar Morgen" (1994–2010) and "Goudkust" (1996–2001). In 2016 "Goede tijden, slechte tijden" spin-off "Nieuwe Tijden" started airing. U.S. daytime serials "As The World Turns" and "The Bold and the Beautiful" have aired in the Netherlands; "As the World Turns" began airing in the country in 1990, with Dutch subtitles.

In the 1980s, West German networks successfully added American daytime and primetime soap operas to their schedule before Das Erste introduced its first self-produced weekly soap with "Lindenstraße", which was seen as a German counterpart to "Coronation Street". Like in other countries, the soap opera met with negative reviews, but eventually proved critics wrong with nearly 13 million viewers tuning in each week. Even though the format proved successful, it was not until 1992 before "Gute Zeiten, schlechte Zeiten" became the first German daily soap opera. Early ratings were bad as were the reviews, but the RTL network was willing to give its first soap opera a chance; ratings would improve, climbing to 7 million viewers by 2002. Not long after "Gute Zeiten, schlechte Zeiten", Das Erste introduced "Marienhof", which aired twice a week.

After successfully creating the first German daily soap, production company Grundy Ufa wanted to produce another soap for RTL. Like "GZSZ", the format was based on an Australian soap opera from Reg Watson. But RTL did not like the plot idea about separated twins who meet each other for the first time after 20 years and fall in love without knowing that they are related. The project was then taken to Das Erste, which commissioned the program, titled "Verbotene Liebe", which premiered on January 2, 1995. With the premiere of "Verbotene Liebe", the network turned "Marienhof" into a daily soap as well. In the meanwhile, RTL debuted the Grundy Ufa-produced "Unter uns" in late 1994.

ZDF started a business venture with Canada and co-produced the short-lived series "Family Passions", starring actors such as Gordon Thomson, Roscoe Born, Dietmar Schönherr and a young Hayden Christensen. The daytime serial premiered on December 5, 1994, lasting 130 episodes. After its cancellation, the network debuted "Jede Menge Leben". Even after a crossover with three soaps, "Freunde fürs Leben", "Forsthaus Falkenau" and "Unser Lehrer Doktor Specht", the soap was canceled after 313 episodes. Sat.1 tried to get into the soap business as well, after successfully airing the Australian soap opera "Neighbours", which was dropped in 1995 due to the talk show phenomenon that took over most of the daytime schedules of German networks. The network first tried to tell a family saga with "So ist das Leben! Die Wagenfelds", before failing with "Geliebte Schwestern". RTL II made its own short-lived attempt with "Alle zusammen – jeder für sich".

The teen soap opera "Schloss Einstein" debuted on September 4, 1998, focusing on the life of a group of teenagers at the fictional titular boarding school near Berlin. As of July 2014, the series has produced over 815 episodes during the course of 17 seasons, a milestone in German television programming, and was renewed for an 18th season to debut in 2015.

In 1999, after the lasting success of "Gute Zeiten, schlechte Zeiten", "Marienhof", "Unter uns" and "Verbotene Liebe", ProSieben aired "Mallorca – Suche nach dem Paradies", set on the Spanish island with the same name. After nine months, the network canceled the program due to low viewership and high production costs. Even though ratings had improved, the show ended its run in a morning timeslot. The soap opera became something of a cult classic, as its 200-episode run was repeated several times on free-to-air and pay television.

In 2006, "Alles was zählt" became the last successful daily soap to make its debut, airing as a lead-in to "Gute Zeiten, schlechte Zeiten" and also produced by Grundy Ufa. Since Germany started to produce its own telenovelas, all soap operas faced declines in ratings. "Unter uns" was in danger of cancellation in 2009, but escaped such a fate due to budget cuts imposed by the show's producers and the firing of original cast member Holger Franke, whose firing and the death of his character outraged fans, resulting in a ratings spike in early 2010. After "Unter uns" was saved, Das Erste planned to make changes to its soap lineup. "Marienhof" had to deal with multiple issues in its storytelling, as well as in producing a successful half-hour show. Several changes were made within months, however "Marienhof" was canceled in June 2011. "Verbotene Liebe" was in danger of being cancelled as well, but convinced the network to renew it with changes that it made in both 2010 and 2011; the soap was later expanded to 45 minutes after "Marienhof" was canceled, and the network tried to decide on whether to revamp its lineup.

While "Gute Zeiten, schlechte Zeiten", "Unter uns" and "Alles was zählt" are currently the only daily soaps on the air after "Verbotene Liebe" has been cancelled and aired its last episode in June, 2015 due to low ratings, the telenovelas "Sturm der Liebe" and "Rote Rosen" are considered soaps by the press as well, thanks to the changing protagonists every season.

In Belgium, the two major soap operas are "Thuis" ("Home") and "Familie" ("Family"). Soap operas have been very popular in Flanders, the Dutch-speaking part of Belgium. "Familie" debuted in late 1991, and with nearly 6,000 half-hour episodes, it has the highest episode total of any soap in Europe outside of the United Kingdom. The highest-rated soap opera is "Thuis", which has aired on "één" since late 1995. "Thuis" is often one of the five most-watched Belgian shows and regularly garners over one million viewers (with 6.3 million Flemmings in total).

During the 1990s, foreign soap operas such as "Neighbours" and "The Bold and the Beautiful" were extremely popular, the latter having achieved a cult status in Belgium and airing in the middle of the decade during prime time. Both soaps still air today, along with other foreign soaps such as "Days of Our Lives", Australia's "Home and Away" and Germany's "Sturm der Liebe". Vitaya unsuccessful attempted to air the Dutch soap opera "Goede Tijden, Slechte Tijden" in 2010. Other foreign soaps that previously aired on Belgian television include "The Young and the Restless", "EastEnders" (both on VTM), "Port Charles" (at één, then known as TV1) and "Coronation Street" (on Vitaya). "Santa Barbara" aired during the 1990s on VTM for its entire run.

The only teen soap opera on Belgian television was "Spring" ("Jump" in English), which aired on the youth-oriented Ketnet and produced over 600 15-minute episodes from late 2002 until 2009, when it was cancelled after a steady decline in ratings following the departures of many of its original characters.

The most successful soap opera in Italy is the evening series "Un posto al sole" ("A Place Under the Sun"), which had aired on Rai 3 since 1996 (whose format is based on the Australian soap opera "Neighbours"). Several other Italian soaps have been produced such as "Ricominciare" ("Starting Over"), "Cuori rubati" ("Stolen Hearts"), "Vivere" ("Living"), "Sottocasa" ("Downstairs"), "Agrodolce" ("Bittersweet") and "Centovetrine" ("Hundred Shop Windows").

The most popular Italian prime-time soap opera, "Incantesimo" ("Enchantment"), which ran from 1998 to 2008, became a daytime soap opera for the final two years of its run, airing five days a week on Rai 1. The same happened with "Il paradiso delle signore" ("Woman’s Paradise"), a period drama, which ran from 2015 to 2017 in prime-time, and became a daytime period soap opera from 2018.

In the early years of RTÉ, the network produced several dramas but had not come close to launching a long-running serial. RTÉ's first television soap was "Tolka Row", which was set in urban Dublin. For several years, both "Tolka Row" and "The Riordans" were produced by RTÉ; however, the urban soap was soon dropped in favor of the more popular rural soap opera "The Riordans", which premiered in 1965. Executives from Yorkshire Television visited during on-location shoots for "The Riordans" in the early 1970s and in 1972, debuted "Emmerdale Farm" (now "Emmerdale"), based on the successful format of the Irish soap opera. In the late 1970s, "The Riordans" was controversially dropped. The creator of that series would then go on to produce the second of his "Agri-soap" trilogy "Bracken", starring Gabriel Byrne, whose character had appeared in the last few seasons of "The Riordans". Bracken was soon replaced by the third "Agri-soap" "Glenroe", which ran until 2001. As RTÉ wanted a drama series for its Sunday night lineup rather than a soap opera, "On Home Ground" (2001–2002), "The Clinic" (2002–2009) and "RAW" (2010–2013) replaced the agri-soaps of the previous decades.

In 1989, RTÉ decided to produce its first Dublin-based soap opera since the 1960s. "Fair City", which is set in the fictional city of Carrickstown, initially aired one night a week during the 1989-90 season, and similar to its rural soaps, much of the footage was filmed on location – in a suburb of Dublin City. In 1992, RTÉ made a major investment into the series by copying the houses used in the on-location shoots for an on-site set in RTÉ's Headquarters in Dublin 4. By the early 1990s, it was airing two nights a week for 35 weeks a year. With competition from the UK soap operas, RTÉ expanded "Fair City" to three nights a week for most of the year and one night a week during the summer in 1996, later expanding to four nights a week and two nights during the summer. Until the early 2000s, the series produced four episodes a week, airing all 52 weeks of the year. "Fair City" airs Sundays, Tuesdays and Thursdays at 8.00 p.m. GMT on RTÉ One; however, after rival network TV3 moved "Coronation Street" to Thursday night, the Wednesday night episode of "Fair City" began airing at 7:30 p.m. each week.

TG4 produce the Irish language soap "Ros na Rún" ("Headland of the Secrets" or "Headland of the Sweethearts"); set in the fictional village of "Ros Na Rún", located outside Galway and near Spiddal, it centres on the domestic and professional lives of the town's residents. It is modeled on an average village in the West of Ireland, but with its own distinct personality – with a diverse population that share secrets, romances and friendships among other things. While the core community has remained the same, the look and feel of "Ros Na Rún" has changed and evolved over the years to incorporate the changing face of rural Ireland. It has an established a place not only in the hearts and minds of the Irish speaking public, but also the wider Irish audience. The program has dealt with many topics, including domestic violence, infidelity, theft, arson, abortion, homosexuality, adoption, murder, rape, drugs, teen pregnancy and paedophilia. It runs twice a week for 35 weeks of the year, currently airing Tuesday and Thursday nights. "Ros na Rún" is the single largest independent production commissioned in the history of Irish broadcasting. Prior to TG4's launch, it originally aired on RTÉ One in the early 1990s.

Although Ireland has access to international soaps (such as "Coronation Street", "Emmerdale", "EastEnders", "Home and Away", "Hollyoaks" and "Neighbours"), "Fair City" continues to outperform them all, and is Ireland's most popular soap opera, with the show peaking at over 700,000 viewers.

January 2015 "Red Rock" has broadcast on TV3. Red Rock airs twice a week on Wednesday and Thursday nights. The series is base in a fishing village in Dublin. The soaps centres around the local Garda station but also includes stories from the village.

RTÉ Radio produced its first radio soap, "Kennedys of Castleross", which ran from April 13, 1955 to 1975. In 1979 RTÉ long running TV soap The Riordans moved to Radio until December 24, 1985. In the mid-1980s, RTÉ debuted a new radio soap, "Harbour Hotel", which ran until the mid-1990s. The network later ran two short-lived radio soaps, "Konvenience Korner" and "Riverrun", which were followed in 2004 by "Driftwood". RTÉ does not run any radio soaps, however RTÉ Radio 1 continues to air radio dramas as part of its nighttime schedule.


In Greece, there have been several soap operas.

An early serial was "Sti skia tou hrimatos" ("Money Shadows"), which ran from 1990 to 1991. September 1991 saw the debut of "Lampsi" ("the Shining"), from creator Nicos Foskolos. The series would become Greece's longest-running soap opera. After the success of "Lampsi" came the short lived "To galazio diamandi" ("Blue Diamond") and "Simphonia siopis" ("Omertà"). "Lampsi" was canceled in June 2005 due to declining ratings. It was replaced by "Erotas" ("Love"), a soap that ran from 2005 to 2008. After that series ended, ANT1 abandoned the soap opera genre and focused on comedy series and weekly dramas.

Greece's second longest-running soap is "Kalimera Zoi" ("Goodmorning Life"), which ran from September 1993 until its cancellation in June 2006 due to low ratings.

Mega Channel began producing soap operas in 1990 with the prime time serial "I Dipsa" ("The Thirst"), which ran for 102 episodes. Other daytime soaps have included "Paralliloi dromoi" (1992–1994) and its successor "Haravgi" ("Daylight", 1994–1995), both of which were cancelled due to low viewership; as well as the serials "Apagorevmeni Agapi" ("Forbidden Love"), which ran from 1998 to 2006; "Gia mia thesi ston Ilio" ("A Spot Under the Sun"), which ran from 1998 to 2002; "Filodoxies" ("Expectations"), which ran from 2002 to 2006; and "Vera Sto Deksi" ("Ring on the Right Hand"), which ran from 2004 to 2006 and proved to be a successful competitor to "Lampsi", causing that show's ratings to decline.

"Ta Mistika Tis Edem" ("Edem Secrets"), which was created by the producers of "Vera Sto Deksi", debuted in 2008 and has eclipsed that show's success. Its ratings place it consistently among the three highest-rated daytime programs.

IENED (which was renamed ERT2 in 1982) was responsible for the first Greek soap operas "I Kravgi Ton Likon" and "Megistanes". ERT also produced the long-running soap "O Simvoleografos". Since 2000 and with the introduction of private television, ERT produced additional daily soap operas, which included "Pathos" ("Passion"), "Erotika tis Edem" ("Loving in Eden") and "Ta ftera tou erota" ("The Wings of Love"). These failed to achieve high ratings and were canceled shortly after their premiere.

Alpha produced "Kato apo tin Acropoli" ("Under the Acropolis"), which ran for 2½ years.

The first daytime soap opera produced by a Cyprus channel was LOGOs TV's "Odos Den Ksehno" ("'Don't Forget' Street"), which ran from January to December 1996. It was followed by "To Serial", which also ran for one year from September 1997 to June 1998. CyBC created the third weekdaily soap, "Anemi Tou Pathous" ("Passion Winds"), running from January 2000 to June 2004, which was replaced by "I Platia" ("The Square") from September 2004 to July 2006. "Epikindini Zoni" ran from 2009 to 2010, and was cancelled after 120 episodes. "Vimata Stin Ammo" made its debut in September 2010.

Sigma TV first commissioned the weekdaily comedic soap "Sto Para Pente", which aired from September 1998 to June 2004, and first held the distinction of being the longest weekday show in Cyprus television history, before it was surpassed by "Se Fonto Kokkino", which ran from September 2008 to July 2012. Other Sigma TV weekday shows include "Akti Oniron" (which ran from 1999 to 2001), "Vourate Geitonoi" (which ran from 2001 to 2005, and was the most successful weekdaily series, achieving ratings shares of up to 70% of all television households in the country), "Oi Takkoi" (which ran from 2002 to 2005), "S' Agapo" (which ran from 2001 to 2002), "Vasiliki" (which ran from 2005 to 2006), "Vendetta" (which ran from September 2005 to December 2006), "30 kai Kati" (which ran from 2006 to 2007) and "Mila Mou" (which ran from September 2007 to January 2009).

ANT1 Cyprus aired the soap "I Goitia Tis Amartias" in 2002, which was soon canceled. "Dikse Mou To Filo Sou" followed from 2006 to 2009, along with "Gia Tin Agapi Sou", which ran from 2008 to 2009 and itself was followed by "Panselinos", which has aired since 2009.

The longest-running weekly show on Cyprus television is "Istories Tou Horkou" ("Villages Stories", which premiered on CyBC in March 1996 and ran until its cancellation in June 2006; it was revived in September 2010 but was cancelled again in March 2011 due to very low ratings), followed by "Manolis Ke Katina" ("Manolis and Katina", which ran from 1995 to 2004). The most controversial of these series was "To Kafenio" ("The Coffee Shop"), which premiered on CyBC on 1993 as a weekly series, before moving to MEGA Channel Cyprus six years later in 1999 as a weekdaily show and then moved to ANT1 Cyprus on 2000, which canceled the show one year later. There were plans to move the show back to CyBC as a weekly series in 2001, with the original cast, however this plan was never realised. The most successful weekly shows in Cyprus currently are ANT1's "Eleni I Porni" ("Eleni, The Whore"), which premiered in October 2010 and CyBC's "Stin Akri Tu Paradisou" ("At The Heaven's Edge"), which premiered in 2007. The most successful weekdaily soap was "Aigia Fuxia", which aired on ANT1 Cyprus from 2008 to 2010.

The only daily Finnish soap opera so far is "Salatut elämät" ("Secret Lives"), which has achieved popularity in Finland since its 1999 debut on MTV3. It focuses on the lives of people along the imaginary Pihlajakatu street in Helsinki. The show has also spawned several Internet spin-off series and a film based on the show that was released in 2012.

Other soap-like shows in Finland are YLE shows "Uusi päivä" (which has aired since 2009) and "Kotikatu" (which ran from 1995 to 2012), however these programs do not adhere to a five-episode-a-week schedule.

With the advent of internet television and mobile phones, several soap operas have also been produced specifically for these platforms, including "", a spin-off of the established "EastEnders". For those produced only for the mobile phone, episodes may generally consist of about six or seven pictures and accompanying text.

On September 13, 2011, TG4 launched a new 10-part online series titled, "Na Rúin" (an Internet spin-off of "Ros na Rún"). The miniseries took on the theme of a mystery; the viewer had to read Rachel and Lorcán's blogs as well as watch video diaries detailing each character's thoughts to solve the mystery of missing teenage character Ciara.

In motion pictures, the 1982 comedy "Tootsie" has the lead character impersonating a woman in order to gain acting work on a long running television soap opera. Several scenes parody the production of soaps, their outrageous storylines and idiosyncratic stylistic elements.

The 1991 comedy "Soapdish" stars Sally Field as an aging soap opera actress on fictional series "The Sun Also Sets" who pines over her own neuroses and misfortunes such as her live-in boyfriend who leaves her to go back to his wife, and the incidents of backstabbing and scheming behind the scenes, some of which are more interesting than the stories on the program.

Another 1991 comedy, Delirious, stars John Candy as a soap opera writer who, after a head injury, has a dream experience of being in his own creation. The dream experience is an increasingly outrageous exaggeration of soap opera plot elements.

On television, several soap opera parodies have been produced:



</doc>
<doc id="27010" url="https://en.wikipedia.org/wiki?curid=27010" title="Software engineering">
Software engineering

Software engineering is the systematic application of engineering approaches to the development of software. Software engineering is a direct sub-field of engineering and has an overlap with computer science and management science. It is also considered a part of overall systems engineering.

When the first digital computers appeared in the early 1940s, the instructions to make them operate were wired into the machine. Practitioners quickly realized that this design was not flexible and came up with the "stored program architecture" or von Neumann architecture. Thus the division between "hardware" and "software" began with abstraction being used to deal with the complexity of computing.

Programming languages started to appear in the early 1950s and this was also another major step in abstraction. Major languages such as Fortran, ALGOL, and COBOL were released in the late 1950s to deal with scientific, algorithmic, and business problems respectively. David Parnas introduced the key concept of modularity and information hiding in 1972 to help programmers deal with the ever-increasing complexity of software systems.

The origins of the term "software engineering" have been attributed to various sources. The term "software engineering" appeared in a list of services offered by companies in the June 1965 issue of COMPUTERS and AUTOMATION and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) “letter to the ACM membership” by the ACM President Anthony A. Oettinger;, it is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer, the first conference on software engineering. Independently, Margaret Hamilton named the discipline "software engineering" during the Apollo missions to give what they were doing legitimacy. At the time there was perceived to be a "software crisis". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of "Software Engineering" with the Plenary Sessions' keynotes of Frederick Brooks and Margaret Hamilton.

In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process. The Process Maturity Levels introduced would become the Capability Maturity Model Integration for Development(CMMI-DEV), which has defined how the US Government evaluates the abilities of a software development team.

Modern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK).

Notable definitions of software engineering include: 

The term has also been used less formally:

Requirements engineering is about the elicitation, analysis, specification, and validation of requirements for software.

Software design is about the process of defining the architecture, components, interfaces, and other characteristics of a system or component. This is also called Software architecture.

Software development, the main activity of software construction: is the combination of programming (aka coding), verification, software testing, and debugging. A Software development process: is the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses Software configuration management which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning.

Software testing: is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the product or service under test, with different approaches such as unit testing and integration testing. It is one aspect of software quality.

Software maintenance: refers to the activities required to provide cost-effective support after shipping the software product.

Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2004, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.
Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014. A number of universities have Software Engineering degree programs; , there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.

In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.

Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title.

The United States, since 2013, has offered an "NCEES" "Professional Engineer" exam for Software Engineering, thereby allowing Software Engineers to be licensed and recognized. NCEES will end the exam after April 2019 due to lack of participation. Mandatory licensing is currently still largely debated, and perceived as controversial. In some parts of the US such as Texas, the use of the term Engineer is regulated by law and reserved only for use by individuals who have a Professional Engineer license.

The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's "Guide to the Software Engineering Body of Knowledge - 2004 Version", or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014. The IEEE also promulgates a "Software Engineering Code of Ethics".

The U. S. Bureau of Labor Statistics counted 1,365,500 software developers holding jobs in the U.S. in 2018. Employment of computer and information technology occupations is projected to grow 13 percent from 2016 to 2026, faster than the average for all occupations. These occupations are projected to add about 557,100 new jobs. Demand for these workers will stem from greater emphasis on cloud computing, the collection and storage of big data, and information security. Yet, the BLS also says some employment in these occupations are slowing and computer programmers is projected to decline 7 percent from 2016 to 2026 since computer programming can be done from anywhere in the world, so companies sometimes hire programmers in countries where wages are lower. Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees.

Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Specializations include: in industry (analysts, architects, developers, testers, technical support, middleware analysts, managers) and in academia (educators, researchers).

Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.

The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture. IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. These certification programs are tailored to the institutions that would employ people who use these technologies.

Broader certification of general software engineering skills is available through various professional societies. , the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP). In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). The ACM had a professional certification program in the early 1980s, which was discontinued due to lack of interest. The ACM examined the possibility of professional certification of software engineers in the late 1990s, but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering.

In the U.K. the British Computer Society has developed a legally recognized professional certification called "Chartered IT Professional (CITP)", available to fully qualified members ("MBCS"). Software engineers may be eligible for membership of the Institution of Engineering and Technology and so qualify for Chartered Engineer status. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called "Information Systems Professional (ISP)". In Ontario, Canada, Software Engineers who graduate from a "Canadian Engineering Accreditation Board (CEAB)" accredited program, successfully complete PEO's ("Professional Engineers Ontario") Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the "Professional Engineers Ontario" and can become Professional Engineers P.Eng. The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.

The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / timezone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers. Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations. When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.

While global outsourcing has several advantages, global - and generally distributed - development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.

Software engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism. This perspective has led to calls for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.

Software craftsmanship has been proposed by a body of software developers as an alternative that emphasizes the coding skills and accountability of the software developers themselves without professionalism or any prescribed curriculum leading to ad-hoc problem-solving (craftmanship) without engineering (lack of predictability, precision, missing risk mitigation, methods are informal and poorly defined). The Software Craftsmanship Manifesto extends the Agile Software Manifesto and draws a metaphor between modern software development and the apprenticeship model of medieval Europe.

Software engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation. The concept is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.

One of the core issues in software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a "theoretical environment."

Edsger Dijkstra, the founder of many of the concepts used within software development today, rejected the idea of "software engineering" up until his death in 2002, arguing that those terms were poor analogies for what
he called the "radical novelty" of computer science:





</doc>
<doc id="27011" url="https://en.wikipedia.org/wiki?curid=27011" title="Software Engineering Institute">
Software Engineering Institute

The Software Engineering Institute (SEI) is an American research and development center headquartered in Pittsburgh, Pennsylvania. Its activities cover cybersecurity, software assurance, software engineering and acquisition, and component capabilities critical to the Department of Defense.

The Carnegie Mellon Software Engineering Institute is a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. The SEI also has offices in Washington, DC and Los Angeles, California. The SEI operates with major funding from the U.S. Department of Defense. The SEI also works with industry and academia through research collaborations.

On November 14, 1984, the U.S. Department of Defense elected Carnegie Mellon University as the host site of the Software Engineering Institute. The institute was founded with an initial allocation of $6 million, with another $97 million to be allocated in the subsequent five years. The SEI's contract with the Department of Defense is subject to review and renewal every five years.

The SEI program of work is conducted in several principal areas: cybersecurity, software assurance, software engineering and acquisition, and component capabilities critical to the Department of Defense.

The SEI defines specific initiatives aimed at improving organizations' software engineering capabilities.

Organizations need to effectively manage the acquisition, development, and evolution (ADE) of software-intensive systems. Success in software engineering management practices helps organizations predict and control quality, schedule, cost, cycle time, and productivity. The best-known example of SEI in management practices is the SEI's Capability Maturity Model (CMM) for Software (now Capability Maturity Model Integration (CMMI)). The CMMI approach consists of models, appraisal methods, and training courses that have been proven to improve process performance. In 2006, Version 1.2 of the CMMI Product Suite included the release of CMMI for Development. CMMI for Development was the first of three constellations defined in Version 1.2: the others include CMMI for Acquisition and CMMI for Services. The CMMI for Services constellation was released in February 2009. Another management practice developed by CERT, which is part of the SEI, is the Resilience Management Model (CERT-RMM). The CERT-RMM is a capability model for operational resilience management. Version 1.0 of the Resilience Management Model was released in May 2010.

SEI work in engineering practices increases the ability of software engineers to analyze, predict, and control selected
functional and non-functional properties of software systems. Key SEI tools and methods include the SEI Architecture Tradeoff Analysis Method (ATAM) method, the SEI Framework for Software Product Line Practice, and the SEI Service Migration and Reuse Technique (SMART).

The SEI is also the home of the CERT/CC (CERT Coordination Center), a federally funded computer security organization. The SEI CERT Program's primary goals are to ensure that appropriate technology and systems-management practices are used to resist attacks on networked systems and to limit damage and ensure continuity of critical services in spite of successful attacks, accidents, or failures. The SEI CERT program is working with US-CERT to produce the Build Security In (BSI) website, which provides guidelines for building security into every phase of the software development lifecycle. The SEI has also conducted research on insider threats and computer forensics. Results of this research and other information now populate the CERT Virtual Training Environment.

Carnegie Mellon, Capability Maturity Model, CMM, CMMI, Architecture Tradeoff Analysis Method, ATAM, and CERT are registered in the U.S. Patent and Trademark Office by Carnegie Mellon University.

The SEI Partner Network helps the SEI disseminate software engineering best practices. Organizations and individuals in the SEI Partner Network are selected, trained, and licensed by the SEI to deliver authentic SEI services, which include courses, consulting methods, and management processes. The network currently consists of nearly 250 partner organizations worldwide.

The SEI sponsors national and international conferences, workshops, and user-group meetings. Other events cover subjects including acquisition of software-intensive systems, commercial off-the-shelf (COTS)-based systems, network security and survivability, software process research, software product lines, CMMI, and the SEI Team Software Process.

SEI courses are currently offered at the SEI's locations in the United States and Europe. In addition, using licensed course materials, SEI Partners train individuals.

The SEI Membership Program helps the software engineering community to network. SEI Members include small business owners, software and systems programmers, CEOs, directors, and managers from both Fortune 500 companies and government organizations

Through the SEI Affiliate Program, organizations place technical experts with the SEI for periods ranging from 12 months to four years. Affiliates currently are working on projects with the SEI to identify, develop, and demonstrate improved software engineering practices.

In order to recognize outstanding achievement in improving an organization's ability to create and evolve software-dependent systems, the SEI and IEEE Computer Society created the Software Process Achievement Award program. In addition to rewarding excellence, the purpose of this award is to foster continuous advancement in the practice of software engineering and to disseminate insights, experiences, and proven practices throughout the relevant research and practitioner communities.

The SEI publishes reports that offer new technical information about software engineering topics, whether theoretical or applied. The SEI also publishes books on software engineering for industry, government and military applications and practices.

In addition, the SEI offers public courses, workshops, and conferences in process improvement, software architecture and product lines, and security.

On November 11, 2015, the Software Engineering Institute was accused of aiding Federal Bureau of Investigation in uncovering the identities of users of the Tor network. Later prosecution showed the hack was paid for by the Department of Defense and subpoena by the FBI.

SEI has been an occasional site of anti-war movement and peace movement protests, many of which have been organized by Pittsburgh's Thomas Merton Center.




</doc>
<doc id="27012" url="https://en.wikipedia.org/wiki?curid=27012" title="Software crisis">
Software crisis

Software crisis is a term used in the early days of computing science for the difficulty of writing useful and efficient computer programs in the required time. The software crisis was due to the rapid increases in computer power and the complexity of the problems that could now be tackled. With the increase in the complexity of the software, many software problems arose because existing methods were inadequate.

The term "software crisis" was coined by some attendees at the first NATO Software Engineering Conference in 1968 at Garmisch, Germany. Edsger Dijkstra's 1972 ACM Turing Award Lecture makes reference to this same problem:
The causes of the software crisis were linked to the overall complexity of hardware and the software development process. The crisis manifested itself in several ways:

The main cause is that improvements in computing power had outpaced the ability of programmers to effectively utilize those capabilities. Various processes and methodologies have been developed over the last few decades to improve software quality management such as procedural programming and object-oriented programming. However software projects that are large, complicated, poorly specified, and involve unfamiliar aspects, are still vulnerable to large, unanticipated problems.




</doc>
<doc id="27013" url="https://en.wikipedia.org/wiki?curid=27013" title="Swedish Academy">
Swedish Academy

The Swedish Academy (), founded in 1786 by King Gustav III, is one of the Royal Academies of Sweden. Its 18 members, who are elected for life, comprise the highest Swedish language authority. Outside Scandinavia, it is best known as the body that chooses the laureates for the annual Nobel Prize in Literature, awarded in memory of the donor Alfred Nobel.

The Swedish Academy was founded in 1786 by King Gustav III. Modelled after the Académie française, it has 18 members. It is said that Gustaf III originally intended there to be twenty members, half the number of those in the French Academy, but eventually decided on eighteen because the Swedish word for that number, "Aderton", had such a fine ring. The academy's motto is "Talent and Taste" (""Snille och Smak"" in Swedish). The academy's primary purpose is to further the "purity, strength, and sublimity of the Swedish language" (""Svenska Språkets renhet, styrka och höghet"") (Walshe, 1965). To that end the academy publishes two dictionaries. The first is a one-volume glossary called "Svenska Akademiens ordlista" ("SAOL"). The second is a multi-volume dictionary, edited on principles similar to those of the "Oxford English Dictionary", entitled "Svenska Akademiens Ordbok" ("SAOB"). The "SAOL" has reached its 14th edition while the first volume of the "SAOB" was published in 1898 and, as of 2017, work has progressed to words beginning with the letter "V".

The building now known as the Stockholm Stock Exchange Building was built for the bourgeoisie. The bottom floor was used as a trading exchange (this later became the stock exchange), and the upper floor was used for balls, New Year's Eve parties, etc. When the academy was founded, the ballroom was the biggest room in Stockholm that could be heated and thus used in the winter, so the King asked if he could borrow it.

The academy has had its annual meeting there every year since, attended by members of the Swedish royal family. However, it was not until 1914 that the academy gained permanent use of the upper floor as their own. It is here that the academy meets and, amongst other business, announces the names of Nobel Prize laureates. This task arguably makes the academy one of the world's most influential literary bodies.

Members are elected by a secret ballot in the Academy and before the result is made public it must be submitted to the Academy's Patron, the King of Sweden, for his approval. Members of the Academy include writers, linguists, literary scholars, historians and a prominent jurist. Initially writers were in the minority in the Academy, but during the twentieth century the number of writers grew to represent more than half of The Eighteen. The Swedish Academy have a long history of being a heavily male dominated institution, but the Academy has recently moved towards better equality. Since 20 December 2019 one third of the chairs belong to female Academy members. 

Prior to 2018 it was not possible for members of the academy to resign; membership was for life, although the academy could decide to exclude members. This happened twice to Gustaf Mauritz Armfelt, who was excluded in 1794, re-elected in 1805 and excluded again in 1811. In 1989, Werner Aspenström, Kerstin Ekman and Lars Gyllensten chose to stop participating in the meetings of the academy, over its refusal to express support for Salman Rushdie when Ayatollah Khomeini condemned him to death for "The Satanic Verses", and in 2005, Knut Ahnlund made the same decision, as a protest against the choice of Elfriede Jelinek as Nobel laureate for 2004. On 25 November 2017, Lotta Lotass said in an interview that she had not participated in the meetings of the academy for more than two years and did not consider herself a member any more.
Dag Hammarskjöld's former farm at Backåkra, close to Ystad in southern Sweden, was bought in 1957 as a summer residence by Hammarskjöld, then Secretary-General of the United Nations (1953–1961). The south wing of the farm is reserved as a summer retreat for the 18 members of the Swedish Academy, of which Hammarskjöld was a member.

On 11 April 2019, the academy published its financial statements for the first time in its history. According to it, the academy owned financial assets worth 1.58 billion Swedish kronor at the end of 2018 (equal to $170M, €150M, or £130M).

In April 2018, three members of the academy board resigned in response to a sexual-misconduct investigation involving author Jean-Claude Arnault, husband of board member Katarina Frostenson. Arnault was accused by at least 18 women of sexual assault and harassment; he denied all accusations. The three members resigned in protest over the lack of appropriate action against Arnault. Two former permanent secretaries, Sture Allén and Horace Engdahl, called the current leader, Sara Danius, a weak leader.

On 10 April, Danius resigned from her position with the academy, bringing the number of empty seats to four. Frostenson voluntarily agreed to withdraw from participating in the academy, bringing the total of withdrawals to five. Because two other seats were still vacant after the Rushdie affair, this left only 11 active members. The scandal was widely seen as damaging to the credibility of the Nobel prize in Literature and the authority of the academy. "With this scandal you cannot possibly say that this group of people has any kind of solid judgment," noted Swedish journalist Björn Wiman.

On 27 April 2018, the Swedish Economic Crime Authority opened a preliminary investigation regarding financial crime linked to an association run by Arnault and Frostenson, which had received funding from the academy.

On 2 May 2018, the Swedish King amended the rules of the academy and made it possible for members to resign. The new rules also state that a member who has been inactive in the work of the academy for more than two years can be asked to resign. Following the new rules, the first members to formally be granted permission to leave the academy and vacate their chairs were Kerstin Ekman, Klas Östergren, Sara Stridsberg and Lotta Lotass.

On 4 May 2018, the Swedish Academy announced that following the preceding internal struggles the Nobel laureate for literature selected in 2018 will be postponed until 2019, when two laureates will be selected.

Since 1901, the Swedish Academy has annually decided who will be the laureate for the Nobel Prize in Literature, awarded in memory of the donor Alfred Nobel.

The Swedish Academy annually awards nearly 50 different prizes and scholarships, most of them for domestic Swedish authors. Common to all is that they are awarded without competition and without application. The Dobloug Prize, the largest of these at $40,000, is a literature prize awarded for Swedish and Norwegian fiction.

Swedish: Stora Priset, literally the Big Prize, was instituted by King Gustav III. The prize, which consists of a single gold medal, is the most prestigious award that can be awarded by the Swedish Academy. It has been awarded to, among others, Selma Lagerlöf (1904 and 1909), Herbert Tingsten (1966), Astrid Lindgren (1971), Evert Taube (1972) and Tove Jansson (1994).

The academy awards around 50 prizes each year. A person does not have to apply nor compete for the prizes.

Full list of awards (in Swedish)

The current members of the Swedish Academy listed by seat number:





</doc>
<doc id="27014" url="https://en.wikipedia.org/wiki?curid=27014" title="Svenska Dagbladet">
Svenska Dagbladet

Svenska Dagbladet (, "The Swedish Daily News"), abbreviated SvD, is a daily newspaper published in Stockholm, Sweden.

The first issue of "Svenska Dagbladet" appeared on 18 December 1884. Ivar Anderson is among its former editors-in-chief who assumed the post in 1940.

The paper is published in Stockholm and provides coverage of national and international news as well as local coverage of the Greater Stockholm region. Its subscribers are concentrated in the capital, but it is distributed in most of Sweden. During the beginning of the 1900s the paper was one of the right-wing publications in Stockholm.

"Svenska Dagbladet" is owned by Schibsted which purchased it in the late 1990s. The stated position of the editorial page is "independently moderate" ("oberoende moderat"), which means it is independent but adheres to the liberal conservatism of the Moderate Party. Despite this position, the paper is also regarded as conservative.

In November 2000 "Svenska Dagbladet" changed its format from broadsheet to tabloid. In 2005 the paper started a Web portal for business news as a joint venture with "Aftonbladet".

Since 1925 "Svenska Dagbladet" has awarded an individual sportsperson or a team the Svenska Dagbladet Gold Medal at the end of each year.

As the only other Swedish morning newspaper to aspire to full national and international coverage, Svenska Dagbladet is the chief rival of Dagens Nyheter.

Anna Careborg was appointed acting CEO and Editor-in-chief in January 2019, taking over from Fredric Karén, who is now working with Torstar Group, owners of the Toronto Star, in Canada. 

Careborg took over fully as new CEO and Editor-in-chief of Svenska Dagbladet in October 2019.

The circulation of "Svenska Dagbladet" was 185,000 copies in 2003. The paper had a circulation of 187,100 copies on weekdays in 2005. Among Swedish morning newspapers "Svenska Dagbladet" had the third largest circulation with 195,200 copies in 2007 after "Dagens Nyheter" and "Göteborgs-Posten". In 2008 "Svenska Dagbladet" had a circulation of 123,383 copies. The circulation of the paper was 185,600 copies in 2011. It was 159,600 copies in 2012 and 143,400 copies in 2013.






</doc>
<doc id="27016" url="https://en.wikipedia.org/wiki?curid=27016" title="Sture Allén">
Sture Allén

Sture Allén (born 31 December 1928) is a retired Swedish professor of computational linguistics at the University of Gothenburg, who was the permanent secretary of the Swedish Academy between 1986 and 1999. Born in Gothenburg, he was elected to chair 3 of the Swedish Academy in 1980. He is also a member of the Norwegian Academy of Science and Letters.


 


</doc>
<doc id="27018" url="https://en.wikipedia.org/wiki?curid=27018" title="Stress">
Stress

Stress may refer to:










</doc>
<doc id="27019" url="https://en.wikipedia.org/wiki?curid=27019" title="South Korea">
South Korea

South Korea (Korean: ;大韓民國 RR: "Hanguk" or literally ; RR: "Namhan"; officially the Republic of Korea ; RR: "Daehan Minguk") is a country in East Asia, constituting the southern part of the Korean Peninsula and sharing a land border with North Korea. 

The name "Korea" is derived from Goguryeo which was one of the great powers in East Asia during its time, ruling most of the Korean Peninsula, Manchuria, parts of the Russian Far East and Inner Mongolia under Gwanggaeto the Great. Its capital, Seoul, is a major global city and half of South Korea's over 51 million people live in the Seoul Capital Area, the fourth largest metropolitan economy in the world.

The Korean Peninsula was inhabited as early as the Lower Paleolithic period. Its first kingdom was noted in Chinese records in the early 7th century BC. Following the unification of the Three Kingdoms of Korea into Silla and Balhae in the late 7th century, Korea was ruled by the Goryeo dynasty (918–1392) and the Joseon dynasty (1392–1897). The succeeding Korean Empire was annexed into the Empire of Japan in 1910. After World War II, Korea was divided into Soviet and U.S.-administered zones, with the latter becoming the Republic of Korea in August 1948. In 1950, a North Korean invasion began the Korean War, one of the deadliest conflicts of the Cold War. After the war's end in 1953, the dictatorship of Park Chung Hee was established, and the country's economy began to soar, recording the fastest rise in average GDP per capita in the world between 1980 and 1990. Authoritarian rule ended in 1987 and the country is now the most advanced democracy with the highest level of press freedom in Asia. South Korea is a member of the OECD's Development Assistance Committee, the G20 and the Paris Club.

South Korea is a highly developed country and the world's 11th largest economy by nominal GDP. Its citizens enjoy the world's fastest Internet connection speeds along with the world's second best healthcare system, resulting in the third highest health adjusted life expectancy in the world. The world's 5th largest exporter and 8th largest importer, South Korea is a global leader in many technology and innovation driven fields. Since 2014, South Korea has been named the world's most innovative country by the Bloomberg Innovation Index for 6 consecutive years. Since the 21st century, South Korea has been renowned for its globally influential pop culture such as K-pop and TV dramas, a phenomenon referred to as the Korean Wave.

The name "Korea" derives from the name "Goryeo". The name "Goryeo" itself was first used by the ancient kingdom of Goguryeo in the 5th century as a shortened form of its name. The 10th-century kingdom of Goryeo succeeded Goguryeo, and thus inherited its name, which was pronounced by the visiting Persian merchants as "Korea". The modern spelling of Korea first appeared in the late 17th century in the travel writings of the Dutch East India Company's Hendrick Hamel. Despite the coexistence of the spellings "Corea" and "Korea" in 19th century publications, some Koreans believe that Imperial Japan, around the time of the Japanese occupation, intentionally standardised the spelling on "Korea", making Japan appear first alphabetically.

After Goryeo was replaced by Joseon in 1392, Joseon became the official name for the entire territory, though it was not universally accepted. The new official name has its origin in the ancient country of Gojoseon (Old Joseon). In 1897, the Joseon dynasty changed the official name of the country from "Joseon" to "Daehan Jeguk" (Korean Empire). The name "Daehan" (Great Han) derives from Samhan (Three Han), referring to the Three Kingdoms of Korea, not the ancient confederacies in the southern Korean Peninsula. However, the name "Joseon" was still widely used by Koreans to refer to their country, though it was no longer the official name. Under Japanese rule, the two names "Han" and "Joseon" coexisted. There were several groups who fought for independence, the most notable being the "Provisional Government of the Republic of Korea" (/).

Following the surrender of Japan, in 1945, the "Republic of Korea" (/, IPA: , ; ) was adopted as the legal English name for the new country. However, it is not a direct translation of the Korean name. As a result, the Korean name "Daehan Minguk" is sometimes used by South Koreans as a metonym to refer to the Korean ethnicity (or "race") as a whole, rather than just the South Korean state.

Since the government only controlled the southern part of the Korean Peninsula, the informal term "South Korea" was coined, becoming increasingly common in the Western world. While South Koreans use "Han" (or "Hanguk") to refer to both Koreas collectively, North Koreans and ethnic Koreans living in China and Japan use the term "Joseon" instead.

The Korean Peninsula was inhabited as early as the Lower Paleolithic period. The history of Korea begins with the founding of Joseon (also known as "Gojoseon", or Old Joseon, to differentiate it with the 14th century dynasty) in 2333 BCE by Dangun, according to Korea's foundation mythology. Gojoseon was noted in Chinese records in the early 7th century. Gojoseon expanded until it controlled the northern Korean Peninsula and parts of Manchuria. Gija Joseon was purportedly founded in the 12th century BC, but its existence and role have been controversial in the modern era. In 108 BCE, the Han dynasty defeated Wiman Joseon and installed four commanderies in the northern Korean peninsula. Three of the commanderies fell or retreated westward within a few decades. As Lelang commandery was destroyed and rebuilt around this time, the place gradually moved toward Liaodong. Thus, its force was diminished and it only served as a trade center until it was conquered by Goguryeo in 313.

During the period known as the Proto–Three Kingdoms of Korea, the states of Buyeo, Okjeo, Dongye and Samhan occupied the whole Korean peninsula and southern Manchuria. From them, Goguryeo, Baekje and Silla emerged to control the peninsula as the Three Kingdoms of Korea. Goguryeo, the largest and most powerful among them, was a highly militaristic state, and competed with various Chinese dynasties during its 700 years of history. Goguryeo experienced a golden age under Gwanggaeto the Great and his son Jangsu, who both subdued Baekje and Silla during their times, achieving a brief unification of the Three Kingdoms of Korea and becoming the most dominant power on the Korean Peninsula. In addition to contesting for control of the Korean Peninsula, Goguryeo had many military conflicts with various Chinese dynasties, most notably the Goguryeo–Sui War, in which Goguryeo defeated a huge force said to number over a million men. Baekje was a great maritime power; its nautical skill, which made it the Phoenicia of East Asia, was instrumental in the dissemination of Buddhism throughout East Asia and continental culture to Japan. Baekje was once a great military power on the Korean Peninsula, especially during the time of Geunchogo, but was critically defeated by Gwanggaeto the Great and declined. Silla was the smallest and weakest of the three, but it used cunning diplomatic means to make opportunistic pacts and alliances with the more powerful Korean kingdoms, and eventually Tang China, to its great advantage.

The unification of the Three Kingdoms by Silla in 676 led to the North South States Period, in which much of the Korean Peninsula was controlled by Later Silla, while Balhae controlled the northern parts of Goguryeo. Balhae was founded by a Goguryeo general and formed as a successor state to Goguryeo. During its height, Balhae controlled most of Manchuria and parts of the Russian Far East, and was called the "Prosperous Country in the East". Later Silla was a golden age of art and culture, as evidenced by the Hwangnyongsa, Seokguram, and Emille Bell. Relationships between Korea and China remained relatively peaceful during this time. Later Silla carried on the maritime prowess of Baekje, which acted like the Phoenicia of medieval East Asia, and during the 8th and 9th centuries dominated the seas of East Asia and the trade between China, Korea and Japan, most notably during the time of Jang Bogo; in addition, Silla people made overseas communities in China on the Shandong Peninsula and the mouth of the Yangtze River. Later Silla was a prosperous and wealthy country, and its metropolitan capital of Gyeongju was the fourth largest city in the world. Buddhism flourished during this time, and many Korean Buddhists gained great fame among Chinese Buddhists and contributed to Chinese Buddhism, including: Woncheuk, Wonhyo, Uisang, Musang, and Kim Gyo-gak, a Silla prince whose influence made Mount Jiuhua one of the Four Sacred Mountains of Chinese Buddhism. However, Later Silla weakened under internal strife and the revival of Baekje and Goguryeo, which led to the Later Three Kingdoms period in the late 9th century.

In 936, the Later Three Kingdoms were united by Wang Geon, a descendant of Goguryeo nobility, who established Goryeo as the successor state of Goguryeo. Balhae had fallen to the Khitan Empire in 926, and a decade later the last crown prince of Balhae fled south to Goryeo, where he was warmly welcomed and included into the ruling family by Wang Geon, thus unifying the two successor nations of Goguryeo. Like Silla, Goryeo was a highly cultural state, and invented the metal movable type printing press. After defeating the Khitan Empire, which was the most powerful empire of its time, in the Goryeo–Khitan War, Goryeo experienced a golden age that lasted a century, during which the Tripitaka Koreana was completed and there were great developments in printing and publishing, promoting learning and dispersing knowledge on philosophy, literature, religion, and science; by 1100, there were 12 universities that produced famous scholars and scientists. However, the Mongol invasions in the 13th century greatly weakened the kingdom. Goryeo was never conquered by the Mongols, but exhausted after three decades of fighting, the Korean court sent its crown prince to the Yuan capital to swear allegiance to Kublai Khan, who accepted, and married one of his daughters to the Korean crown prince. Henceforth, Goryeo continued to rule Korea, though as a tributary ally to the Mongols for the next 86 years. During this period, the two nations became intertwined as all subsequent Korean kings married Mongol princesses, and the last empress of the Yuan dynasty was a Korean princess. In the mid-14th century, Goryeo drove out the Mongols to regain its northern territories, briefly conquered Liaoyang, and defeated invasions by the Red Turbans. However, in 1392, General Yi Seong-gye, who had been ordered to attack China, turned his army around and staged a coup.

Yi Seong-gye declared the new name of Korea as "Joseon" in reference to Gojoseon, and moved the capital to Hanseong (one of the old names of Seoul). The first 200 years of the Joseon dynasty were marked by peace, and saw great advancements in science and education, as well as the creation of Hangul by Sejong the Great to promote literacy among the common people. The prevailing ideology of the time was Neo-Confucianism, which was epitomized by the seonbi class: nobles who passed up positions of wealth and power to lead lives of study and integrity. Between 1592 and 1598, Toyotomi Hideyoshi launched invasions of Korea, but his advance was halted by Korean forces (most notably the Joseon Navy led by Admiral Yi Sun-sin and his renowned "turtle ship") with assistance from Righteous Army militias formed by Korean civilians, and Ming dynasty Chinese troops. Through a series of successful battles of attrition, the Japanese forces were eventually forced to withdraw, and relations between all parties became normalized. However, the Manchus took advantage of Joseon's war-weakened state and invaded in 1627 and 1637, and then went on to conquer the destabilized Ming dynasty. After normalizing relations with the new Qing dynasty, Joseon experienced a nearly 200-year period of peace. Kings Yeongjo and Jeongjo particularly led a new renaissance of the Joseon dynasty during the 18th century. In the 19th century, the royal in-law families gained control of the government, leading to mass corruption and weakening of the state, and severe poverty and peasant rebellions throughout the country. Furthermore, the Joseon government adopted a strict isolationist policy, earning the nickname "the hermit kingdom", but ultimately failed to protect itself against imperialism and was forced to open its borders. After the First Sino-Japanese War and the Russo-Japanese War, Korea was occupied by Japan (1910–45). At the end of World War II, the Japanese surrendered to Soviet and U.S. forces who occupied the northern and southern halves of Korea, respectively.

Despite the initial plan of a unified Korea in the 1943 Cairo Declaration, escalating Cold War antagonism between the Soviet Union and the United States eventually led to the establishment of separate governments, each with its own ideology, leading to the division of Korea into two political entities in 1948: North Korea and South Korea. In the South, Syngman Rhee, an opponent of communism, who had been backed and appointed by the United States as head of the provisional government, won the first presidential elections of the newly declared Republic of Korea in May. In the North, however, a former anti-Japanese guerrilla and communist activist, Kim Il-sung was appointed premier of the Democratic People's Republic of Korea in September.

In October, the Soviet Union declared Kim Il-sung's government as sovereign over both parts. The UN declared Rhee's government as "a lawful government having effective control and jurisdiction over that part of Korea where the UN Temporary Commission on Korea was able to observe and consult" and the Government "based on elections which was observed by the Temporary Commission" in addition to a statement that "this is the only such government in Korea." Both leaders began an authoritarian repression of their political opponents inside their region, seeking for a unification of Korea under their control. While South Korea's request for military support was denied by the United States, North Korea's military was heavily reinforced by the Soviet Union.

On June 25, 1950, North Korea invaded South Korea, sparking the Korean War, the Cold War's first major conflict, which continued until 1953. At the time, the Soviet Union had boycotted the United Nations (UN), thus forfeiting their veto rights. This allowed the UN to intervene in a civil war when it became apparent that the superior North Korean forces would unify the entire country. The Soviet Union and China backed North Korea, with the later participation of millions of Chinese troops. After an ebb and flow that saw both sides almost pushed to the brink of extinction, and massive losses among Korean civilians in both the north and the south, the war eventually reached a stalemate. During the war, Rhee's party promoted the One-People Principle (based on the German ideology of the "Herrenvolk") an effort to build an obedient citizenry through ethnic homogeneity and authoritarian appeals to nationalism.

The 1953 armistice, never signed by South Korea, split the peninsula along the demilitarized zone near the original demarcation line. No peace treaty was ever signed, resulting in the two countries remaining technically at war. Approximately 3 million people died in the Korean War, with a higher proportional civilian death toll than World War II or the Vietnam War, making it perhaps the deadliest conflict of the Cold War-era. In addition, virtually all of Korea's major cities were destroyed by the war.

In 1960, a student uprising (the "April 19 Revolution") led to the resignation of the autocratic then-President Syngman Rhee. This was followed by 13 months of political instability as South Korea was led by a weak and ineffectual government. This instability was broken by the May 16, 1961, coup led by General Park Chung-hee. As president, Park oversaw a period of rapid export-led economic growth enforced by political repression.

Park was heavily criticized as a ruthless military dictator, who in 1972 extended his rule by creating a new constitution, which gave the president sweeping (almost dictatorial) powers and permitted him to run for an unlimited number of six-year terms. The Korean economy developed significantly during Park's tenure. The government developed the nationwide expressway system, the Seoul subway system, and laid the foundation for economic development during his 17-year tenure, which ended with his assassination in 1979.

The years after Park's assassination were marked again by political turmoil, as the previously suppressed opposition leaders all campaigned to run for president in the sudden political void. In 1979 there came the Coup d'état of December Twelfth led by General Chun Doo-hwan. Following the Coup d'état, Chun Doo-hwan planned to rise to power through several measures. On May 17, Chun Doo-hwan forced the Cabinet to expand martial law to the whole nation, which had previously not applied to the island of Jejudo. The expanded martial law closed universities, banned political activities and further curtailed the press. Chun's assumption of the presidency in the events of May 17, triggered nationwide protests demanding democracy, in particular in the city of Gwangju, to which Chun sent special forces to violently suppress the Gwangju Democratization Movement.

Chun subsequently created the National Defense Emergency Policy Committee and took the presidency according to his political plan. Chun and his government held South Korea under a despotic rule until 1987, when a Seoul National University student, Park Jong-chul, was tortured to death. On , the Catholic Priests Association for Justice revealed the incident, igniting the June Democracy Movement around the country. Eventually, Chun's party, the Democratic Justice Party, and its leader, Roh Tae-woo announced the 6.29 Declaration, which included the direct election of the president. Roh went on to win the election by a narrow margin against the two main opposition leaders, Kim Dae-Jung and Kim Young-Sam. Seoul hosted the Olympic Games in 1988, widely regarded as successful and a significant boost for South Korea's global image and economy.

South Korea was formally invited to become a member of the United Nations in 1991. The transition of Korea from autocracy to modern democracy was marked in 1997 by the election of Kim Dae-jung, who was sworn in as the eighth president of South Korea, on February 25, 1998. His election was significant given that he had in earlier years been a political prisoner sentenced to death (later commuted to exile). He won against the backdrop of the 1997 Asian Financial Crisis, where he took IMF advice to restructure the economy and the nation soon recovered its economic growth, albeit at a slower pace.

In June 2000, as part of president Kim Dae-jung's "Sunshine Policy" of engagement, a North–South summit took place in Pyongyang, the capital of North Korea. Later that year, Kim received the Nobel Peace Prize "for his work for democracy and human rights in South Korea and in East Asia in general, and for peace and reconciliation with North Korea in particular". However, because of discontent among the population for fruitless approaches to the North under the previous administrations and, amid North Korean provocations, a conservative government was elected in 2007 led by President Lee Myung-bak, former mayor of Seoul. Meanwhile, South Korea and Japan jointly co-hosted the 2002 FIFA World Cup. However, South Korean and Japanese relations later soured because of conflicting claims of sovereignty over the Liancourt Rocks.

In 2010, there was an escalation in attacks by North Korea. In March 2010 the South Korean warship ROKS Cheonan was sunk with the loss of 46 South Korean sailors, allegedly by a North Korean submarine. In November 2010 Yeonpyeong island was attacked by a significant North Korean artillery barrage, with 4 people losing their lives. The lack of a strong response to these attacks from both South Korea and the international community (the official UN report declined to explicitly name North Korea as the perpetrator for the Cheonan sinking) caused significant anger with the South Korean public. South Korea saw another milestone in 2012 with the first ever female president Park Geun-hye elected and assuming office. Daughter of another former president, Park Chung-hee, she carried on a conservative brand of politics. President Park Geun-hye's administration was formally accused of corruption, bribery, and influence-peddling for the involvement of close friend Choi Soon-sil in state affairs. There followed a series of massive public demonstrations from November 2016 and she was removed from office. After the fallout of President Park's impeachment and dismissal, new elections were held and Moon Jae-in of the Democratic party won the presidency, assuming office on 10 May 2017. His tenure so far has seen an improving political relationship with North Korea, some increasing divergence in the military alliance with the United States, and the successful hosting of the Winter Olympics in Pyeongchang.

South Korea occupies the southern portion of the Korean Peninsula, which extends some from the Asian mainland. This mountainous peninsula is flanked by the Yellow Sea to the west, and the Sea of Japan to the east. Its southern tip lies on the Korea Strait and the East China Sea.

The country, including all its islands, lies between latitudes 33° and 39°N, and longitudes 124° and 130°E. Its total area is .

South Korea can be divided into four general regions: an eastern region of high mountain ranges and narrow coastal plains; a western region of broad coastal plains, river basins, and rolling hills; a southwestern region of mountains and valleys; and a southeastern region dominated by the broad basin of the Nakdong River.

South Korea's terrain is mostly mountainous, most of which is not arable. Lowlands, located primarily in the west and southeast, make up only 30% of the total land area.

About three thousand islands, mostly small and uninhabited, lie off the western and southern coasts of South Korea. Jeju-do is about off the southern coast of South Korea. It is the country's largest island, with an area of . Jeju is also the site of South Korea's highest point: Hallasan, an extinct volcano, reaches above sea level. The easternmost islands of South Korea include Ulleungdo and Liancourt Rocks (Dokdo/Takeshima), while Marado and Socotra Rock are the southernmost islands of South Korea.

South Korea has 20 national parks and popular nature places like the Boseong Tea Fields, Suncheon Bay Ecological Park, and the first national park of Jirisan.

South Korea tends to have a humid continental climate and a humid subtropical climate, and is affected by the East Asian monsoon, with precipitation heavier in summer during a short rainy season called "jangma" (), which begins end of June through the end of July. Winters can be extremely cold with the minimum temperature dropping below in the inland region of the country: in Seoul, the average January temperature range is , and the average August temperature range is . Winter temperatures are higher along the southern coast and considerably lower in the mountainous interior. Summer can be uncomfortably hot and humid, with temperatures exceeding in most parts of the country. South Korea has four distinct seasons; spring, summer, autumn and winter. Spring usually lasts from late March to early May, summer from mid-May to early September, autumn from mid-September to early November, and winter from mid-November to mid-March.

Rainfall is concentrated in the summer months of June through September. The southern coast is subject to late summer typhoons that bring strong winds, heavy rains and sometime floods. The average annual precipitation varies from in Seoul to in Busan.

During the first 20 years of South Korea's growth surge, little effort was made to preserve the environment. Unchecked industrialization and urban development have resulted in deforestation and the ongoing destruction of wetlands such as the Songdo Tidal Flat. However, there have been recent efforts to balance these problems, including a government run five-year green growth project that aims to boost energy efficiency and green technology.

The green-based economic strategy is a comprehensive overhaul of South Korea's economy, utilizing nearly two percent of the national GDP. The greening initiative includes such efforts as a nationwide bike network, solar and wind energy, lowering oil dependent vehicles, backing daylight savings and extensive usage of environmentally friendly technologies such as LEDs in electronics and lighting. The country – already the world's most wired – plans to build a nationwide next-generation network that will be 10 times faster than broadband facilities, in order to reduce energy usage.

The renewable portfolio standard program with renewable energy certificates runs from 2012 to 2022.
Quota systems favor large, vertically integrated generators and multinational electric utilities, if only because certificates are generally denominated in units of one megawatt-hour. They are also more difficult to design and implement than a Feed-in tariff. Around 350 residential micro combined heat and power units were installed in 2012.

Seoul's tap water recently became safe to drink, with city officials branding it "Arisu" in a bid to convince the public. Efforts have also been made with afforestation projects. Another multibillion-dollar project was the restoration of Cheonggyecheon, a stream running through downtown Seoul that had earlier been paved over by a motorway. One major challenge is air quality, with acid rain, sulfur oxides, and annual yellow dust storms being particular problems. It is acknowledged that many of these difficulties are a result of South Korea's proximity to China, which is a major air polluter.

South Korea is a member of the Antarctic-Environmental Protocol, Antarctic Treaty, Biodiversity Treaty, Kyoto Protocol (forming the Environmental Integrity Group (EIG), regarding UNFCCC, with Mexico and Switzerland), Desertification, Endangered Species, Environmental Modification, Hazardous Wastes, Law of the Sea, Marine Dumping, Comprehensive Nuclear-Test-Ban Treaty (not into force), Ozone Layer Protection, Ship Pollution, Tropical Timber 83, Tropical Timber 94, Wetlands, and Whaling.

The South Korean government's structure is determined by the Constitution of the Republic of Korea. Like many democratic states, South Korea has a government divided into three branches: executive, judicial, and legislative. The executive and legislative branches operate primarily at the national level, although various ministries in the executive branch also carry out local functions. Local governments are semi-autonomous, and contain executive and legislative bodies of their own. The judicial branch operates at both the national and local levels. South Korea is a constitutional democracy.
The constitution has been revised several times since its first promulgation in 1948 at independence. However, it has retained many broad characteristics and with the exception of the short-lived Second Republic of South Korea, the country has always had a presidential system with an independent chief executive. Under its current constitution the state is sometimes referred to as the Sixth Republic of South Korea. The first direct election was also held in 1948.

Although South Korea experienced a series of military dictatorships from the 1960s until the 1980s, it has since developed into a successful liberal democracy. Today, the CIA World Factbook describes South Korea's democracy as a "fully functioning modern democracy". South Korea is ranked 45th on the Corruption Perceptions Index (9th in the Asia-Pacific region), with a score of 57 out of 100.

The major administrative divisions in South Korea are eight provinces, one special self-governing province, six metropolitan cities (self-governing cities that are not part of any province), one special city and one metropolitan autonomous city.

In April 2016, South Korea's population was estimated to be around 50.8 million by National Statistical Office, with continuing decline of working age population and total fertility rate. The country is noted for its population density, which was an estimated 505 per square kilometer in 2015, more than 10 times the global average. Aside from micro-states and city-states, South Korea is the world's third most densely-populated country. In practice the population density in much of South Korea is higher than the national one, as most of the country's land is uninhabitable due to being used for other purposes such as farming. Most South Koreans live in urban areas, because of rapid migration from the countryside during the country's quick economic expansion in the 1970s, 1980s and 1990s. The capital city of Seoul is also the country's largest city and chief industrial center. According to the 2005 census, Seoul had a population of inhabitants. The Seoul National Capital Area has inhabitants (about half of South Korea's entire population) making it the world's second largest metropolitan area. Other major cities include Busan (), Incheon (), Daegu (), Daejeon (), Gwangju () and Ulsan ().
The population has also been shaped by international migration. After World War II and the division of the Korean Peninsula, about four million people from North Korea crossed the border to South Korea. This trend of net entry reversed over the next 40 years because of emigration, especially to North America through the United States and Canada. South Korea's total population in 1955 was , and has more than doubled, to 50 million, by 2010.

South Korea is considered one of the most ethnically homogeneous societies in the world with ethnic Koreans representing approximately 96% of total population. Precise numbers are difficult since statistics do not record ethnicity and given many immigrants are ethnically Korean themselves, and some South Korean citizens are not ethnically Korean. South Korea is nevertheless becoming a more multi-ethnic society over time due to immigration.

The percentage of foreign nationals has been growing rapidly. , South Korea had 1,413,758 foreign residents, 2.75% of the population; however, many of them are ethnic Koreans with a foreign citizenship. For example, migrants from China (PRC) make up 56.5% of foreign nationals, but approximately 70% of the Chinese citizens in Korea are (), PRC citizens of Korean ethnicity. Regardless of the ethnicity, there are 28,500 US military personnel serving in South Korea, most serving a one-year unaccompanied tour (though approximately 10% serve longer tours accompanied by family), according to the Korea National Statistical Office. In addition, about 43,000 English teachers from English-speaking countries reside temporarily in Korea. Currently, South Korea has one of the highest rates of growth of foreign born population, with about 30,000 foreign born residents obtaining South Korean citizenship every year since 2010.

Large number of ethnic Koreans live overseas, sometimes in Korean ethnic neighbourhoods also known as Koreatowns. The four largest diaspora population can be found in China (2.3 million), USA (1.8 million), Japan (0.85 million), and Canada (0.25 million).

South Korea's birthrate was the world's lowest in 2009, at an annual rate of approximately 9 births per 1000 people. Fertility saw some modest increase afterwards, but dropped to a new global low in 2017, with fewer than 30,000 births per month for the first time since records began and less than 1 child per woman as of 2018 trends. The average life expectancy in 2008 was 79.10 years, (which was 34th in the world) but by 2015 it had increased to around 81. South Korea has the steepest decline in working age population of the OECD nations. In 2015, National Statistical Office estimated that the population of the country will have reached its peak by 2035.

A centralized administration in South Korea oversees the process for the education of children from kindergarten to the third and final year of high school. The school year is divided into two semesters, the first of which begins at the beginning of March and ends in mid-July, the second of which begins in late August and ends in mid-February. The schedules are not uniformly standardized and vary from school to school. Most South Korean middle schools and high schools have school uniforms, modeled on western-style uniforms. Boys' uniforms usually consist of trousers and white shirts, and girls wear skirts and white shirts (this only applies in middle schools and high schools). The country adopted a new educational program to increase the number of their foreign students through 2010. According to the Ministry of Education, Science and Technology, the number of scholarships for foreign students in South Korea would have (under the program) doubled by that time, and the number of foreign students would have reached 100,000.

South Korea is one of the top-performing OECD countries in reading literacy, mathematics and sciences with the average student scoring 519, compared with the OECD average of 492, placing it ninth in the world and has one of the world's most highly educated labor forces among OECD countries. The country has one of the world's highest-educated labour forces among OECD countries. The country is well known for its highly feverish outlook on education, where its national obsession with education has been called "education fever". This obsession with education has catapulted the resource poor nation consistently atop the global education rankings where in 2014 national rankings of students' math and science scores by the Organization for Economic and Cooperation and Development (OECD), South Korea ranked second place worldwide, after Singapore.

Higher education is a serious issue in South Korea society, where it is viewed as one of the fundamental cornerstones of South Korean life. Education is regarded with a high priority for South Korean families as success in education is often a source of pride for families and within South Korean society at large, and is a necessity to improve one's socioeconomic position in South Korean society. South Koreans view education as the main propeller of social mobility for themselves and their family as a gateway to the South Korean middle class. Graduating from a top university is the ultimate marker of prestige, high socioeconomic status, promising marriage prospects, and a respectable career path. The entrance into a top tier higher educational institution leads to a prestigious, secure and well-paid white collar job with the government, banks, or a major South Korean conglomerate such as Samsung, Hyundai or LG Electronics. An average South Korean child's life revolves around education as pressure to succeed academically is deeply ingrained in South Korean children from an early age. With incredible pressure on high school students to secure places at the nation's best universities, its institutional reputation and alumni networks are strong predictors of future career prospects. The top three universities in South Korea, often referred to as "SKY", are Seoul National University, Korea University and Yonsei University. Intense competition for top grades and academic pressure to be the top student is deeply ingrained in the psyche of South Korean students at a young age. Yet with only so many places at the nations most prestigious universities and even fewer places at top-tier companies, many young people remain disappointed and are often unwilling to lower their sights with the result of many feeling as underachievers. There is a major cultural taboo in South Korean society attached to those who have not achieved formal university education where those who do not hold university degrees face social prejudice and are often looked down by others as second-class citizens resulting in fewer opportunities for employment, improvement of one's socioeconomic position and prospects for marriage.
In 2015, the country spent 5.1% of its GDP on all levels of education – roughly 0.8 percentage points above the Organisation for Economic Co-operation and Development (OECD) average of 4.3%. A strong investment in education, a militant drive for success as well as the passion for excellence has helped the resource poor country rapidly grow its economy over the past 60 years from a war torn wasteland.

International opinion regarding the South Korean education system has been divided. It has been praised for various reasons, including its comparatively high test results and its major role in ushering South Korea's economic development creating one of the world's most educated workforces.
South Korea's highly enviable academic performance has persuaded British education ministers to actively remodel their own curriculums and exams to try to emulate Korea's militant drive and passion for excellence and high educational achievement. Former U.S. President Barack Obama has also praised the country's rigorous school system, where over 80 percent of South Korean high school graduates go on to university. The nation's high university entrance rate has created a highly skilled workforce making South Korea among the most highly educated countries in the world with one of the highest percentages of its citizens holding a tertiary education degree. In 2017, the country ranked fifth for the percentage of 25 to 64 year olds that have attained tertiary education with 47.7 percent. In addition, 69.8 percent of South Koreans aged 25–34 have completed some form of tertiary education qualification and bachelor's degrees are held by 34.2 percent of South Koreans aged 25–64, the most in the OECD.

The system's rigid and hierarchical structure has been criticized for stifling creativity and innovation; described as intensely and "brutally" competitive, the system is often blamed for the high suicide rate in the country, particularly the growing rates among those aged 10–19. Various media outlets attribute the country's high suicide rate to the nationwide anxiety around the country's college entrance exams, which determine the trajectory of students' entire lives and careers. Former South Korean "hagwon" teacher Se-Woong Koo wrote that the South Korean education system amounts to child abuse and that it should be "reformed and restructured without delay". The system has also been criticized for producing an excess supply of university graduates creating an overeducated and underemployed labor force; in the first quarter of 2013 alone, nearly 3.3 million South Korean university graduates were jobless, leaving many graduates overqualified for jobs requiring less education. Further criticism has been stemmed for causing labor shortages in various skilled blue collar labor and vocational occupations, where many go unfilled as the negative social stigma associated with vocational careers and not having a university degree continues to remain deep-rooted in South Korean society.

Korean is the official language of South Korea, and is classified by most linguists as a language isolate. Korean is not related to any Chinese languages, although it incorporates a number of words that are Chinese in origin. Additionally, Korean spoken in South Korea uses a significant number of loan words from English and other European languages. Korean uses an indigenous writing system called Hangul, created in 1446 by King Sejong to provide a convenient alternative to the Classical Chinese Hanja characters that were difficult to learn and did not fit the Korean language well. South Korea still uses some Chinese Hanja characters in limited areas, such as print media and legal documentation.

The Korean language in South Korea has a standard dialect known as Seoul (after the capital city), with an additional 4 Korean language dialect groups in use around the country.

Almost all South Korean students today learn English throughout their education, with some optionally choosing Japanese or Mandarin as well.

According to the results of the census of 2015 more than half of the South Korean population (56.9%) declared themselves not affiliated with any religious organizations. In a 2012 survey, 52% declared themselves "religious", 31% said they were "not religious" and 15% identified themselves as "convinced atheists". Of the people who are affiliated with a religious organization, most are Christians and Buddhists. According to the 2015 census, 27.6% of the population were Christians (19.7% identified themselves as Protestants, 7.9% as Roman Catholics), and 15.5% were Buddhists. Other religions include Islam (130,000 Muslims, mostly migrant workers from Pakistan and Bangladesh but including some 35,000 Korean Muslims,) the homegrown sect of Wonbuddhism, and a variety of indigenous religions, including Cheondoism (a Confucianizing religion), Jeungsanism, Daejongism, Daesun Jinrihoe and others. Freedom of religion is guaranteed by the constitution, and there is no state religion. Overall, between the 2005 and 2015 censuses there has been a slight decline of Christianity (down from 29% to 27.6%), a sharp decline of Buddhism (down from 22.8% to 15.5%), and a rise of the unaffiliated population (from 47.2% to 56.9%).

Christianity is South Korea's largest organized religion, accounting for more than half of all South Korean adherents of religious organizations. There are approximately 13.5 million Christians in South Korea today; about two thirds of them belonging to Protestant churches, and the rest to the Roman Catholic Church. The number of Protestants has been stagnant throughout the 1990s and the 2000s, but increased to a peak level throughout the 2010s. Roman Catholics increased significantly between the 1980s and the 2000s, but declined throughout the 2010s. Christianity, unlike in other East Asian countries, found fertile ground in Korea in the 18th century, and by the end of the 18th century it persuaded a large part of the population as the declining monarchy supported it and opened the country to widespread proselytism as part of a project of Westernization. The weakness of Korean Sindo, which, unlike Japanese Shinto and China's religious system, never developed into a national religion of high status, combined with the impoverished state of Korean Buddhism (after 500 years of suppression at the hands of the Joseon state, by the 20th century it was virtually extinct) left a free hand to Christian churches. Christianity's similarity to native religious narratives has been studied as another factor that contributed to its success in the peninsula. The Japanese colonization of the first half of the 20th century further strengthened the identification of Christianity with Korean nationalism, as the Japanese coopted native Korean Sindo into the Nipponic Imperial Shinto that they tried to establish in the peninsula. Widespread Christianization of the Koreans took place during State Shinto, after its abolition, and then in the independent South Korea as the newly established military government supported Christianity and tried to utterly oust native Sindo.
Among Christian denominations, Presbyterianism is the largest. About nine million people belong to one of the hundred different Presbyterian churches; the biggest ones are the HapDong Presbyterian Church, TongHap Presbyterian Church, the Koshin Presbyterian Church. South Korea is also the second-largest missionary-sending nation, after the United States.

Buddhism was introduced to Korea in the 4th century. It became soon a dominant religion in the southeastern kingdom of Silla, the region that hitherto hosts the strongest concentration of Buddhists in South Korea. In the other states of the Three Kingdoms Period, Goguryeo and Baekje, it was made the state religion respectively in 372 and 528. It remained the state religion in Later Silla (North South States Period) and Goryeo. It was later suppressed throughout much of the subsequent history under the unified kingdom of Joseon (1392–1897), which officially adopted a strict Korean Confucianism. Today, South Korea has about 7 million Buddhists, most of them affiliated to the Jogye Order. Most of the National Treasures of South Korea are Buddhist artifacts.

South Korea has a universal healthcare system. It has the world's second best healthcare system.

Suicide in South Korea is a serious and widespread problem and the country ranks poorly on world happiness reports for a high-income state. The suicide rate was the highest in the G20 in 2015 (24.1 deaths per 100,000 persons).

South Korean hospitals have advanced medical equipment and facilities readily available, ranking 4th for MRI units per capita and 6th for CT scanners per capita in the OECD. It also had the OECD's second largest number of hospital beds per 1000 people at 9.56 beds.

Life expectancy has been rising rapidly and South Korea ranked 11th in the world for life expectancy at 82.3 years by the WHO in 2015. It also has the third highest health adjusted life expectancy in the world.

South Korea maintains diplomatic relations with more than 188 countries. The country has also been a member of the United Nations since 1991, when it became a member state at the same time as North Korea. On January 1, 2007, Former South Korean Foreign Minister Ban Ki-moon served as UN Secretary-General from 2007 to 2016. It has also developed links with the Association of Southeast Asian Nations as both a member of "ASEAN Plus three," a body of observers, and the East Asia Summit (EAS).

In November 2009 South Korea joined the OECD Development Assistance Committee, marking the first time a former aid recipient country joined the group as a donor member.

South Korea hosted the G-20 Summit in Seoul in November 2010, a year that saw South Korea and the European Union conclude a free trade agreement (FTA) to reduce trade barriers. South Korea went on to sign a Free Trade Agreements with Canada and Australia in 2014, and another with New Zealand in 2015.

Both North and South Korea claim complete sovereignty over the entire peninsula and outlying islands. Despite mutual animosity, reconciliation efforts have continued since the initial separation between North and South Korea. Political figures such as Kim Koo worked to reconcile the two governments even after the Korean War. With longstanding animosity following the Korean War from 1950 to 1953, North Korea and South Korea signed an agreement to pursue peace. On October 4, 2007, Roh Moo-Hyun and North Korean leader Kim Jong-il signed an eight-point agreement on issues of permanent peace, high-level talks, economic cooperation, renewal of train services, highway and air travel, and a joint Olympic cheering squad.
Despite the Sunshine Policy and efforts at reconciliation, the progress was complicated by North Korean missile tests in 1993, 1998, 2006, 2009, and 2013. , relationships between North and South Korea were very tense; North Korea had been reported to have deployed missiles, ended its former agreements with South Korea, and threatened South Korea and the United States not to interfere with a satellite launch it had planned. North and South Korea are still technically at war (having never signed a peace treaty after the Korean War) and share the world's most heavily fortified border. On May 27, 2009, North Korean media declared that the Armistice is no longer valid because of the South Korean government's pledge to "definitely join" the Proliferation Security Initiative. To further complicate and intensify strains between the two nations, the sinking of the South Korean warship Cheonan in March 2010, is affirmed by the South Korean government to have been caused by a North Korean torpedo, which the North denies. President Lee Myung-bak declared in May 2010 that Seoul would cut all trade with North Korea as part of measures primarily aimed at striking back at North Korea diplomatically and financially, except for the joint Kaesong Industrial Project, and humanitarian aid. North Korea initially threatened to sever all ties, to completely abrogate the previous pact of non-aggression, and to expel all South Koreans from a joint industrial zone in Kaesong, but backtracked on its threats and decided to continue its ties with South Korea. Despite the continuing ties, Kaesong industrial zone has seen a large decrease in investment and manpower as a result of this military conflict. In February 2016, the Kaesong complex was closed by Seoul in reaction to North Korea's launch of a rocket earlier in the month unanimously condemned by the United Nations security council.
The 2017 election of President Moon Jae-in has seen a change in approach towards the North, and both sides used the South Korean held 2018 Winter Olympics as an opportunity for engagement, with a very senior North Korean political delegation sent to the games, along with a reciprocal visit by senior South Korean cabinet members to the North soon afterwards.

Historically, Korea had close relations with the dynasties in China, and some Korean kingdoms were members of the Imperial Chinese tributary system. The Korean kingdoms also ruled over some Chinese kingdoms including the Kitan people and the Manchurians before the Qing dynasty and received tributes from them. In modern times, before the formation of South Korea, Korean independence fighters worked with Chinese soldiers during the Japanese occupation. However, after World War II, the People's Republic of China embraced Maoism while South Korea sought close relations with the United States. The PRC assisted North Korea with manpower and supplies during the Korean War, and in its aftermath the diplomatic relationship between South Korea and the PRC almost completely ceased. Relations thawed gradually and South Korea and the PRC re-established formal diplomatic relations on August 24, 1992. The two countries sought to improve bilateral relations and lifted the forty-year-old trade embargo, and South Korean–Chinese relations have improved steadily since 1992. The Republic of Korea broke off official relations with the Republic of China (Taiwan) upon gaining official relations with the People's Republic of China, which does not recognise Taiwan's sovereignty.
China has become South Korea's largest trading partner by far, sending 26% of South Korean exports in 2016 worth $124 billion, as well as an additional $32 billion worth of exports to Hong Kong. South Korea is also China's 4th largest trading partner, with $93 billion of Chinese imports in 2016.

The 2017 deployment of THAAD defence missiles by the United States military in South Korea in response to North Korean missile tests has been protested strongly by the Chinese government, concerned that the technologically advanced missile defence could be used more broadly against China. Relations between the governments have cooled in response, with South Korean commercial and cultural interests in China having been targeted, and Chinese tourism to South Korea having been curtailed. The situation was largely resolved by South Korea making significant military concessions to China in exchange for THAAD, including not deploying any more anti-ballistic missile systems in South Korea and not participating in an alliance between the United States and Japan.

South Korea and Russia are participants in the Six-party talks on the North Korea's nuclear proliferation issue. Moon Jae-in's administration has focused on increasing South Korea's consumption of natural gas. These plans include re-opening dialogue around a natural gas pipeline that would come from Russia and pass through North Korea. In June 2018, president Moon Jae-in became the first South Korean leader to speak in the Russian Parliament. On June 22, Moon Jae-in and Putin signed a document for foundation of free trade area.

Korea and Japan have had difficult relations since ancient times, but also significant cultural exchange, with Korea acting as the gateway between Asia and Japan. Contemporary perceptions of Japan are still largely defined by Japan's 35 year colonization of Korea in the 20th century, which is generally regarded in South Korea as having been very negative. Japan is today South Korea's third largest trading partner, with 12% ($46 billion) of exports in 2016.

There were no formal diplomatic ties between South Korea and Japan directly after independence the end of World War II in 1945. South Korea and Japan eventually signed the Treaty on Basic Relations between Japan and the Republic of Korea in 1965 to establish diplomatic ties. There is heavy anti-Japanese sentiment in South Korea because of a number of unsettled Japanese-Korean disputes, many of which stem from the period of Japanese occupation after the Japanese annexation of Korea. During World War II, more than 100,000 Koreans served in the Imperial Japanese Army. Korean women were coerced and forced to serve the Imperial Japanese Army as sexual slaves, called comfort women, in both Korea and throughout the Japanese war fronts.

Longstanding issues such as Japanese war crimes against Korean civilians, the negationist re-writing of Japanese textbooks relating Japanese atrocities during World War II, the territorial disputes over the Liancourt Rocks, known in South Korea as "Dokdo" and in Japan as "Takeshima", and visits by Japanese politicians to the Yasukuni Shrine, honoring Japanese people (civilians and military) killed during the war continue to trouble Korean-Japanese relations. The Liancourt Rocks were the first Korean territories to be forcibly colonized by Japan in 1905. Although it was again returned to Korea along with the rest of its territory in 1951 with the signing of the Treaty of San Francisco, Japan does not recant on its claims that the Liancourt Rocks are Japanese territory.
In response to then-Prime Minister Junichiro Koizumi's visits to the Yasukuni Shrine, former President Roh Moo-hyun suspended all summit talks between South Korea and Japan in 2009.
A summit between the nations' leaders was eventually held on February 9, 2018 during the Korean held Winter Olympics. South Korea asked the International Olympic Committee to ban the Japanese Rising Sun Flag from the 2020 Summer Olympics in Tokyo.

The European Union (EU) and South Korea are important trading partners, having negotiated a free trade agreement for many years since South Korea was designated as a priority FTA partner in 2006. The free trade agreement was approved in September 2010, and took effect on July 1, 2011. South Korea is the EU's tenth largest trade partner, and the EU has become South Korea's fourth largest export destination. EU trade with South Korea exceeded €90 billion in 2015 and has enjoyed an annual average growth rate of 9.8% between 2003 and 2013.

The EU has been the single largest foreign investor in South Korea since 1962, and accounted for almost 45% of all FDI inflows into Korea in 2006. Nevertheless, EU companies have significant problems accessing and operating in the South Korean market because of stringent standards and testing requirements for products and services often creating barriers to trade. Both in its regular bilateral contacts with South Korea and through its FTA with Korea, the EU is seeking to improve this situation.

The close relationship began directly after World War II, when the United States temporarily administrated Korea for three years (mainly in the South, with the Soviet Union engaged in North Korea) after Japan. Upon the onset of the Korean War in 1950, U.S. forces were sent to defend against an invasion from North Korea of the South, and subsequently fought as the largest contributor of UN troops. The United States participation was critical for preventing the near defeat of the Republic of Korea by northern forces, as well as fighting back for the territory gains that define the South Korean nation today.

Following the Armistice, South Korea and the U.S. agreed to a "Mutual Defense Treaty", under which an attack on either party in the Pacific area would summon a response from both. In 1967, South Korea obliged the mutual defense treaty, by sending a large combat troop contingent to support the United States in the Vietnam War. The US has over 23,000 troops stationed in South Korea, including the U.S. Eighth Army, Seventh Air Force, and U.S. Naval Forces Korea. The two nations have strong economic, diplomatic, and military ties, although they have at times disagreed with regard to policies towards North Korea, and with regard to some of South Korea's industrial activities that involve usage of rocket or nuclear technology. There had also been strong anti-American sentiment during certain periods, which has largely moderated in the modern day.

The two nations also share a close economic relationship, with the U.S being South Korea's second largest trading partner, receiving $66 billion in exports in 2016. In 2007, a free trade agreement known as the Republic of Korea-United States Free Trade Agreement (KORUS FTA) was signed between South Korea and the United States, but its formal implementation was repeatedly delayed, pending approval by the legislative bodies of the two countries. On October 12, 2011, the U.S. Congress passed the long-stalled trade agreement with South Korea. It went into effect on March 15, 2012.

Unresolved tension with North Korea has prompted South Korea to allocate 2.6% of its GDP and 15% of all government spending to its military (Government share of GDP: 14.967%), while maintaining compulsory conscription for men. Consequently, South Korea has the world's seventh largest number of active troops (599,000 in 2018), the world's highest number of reserve troops (3,100,000 in 2018) and the tenth largest defense budget. As of 2019 South Korea has a defense budget of $43.1 billion. The South Korean military is ranked as the 7th most powerful military force in the world as of 2019.

The South Korean military consists of the Army (ROKA), the Navy (ROKN), the Air Force (ROKAF), and the Marine Corps (ROKMC), and reserve forces. Many of these forces are concentrated near the Korean Demilitarized Zone. All South Korean males are constitutionally required to serve in the military, typically 21 months. Previous exceptions for South Korean citizens of mixed race no longer apply since 2011.

In addition to male conscription in South Korea's sovereign military, 1,800 Korean males are selected every year to serve 21 months in the KATUSA Program to further augment the United States Forces Korea (USFK). In 2010, South Korea was spending ₩1.68 trillion in a cost-sharing agreement with the US to provide budgetary support to the US forces in Korea, on top of the ₩29.6 trillion budget for its own military.

The South Korean army has 2,500 tanks in operation, including the K1A1 and K2 Black Panther, which form the backbone of the South Korean army's mechanized armor and infantry forces. A sizable arsenal of many artillery systems, including 1,700 self-propelled K55 and K9 Thunder howitzers and 680 helicopters and UAVs of numerous types, are assembled to provide additional fire, reconnaissance, and logistics support. South Korea's smaller but more advanced artillery force and wide range of airborne reconnaissance platforms are pivotal in the counter-battery suppression of North Korea's large artillery force, which operates more than 13,000 artillery systems deployed in various state of fortification and mobility.

The South Korean navy has made its first major transformation into a blue-water navy through the formation of the Strategic Mobile Fleet, which includes a battle group of Chungmugong Yi Sun-sin class destroyers, Dokdo class amphibious assault ship, AIP-driven Type 214 submarines, and King Sejong the Great class destroyers, which is equipped with the latest baseline of Aegis fleet-defense system that allows the ships to track and destroy multiple cruise missiles and ballistic missiles simultaneously, forming an integral part of South Korea's indigenous missile defense umbrella against the North Korean military's missile threat.

The South Korean air force operates 840 aircraft, making it world's ninth largest air force, including several types of advanced fighters like F-15K, heavily modified KF-16C/D, and the indigenous T-50 Golden Eagle, supported by well-maintained fleets of older fighters such as F-4E and KF-5E/F that still effectively serve the air force alongside the more modern aircraft. In an attempt to gain strength in terms of not just numbers but also modernity, the commissioning of four Boeing 737 AEW&C aircraft, under Project Peace Eye for centralized intelligence gathering and analysis on a modern battlefield, will enhance the fighters' and other support aircraft's ability to perform their missions with awareness and precision.

In May 2011, Korea Aerospace Industries Ltd., South Korea's largest plane maker, signed a $400 million deal to sell 16 T-50 Golden Eagle trainer jets to Indonesia, making South Korea the first country in Asia to export supersonic jets.

From time to time, South Korea has sent its troops overseas to assist American forces. It has participated in most major conflicts that the United States has been involved in the past 50 years. South Korea dispatched 325,517 troops to fight alongside American, Australian, Filipino, New Zealand and South Vietnamese soldiers in the Vietnam War, with a peak strength of 50,000. In 2004, South Korea sent 3,300 troops of the Zaytun Division to help re-building in northern Iraq, and was the third largest contributor in the coalition forces after only the US and Britain. Beginning in 2001, South Korea had so far deployed 24,000 troops in the Middle East region to support the War on Terrorism. A further 1,800 were deployed since 2007 to reinforce UN peacekeeping forces in Lebanon.

The United States has stationed a substantial contingent of troops to defend South Korea. There are approximately 28,500 U.S. military personnel stationed in South Korea, most of them serving one year unaccompanied tours. The U.S. troops, which are primarily ground and air units, are assigned to USFK and mainly assigned to the Eighth United States Army of the U.S. Army and Seventh Air Force of the U.S. Air Force. They are stationed in installations at Osan, Kunsan, Yongsan, Dongducheon, Sungbuk, Camp Humphreys, and Daegu, as well as at Camp Bonifas in the DMZ Joint Security Area.

A fully functioning UN Command is at the top of the chain of command of all forces in South Korea, including the U.S. forces and the entire South Korean military – if a sudden escalation of war between North and South Korea were to occur the United States would assume control of the South Korean armed forces in all military and paramilitary moves. There has been long term agreement between the United States and South Korea that South Korea should eventually assume the lead for its own defense. This transition to a South Korean command has been slow and often postponed, although it is currently scheduled to occur in the early 2020s.

Male citizens who refuse or reject to undertake military services because of conscientious objection are typically imprisoned, with over 600 individuals usually imprisoned at any given time; more than the rest of the world put together. The vast majority of these are young men from the Jehovah's Witnesses Christian denomination.
See Conscription in South Korea. However, in a recent court ruling, conscientious objectors were permitted to reject military service.

South Korea's mixed economy ranks 11th nominal and 13th purchasing power parity GDP in the world, identifying it as one of the G-20 major economies. It is a developed country with a high-income economy and is the most industrialized member country of the OECD. South Korean brands such as LG Electronics and Samsung are internationally famous and garnered South Korea's reputation for its quality electronics and other manufactured goods.

Its massive investment in education has taken the country from mass illiteracy to a major international technological powerhouse. The country's national economy benefits from a highly skilled workforce and is among the most educated countries in the world with one of the highest percentages of its citizens holding a tertiary education degree. South Korea's economy was one of the world's fastest-growing from the early 1960s to the late 1990s, and was still one of the fastest-growing developed countries in the 2000s, along with Hong Kong, Singapore and Taiwan, the other three Asian Tigers. It recorded the fastest rise in average GDP per capita in the world between 1980 and 1990. South Koreans refer to this growth as the Miracle on the Han River. The South Korean economy is heavily dependent on international trade, and in 2014, South Korea was the fifth-largest exporter and seventh-largest importer in the world.

Despite the South Korean economy's high growth potential and apparent structural stability, the country suffers damage to its credit rating in the stock market because of the belligerence of North Korea in times of deep military crises, which has an adverse effect on South Korean financial markets. The International Monetary Fund compliments the resilience of the South Korean economy against various economic crises, citing low state debt and high fiscal reserves that can quickly be mobilized to address financial emergencies. Although it was severely harmed by the Asian economic crisis of the late 1990s, the South Korean economy managed a rapid recovery and subsequently tripled its GDP.

Furthermore, South Korea was one of the few developed countries that were able to avoid a recession during the global financial crisis. Its economic growth rate reached 6.2 percent in 2010 (the fastest growth for eight years after significant growth by 7.2 percent in 2002), a sharp recovery from economic growth rates of 2.3% in 2008 and 0.2% in 2009, when the global financial crisis hit. The unemployment rate in South Korea also remained low in 2009, at 3.6%.

South Korea became a member of the Organisation for Economic Co-operation and Development (OECD) in 1996.

The following list includes the largest South Korean companies by revenue in 2017 who are all listed as part of the Fortune Global 500:

South Korea has a technologically advanced transport network consisting of high-speed railways, highways, bus routes, ferry services, and air routes that crisscross the country. Korea Expressway Corporation operates the toll highways and service amenities en route.

Korail provides frequent train services to all major South Korean cities. Two rail lines, Gyeongui and Donghae Bukbu Line, to North Korea are now being reconnected. The Korean high-speed rail system, KTX, provides high-speed service along Gyeongbu and Honam Line. Major cities including Seoul, Busan, Incheon, Daegu, Daejeon and Gwangju have urban rapid transit systems. Express bus terminals are available in most cities.

South Korea's main gateway and largest airport is Incheon International Airport, serving passengers in 2016. Other international airports include Gimpo, Busan and Jeju. There are also many airports that were built as part of the infrastructure boom but are barely used. There are also many heliports.

The national carrier, Korean Air served over 26,800,000 passengers, including almost 19,000,000 international passengers in 2016. A second carrier, Asiana Airlines also serves domestic and international traffic. Combined, South Korean airlines serve 297 international routes. Smaller airlines, such as Jeju Air, provide domestic service with lower fares.

South Korea is the world's fifth-largest nuclear power producer and the second-largest in Asia . Nuclear power in South Korea supplies 45% of electricity production, and research is very active with investigation into a variety of advanced reactors, including a small modular reactor, a liquid-metal fast/transmutation reactor and a high-temperature hydrogen generation design. Fuel production and waste handling technologies have also been developed locally. It is also a member of the ITER project.

South Korea is an emerging exporter of nuclear reactors, having concluded agreements with the UAE to build and maintain four advanced nuclear reactors, with Jordan for a research nuclear reactor, and with Argentina for construction and repair of heavy-water nuclear reactors. , South Korea and Turkey are in negotiations regarding construction of two nuclear reactors. South Korea is also preparing to bid on construction of a light-water nuclear reactor for Argentina.

South Korea is not allowed to enrich uranium or develop traditional uranium enrichment technology on its own, because of US political pressure, unlike most major nuclear powers such as Japan, Germany, and France, competitors of South Korea in the international nuclear market. This impediment to South Korea's indigenous nuclear industrial undertaking has sparked occasional diplomatic rows between the two allies. While South Korea is successful in exporting its electricity-generating nuclear technology and nuclear reactors, it cannot capitalize on the market for nuclear enrichment facilities and refineries, preventing it from further expanding its export niche. South Korea has sought unique technologies such as pyroprocessing to circumvent these obstacles and seek a more advantageous competition. The US has recently been wary of South Korea's burgeoning nuclear program, which South Korea insists will be for civilian use only.

South Korea is the third highest ranked Asian country in the World Economic Forum's Network Readiness Index (NRI) after Singapore and Hong Kong respectively – an indicator for determining the development level of a country's information and communication technologies. South Korea ranked number 10 overall in the 2014 NRI ranking, up from 11 in 2013.

In 2016, 17 million foreign tourists visited South Korea With rising tourist prospects, especially from foreign countries outside of Asia, the South Korean government has set a target of attracting 20 million foreign tourists a year by 2017.

South Korean tourism is driven by many factors, including the prominence of Korean pop culture such as South Korean pop music and television dramas, known as the Korean Wave or (Hallyu), has gained popularity throughout East Asia. The Hyundai Research Institute reported that the Korean Wave has a direct impact in encouraging direct foreign investment back into the country through demand for products, and the tourism industry. Among East Asian countries, China was the most receptive, investing 1.4 billion in South Korea, with much of the investment within its service sector, a sevenfold increase from 2001. According to an analysis by economist Han Sang-Wan, a 1 percent increase in the exports of Korean cultural content pushes consumer goods exports up 0.083 percent while a 1 percent increase in Korean pop content exports to a country produces a 0.019 percent bump in tourism.

The South Korean pension system was created to provide benefits to persons reaching old age, families and persons stricken with death of their primary breadwinner, and for the purposes of stabilizing its nations welfare state. South Korea's pension system structure is primarily based on taxation and is income-related. In 2007 there was a total of 18,367,000 insured individuals with only around 511,000 persons excluded from mandatory contribution. The current pension system is divided into four categories distributing benefits to participants through national, military personnel, governmental, and private school teacher pension schemes. The national pension scheme is the primary welfare system providing allowances to the majority of persons. Eligibility for the national pension scheme is not dependent on income but on age and residence, where those between the ages of 18 to 59 are covered. Any one who is under the age of 18 are dependents of someone who is covered or under a special exclusion where they are allowed to alternative provisions. The national pension scheme is divided into four categories of insured persons – the workplace-based insured, the individually insured, the voluntarily insured, and the voluntarily and continuously insured.

Employees between the ages of 18 to 59 are covered under the workplace-based pension scheme and contribute 4.5% of their gross monthly earnings. The national pension covers employees who work in firms that employ five or more employees, fishermen, farmers, and the self-employed in both rural and urban areas. Employers are also covered under the workplace-based pension scheme and help cover their employees obligated 9% contribution by providing the remaining 4.5%. Anyone who is not employed, of the age of 60 or above, and excluded by article 6 of the National Pension Act but of the ages between 18 and 59, is covered under the individually insured pension scheme. Persons covered by the individually insured pension scheme are in charge of paying the entire 9% contribution themselves. Voluntarily insured persons are not subjected to mandatory coverage but can choose to be. This category comprises retirees who voluntarily choose to have additional benefits, individuals under the age of 27 without income, and individuals whose spouses are covered under a public welfare system, whether military, governmental, or private school teacher pensions. Like the Individually insured persons, they too are in charge of covering the full amount of the contribution. Voluntarily and continuously insured persons consists of individuals 60 years of age who want to fulfill the minimum insured period of 20 years to qualify for old age pension benefits. Excluding the workplace-based insured persons, all the other insured persons personally cover their own 9% contribution.

South Korea's old-age pension scheme covers individuals age 60 or older for the rest of their life as long as they have satisfied the minimum of 20 years of national pension coverage beforehand. Individuals with a minimum of 10 years covered under the national pension scheme and who are 60 years of age are able to be covered by under a 'reduced old-age pension' scheme. There also is an 'active old-age pension' scheme that covers individuals age 60 to 65 engaged in activities yielding earned income. Individuals age of 55 and younger than 60 who are not engaged in activities yielding earned income are eligible to be covered under the 'early old-age pension' scheme. Around 60% of all Korean elders, age 65 and over are entitled to a 5% benefit of their past average income at an average of 90,000 Korean Won (KRW). Basic old-age pension schemes covered individuals 65 years of age who earned below an amount set by presidential order. In 2010, that ceiling was 700,00 KRW for a single individual and 1,120,000 for a couple, equivalent to around $600.00 and $960.00.

Scientific and technological development in the South Korea at first did not occur largely because of more pressing matters such as the division of Korea and the Korean War that occurred right after its independence. It was not until the 1960s under the dictatorship of Park Chung-hee where South Korea's economy rapidly grew from industrialisation and the Chaebol corporations such as Samsung and LG. Ever since the industrialization of South Korea's economy, South Korea has placed its focus on technology-based corporations, which has been supported by infrastructure developments by the government. South Korean corporations Samsung and LG were ranked first and third largest mobile phone companies in the world in the first quarter of 2012, respectively. An estimated 90% of South Koreans own a mobile phone. Aside from placing/receiving calls and text messaging, mobile phones in the country are widely used for watching Digital Multimedia Broadcasting (DMB) or viewing websites. Over one million DMB phones have been sold and the three major wireless communications providers SK Telecom, KT, and LG U+ provide coverage in all major cities and other areas. South Korea has the fastest Internet download speeds in the world, with an average download speed of 25.3 Mbit/s.

South Korea leads the OECD in graduates in science and engineering. Since 2014, the country ranked first among the most innovative countries in the Bloomberg Innovation Index for 6 consecutive years. Additionally, South Korea today is known as a Launchpad of a mature mobile market, where developers can reap benefits of a market where very few technology constraints exist. There is a growing trend of inventions of new types of media or apps, utilizing the 4G and 5G internet infrastructure in South Korea. South Korea has today the infrastructures to meet a density of population and culture that has the capability to create strong local particularity.

Following cyberattacks in the first half of 2013, whereby government, news-media, television station, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed North Korea for these attacks, as well as incidents that occurred in 2009, 2011 and 2012, but Pyongyang denies the accusations.

In late September 2013, a computer-security competition jointly sponsored by the defense ministry and the National Intelligence Service was announced. The winners were announced on September 29, 2013 and shared a total prize pool of 80 million won (US$74,000).

South Korea's government maintains a broad-ranging approach toward the regulation of specific online content and imposes a substantial level of censorship on election-related discourse and on many websites that the government deems subversive or socially harmful.

South Korea has sent up 10 satellites since 1992, all using foreign rockets and overseas launch pads, notably Arirang-1 in 1999, and Arirang-2 in 2006 as part of its space partnership with Russia. Arirang-1 was lost in space in 2008, after nine years in service.

In April 2008, Yi So-yeon became the first Korean to fly in space, aboard the Russian Soyuz TMA-12.

In June 2009, the first spaceport of South Korea, Naro Space Center, was completed at Goheung, Jeollanam-do. The launch of Naro-1 in August 2009 resulted in a failure. The second attempt in June 2010 was also unsuccessful. However, the third launch of the Naro 1 in January 2013 was successful. The government plans to develop Naro-2 by the year 2018.

South Korea's efforts to build an indigenous space launch vehicle have been marred due to persistent political pressure from the United States, who had for many decades hindered South Korea's indigenous rocket and missile development programs in fear of their possible connection to clandestine military ballistic missile programs, which Korea many times insisted did not violate the research and development guidelines stipulated by US-Korea agreements on restriction of South Korean rocket technology research and development. South Korea has sought the assistance of foreign countries such as Russia through MTCR commitments to supplement its restricted domestic rocket technology. The two failed KSLV-I launch vehicles were based on the Universal Rocket Module, the first stage of the Russian Angara rocket, combined with a solid-fueled second stage built by South Korea.

Robotics has been included in the list of main national R&D projects in Korea since 2003. In 2009, the government announced plans to build robot-themed parks in Incheon and Masan with a mix of public and private funding.

In 2005, Korea Advanced Institute of Science and Technology (KAIST) developed the world's second walking humanoid robot, HUBO. A team in the Korea Institute of Industrial Technology developed the first Korean android, EveR-1 in May 2006.
EveR-1 has been succeeded by more complex models with improved movement and vision.

Plans of creating English-teaching robot assistants to compensate for the shortage of teachers were announced in February 2010, with the robots being deployed to most preschools and kindergartens by 2013. Robotics are also incorporated in the entertainment sector as well; the "Korean Robot Game Festival" has been held every year since 2004 to promote science and robot technology.

Since the 1980s, the Korean government has invested in the development of a domestic biotechnology industry, and the sector is projected to grow to by 2010. The medical sector accounts for a large part of the production, including production of hepatitis vaccines and antibiotics.

Recently, research and development in genetics and cloning has received increasing attention, with the first successful cloning of a dog, Snuppy (in 2005), and the cloning of two females of an endangered species of wolves by the Seoul National University in 2007.

The rapid growth of the industry has resulted in significant voids in regulation of ethics, as was highlighted by the scientific misconduct case involving Hwang Woo-Suk.

South Korea shares its traditional culture with North Korea, but the two Koreas have developed distinct contemporary forms of culture since the peninsula was divided in 1945. Historically, while the culture of Korea has been heavily influenced by that of neighboring China, it has nevertheless managed to develop a unique cultural identity that is distinct from its larger neighbor. Its rich and vibrant culture left 19 UNESCO Intangible Cultural Heritages of Humanity, the third largest in the world, along with 12 World Heritage Sites. The South Korean Ministry of Culture, Sports and Tourism actively encourages the traditional arts, as well as modern forms, through funding and education programs.

The industrialization and urbanization of South Korea have brought many changes to the way modern Koreans live. Changing economics and lifestyles have led to a concentration of population in major cities, especially the capital Seoul, with multi-generational households separating into nuclear family living arrangements. A 2014 Euromonitor study found that South Koreans drink the most alcohol on a weekly basis compared to the rest of the world. South Koreans drink 13.7 shots of liquor per week on average and, of the 44 other countries analyzed, Russia, the Philippines, and Thailand follow.

Korean art has been highly influenced by Buddhism and Confucianism, which can be seen in the many traditional paintings, sculptures, ceramics and the performing arts. Korean pottery and porcelain, such as Joseon's "baekja" and buncheong, and Goryeo's celadon are well known throughout the world. The Korean tea ceremony, pansori, talchum and buchaechum are also notable Korean performing arts.

Post-war modern Korean art started to flourish in the 1960s and 1970s, when South Korean artists took interest in geometrical shapes and intangible subjects. Establishing a harmony between man and nature was also a favorite of this time. Because of social instability, social issues appeared as main subjects in the 1980s. Art was influenced by various international events and exhibits in Korea, and with it brought more diversity. The Olympic Sculpture Garden in 1988, the transposition of the 1993 edition of the Whitney Biennial to Seoul, the creation of the Gwangju Biennale and the Korean Pavilion at the Venice Biennale in 1995 were notable events.

Because of South Korea's tumultuous history, construction and destruction has been repeated endlessly, resulting in an interesting melange of architectural styles and designs.

Korean traditional architecture is characterized by its harmony with nature. Ancient architects adopted the bracket system characterized by thatched roofs and heated floors called "ondol". People of the upper classes built bigger houses with elegantly curved tiled roofs with lifting eaves. Traditional architecture can be seen in the palaces and temples, preserved old houses called "hanok", and special sites like Hahoe Folk Village, Yangdong Village of Gyeongju and Korean Folk Village. Traditional architecture may also be seen at the nine UNESCO World Heritage Sites in South Korea.

Western architecture was first introduced to Korea at the end of the 19th century. Churches, offices for foreign legislation, schools and university buildings were built in new styles. With the annexation of Korea by Japan in 1910 the colonial regime intervened in Korea's architectural heritage, and Japanese-style modern architecture was imposed. The anti-Japanese sentiment, and the Korean War, led to the destruction of most buildings constructed during that time.

Korean architecture entered a new phase of development during the post-Korean War reconstruction, incorporating modern architectural trends and styles. Stimulated by the economic growth in the 1970s and 1980s, active redevelopment saw new horizons in architectural design. In the aftermath of the 1988 Seoul Olympics, South Korea has witnessed a wide variation of styles in its architectural landscape due, in large part, to the opening up of the market to foreign architects. Contemporary architectural efforts have been constantly trying to balance the traditional philosophy of "harmony with nature" and the fast-paced urbanization that the country has been going through in recent years.

Korean cuisine, "hanguk yori" (한국요리; 韓國料理), or "hansik" (한식; 韓食), has evolved through centuries of social and political change. Ingredients and dishes vary by province. There are many significant regional dishes that have proliferated in different variations across the country in the present day. The Korean royal court cuisine once brought all of the unique regional specialties together for the royal family. Meals consumed both by the royal family and ordinary Korean citizens have been regulated by a unique culture of etiquette.

Korean cuisine is largely based on rice, noodles, tofu, vegetables, fish and meats. Traditional Korean meals are noted for the number of side dishes, "banchan" (반찬), which accompany steam-cooked short-grain rice. Every meal is accompanied by numerous banchan. Kimchi (김치), a fermented, usually spicy vegetable dish is commonly served at every meal and is one of the best known Korean dishes. Korean cuisine usually involves heavy seasoning with sesame oil, "doenjang" (된장), a type of fermented soybean paste, soy sauce, salt, garlic, ginger, and "gochujang" (고추장), a hot pepper paste. Other well-known dishes are "Bulgogi" (불고기), grilled marinated beef, "Gimbap" (김밥), and "Tteokbokki" (떡볶이), a spicy snack consisting of rice cake seasoned with gochujang or a spicy chili paste.

Soups are also a common part of a Korean meal and are served as part of the main course rather than at the beginning or the end of the meal. Soups known as "guk" (국) are often made with meats, shellfish and vegetables. Similar to guk, "tang" (탕; 湯) has less water, and is more often served in restaurants. Another type is "jjigae" (찌개), a stew that is typically heavily seasoned with chili pepper and served boiling hot.

Popular Korean alcoholic beverages include Soju, Makgeolli and Bokbunja ju.

Korea is unique among East Asian countries in its use of metal chopsticks. Metal chopsticks have been discovered in Goguryeo archaeological sites.

In addition to domestic consumption, South Korea has a thriving entertainment industry where various facets of South Korean entertainment including television dramas, films, and popular music has generated significant financial revenues for the nation's economy. The cultural phenomenon known as "Hallyu" or the "Korean Wave", has swept many countries across Asia making South Korea a major soft power as an exporter of popular culture and entertainment, rivaling Western nations such as the United States and the United Kingdom.

Until the 1990s, trot and traditional Korean folk based ballads dominated South Korean popular music. The emergence of the South Korean pop group Seo Taiji and Boys in 1992 marked a turning point for South Korean popular music, also known as K-pop, as the genre modernized itself from incorporating elements of popular musical genres from across the world such as Western popular music, experimental, jazz, gospel, Latin, classical, hip hop, rhythm and blues, electronic dance, reggae, country, folk, and rock on top of its uniquely traditional Korean music roots. Western-style pop, hip hop, rhythm and blues, rock, folk, electronic dance oriented acts have become dominant in the modern South Korean popular music scene, though trot is still enjoyed among older South Koreans. K-pop stars and groups are well known across Asia and have found international fame making millions of dollars in export revenue. Many K-pop acts have also been able secure a strong overseas following using online social media platforms such as the video sharing website YouTube. South Korean singer PSY became an international sensation when his song "Gangnam Style" topped global music charts in 2012. 

Since the success of the film "Shiri" in 1999, the Korean film industry has begun to gain recognition internationally. Domestic film has a dominant share of the market, partly because of the existence of screen quotas requiring cinemas to show Korean films at least 73 days a year.

South Korean television shows have become popular outside of Korea. South Korean television dramas, known as K-dramas have begun to find fame internationally. Many dramas tend to have a romantic focus, such as "Princess Hours", "You're Beautiful", "Playful Kiss", "My Name is Kim Sam Soon", "Boys Over Flowers", "Winter Sonata", "Autumn in My Heart", "Full House", "City Hunter", "All About Eve", "Secret Garden", "I Can Hear Your Voice", "Master's Sun", "My Love from the Star", "Healer", "Descendants of the Sun" and "". Historical dramas have included "Faith", "Dae Jang Geum", "The Legend", "Dong Yi", "Moon Embracing the Sun", "Sungkyunkwan Scandal", and "Iljimae".

There are many official public holidays in South Korea. Korean New Year's Day, or "Seollal", is celebrated on the first day of the Korean lunar calendar. Korean Independence Day falls on March 1, and commemorates the March 1 Movement of 1919. Memorial Day is celebrated on June 6, and its purpose is to honor the men and women who died in South Korea's independence movement. Constitution Day is on July 17, and it celebrates the promulgation of Constitution of the Republic of Korea. Liberation Day, on August 15, celebrates Korea's liberation from the Empire of Japan in 1945. Every 15th day of the 8th lunar month, Koreans celebrate the Midautumn Festival, in which Koreans visit their ancestral hometowns and eat a variety of traditional Korean foods. On October 1, Armed Forces day is celebrated, honoring the military forces of South Korea. October 3 is National Foundation Day. Hangul Day, on October 9 commemorates the invention of hangul, the native alphabet of the Korean language.

The martial art taekwondo originated in Korea. In the 1950s and 1960s, modern rules were standardized, with taekwondo becoming an official Olympic sport in 2000. Other Korean martial arts include Taekkyon, hapkido, Tang Soo Do, Kuk Sool Won, kumdo and subak.

Football and baseball have traditionally been regarded as the most popular sports in Korea. Recent polling indicates that a majority, 41% of South Korean sports fans continue to self-identify as football fans, with baseball ranked 2nd at 25% of respondents. However, the polling did not indicate the extent to which respondents follow both sports. The national football team became the first team in the Asian Football Confederation to reach the FIFA World Cup semi-finals in the 2002 FIFA World Cup, jointly hosted by South Korea and Japan. The Korea Republic national team (as it is known) has qualified for every World Cup since Mexico 1986, and has broken out of the group stage twice: first in 2002, and again in 2010, when it was defeated by eventual semi-finalist Uruguay in the Round of 16. At the 2012 Summer Olympics, South Korea won the Bronze Medal for football.

Baseball was first introduced to Korea in 1905 and has since become increasingly popular, with some sources claiming it has surpassed football as the most popular sport in the country. Recent years have been characterized by increasing attendance and ticket prices for professional baseball games. The Korea Professional Baseball league, a 10-team circuit, was established in 1982. The South Korea national team finished third in the 2006 World Baseball Classic and second in the 2009 tournament. The team's 2009 final game against Japan was widely watched in Korea, with a large screen at Gwanghwamun crossing in Seoul broadcasting the game live. In the 2008 Summer Olympics, South Korea won the gold medal in baseball. Also in 1982, at the Baseball Worldcup, Korea won the gold medal. At the 2010 Asian Games, the Korean National Baseball team won the gold medal. Several Korean players have gone on to play in Major League Baseball.

Basketball is a popular sport in the country as well. South Korea has traditionally had one of the top basketball teams in Asia and one of the continent's strongest basketball divisions. Seoul hosted the 1967 and 1995 Asian Basketball Championship. The Korea national basketball team has won a record number of 23 medals at the event to date.
South Korea hosted the Asian Games in 1986 (Seoul), 2002 (Busan) and 2014 (Incheon). It also hosted the Winter Universiade in 1997, the Asian Winter Games in 1999 and the Summer Universiade in 2003, 2015. In 1988, South Korea hosted the Summer Olympics in Seoul, coming fourth with 12 gold medals, 10 silver medals and 11 bronze medals. South Korea regularly performs well in archery, shooting, table tennis, badminton, short track speed skating, handball, hockey, freestyle wrestling, Greco-Roman wrestling, baseball, judo, taekwondo, speed skating, figure Skating, and weightlifting. The Seoul Olympic Museum is a museum in Seoul, South Korea, dedicated to the 1988 Summer Olympics. On July 6, 2011 Pyeongchang was chosen by the IOC to host the 2018 Winter Olympics.

South Korea has won more medals in the Winter Olympics than any other Asian country with a total of 45 medals (23 gold, 14 silver, and 8 bronze). At the 2010 Winter Olympics, South Korea ranked fifth in the overall medal rankings. South Korea is especially strong in short track speed skating. However, speed skating and figure skating are very popular, too, and ice hockey is an emerging sport with Anyang Halla winning their first ever Asia League Ice Hockey title in March 2010.

Seoul hosted a professional triathlon race, which is part of the International Triathlon Union (ITU) World Championship Series in May 2010. In 2011, the South Korean city of Daegu hosted the 2011 IAAF World Championships in Athletics.
In October 2010, South Korea hosted its first Formula One race at the Korea International Circuit in Yeongam, about south of Seoul. The Korean Grand Prix was held from 2010 to 2013, but was not placed on the 2014 F1 calendar.

Domestic horse racing events are also followed by South Koreans and Seoul Race Park in Gwacheon, Gyeonggi-do is located closest to Seoul out of the country's three tracks.

Competitive video gaming, also called eSports (sometimes written e-Sports), has become more popular in South Korea in recent years, particularly among young people. The two most popular games are League of Legends and StarCraft. The gaming scene of South Korea is managed by the Korean e-Sports Association (KeSPA for short) and has become something of a career for many players. They can make a living out of their activity and top players can even make a significant amount of money with some high end Starcraft II players ending up making six figure salaries.





</doc>
<doc id="27020" url="https://en.wikipedia.org/wiki?curid=27020" title="History of South Korea">
History of South Korea

The history of South Korea formally begins with its establishment on 15 August 1948.

Korea was administratively partitioned in 1945, at the end of World War II. As Korea was under Japanese rule during World War II, Korea was officially a belligerent against the Allies by virtue of being Japanese territory. The unconditional surrender of Japan led to the division of Korea into two occupation zones (similar to the four zones in Germany), with the United States administering the southern half of the peninsula and the Soviet Union administering the area north of the 38th parallel. This division was meant to be temporary (as was in Germany) and was first intended to return a unified Korea back to its people after the United States, United Kingdom, Soviet Union, and Republic of China could arrange a single government for the peninsula.

The two parties were unable to agree on the implementation of Joint Trusteeship over Korea. This led in 1948 to the establishment of two separate governments – the Communist-aligned Democratic People's Republic of Korea (DPRK) and the West-aligned First Republic of Korea – each claiming to be the legitimate government of all of Korea. On June 25, 1950 the Korean War broke out. After much destruction, the war ended on July 27, 1953 with the 1948 status quo being restored, as neither the DPRK nor the First Republic had succeeded in conquering the other's portion of the divided Korea. The peninsula was divided by the Korean Demilitarized Zone and the two separate governments stabilised into the existing political entities of North and South Korea.

South Korea's subsequent history is marked by alternating periods of democratic and autocratic rule. Civilian governments are conventionally numbered from the First Republic of Syngman Rhee to the contemporary Sixth Republic. The First Republic, arguably democratic at its inception, became increasingly autocratic until its collapse in 1960. The Second Republic was strongly democratic, but was overthrown in less than a year and replaced by an autocratic military regime. The Third, Fourth, and Fifth Republics were nominally democratic, but are widely regarded as the continuation of military rule. With the Sixth Republic, the country has gradually stabilized into a liberal democracy.

Since its inception, South Korea has seen substantial development in education, economy, and culture. Since the 1960s, the country has developed from one of Asia's poorest to one of the world's wealthiest nations. Education, particularly at the tertiary level, has expanded dramatically. It is said to be one of the "Four Tigers" of rising Asian states along with Singapore, Taiwan and Hong Kong.

Emperor Hirohito announced the surrender of the Empire of Japan to the Allied Powers on 15 August 1945. General Order No. 1 for the surrender of Japan (prepared by the Joint Chiefs of Staff of U.S. military forces and approved on 17 August 1945) prescribed separate surrender procedures for Japanese forces in Korea north and south of the 38th parallel. After Japan's surrender to the Allies (formalised on 2 September 1945), division at the 38th parallel marked the beginning of Soviet and U.S. occupation the North and South, respectively. This division was meant to be temporary, to be replaced by a trusteeship of the United States, United Kingdom, Soviet Union, and Republic of China which would prepare for Korean independence. The trusteeship had been discussed at the Yalta Conference in February 1945. U.S. forces landed at Incheon on September 8, 1945 and established a military government shortly thereafter. Lieutenant General John R. Hodge, their commander, took charge of the government. Faced with mounting popular discontent, in October 1945 Hodge established the Korean Advisory Council. The Provisional Government of the Republic of Korea, which had operated from China, sent a delegation with three interpreters to Hodge, but he refused to meet with them. Likewise, Hodge refused to recognize the newly formed People's Republic of Korea and its People's Committees, and outlawed it on 12 December. A year later, an interim legislature and interim government were established, headed by Kim Kyu-shik and Syngman Rhee respectively. Political and economic chaos - arising from a variety of causes - plagued the country in this period. The after-effects of the Japanese exploitation remained in the South, as in the North. In addition, the U.S. military was largely unprepared for the challenge of administering the country, arriving with no knowledge of the language, culture or political situation. Thus many of their policies had unintended destabilizing effects. Waves of refugees from North Korea and returnees from abroad added to the turmoil.

In December 1945 a conference convened in Moscow to discuss the future of Korea.
A 5-year trusteeship was discussed, and a was established. The commission met intermittently in Seoul but deadlocked over the issue of establishing a national government. In September 1947, with no solution in sight, the United States submitted the Korean question to the UN General Assembly.

The resolution from the UN General Assembly called for a UN-supervised general election in Korea, but after the North rejected this proposition, a general election for a Constitutional Assembly took place in the South only, in May 1948. A constitution was adopted, setting forth a presidential form of government and specifying a four-year term for the presidency. According to the provisions of the Constitution, an indirect presidential election took place in July. Rhee Syngman, as head of the new assembly, assumed the presidency and proclaimed the Republic of Korea (South Korea) on August 15, 1948.

On 15 August 1948, the Republic of Korea was formally established, with Syngman Rhee as the first president. With the establishment of Rhee's government, de jure sovereignty also passed into the new government. On September 9, 1948, a communist regime, the Democratic People's Republic of Korea (North Korea), was proclaimed under Kim Il-sung. However, on December 12, 1948, by its resolution 195 in the Third General Assembly, the United Nations recognized the Republic of Korea as the sole legal government of Korea.

In 1946, the North implemented land reforms by confiscating private property, Japanese and pro-Japanese owned facilities and factories, and placed them under state ownership. Demand for land reform in the South grew strong, and it was eventually enacted in June 1949. Koreans with large landholdings were obliged to divest most of their land. Approximately 40 percent of total farm households became small landowners. However, because preemptive rights were given to people who had ties with landowners before liberation, many pro-Japanese groups obtained or retained properties.

The country now divided, the relationship between the two Koreas turned more antagonistic as time passed. The Soviet forces having withdrawn in 1948, North Korea pressured the South to expel the United States forces, but Rhee sought to align his government strongly with America, and against both North Korea and Japan. Although talks towards normalization of relations with Japan took place, they achieved little. Meanwhile, the government took in vast sums of American aid, in amounts sometimes near the total size of the national budget. The nationalist government also continued many of the practices of the U.S. military government. In 1948, the Rhee government repressed military uprisings in Jeju, Suncheon and Yeosu. During the rebellion and its suppression 14,000 to 60,000 people were killed in all fighting. Of note, President Rhee's regime was intolerant of opposition. A famous event that highlighted this was the arrest and conviction of future President Park Chung-hee, for communist conspiracy in 1948.

The main policy of the First Republic of South Korea was anti-communism and "unification by expanding northward". The South's military was neither sufficiently equipped nor prepared, but the Rhee administration was determined to reunify Korea by military force with aid from the United States. However, in the second parliamentary elections held on May 30, 1950, the majority of seats went to independents who did not endorse this position, confirming the lack of support and the fragile state of the nation.

When the communist army attacked from the North in June, retreating South Korean forces executed tens of thousands suspected communists or sympathisers, either in prison or a in a reeducation movement, in what is known as the Bodo League massacre.

On 25 June 1950, North Korean forces invaded South Korea. Led by the U.S., a 16-member coalition undertook the first collective action under the United Nations Command (UNC) in defense of South Korea. Oscillating battle lines inflicted a high number of civilian casualties and wrought immense destruction. With the People's Republic of China's entry on behalf of North Korea in late 1950, the fighting came to a stalemate close to the original line of demarcation. Armistice negotiations, initiated in July 1951, finally concluded on 27 July 1953 at Panmunjeom, now in the Demilitarized Zone (DMZ). Following the armistice, the South Korean government returned to Seoul on the symbolic date of 15 August 1953.

After the armistice, South Korea experienced political turmoil under years of autocratic leadership of Syngman Rhee, which was ended by student revolt in 1960. Throughout his rule, Rhee sought to take additional steps to cement his control of government. These began in 1952, when the government was still based in Busan due to the ongoing war. In May of that year, Rhee pushed through constitutional amendments which made the presidency a directly-elected position. To do this, he declared martial law, arrested opposing members of parliament, demonstrators, and anti-government groups. Rhee was subsequently elected by a wide margin.

Rhee regained control of parliament in the 1954 election, and thereupon pushed through an amendment to exempt himself from the eight-year term limit, and was once again re-elected in 1956. Soon after, Rhee's administration arrested members of the opposing party and executed the leader after accusing him of being a North Korean spy.

The administration became increasingly repressive while dominating the political arena, and in 1958, it sought to amend the National Security Law to tighten government control over all levels of administration, including the local units. These measures caused much outrage among the people, but despite public outcry, Rhee's administration rigged the March 1960 presidential election and won by a landslide.

On that election day, protests by students and citizens against the irregularities of the election burst out in the city of Masan. Initially these protests were quelled with force by local police, but when the body of a student was found floating in the harbor of Masan, the whole country was enraged and protests spread nationwide. On 19 April, students from various universities and schools rallied and marched in protest in the Seoul streets, in what would be called the April Revolution. The government declared martial law, called in the army, and suppressed the crowds with open fire. Subsequent protests throughout the country shook the government, and after an escalated protest with university professors taking to the streets on April 25, Rhee submitted his official resignation on April 26 and fled into exile.

After the student revolution, power was briefly held by an interim administration under the Foreign Minister Heo Jeong. A new parliamentary election was held on July 29, 1960. The Democratic Party, which had been in the opposition during the First Republic, easily gained power and the Second Republic was established. The revised constitution dictated the Second Republic to take the form of a parliamentary cabinet system where the President took only a nominal role. This was the first and the only instance South Korea turned to a parliamentary cabinet system instead of a presidential system. The assembly elected Yun Bo-seon as President and Chang Myon as the Prime Minister and head of government in August, 1960.

The Second Republic saw the proliferation of political activity which had been repressed under the Rhee regime. Much of this activity was from leftist and student groups, which had been instrumental in the overthrow of the First Republic. Union membership and activity grew rapidly during the later months of 1960, including the Teachers' Union, Journalists' Union, and the Federation of Korean Trade Union. Around 2,000 demonstrations were held during the eight months of the Second Republic.

Under pressure from the left, the Chang government carried out a series of purges of military and police officials who had been involved in anti-democratic activities or corruption. A Special Law to this effect was passed on October 31, 1960. 40,000 people were placed under investigation; of these, more than 2,200 government officials and 4,000 police officers were purged. In addition, the government considered reducing the size of the army by 100,000, although this plan was shelved.

In economic terms as well, the government was faced with mounting instability. The government formulated a Five-Year Economic Development Plan, although it was unable to act on it prior to being overthrown. The Second Republic saw the "hwan" lose half of its value against the dollar between fall 1960 and spring 1961.

Although the government had been established with support of the people, it had failed to implement effective reforms which brought about endless social unrest, political turmoil and ultimately, the May 16 coup.

The May 16 coup, led by Major General Park Chung-hee on May 16, 1961, put an effective end to the Second Republic. Park was one of a group of military leaders who had been pushing for the de-politicization of the military. Dissatisfied with the cleanup measures undertaken by the Second Republic and convinced that the current disoriented state would collapse into communism, they chose to take matters into their own hands.

The National Assembly was dissolved and military officers replaced the civilian officials. In May 1961, the junta declared "Pledges of the Revolution": anticommunism was to be the nation's main policy; friendly relations would be strengthened with allies of the free world, notably the United States; all corruption and government misdeed would be disposed and "fresh and clean morality" would be introduced; the reconstruction of a self-reliant economy would be priority; the nation's ability would be nurtured to fight against communism and achieve reunification; and that government would be returned to a democratic civilian government within two years.

As a means to check the opposition, the military authority created the Korean Central Intelligence Agency (KCIA) in June 1961, with Kim Jong-pil, a relative of Park, as its first director. In December 1962, a referendum was held on returning to a presidential system of rule, which was allegedly passed with a 78% majority. Park and the other military leaders pledged not to run for office in the next elections. However, Park became presidential candidate of the new Democratic Republican Party (DRP), which consisted of mainly KCIA officials, ran for president and won the election of 1963 by a narrow margin.

Park's administration started the Third Republic by announcing the Five-Year Economic Development Plan, an export-oriented industrialization policy. Top priority was placed on the growth of a self-reliant economy and modernization; "Development First, Unification Later" became the slogan of the times and the economy grew rapidly with vast improvement in industrial structure, especially in the basic and heavy chemical industries. Capital was needed for such development, so the Park regime used the influx of foreign aid from Japan and the United States to provide loans to export businesses, with preferential treatment in obtaining low-interest bank loans and tax benefits. Cooperating with the government, these businesses would later become the "chaebol".

Relations with Japan were normalized by the Korea-Japan treaty ratified in June 1965. This treaty brought Japanese funds in the form of loans and compensation for the damages suffered during the colonial era without an official apology from the Japanese government, sparking much protest across the nation.

The government also kept close ties with the United States, and continued to receive large amounts of aid. A status of forces agreement was concluded in 1966, clarifying the legal situation of the US forces stationed there. Soon thereafter, Korea joined the Vietnam War, eventually sending a total of 300,000 soldiers from 1964 to 1973 to fight alongside US troops and South Vietnamese Armed Forces.

Economic and technological growth during this period improved the standard for living, which expanded opportunities for education. Workers with higher education were absorbed by the rapidly growing industrial and commercial sectors, and urban population surged. Construction of the Gyeongbu Expressway was completed and linked Seoul to the nation's southeastern region and the port cities of Incheon and Busan. Despite the immense economic growth, however, the standard of living for city laborers and farmers was still low. Laborers were working with low wages to increase the price competitiveness for the export-oriented economy plan, and farmers were in near poverty as the government controlled prices. As the rural economy steadily lost ground and caused dissent among the farmers, however, the government decided to implement measures to increase farm productivity and income by instituting the Saemaul Movement ("New Village Movement") in 1971. The movement's goal was to improve the quality of rural life, modernize both rural and urban societies and narrow the income gap between them.

Park ran again in the 1967 presidential election, taking 51.4% of the vote. At the time the presidency was constitutionally limited to two terms, but a constitutional amendment was forced through the National Assembly in 1969 to allow him to seek a third term. Major protests and demonstrations against the constitutional amendment broke out, with large support gaining for the opposition leader Kim Dae-jung, but Park was again re-elected in the 1971 presidential election.

Parliamentary elections followed shortly after the presidential election where the opposition party garnered most of the seats, giving them the power to pass constitutional amendments. Park, feeling threatened, declared a state of national emergency on December 6, 1971. In the midst of this domestic insecurity, the Nixon Doctrine had eased tensions among the world superpowers on the international scene, which caused a dilemma for Park, who had justified his regime based on the state policy of anti-communism. In a sudden gesture, the government proclaimed a joint communiqué for reunification with North Korea on July 4, 1972, and held Red Cross talks in Seoul and Pyongyang. However, there was no change in government policy regarding reunification, and on October 17, 1972, Park declared martial law, dissolving the National Assembly and suspending the constitution.

The Fourth Republic began with the adoption of the Yushin Constitution on November 21, 1972. This new constitution gave Park effective control over the parliament and the possibility of permanent presidency. The president would be elected through indirect election by an elected body, and the term of presidency was extended to six years with no restrictions on reappointment. The legislature and judiciary were controlled by the government, and educational guidelines were under direct surveillance as well. Textbooks supporting the ideology of the military government were authorized by the government, diminishing the responsibilities of the Ministry of Education.

Despite social and political unrest, the economy continued to flourish under the authoritarian rule with the export-based industrialization policy. The first two five-year economic development plans were successful, and the 3rd and 4th five-year plans focused on expanding the heavy and chemical industries, raising the capability for steel production and oil refining. However, large conglomerate "chaebols" continuously received preferential treatment and came to dominate the domestic market. As most of the development had come from foreign capital, most of the profit went back to repaying the loans and interest.

Students and activists for democracy continued their demonstrations and protests for the abolition of the Yushin system and in the face of continuing popular unrest, Park's administration promulgated emergency decrees in 1974 and 1975, which led to the jailing of hundreds of dissidents. The protests grew larger and stronger, with politicians, intellectuals, religious leaders, laborers and farmers all joining in the movement for democracy. In 1978, Park was elected to another term by indirect election, which was met with more demonstrations and protests. The government retaliated by removing the opposition leader Kim Young-sam from the assembly and suppressing the activists with violent means. In 1979, mass anti-government demonstrations occurred nationwide, in the midst of this political turmoil, Park Chung-hee was assassinated by the director of the KCIA, Kim Jae-gyu, thus bringing the 18-year rule of military regime to an end.

After the assassination of Park Chung-hee, Prime Minister Choi Kyu-hah took the president's role only to be usurped 6 days later by Major General Chun Doo-hwan's 1979 Coup d'état of December Twelfth. In May of the following year, a vocal civil society composed primarily of university students and labour unions led strong protests against authoritarian rule all over the country. Chun Doo-hwan declared martial law on May 17, 1980, and protests escalated. Political opponents Kim Dae-jung and Kim Jong-pil were arrested, and Kim Young-sam was confined to house arrest.

On May 18, 1980, a confrontation broke out in the city of Gwangju between protesting students of Chonnam National University and the armed forces dispatched by the Martial Law Command. The incident turned into a citywide protest that lasted nine days until May 27 and resulted in the Gwangju massacre. Immediate estimates of the civilian death toll ranged from a few dozen to 2000, with a later full investigation by the civilian government finding nearly 200 deaths and 850 injured. In June 1980, Chun ordered the National Assembly to be dissolved. He subsequently created the National Defense Emergency Policy Committee, and installed himself as a member. On 17 July, he resigned his position of KCIA Director, and then held only the position of committee member. In September 1980, President Choi Kyu-hah was forced to resign from president to give way to the new military leader, Chun Doo-hwan.

In September of that year, Chun was elected president by indirect election and inaugurated in March of the following year, officially starting the Fifth Republic. A new Constitution was established with notable changes; maintaining the presidential system but limiting it to a single 7-year term, strengthening the authority of the National Assembly, and conferring the responsibilities of appointing judiciary to the Chief Justice of the Supreme Court. However, the system of indirect election of the president stayed and many military persons were appointed to highly ranked government positions, keeping the remnants of the Yushin era.

The government promised a new era of economic growth and democratic justice. Tight monetary laws and low interest rates contributed to price stability and helped the economy boom with notable growth in the electronics, semi-conductor, and automobile industries. The country opened up to foreign investments and GDP rose as Korean exports increased. This rapid economic growth, however, widened the gap between the rich and the poor, the urban and rural regions, and also exacerbated inter-regional conflicts. These dissensions, added to the hard-line measures taken against opposition to the government, fed intense rural and student movements, which had continued since the beginning of the republic.

In foreign policy, ties with Japan were strengthened by state visits by Chun to Japan and Japanese Prime Minister Yasuhiro Nakasone to Korea. U.S. President Ronald Reagan also paid a visit, and relations with the Soviet Union and China improved. The relationship with North Korea was strained when in 1983 a terrorist bomb attack in Burma killed 17 high-ranking officials attending memorial ceremonies and North Korea was alleged to be behind the attacks. However, in 1980 North Korea had submitted a "one nation, two system" reunification proposal which was met with a suggestion from the South to meet and prepare a unification constitution and government through a referendum. The humanitarian issue of reuniting separated families was dealt with first, and in September 1985, families from both sides of the border made cross visits to Seoul and Pyongyang in an historic event.The government made many efforts for cultural development: the National Museum of Korea, Seoul Arts Center, and National Museum of Contemporary Art were all constructed during this time. The 1986 Asian Games were held successfully, and the bid for the 1988 Summer Olympics in Seoul was successful as well.

Despite economic growth and success in diplomatic relations, the government that gained power by coup d'etat was essentially a military regime and the public's support and trust in it was low when the promises for democratic reform never materialized. In the 1985 National Assembly elections, opposition parties won more votes than the government party, clearly indicating that the public wanted a change. Many started to sympathize with the protesting students. The Gwangju massacre was never forgotten and in January 1987, when a protesting Seoul National University student died under police interrogation, public fury was immense. In April 1987, President Chun made a declaration that measures would be taken to protect the current constitution, instead of reforming it to allow for the direct election of the president. This announcement consolidated and strengthened the opposition; in June 1987, more than a million students and citizens participated in the nationwide anti-government protests of the June Democracy Movement.

On June 29, 1987, the government's presidential nominee Roh Tae-woo gave in to the demands and announced the June 29 Declaration, which called for the holding of direct presidential elections and restoration of civil rights. In October 1987 a revised Constitution was approved by a national referendum and direct elections for a new president were held in December, bringing the Fifth Republic to a close.

The Sixth Republic was established in 1987 and remains the current polity of South Korea.

Roh Tae-woo became president for the 13th presidential term in the first direct presidential election in 16 years. Although Roh was from a military background and one of the leaders of Chun's coup d'état, the inability of the opposition leaders Kim Dae-jung and Kim Young-sam to agree on a unified candidacy, led to his being elected. The first female presidential candidate, Hong Sook-ja, even withdrew from the race in order to back Kim Young-sam against Roh.Roh was officially inaugurated in February 1988. The government set out to eliminate past vestiges of authoritarian rule, by revising laws and decrees to fit democratic provisions. Freedom of the press was expanded, university autonomy recognised, and restrictions on overseas travels were lifted. However, the growth of the economy had slowed down compared to the 1980s, with strong labor unions and higher wages reducing the competitiveness of Korean products on the international market, resulting in stagnant exports, while commodity prices kept on rising.

Shortly after Roh's inauguration, the Seoul Olympics took place, raising South Korea's international recognition and also greatly influencing foreign policy. Roh's government announced the official unification plan, "Nordpolitik", and established diplomatic ties with the Soviet Union, China, and countries in East Europe.

A historic event was held in 1990 when North Korea accepted the proposal for exchange between the two Koreas, resulting in high-level talks, and cultural and sports exchanges. In 1991, a joint communiqué on denuclearization was agreed upon, and the two Koreas simultaneously became members of the UN.

Kim Young-sam was elected president in the 1992 elections after Roh's tenure. He was the country's first civilian president in 30 years and promised to build a "New Korea". The government set out to correct the mistakes of the previous administrations. Local government elections were held in 1995, and parliamentary elections in 1996. In a response to popular demand, former presidents Chun and Roh were both indicted on charges linked to bribery, illegal funds, and in the case of Chun, responsibility for the Gwangju massacre. They were tried and sentenced to prison in December 1996.

Relations with the North improved and a summit meeting was planned, but postponed indefinitely with the death of Kim Il-sung. Tensions varied between the two Koreas thereafter, with cycles of small military skirmishes and apologies. The government also carried out substantial financial and economical reforms, joining the OECD in 1996, but encountered difficulties with political and financial scandals involving his son. The country also faced a variety of catastrophes which claimed many lives: a train collision and a ship sinking in 1993, and the Seongsu Bridge and Sampoong Department Store collapses in 1994 and 1995. These incidents were a blow to the civilian government.

In 1997, the nation suffered a severe financial crisis, and the government approached the International Monetary Fund for relief funds. This was the limit to what the nation could bear and led to the opposition leader Kim Dae-jung winning the presidency in the same year. This is the first time an opposition candidate won the presidency.

In February 1998, Kim Dae-jung was officially inaugurated. South Korea had maintained its commitment to democratize its political processes and this was the first transfer of the government between parties by peaceful means. Kim's government faced the daunting task of overcoming the economic crisis, but with the joint efforts of the government's aggressive pursuit of foreign investment, cooperation from the industrial sector, and the citizen's gold-collecting campaign, the country was able to come out of the crisis in a relatively short period of time.

Industrial reconstruction of the big conglomerate "chaebols" was pursued, a national pension system was established in 1998, educational reforms were carried out, government support for the IT field was increased, and notable cultural properties were registered as UNESCO Cultural Heritage sites. The 2002 FIFA World Cup, co-hosted with Japan, was a major cultural event where millions of supporters gathered to cheer in public places.

In diplomacy, Kim Dae-jung pursued the "Sunshine Policy", a series of efforts to reconcile with North Korea. This culminated in reunions of the separated families of the Korean War and a summit talk with North Korean leader Kim Jong-il. For these efforts, Kim Dae-jung was awarded the Nobel Peace Prize in 2000. However, between a lack of peaceful cooperation from North Korea and the terrorist attacks on the United States on September 11, 2001, changing the view of the U.S. on North Korea, the efficacy of the Sunshine Policy was brought into question. With added allegations of corruption, support waned in the later years of the administration.

Roh Moo-hyun was elected to the presidency in December 2002 by direct election. His victory came with much support from the younger generation and civic groups who had hopes of a participatory democracy, and Roh's administration consequently launched with the motto of "participation government". Unlike the previous governments, the administration decided to take a long-term view and execute market-based reforms at a gradual pace. This approach did not please the public, however, and by the end of 2003, approval ratings were falling.

The Roh administration succeeded in overcoming regionalism in South Korean politics, diluting the collusive ties between politics and business, empowering the civil society, settling the Korea-United States FTA issue, continuing summit talks with North Korea, and launching the high-speed train system, KTX. But despite a boom in the stock market, youth unemployment rates were high, real estate prices skyrocketed and the economy lagged.

In March 2004, the National Assembly voted to impeach Roh on charges of breach of election laws and corruption. This motion rallied his supporters and affected the outcome of the parliamentary election held in April, with the ruling party becoming the majority. Roh was reinstated in May by the Constitutional Court, who had overturned the verdict. However, the ruling party then lost its majority in by-elections in 2005, as discontinued reform plans, continual labor unrest, Roh's personal feuds with the media, and diplomatic friction with the United States and Japan caused criticism of the government's competence on political and socioeconomic issues and on foreign affairs.

In April 2009, Roh Moo-hyun and his family members were investigated for bribery and corruption; Roh denied the charges. On 23 May 2009, Roh committed suicide by jumping into a ravine.

Roh's successor, Lee Myung-bak, was inaugurated in February 2008. Stating "creative pragmatism" as a guiding principle, Lee's administration set out to revitalize the flagging economy, re-energize diplomatic ties, stabilize social welfare, and meet the challenges of globalization. In April 2008, the ruling party secured a majority in the National Assembly elections. Also that month, summit talks with the United States addressed the Korea-US Free Trade Agreement and helped ease tensions between the two countries caused by the previous administrations. Lee agreed to lift the ban on US beef imports, which caused massive protests and demonstrations in the months that followed, as paranoia of potential mad cow disease gripped the country.

Many issues plagued the government in the beginning of the administration: controversies regarding the appointment of high-ranking government officials, rampant political conflicts, accusations of oppression of media and strained diplomatic relationships with North Korea and Japan. The economy was affected by the global recession as the worst economic crisis since 1997 hit the country. The Lee administration tackled these issues by actively issuing statements, reshuffling the cabinet, and implementing administrative and industrial reforms.

After regulatory and economic reforms, the economy bounced back, with the country's economy marking growth and apparently recovering from the global recession. The administration also pursued improved diplomatic relations by holding summit talks with the United States, China and Japan, and participating in the ASEAN-ROK Commemorative Summit to strengthen ties with other Asian countries. The 2010 G20 summit was held in Seoul, where issues regarding the global economic crisis were discussed.

Park Geun-hye was inaugurated in February 2013. She is the eighteenth President of South Korea and is the eldest child of South Korea's third President, Park Chung-hee. She was the first woman to be elected South Korean president, and to be elected as a head of state in the modern history of Northeast Asia. Over the years, however, her reputation was marred by her incompetency of handling the 2014 Sewol tragedy, and later a major scandal, leading to her impeachment in December 2016. The corruption scandal involving Choi Soon-sil quickly blew up after reports from multiple news organizations (the most notable of which was JTBC) in 2016, nationwide protests ensued on a weekly basis, with participant count hitting a maximum of over 2.3 million (as reported by the protesters). These protests turned out to be the biggest series of mass protests in Korean history. The protests continued even after the Congress voted on Park's impeachment. Prime Minister Hwang Kyo-ahn acted as President of South Korea pending completion of investigations into the actions of Park Geun-hye, and in the absence of any intervening election. The impeachment was upheld by the Constitutional Court on 10 March 2017, ending Park's presidency and forcing her out of office.

Moon Jae-in is the current president of South Korea. He was inaugurated on May 10, 2017. As President, Moon Jae-in has met with North Korean chairman Kim Jong-un at the April 2018 inter-Korean summit, May 2018 inter-Korean summit, and September 2018 inter-Korean summit.





</doc>
<doc id="27021" url="https://en.wikipedia.org/wiki?curid=27021" title="Geography of South Korea">
Geography of South Korea

South Korea is located in East Asia, on the southern half of the Korean Peninsula located out from the far east of the Asian landmass. The only country with a land border to South Korea is North Korea, lying to the north with of border running along the Korean Demilitarized Zone. South Korea is mostly surrounded by water and has of coastline along three seas; to the west is the Yellow Sea (West Sea), to the south is the East China Sea, and to the east is the East Sea. Geographically, South Korea's land mass is approximately . of South Korea are occupied by water. The approximate coordinates are 37° North, 127° 30 East.

The Korean Peninsula extends southward from the northeast part of the Asian continental landmass. The Japanese islands of Honshū and Kyūshū are located some 200 km (124 mi) to the southeast across the Korea Strait; the Shandong Peninsula of China lies 190 kilometers to the west. The west coast of the peninsula is bordered by the Korea Bay to the north and the Yellow Sea and Korea Strait to the south; the east coast is bordered by the East sea. The 8,640-kilometer coastline is highly indented. Some 3,579 islands lie adjacent to the peninsula. Most of them are found along the south and west coasts.

The line between the two Korean states was the thirty-eighth parallel of latitude. After the Korean War, the Korean Demilitarized Zone (DMZ) formed the boundary between the two. The DMZ is a heavily guarded, 4,000-meter-wide strip of land that runs along the demarcation line established by the Korean Armistice Agreement from the east to the west coasts for a distance of 241 kilometers (238 kilometers of that line from the land boundary with North Korea).

The total land area of the peninsula, including the islands, is 223,170 square kilometers. Some 44.8 percent (100 210 square kilometers) of this total, excluding the area within the DMZ, constitutes the territory of the Republic of Korea. The combined territories of North Korea and South Korea are about the same size as the U.S. state of Minnesota. South Korea alone is about the size of Portugal or Hungary, or the U.S. state of Indiana.

The largest island, Jeju-do, lies off the southwest corner of the peninsula and has a land area of 1,825 square kilometers. Other important islands include Ulleung and Liancourt Rocks in the Sea of Japan and Ganghwa Island at the mouth of the Han River. Although the eastern coastline of South Korea is generally unindented, the southern and western coasts are jagged and irregular. The difference is caused by the fact that the eastern coast is gradually rising, while the southern and western coasts are subsiding.

Early European visitors to Korea remarked that the land resembled "a sea in a heavy gale" because of the large number of successive mountain ranges that crisscross the peninsula. The highest mountains are in North Korea. The highest mountain peak in South Korea is Hallasan (1,950 m), which is the cone of a volcanic formation constituting Jeju Island. There are two major mountain ranges within South Korea: the Taebaek Mountains, and the Sobaek Mountains.

Unlike Japan or the northern provinces of China, the Korean Peninsula is geologically stable. There are no active volcanoes (aside from Baekdu Mountain on the border between North Korea and China, most recently active in 1903), and there have been no strong earthquakes. Historical records, however, describe volcanic activity on Mount Halla during the Goryeo Dynasty (918–1392).

South Korea has no extensive plains; its lowlands are the product of mountain erosion. Approximately 30 percent of the area of South Korea consists of lowlands, with the rest consisting of uplands and mountains. The great majority of the lowland area lies along the coasts, particularly the west coast, and along the major rivers. The most important lowlands are the Han River plain around Seoul, the Pyeongtaek coastal plain southwest of Seoul, the Geum River basin, the Nakdong River basin, and the Yeongsan River and the Honam plains in the southwest. A narrow littoral plain extends along the east coast.

The Nakdong is South Korea's longest river (521 kilometers). The Han River, which flows through Seoul, is 514 kilometers long, and the Geum River is 401 kilometers long. Other major rivers include the Imjin, which flows through both North Korea and South Korea and forms an estuary with the Han River; the Bukhan, a tributary of the Han that also flows out of North Korea; and the Somjin. The major rivers flow north to south or east to west and empty into the Yellow Sea or the Korea Strait. They tend to be broad and shallow and to have wide seasonal variations in water flow.

In the early part of 20th century and especially the period during and after World War II and the Korean War, much of the existing Korean forests were cut down, which led to problems with flooding and soil erosion. Combination of reforestation efforts (e.g. Arbor day was celebrated as a national holiday starting in 1949) and policies designed to reduce use of firewood as a source of energy (e.g. restriction of inflow of firewood into Seoul and other major cities starting in 1958) helped to spark a recovery in the 1950s. Comprehensive reforestation programs starting in the 1970s and continuing into the late 1990s aided in an acceleration of forest volume increase, and the forest cover reached a peak of 65% of national land area in 1980 as opposed to a low of 35% in 1955.

News that North Korea was constructing a huge multipurpose dam at the base of Geumgangsan (1,638 m) north of the DMZ caused considerable consternation in South Korea during the mid-1980s. South Korean authorities feared that once completed, a sudden release of the dam's waters into the Pukhan River during north-south hostilities could flood Seoul and paralyze the capital region. During 1987 the Geumgangsan Dam was a major issue that Seoul sought to raise in talks with Pyongyang. Though Seoul completed a "Peace Dam" on the Pukhan River to counteract the potential threat of Pyongyang's dam project before the 1988 Olympics, the North Korean project apparently still was in its initial stages of construction in 1990.

Maritime claims:
"territorial sea:"
"contiguous zone:"

"exclusive economic zone:"

"continental shelf:"
not specified

Elevation extremes:
"lowest point:"
Sea level 0 m
"highest point:"
Hallasan 1,950 m

Part of the East Asian Monsoon region, South Korea has a temperate climate with four distinct seasons. The movement of air masses from the Asian continent exerts greater influence on South Korea's weather than does air movement from the Pacific Ocean. Winters are usually long, cold, and dry, whereas summers are short, hot, and humid. Spring and autumn are pleasant but short in duration. Seoul's mean temperature in January is ; in July the mean temperature is about . Because of its southern and seagirt location, Jeju Island has warmer and milder weather than other parts of South Korea. Mean temperatures on Jeju range from in January to in July.

The country generally has sufficient rainfall to sustain its agriculture. Rarely does less than of rain fall in any given year; for the most part, rainfall is over . Amounts of precipitation, however, can vary from year to year. Serious droughts occur about once every eight years, especially in the rice-producing southwestern part of the country. About two-thirds of the annual precipitation occurs between June and September.

South Korea is less vulnerable to typhoons than Japan, Taiwan, the east coast of China, or the Philippines. From one to three typhoons can be expected per year. Typhoons usually pass over South Korea in late summer, especially in August, and bring torrential rains. Flooding occasionally causes considerable damage, as do landslides, given the country's generally mountainous terrain.

In September 1984, record floods caused the deaths of 190 people and left 200,000 homeless. This disaster prompted the North Korean government to make an unprecedented offer of humanitarian aid in the form of rice, medicine, clothes, and building materials. South Korea accepted these items and distributed them to flood victims.

Graphically the seasons can be represented this way:






There are occasional typhoons that bring high winds and floods. There is also low-level seismic activity, which is common in the southwest.

Hallasan (elev. 1,950 m) is considered historically active although it has not erupted in many centuries. Earthquake activity is minimal, however, since 2016 there have been two earthquakes over 5.4 magnitude.

Habitat loss and degradation, especially of wetlands, through coastal reclamation (e.g. Saemangeum, Shiwa, Song Do, Namyang Bay, Asan Bay, in the south-west, Gwangyang Bay and the Nakdong Estuary) have caused huge declines in fisheries and of biodiversity. Most riverine wetland in Korea is now threatened by the proposed Grand Korean Waterway project. There are also some problems air pollution in large cities; as well as water pollution from the discharge of sewage and industrial effluents. Drift netting is another issue.

South Korea is a party to: Antarctic-Environmental Protocol, Antarctic-Marine Living Resources, Antarctic Treaty, Biodiversity, Climate Change, Climate Change-Kyoto Protocol, Desertification, Endangered Species, Environmental Modification, Hazardous Wastes, Law of the Sea, Marine Dumping, Ozone Layer Protection, Ship Pollution (MARPOL 73/78), Tropical Timber 83, Tropical Timber 94, Wetlands, Whaling




</doc>
<doc id="27022" url="https://en.wikipedia.org/wiki?curid=27022" title="Demographics of South Korea">
Demographics of South Korea

This article is about the demographic features of the population of South Korea, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.

In June 2012, South Korea's population reached 50 million, and by the end of 2016, South Korea's population had surpassed 51 million people. Since the 2000s, South Korea has been struggling with a low birthrate, leading some researchers to suggest that if current population trends hold, the country's population will shrink to approximately 38 million population towards the end of the 21st century. In 2018, fertility in South Korea became again a topic of international debate after only 26,500 babies were born in October and an estimated of 325,000 babies in the year, causing the country to have the lowest birth rate in the world.

In South Korea, a variety of different Asian people had migrated to the Korean Peninsula in past centuries, however few have remained permanently. South Korea and North Korea are among the world's most ethnically homogenous nations. Both North Korea and South Korea equate nationality or citizenship with membership in a single, homogenous ethnic group and politicized notion of "race."

The common language and especially race are viewed as important elements by South Koreans in terms of identity, more than citizenship.

Population of South Korea by age and sex (demographic pyramid)

According to Worldometers' South Korea Population Forecast statistics, South Korea is supposed to have a 0.36% yearly change increase by 2020, a 0.28% yearly change increase by 2025, a 0.18% yearly change increase by 52,701,817, and a 0.04% yearly change increase by 2035. According to those same statistics, the years from 2040 to 2050 are supposed to have a steady decline of yearly change percentages.

The population of South Korea showed robust growth since the republic's establishment in 1948, and then dramatically slowed down with the effects of its economic growth. In the first official census, taken in 1949, the total population of South Korea was calculated at 20,188,641 people. The 1985 census total was 40,466,577. Population growth was slow, averaging about 1.1% annually during the period from 1949 to 1955, when the population registered at 21.5 million. Growth accelerated between 1955 and 1966 to 29.2 million or an annual average of 2.8%, but declined significantly during the period 1966 to 1985 to an annual average of 1.7%. Thereafter, the annual average growth rate was estimated to be less than 1%, similar to the low growth rates of most industrialized countries and to the target figure set by the Ministry of Health and Social Affairs for the 1990s. As of January 1, 1989, the population of South Korea was estimated to be approximately 42.2 million.

The proportion of the total population under fifteen years of age has risen and fallen with the growth rate. In 1955 approximately 41.2% of the population was under fifteen years of age, a percentage that rose to 43.5% in 1966 before falling to 38.3% in 1975, 34.2% in 1980, and 29.9% in 1985. In the past, the large proportion of children relative to the total population put great strains on the country's economy, particularly because substantial resources were invested in education facilities. With the slowdown in the population growth rate and a rise in the median age (from 18.7 years to 21.8 years between 1960 and 1980), the age structure of the population has begun to resemble the columnar pattern typical of developed countries, rather than the pyramidal pattern found in most parts of the Third World.

The decline in the population growth rate and in the proportion of people under fifteen years of age after 1966 reflected the success of official and unofficial birth control programs. The government of President Syngman Rhee (1948–60) was conservative in such matters. Although Christian churches initiated a family planning campaign in 1957, it was not until 1962 that the government of Park Chung Hee, alarmed at the way in which the rapidly increasing population was undermining economic growth, began a nationwide family planning program. Other factors that contributed to a slowdown in population growth included urbanization, later marriage ages for both men and women, higher education levels, a greater number of women in the labor force, and better health standards.

Public and private agencies involved in family planning included the Ministry of Health and Social Affairs, the Ministry of Home Affairs, the Planned Parenthood Federation of Korea, and the Korea Institute of Family Planning. In the late 1980s, their activities included distribution of free birth control devices and information, classes for women on family planning methods, and the granting of special subsidies and privileges (such as low-interest housing loans) to parents who agreed to undergo sterilization. There were 502,000 South Koreans sterilized in 1984, as compared with 426,000 in the previous year.

The 1973 Maternal and Child Health Law legalized abortion. In 1983 the government began suspending medical insurance benefits for maternal care for pregnant women with three or more children. It also denied tax deductions for education expenses to parents with two or more children.

As in China, cultural attitudes posed problems for family planning programs. A strong preference for sons—who in Korea's traditional Confucian value system are expected to care for their parents in old age and carry on the family name—means that parents with only daughters usually continued to have children until a son is born. The government encouraged married couples to have only one child. This has been a prominent theme in public service advertising, which stresses "have a single child and raise it well."

Total fertility rates (the average number of births a woman will have during her lifetime) fell from 6.1 births per female in 1960 to 4.2 in 1970, 2.8 in 1980, and 2.4 in 1984. The number of live births, recorded as 711,810 in 1978, grew to a high of 917,860 in 1982. This development stirred apprehensions among family planning experts of a new "baby boom." By 1986, however, the number of live births had declined to 806,041.

Decline in population growth continued, and between 2005 and 2010 total fertility rate for South Korean women was 1.21, one of the world's lowest according to the United Nations. Fertility rate well below the replacement level of 2.1 births per female has triggered a national alarm, with dire predictions of an aging society unable to grow or support its elderly. Recent Korean governments have prioritized the issue on its agenda, promising to enact social reforms that will encourage women to have children.

The country's population increased to 46 million by the end of the twentieth century, with growth rates ranging between 0.9% and 1.2%. The population is expected to stabilize (that is, cease to grow) in the year 2023 at around 52.6 million people. In the words of "Asiaweek" magazine, the "stabilized tally will approximate the number of Filipinos in 1983, but squeezed into less than a third of their [the Philippines'] space."

As of early 2019, the birth rate of South Korea reached an alarmingly low number. In February 2019, the Korean birth rate fell to 0.98, well below the replacement level of 2.1 births. South Korea is now the fastest aging developed country in the world. The Korean government (and their failing actions against the birth rate issue) and the worsening economic environment for young people are blamed as main cause.

South Korea is one of the world's most densely populated countries, with an estimated 425 people per square kilometer in 1989—over sixteen times the average population density of the United States in the late 1980s. By comparison, China had an estimated 114 people, the Federal Republic of Germany (West Germany) 246 people, and Japan 323 people per square kilometer in the late 1980s. Because about 70% of South Korea's land area is mountainous and the population is concentrated in the lowland areas, actual population densities were in general greater than the average. As early as 1975, it was estimated that the density of South Korea's thirty-five cities, each of which had a population of 50,000 or more inhabitants, was 3,700 people per square kilometer. Because of continued migration to urban areas, the figure was higher in the late 1980s.

In 1988 Seoul had a population density of 17,030 people per square kilometer as compared with 13,816 people per square kilometer in 1980. The second largest city, Busan, had a density of 8,504 people per square kilometer in 1988 as compared with 7,272 people in 1980. Kyonggi Province, which surrounds the capital and contains Inch'on, the country's fourth largest city, was the most densely populated province; Kangwon Province in the northeast was the least densely populated province.

According to the government's Economic Planning Board, the population density will be 530 people per square kilometer by 2023, the year the population is expected to stabilize.

Rural areas in South Korea consist of agglomerated villages in river valleys and range from a few houses to several hundred. These villages are located in the south that are backed by hills and give strong protection from winter winds.

Since 1960, the pace of urbanization in South Korea has hit a considerable decline in population of rural areas and the traditional rural lifestyle has been slowly fading away.

South Korea faces the problem of a rapidly aging population. In fact, the speed of aging in Korea is unprecedented in human history, 18 years to double aging population from 7 – 14% (least number of years), overtaking even Japan. Statistics support this observation, the percentage of elderly aged 65 and above, has sharply risen from 3.3% in 1955 to 10.7% in 2009. The shape of its population has changed from a pyramid in the 1990s, with more young people and fewer old people, to a diamond shape in 2010, with less young people and a large proportion of middle-age individuals.

There are several implications and issues associated with an aging population. A rapidly aging population is likely to have several negative implications on the labour force. In particular, experts predict that this might lead to a shrinking of the labour force. As an increasing proportion of people enter their 50s and 60s, they either choose to retire or are forced to retire by their companies. As such, there would be a decrease in the percentage of economically active people in the population. Also, with rapid aging, it is highly likely that there would be an imbalance in the young-old percentage of the workforce. This might lead to a lack of vibrancy and innovation in the labour force, since it is helmed mainly by the middle-age workers. Data shows that while there are fewer young people in society, the percentage of economically active population, made up of people ages 15 – 64, has gone up by 20% from 55.5% to 72.5%. This shows that the labour force is indeed largely made up of middle-aged workers.

A possible consequence might be that South Korea would be a less attractive candidate for investment. Investors might decide to relocate to countries like Vietnam and China, where there is an abundance of cheaper, younger labour. If employers were to choose to maintain operations in South Korea, there is a possibility that they might incur higher costs in retraining or upgrading the skills of this group of middle-age workers. On top of that, higher healthcare costs might also be incurred and the government would need to set aside more money to maintain a good healthcare system to cater to the elderly.

Due to the very low birth rate, South Korea is predicted to enter a Russian Cross pattern once the large generation born in the 1960s starts to die off, with potentially decades of population decline.

Since 2016, the number of elderly people (+65 years old) outnumbered children (0 - 14 years) and the country became an "aged society". People older than 65 make up more than 14% of the total population.

Like other newly industrializing economies, South Korea experienced rapid growth of urban areas caused by the migration of large numbers of people from the countryside. In the eighteenth and nineteenth centuries, Seoul, by far the largest urban settlement, had a population of about 190,000 people. There was a striking contrast with Japan, where Edo (Tokyo) had as many as 1 million inhabitants and the urban population comprised as much as 10% to 15% of the total during the Tokugawa Period (1600–1868). During the closing years of the Choson Dynasty and the first years of Japanese colonial rule, the urban population of Korea was no more than 3% of the total. After 1930, when the Japanese began industrial development on the Korean Peninsula, particularly in the northern provinces adjacent to Manchuria, the urban portion of the population began to grow, reaching 11.6% for all of Korea in 1940.

Between 1945 and 1985, the urban population of South Korea grew from 14.5% to 65.4% of the total population. In 1988 the Economic Planning Board estimated that the urban portion of the population will reach 78.3% by the end of the twentieth century. Most of this urban increase was attributable to migration rather than to natural growth of the urban population. Urban birth rates have generally been lower than the national average. The extent of urbanization in South Korea, however, is not fully revealed in these statistics. Urban population was defined in the national census as being restricted to those municipalities with 50,000 or more inhabitants. Although many settlements with fewer than 50,000 inhabitants were satellite towns of Seoul or other large cities or mining communities in northeastern Kangwon Province, which would be considered urban in terms of the living conditions and occupations of the inhabitants, they still were officially classified as rural.

The dislocation caused by the Korean War accounted for the rapid increase in urban population during the early 1950s. Hundreds of thousands of refugees, many of them from North Korea, streamed into the cities. During the post-Korean War period, rural people left their ancestral villages in search of greater economic and educational opportunities in the cities. By the late 1960s, migration had become a serious problem, not only because cities were terribly overcrowded, but also because the rural areas were losing the most youthful and productive members of their labor force.

In 1970, the Park Chung Hee government launched the Saemaul Undong (New Community Movement) as a rural reconstruction and self-help movement to improve economic conditions in the villages, close the wide gap in income between rural and urban areas, and stem urban migration—as well as to build a political base. Despite a huge amount of government sponsored publicity, especially during the Park era, it was not clear by the late 1980s that the Saemaul undong had achieved its objectives. By that time many, if not most, farming and fishing villages consisted of older persons; relatively few able-bodied men and women remained to work in the fields or to fish. This trend was apparent in government statistics for the 1986–87 period: the proportion of people fifty years old or older living in farming communities grew from 28.7% in 1986 to 30.6% in 1987, while the number of people in their twenties living in farming communities declined from 11.3% to 10.8%. The nationwide percentages for people fifty years old or older and in their twenties were, in 1986, 14.9% and 20.2%, respectively.

In 1985 the largest cities were Seoul (9,645,932 inhabitants), Busan (3,516,807), Daegu (2,030,672), Incheon (1,387,491), Gwangju (906,129), and Daejeon (866,695). According to government statistics, the population of Seoul, one of the world's largest cities, surpassed 10 million people in late 1988. Seoul's average annual population growth rate during the late 1980s was more than 3%. Two-thirds of this growth was attributable to migration rather than to natural increase. Surveys revealed that "new employment or seeking a new job," "job transfer," and "business" were major reasons given by new immigrants for coming to the capital. Other factors cited by immigrants included "education" and "a more convenient area to live."

To alleviate overcrowding in Seoul's downtown area, the city government drew up a master plan in the mid-1980s that envisioned the development of four "core zones" by 2000: the original downtown area, Yongdongpo-Yeouido, Yongdong, and Jamsil. Satellite towns also would be established or expanded. In the late 1980s, statistics revealed that the daytime or commuter population of downtown Seoul was as much as six times the officially registered population. If the master plan is successful, many commuters will travel to work in a core area nearer their homes, and the downtown area's daytime population will decrease. Many government ministries have been moved out of Seoul, and the army, navy, and air force headquarters have been relocated to Daejeon.

In 1985 the population of Seoul constituted 23.8% of the national total. Provincial cities, however, experienced equal and, in many cases, greater expansion than the capital. Growth was particularly spectacular in the southeastern coastal region, which encompasses the port cities of Busan, Masan, Yosu, Chinhae, Ulsan, and Pohang. Census figures show that Ulsan's population increased eighteenfold, growing from 30,000 to 551,300 inhabitants between 1960 and 1985. With the exception of Yosu, all of these cities are in South Kyongsang Province, a region that has been an especially favored recipient of government development projects. By comparison, the population of Kwangju, capital of South Cholla Province, increased less than threefold between 1960 and 1985, growing from 315,000 to 906,129 inhabitants.

Rapid urban growth has brought familiar problems to developed and developing countries alike. The construction of large numbers of high-rise apartment complexes in Seoul and other large cities alleviated housing shortages to some extent. But it also imposed hardship on the tens of thousands of people who were obliged to relocate from their old neighborhoods because they could not afford the rents in the new buildings. In the late 1980s, squatter areas consisting of one-story shacks still existed in some parts of Seoul. Housing for all but the wealthiest was generally cramped. The concentration of factories in urban areas, the rapid growth of motorized traffic, and the widespread use of coal for heating during the severe winter months caused dangerous levels of air and water pollution, issues that still persist today even after years of environmentally friendly policies.

Like other newly industrializing economies, South Korea experienced rapid growth of urban areas caused by the migration of large numbers of people from the countryside. In 2016, 82.59 percent of South Korea's total population lived in urban areas and cities.

Source:
Sources: Our World In Data and the United Nations.

1865-1949

1950-2015

Source: "UN World Population Prospects"

The total fertility rate is the number of children born per woman. It is based on fairly good data for the entire period. Sources: Our World In Data and Gapminder Foundation.

Source:


South Korea is one of the most ethnically homogeneous countries with an absolute majority of the population of Korean ethnicity who account for approximately 96% of the total population. However, with its emergence as an economic powerhouse, opportunities for foreign immigrants increased and in 2007 the number of foreign citizens resident in South Korea passed the million mark for the first time in history, and the number reached 2 million in 2016. 1,016,000 of them came from China, with more than half of them being ethnic Koreans of Chinese citizenship. The next largest group was from Vietnam with 149,000 residents. The third largest group was from the United States with 117,000 residents, excluding the American troops stationed in the country. Thailand, Philippines, Uzbekistan and other countries followed. Many of the foreign residents from China and the former Soviet Union, including Russia and Uzbekistan, are ethnic Koreans (see Koreans in China, Koryo-saram).

When the People's Republic of China and South Korea reformed relationship was settled in, several Chinese migrants had emerged in South Korea in 1992. In the early 1900s, a trade agreement allowed merchants from China conduct business trades in South Korea.

The south Koreans place various regulations on individuals applying for citizenship via marriage with one required to pass a Korean language proficiency test and have an annual income more than 14 million wons. Due to this fact, most North Americans come to the country either as tourists or professionals.

The relationship between Vietnamese and South Koreans go back to when Lý Dương left to go to "Goryeo" in South Korea after a succession of power dispute. Likewise in 1226, Lý Long Tường, a prince of the Lý Dynasty of Đại Việt (in modern-day Vietnam), later became "Lee Yong-sang" (이용상) of Hwasan, a general of Korea. He is an ancestor of one branch of the Lee (or Rhee) family today in South Korea. Nowadays, Vietnamese migrants who go to South Korea are introduced to local husbands via marriage agencies or via study abroad.

The relationship between Filipinos and South Koreans can be traced back to the 1950s during the Korean War. Over 7,500 Filipino soldiers fought on the United Nations' side to assist South Korea's conflict with North Korea. During 2007, there was estimated to be around 70,000 Filipino immigrants in South Korea. The mass rural urban migration led to a shortage of young women in those areas. This led many Filipino brides find their way to South Korea and migrate over there.

Below are the foreigner groups in South Korea that number more than 4,000.

The Korean language is the native language spoken by the vast majority of the population. English is widely taught in both public and private schools as a foreign language. However, general fluency in English in the country is relatively low compared to other industrialized developed countries. There is a Chinese minority who speak Mandarin and Cantonese. Some elderly may still speak Japanese, which was official during the Japanese rule in Korea (1905–1945).

In different areas of South Korea, different dialects are spoken. For example, the Gyeongsang dialect spoken around Busan and Daegu to the south sounds quite rough and aggressive compared to standard Korean.

Koreans have historically, lived under the religious influences of shamanism, Buddhism, Daoism, or Confucianism.

Korea is a country where the world's most major religions, Christianity, Buddhism, and Confucianism peacefully coexist. According to 2015 statistics, 44% of Korean population has a religion and 2008 statistics show that over 510 religious organizations were in the South Korea population.


The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.



Large-scale emigration from Korea began around 1904 and continued until the end of World War II. During the Korea under Japanese rule period, many Koreans emigrated to Manchuria (present-day China's northeastern provinces of Liaoning, Jilin, and Heilongjiang), other parts of China, the Soviet Union, Hawaii, and the contiguous United States.

Most emigrated for economic reasons; employment opportunities were scarce, and many Korean farmers lost their land after the Japanese introduced a system of land registration and private land tenure, imposed higher land taxes, and promoted the growth of an absentee landlord class charging exorbitant rents. Koreans from the northern provinces of Korea went mainly to Manchuria, China, and Siberia. Many people from the southern provinces went to Japan. Koreans were conscripted into Japanese labor battalions or the Japanese army, especially during World War II. In the 1940–44 period, nearly 2 million Koreans lived in Japan, 1.4 million in Manchuria, 600,000 in Siberia, and 130,000 in China. An estimated 40,000 Koreans were scattered among other countries. At the end of World War II, approximately 2 million Koreans were repatriated from Japan and Manchuria.

More than 4 million ethnic Koreans lived outside the peninsula during the early 1980s. The largest group, about 1.7 million people, lived in China, the descendants of the Korean farmers who had left the country during the Japanese occupation. Most had assumed Chinese citizenship. The Soviet Union had about 430,000 ethnic Koreans.

By contrast, many of Japan's approximately 700,000 Koreans had below-average standards of living. This situation occurred partly because of discrimination by the Japanese majority and partly because a large number of resident Koreans, loyal to the North Korean regime of Kim Il Sung, preferred to remain separate from and hostile to the Japanese mainstream. The pro–North Korea Chongryon (General Association of Korean Residents in Japan) initially was more successful than the pro–South Korea Mindan (Association for Korean Residents in Japan) in attracting adherents among residents in Japan. Since diplomatic relations were established between Seoul and Tokyo in 1965, however, the South Korean government has taken an active role in promoting the interests of their residents in Japan in negotiations with the Japanese government. It also has provided subsidies to Korean schools in Japan and other community activities.

By the end of 1988, there were over two million South Koreans residing overseas. North America was home to over 1.2 million. South Koreans also were residents of Australia (100,000), Central and South America (45,000), the Middle East (12,000), Western Europe (40,000), New Zealand (30,000), other Asian countries (27,000), and Africa (25,000). A limited number of South Korean government-sponsored migrants settled in Chile, Argentina, and other Latin American countries.

Because of South Korea's rapid economic expansion, an increasing number of its citizens reside abroad on a temporary basis as business executives, technical personnel, foreign students, and construction workers. A large number of formerly expatriate South Koreans have returned to South Korea primarily because of the country's much improved economic conditions and the difficulties they experienced in adjusting to living abroad.




</doc>
<doc id="27023" url="https://en.wikipedia.org/wiki?curid=27023" title="Politics of South Korea">
Politics of South Korea

The politics of the Republic of Korea takes in place in the framework of a presidential representative democratic republic, whereby the President is the head of state, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and the National Assembly. The Judiciary is independent of the executive and the legislature and comprises a Supreme Court, appellate courts and a Constitutional Court. Since 1948, the constitution has undergone five major revisions, each signifying a new republic. The current Sixth Republic began with the last major constitutional revision in 1987.

Under Lee Myung-bak's conservative presidency, the South Korean intelligence services (NIS) orchestrated campaigns to manipulate public opinion. NIS-led "NGOs" have conducted media campaigns against opponents of the government; denounced the "buses of hope" (which emerged to support a trade union movement in 2011), criticized the proposals for free school meals and free medical care and called for the disbandment of the Democratic Labour Party. In 2012, the NIS conducted a slander campaign against the presidential candidate Moon Jae-in in order to divert voters to the conservative candidate Park Geun-hye. In February 2015, the former head of the NIS was sentenced to three years in prison for his role in these manipulations.

The head of state is the president, who is elected by direct popular vote for a single five-year term. The president is Commander-in-Chief of the armed force of South Korea and enjoys considerable executive powers.

The president appoints the prime minister with approval of the National Assembly, as well as appointing and presiding over the State Council of chief ministers as the head of government. On 12 March 2004, the executive power of then president Roh Moo-hyun was suspended when the Assembly voted to impeach him and Prime Minister Goh Kun became an Acting President. On 14 May 2004, the Constitutional Court overturned the impeachment decision made by the Assembly and Roh was reinstated.

On 10 March 2017, Park Geun-hye became the first president to be removed by the Constitutional Court after impeachment by the National Assembly. Prime Minister Hwang Kyo-ahn temporarily served as an acting president between the suspension of Park from 9 December 2016 until the next presidential election, which was held in May 2017. On 9 May 2017, Moon Jae-in became the 19th president of South Korea, replacing acting president Hwang Kyo-ahn.

The National Assembly (, , "gukhoe") has 300 members, elected for a four-year term, 253 members in single-seat constituencies and 47 members by proportional representation. The ruling Democratic Party of Korea is the largest party in the Assembly.

The South Korean judiciary is independent of the other two branches. The highest judiciary body is the Supreme Court, whose justices are appointed by the president with the consent of the National Assembly. In addition, the Constitutional Court oversees questions of constitutionality. South Korea has not accepted compulsory ICJ jurisdiction.

South Korea elects on national level a head of state – the president – and a legislature. The president is elected for a five-year term by the people. The National Assembly ("Gukhoe") has 300 members, elected for a four-year term, 246 members in single-seat constituencies and 54 members by proportional representation.

The main two political parties in South Korea are the liberal Democratic Party of Korea (lit. "Together Democratic Party", DPK) and the conservative Liberty Korea Party (LKP). The liberal camp and the conservative camp are the dominant forces of South Korean politics at present.

South Korea's political history has always been prone to splits from and merges with other parties. One reason is that there is greater emphasis around the 'politics of the person' and rather than party, therefore party loyalty is not strong when disagreements occur. The graph below illustrates the extent of the political volatility within the last 10 years alone. These splits were intensified after the 2016 South Korean political scandal.


One Special City ("Teukbyeolsi", Capital City), six Metropolitan Cities ("Gwangyeoksi," singular and plural), nine Provinces ("Do," singular and plural) and one Special Autonomous City (Sejong City).

AfDB, APEC, AsDB, BIS, CP, EBRD, ESCAP, FAO, G-77, IAEA, IBRD, ICAO, ICCt, ICC, ICRM, IDA, IEA (observer), IFAD, IFC, IFRCS, IHO, ILO, IMF, IMO, Inmarsat, Intelsat, Interpol, IOC, IOM, ISO, ITU, ITUC, MINURSO, NAM (guest), NSG, OAS (observer), OECD, OPCW, OSCE (partner), UN, UNCTAD, UNESCO, UNIDO, UNMOGIP, UNOMIG, UNU, UPU, WCO, WHO, WIPO, WMO, WToO, WTrO, Zangger Committee



</doc>
<doc id="27024" url="https://en.wikipedia.org/wiki?curid=27024" title="Economy of South Korea">
Economy of South Korea

The economy of South Korea is the 4th largest in Asia and the 12th largest in the world. It is a highly developed mixed economy dominated by family-owned conglomerates called chaebols; however, the dominance of the chaebol is unlikely to last and engenders risk of slowing down the transformation of the South Korean economy for the benefit of future generations. South Korea is known for its spectacular rise from one of the poorest countries in the world to a developed, high-income country in just a few generations. This economic growth is called by some a miracle, and described as the Miracle on the Han River, which has brought South Korea to the ranks of elite countries in the OECD and the G-20. South Korea still remains one of the fastest growing developed countries in the world following the Great Recession. It is included in the group of Next Eleven countries that will dominate the global economy in the middle of the 21st century.

South Korea's rigorous education system and the establishment of a highly motivated and educated populace is largely responsible for spurring the country's high technology boom and rapid economic development. Having almost no natural resources and a high population density in its territory, which deterred continued population growth and the formation of a large internal consumer market, South Korea adapted an export-oriented economic strategy to fuel its economy, and in 2014, South Korea was the seventh largest exporter and seventh largest importer in the world. Bank of Korea and Korea Development Institute periodically release major economic indicators and economic trends of the economy of South Korea.

Despite the South Korean economy's high growth potential and apparent structural stability, South Korea suffers perpetual damage to its credit rating in the stock market due to the belligerence of North Korea in times of deep military crises, which has an adverse effect on the financial markets of the South Korean economy. However, renowned financial organizations, such as the International Monetary Fund, also compliment the resilience of the South Korean economy against various economic crises, citing low state debt, and high fiscal reserves that can quickly be mobilized to address any expected financial emergencies. Other financial organizations like the World Bank describe Korea as one of the fastest-growing major economies of the next generation along with BRIC and Indonesia. South Korea was one of the few developed countries that was able to avoid a recession during the global financial crisis, and its economic growth rate reached 6.2% in 2010, a sharp recovery from economic growth rates of 2.3% in 2008 and 0.2% in 2009 when the global financial crisis hit. The South Korean economy again recovered with the record-surplus of US$70.7 billion mark of the current account in the end of 2013, up 47 percent growth from 2012, amid uncertainties of the global economic turmoil, with major economic output being the technology products exports.

Following the Korean War, South Korea remained one of the poorest countries in the world for over a decade. In 1960 its gross domestic product per capita was $79. The growth of the industrial sector was the principal stimulus to economic development. In 1986, manufacturing industries accounted for approximately 30 percent of the gross domestic product (GDP) and 25 percent of the work force. Benefiting from strong domestic encouragement and foreign aid, Seoul's industrialists introduced modern technologies into outmoded or newly built facilities at a rapid pace, increased the production of commodities—especially those for sale in foreign markets—and plowed the proceeds back into further industrial expansion. As a result, industry altered the country's landscape, drawing millions of laborers to urban manufacturing centers.

A downturn in the South Korean economy in 1989 spurred by a sharp decrease in exports and foreign orders caused deep concern in the industrial sector. Ministry of Trade and Industry analysts stated that poor export performance resulted from structural problems embedded in the nation's economy, including an overly strong won, increased wages and high labor costs, frequent strikes, and high interest rates. The result was an increase in inventories and severe cutbacks in production at a number of electronics, automobile, and textile manufacturers, as well as at the smaller firms that supplied the parts. Factory automation systems were introduced to reduce dependence on labor, to boost productivity with a much smaller work force, and to improve competitiveness. It was estimated that over two-thirds of South Korea's manufacturers spent over half of the funds available for facility investments on automation.

With the coup of General Park Chung-hee in 1961, a protectionist economic policy began, pushing a bourgeoisie that developed in the shadow of the State to reactivate the internal market. In order to promote development, a policy of industrialization by import substitution was applied, closing the entry into the country of all kinds of foreign products, except raw materials. Nor did they resort to foreign investment. An agrarian reform was carried out with expropriation without compensation of Japanese large estates. General Park nationalized the financial system to swell the powerful state arm, whose intervention in the economy was through five-year plans.

The spearhead was the chaebols, those diversified family conglomerates such as Hyundai, Samsung and LG Corporation, which received state incentives such as tax breaks, legality for their hyper-exploitation system and cheap or free financing: the state bank facilitated the planning of concentrated loans by item according to each five-year plan, and by economic group selected to lead it.

Until 1961, South Korea received a 3100 million dollar donation from the United States, a very high figure for the time, a privilege for being on the hottest frontier of the Cold War. This policy of foreign economic and military support continued for decades. The chaebols started to dominate the domestic economy and, eventually, began to become internationally competitive. Workers' saw their wages and working conditions steadily improve, which increased domestic consumption. And the country steadily rose from low income to middle income status by the 1980s.

South Korea's real gross domestic product expanded by an average of more than 8 percent per year, from US$2.7 billion in 1962 to US$230 billion in 1989, breaking the trillion dollar mark in 2006. Nominal GDP per capita grew from $103.88 in 1962 to $5,438.24 in 1989, reaching the $20,000 milestone in 2006. The manufacturing sector grew from 14.3 percent of the GNP in 1962 to 30.3 percent in 1987. Commodity trade volume rose from US$480 million in 1962 to a projected US$127.9 billion in 1990. The ratio of domestic savings to GNP grew from 3.3 percent in 1962 to 35.8 percent in 1989. In 1965 South Korea's rate of growth first exceeded North Korea's rate of growth in most industrial areas, though South Korea's per capita GNP was still lower.

The most significant factor in rapid industrialization was the adoption of an outward-looking strategy in the early 1960s. This strategy was particularly well-suited to that time because of South Korea's poor natural resource endowment, low savings rate, and tiny domestic market. The strategy promoted economic growth through labor-intensive manufactured exports, in which South Korea could develop a competitive advantage. Government initiatives played an important role in this process. Through the model of export-led industrialization, the South Korean government incentivized corporations to develop new technology and upgrade productive efficiency in order to compete in the highly-competitive, global market. By adhering to state regulations and demands, firms were awarded subsidization and investment support to rapidly develop their export markets in the fast-paced, evolving international arena. In addition, the inflow of foreign capital was greatly encouraged to supplement the shortage of domestic savings. These efforts enabled South Korea to achieve rapid growth in exports and subsequent increases in income.

By emphasizing the industrial sector, Seoul's export-oriented development strategy left the rural sector relatively underdeveloped. The steel and shipbuilding industries in particular played crucial roles in developing South Korea's economy during this time. Except for mining, most industries were located in the urban areas of the northwest and southeast. Heavy industries generally were located in the south of the country. Factories in Seoul contributed over 25 percent of all manufacturing value-added in 1978; taken together with factories in surrounding Gyeonggi Province, factories in the Seoul area produced 46 percent of all manufacturing that year. Factories in Seoul and Gyeonggi Province employed 48 percent of the nation's 2.1 million factory workers. Increasing income disparity between the industrial and agricultural sectors became a serious problem by the 1970s and remained a problem, despite government efforts to raise farm income and improve rural living standards.

In the early 1980s, in order to control inflation, a conservative monetary policy and tight fiscal measures were adopted. Growth of the money supply was reduced from the 30 percent level of the 1970s to 15 percent. Seoul even froze its budget for a short while. Government intervention in the economy was greatly reduced and policies on imports and foreign investment were liberalized to promote competition. To reduce the imbalance between rural and urban sectors, Seoul expanded investments in public projects, such as roads and communications facilities, while further promoting farm mechanization.

The measures implemented early in the decade, coupled with significant improvements in the world economy, helped the South Korean economy regain its lost momentum in the late 1980s. South Korea achieved an average of 9.2 percent real growth between 1982 and 1987 and 12.5 percent between 1986 and 1988. The double-digit inflation of the 1970s was brought under control. Wholesale price inflation averaged 2.1 percent per year from 1980 through 1988; consumer prices increased by an average of 4.7 percent annually. Seoul achieved its first significant surplus in its balance of payments in 1986 and recorded a US$7.7 billion and a US$11.4 billion surplus in 1987 and 1988 respectively. This development permitted South Korea to begin reducing its level of foreign debt. The trade surplus for 1989, however, was only US$4.6 billion, and a small negative balance was projected for 1990.

For the first half of the 1990s, the South Korean economy continued a stable and strong growth in both private consumption and GDP. Things changed quickly in 1997 with the Asian Financial crisis. After several other Asian currencies were attacked by speculators, the Korean won started to heavily depreciate in October 1997. The problem was exacerbated by the problem of non-performing loans at many of Korea's merchant banks. By December 1997, the IMF had approved a US$21 billion loan, that would be part of a US$58.4 billion bailout plan. By January 1998, the government had shut down a third of Korea's merchant banks. Throughout 1998, Korea's economy would continue to shrink quarterly at an average rate of -6.65%. South Korean chaebol Daewoo became a casualty of the crisis as it was dismantled by the government in 1999 due to debt problems. American company General Motors managed to purchase the motors division. Indian conglomerate Tata Group, purchased the trucks and heavy vehicles division of Daewoo.

Actions by the South Korean government and debt swaps by international lenders contained the country's financial problems. Much of South Korea's recovery from the Asian Financial Crisis can be attributed to labor adjustments (i.e. a dynamic and productive labor market with flexible wage rates) and alternative funding sources. By the first quarter of 1999, GDP growth had risen to 5.4%, and strong growth thereafter combined with deflationary pressure on the currency led to a yearly growth of 10.5%. In December 1999, president Kim Dae-jung declared the currency crisis over.

Korea's economy moved away from the centrally planned, government-directed investment model toward a more market-oriented one. These economic reforms, pushed by President Kim Dae-jung, helped Korea maintain one of Asia's few expanding economies, with growth rates of 10.8% in 1999 and 9.2% in 2000. Growth fell back to 3.3% in 2001 because of the slowing global economy, falling exports, and the perception that much-needed corporate and financial reforms have stalled.

After the bounce back from the crisis of the late nineties, the economy continued strong growth in 2000 with a GDP growth of 9.08%. However, the South Korean economy was affected by the September 11 Attacks. The slowing global economy, falling exports, and the perception that corporate and financial reforms had stalled caused growth to fall back to 3.8% in 2001 Thanks to industrialization GDP per hour worked (labor output) more than tripled from US$2.80 in 1963 to US$10.00 in 1989. More recently the economy stabilized and maintain a growth rate between 4-5% from 2003 onwards.

Led by industry and construction, growth in 2002 was 5.8%, despite anemic global growth. The restructuring of Korean conglomerates ("chaebols"), bank privatization, and the creation of a more liberalized economy—with a mechanism for bankrupt firms to exit the market—remain Korea's most important unfinished reform tasks. Growth slowed again in 2003, but production expanded 5% in 2006, due to popular demand for key export products such as HDTVs and mobile phones.

Like most industrialized economies, Korea suffered significant setbacks during the late-2000s recession that began in 2007. Growth fell by 3.4% in the fourth quarter of 2008 from the previous quarter, the first negative quarterly growth in 10 years, with year on year quarterly growth continuing to be negative into 2009. Most sectors of the economy reported declines, with manufacturing dropping 25.6% as of January 2009, and consumer goods sales dropping 3.1%. Exports in autos and semiconductors, two critical pillars of the economy, shrank 55.9% and 46.9% respectively, while exports overall fell by a record 33.8% in January, and 18.3% in February 2009 year on year. As in the 1997 crisis, Korea's currency also experienced massive fluctuations, declining by 34% against the dollar. Annual growth in the economy slowed to 2.3% in 2008, and was expected to drop to as low as -4.5% by Goldman Sachs, but South Korea was able to limit the downturn to a near standstill at 0.2% in 2009.

Despite the global financial crisis, the South Korean economy, helped by timely stimulus measures and strong domestic consumption of products that compensated for a drop in exports, was able to avoid a recession unlike most industrialized economies, posting positive economic growth for two consecutive years of the crisis. In 2010, South Korea made a strong economic rebound with a growth rate of 6.1%, signaling a return of the economy to pre-crisis levels. South Korea's export has recorded $424 billion in the first eleven months of the year 2010, already higher than its export in the whole year of 2008. The South Korean economy of the 21st century, as a Next Eleven economy, is expected to grow from 3.9% to 4.2% annually between 2011 and 2030, similar to growth rates of developing countries such as Brazil or Russia.

The South Korean government signed the Korea-Australia Free Trade Agreement (KAFTA) on December 5, 2013, with the Australian government seeking to benefit its numerous industries—including automotive, services, and resources and energy—and position itself alongside competitors, such as the US and ASEAN. South Korea is Australia's third largest export market and fourth largest trading partner with a 2012 trade value of A$32 billion. The agreement contains an Investor State Dispute Settlement (ISDS) clause that permits legal action from South Korean corporations against the Australian government if their trade rights are infringed upon.

The government cut the work week from six days to five in phases, from 2004 to 2011, depending on the size of the firm. The number of public holidays was expanded to 16 by 2013.

South Korean economy fell in 2019’s first quarter, which was the worst performance since the global financial crisis. GDP declined a seasonally adjusted 0.3 percent from the previous quarter.

In 1990, South Korean manufacturers planned a significant shift in future production plans toward high-technology industries. In June 1989, panels of government officials, scholars, and business leaders held planning sessions on the production of such goods as new materials, mechatronics—including industrial robotics—bioengineering, microelectronics, fine chemistry, and aerospace. This shift in emphasis, however, did not mean an immediate decline in heavy industries such as automobile and ship production, which had dominated the economy in the 1980s.

South Korea relies largely upon exports to fuel the growth of its economy, with finished products such as electronics, textiles, ships, automobiles, and steel being some of its most important exports. Although the import market has liberalized in recent years, the agricultural market has remained largely protectionist due to serious disparities in the price of domestic agricultural products such as rice with the international market. As of 2005, the price of rice in South Korea is about four times that of the average price of rice on the international market, and it was generally feared that opening the agricultural market would have disastrous effects upon the South Korean agricultural sector. In late 2004, however, an agreement was reached with the WTO in which South Korean rice imports will gradually increase from 4% to 8% of consumption by 2014. In addition, up to 30% of imported rice will be made available directly to consumers by 2010, where previously imported rice was only used for processed foods. Following 2014, the South Korean rice market will be fully opened.

Additionally, South Korea today is known as a Launchpad of a mature mobile market, where developers can reap benefits of a market where very few technology constraints exist. There is a growing trend of inventions of new types of media or apps, utilizing the 4G and 5G internet infrastructure in South Korea. South Korea has today the infrastructures to meet a density of population and culture that has the capability to create strong local particularity.

The following table shows the main economic indicators in 1980–2018. Inflation under 2% is in green.

During the 1970s and 1980s, South Korea became a leading producer of ships, including oil supertankers, and oil-drilling platforms. The country's major shipbuilder was Hyundai, which built a 1-million-ton capacity drydock at Ulsan in the mid-1970s. Daewoo joined the shipbuilding industry in 1980 and finished a 1.2-million-ton facility at Okpo on Geoje Island, south of Busan, in mid-1981. The industry declined in the mid-1980s because of the oil glut and because of a worldwide recession. There was a sharp decrease in new orders in the late 1980s; new orders for 1988 totaled 3 million gross tons valued at US$1.9 billion, decreases from the previous year of 17.8 percent and 4.4 percent, respectively. These declines were caused by labor unrest, Seoul's unwillingness to provide financial assistance, and Tokyo's new low-interest export financing in support of Japanese shipbuilders. However, the South Korean shipping industry was expected to expand in the early 1990s because older ships in world fleets needed replacing. South Korea eventually became the world's dominant shipbuilder with a 50.6% share of the global shipbuilding market as of 2008. Notable Korean shipbuilders are Hyundai Heavy Industries, Samsung Heavy Industries, Daewoo Shipbuilding & Marine Engineering, and the now bankrupt STX Offshore & Shipbuilding.

Electronics is one of South Korea's main industries. During the 1980s through the 2000s, South Korean companies such as Samsung, LG and SK have led South Korea's growth in Electronics. In 2017, 17.1% of South Korea's exports were semiconductors produced by Samsung Electronics and SK Hynix. Samsung and LG are also major producers in electronic devices such as Televisions, Smartphones, Display, and computers.

The automobile industry was one of South Korea's major growth and export industries in the 1980s. By the late 1980s, the capacity of the South Korean motor industry had increased more than fivefold since 1984; it exceeded 1 million units in 1988. Total investment in car and car-component manufacturing was over US$3 billion in 1989. Total production (including buses and trucks) for 1988 totaled 1.1 million units, a 10.6 percent increase over 1987, and grew to an estimated 1.3 million vehicles (predominantly passenger cars) in 1989. Almost 263,000 passenger cars were produced in 1985—a figure that grew to approximately 846,000 units in 1989. In 1988 automobile exports totaled 576,134 units, of which 480,119 units (83.3 percent) were sent to the United States. Throughout most of the late 1980s, much of the growth of South Korea's automobile industry was the result of a surge in exports; 1989 exports, however, declined 28.5 percent from 1988. This decline reflected sluggish car sales to the United States, especially at the less expensive end of the market, and labor strife at home. South Korea today has developed into one of the world's largest automobile producers. The Hyundai Kia Automotive Group is South Korea's largest automaker in terms of revenue, production units and worldwide presence.

Most of the mineral deposits in the Korean Peninsula are located in North Korea, with the South only possessing an abundance of tungsten and graphite. Coal, iron ore, and molybdenum are found in South Korea, but not in large quantities and mining operations are on a small scale. Much of South Korea's minerals and ore are imported from other countries. Most South Korean coal is low-grade anthracite that is only used for heating homes and boilers.

Construction has been an important South Korean export industry since the early 1960s and remains a critical source of foreign currency and "invisible" export earnings. By 1981 overseas construction projects, most of them in the Middle East, accounted for 60 percent of the work undertaken by South Korean construction companies. Contracts that year were valued at US$13.7 billion. In 1988, however, overseas construction contracts totaled only US$2.6 billion (orders from the Middle East were US$1.2 billion), a 1 percent increase over the previous year, while new orders for domestic construction projects totaled US$13.8 billion, an 8.8 percent increase over 1987. South Korean construction companies therefore concentrated on the rapidly growing domestic market in the late 1980s. By 1989 there were signs of a revival of the overseas construction market: the Dong Ah Construction Company signed a US$5.3 billion contract with Libya to build the second phase (and other subsequent phases) of Libya's Great Man-Made River Project, with a projected cost of US$27 billion when all 5 phases were completed. South Korean construction companies signed over US$7 billion of overseas contracts in 1989. Korea's largest construction companies include Samsung C&T Corporation, which built some of the highest building's and most noteworthy skyscrapers such as three consecutively world's tallest buildings: Petronas Towers, Taipei 101, and Burj Khalifa.

During the 1960s, South Korea was largely dependent on the United States to supply its armed forces, but after the elaboration of President Richard M. Nixon's policy of Vietnamization in the early 1970s, South Korea began to manufacture many of its own weapons.

Since the 1980s, South Korea, now in possession of more modern military technology than in previous generations, has actively begun shifting its defense industry's areas of interest more from its previously homeland defense-oriented militarization efforts, to the promotion of military equipment and technology as mainstream products of exportation to boost its international trade. Some of its key military export projects include the T-155 Firtina self-propelled artillery for Turkey; the K11 air-burst rifle for United Arab Emirates; the Bangabandhu class guided-missile frigate for Bangladesh; fleet tankers such as Sirius class for the navies of Australia, New Zealand, and Venezuela; Makassar class amphibious assault ships for Indonesia; and the KT-1 trainer aircraft for Turkey, Indonesia and Peru.

South Korea has also outsourced its defense industry to produce various core components of other countries' advanced military hardware. Those hardware include modern aircraft such as F-15K fighters and AH-64 attack helicopters which will be used by Singapore, whose airframes will be built by Korea Aerospace Industries in a joint-production deal with Boeing. In other major outsourcing and joint-production deals, South Korea has jointly produced the S-300 air defense system of Russia via Samsung Group, and will facilitate the sales of Mistral class amphibious assault ships to Russia that will be produced by STX Corporation. South Korea's defense exports were $1.03 billion in 2008 and $1.17 billion in 2009.

In 2012, 11.1 million foreign tourists visited South Korea, making it the 20th most visited country in the world, up from 8.5 million in 2010. Recently, the number of tourists, especially from mainland China, Taiwan, Hong Kong, and Southeast Asia, has grown dramatically due to the increased popularity of the Korean Wave ("Hallyu").

Seoul is the principal tourist destination for visitors; popular tourist destinations outside of Seoul include Seorak-san national park, the historic city of Gyeongju and semi-tropical Jeju Island.
In 2014 South Korea hosted the League of Legends season 4 championship and then, in 2018, the season 8 championship.

Since 1991 there has been a steady upwards trend in South Korean M&A until 2018 with only a short break around 2004. Since 1991 around 18,300 deals in, into or out of South Korea have been announced, which sum up to a total value of over 941. bil. USD. The year 2016 has been the year with the largest deal value (1,818 in bil. USD) and the most number of deals (82,3).

Target industries are distributed very evenly with no industry taking a larger share than 10%. The top three target industries are Electronics (9.7%), Semiconductors (9.1%) and Metals and Mining (7.7%). However, over 51% of the acquiring companies originate from the financial and brokerage sector.


 


</doc>
<doc id="27025" url="https://en.wikipedia.org/wiki?curid=27025" title="Telecommunications in South Korea">
Telecommunications in South Korea

In South Korea, Telecommunications services improved dramatically in the 1980s with the assistance of foreign partners and as a result of the development of the electronics industry. The number of telephones in use in 1987 reached 9.2 million, a considerable increase from 1980, when there were 2.8 million subscribers (which, in turn, was four times the number of subscribers in 1972).

Radio, and in more recent years television, reached virtually every resident. By 1945 there were about 60,000 radio sets in the country. By 1987 there were approximately 42 million radio receivers in use, and more than 100 radio stations were broadcasting. Transistor radios and television sets have made their way to the most remote rural areas. Television sets, now mass-produced in South Korea, became far less expensive; most city people and a significant number of rural families owned or had access to a television. Ownership of television sets grew from 25,000 sets when broadcasting was initiated in 1961 to an estimated 8.6 million sets in 1987, and more than 250 television stations were broadcasting.


There are three mobile phone service providers: SK Telecom, KT and LG Uplus.



South Korea has six national terrestrial television networks from four broadcaster; KBS 1TV, KBS 2TV, MBC TV, SBS TV, EBS 1TV, and EBS 2TV. All terrestrial channels are digital (ATSC) since January 2013.

From November 2011, four generalist channel are available on cable television; JTBC, Channel A, TV Chosun, and Maeil Broadcasting Network.

(Total population: 50 million (July 2012 est.)


Today, South Korea has the highest number of broadband users. The rapid growth of the Korean broadband market was the result of a combination of government pushes and market factors. The government was active in promoting privatization and deregulation in general, and the information technology (IT) sector was no exception.

The government implemented structural reforms in July 1990. Since the mid-1990s, the Ministry of Information and Communications (MIC) has pursued a policy of high-speed telecommunication infrastructure as a foundation to build a “knowledge-based society.” In the telecommunications sector, competition was allowed on an incremental basis and, in the market for value added services, full competition was allowed. In March 1995, Korea Information Infrastructure (KII) was established. KII's goal was to advance the nation's IT infrastructure. In August 1995, the Framework Act on Information Promotion was enacted.

The country then experienced economic crisis in 1997 with the rest of the region. During the economic reforms being implemented after the financial crisis, the information technology (IT) sector was one of several that was targeted and considered to be an important factor in the recovery of the nation's economy. In 1999, the government implemented the program known as Cyber Korea 21, which was intended to accelerate IT development.

In 1999, the government provided US$77 million in loans with preferential rates to facilities service providers (FSP). In 2000, another US$77 million was provided in loans for suburban areas, small cities and towns, and regional industrial areas. Another US$926 million was provided until 2005 in order to supply the rural areas with broadband.

Commensurate with its investment funding, the government implemented various policies designed to increase internet use among the general population. The government provided “internet literacy” lessons to homemakers, the elderly, military personnel, and farmers. In June 2000, the government implemented what was known as the “Ten Million People Internet Education” project, the purpose of which was to provide internet education to ten million people.

The number of broadband subscribers in Korea reached 10 million in October 2002, with about 70% out of 14.3 million homes connected at the speed of over 2 Mbit/s.

In 2002, there were six operators providing broadband services in Korea. The market share leader was Korea Telecom (KT), with approximately 45.8% market share (4.5 million subscribers), followed by Hanaro Telecom with approximately 28.6% of the market and Thrunet with approximately 13.1%. of the market. In terms of technology, KT primarily uses Digital Subscriber Line (DSL). Hanaro uses a mix of cable and DSL. Thrunet service is mainly provided through cable modem.

At end of June 2011, subscribers of Voice over Internet Protocol (VoIP) service achieve 10.1 million or around 20 percent of South Korea's population.

This article relied on information from:

Yun, Kyounglim, Heejin Lee and So-Hye Lim, The Growth of Broadband Internet Connections in South Korea: Contributing Factors, Asia/Pacific Research Center, Stanford University (September 2002).

Choudrie, Jyoti and Heejin Lee, Broadband Development in South Korea: Institutional and Cultural Factors, European Journal of Information Systems v. 13, pp. 103–14 (2004).




</doc>
<doc id="27026" url="https://en.wikipedia.org/wiki?curid=27026" title="Transport in South Korea">
Transport in South Korea

Transportation in South Korea is provided by extensive networks of railways, highways, bus routes, ferry services and air routes that criss-cross the country. South Korea is the third country in the world to operate a commercial maglev train.

Development of modern infrastructure began with the first Five-Year Development Plan (1962–66), which included the construction of 275 kilometers of railways and several small highway projects. Construction of the Gyeongbu Expressway, which connects the two major cities of Seoul and Busan, was completed on 7 July 1970.

The 1970s saw increased commitment to infrastructure investments. The third Five-Year Development Plan (1972–76) added the development of airports, seaports. The Subway system was built in Seoul, the highway network was expanded by 487 km and major port projects were started in Pohang, Ulsan, Masan, Incheon and Busan.

The railroad network experienced improvements in the 1980s with electrification and additional track projects. Operation speed was also increased on the main lines. Though the railroad was still more useful for transportation of freight, passenger traffic was also growing. There was 51,000 kilometers of roadways by 1988. Expressway network was expanded to connect more major cities and reached a combined length of 1,539 kilometers before the end of the decade.

The largest railway operator is Korail. Railway network is managed by Korea Rail Network Authority.

Korea Train Express began service in April 2004, as Korea's first high-speed service. Intercity services are provided by ITX-Saemaeul and Mugunghwa-ho. ITX-Saemaeul generally stops less than Mugunghwa-ho. They stop in all stations and seat reservation is not available. On routes where KTX operates, air travel significantly declined with less passengers choosing to fly and airlines offering fewer flights.

Nuriro Train service runs between Seoul-Sinchang route and other lines. Nuriro Train serves commuters around Seoul Metropolitan Area, providing shorter travel time than Seoul Subway. The rapid trains have same cost and seat reservation as Mugunghwa-ho. Korail plans to expand the service area.

South Korea's six largest cities — Seoul, Busan, Daegu, Gwangju, Daejeon and Incheon — all have subway systems.

Seoul's subway system is the oldest system in the country, with the Seoul Station – Cheongnyangni section of Line 1 opening in 1974.

The first tram line in Seoul started operation between Seodaemun and Cheongnyangni in December 1898. The network was expanded to cover the whole downtown area (Jung-gu and Jongno-gu districts) as well as surrounding neighbourhoods, including Cheongnyangni in the east, Mapo-gu in the west, and Noryangjin across the Han River to the south.

The networks reached its peak in 1941, but was abandoned in favor of cars and the development of a subway system in 1968. Seoul Subway Line 1 and Line 2 follow the old streetcar routes along Jongno and Euljiro, respectively.

Virtually all towns in South Korea of all sizes are served by regional bus service. Regional routes are classified as "gosok bus" (고속버스, "high speed" express bus) or "sioe bus" (시외버스, "suburban" intercity bus) with gosok buses operating over the longer distances and making the fewest (if any) stops en route. Shioe buses typically operate over shorter distances, are somewhat slower, and make more stops. It is possible to reach an other city by intercity buses. From Seoul, the place is Express Bus Terminal, the subway sation is served by Seoul Subway Lines 3, 7 and 9.

Within cities and towns, two types of city bus operate in general: "jwaseok" (좌석, "coach") and "dosihyeong" (도시형, "city type") or "ipseok" (입석, "standing"). Both types of bus often serve the same routes, make the same (or fewer) stops and operate on similar frequencies, but jwaseok buses are more expensive and offer comfortable seating, while doshihyeong buses are cheaper and have fewer and less comfortable seats. Many small cities and towns do not have jwaseok buses and their buses are officially called "nongeochon" (농어촌, "rural area" bus). The local buses in Seoul and other cities work by colours: the blue buses cross the entire city, the green ones mean that some of their stops are close to a subway station, and the red buses go out of the city.

Some cities have their own bus classifying systems.

Incheon International Airport is served by an extensive network of high-speed buses from all parts of the country.

Beginning in the late 1990s, many department stores operated their own small networks of free buses for shoppers, but government regulation, confirmed by a court decision on June 28, 2001, have banned department stores from operating buses. However, most churches, daycare centres and private schools send buses around to pick up their congregants, patients or pupils.

Highways in South Korea are classified as freeways (expressways/motorways), national roads and various classifications below the national level. Almost all freeways are toll highways and most of the expressways are built, maintained and operated by Korea Expressway Corporation (KEC).

The freeway network serves most parts of South Korea. Tolls are collected using an electronic toll collection system. KEC also operates service amenities (dining and service facilities) en route.

There are also several privately financed toll roads. Nonsan-Cheonan Expressway, Daegu-Busan Expressway, Incheon International Airport Expressway, Seoul-Chuncheon Expressway and parts of the Seoul Ring Expressway are wholly privately funded and operated BOT concessions. Donghae Expressway was built in cooperation between KEC and the National Pension Service.

Total length of the South Korean road network was 86,989 km in 1998. Of this, 1,996 km was expressways and 12,447 km national roads. By 2009, combined length of the expressways had reached approximately 3,000 km, it mostly equal to the whole area of South Korea

Virtually cut off from the Asian mainland, South Korea is a seafaring nation, with one of the world's largest shipbuilding industries and an extensive system of ferry services. South Korea operates one of the largest merchant fleets serving China, Japan and the Middle East. Most fleet operators are large conglomerates, while most ferry operators are small, private operators.

There are 1,609 km of navigable waterways in South Korea, though use is restricted to small craft.

The southern and westerns coasts of the country are dotted with small islands which are served by ferries. In addition, the larger offshore Jeju and Ulleung Islands are also served by ferry. Major centres for ferry service include Incheon, Mokpo, Pohang and Busan, as well as China and Japan.

The cities have major ports Jinhae, Incheon, Gunsan, Masan, Mokpo, Pohang, Busan ( Busan Port), Donghae, Ulsan, Yeosu, Jeju.

In 1999, there was a total of 461 merchant ships (1,000 GRT or over) totalling 5,093,620 GRT/. These are divisible by type as follows:

Korean Air was founded by the government in 1962 to replace Korean National Airlines and has been privately owned since 1969. It was South Korea's sole airline until 1988. In 2008, Korean Air served 2,164 million passengers, including 1,249 million international passengers.

A second carrier, Asiana Airlines, was established in 1988 and originally served Seoul, Jeju and Busan domestically and Bangkok, Singapore, Japan and Los Angeles internationally. By 2006, Asiana served 12 domestic cities, 66 cities in 20 foreign countries for commercial traffic and 24 cities in 17 countries for cargo traffic.

Combined, South Korean airlines currently serve 297 international routes. Smaller airliners, such as Air Busan, Jin Air, Eastar Jet and Jeju Air, provide domestic service and Japan/Southeast Asian route with lower fares.

South Korea contains the busiest passenger air corridor as measured by passengers per year. Over ten million people traveled between Seoul Gimpo Airport and Jeju in 2015 alone. As competition is fierce and prices affordable, the trend has been increasingly towards more air travel on this route. Similarly, air travel is also growing between Jeju and other mainland airports.

Along other routes, air travel competes with the KTX high speed rail service and has declined in the 2000s and 2010s

Construction of South Korea's largest airport, Incheon International Airport, was completed in 2001, in time for the 2002 FIFA World Cup. By 2007, the airport was serving 30 million passengers a year. The airport has been selected as the "Best Airport Worldwide" for four consecutive years since 2005 by Airports Council International.

Seoul is also served by Gimpo International Airport (formerly Kimpo International Airport). International routes mainly serve Incheon, while domestic services mainly use Gimpo. Other major airports are in Busan and Jeju.

There are 103 airports in South Korea (1999 est.) and these may be classified as follows.

Airports with paved runways:
"total:"
67
"over 3,047 m:"
1
"2,438 to 3,047 m:"
18
"1,524 to 2,437 m:"
15
"914 to 1,523 m:"
13
"under 914 m:"
20 (1999 est.)

Airports with unpaved runways:
"total:"
36
"over 3,047 m:"
1
"914 to 1,523 m:"
3
"under 914 m:"
32 (1999 est.)

Heliports:
203 (1999 est.)

These pipelines are for petroleum products.
Additionally, there is a parallel petroleum, oils and lubricants (POL) pipeline being completed




</doc>
<doc id="27027" url="https://en.wikipedia.org/wiki?curid=27027" title="Republic of Korea Armed Forces">
Republic of Korea Armed Forces

The Republic of Korea Armed Forces (), also known as the ROK Armed Forces, are the armed forces of South Korea. The ROK Armed Forces is one of the largest standing armed forces in the world with a reported personnel strength of 3,699,000 in 2018 (599,000 active and 3,100,000 reserve). South Korea has one of the highest defense budgets in the world, ranking 10th globally in 2019, with a budget of more than $43 billion U.S. dollars. The South Korean military is ranked as the 7th most powerful military force in the world as of 2019.

The Republic of Korea Armed Forces were founded in 1948, following the establishment of the South Korean government after Korea's liberation from the Empire of Japan. South Korea's military forces are responsible for maintaining the sovereignty and territorial integrity of the state, and also engage in peacekeeping operations and humanitarian, disaster-relief efforts worldwide.

The origin of the Republic of Korea Armed Forces can be traced back to the Korean Independence Army, which was established by the Provisional Government of Korea in exile in Chongking, Republic of China in 1940 during the Japanese rule of Korea. Many of its members became part of the South Korean armed forces later. In addition, some ethnic Korean Kuomintang and Manchukuo soldiers also contributed to the forces.

After Korea was liberated from the Empire of Japan on August 15, 1945, the Korean Constabulary and the Korean Coast Guard (organized by Sohn Won-yil and others) were established through the United States Army Military Government in Korea. The Korean Constabulary and the Korean Coast Guard became the Republic of Korea Army and Republic of Korea Navy respectively, and formed the Republic of Korea Armed Forces after the South Korean government was established on August 15, 1948. The Republic of Korea Air Force was founded in October 1949.

The South Korean armed forces remained largely constabulary forces until the outbreak of the Korean War on June 25, 1950, requiring the United Nations to intervene with United States-led forces. The South Korean military rapidly developed during the Korean War, despite suffering enormous casualties. As the Soviets had armed North Korea, the U.S. armed and trained the South Korean military throughout the Korean War. After the Korean War, South Korea established a joint military partnership with the United States, termed the ROK-U.S. Alliance, as outlined by the Mutual Defense Treaty. During the Vietnam War, the ROK Army and ROK Marines were among those fighting alongside South Vietnam and the United States.

In the 1970s, through the Park Chung-hee Administration's ""Yulgok" Plan", South Korea began to build up self-reliant, national defense capability. During South Korea's period of rapid growth in the 1980s, the military modernized, benefiting from several government-sponsored technology transfer projects and indigenous defense capability initiatives. In the 1990s, "South Korean industries provided about 70 percent of the weapons, ammunition, communications and other types of equipment, vehicles, clothing, and other supplies needed by the military."

Today, the South Korean armed forces enjoy a good mix of avant-garde as well as older conventional weapons. Its capabilities include many sophisticated U.S. and European weapon systems, complemented by a growing and increasingly more advanced indigenous defense manufacturing sector. For example, by taking advantage of the strong local shipbuilding industry, the ROK Navy has embarked on a rigorous modernization plan with ambitions to become a blue-water navy in the 2020s.

The ROK military forces are undergoing rapid modernization in preparation for assuming wartime operational control of the ROK's defenses. Several cutting-edge military systems are currently being inducted.

Based on the Moon Jae-in Administration's "Defense Reform 2.0" and in line with the overall troop drawdown scheme, the number of generals and admirals will be reduced by 17 percent from the current 436 to 360 by 2022 to reduce the bloated top command apparatus. This means the removal of 66 general-level positions for the Army and five each for the Navy and Air Force. At the same time, the ROK Armed Forces will see a reduction in active duty personnel from 640,000 to 517,000, and the length of compulsory military service will also be reduced to 18 – 22 months by 2022.

Command over the ROK Armed Forces is established in the Constitution. The President is the Commander-in-Chief Forces ex officio. The military authority runs from the President to the Minister of National Defense, who is often to be (but not legally bound to be) a retired 4-star general. The President and Minister of National Defense are in charge of the entire military establishment, maintaining civilian control of the military. The Minister of National Defense, by order of the President, takes charge of military affairs, and supervises the Chairman of the Joint Chiefs of Staff and the chief of staff of each service of the Armed Forces.

To coordinate military strategy with political affairs, the President has a National Security Council headed by the National Security Advisor.

The Joint Chiefs of Staff consists of the Chairman of the Joint Chiefs of Staff, and the military service chiefs from the Army, Navy, and Air Force. Unlike the U.S. counterpart, operational command of combat units falls within the purview of the Chairman of the Joint Chiefs of Staff who reports to the Minister of National Defense.

The Chairman of the Joint Chiefs of Staff, a 4-star General or Admiral, is the senior officer of the Armed Forces. The Chairman of the Joint Chiefs of Staff assists the Minister of National Defense with regard to operational command authority, and supervises the combat units of each service of the Armed Forces, by order of the Minister of National Defense. The chain of operational control runs straight from the Chairman of the Joint Chiefs of Staff to the commandants of the Army, Navy, and Air Force operational commands. The respective chiefs of staff of each service branch (Army, Navy, and Air Force) has administrative control over his or her own service.

The ROK Armed Forces consists of the ROK Army (), ROK Navy (대한민국 해군), and ROK Air Force (대한민국 공군). The ROK Marine Corps (대한민국 해병대) functions as a branch of the Navy. The ROK Reserve Forces (대한민국 예비군) is a reserve component.

The ROK Army (ROKA) is by far the largest of the military branches, with 495,000 personnel as of 2014. This comes as a response to both the mountainous terrain native to the Korean Peninsula (70% mountainous) as well as the heavy North Korean presence, with its 1-million-strong army, two-thirds of which is permanently garrisoned in the frontline near the DMZ. The current administration has initiated a program of self-defense, whereby South Korea would be able to fully counter the North Korean threat with purely domestic means within the next two decades.

The ROK Army was formerly organized into three armies: the First Army (FROKA), Third Army (TROKA) and Second Operational Command each with its own headquarters, corps (not Second Operational Command), and divisions. The Third Army was responsible for the defense of the capital as well as the western section of the DMZ. The First Army was responsible for the defense of the eastern section of the DMZ whereas the Second Operational Command formed the rearguard.

Under a restructuring plan aimed at reducing redundancy, the First and Third Armies will be incorporated into the newly formed First Operations Command, whereas the Second ROK Army has been converted into the Second Operational Command. The army consists of the Army Headquarters, the Aviation Command, and the Special Warfare Command, with 7 corps, 39 divisions, some 520,000 troops and estimated as many as 5,850 tanks and armored vehicles, 11,337 artillery systems, 7,032 missile defense systems and 13,000 infantry support systems.

The army will take the brunt of the personnel reduction part of the Defense Reform 307. Associated with this personnel reduction would be a significant reduction in the ROK Army force structure, in particular decreasing the current force of 47 divisions (active duty and reserve) down to a force of about 28 divisions.

The ROK Navy (ROKN) is responsible for naval and amphibious operations. The ROK Navy has about 70,000 regular personnel including 29,000 Republic of Korea Marines. There are about 150 commissioned ships with the ROK Navy (a total displacement of about 215,000 tonnes). The naval aviation force consists of about 70 fixed-wing and rotary-wing aircraft.

The Republic of Korea Navy includes the Republic of Korea Navy Headquarters, Republic of Korea Fleet, and Republic of Korea Marine Corps. The Chief of Naval Operations (CNO) is the highest-ranking officer of the ROK Navy, and oversees the administration of organizing, recruiting, training, equipping, supplying, and mobilizing the ROK Navy. The Republic of Korea Fleet is the highest operational command of the ROK Navy.

Since the 1990s, the ROK Navy has been trying to build an ocean-going fleet to protect the sea lines of communication. During Admiral An Pyong-tae's tenure as CNO, President Kim Young-sam supported the Navy by approving a long-term shipbuilding plan for the ocean-going navy. In the first decade of the 21st century, the ROK Navy launched the lead ships of larger and better equipped warships with local shipbuilders: In 2002, ROKS "Chungmugong Yi Sunshin" (DDH 975), a 4,500-ton destroyer, was launched; in 2005, the 14,000-ton amphibious warfare ship, ROKS "Dokdo" (LPH 6111) was launched; in 2006, the ROK Navy launched ROKS "Sohn Wonyil" (SS 072), an 1,800-ton Type 214 submarine with Air-Independent propulsion (AIP) system. In 2007, the ROK Navy launched the lead ship (DDG 991) of "Sejong the Great" class destroyers with the Aegis combat system.

The ROK Navy completed a new naval base called Jeju Civilian-Military Complex Port in 2016 on the southern coast of Jeju Island to protect the sea lines of communication. In order to support ocean-going operations, the ROK Navy commissioned the 10,000-ton logistics support ship, ROKS "Soyang" (AOE 51), and launched the first locally designed 3,000-ton submarine, "Dosan Ahn Changho" (SS 083) in 2018. The ROK Navy continues to upgrade ongoing shipbuilding programs such as the Korean Submarine (KSS), Korean Destroyer Experimental (KDX), Frigate Experimental (FFX), and Landing Transport Experimental (LPX).

The ROK Navy aims to become a blue-water navy in the 2020s.

The ROK Marine Corps (ROKMC) is a branch of the Republic of Korea Navy responsible for amphibious operations, and also functions as a rapid reaction force and a strategic reserve. The ROK Marine Corps, with 29,000 personnel, is organized into two divisions and two brigades under the Headquarters ROK Marine Corps. The ROK Marine Corps has about 300 tracked vehicles including assault amphibious vehicles, main battle tanks, and self-propelled artillery.

The Commandant of the Republic of Korea Marine Corps is a three-star general. After the bombardment of Yeonpyeong in 2010, the Commandant of the ROKMC also holds the commander position of the Northwest Islands Defense Command (NWIDC).

The ROK Air Force (ROKAF) maintains a modern air force in order to defend itself from various modes of threats, including the North Korean Army. The ROK Air Force fields some 450 combat aircraft of American design. In contrast, the North Korean Army has roughly 650 combat aircraft, but mostly obsolete types of Soviet and Chinese origin.

Korea began a program for the development of indigenous jet trainers beginning in 1997. This project eventually culminated in the KAI T-50, dubbed the "Golden Eagle" which is used as a trainer for jet pilots, now being exported to Indonesia. A multirole all-weather version of the T-50 is the modified FA-50, which can be externally fitted with Rafael's Sky Shield or LIG Nex1's ALQ-200K ECM pods, Sniper or LITENING targeting pods, and Condor 2 reconnaissance pods to further improve the fighter's electronic warfare, reconnaissance, and targeting capabilities. Other improved weapon systems over FA-50 include SPICE multifunctional guidance kits, Textron CBU-97/105 Sensor Fuzed Weapon with WCMD tail kits, JDAM, and JDAM-ER for more comprehensive air-to-ground operations, and AIM-120 missiles for BVR air-to-air operations. FA-50 has provisions for, but does not yet integrate, Python and Derby missiles, also produced by Rafael, and other anti-ship missiles, stand-off weapons, and sensors to be domestically developed by Korea.

The Republic of Korea Air Force also expressed interests in acquiring the RQ-4 Global Hawk and Joint Direct Attack Munition kits to further improve their intelligence and offensive capabilities.

The replacement programs for the F-4D/E and F-5A/B/E/F are the KTX-2 and F-X, respectively. The latter has been fulfilled by the Boeing F-15K.

The South Korean government also announced its plan to develop indigenous helicopter manufacturing capacities to replace the aging UH-1 helicopters, many of which had seen service during the Vietnam War. The program originally included plans for the development of both a civilian and a military helicopter. This was later revised and gave priority to the utility helicopter program. Based on the success and experience of the civilian KMH (Korean Multi-purpose Helicopter) the attack helicopter, which would share a common configuration, will be developed.

Conscription in South Korea requires male citizens over the age of 16 to perform compulsory military service. Women are not required to perform military service, but they may volunteer as officers, warrant officers, or non-commissioned officers.

The length of compulsory military service varies based on service branches: Active duty enlisted personnel serve 21 months in the Army or Marine Corps, 23 months in the Navy, and 24 months in the Air Force (the length of military service will be reduced to 18 – 22 months by 2022.). Commissioned officers, warrant officers, and non-commissioned officers are volunteer-based, and serve longer terms than those of enlisted personnel, or as career. Non-active duty personnel such as social work personnel serve for various lengths. After conscripts finish their military service, they are automatically placed on the reserve roster.

In the South Korean armed forces, ranks fall into one of four categories: commissioned officer, warrant officer, non-commissioned officer, and junior enlisted (""Byeong""), in decreasing order of authority. Commissioned officer ranks are subdivided into ""Jangseong""-level (general) officers, ""Yeonggwan""-level (field-grade) officers, and ""Wigwan""-level (company-grade) officers. All three branches of the South Korean Armed Forces share the same rank insignia and titles in Korean (The English titles are given as comparative examples with the US Army ranks.).

ROK Navy commissioned officer ranks have two distinct sets of rank insignia: On dress uniform a series of stripes similar to Commonwealth naval ranks are worn; on service uniforms, working uniforms, and special uniform situations (combat utilities and flight suits), the rank insignia are the same as the equivalent rank in the Army or the Air Force.

<br> 
South Korea has one of the highest defense budgets in the world, ranking 10th globally in 2019, with a budget of more than $43 billion U.S. dollars.

As part of its mission, the ROK Armed Forces have engaged in peacekeeping operations and humanitarian, disaster-relief efforts worldwide. In 2008, officers and soldiers of Unit Dongmyeong, stationed in Lebanon with the UNIFIL, received honorary medals from the United Nations.




</doc>
<doc id="27028" url="https://en.wikipedia.org/wiki?curid=27028" title="Foreign relations of South Korea">
Foreign relations of South Korea

South Korea maintains diplomatic relations with 191 countries. The country has also been a member of the United Nations since 1991, when it became a member state at the same time as North Korea. South Korea has also hosted major international events such as the 1988 Summer Olympics and 2002 World Cup Soccer Tournament (2002 FIFA World Cup co-hosted with Japan) and the 2011 IAAF World Championships Daegu South Korea. Furthermore, South Korea had hosted the 2018 Winter Olympics which took place in Pyeongchang, South Korea from 9 to 25 February.

South Korea is a member of the United Nations, WTO, OECD/DAC, ASEAN Plus Three, East Asia Summit (EAS), and G-20. It is also a founding member of Asia-Pacific Economic Cooperation (APEC) and the East Asia Summit.

On January 1, 2007, South Korean Foreign Minister Ban Ki-moon assumed the post of UN Secretary-General, serving in that post until December 31, 2016.

Inter-Korean relations may be divided into five periods. The first stage was between 1972 and 1973; the second stage was Pyongyang North Korea's delivery of relief goods to South Korea after a typhoon caused devastating floods in 1984 and the third stage was the exchange of home visits and performing artists in 1985. The fourth stage, activated by Nordpolitik under Roh, was represented by expanding public and private contacts between the two Koreas. The fifth stage was improved following the 1997 election of Kim Dae-jung. His "Sunshine Policy" of engagement with North Korea set the stage for the historic June 2000 Inter-Korean summit.

The possibility of Korean reunification has remained a prominent topic. However, no peace treaty has yet been signed with the North. In June 2000, a historic first North Korea-South Korea summit took place, part of the South Korea's continuing Sunshine Policy of engagement. Since then, regular contacts have led to a cautious thaw. President Kim was awarded the Nobel Peace Prize in 2000 for the policy.

With that policy, continued by the following administration of president Roh Moo-hyun, economic ties between the two countries have increased, humanitarian aid has been sent to North Korea and some divided families have been briefly reunited. Military ties remain fraught with tension, however, and in 2002 a brief naval skirmish left four South Korean sailors dead, leaving the future of the Sunshine policy uncertain. The North Korea cut off talks but the South remained committed to the policy of reconciliation and relations began to thaw again. The resurgence of the nuclear issue two years later would again cast relations in doubt, but South Korea has sought to play the role of intermediary rather than antagonist, and economic ties at the time seemed to be growing again.

Despite the Sunshine Policy and efforts at reconciliation, the progress was complicated by North Korean missile tests in 1993, 1998, 2006 and 2009. , relationships between North Korea and South Korea were very tense; North Korea had been reported to have deployed missiles, Ended its former agreements with South Korea and threatened South Korea and the United States not to interfere with a satellite launch it had planned.
As of 2009 North Korea and South Korea are still opposed and share a heavily fortified border.

On May 27, 2009 North Korea media declared that the armistice is no longer valid due to the South Korean government's pledge to "definitely join" the Proliferation Security Initiative. To further complicate and intensify strains between the two nations, the sinking of the South Korean warship Cheonan in March 2010, killing 46 seamen, is as of May 20, 2010 claimed by a team of researchers around the world to have been caused by a North Korean torpedo, which the North denies. South Korea agreed with the findings from the research group and president Lee Myung-bak declared in May 2010 that Seoul would cut all trade with North Korea as part of measures primarily aimed at striking back at North Korea diplomatically and financially. As a result of this, North Korea severed all ties and completely abrogated the previous pact of non aggression.

In November 2010, Unification Ministry officially declared the Sunshine Policy a failure, thus bringing the policy to an end. On November 23, 2010, North Korean artillery shelled Yeonpyeong with dozens of rounds at Yeonpyeong-ri and the surrounding area.

South Korea has the following trade agreements:

As of late 2016 states of Central America (Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, Panama, Paraguay), GCC (Gulf Cooperation Council—Bahrain, Kuwait, Oman, Qatar, Saudi Arabia, United Arab Emirates), Indonesia, Israel, Japan, Malaysia, MERCOSUR (Southern Common Market—Mercado comun del sur), Mexico, Mongolia, RCEP (Asian 10 Countries, Korea, China, Japan, Australia, New Zealand, India), Russia (BEPA), SACU (South Asia Cooperation Union) and Korea-China-Japan are in negotiations about the FTA with South Korea.

Active South Korean-Chinese people-to-people contacts have been encouraged. Academics, journalists and particularly families divided between South Korea and the People's Republic of China were able to exchange visits freely in the late 1980s. Nearly 2 million ethnic Koreans especially in the Yanbian Korean Autonomous Prefecture in China's Jilin Province have interacted with South Koreans.

Trade between the two countries continued to increase nonetheless, Furthermore, China has attempted to mediate between North Korea and the United States and between North Korea and the State of Japan also initiated and promoted tripartite talks—between Pyongyang Seoul and Washington United States of America.

South Korea had long been an ally of Taiwan. Diplomatic ties between Seoul and Taipei were nevertheless severed in 1992. Formal diplomatic relations were established between Seoul and Beijing on August 24, 1992.

In 2004 the PRC government began the Northeast Project This sparked a massive uproar in South Korea when the project was widely publicized.

After the KORUS FTA (United States-South Korea Free Trade Agreement) was finalized on June 30, 2007 the Chinese government has immediately begun seeking an FTA agreement with South Korea. The FTA between Korea and China are under discussion South Korea has been running a trade surplus with China which hit a record US$32.5 billion in 2009.

On 23 August 1992, the Taiwanese government (by then only in control of the island of Taiwan and a few major outlying areas) severed diplomatic relations with South Korea in advance of its announcement of formal recognition of the People's Republic of China based in Beijing, China. The Yonhap News said in 2002 that since then relations between the two governments have been "in a rut".

The relation between South Korea and Japan has both political conflicts and economic intimacies. Examples of conflicts include the East sea naming dispute, visits by successive Japanese Prime Ministers to the Yasukuni Shrine and the disputed ownership of Dokdo of the island Korea.

On January 18, 1952 The first president of South Korea Syngman Rhee declared that the vicinity of Dokdo was a territory of South Korea (Syngman Rhee line). Subsequently, some 3,000 Japanese fishermen who conducted fishery operations in this vicinity were captured. This incident, called the Dai Ichi Daihoumaru Ship case strained relations between South Korea and Japan.

June 22, 1965, The president in South Korea Park Chung-hee concluded the Treaty on Basic Relations between Japan and South Korea As a result, Japan considered South Korea to be the legitimate successor of its rule over the Korean Peninsula.

South Korea's trade with Japan was US$892.1 million in 2008, with a surplus of nearly US$327.1 million on the Japanese side. Japanese and South Koreans firms often had interdependent relations, which gave Japan advantages in South Korea's growing market.

In 1996 FIFA announced that the South Korea-Japan would jointly host the 2002 FIFA World Cup. The next few years would see leaders of both countries meet to warm relations in preparations for the games. The year 2005 was designated as the "Japan-South Korea Friendship Year".

However, the Liancourt Rocks controversy erupted again when Japan's Shimane prefecture declared "Takeshima Day", inciting mass demonstrations in South Korea.

Both countries established diplomatic relations on March 26, 1990. South Korea has an embassy in Ulaanbaatar Mongolia. Mongolia has an embassy in Seoul.

According to a 2013 BBC World Service Poll, 3% of South Koreans view the Democratic People's Republic of Korea's influence positively, with 91% expressing a negative view. A 2015 government-sponsored poll revealed that 41% of South Koreans consider North Korea to be an enemy, with negative views being more prevalent among younger respondents. Still, in a 2017 poll, 58% of South Koreans said they don't expect another war to break out with North Korea.

Since the establishment of diplomatic ties on 3 March 1949, the relationship between the Philippines and South Korea has flourished. The Philippines was one of the first countries that extended diplomatic recognition to South Korea. This was cemented with the Philippine government's deployment of the Philippine Expeditionary Force to Korea (PEFTOK) to help South Korea against the invasion of the communist North during the Korean War in the 1950s. After the war, the Philippines provided development assistance to South Korea and helped the country rebuild itself.

Since then, the Philippines's relations with South Korea have evolved with South Korea becoming one of the Philippines's most important bilateral partners aside from the United States, China and Japan. The Philippine government seeks to cultivate strategic ties with South Korea given its increasing presence in the country. In the coming years, the Philippines anticipates to benefit from exploring unprecedented opportunities from South Korea that shall contribute significantly to the country's trade and economy, defense and security, and society and culture.

In the 1980s South Korean president Roh Tae Woo's Nordpolitik and Mikhail Gorbachev's "New Thinking" were both attempts to reverse their nations' recent histories. Gorbachev had signaled Soviet interest in improving relations with all countries in the Asia-Pacific region including South Korea as explained in his July 1986 Vladivostok and August 1988 Krasnoyarsk speeches.

In initiating Nordpolitik Roh's confidential foreign policy adviser was rumored to have visited Moscow Russia to consult with Soviet policymakers. Kim Young Sam visited Moscow Russian Federation from June 2 to June 10, 1989 as the Kremlin announced that it would allow some 300,000 Soviet-South Koreans who had been on the Soviet island of Sahkalin since the end of World War II to return permanently to South Korea. Moscow even arranged Kim's meeting with the North Korean ambassador to the Soviet Union In June 1990, Roh held his first summit with president Gorbachev in San Francisco, United States.

South Korea and the Soviet Union established diplomatic relations on September 30, 1990. These relations continued by the Russian Federation on December 27, 1991. Russian president Vladimir Putin visited Seoul in February 2001 while South Korean president Roh Moo-hyun visited Moscow Russia in September 2004.

Russian Federal Space Agency and the Korean Astronaut Program cooperated together to send South Korea's first astronaut into space. Yi So-Yeon became the first South Korean national as well as the third woman to be the first national in space on 8 April 2008 when Soyuz TMA-12 departed from Baikonur Cosmodrome.

Since the 1990s there has been greater trade and cooperation between the Russian Federation and South Korea. The total trade volume between South Korea and Russia in 2003 was 4.2 billion US dollars.

The establishment of diplomatic relations between the United Kingdom and South Korea began on 18 January 1949.

From South Korea to the United Kingdom:

From the United Kingdom to South Korea:

The United States engaged in the decolonization of Korea (mainly in the South, with the Soviet Union engaged in North Korea) from Japan after World War II. After three years of military administration by the United States, the South Korean government was established. Upon the onset of the Korean War, U.S. forces were sent to defend South Korea against invasion by North Korea and later China. Following the Armistice, South Korea and the U.S. agreed to a "Mutual Defense Treaty", under which an attack on either party in the Pacific area would summon a response from both.

In 1968, South Korea obliged the mutual defense treaty, by sending a large combat troop contingent to support the United States in the Vietnam War. The U.S. Eighth Army, Seventh Air Force, and U.S. Naval Forces Korea are stationed in South Korea. The two nations have strong economic, diplomatic, and military ties, although they have at times disagreed with regard to policies towards North Korea, and with regard to some of South Korea's industrial activities that involve usage of rocket or nuclear technology. There had also been strong anti-American sentiment during certain periods, which has largely moderated in the modern day.

Since the late 1980s, the country has instead sought to establish an American partnership, which has made the Seoul–Washington relationship subject to severe strains. Trade had become a serious source of friction between the two countries. In 1989, the United States was South Korea's largest and most important trading partner and South Korea was the seventh-largest market for United States goods and the second largest market for its agricultural products.

From Roh Tae-woo's administration to Roh Moo Hyun's administration, South Korea sought to establish a U.S. partnership, which has made the Seoul–Washington relationship subject to some strains. In 2007, a free trade agreement known as the Republic of Korea-United States Free Trade Agreement (KORUS FTA) was reportedly signed between South Korea and the United States, but its formal implementation has been repeatedly delayed, pending further approval by the legislative bodies of the two countries.

The relations between the United States and South Korea have greatly strengthened under the Lee Myung-bak administration. At the 2009 G-20 London summit, U.S. President Barack Obama called South Korea "one of America's closest allies and greatest friends."
However, some anti-American sentiment in South Korea still exists; The United States' alleged role in the May 1980 Gwangju uprising was a pressing South Korean political issue of the 1980s. Even after a decade, some Gwangju citizens and other South Koreans still blamed the United States for its perceived involvement in the bloody uprising. In 2008, the protests against U.S. beef was a center of a major controversy that year.

In a June 2010 open letter from President of South Korea Lee Myung-bak published in the "Los Angeles Times", he expressed gratitude for the 37,000 Americans who were killed in the Korean War defending South Korea, saying that they fought for the freedom of South Koreans they did not even know. He stated that thanks to their sacrifices, the peace and democracy of the South Korean state was protected.

The U.S. states that "The Alliance is adapting to changes in the 21st Century security environment. We will maintain a robust defense posture, backed by allied capabilities which support both nations' security interests We will continue to deepen our strong bilateral economic, trade and investment relations In the Asia-Pacific region we will work jointly with regional institutions and partners to foster prosperity, keep the peace, and improve the daily lives of the people of the region The United States and South Korea will work to achieve our common Alliance goals through strategic cooperation at every level."

The European Union (EU) and South Korea are important trading partners, having negotiated a free trade agreement for many years since South Korea was designated as a priority FTA partner in 2006. The free trade agreement has been approved in September 2010, following Italy's conditional withdrawal of its veto of the free trade agreement. The compromise made by Italy was that free trade agreement would take provisional effect on July 1, 2011. South Korea is the EU's eighth largest trade partner and the EU has become South Korea's second largest export destination. EU trade with South Korea exceeded €65 billion in 2008 and has enjoyed an annual average growth rate of 7.5% between 2004 and 2008.

The EU has been the single largest foreign investor in South Korea since 1962 and accounted for almost 45% of all FDI inflows into South Korea in 2006. Nevertheless, EU companies have significant problems accessing and operating in South Korea market due to stringent standards and testing requirements for products and services often creating barriers to trade. Both in its regular bilateral contacts with South Korea and through its FTA with South Korea The EU is seeking to improve this situation.

South Korea does not currently have any diplomatic relations with the following nations.

There are also no diplomatic relations with several unrecognized territories:





</doc>
<doc id="27029" url="https://en.wikipedia.org/wiki?curid=27029" title="List of cities in South Korea">
List of cities in South Korea

The largest cities of South Korea have an autonomous status equivalent to that of provinces. Seoul, the largest city and capital, is classified as a teukbyeolsi (Special City), while the next 6 largest cities (see the list below) are classified as gwangyeoksi (Metropolitan Cities; see: List of special cities of South Korea). Smaller cities are classified as si ("cities") and are under provincial jurisdiction, at the same level as counties (see Administrative divisions of South Korea).

Under South Korean laws sets the following condition for a municipality to be designated as a city: population of a county must generally be 150,000 or greater or passage of a special legislative bill by the National Assembly such as Gyeryong.

The national government can designate cities of at least 500,000 inhabitants to have the status of special status city. These statuses expand the scope of administrative authority delegated from the provincial government to the city government. A can also be reclassified as a Metropolitan city if it contains at least 1,000,000 inhabitants. Only Suwon and Changwon meet these requirements but neither city has been nominated.

A specific city is a municipal city that has a population greater than 500,000 and has been designated as such by an order of the national government under Article 175 of the Local Autonomy Law. Specific city are given powers to subdivide themselves into non-autonomous districts (; ) but, not all Specific city are subdivided into non-autonomous districts such as Bucheon, Gimhae, Hwaseong, or Namyangju. Currently South Korea have a total of 15 Specific cities.






</doc>
<doc id="27031" url="https://en.wikipedia.org/wiki?curid=27031" title="Schoolly D">
Schoolly D

Jesse Bonds Weaver Jr. (born June 22, 1962), better known by the stage name Schoolly D (sometimes spelled Schooly D), is an American rapper from Philadelphia, Pennsylvania.

Schoolly D teamed up with DJ Code Money in the mid-1980s. His lyrics reflected urban realism, violence, and sexual bravado, making Schoolly D the first gangsta rapper (see below). He is also interviewed in the 1986 cult documentary "Big Fun in the Big Town". He later embraced an Afrocentric style, bringing Afrocentric culture to hip hop along with KRS-One.

Schoolly D contributed songs and music to many Abel Ferrara films, including "P.S.K." and "Saturday Night" (from "Saturday Night! – The Album") as well as "King of New York" to Ferrara's film of the same name and the title track from "Am I Black Enough For You?" that was played during the climactic shoot-out in that film, the title track from "How a Black Man Feels", and "Signifying Rapper" (from "Smoke Some Kill"), which was used in Ferrara's film "Bad Lieutenant". Because Led Zeppelin successfully sued due to an uncleared interpolation of its song "Kashmir" in "Signifying Rapper", the song was omitted from the soundtrack of the film and from subsequent releases of the film.

Composer Joe Delia tapped Schoolly to co-write and record "The Player" for Ferrara's film "The Blackout", which Delia scored. Schoolly also wrote the score to Ferrara's "'R Xmas". In 2006, Schoolly D co-wrote the indie film soundtrack of the historical science fiction thriller "Order of the Quest" with Chuck Treece. The project series is produced by Benjamin Barnett, and Jay D Clark of Media Bureau. His last album, "Funk 'N Pussy", on Jeff "Met" Thies' Chord Recordings features guest appearances by Public Enemy's Chuck D, Chuck Chillout, Lady B and a drum and bass remix of the classic Schoolly D track "Mr. Big Dick" (remixed by UK trip hop crew The Sneaker Pimps).

Schoolly also performed the music and occasional narration for the cult animated series "Aqua Teen Hunger Force" on the Cartoon Network's Adult Swim programming block. He was a guest on an episode of "Space Ghost Coast to Coast". He also created the song "Sharkian Nights" for the "12 oz. Mouse". The character Jesse B. Weaver from "The Rudy and Gogo World Famous Cartoon Show" was also named after him.

In November 2006 Schoolly D and Cartoon Network were sued over the "Aqua Teen Hunger Force" theme music. A drummer by the name of Terence Yerves claimed he had also written the theme music alongside Schoolly D in 1999 while working at the Meat Locker Studio. Yerves was aware the song would be used for a television series but did not approve of it being used for "Aqua Teen Hunger Force", however, did not file the copyright to the Library of Congress until May 2006, after the series' fourth season had already started airing. In the lawsuit Yerves demanded he receive $150,000 for every time the series was aired after the lawsuit was filed, he also demanded that all existing copies of the series' DVDs be impounded and for "Aqua Teen Hunger Force" to cease broadcast.

Rapper Ice-T, who is often given credit for the creation of gangsta rap, says that Schoolly D was the first gangsta rapper.

In the DVD extra on the "King of New York", Schoolly D claims to have independently invented the sport of snowboarding by sledding down Philadelphia hills on pieces of linoleum. (Snowboarding has roots in snurfing, which was invented in 1965.)

Funk metal band Primus mentions Schoolly D in their song "Harold of the Rocks" on the album "Frizzle Fry".

The Beastie Boys sampled Schoolly D's song "Gucci Time" on their 1986 hit "Time to Get Ill."





</doc>
<doc id="27032" url="https://en.wikipedia.org/wiki?curid=27032" title="Rock paper scissors">
Rock paper scissors

Rock paper scissors (also known as scissors rock paper, paper rock scissors and scissors paper stone) is a hand game usually played between two people, in which each player simultaneously forms one of three shapes with an outstretched hand. These shapes are "rock" (a closed fist), "paper" (a flat hand), and "scissors" (a fist with the index finger and middle 
finger extended, forming a V). "Scissors" is identical to the two-fingered V sign (also indicating "victory" or "peace") except that it is pointed horizontally instead of being held upright in the air. 
A simultaneous, zero-sum game, it has only two possible outcomes: a draw, or a win for one player and a loss for the other.

A player who decides to play rock will beat another player who has chosen scissors ("rock crushes scissors" or sometimes "blunts scissors"), but will lose to one who has played paper ("paper covers rock"); a play of paper will lose to a play of scissors ("scissors cuts paper"). If both players choose the same shape, the game is tied and is usually immediately replayed to break the tie. The type of game originated in China and spread with increased contact with East Asia, while developing different variants in signs over time. Other names for the game in the English-speaking world include roshambo and other orderings of the three items, with "rock" sometimes being called "stone".

Rock paper scissors is often used as a fair choosing method between two people, similar to coin flipping, drawing straws, or throwing dice in order to settle a dispute or make an unbiased group decision. Unlike truly random selection methods, however, rock paper scissors can be played with a degree of skill by recognizing and exploiting non-random behavior in opponents.

The players usually count aloud to three, or speak the name of the game (e.g. "Rock! Paper! Scissors!" or "Ro Sham Bo!"). Sometimes people would say "Go!" after "Scissors!". Each time either raising one hand in a fist and swinging it down on the count or holding it behind. They then "throw" by extending it towards their opponent. Variations include a version where players use only three counts before throwing their gesture (thus throwing on the count of "Scissors!" or "Bo!"), or a version where they shake their hands three times before "throwing".

The first known mention of the game was in the book "" by the Chinese Ming-dynasty writer ( 1600), who wrote that the game dated back to the time of the Chinese Han dynasty (206 BC – 220 AD). In the book, the game was called "shoushiling". Li Rihua's book "Note of Liuyanzhai" also mentions this game, calling it "shoushiling" (t. 手勢令; s. 手势令), "huozhitou" (t. 豁指頭; s. 豁指头), or "huaquan" (划拳).
Throughout Japanese history there are frequent references to "sansukumi-ken", meaning "ken" (fist) games where "the three who are afraid of one another" (i.e. A beats B, B beats C, and C beats A). This type of game originated in China before being imported to Japan and subsequently also becoming popular among the Japanese.

The earliest Japanese "sansukumi-ken" game was known as "mushi-ken" (虫拳), which was imported directly from China. In "mushi-ken" the "frog" (represented by the thumb) is superseded by the "slug" (represented by the little finger), which, in turn is superseded by the "snake" (represented by the index finger), which is superseded by the "frog". Although this game was imported from China the Japanese version differs in the animals represented. In adopting the game, the original Chinese characters for the poisonous centipede (蜈蜙) were apparently confused with the characters for the slug (蛞蝓). The most popular "sansukumi-ken" game in Japan was "kitsune-ken" (). In the game, a supernatural fox called a kitsune (狐) defeats the village head, the village head (庄屋) defeats the hunter, and the hunter (猟師) defeats the fox. "Kitsune-ken", unlike "mushi-ken" or rock–paper–scissors, is played by making gestures with both hands.

Today, the best-known "sansukumi-ken" is called , which is a variation of the Chinese games introduced in the 17th century. "Jan-ken" uses the rock, paper, and scissors signs and is the game that the modern version of rock paper scissors derives from directly. Hand-games using gestures to represent the three conflicting elements of rock, paper, and scissors have been most common since the modern version of the game was created in the late 19th century, between the Edo and Meiji periods.

By the early 20th century, rock paper scissors had spread beyond Asia, especially through increased Japanese contact with the west. Its English-language name is therefore taken from a translation of the names of the three Japanese hand-gestures for rock, paper and scissors: elsewhere in Asia the open-palm gesture represents "cloth" rather than "paper". The shape of the scissors is also adopted from the Japanese style.

A 1921 article about cricket in the "Sydney Morning Herald" described "stone, scissors, and paper" as a "Teutonic method of drawing lots", which the writer "came across when travelling on the Continent once". Another article, from the same year, in the "Washington Herald" described it as a method of "Chinese gambling".
In Britain in 1924 it was described in a letter to "The Times" as a hand game, possibly of Mediterranean origin, called "zhot".
A reader then wrote in to say that the game "zhot" referred to was evidently Jan-ken-pon, which she had often seen played throughout Japan. Although at this date the game appears to have been new enough to British readers to need explaining, the appearance by 1927 of a popular thriller with the title "Scissors Cut Paper", followed by "Stone Blunts Scissors" (1929), suggests it quickly became popular.

In 1927 "La Vie au patronage", a children's magazine in France, described it in detail, referring to it as a "jeu japonais" ("Japanese game"). Its French name, "Chi-fou-mi", is based on the Old Japanese words for "one, two, three" ("hi, fu, mi").

A 1932 "New York Times" article on the Tokyo rush hour describes the rules of the game for the benefit of American readers, suggesting it was not at that time widely known in the U.S. The 1933 edition of the "Compton's Pictured Encyclopedia" described it as a common method of settling disputes between children in its article on Japan; the name was given as "John Kem Po" and the article pointedly asserted, "This is such a good way of deciding an argument that American boys and girls might like to practice it too."

It is impossible to gain an advantage over a truly random opponent. However, by exploiting the psychological weaknesses of inherently non-random opponents, it is possible to gain a significant advantage. Indeed, human players tend to be non-random. As a result, there have been programming competitions for algorithms that play rock paper scissors.

During tournaments, players often prepare their sequence of three gestures prior to the tournament's commencement. Some tournament players employ tactics to confuse or trick the other player into making an illegal move, resulting in a loss. One such tactic is to shout the name of one move before throwing another, in order to misdirect and confuse their opponent.

The "rock" move, in particular, is notable in that it is typically represented by a closed fist—often identical to the fist made by players during the initial countdown. If a player is attempting to beat their opponent based on quickly reading their hand gesture as the players are making their moves, it is possible to determine if the opponent is about to throw "rock" based on their lack of hand movement, as both "scissors" and "paper" require the player to reposition their hand. This can likewise be used to deceive an anticipating opponent by keeping one's fist closed until the last possible moment, leading them to believe that you are about to throw "rock".

As a consequence of rock paper scissors programming contests, many strong algorithms have emerged. For example, Iocaine Powder, which won the First International RoShamBo Programming Competition in 1999, uses a heuristically designed compilation of strategies. For each strategy it employs, it also has six metastrategies which defeat second-guessing, triple-guessing, as well as second-guessing the opponent, and so on. The optimal strategy or metastrategy is chosen based on past performance. The main strategies it employs are history matching, frequency analysis, and random guessing. Its strongest strategy, history matching, searches for a sequence in the past that matches the last few moves in order to predict the next move of the algorithm. In frequency analysis, the program simply identifies the most frequently played move. The random guess is a fallback method that is used to prevent a devastating loss in the event that the other strategies fail. More than ten years later, the top performing strategies on an ongoing rock–paper–scissors programming competition similarly use metastrategies. However, there have been some innovations, such as using multiple history matching schemes that each match a different aspect of the history – for example, the opponent's moves, the program's own moves, or a combination of both. There have also been other algorithms based on Markov chains.

In 2012, researchers from the Ishikawa Watanabe Laboratory at the University of Tokyo created a robot hand that can play rock paper scissors with a 100% win rate against a human opponent. Using a high-speed camera the robot recognizes within one millisecond which shape the human hand is making, then produces the corresponding winning shape.

In 2006, American federal judge Gregory Presnell from the Middle District of Florida ordered opposing sides in a lengthy court case to settle a trivial (but lengthily debated) point over the appropriate place for a deposition using the game of rock paper scissors. The ruling in "Avista Management v. Wausau Underwriters" stated:

In 2005, when Takashi Hashiyama, CEO of Japanese television equipment manufacturer Maspro Denkoh, decided to auction off the collection of Impressionist paintings owned by his corporation, including works by Paul Cézanne, Pablo Picasso, and Vincent van Gogh, he contacted two leading auction houses, Christie's International and Sotheby's Holdings, seeking their proposals on how they would bring the collection to the market as well as how they would maximize the profits from the sale. Both firms made elaborate proposals, but neither was persuasive enough to earn Hashiyama's approval. Unwilling to split up the collection into separate auctions, Hashiyama asked the firms to decide between themselves who would hold the auction, which included Cézanne's "Large Trees Under the Jas de Bouffan", worth $12–16 million.

The houses were unable to reach a decision. Hashiyama told the two firms to play rock paper scissors to decide who would get the rights to the auction, explaining that "it probably looks strange to others, but I believe this is the best way to decide between two things which are equally good".

The auction houses had a weekend to come up with a choice of move. Christie's went to the 11-year-old twin daughters of the international director of Christie's Impressionist and Modern Art Department Nicholas Maclean, who suggested "scissors" because "Everybody expects you to choose 'rock'." Sotheby's said that they treated it as a game of chance and had no particular strategy for the game, but went with "paper". Christie's won the match and sold the $20 million collection, earning millions of dollars of commission for the auction house.

Prior to a 26 October 2018 match in the FA Women's Super League, the referee, upon being without a coin for the pregame coin toss, had the team captains play rock paper scissors to determine which team would kick-off. The referee was subsequently suspended for three weeks by The Football Association.

In Japan, researchers have taught chimpanzees the rules of rock paper scissors.

In many games, it is common for a group of possible choices to interact in a rock paper scissors style, where each selection is strong against a particular choice, but weak against another. For instance, big ship – small ship – submarine, or cavalry–artillery–infantry. Such mechanics can make a game somewhat self-balancing, prevent gameplay from being overwhelmed by a single dominant strategy and single dominant type of unit.

Many card-based video games in Japan use the rock paper scissors system as their core fighting system, with the winner of each round being able to carry out their designated attack. In "Alex Kidd in Miracle World", the player has to win games of rock paper scissors against each boss to proceed. Others use simple variants of rock paper scissors as subgames like "" (Reflex Rally) and "" (Super Roshambo).

Many Nintendo role-playing games prominently feature a rock paper scissors gameplay element. In "Pokémon", there is a rock paper scissors element in the type effectiveness system. For example, a Grass-type Pokémon is weak to Fire, Fire is weak to Water, and Water is weak to Grass. In "" and "", the battles in the second mode (Minion Quest: The Search for Bowser / Bowser Jr.'s Journey) use a “Power Triangle” system based on the game's three attack types: Melee, Ranged, and Flying. In the "Fire Emblem" series of strategy role-playing games, the Weapon Triangle and Trinity of Magic influence the hit and damage rates of weapon types based on whether they are at an advantage or a disadvantage in their respective rock paper scissors system. In "Super Smash Bros. Ultimate", Primary Spirits have three different types based on whether they are at an advantage or a disadvantage in their respective rock paper scissors system during a Spirit Battle. These types are Attack (red), Defense (blue), and Grab (green).

The common side-blotched lizard ("Uta stansburiana") exhibits a rock paper scissors pattern in its mating strategies. Of its three color types of males, "orange beats blue, blue beats yellow, and yellow beats orange" in competition for females, which is similar to the rules of rock-paper-scissors.

Some bacteria also exhibit a rock paper scissors dynamic when they engage in antibiotic production. The theory for this finding was demonstrated by computer simulation and in the laboratory by Benjamin Kerr, working at Stanford University with Brendan Bohannan. Additional "in vitro" results demonstrate rock paper scissors dynamics in additional species of bacteria. Biologist Benjamin C. Kirkup, Jr. demonstrated that these antibiotics, bacterioicins, were active as "Escherichia coli" compete with each other in the intestines of mice, and that the rock paper scissors dynamics allowed for the continued competition among strains: antibiotic-producers defeat antibiotic-sensitives; antibiotic-resisters multiply and withstand and out-compete the antibiotic-producers, letting antibiotic-sensitives multiply and out-compete others; until antibiotic-producers multiply again.

Rock paper scissors is the subject of continued research in bacterial ecology and evolution. It is considered one of the basic applications of game theory and non-linear dynamics to bacteriology. Models of evolution demonstrate how intragenomic competition can lead to rock paper scissors dynamics from a relatively general evolutionary model. The general nature of this basic non-transitive model is widely applied in theoretical biology to explore bacterial ecology and evolution.

In the televised robot combat competition "BattleBots", relations between "lifters, which had wedged sides and could use forklift-like prongs to flip pure wedges", "spinners, which were smooth, circular wedges with blades on their bottom side for disabling and breaking lifters", and "pure wedges, which could still flip spinners" are analogical to relations in rock paper scissors games and called "robot Darwinism". Also specially designed rock paper scissors game" mechanical devices can demonstrate intransitivity of relations such as "to rotate faster than", "to lift and be not be lifted", "to be stronger than" in some geometrical constructions.

Various competitive rock paper scissors tournaments have been organised by different groups.

Starting in 2002, the World Rock Paper Scissors Society standardized a set of rules for international play and has overseen annual International World Championships. These open, competitive championships have been widely attended by players from around the world and have attracted widespread international media attention. WRPS events are noted for their large cash prizes, elaborate staging, and colorful competitors.
In 2004, the championships were broadcast on the U.S. television network Fox Sports Net, with the winner being Lee Rammage, who went on to compete in at least one subsequent championship. The 2007 tournament was won by Andrea Farina. The last tournament hosted by the World RPS Society was in Toronto, Canada, on November 14, 2009.

Several RPS events have been organised in the United Kingdom by Wacky Nation. The 1st UK Championship took place on 13 July 2007, and were then held annually. 

USA Rock Paper Scissors League is sponsored by Bud Light. Leo Bryan Pacis was the first commissioner of the USARPS. Cody Louis Brown was elected as the second commissioner of the USARPS in 2014.

In April 2006, the inaugural USARPS Championship was held in Las Vegas. Following months of regional qualifying tournaments held across the US, 257 players were flown to Las Vegas for a single-elimination tournament at the House of Blues where the winner received $50,000. The tournament was shown on the A&E Network on 12 June 2006.

The $50,000 2007 USARPS Tournament took place at the Las Vegas Mandalay Bay in May 2007.

In 2008, Sean "Wicked Fingers" Sears beat 300 other contestants and walked out of the Mandalay Bay Hotel and Casino with $50,000 after defeating Julie "Bulldog" Crossley in the finals.

The inaugural Budweiser International Rock, Paper, Scissors Federation Championship was held in Beijing, China after the close of the 2008 Summer Olympics at Club Bud. A Belfast man won the competition.

The international tournament was held in London 2012. UK Champions Team GB (Andrew Bladon, Jamie Burland, Tom Wilkinson and Stephen Preston) went in as overwhelming favorites, but after a "domestic incident" team captain and UK Team Champion Joe Kenny was forced to pull out, allowing Stephen Preston to take his place. Great Britain came a respectable third to achieve the Bronze Medal, while the crowd favorite Vatican City got the Silver and Lapland A took the prestigious Gold Medal. British team captain Tom Wilkinson commented "after a 4-0 whitewash of hot favorites Vatican City we thought we had it. A simple lapse of concentration lost it for us, but we are happy with our bronze medal. We'll come back from this and look to take the title back again next year. The support was immense, and we are thankful of everyone who came out to support us".

The XtremeRPS National Competition is a US nationwide RPS competition with Preliminary Qualifying contests that started in January 2007 and ended in May 2008, followed by regional finals in June and July 2008. The national finals were to be held in Des Moines, Iowa in August 2008, with a chance to win up to $5,000.

The largest Rock Paper Scissors tournament is 2,950 and was achieved by Oomba, Inc. (USA) at Gen Con 2014 in Indianapolis, Indiana, United States, on 17 August 2014.

Former Celebrity Poker Showdown host and USARPS Head Referee Phil Gordon has hosted an annual $500 World Series of Rock Paper Scissors event in conjunction with the World Series of Poker since 2005. The winner of the WSORPS receives an entry into the WSOP Main Event. The event is an annual fundraiser for the "Cancer Research and Prevention Foundation" via Gordon's charity "Bad Beat on Cancer". Poker player Annie Duke won the Second Annual World Series of Rock Paper Scissors. The tournament is taped by ESPN and highlights are covered during "The Nuts" section of ESPN's annual WSOP broadcast. 2009 was the fifth year of the tournament.

Jackpot En Poy is a game segment on the Philippines' longest running midday show, "Eat Bulaga!". The game is based on the classic children's game rock paper scissors where four players are paired to compete in the three-round segment. In the first round, the first pair plays against each other until one player wins three times. The next pair then plays against each other in the second round. The winners from the first two rounds then compete against each other to finally determine the ultimate winner. The winner of the game then moves on to the final round. In the final round, the player is presented with several Dabarkads, each holding different amounts of cash prize. The player will then pick three Dabarkads who he or she will play rock paper scissors against. The player plays against them one at a time. If the player wins against any of the Eat Bulaga! host, he or she will win the cash prize.

Players have developed numerous cultural and personal variations on the game, from simply playing the same game with different objects, to expanding into more weapons and rules, to giving their own name to the game in their national language.

In Korea, a two-player upgraded version exists by the name muk-jji-ppa.

In Japan, a "strip-poker" variant of rock paper scissors is known as 野球拳 (Yakyuken). The loser of each round removes an article of clothing. The game is a minor part of porn culture in Japan and other Asian countries after the influence of TV variety shows and Soft On Demand.

In the Philippines, the game is called "jak-en-poy", from one of the Japanese names of the game, transliterated as "jan-ken-pon". In a longer version of the game, a four-line song is sung, with hand gestures displayed at the end of each (or the final) line: "Jack-en-poy! / Hali-hali-hoy! / Sino'ng matalo, / siya'ng unggoy!" ("Jack-en-poy! / Hali-hali-hoy! / Whoever loses is the monkey!") In the former case, the person with the most wins at the end of the song, wins the game. A shorter version of the game uses the chant "Bato-bato-pick" ("Rock-rock-pick [i.e. choose]") instead.

A multiple player variation can be played: Players stand in a circle and all throw at once. If rock, paper, and scissors are all thrown, it is a stalemate, and they rethrow. If only two throws are present, all players with the losing throw are eliminated. Play continues until only the winner remains.

In the Malaysian version of the game, "scissors" is replaced by "bird," represented with the finger tips of five fingers brought together to form a beak. The open palm represents water. Bird beats water (by drinking it); stone beats bird (by hitting it); and stone loses to water (because it sinks in it).

Singapore also has a related hand-game called "ji gu pa," where "ji" refers to the bird gesture, "gu" refers to the stone gesture, and "pa" refers to the water gesture. The game is played by two players using both hands. At the same time, they both say, ji gu pa!" At "pa!" they both show two open-palmed hands. One player then changes his hand gestures while calling his new combination out (e.g., "pa gu!"). At the same time, the other player changes his hand gestures as well. If one of his hand gestures is the same as the other one, that hand is "out" and he puts it behind his back; he is no longer able to play that hand for the rest of the round. The players take turns in this fashion, until one player loses by having both hands sent "out." "Ji gu pa" is most likely a transcription of the Japanese names for the different hand gestures in the original jan-ken game, "choki" (scissors), "guu" (rock) and "paa" (paper).

Using the same tripartite division, there is a full-body variation in lieu of the hand signs called "Bear, Hunter, Ninja". In this iteration the participants stand back-to-back and at the count of three (or ro-sham-bo as is traditional) turn around facing each other using their arms evoking one of the totems. The players' choices break down as: Hunter shoots bear; Bear eats ninja; Ninja kills hunter. The game was popularized with a FedEx commercial where warehouse employees had too much free time on their hands.

As long as the number of moves is an odd number and each move defeats exactly half of the other moves while being defeated by the other half, any combination of moves will function as a game. For example, 5-, 7-, 9-, 11-, 15-, 25-, and 101-weapon versions exist. Adding new gestures has the effect of reducing the odds of a tie, while increasing the complexity of the game. The probability of a tie in an odd-number-of-weapons game can be calculated based on the number of weapons n as 1/n, so the probability of a tie is 1/3 in standard rock paper scissors, but 1/5 in a version that offered five moves instead of three.

Similarly, the French game "pierre, papier, ciseaux, puits" (stone, paper, scissors, well) is unbalanced; both the stone and scissors fall in the well and lose to it, while paper covers both stone and well. This means two "weapons", well and paper, can defeat two moves, while the other two weapons each defeat only one of the other three choices. The rock has no advantage to well, so optimal strategy is to play each of the other objects (paper, scissors and well) one third of the time.
One popular five-weapon expansion is "", invented by Sam Kass and Karen Bryla, which adds "Spock" and "lizard" to the standard three choices. "Spock" is signified with the "Star Trek" Vulcan salute, while "lizard" is shown by forming the hand into a sock-puppet-like mouth. Spock smashes scissors and vaporizes rock; he is poisoned by lizard and disproved by paper. Lizard poisons Spock and eats paper; it is crushed by rock and decapitated by scissors. This variant was mentioned in a 2005 article in "The Times" of London and was later the subject of an episode of the American sitcom "The Big Bang Theory" in 2008 (as rock-paper-scissors-lizard-Spock).

The majority of such proposed generalizations are isomorphic to a simple game of modular arithmetic, where half the differences are wins for player one. For instance, rock paper scissors Spock lizard (note the different order of the last two moves) may be modeled as a game in which each player picks a number from one to five. Subtract the number chosen by player two from the number chosen by player one, and then take the remainder modulo 5 of the result. Player one is the victor if the difference is one or three, and player two is the victor if the difference is two or four. If the difference is zero, the game is a tie.

Alternatively, the rankings in rock paper scissors Spock lizard may be modeled by a comparison of the parity of the two choices. If it is the same (two odd-numbered moves or two even-numbered ones) then the lower number wins, while if they are different (one odd and one even) the higher wins. Using this algorithm, additional moves can easily be added two at a time while keeping the game balanced:

Any variation of Rock-Paper-Scissors is an oriented graph. According to theoretical calculations, the number of distiguishable oriented graphs, every of which is a potentially playable Rock-Paper-Scissors game, grows with the number of weapons = 3, 4, 5, … as follow:
Of cause, most of them are either unreasonable or unbalanced. Anyway, a game-theoretic analysis showed that 4 variants of 582 possible variations using 5 different weapons have non-trivial mixed strategy equilibria. The most representative game of these 4 is Rock-Paper-Scissors-Fire-Water. Rock beats Scissors, Paper beats Rock, Scissors beats Paper, Fire beats everything except Water and Water is beaten by everything except it beats Fire. The perfect game-theoretic strategy is to use Rock, Paper and Scissors formula_1 of the time and formula_2 of the time for Fire and Water. Nevertheless, experiments show that people underuse Water and overuse Rock, Paper and Scissors in this game.


Notes
Bibliography



</doc>
<doc id="27033" url="https://en.wikipedia.org/wiki?curid=27033" title="Logudorese dialect">
Logudorese dialect

Logudorese Sardinian (, ) is one of the two written standards of Sardinian. The orthography is based on the spoken dialects of central northern Sardinia, identified by certain attributes which are not found, or found to a lesser degree, among the Sardinian dialects centered on the other written form, Campidanese. Its ISO 639-3 code is "src". Italian-speakers do not understand Logudorese, like any other dialect of the Sardinian language: Sardinian is an autonomous linguistic group rather than a dialect of Italian as it is often noted because of its morphological, synctatic, and lexical differences from Italian.

Latin G and K before , are not palatalized in Logudorese, in stark contrast with all other Romance languages. Compare Logudorese ' with Italian ' , Spanish ' and French ' . Like the other varieties of Sardinian, most subdialects of Logudorese also underwent lenition in the intervocalic plosives of --, --, and --/ (e.g. Lat. "focum" > "fogu" "fire", "ripa" > "riba" "shore, bank", "rota" > "roda" "wheel"). Logudorese also turns medial and into and and , respectively (e.g. Lat. "Sardinia" > "Sardigna" and "folium" > "fogia" "leaf"). Finally, Logudorese shifts the Latin labiovelars and into medially and word-initially (Lat. "lingua" > "limba" "tongue", "qualem" > "cale" "what")

Logudorese is intelligible to those from the southern part of Sardinia, where Campidanese Sardinian is spoken, but it is not to those from the extreme north of the island, where Corsican–Sardinian dialects are spoken.

The area of Logudoro (term originated as a blend of the kingdom's name of Logu de Torres) in which it is spoken, a northern subregion of the island of Sardinia with close ties to Ozieri ("Othieri") and Nuoro ("Nùgoro") for culture and language, as well as history, with important particularities in the western area, where the most important town is Ittiri. It is an area of roughly 150 × 100 km with some 500,000–700,000 inhabitants.

The origins of Sardinian have been investigated by Eduardo Blasco Ferrer and others. The language derives from Latin and a pre-Latin, Paleo-Sardinian (Nuragic) substratum, but has been influenced by Catalan and Spanish due to the dominion of the Crown of Aragon and later the Spanish Empire over the island. Logudorese is the northern macro-dialect of the Sardinian language, the southern macro-dialect being Campidanese, spoken in the southern half of the island. The two dialects share a clear common origin and history, but have experienced somewhat different developments.

Though the language is typically Romance, some words are not of Latin origin, and are of uncertain etymology. One such is "nura", found in "nuraghe", the main form of pre-Roman building, hence the term for the pre-Roman era as the Nuragic Period. Various place names similarly have roots that defy analysis.

Logudorese Sardinian changed only very slowly from Vulgar Latin in comparison to other Romance lects, with Linguist Mario Pei reporting an 8% degree of separation from Latin in the Nuorese subdialect, the most conservative compared to other Romance Languages. Because of this reason, as well as the preservation of many works of traditional literature from the 15th century onwards, Logudorese is often considered to be the most prestigious variety of Sardinian.

Logudorese Sardinian has multiple subdialects, some confined to individual villages or valleys. Though such differences can be noticeable, the dialects are mutually intelligible, and share mutual intelligibility with the neighbouring Campidanese dialects as well.

Spoken in the north of Sardinia, this subdialect contains the following features:

, , changes to , , (Lat. "plovere" > "piòere" "rain", "florem" > "fiore" "flower", "clavem" > "ciae" "key")

Spoken in Central Sardinia, this subdialect contains the following features:

, , changes to , , (Lat. "plovere" > "pròere" "rain", "florem" > "frore" "flower", "clavem" > "crae" "key")

The Nuorese dialects (spoken in Nuoro and Baronia) have some distinctive features not found anywhere else in Sardinia, many features demonstrating the conservative nature of the dialect:

No lenition of intervocalic plosives (e.g. Lat. "focum" > "focu" "fire", "ripa" > "ripa" "shore, bank", "rota" > "rota" "wheel")

No palatal realisation of and , instead turning into and and , respectively (e.g. Lat. "Sardinia" > "Sardinna" and "folium" > "folla" "leaf").

Preservation of intervocalic , , and (Lat. "augustus" "August" > Log. "austu" but Nuo. "agustu", Lat. "credere" "to believe" > Log. "creere" but Nuo. "credere", Lat. "novem" "nine" > Log. "noe" vs Nuo. "nobe" < "nove")

Betacism of in Nuoro but not in Baronia.

Latin before yod to in Nuorese ("plateam" "street, courtyard" > "pratha"), albeit the sound is in the process of becoming ("pratza").

A large body of Sardinian poetry, songs and literature is composed in Logudorese.




</doc>
